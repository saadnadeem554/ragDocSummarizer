[
  "Artiﬁcial Intelligence\nA Modern Approach\nThird Edition PRENTICE HALL SERIES\nIN ARTIFICIAL INTELLIGENCE\nStuart Russell and Peter Norvig, Editors\nFORSYTH & PONCE\nComputer Vision: A Modern Approach\nGRAHAM\nANSI Common Lisp\nJURAFSKY & MARTIN\nSpeech and Language Processing, 2nd ed.\nNEAPOLITAN\nLearning Bayesian Networks\nRUSSELL & NORVIG\nArtiﬁcial Intelligence: A Modern Approach, 3rd ed. Artiﬁcial Intelligence\nA Modern Approach\nThird Edition\nStuart J. Russell and Peter Norvig\nContributing writers:\nErnest Davis\nDouglas D. Edwards\nDavid Forsyth\nNicholas J. Hay\nJitendra M. Malik\nVibhu Mittal\nMehran Sahami\nSebastian Thrun\nUpper Saddle River\nBoston\nColumbus\nSan Francisco\nNew York\nIndianapolis\nLondon\nToronto\nSydney\nSingapore\nTokyo\nMontreal\nDubai\nMadrid\nHong Kong\nMexico City\nMunich\nParis\nAmsterdam\nCape Town Vice President and Editorial Director, ECS: Marcia J. Horton\nEditor-in-Chief: Michael Hirsch\nExecutive Editor: Tracy Dunkelberger\nAssistant Editor: Melinda Haggerty\nEditorial Assistant: Allison Michael\nVice President, Production: Vince O’Brien\nSenior Managing Editor: Scott Disanno\nProduction Editor: Jane Bonnell\nSenior Operations Supervisor: Alan Fischer\nOperations Specialist: Lisa McDowell\nMarketing Manager: Erin Davis\nMarketing Assistant: Mack Patterson\nCover Designers: Kirsten Sims and Geoffrey Cassar\nCover Images: Stan Honda/Getty, Library of Congress, NASA, National Museum of Rome,\nPeter Norvig, Ian Parker, Shutterstock, Time Life/Getty\nInterior Designers: Stuart Russell and Peter Norvig\nCopy Editor: Mary Lou Nohr\nArt Editor: Greg Dulles\nMedia Editor: Daniel Sandin\nMedia Project Manager: Danielle Leone\nCopyright c⃝2010, 2003, 1995 by Pearson Education, Inc.,\nUpper Saddle River, New Jersey 07458.\nAll rights reserved. Manufactured in the United States of America. This publication is protected by\nCopyright and permissions should be obtained from the publisher prior to any prohibited reproduction,\nstorage in a retrieval system, or transmission in any form or by any means, electronic, mechanical,\nphotocopying, recording, or likewise. To obtain permission(s) to use materials from this work, please\nsubmit a written request to Pearson Higher Education, Permissions Department, 1 Lake Street, Upper\nSaddle River, NJ 07458.\nThe author and publisher of this book have used their best efforts in preparing this book. These\nefforts include the development, research, and testing of the theories and programs to determine their",
  "Saddle River, NJ 07458.\nThe author and publisher of this book have used their best efforts in preparing this book. These\nefforts include the development, research, and testing of the theories and programs to determine their\neffectiveness. The author and publisher make no warranty of any kind, expressed or implied, with\nregard to these programs or the documentation contained in this book. The author and publisher shall\nnot be liable in any event for incidental or consequential damages in connection with, or arising out\nof, the furnishing, performance, or use of these programs.\nLibrary of Congress Cataloging-in-Publication Data on File\n10 9 8 7 6 5 4 3 2 1\nISBN-13:\n978-0-13-604259-4\nISBN-10:\n0-13-604259-7 For Loy, Gordon, Lucy, George, and Isaac — S.J.R.\nFor Kris, Isabella, and Juliet — P.N. This page intentionally left blank Preface\nArtiﬁcial Intelligence (AI) is a big ﬁeld, and this is a big book. We have tried to explore the\nfull breadth of the ﬁeld, which encompasses logic, probability, and continuous mathematics;\nperception, reasoning, learning, and action; and everything from microelectronic devices to\nrobotic planetary explorers. The book is also big because we go into some depth.\nThe subtitle of this book is “A Modern Approach.” The intended meaning of this rather\nempty phrase is that we have tried to synthesize what is now known into a common frame-\nwork, rather than trying to explain each subﬁeld of AI in its own historical context. We\napologize to those whose subﬁelds are, as a result, less recognizable.\nNew to this edition\nThis edition captures the changes in AI that have taken place since the last edition in 2003.\nThere have been important applications of AI technology, such as the widespread deploy-\nment of practical speech recognition, machine translation, autonomous vehicles, and house-\nhold robotics. There have been algorithmic landmarks, such as the solution of the game of\ncheckers. And there has been a great deal of theoretical progress, particularly in areas such\nas probabilistic reasoning, machine learning, and computer vision. Most important from our\npoint of view is the continued evolution in how we think about the ﬁeld, and thus how we\norganize the book. The major changes are as follows:\n• We place more emphasis on partially observable and nondeterministic environments,\nespecially in the nonprobabilistic settings of search and planning. The concepts of",
  "organize the book. The major changes are as follows:\n• We place more emphasis on partially observable and nondeterministic environments,\nespecially in the nonprobabilistic settings of search and planning. The concepts of\nbelief state (a set of possible worlds) and state estimation (maintaining the belief state)\nare introduced in these settings; later in the book, we add probabilities.\n• In addition to discussing the types of environments and types of agents, we now cover\nin more depth the types of representations that an agent can use. We distinguish among\natomic representations (in which each state of the world is treated as a black box),\nfactored representations (in which a state is a set of attribute/value pairs), and structured\nrepresentations (in which the world consists of objects and relations between them).\n• Our coverage of planning goes into more depth on contingent planning in partially\nobservable environments and includes a new approach to hierarchical planning.\n• We have added new material on ﬁrst-order probabilistic models, including open-universe\nmodels for cases where there is uncertainty as to what objects exist.\n• We have completely rewritten the introductory machine-learning chapter, stressing a\nwider variety of more modern learning algorithms and placing them on a ﬁrmer theo-\nretical footing.\n• We have expanded coverage of Web search and information extraction, and of tech-\nniques for learning from very large data sets.\n• 20% of the citations in this edition are to works published after 2003.\n• We estimate that about 20% of the material is brand new. The remaining 80% reﬂects\nolder work but has been largely rewritten to present a more uniﬁed picture of the ﬁeld.\nvii viii\nPreface\nOverview of the book\nThe main unifying theme is the idea of an intelligent agent. We deﬁne AI as the study of\nagents that receive percepts from the environment and perform actions. Each such agent im-\nplements a function that maps percept sequences to actions, and we cover different ways to\nrepresent these functions, such as reactive agents, real-time planners, and decision-theoretic\nsystems. We explain the role of learning as extending the reach of the designer into unknown\nenvironments, and we show how that role constrains agent design, favoring explicit knowl-\nedge representation and reasoning. We treat robotics and vision not as independently deﬁned\nproblems, but as occurring in the service of achieving goals. We stress the importance of the",
  "edge representation and reasoning. We treat robotics and vision not as independently deﬁned\nproblems, but as occurring in the service of achieving goals. We stress the importance of the\ntask environment in determining the appropriate agent design.\nOur primary aim is to convey the ideas that have emerged over the past ﬁfty years of AI\nresearch and the past two millennia of related work. We have tried to avoid excessive formal-\nity in the presentation of these ideas while retaining precision. We have included pseudocode\nalgorithms to make the key ideas concrete; our pseudocode is described in Appendix B.\nThis book is primarily intended for use in an undergraduate course or course sequence.\nThe book has 27 chapters, each requiring about a week’s worth of lectures, so working\nthrough the whole book requires a two-semester sequence. A one-semester course can use\nselected chapters to suit the interests of the instructor and students. The book can also be\nused in a graduate-level course (perhaps with the addition of some of the primary sources\nsuggested in the bibliographical notes). Sample syllabi are available at the book’s Web site,\naima.cs.berkeley.edu. The only prerequisite is familiarity with basic concepts of\ncomputer science (algorithms, data structures, complexity) at a sophomore level. Freshman\ncalculus and linear algebra are useful for some of the topics; the required mathematical back-\nground is supplied in Appendix A.\nExercises are given at the end of each chapter. Exercises requiring signiﬁcant pro-\ngramming are marked with a keyboard icon. These exercises can best be solved by taking\nadvantage of the code repository at aima.cs.berkeley.edu. Some of them are large\nenough to be considered term projects. A number of exercises require some investigation of\nthe literature; these are marked with a book icon.\nThroughout the book, important points are marked with a pointing icon. We have in-\ncluded an extensive index of around 6,000 items to make it easy to ﬁnd things in the book.\nWherever a new term is ﬁrst deﬁned, it is also marked in the margin.\nNEW TERM\nAbout the Web site\naima.cs.berkeley.edu, the Web site for the book, contains\n• implementations of the algorithms in the book in several programming languages,\n• a list of over 1000 schools that have used the book, many with links to online course\nmaterials and syllabi,\n• an annotated list of over 800 links to sites around the Web with useful AI content,",
  "• a list of over 1000 schools that have used the book, many with links to online course\nmaterials and syllabi,\n• an annotated list of over 800 links to sites around the Web with useful AI content,\n• a chapter-by-chapter list of supplementary material and links,\n• instructions on how to join a discussion group for the book, Preface\nix\n• instructions on how to contact the authors with questions or comments,\n• instructions on how to report errors in the book, in the likely event that some exist, and\n• slides and other materials for instructors.\nAbout the cover\nThe cover depicts the ﬁnal position from the decisive game 6 of the 1997 match between\nchess champion Garry Kasparov and program DEEP BLUE. Kasparov, playing Black, was\nforced to resign, making this the ﬁrst time a computer had beaten a world champion in a\nchess match. Kasparov is shown at the top. To his left is the Asimo humanoid robot and\nto his right is Thomas Bayes (1702–1761), whose ideas about probability as a measure of\nbelief underlie much of modern AI technology. Below that we see a Mars Exploration Rover,\na robot that landed on Mars in 2004 and has been exploring the planet ever since. To the\nright is Alan Turing (1912–1954), whose fundamental work deﬁned the ﬁelds of computer\nscience in general and artiﬁcial intelligence in particular. At the bottom is Shakey (1966–\n1972), the ﬁrst robot to combine perception, world-modeling, planning, and learning. With\nShakey is project leader Charles Rosen (1917–2002). At the bottom right is Aristotle (384\nB.C.–322 B.C.), who pioneered the study of logic; his work was state of the art until the 19th\ncentury (copy of a bust by Lysippos). At the bottom left, lightly screened behind the authors’\nnames, is a planning algorithm by Aristotle from De Motu Animalium in the original Greek.\nBehind the title is a portion of the CPSC Bayesian network for medical diagnosis (Pradhan\net al., 1994). Behind the chess board is part of a Bayesian logic model for detecting nuclear\nexplosions from seismic signals.\nCredits: Stan Honda/Getty (Kasparaov), Library of Congress (Bayes), NASA (Mars\nrover), National Museum of Rome (Aristotle), Peter Norvig (book), Ian Parker (Berkeley\nskyline), Shutterstock (Asimo, Chess pieces), Time Life/Getty (Shakey, Turing).\nAcknowledgments\nThis book would not have been possible without the many contributors whose names did not\nmake it to the cover. Jitendra Malik and David Forsyth wrote Chapter 24 (computer vision)",
  "Acknowledgments\nThis book would not have been possible without the many contributors whose names did not\nmake it to the cover. Jitendra Malik and David Forsyth wrote Chapter 24 (computer vision)\nand Sebastian Thrun wrote Chapter 25 (robotics). Vibhu Mittal wrote part of Chapter 22\n(natural language). Nick Hay, Mehran Sahami, and Ernest Davis wrote some of the exercises.\nZoran Duric (George Mason), Thomas C. Henderson (Utah), Leon Reznik (RIT), Michael\nGourley (Central Oklahoma) and Ernest Davis (NYU) reviewed the manuscript and made\nhelpful suggestions. We thank Ernie Davis in particular for his tireless ability to read multiple\ndrafts and help improve the book. Nick Hay whipped the bibliography into shape and on\ndeadline stayed up to 5:30 AM writing code to make the book better. Jon Barron formatted\nand improved the diagrams in this edition, while Tim Huang, Mark Paskin, and Cynthia\nBruyns helped with diagrams and algorithms in previous editions. Ravi Mohan and Ciaran\nO’Reilly wrote and maintain the Java code examples on the Web site. John Canny wrote\nthe robotics chapter for the ﬁrst edition and Douglas Edwards researched the historical notes.\nTracy Dunkelberger, Allison Michael, Scott Disanno, and Jane Bonnell at Pearson tried their\nbest to keep us on schedule and made many helpful suggestions. Most helpful of all has x\nPreface\nbeen Julie Sussman, P.P.A., who read every chapter and provided extensive improvements. In\nprevious editions we had proofreaders who would tell us when we left out a comma and said\nwhich when we meant that; Julie told us when we left out a minus sign and said xi when we\nmeant xj. For every typo or confusing explanation that remains in the book, rest assured that\nJulie has ﬁxed at least ﬁve. She persevered even when a power failure forced her to work by\nlantern light rather than LCD glow.\nStuart would like to thank his parents for their support and encouragement and his\nwife, Loy Sheﬂott, for her endless patience and boundless wisdom. He hopes that Gordon,\nLucy, George, and Isaac will soon be reading this book after they have forgiven him for\nworking so long on it. RUGS (Russell’s Unusual Group of Students) have been unusually\nhelpful, as always.\nPeter would like to thank his parents (Torsten and Gerda) for getting him started,\nand his wife (Kris), children (Bella and Juliet), colleagues, and friends for encouraging and\ntolerating him through the long hours of writing and longer hours of rewriting.",
  "and his wife (Kris), children (Bella and Juliet), colleagues, and friends for encouraging and\ntolerating him through the long hours of writing and longer hours of rewriting.\nWe both thank the librarians at Berkeley, Stanford, and NASA and the developers of\nCiteSeer, Wikipedia, and Google, who have revolutionized the way we do research. We can’t\nacknowledge all the people who have used the book and made suggestions, but we would like\nto note the especially helpful comments of Gagan Aggarwal, Eyal Amir, Ion Androutsopou-\nlos, Krzysztof Apt, Warren Haley Armstrong, Ellery Aziel, Jeff Van Baalen, Darius Bacon,\nBrian Baker, Shumeet Baluja, Don Barker, Tony Barrett, James Newton Bass, Don Beal,\nHoward Beck, Wolfgang Bibel, John Binder, Larry Bookman, David R. Boxall, Ronen Braf-\nman, John Bresina, Gerhard Brewka, Selmer Bringsjord, Carla Brodley, Chris Brown, Emma\nBrunskill, Wilhelm Burger, Lauren Burka, Carlos Bustamante, Joao Cachopo, Murray Camp-\nbell, Norman Carver, Emmanuel Castro, Anil Chakravarthy, Dan Chisarick, Berthe Choueiry,\nRoberto Cipolla, David Cohen, James Coleman, Julie Ann Comparini, Corinna Cortes, Gary\nCottrell, Ernest Davis, Tom Dean, Rina Dechter, Tom Dietterich, Peter Drake, Chuck Dyer,\nDoug Edwards, Robert Egginton, Asma’a El-Budrawy, Barbara Engelhardt, Kutluhan Erol,\nOren Etzioni, Hana Filip, Douglas Fisher, Jeffrey Forbes, Ken Ford, Eric Fosler-Lussier,\nJohn Fosler, Jeremy Frank, Alex Franz, Bob Futrelle, Marek Galecki, Stefan Gerberding,\nStuart Gill, Sabine Glesner, Seth Golub, Gosta Grahne, Russ Greiner, Eric Grimson, Bar-\nbara Grosz, Larry Hall, Steve Hanks, Othar Hansson, Ernst Heinz, Jim Hendler, Christoph\nHerrmann, Paul Hilﬁnger, Robert Holte, Vasant Honavar, Tim Huang, Seth Hutchinson, Joost\nJacob, Mark Jelasity, Magnus Johansson, Istvan Jonyer, Dan Jurafsky, Leslie Kaelbling, Keiji\nKanazawa, Surekha Kasibhatla, Simon Kasif, Henry Kautz, Gernot Kerschbaumer, Max\nKhesin, Richard Kirby, Dan Klein, Kevin Knight, Roland Koenig, Sven Koenig, Daphne\nKoller, Rich Korf, Benjamin Kuipers, James Kurien, John Lafferty, John Laird, Gus Lars-\nson, John Lazzaro, Jon LeBlanc, Jason Leatherman, Frank Lee, Jon Lehto, Edward Lim,\nPhil Long, Pierre Louveaux, Don Loveland, Sridhar Mahadevan, Tony Mancill, Jim Martin,\nAndy Mayer, John McCarthy, David McGrane, Jay Mendelsohn, Risto Miikkulanien, Brian\nMilch, Steve Minton, Vibhu Mittal, Mehryar Mohri, Leora Morgenstern, Stephen Muggleton,",
  "Andy Mayer, John McCarthy, David McGrane, Jay Mendelsohn, Risto Miikkulanien, Brian\nMilch, Steve Minton, Vibhu Mittal, Mehryar Mohri, Leora Morgenstern, Stephen Muggleton,\nKevin Murphy, Ron Musick, Sung Myaeng, Eric Nadeau, Lee Naish, Pandu Nayak, Bernhard\nNebel, Stuart Nelson, XuanLong Nguyen, Nils Nilsson, Illah Nourbakhsh, Ali Nouri, Arthur\nNunes-Harwitt, Steve Omohundro, David Page, David Palmer, David Parkes, Ron Parr, Mark Preface\nxi\nPaskin, Tony Passera, Amit Patel, Michael Pazzani, Fernando Pereira, Joseph Perla, Wim Pi-\njls, Ira Pohl, Martha Pollack, David Poole, Bruce Porter, Malcolm Pradhan, Bill Pringle, Lor-\nraine Prior, Greg Provan, William Rapaport, Deepak Ravichandran, Ioannis Refanidis, Philip\nResnik, Francesca Rossi, Sam Roweis, Richard Russell, Jonathan Schaeffer, Richard Scherl,\nHinrich Schuetze, Lars Schuster, Bart Selman, Soheil Shams, Stuart Shapiro, Jude Shav-\nlik, Yoram Singer, Satinder Singh, Daniel Sleator, David Smith, Bryan So, Robert Sproull,\nLynn Stein, Larry Stephens, Andreas Stolcke, Paul Stradling, Devika Subramanian, Marek\nSuchenek, Rich Sutton, Jonathan Tash, Austin Tate, Bas Terwijn, Olivier Teytaud, Michael\nThielscher, William Thompson, Sebastian Thrun, Eric Tiedemann, Mark Torrance, Randall\nUpham, Paul Utgoff, Peter van Beek, Hal Varian, Paulina Varshavskaya, Sunil Vemuri, Vandi\nVerma, Ubbo Visser, Jim Waldo, Toby Walsh, Bonnie Webber, Dan Weld, Michael Wellman,\nKamin Whitehouse, Michael Dean White, Brian Williams, David Wolfe, Jason Wolfe, Bill\nWoods, Alden Wright, Jay Yagnik, Mark Yasuda, Richard Yen, Eliezer Yudkowsky, Weixiong\nZhang, Ming Zhao, Shlomo Zilberstein, and our esteemed colleague Anonymous Reviewer. About the Authors\nStuart Russell was born in 1962 in Portsmouth, England. He received his B.A. with ﬁrst-\nclass honours in physics from Oxford University in 1982, and his Ph.D. in computer science\nfrom Stanford in 1986. He then joined the faculty of the University of California at Berkeley,\nwhere he is a professor of computer science, director of the Center for Intelligent Systems,\nand holder of the Smith–Zadeh Chair in Engineering. In 1990, he received the Presidential\nYoung Investigator Award of the National Science Foundation, and in 1995 he was cowinner\nof the Computers and Thought Award. He was a 1996 Miller Professor of the University of\nCalifornia and was appointed to a Chancellor’s Professorship in 2000. In 1998, he gave the",
  "of the Computers and Thought Award. He was a 1996 Miller Professor of the University of\nCalifornia and was appointed to a Chancellor’s Professorship in 2000. In 1998, he gave the\nForsythe Memorial Lectures at Stanford University. He is a Fellow and former Executive\nCouncil member of the American Association for Artiﬁcial Intelligence. He has published\nover 100 papers on a wide range of topics in artiﬁcial intelligence. His other books include\nThe Use of Knowledge in Analogy and Induction and (with Eric Wefald) Do the Right Thing:\nStudies in Limited Rationality.\nPeter Norvig is currently Director of Research at Google, Inc., and was the director respon-\nsible for the core Web search algorithms from 2002 to 2005. He is a Fellow of the American\nAssociation for Artiﬁcial Intelligence and the Association for Computing Machinery. Previ-\nously, he was head of the Computational Sciences Division at NASA Ames Research Center,\nwhere he oversaw NASA’s research and development in artiﬁcial intelligence and robotics,\nand chief scientist at Junglee, where he helped develop one of the ﬁrst Internet information\nextraction services. He received a B.S. in applied mathematics from Brown University and\na Ph.D. in computer science from the University of California at Berkeley. He received the\nDistinguished Alumni and Engineering Innovation awards from Berkeley and the Exceptional\nAchievement Medal from NASA. He has been a professor at the University of Southern Cal-\nifornia and a research faculty member at Berkeley. His other books are Paradigms of AI\nProgramming: Case Studies in Common Lisp and Verbmobil: A Translation System for Face-\nto-Face Dialog and Intelligent Help Systems for UNIX.\nxii Contents\nI\nArtiﬁcial Intelligence\n1\nIntroduction\n1\n1.1\nWhat Is AI?\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nThe Foundations of Artiﬁcial Intelligence . . . . . . . . . . . . . . . . . .\n5\n1.3\nThe History of Artiﬁcial Intelligence . . . . . . . . . . . . . . . . . . . .\n16\n1.4\nThe State of the Art . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n1.5\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n29\n2\nIntelligent Agents\n34\n2.1\nAgents and Environments . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n2.2\nGood Behavior: The Concept of Rationality\n. . . . . . . . . . . . . . . .\n36\n2.3\nThe Nature of Environments . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n2.4",
  "Agents and Environments . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n2.2\nGood Behavior: The Concept of Rationality\n. . . . . . . . . . . . . . . .\n36\n2.3\nThe Nature of Environments . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n2.4\nThe Structure of Agents . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n2.5\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n59\nII\nProblem-solving\n3\nSolving Problems by Searching\n64\n3.1\nProblem-Solving Agents . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n3.2\nExample Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\n3.3\nSearching for Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n3.4\nUninformed Search Strategies . . . . . . . . . . . . . . . . . . . . . . . .\n81\n3.5\nInformed (Heuristic) Search Strategies . . . . . . . . . . . . . . . . . . .\n92\n3.6\nHeuristic Functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n102\n3.7\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n108\n4\nBeyond Classical Search\n120\n4.1\nLocal Search Algorithms and Optimization Problems\n. . . . . . . . . . .\n120\n4.2\nLocal Search in Continuous Spaces . . . . . . . . . . . . . . . . . . . . .\n129\n4.3\nSearching with Nondeterministic Actions . . . . . . . . . . . . . . . . . .\n133\n4.4\nSearching with Partial Observations . . . . . . . . . . . . . . . . . . . . .\n138\n4.5\nOnline Search Agents and Unknown Environments\n. . . . . . . . . . . .\n147\n4.6\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n153\n5\nAdversarial Search\n161\n5.1\nGames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n161\n5.2\nOptimal Decisions in Games\n. . . . . . . . . . . . . . . . . . . . . . . .\n163\n5.3\nAlpha–Beta Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n167\n5.4\nImperfect Real-Time Decisions . . . . . . . . . . . . . . . . . . . . . . .\n171\n5.5\nStochastic Games\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n177\nxiii xiv\nContents\n5.6\nPartially Observable Games . . . . . . . . . . . . . . . . . . . . . . . . .\n180\n5.7\nState-of-the-Art Game Programs\n. . . . . . . . . . . . . . . . . . . . . .\n185\n5.8\nAlternative Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . .\n187\n5.9\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n189\n6\nConstraint Satisfaction Problems\n202\n6.1\nDeﬁning Constraint Satisfaction Problems\n. . . . . . . . . . . . . . . . .",
  "187\n5.9\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n189\n6\nConstraint Satisfaction Problems\n202\n6.1\nDeﬁning Constraint Satisfaction Problems\n. . . . . . . . . . . . . . . . .\n202\n6.2\nConstraint Propagation: Inference in CSPs . . . . . . . . . . . . . . . . .\n208\n6.3\nBacktracking Search for CSPs . . . . . . . . . . . . . . . . . . . . . . . .\n214\n6.4\nLocal Search for CSPs . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n220\n6.5\nThe Structure of Problems . . . . . . . . . . . . . . . . . . . . . . . . . .\n222\n6.6\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n227\nIII\nKnowledge, reasoning, and planning\n7\nLogical Agents\n234\n7.1\nKnowledge-Based Agents . . . . . . . . . . . . . . . . . . . . . . . . . .\n235\n7.2\nThe Wumpus World . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n236\n7.3\nLogic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n240\n7.4\nPropositional Logic: A Very Simple Logic . . . . . . . . . . . . . . . . .\n243\n7.5\nPropositional Theorem Proving . . . . . . . . . . . . . . . . . . . . . . .\n249\n7.6\nEffective Propositional Model Checking\n. . . . . . . . . . . . . . . . . .\n259\n7.7\nAgents Based on Propositional Logic . . . . . . . . . . . . . . . . . . . .\n265\n7.8\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n274\n8\nFirst-Order Logic\n285\n8.1\nRepresentation Revisited\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n285\n8.2\nSyntax and Semantics of First-Order Logic . . . . . . . . . . . . . . . . .\n290\n8.3\nUsing First-Order Logic . . . . . . . . . . . . . . . . . . . . . . . . . . .\n300\n8.4\nKnowledge Engineering in First-Order Logic . . . . . . . . . . . . . . . .\n307\n8.5\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n313\n9\nInference in First-Order Logic\n322\n9.1\nPropositional vs. First-Order Inference . . . . . . . . . . . . . . . . . . .\n322\n9.2\nUniﬁcation and Lifting\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n325\n9.3\nForward Chaining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n330\n9.4\nBackward Chaining . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n337\n9.5\nResolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n345\n9.6\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n357\n10 Classical Planning\n366\n10.1\nDeﬁnition of Classical Planning . . . . . . . . . . . . . . . . . . . . . . .\n366\n10.2",
  "345\n9.6\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n357\n10 Classical Planning\n366\n10.1\nDeﬁnition of Classical Planning . . . . . . . . . . . . . . . . . . . . . . .\n366\n10.2\nAlgorithms for Planning as State-Space Search . . . . . . . . . . . . . . .\n373\n10.3\nPlanning Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n379 Contents\nxv\n10.4\nOther Classical Planning Approaches . . . . . . . . . . . . . . . . . . . .\n387\n10.5\nAnalysis of Planning Approaches . . . . . . . . . . . . . . . . . . . . . .\n392\n10.6\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n393\n11 Planning and Acting in the Real World\n401\n11.1\nTime, Schedules, and Resources . . . . . . . . . . . . . . . . . . . . . . .\n401\n11.2\nHierarchical Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n406\n11.3\nPlanning and Acting in Nondeterministic Domains . . . . . . . . . . . . .\n415\n11.4\nMultiagent Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n425\n11.5\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n430\n12 Knowledge Representation\n437\n12.1\nOntological Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . .\n437\n12.2\nCategories and Objects\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n440\n12.3\nEvents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n446\n12.4\nMental Events and Mental Objects\n. . . . . . . . . . . . . . . . . . . . .\n450\n12.5\nReasoning Systems for Categories\n. . . . . . . . . . . . . . . . . . . . .\n453\n12.6\nReasoning with Default Information\n. . . . . . . . . . . . . . . . . . . .\n458\n12.7\nThe Internet Shopping World . . . . . . . . . . . . . . . . . . . . . . . .\n462\n12.8\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n467\nIV\nUncertain knowledge and reasoning\n13 Quantifying Uncertainty\n480\n13.1\nActing under Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . .\n480\n13.2\nBasic Probability Notation . . . . . . . . . . . . . . . . . . . . . . . . . .\n483\n13.3\nInference Using Full Joint Distributions . . . . . . . . . . . . . . . . . . .\n490\n13.4\nIndependence\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n494\n13.5\nBayes’ Rule and Its Use . . . . . . . . . . . . . . . . . . . . . . . . . . .\n495\n13.6\nThe Wumpus World Revisited . . . . . . . . . . . . . . . . . . . . . . . .\n499\n13.7\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n503",
  "Bayes’ Rule and Its Use . . . . . . . . . . . . . . . . . . . . . . . . . . .\n495\n13.6\nThe Wumpus World Revisited . . . . . . . . . . . . . . . . . . . . . . . .\n499\n13.7\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n503\n14 Probabilistic Reasoning\n510\n14.1\nRepresenting Knowledge in an Uncertain Domain . . . . . . . . . . . . .\n510\n14.2\nThe Semantics of Bayesian Networks . . . . . . . . . . . . . . . . . . . .\n513\n14.3\nEfﬁcient Representation of Conditional Distributions . . . . . . . . . . . .\n518\n14.4\nExact Inference in Bayesian Networks\n. . . . . . . . . . . . . . . . . . .\n522\n14.5\nApproximate Inference in Bayesian Networks\n. . . . . . . . . . . . . . .\n530\n14.6\nRelational and First-Order Probability Models . . . . . . . . . . . . . . .\n539\n14.7\nOther Approaches to Uncertain Reasoning . . . . . . . . . . . . . . . . .\n546\n14.8\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n551\n15 Probabilistic Reasoning over Time\n566\n15.1\nTime and Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n566 xvi\nContents\n15.2\nInference in Temporal Models . . . . . . . . . . . . . . . . . . . . . . . .\n570\n15.3\nHidden Markov Models . . . . . . . . . . . . . . . . . . . . . . . . . . .\n578\n15.4\nKalman Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n584\n15.5\nDynamic Bayesian Networks . . . . . . . . . . . . . . . . . . . . . . . .\n590\n15.6\nKeeping Track of Many Objects . . . . . . . . . . . . . . . . . . . . . . .\n599\n15.7\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n603\n16 Making Simple Decisions\n610\n16.1\nCombining Beliefs and Desires under Uncertainty . . . . . . . . . . . . .\n610\n16.2\nThe Basis of Utility Theory . . . . . . . . . . . . . . . . . . . . . . . . .\n611\n16.3\nUtility Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n615\n16.4\nMultiattribute Utility Functions . . . . . . . . . . . . . . . . . . . . . . .\n622\n16.5\nDecision Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n626\n16.6\nThe Value of Information . . . . . . . . . . . . . . . . . . . . . . . . . .\n628\n16.7\nDecision-Theoretic Expert Systems . . . . . . . . . . . . . . . . . . . . .\n633\n16.8\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n636\n17 Making Complex Decisions\n645\n17.1\nSequential Decision Problems . . . . . . . . . . . . . . . . . . . . . . . .\n645\n17.2",
  "633\n16.8\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n636\n17 Making Complex Decisions\n645\n17.1\nSequential Decision Problems . . . . . . . . . . . . . . . . . . . . . . . .\n645\n17.2\nValue Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n652\n17.3\nPolicy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n656\n17.4\nPartially Observable MDPs . . . . . . . . . . . . . . . . . . . . . . . . .\n658\n17.5\nDecisions with Multiple Agents: Game Theory . . . . . . . . . . . . . . .\n666\n17.6\nMechanism Design\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n679\n17.7\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n684\nV\nLearning\n18 Learning from Examples\n693\n18.1\nForms of Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n693\n18.2\nSupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n695\n18.3\nLearning Decision Trees . . . . . . . . . . . . . . . . . . . . . . . . . . .\n697\n18.4\nEvaluating and Choosing the Best Hypothesis\n. . . . . . . . . . . . . . .\n708\n18.5\nThe Theory of Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n713\n18.6\nRegression and Classiﬁcation with Linear Models . . . . . . . . . . . . .\n717\n18.7\nArtiﬁcial Neural Networks\n. . . . . . . . . . . . . . . . . . . . . . . . .\n727\n18.8\nNonparametric Models\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n737\n18.9\nSupport Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . . .\n744\n18.10 Ensemble Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n748\n18.11 Practical Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . .\n753\n18.12 Summary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n757\n19 Knowledge in Learning\n768\n19.1\nA Logical Formulation of Learning . . . . . . . . . . . . . . . . . . . . .\n768 Contents\nxvii\n19.2\nKnowledge in Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .\n777\n19.3\nExplanation-Based Learning\n. . . . . . . . . . . . . . . . . . . . . . . .\n780\n19.4\nLearning Using Relevance Information . . . . . . . . . . . . . . . . . . .\n784\n19.5\nInductive Logic Programming . . . . . . . . . . . . . . . . . . . . . . . .\n788\n19.6\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n797\n20 Learning Probabilistic Models\n802\n20.1\nStatistical Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n802\n20.2",
  "788\n19.6\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n797\n20 Learning Probabilistic Models\n802\n20.1\nStatistical Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n802\n20.2\nLearning with Complete Data . . . . . . . . . . . . . . . . . . . . . . . .\n806\n20.3\nLearning with Hidden Variables: The EM Algorithm . . . . . . . . . . . .\n816\n20.4\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n825\n21 Reinforcement Learning\n830\n21.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n830\n21.2\nPassive Reinforcement Learning\n. . . . . . . . . . . . . . . . . . . . . .\n832\n21.3\nActive Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . .\n839\n21.4\nGeneralization in Reinforcement Learning . . . . . . . . . . . . . . . . .\n845\n21.5\nPolicy Search\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n848\n21.6\nApplications of Reinforcement Learning . . . . . . . . . . . . . . . . . .\n850\n21.7\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n853\nVI\nCommunicating, perceiving, and acting\n22 Natural Language Processing\n860\n22.1\nLanguage Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n860\n22.2\nText Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n865\n22.3\nInformation Retrieval\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n867\n22.4\nInformation Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n873\n22.5\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n882\n23 Natural Language for Communication\n888\n23.1\nPhrase Structure Grammars . . . . . . . . . . . . . . . . . . . . . . . . .\n888\n23.2\nSyntactic Analysis (Parsing) . . . . . . . . . . . . . . . . . . . . . . . . .\n892\n23.3\nAugmented Grammars and Semantic Interpretation\n. . . . . . . . . . . .\n897\n23.4\nMachine Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n907\n23.5\nSpeech Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n912\n23.6\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n918\n24 Perception\n928\n24.1\nImage Formation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n929\n24.2\nEarly Image-Processing Operations . . . . . . . . . . . . . . . . . . . . .\n935\n24.3\nObject Recognition by Appearance . . . . . . . . . . . . . . . . . . . . .\n942\n24.4",
  "929\n24.2\nEarly Image-Processing Operations . . . . . . . . . . . . . . . . . . . . .\n935\n24.3\nObject Recognition by Appearance . . . . . . . . . . . . . . . . . . . . .\n942\n24.4\nReconstructing the 3D World . . . . . . . . . . . . . . . . . . . . . . . .\n947\n24.5\nObject Recognition from Structural Information . . . . . . . . . . . . . .\n957 xviii\nContents\n24.6\nUsing Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n961\n24.7\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . .\n965\n25 Robotics\n971\n25.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n971\n25.2\nRobot Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n973\n25.3\nRobotic Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n978\n25.4\nPlanning to Move\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n986\n25.5\nPlanning Uncertain Movements . . . . . . . . . . . . . . . . . . . . . . .\n993\n25.6\nMoving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n997\n25.7\nRobotic Software Architectures . . . . . . . . . . . . . . . . . . . . . . . 1003\n25.8\nApplication Domains\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 1006\n25.9\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . . 1010\nVII\nConclusions\n26 Philosophical Foundations\n1020\n26.1\nWeak AI: Can Machines Act Intelligently? . . . . . . . . . . . . . . . . . 1020\n26.2\nStrong AI: Can Machines Really Think? . . . . . . . . . . . . . . . . . . 1026\n26.3\nThe Ethics and Risks of Developing Artiﬁcial Intelligence . . . . . . . . . 1034\n26.4\nSummary, Bibliographical and Historical Notes, Exercises . . . . . . . . . 1040\n27 AI: The Present and Future\n1044\n27.1\nAgent Components\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1044\n27.2\nAgent Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1047\n27.3\nAre We Going in the Right Direction?\n. . . . . . . . . . . . . . . . . . . 1049\n27.4\nWhat If AI Does Succeed?\n. . . . . . . . . . . . . . . . . . . . . . . . . 1051\nA Mathematical background\n1053\nA.1\nComplexity Analysis and O() Notation . . . . . . . . . . . . . . . . . . . 1053\nA.2\nVectors, Matrices, and Linear Algebra\n. . . . . . . . . . . . . . . . . . . 1055\nA.3\nProbability Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 1057\nB\nNotes on Languages and Algorithms\n1060\nB.1\nDeﬁning Languages with Backus–Naur Form (BNF) . . . . . . . . . . . . 1060",
  "A.3\nProbability Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 1057\nB\nNotes on Languages and Algorithms\n1060\nB.1\nDeﬁning Languages with Backus–Naur Form (BNF) . . . . . . . . . . . . 1060\nB.2\nDescribing Algorithms with Pseudocode . . . . . . . . . . . . . . . . . . 1061\nB.3\nOnline Help . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1062\nBibliography\n1063\nIndex\n1095 1\nINTRODUCTION\nIn which we try to explain why we consider artiﬁcial intelligence to be a subject\nmost worthy of study, and in which we try to decide what exactly it is, this being a\ngood thing to decide before embarking.\nWe call ourselves Homo sapiens—man the wise—because our intelligence is so important\nINTELLIGENCE\nto us. For thousands of years, we have tried to understand how we think; that is, how a mere\nhandful of matter can perceive, understand, predict, and manipulate a world far larger and\nmore complicated than itself. The ﬁeld of artiﬁcial intelligence, or AI, goes further still: it\nARTIFICIAL\nINTELLIGENCE\nattempts not just to understand but also to build intelligent entities.\nAI is one of the newest ﬁelds in science and engineering. Work started in earnest soon\nafter World War II, and the name itself was coined in 1956. Along with molecular biology,\nAI is regularly cited as the “ﬁeld I would most like to be in” by scientists in other disciplines.\nA student in physics might reasonably feel that all the good ideas have already been taken by\nGalileo, Newton, Einstein, and the rest. AI, on the other hand, still has openings for several\nfull-time Einsteins and Edisons.\nAI currently encompasses a huge variety of subﬁelds, ranging from the general (learning\nand perception) to the speciﬁc, such as playing chess, proving mathematical theorems, writing\npoetry, driving a car on a crowded street, and diagnosing diseases. AI is relevant to any\nintellectual task; it is truly a universal ﬁeld.\n1.1\nWHAT IS AI?\nWe have claimed that AI is exciting, but we have not said what it is. In Figure 1.1 we see\neight deﬁnitions of AI, laid out along two dimensions. The deﬁnitions on top are concerned\nwith thought processes and reasoning, whereas the ones on the bottom address behavior. The\ndeﬁnitions on the left measure success in terms of ﬁdelity to human performance, whereas\nthe ones on the right measure against an ideal performance measure, called rationality. A\nRATIONALITY\nsystem is rational if it does the “right thing,” given what it knows.",
  "the ones on the right measure against an ideal performance measure, called rationality. A\nRATIONALITY\nsystem is rational if it does the “right thing,” given what it knows.\nHistorically, all four approaches to AI have been followed, each by different people\nwith different methods. A human-centered approach must be in part an empirical science, in-\n1 2\nChapter\n1.\nIntroduction\nThinking Humanly\nThinking Rationally\n“The exciting new effort to make comput-\ners think . . . machines with minds, in the\nfull and literal sense.” (Haugeland, 1985)\n“The study of mental faculties through the\nuse of computational models.”\n(Charniak and McDermott, 1985)\n“[The automation of] activities that we\nassociate with human thinking, activities\nsuch as decision-making, problem solv-\ning, learning . . .” (Bellman, 1978)\n“The study of the computations that make\nit possible to perceive, reason, and act.”\n(Winston, 1992)\nActing Humanly\nActing Rationally\n“The art of creating machines that per-\nform functions that require intelligence\nwhen performed by people.” (Kurzweil,\n1990)\n“Computational Intelligence is the study\nof the design of intelligent agents.” (Poole\net al., 1998)\n“The study of how to make computers do\nthings at which, at the moment, people are\nbetter.” (Rich and Knight, 1991)\n“AI . . . is concerned with intelligent be-\nhavior in artifacts.” (Nilsson, 1998)\nFigure 1.1\nSome deﬁnitions of artiﬁcial intelligence, organized into four categories.\nvolving observations and hypotheses about human behavior. A rationalist1 approach involves\na combination of mathematics and engineering. The various group have both disparaged and\nhelped each other. Let us look at the four approaches in more detail.\n1.1.1\nActing humanly: The Turing Test approach\nThe Turing Test, proposed by Alan Turing (1950), was designed to provide a satisfactory\nTURING TEST\noperational deﬁnition of intelligence. A computer passes the test if a human interrogator, after\nposing some written questions, cannot tell whether the written responses come from a person\nor from a computer. Chapter 26 discusses the details of the test and whether a computer would\nreally be intelligent if it passed. For now, we note that programming a computer to pass a\nrigorously applied test provides plenty to work on. The computer would need to possess the\nfollowing capabilities:\n• natural language processing to enable it to communicate successfully in English;\nNATURAL LANGUAGE\nPROCESSING",
  "rigorously applied test provides plenty to work on. The computer would need to possess the\nfollowing capabilities:\n• natural language processing to enable it to communicate successfully in English;\nNATURAL LANGUAGE\nPROCESSING\n• knowledge representation to store what it knows or hears;\nKNOWLEDGE\nREPRESENTATION\n• automated reasoning to use the stored information to answer questions and to draw\nAUTOMATED\nREASONING\nnew conclusions;\n• machine learning to adapt to new circumstances and to detect and extrapolate patterns.\nMACHINE LEARNING\n1 By distinguishing between human and rational behavior, we are not suggesting that humans are necessarily\n“irrational” in the sense of “emotionally unstable” or “insane.” One merely need note that we are not perfect:\nnot all chess players are grandmasters; and, unfortunately, not everyone gets an A on the exam. Some systematic\nerrors in human reasoning are cataloged by Kahneman et al. (1982). Section 1.1.\nWhat Is AI?\n3\nTuring’s test deliberately avoided direct physical interaction between the interrogator and the\ncomputer, because physical simulation of a person is unnecessary for intelligence. However,\nthe so-called total Turing Test includes a video signal so that the interrogator can test the\nTOTAL TURING TEST\nsubject’s perceptual abilities, as well as the opportunity for the interrogator to pass physical\nobjects “through the hatch.” To pass the total Turing Test, the computer will need\n• computer vision to perceive objects, and\nCOMPUTER VISION\n• robotics to manipulate objects and move about.\nROBOTICS\nThese six disciplines compose most of AI, and Turing deserves credit for designing a test\nthat remains relevant 60 years later. Yet AI researchers have devoted little effort to passing\nthe Turing Test, believing that it is more important to study the underlying principles of in-\ntelligence than to duplicate an exemplar. The quest for “artiﬁcial ﬂight” succeeded when the\nWright brothers and others stopped imitating birds and started using wind tunnels and learn-\ning about aerodynamics. Aeronautical engineering texts do not deﬁne the goal of their ﬁeld\nas making “machines that ﬂy so exactly like pigeons that they can fool even other pigeons.”\n1.1.2\nThinking humanly: The cognitive modeling approach\nIf we are going to say that a given program thinks like a human, we must have some way of\ndetermining how humans think. We need to get inside the actual workings of human minds.",
  "1.1.2\nThinking humanly: The cognitive modeling approach\nIf we are going to say that a given program thinks like a human, we must have some way of\ndetermining how humans think. We need to get inside the actual workings of human minds.\nThere are three ways to do this: through introspection—trying to catch our own thoughts as\nthey go by; through psychological experiments—observing a person in action; and through\nbrain imaging—observing the brain in action. Once we have a sufﬁciently precise theory of\nthe mind, it becomes possible to express the theory as a computer program. If the program’s\ninput–output behavior matches corresponding human behavior, that is evidence that some of\nthe program’s mechanisms could also be operating in humans. For example, Allen Newell\nand Herbert Simon, who developed GPS, the “General Problem Solver” (Newell and Simon,\n1961), were not content merely to have their program solve problems correctly. They were\nmore concerned with comparing the trace of its reasoning steps to traces of human subjects\nsolving the same problems. The interdisciplinary ﬁeld of cognitive science brings together\nCOGNITIVE SCIENCE\ncomputer models from AI and experimental techniques from psychology to construct precise\nand testable theories of the human mind.\nCognitive science is a fascinating ﬁeld in itself, worthy of several textbooks and at least\none encyclopedia (Wilson and Keil, 1999). We will occasionally comment on similarities or\ndifferences between AI techniques and human cognition. Real cognitive science, however, is\nnecessarily based on experimental investigation of actual humans or animals. We will leave\nthat for other books, as we assume the reader has only a computer for experimentation.\nIn the early days of AI there was often confusion between the approaches: an author\nwould argue that an algorithm performs well on a task and that it is therefore a good model\nof human performance, or vice versa. Modern authors separate the two kinds of claims;\nthis distinction has allowed both AI and cognitive science to develop more rapidly. The two\nﬁelds continue to fertilize each other, most notably in computer vision, which incorporates\nneurophysiological evidence into computational models. 4\nChapter\n1.\nIntroduction\n1.1.3\nThinking rationally: The “laws of thought” approach\nThe Greek philosopher Aristotle was one of the ﬁrst to attempt to codify “right thinking,” that",
  "neurophysiological evidence into computational models. 4\nChapter\n1.\nIntroduction\n1.1.3\nThinking rationally: The “laws of thought” approach\nThe Greek philosopher Aristotle was one of the ﬁrst to attempt to codify “right thinking,” that\nis, irrefutable reasoning processes. His syllogisms provided patterns for argument structures\nSYLLOGISM\nthat always yielded correct conclusions when given correct premises—for example, “Socrates\nis a man; all men are mortal; therefore, Socrates is mortal.” These laws of thought were\nsupposed to govern the operation of the mind; their study initiated the ﬁeld called logic.\nLOGIC\nLogicians in the 19th century developed a precise notation for statements about all kinds\nof objects in the world and the relations among them. (Contrast this with ordinary arithmetic\nnotation, which provides only for statements about numbers.) By 1965, programs existed\nthat could, in principle, solve any solvable problem described in logical notation. (Although\nif no solution exists, the program might loop forever.) The so-called logicist tradition within\nLOGICIST\nartiﬁcial intelligence hopes to build on such programs to create intelligent systems.\nThere are two main obstacles to this approach. First, it is not easy to take informal\nknowledge and state it in the formal terms required by logical notation, particularly when\nthe knowledge is less than 100% certain. Second, there is a big difference between solving\na problem “in principle” and solving it in practice. Even problems with just a few hundred\nfacts can exhaust the computational resources of any computer unless it has some guidance\nas to which reasoning steps to try ﬁrst. Although both of these obstacles apply to any attempt\nto build computational reasoning systems, they appeared ﬁrst in the logicist tradition.\n1.1.4\nActing rationally: The rational agent approach\nAn agent is just something that acts (agent comes from the Latin agere, to do). Of course,\nAGENT\nall computer programs do something, but computer agents are expected to do more: operate\nautonomously, perceive their environment, persist over a prolonged time period, adapt to\nchange, and create and pursue goals. A rational agent is one that acts so as to achieve the\nRATIONAL AGENT\nbest outcome or, when there is uncertainty, the best expected outcome.\nIn the “laws of thought” approach to AI, the emphasis was on correct inferences. Mak-\ning correct inferences is sometimes part of being a rational agent, because one way to act",
  "best outcome or, when there is uncertainty, the best expected outcome.\nIn the “laws of thought” approach to AI, the emphasis was on correct inferences. Mak-\ning correct inferences is sometimes part of being a rational agent, because one way to act\nrationally is to reason logically to the conclusion that a given action will achieve one’s goals\nand then to act on that conclusion. On the other hand, correct inference is not all of ration-\nality; in some situations, there is no provably correct thing to do, but something must still be\ndone. There are also ways of acting rationally that cannot be said to involve inference. For\nexample, recoiling from a hot stove is a reﬂex action that is usually more successful than a\nslower action taken after careful deliberation.\nAll the skills needed for the Turing Test also allow an agent to act rationally. Knowledge\nrepresentation and reasoning enable agents to reach good decisions. We need to be able to\ngenerate comprehensible sentences in natural language to get by in a complex society. We\nneed learning not only for erudition, but also because it improves our ability to generate\neffective behavior.\nThe rational-agent approach has two advantages over the other approaches. First, it\nis more general than the “laws of thought” approach because correct inference is just one\nof several possible mechanisms for achieving rationality. Second, it is more amenable to Section 1.2.\nThe Foundations of Artiﬁcial Intelligence\n5\nscientiﬁc development than are approaches based on human behavior or human thought. The\nstandard of rationality is mathematically well deﬁned and completely general, and can be\n“unpacked” to generate agent designs that provably achieve it. Human behavior, on the other\nhand, is well adapted for one speciﬁc environment and is deﬁned by, well, the sum total\nof all the things that humans do. This book therefore concentrates on general principles\nof rational agents and on components for constructing them. We will see that despite the\napparent simplicity with which the problem can be stated, an enormous variety of issues\ncome up when we try to solve it. Chapter 2 outlines some of these issues in more detail.\nOne important point to keep in mind: We will see before too long that achieving perfect\nrationality—always doing the right thing—is not feasible in complicated environments. The\ncomputational demands are just too high. For most of the book, however, we will adopt the",
  "rationality—always doing the right thing—is not feasible in complicated environments. The\ncomputational demands are just too high. For most of the book, however, we will adopt the\nworking hypothesis that perfect rationality is a good starting point for analysis. It simpliﬁes\nthe problem and provides the appropriate setting for most of the foundational material in\nthe ﬁeld. Chapters 5 and 17 deal explicitly with the issue of limited rationality—acting\nLIMITED\nRATIONALITY\nappropriately when there is not enough time to do all the computations one might like.\n1.2\nTHE FOUNDATIONS OF ARTIFICIAL INTELLIGENCE\nIn this section, we provide a brief history of the disciplines that contributed ideas, viewpoints,\nand techniques to AI. Like any history, this one is forced to concentrate on a small number\nof people, events, and ideas and to ignore others that also were important. We organize the\nhistory around a series of questions. We certainly would not wish to give the impression that\nthese questions are the only ones the disciplines address or that the disciplines have all been\nworking toward AI as their ultimate fruition.\n1.2.1\nPhilosophy\n• Can formal rules be used to draw valid conclusions?\n• How does the mind arise from a physical brain?\n• Where does knowledge come from?\n• How does knowledge lead to action?\nAristotle (384–322 B.C.), whose bust appears on the front cover of this book, was the ﬁrst\nto formulate a precise set of laws governing the rational part of the mind. He developed an\ninformal system of syllogisms for proper reasoning, which in principle allowed one to gener-\nate conclusions mechanically, given initial premises. Much later, Ramon Lull (d. 1315) had\nthe idea that useful reasoning could actually be carried out by a mechanical artifact. Thomas\nHobbes (1588–1679) proposed that reasoning was like numerical computation, that “we add\nand subtract in our silent thoughts.” The automation of computation itself was already well\nunder way. Around 1500, Leonardo da Vinci (1452–1519) designed but did not build a me-\nchanical calculator; recent reconstructions have shown the design to be functional. The ﬁrst\nknown calculating machine was constructed around 1623 by the German scientist Wilhelm\nSchickard (1592–1635), although the Pascaline, built in 1642 by Blaise Pascal (1623–1662), 6\nChapter\n1.\nIntroduction\nis more famous. Pascal wrote that “the arithmetical machine produces effects which appear",
  "Schickard (1592–1635), although the Pascaline, built in 1642 by Blaise Pascal (1623–1662), 6\nChapter\n1.\nIntroduction\nis more famous. Pascal wrote that “the arithmetical machine produces effects which appear\nnearer to thought than all the actions of animals.” Gottfried Wilhelm Leibniz (1646–1716)\nbuilt a mechanical device intended to carry out operations on concepts rather than numbers,\nbut its scope was rather limited. Leibniz did surpass Pascal by building a calculator that\ncould add, subtract, multiply, and take roots, whereas the Pascaline could only add and sub-\ntract. Some speculated that machines might not just do calculations but actually be able to\nthink and act on their own. In his 1651 book Leviathan, Thomas Hobbes suggested the idea\nof an “artiﬁcial animal,” arguing “For what is the heart but a spring; and the nerves, but so\nmany strings; and the joints, but so many wheels.”\nIt’s one thing to say that the mind operates, at least in part, according to logical rules, and\nto build physical systems that emulate some of those rules; it’s another to say that the mind\nitself is such a physical system. Ren´e Descartes (1596–1650) gave the ﬁrst clear discussion\nof the distinction between mind and matter and of the problems that arise. One problem with\na purely physical conception of the mind is that it seems to leave little room for free will:\nif the mind is governed entirely by physical laws, then it has no more free will than a rock\n“deciding” to fall toward the center of the earth. Descartes was a strong advocate of the power\nof reasoning in understanding the world, a philosophy now called rationalism, and one that\nRATIONALISM\ncounts Aristotle and Leibnitz as members. But Descartes was also a proponent of dualism.\nDUALISM\nHe held that there is a part of the human mind (or soul or spirit) that is outside of nature,\nexempt from physical laws. Animals, on the other hand, did not possess this dual quality;\nthey could be treated as machines. An alternative to dualism is materialism, which holds\nMATERIALISM\nthat the brain’s operation according to the laws of physics constitutes the mind. Free will is\nsimply the way that the perception of available choices appears to the choosing entity.\nGiven a physical mind that manipulates knowledge, the next problem is to establish\nthe source of knowledge. The empiricism movement, starting with Francis Bacon’s (1561–\nEMPIRICISM\n1626) Novum Organum,2 is characterized by a dictum of John Locke (1632–1704): “Nothing",
  "the source of knowledge. The empiricism movement, starting with Francis Bacon’s (1561–\nEMPIRICISM\n1626) Novum Organum,2 is characterized by a dictum of John Locke (1632–1704): “Nothing\nis in the understanding, which was not ﬁrst in the senses.” David Hume’s (1711–1776) A\nTreatise of Human Nature (Hume, 1739) proposed what is now known as the principle of\ninduction: that general rules are acquired by exposure to repeated associations between their\nINDUCTION\nelements. Building on the work of Ludwig Wittgenstein (1889–1951) and Bertrand Russell\n(1872–1970), the famous Vienna Circle, led by Rudolf Carnap (1891–1970), developed the\ndoctrine of logical positivism. This doctrine holds that all knowledge can be characterized by\nLOGICAL POSITIVISM\nlogical theories connected, ultimately, to observation sentences that correspond to sensory\nOBSERVATION\nSENTENCES\ninputs; thus logical positivism combines rationalism and empiricism.3 The conﬁrmation the-\nory of Carnap and Carl Hempel (1905–1997) attempted to analyze the acquisition of knowl-\nCONFIRMATION\nTHEORY\nedge from experience. Carnap’s book The Logical Structure of the World (1928) deﬁned an\nexplicit computational procedure for extracting knowledge from elementary experiences. It\nwas probably the ﬁrst theory of mind as a computational process.\n2 The Novum Organum is an update of Aristotle’s Organon, or instrument of thought. Thus Aristotle can be\nseen as both an empiricist and a rationalist.\n3 In this picture, all meaningful statements can be veriﬁed or falsiﬁed either by experimentation or by analysis\nof the meaning of the words. Because this rules out most of metaphysics, as was the intention, logical positivism\nwas unpopular in some circles. Section 1.2.\nThe Foundations of Artiﬁcial Intelligence\n7\nThe ﬁnal element in the philosophical picture of the mind is the connection between\nknowledge and action. This question is vital to AI because intelligence requires action as well\nas reasoning. Moreover, only by understanding how actions are justiﬁed can we understand\nhow to build an agent whose actions are justiﬁable (or rational). Aristotle argued (in De Motu\nAnimalium) that actions are justiﬁed by a logical connection between goals and knowledge of\nthe action’s outcome (the last part of this extract also appears on the front cover of this book,\nin the original Greek):\nBut how does it happen that thinking is sometimes accompanied by action and sometimes",
  "the action’s outcome (the last part of this extract also appears on the front cover of this book,\nin the original Greek):\nBut how does it happen that thinking is sometimes accompanied by action and sometimes\nnot, sometimes by motion, and sometimes not? It looks as if almost the same thing\nhappens as in the case of reasoning and making inferences about unchanging objects. But\nin that case the end is a speculative proposition . . . whereas here the conclusion which\nresults from the two premises is an action. . . . I need covering; a cloak is a covering. I\nneed a cloak. What I need, I have to make; I need a cloak. I have to make a cloak. And\nthe conclusion, the “I have to make a cloak,” is an action.\nIn the Nicomachean Ethics (Book III. 3, 1112b), Aristotle further elaborates on this topic,\nsuggesting an algorithm:\nWe deliberate not about ends, but about means. For a doctor does not deliberate whether\nhe shall heal, nor an orator whether he shall persuade, . . . They assume the end and\nconsider how and by what means it is attained, and if it seems easily and best produced\nthereby; while if it is achieved by one means only they consider how it will be achieved\nby this and by what means this will be achieved, till they come to the ﬁrst cause, . . . and\nwhat is last in the order of analysis seems to be ﬁrst in the order of becoming. And if we\ncome on an impossibility, we give up the search, e.g., if we need money and this cannot\nbe got; but if a thing appears possible we try to do it.\nAristotle’s algorithm was implemented 2300 years later by Newell and Simon in their GPS\nprogram. We would now call it a regression planning system (see Chapter 10).\nGoal-based analysis is useful, but does not say what to do when several actions will\nachieve the goal or when no action will achieve it completely. Antoine Arnauld (1612–1694)\ncorrectly described a quantitative formula for deciding what action to take in cases like this\n(see Chapter 16). John Stuart Mill’s (1806–1873) book Utilitarianism (Mill, 1863) promoted\nthe idea of rational decision criteria in all spheres of human activity. The more formal theory\nof decisions is discussed in the following section.\n1.2.2\nMathematics\n• What are the formal rules to draw valid conclusions?\n• What can be computed?\n• How do we reason with uncertain information?\nPhilosophers staked out some of the fundamental ideas of AI, but the leap to a formal science",
  "1.2.2\nMathematics\n• What are the formal rules to draw valid conclusions?\n• What can be computed?\n• How do we reason with uncertain information?\nPhilosophers staked out some of the fundamental ideas of AI, but the leap to a formal science\nrequired a level of mathematical formalization in three fundamental areas: logic, computa-\ntion, and probability.\nThe idea of formal logic can be traced back to the philosophers of ancient Greece, but\nits mathematical development really began with the work of George Boole (1815–1864), who 8\nChapter\n1.\nIntroduction\nworked out the details of propositional, or Boolean, logic (Boole, 1847). In 1879, Gottlob\nFrege (1848–1925) extended Boole’s logic to include objects and relations, creating the ﬁrst-\norder logic that is used today.4 Alfred Tarski (1902–1983) introduced a theory of reference\nthat shows how to relate the objects in a logic to objects in the real world.\nThe next step was to determine the limits of what could be done with logic and com-\nputation. The ﬁrst nontrivial algorithm is thought to be Euclid’s algorithm for computing\nALGORITHM\ngreatest common divisors. The word algorithm (and the idea of studying them) comes from\nal-Khowarazmi, a Persian mathematician of the 9th century, whose writings also introduced\nArabic numerals and algebra to Europe. Boole and others discussed algorithms for logical\ndeduction, and, by the late 19th century, efforts were under way to formalize general mathe-\nmatical reasoning as logical deduction. In 1930, Kurt G¨odel (1906–1978) showed that there\nexists an effective procedure to prove any true statement in the ﬁrst-order logic of Frege and\nRussell, but that ﬁrst-order logic could not capture the principle of mathematical induction\nneeded to characterize the natural numbers. In 1931, G¨odel showed that limits on deduc-\ntion do exist. His incompleteness theorem showed that in any formal theory as strong as\nINCOMPLETENESS\nTHEOREM\nPeano arithmetic (the elementary theory of natural numbers), there are true statements that\nare undecidable in the sense that they have no proof within the theory.\nThis fundamental result can also be interpreted as showing that some functions on the\nintegers cannot be represented by an algorithm—that is, they cannot be computed. This\nmotivated Alan Turing (1912–1954) to try to characterize exactly which functions are com-\nputable—capable of being computed. This notion is actually slightly problematic because\nCOMPUTABLE",
  "motivated Alan Turing (1912–1954) to try to characterize exactly which functions are com-\nputable—capable of being computed. This notion is actually slightly problematic because\nCOMPUTABLE\nthe notion of a computation or effective procedure really cannot be given a formal deﬁnition.\nHowever, the Church–Turing thesis, which states that the Turing machine (Turing, 1936) is\ncapable of computing any computable function, is generally accepted as providing a sufﬁcient\ndeﬁnition. Turing also showed that there were some functions that no Turing machine can\ncompute. For example, no machine can tell in general whether a given program will return\nan answer on a given input or run forever.\nAlthough decidability and computability are important to an understanding of computa-\ntion, the notion of tractability has had an even greater impact. Roughly speaking, a problem\nTRACTABILITY\nis called intractable if the time required to solve instances of the problem grows exponentially\nwith the size of the instances. The distinction between polynomial and exponential growth\nin complexity was ﬁrst emphasized in the mid-1960s (Cobham, 1964; Edmonds, 1965). It is\nimportant because exponential growth means that even moderately large instances cannot be\nsolved in any reasonable time. Therefore, one should strive to divide the overall problem of\ngenerating intelligent behavior into tractable subproblems rather than intractable ones.\nHow can one recognize an intractable problem? The theory of NP-completeness, pio-\nNP-COMPLETENESS\nneered by Steven Cook (1971) and Richard Karp (1972), provides a method. Cook and Karp\nshowed the existence of large classes of canonical combinatorial search and reasoning prob-\nlems that are NP-complete. Any problem class to which the class of NP-complete problems\ncan be reduced is likely to be intractable. (Although it has not been proved that NP-complete\n4 Frege’s proposed notation for ﬁrst-order logic—an arcane combination of textual and geometric features—\nnever became popular. Section 1.2.\nThe Foundations of Artiﬁcial Intelligence\n9\nproblems are necessarily intractable, most theoreticians believe it.) These results contrast\nwith the optimism with which the popular press greeted the ﬁrst computers—“Electronic\nSuper-Brains” that were “Faster than Einstein!” Despite the increasing speed of computers,\ncareful use of resources will characterize intelligent systems. Put crudely, the world is an",
  "Super-Brains” that were “Faster than Einstein!” Despite the increasing speed of computers,\ncareful use of resources will characterize intelligent systems. Put crudely, the world is an\nextremely large problem instance! Work in AI has helped explain why some instances of\nNP-complete problems are hard, yet others are easy (Cheeseman et al., 1991).\nBesides logic and computation, the third great contribution of mathematics to AI is the\ntheory of probability. The Italian Gerolamo Cardano (1501–1576) ﬁrst framed the idea of\nPROBABILITY\nprobability, describing it in terms of the possible outcomes of gambling events. In 1654,\nBlaise Pascal (1623–1662), in a letter to Pierre Fermat (1601–1665), showed how to pre-\ndict the future of an unﬁnished gambling game and assign average payoffs to the gamblers.\nProbability quickly became an invaluable part of all the quantitative sciences, helping to deal\nwith uncertain measurements and incomplete theories. James Bernoulli (1654–1705), Pierre\nLaplace (1749–1827), and others advanced the theory and introduced new statistical meth-\nods. Thomas Bayes (1702–1761), who appears on the front cover of this book, proposed\na rule for updating probabilities in the light of new evidence. Bayes’ rule underlies most\nmodern approaches to uncertain reasoning in AI systems.\n1.2.3\nEconomics\n• How should we make decisions so as to maximize payoff?\n• How should we do this when others may not go along?\n• How should we do this when the payoff may be far in the future?\nThe science of economics got its start in 1776, when Scottish philosopher Adam Smith\n(1723–1790) published An Inquiry into the Nature and Causes of the Wealth of Nations.\nWhile the ancient Greeks and others had made contributions to economic thought, Smith was\nthe ﬁrst to treat it as a science, using the idea that economies can be thought of as consist-\ning of individual agents maximizing their own economic well-being. Most people think of\neconomics as being about money, but economists will say that they are really studying how\npeople make choices that lead to preferred outcomes. When McDonald’s offers a hamburger\nfor a dollar, they are asserting that they would prefer the dollar and hoping that customers will\nprefer the hamburger. The mathematical treatment of “preferred outcomes” or utility was\nUTILITY\nﬁrst formalized by L´eon Walras (pronounced “Valrasse”) (1834-1910) and was improved by\nFrank Ramsey (1931) and later by John von Neumann and Oskar Morgenstern in their book",
  "UTILITY\nﬁrst formalized by L´eon Walras (pronounced “Valrasse”) (1834-1910) and was improved by\nFrank Ramsey (1931) and later by John von Neumann and Oskar Morgenstern in their book\nThe Theory of Games and Economic Behavior (1944).\nDecision theory, which combines probability theory with utility theory, provides a for-\nDECISION THEORY\nmal and complete framework for decisions (economic or otherwise) made under uncertainty—\nthat is, in cases where probabilistic descriptions appropriately capture the decision maker’s\nenvironment. This is suitable for “large” economies where each agent need pay no attention\nto the actions of other agents as individuals. For “small” economies, the situation is much\nmore like a game: the actions of one player can signiﬁcantly affect the utility of another\n(either positively or negatively). Von Neumann and Morgenstern’s development of game\ntheory (see also Luce and Raiffa, 1957) included the surprising result that, for some games,\nGAME THEORY 10\nChapter\n1.\nIntroduction\na rational agent should adopt policies that are (or least appear to be) randomized. Unlike de-\ncision theory, game theory does not offer an unambiguous prescription for selecting actions.\nFor the most part, economists did not address the third question listed above, namely,\nhow to make rational decisions when payoffs from actions are not immediate but instead re-\nsult from several actions taken in sequence. This topic was pursued in the ﬁeld of operations\nresearch, which emerged in World War II from efforts in Britain to optimize radar installa-\nOPERATIONS\nRESEARCH\ntions, and later found civilian applications in complex management decisions. The work of\nRichard Bellman (1957) formalized a class of sequential decision problems called Markov\ndecision processes, which we study in Chapters 17 and 21.\nWork in economics and operations research has contributed much to our notion of ra-\ntional agents, yet for many years AI research developed along entirely separate paths. One\nreason was the apparent complexity of making rational decisions. The pioneering AI re-\nsearcher Herbert Simon (1916–2001) won the Nobel Prize in economics in 1978 for his early\nwork showing that models based on satisﬁcing—making decisions that are “good enough,”\nSATISFICING\nrather than laboriously calculating an optimal decision—gave a better description of actual\nhuman behavior (Simon, 1947). Since the 1990s, there has been a resurgence of interest in",
  "SATISFICING\nrather than laboriously calculating an optimal decision—gave a better description of actual\nhuman behavior (Simon, 1947). Since the 1990s, there has been a resurgence of interest in\ndecision-theoretic techniques for agent systems (Wellman, 1995).\n1.2.4\nNeuroscience\n• How do brains process information?\nNeuroscience is the study of the nervous system, particularly the brain. Although the exact\nNEUROSCIENCE\nway in which the brain enables thought is one of the great mysteries of science, the fact that it\ndoes enable thought has been appreciated for thousands of years because of the evidence that\nstrong blows to the head can lead to mental incapacitation. It has also long been known that\nhuman brains are somehow different; in about 335 B.C. Aristotle wrote, “Of all the animals,\nman has the largest brain in proportion to his size.”5 Still, it was not until the middle of the\n18th century that the brain was widely recognized as the seat of consciousness. Before then,\ncandidate locations included the heart and the spleen.\nPaul Broca’s (1824–1880) study of aphasia (speech deﬁcit) in brain-damaged patients\nin 1861 demonstrated the existence of localized areas of the brain responsible for speciﬁc\ncognitive functions. In particular, he showed that speech production was localized to the\nportion of the left hemisphere now called Broca’s area.6 By that time, it was known that\nthe brain consisted of nerve cells, or neurons, but it was not until 1873 that Camillo Golgi\nNEURON\n(1843–1926) developed a staining technique allowing the observation of individual neurons\nin the brain (see Figure 1.2). This technique was used by Santiago Ramon y Cajal (1852–\n1934) in his pioneering studies of the brain’s neuronal structures.7 Nicolas Rashevsky (1936,\n1938) was the ﬁrst to apply mathematical models to the study of the nervous sytem.\n5 Since then, it has been discovered that the tree shrew (Scandentia) has a higher ratio of brain to body mass.\n6 Many cite Alexander Hood (1824) as a possible prior source.\n7 Golgi persisted in his belief that the brain’s functions were carried out primarily in a continuous medium in\nwhich neurons were embedded, whereas Cajal propounded the “neuronal doctrine.” The two shared the Nobel\nprize in 1906 but gave mutually antagonistic acceptance speeches. Section 1.2.\nThe Foundations of Artiﬁcial Intelligence\n11\nAxon\nCell body or Soma\nNucleus\nDendrite\nSynapses\nAxonal arborization\nAxon from another cell\nSynapse\nFigure 1.2",
  "prize in 1906 but gave mutually antagonistic acceptance speeches. Section 1.2.\nThe Foundations of Artiﬁcial Intelligence\n11\nAxon\nCell body or Soma\nNucleus\nDendrite\nSynapses\nAxonal arborization\nAxon from another cell\nSynapse\nFigure 1.2\nThe parts of a nerve cell or neuron. Each neuron consists of a cell body,\nor soma, that contains a cell nucleus. Branching out from the cell body are a number of\nﬁbers called dendrites and a single long ﬁber called the axon. The axon stretches out for a\nlong distance, much longer than the scale in this diagram indicates. Typically, an axon is\n1 cm long (100 times the diameter of the cell body), but can reach up to 1 meter. A neuron\nmakes connections with 10 to 100,000 other neurons at junctions called synapses. Signals are\npropagated from neuron to neuron by a complicated electrochemical reaction. The signals\ncontrol brain activity in the short term and also enable long-term changes in the connectivity\nof neurons. These mechanisms are thought to form the basis for learning in the brain. Most\ninformation processing goes on in the cerebral cortex, the outer layer of the brain. The basic\norganizational unit appears to be a column of tissue about 0.5 mm in diameter, containing\nabout 20,000 neurons and extending the full depth of the cortex about 4 mm in humans).\nWe now have some data on the mapping between areas of the brain and the parts of the\nbody that they control or from which they receive sensory input. Such mappings are able to\nchange radically over the course of a few weeks, and some animals seem to have multiple\nmaps. Moreover, we do not fully understand how other areas can take over functions when\none area is damaged. There is almost no theory on how an individual memory is stored.\nThe measurement of intact brain activity began in 1929 with the invention by Hans\nBerger of the electroencephalograph (EEG). The recent development of functional magnetic\nresonance imaging (fMRI) (Ogawa et al., 1990; Cabeza and Nyberg, 2001) is giving neu-\nroscientists unprecedentedly detailed images of brain activity, enabling measurements that\ncorrespond in interesting ways to ongoing cognitive processes. These are augmented by\nadvances in single-cell recording of neuron activity. Individual neurons can be stimulated\nelectrically, chemically, or even optically (Han and Boyden, 2007), allowing neuronal input–\noutput relationships to be mapped. Despite these advances, we are still a long way from",
  "electrically, chemically, or even optically (Han and Boyden, 2007), allowing neuronal input–\noutput relationships to be mapped. Despite these advances, we are still a long way from\nunderstanding how cognitive processes actually work.\nThe truly amazing conclusion is that a collection of simple cells can lead to thought,\naction, and consciousness or, in the pithy words of John Searle (1992), brains cause minds. 12\nChapter\n1.\nIntroduction\nSupercomputer\nPersonal Computer\nHuman Brain\nComputational units\n104 CPUs, 1012 transistors 4 CPUs, 109 transistors 1011 neurons\nStorage units\n1014 bits RAM\n1011 bits RAM\n1011 neurons\n1015 bits disk\n1013 bits disk\n1014 synapses\nCycle time\n10−9 sec\n10−9 sec\n10−3 sec\nOperations/sec\n1015\n1010\n1017\nMemory updates/sec\n1014\n1010\n1014\nFigure 1.3\nA crude comparison of the raw computational resources available to the IBM\nBLUE GENE supercomputer, a typical personal computer of 2008, and the human brain. The\nbrain’s numbers are essentially ﬁxed, whereas the supercomputer’s numbers have been in-\ncreasing by a factor of 10 every 5 years or so, allowing it to achieve rough parity with the\nbrain. The personal computer lags behind on all metrics except cycle time.\nThe only real alternative theory is mysticism: that minds operate in some mystical realm that\nis beyond physical science.\nBrains and digital computers have somewhat different properties. Figure 1.3 shows that\ncomputers have a cycle time that is a million times faster than a brain. The brain makes up\nfor that with far more storage and interconnection than even a high-end personal computer,\nalthough the largest supercomputers have a capacity that is similar to the brain’s. (It should\nbe noted, however, that the brain does not seem to use all of its neurons simultaneously.)\nFuturists make much of these numbers, pointing to an approaching singularity at which\nSINGULARITY\ncomputers reach a superhuman level of performance (Vinge, 1993; Kurzweil, 2005), but the\nraw comparisons are not especially informative. Even with a computer of virtually unlimited\ncapacity, we still would not know how to achieve the brain’s level of intelligence.\n1.2.5\nPsychology\n• How do humans and animals think and act?\nThe origins of scientiﬁc psychology are usually traced to the work of the German physi-\ncist Hermann von Helmholtz (1821–1894) and his student Wilhelm Wundt (1832–1920).\nHelmholtz applied the scientiﬁc method to the study of human vision, and his Handbook",
  "cist Hermann von Helmholtz (1821–1894) and his student Wilhelm Wundt (1832–1920).\nHelmholtz applied the scientiﬁc method to the study of human vision, and his Handbook\nof Physiological Optics is even now described as “the single most important treatise on the\nphysics and physiology of human vision” (Nalwa, 1993, p.15). In 1879, Wundt opened the\nﬁrst laboratory of experimental psychology, at the University of Leipzig. Wundt insisted\non carefully controlled experiments in which his workers would perform a perceptual or as-\nsociative task while introspecting on their thought processes. The careful controls went a\nlong way toward making psychology a science, but the subjective nature of the data made\nit unlikely that an experimenter would ever disconﬁrm his or her own theories. Biologists\nstudying animal behavior, on the other hand, lacked introspective data and developed an ob-\njective methodology, as described by H. S. Jennings (1906) in his inﬂuential work Behavior of\nthe Lower Organisms. Applying this viewpoint to humans, the behaviorism movement, led\nBEHAVIORISM\nby John Watson (1878–1958), rejected any theory involving mental processes on the grounds Section 1.2.\nThe Foundations of Artiﬁcial Intelligence\n13\nthat introspection could not provide reliable evidence. Behaviorists insisted on studying only\nobjective measures of the percepts (or stimulus) given to an animal and its resulting actions\n(or response). Behaviorism discovered a lot about rats and pigeons but had less success at\nunderstanding humans.\nCognitive psychology, which views the brain as an information-processing device,\nCOGNITIVE\nPSYCHOLOGY\ncan be traced back at least to the works of William James (1842–1910). Helmholtz also\ninsisted that perception involved a form of unconscious logical inference. The cognitive\nviewpoint was largely eclipsed by behaviorism in the United States, but at Cambridge’s Ap-\nplied Psychology Unit, directed by Frederic Bartlett (1886–1969), cognitive modeling was\nable to ﬂourish. The Nature of Explanation, by Bartlett’s student and successor Kenneth\nCraik (1943), forcefully reestablished the legitimacy of such “mental” terms as beliefs and\ngoals, arguing that they are just as scientiﬁc as, say, using pressure and temperature to talk\nabout gases, despite their being made of molecules that have neither. Craik speciﬁed the\nthree key steps of a knowledge-based agent: (1) the stimulus must be translated into an inter-",
  "about gases, despite their being made of molecules that have neither. Craik speciﬁed the\nthree key steps of a knowledge-based agent: (1) the stimulus must be translated into an inter-\nnal representation, (2) the representation is manipulated by cognitive processes to derive new\ninternal representations, and (3) these are in turn retranslated back into action. He clearly\nexplained why this was a good design for an agent:\nIf the organism carries a “small-scale model” of external reality and of its own possible\nactions within its head, it is able to try out various alternatives, conclude which is the best\nof them, react to future situations before they arise, utilize the knowledge of past events\nin dealing with the present and future, and in every way to react in a much fuller, safer,\nand more competent manner to the emergencies which face it. (Craik, 1943)\nAfter Craik’s death in a bicycle accident in 1945, his work was continued by Donald Broad-\nbent, whose book Perception and Communication (1958) was one of the ﬁrst works to model\npsychological phenomena as information processing. Meanwhile, in the United States, the\ndevelopment of computer modeling led to the creation of the ﬁeld of cognitive science. The\nﬁeld can be said to have started at a workshop in September 1956 at MIT. (We shall see that\nthis is just two months after the conference at which AI itself was “born.”) At the workshop,\nGeorge Miller presented The Magic Number Seven, Noam Chomsky presented Three Models\nof Language, and Allen Newell and Herbert Simon presented The Logic Theory Machine.\nThese three inﬂuential papers showed how computer models could be used to address the\npsychology of memory, language, and logical thinking, respectively. It is now a common\n(although far from universal) view among psychologists that “a cognitive theory should be\nlike a computer program” (Anderson, 1980); that is, it should describe a detailed information-\nprocessing mechanism whereby some cognitive function might be implemented.\n1.2.6\nComputer engineering\n• How can we build an efﬁcient computer?\nFor artiﬁcial intelligence to succeed, we need two things: intelligence and an artifact. The\ncomputer has been the artifact of choice. The modern digital electronic computer was in-\nvented independently and almost simultaneously by scientists in three countries embattled in 14\nChapter\n1.\nIntroduction\nWorld War II. The ﬁrst operational computer was the electromechanical Heath Robinson,8",
  "vented independently and almost simultaneously by scientists in three countries embattled in 14\nChapter\n1.\nIntroduction\nWorld War II. The ﬁrst operational computer was the electromechanical Heath Robinson,8\nbuilt in 1940 by Alan Turing’s team for a single purpose: deciphering German messages. In\n1943, the same group developed the Colossus, a powerful general-purpose machine based\non vacuum tubes.9 The ﬁrst operational programmable computer was the Z-3, the inven-\ntion of Konrad Zuse in Germany in 1941. Zuse also invented ﬂoating-point numbers and the\nﬁrst high-level programming language, Plankalk¨ul. The ﬁrst electronic computer, the ABC,\nwas assembled by John Atanasoff and his student Clifford Berry between 1940 and 1942\nat Iowa State University. Atanasoff’s research received little support or recognition; it was\nthe ENIAC, developed as part of a secret military project at the University of Pennsylvania\nby a team including John Mauchly and John Eckert, that proved to be the most inﬂuential\nforerunner of modern computers.\nSince that time, each generation of computer hardware has brought an increase in speed\nand capacity and a decrease in price. Performance doubled every 18 months or so until around\n2005, when power dissipation problems led manufacturers to start multiplying the number of\nCPU cores rather than the clock speed. Current expectations are that future increases in power\nwill come from massive parallelism—a curious convergence with the properties of the brain.\nOf course, there were calculating devices before the electronic computer. The earliest\nautomated machines, dating from the 17th century, were discussed on page 6. The ﬁrst pro-\ngrammable machine was a loom, devised in 1805 by Joseph Marie Jacquard (1752–1834),\nthat used punched cards to store instructions for the pattern to be woven. In the mid-19th\ncentury, Charles Babbage (1792–1871) designed two machines, neither of which he com-\npleted. The Difference Engine was intended to compute mathematical tables for engineering\nand scientiﬁc projects. It was ﬁnally built and shown to work in 1991 at the Science Museum\nin London (Swade, 2000). Babbage’s Analytical Engine was far more ambitious: it included\naddressable memory, stored programs, and conditional jumps and was the ﬁrst artifact capa-\nble of universal computation. Babbage’s colleague Ada Lovelace, daughter of the poet Lord\nByron, was perhaps the world’s ﬁrst programmer. (The programming language Ada is named",
  "ble of universal computation. Babbage’s colleague Ada Lovelace, daughter of the poet Lord\nByron, was perhaps the world’s ﬁrst programmer. (The programming language Ada is named\nafter her.) She wrote programs for the unﬁnished Analytical Engine and even speculated that\nthe machine could play chess or compose music.\nAI also owes a debt to the software side of computer science, which has supplied the\noperating systems, programming languages, and tools needed to write modern programs (and\npapers about them). But this is one area where the debt has been repaid: work in AI has pio-\nneered many ideas that have made their way back to mainstream computer science, including\ntime sharing, interactive interpreters, personal computers with windows and mice, rapid de-\nvelopment environments, the linked list data type, automatic storage management, and key\nconcepts of symbolic, functional, declarative, and object-oriented programming.\n8 Heath Robinson was a cartoonist famous for his depictions of whimsical and absurdly complicated contrap-\ntions for everyday tasks such as buttering toast.\n9 In the postwar period, Turing wanted to use these computers for AI research—for example, one of the ﬁrst\nchess programs (Turing et al., 1953). His efforts were blocked by the British government. Section 1.2.\nThe Foundations of Artiﬁcial Intelligence\n15\n1.2.7\nControl theory and cybernetics\n• How can artifacts operate under their own control?\nKtesibios of Alexandria (c. 250 B.C.) built the ﬁrst self-controlling machine: a water clock\nwith a regulator that maintained a constant ﬂow rate. This invention changed the deﬁnition\nof what an artifact could do. Previously, only living things could modify their behavior in\nresponse to changes in the environment. Other examples of self-regulating feedback control\nsystems include the steam engine governor, created by James Watt (1736–1819), and the\nthermostat, invented by Cornelis Drebbel (1572–1633), who also invented the submarine.\nThe mathematical theory of stable feedback systems was developed in the 19th century.\nThe central ﬁgure in the creation of what is now called control theory was Norbert\nCONTROL THEORY\nWiener (1894–1964). Wiener was a brilliant mathematician who worked with Bertrand Rus-\nsell, among others, before developing an interest in biological and mechanical control systems\nand their connection to cognition. Like Craik (who also used control systems as psychological",
  "sell, among others, before developing an interest in biological and mechanical control systems\nand their connection to cognition. Like Craik (who also used control systems as psychological\nmodels), Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow challenged the\nbehaviorist orthodoxy (Rosenblueth et al., 1943). They viewed purposive behavior as aris-\ning from a regulatory mechanism trying to minimize “error”—the difference between current\nstate and goal state. In the late 1940s, Wiener, along with Warren McCulloch, Walter Pitts,\nand John von Neumann, organized a series of inﬂuential conferences that explored the new\nmathematical and computational models of cognition. Wiener’s book Cybernetics (1948) be-\nCYBERNETICS\ncame a bestseller and awoke the public to the possibility of artiﬁcially intelligent machines.\nMeanwhile, in Britain, W. Ross Ashby (Ashby, 1940) pioneered similar ideas. Ashby, Alan\nTuring, Grey Walter, and others formed the Ratio Club for “those who had Wiener’s ideas\nbefore Wiener’s book appeared.” Ashby’s Design for a Brain (1948, 1952) elaborated on his\nidea that intelligence could be created by the use of homeostatic devices containing appro-\nHOMEOSTATIC\npriate feedback loops to achieve stable adaptive behavior.\nModern control theory, especially the branch known as stochastic optimal control, has\nas its goal the design of systems that maximize an objective function over time. This roughly\nOBJECTIVE\nFUNCTION\nmatches our view of AI: designing systems that behave optimally. Why, then, are AI and\ncontrol theory two different ﬁelds, despite the close connections among their founders? The\nanswer lies in the close coupling between the mathematical techniques that were familiar to\nthe participants and the corresponding sets of problems that were encompassed in each world\nview. Calculus and matrix algebra, the tools of control theory, lend themselves to systems that\nare describable by ﬁxed sets of continuous variables, whereas AI was founded in part as a way\nto escape from the these perceived limitations. The tools of logical inference and computation\nallowed AI researchers to consider problems such as language, vision, and planning that fell\ncompletely outside the control theorist’s purview.\n1.2.8\nLinguistics\n• How does language relate to thought?\nIn 1957, B. F. Skinner published Verbal Behavior. This was a comprehensive, detailed ac-\ncount of the behaviorist approach to language learning, written by the foremost expert in 16",
  "1.2.8\nLinguistics\n• How does language relate to thought?\nIn 1957, B. F. Skinner published Verbal Behavior. This was a comprehensive, detailed ac-\ncount of the behaviorist approach to language learning, written by the foremost expert in 16\nChapter\n1.\nIntroduction\nthe ﬁeld. But curiously, a review of the book became as well known as the book itself, and\nserved to almost kill off interest in behaviorism. The author of the review was the linguist\nNoam Chomsky, who had just published a book on his own theory, Syntactic Structures.\nChomsky pointed out that the behaviorist theory did not address the notion of creativity in\nlanguage—it did not explain how a child could understand and make up sentences that he or\nshe had never heard before. Chomsky’s theory—based on syntactic models going back to the\nIndian linguist Panini (c. 350 B.C.)—could explain this, and unlike previous theories, it was\nformal enough that it could in principle be programmed.\nModern linguistics and AI, then, were “born” at about the same time, and grew up\ntogether, intersecting in a hybrid ﬁeld called computational linguistics or natural language\nCOMPUTATIONAL\nLINGUISTICS\nprocessing. The problem of understanding language soon turned out to be considerably more\ncomplex than it seemed in 1957. Understanding language requires an understanding of the\nsubject matter and context, not just an understanding of the structure of sentences. This might\nseem obvious, but it was not widely appreciated until the 1960s. Much of the early work in\nknowledge representation (the study of how to put knowledge into a form that a computer\ncan reason with) was tied to language and informed by research in linguistics, which was\nconnected in turn to decades of work on the philosophical analysis of language.\n1.3\nTHE HISTORY OF ARTIFICIAL INTELLIGENCE\nWith the background material behind us, we are ready to cover the development of AI itself.\n1.3.1\nThe gestation of artiﬁcial intelligence (1943–1955)\nThe ﬁrst work that is now generally recognized as AI was done by Warren McCulloch and\nWalter Pitts (1943). They drew on three sources: knowledge of the basic physiology and\nfunction of neurons in the brain; a formal analysis of propositional logic due to Russell and\nWhitehead; and Turing’s theory of computation. They proposed a model of artiﬁcial neurons\nin which each neuron is characterized as being “on” or “off,” with a switch to “on” occurring",
  "Whitehead; and Turing’s theory of computation. They proposed a model of artiﬁcial neurons\nin which each neuron is characterized as being “on” or “off,” with a switch to “on” occurring\nin response to stimulation by a sufﬁcient number of neighboring neurons. The state of a\nneuron was conceived of as “factually equivalent to a proposition which proposed its adequate\nstimulus.” They showed, for example, that any computable function could be computed by\nsome network of connected neurons, and that all the logical connectives (and, or, not, etc.)\ncould be implemented by simple net structures. McCulloch and Pitts also suggested that\nsuitably deﬁned networks could learn. Donald Hebb (1949) demonstrated a simple updating\nrule for modifying the connection strengths between neurons. His rule, now called Hebbian\nlearning, remains an inﬂuential model to this day.\nHEBBIAN LEARNING\nTwo undergraduate students at Harvard, Marvin Minsky and Dean Edmonds, built the\nﬁrst neural network computer in 1950. The SNARC, as it was called, used 3000 vacuum\ntubes and a surplus automatic pilot mechanism from a B-24 bomber to simulate a network of\n40 neurons. Later, at Princeton, Minsky studied universal computation in neural networks.\nHis Ph.D. committee was skeptical about whether this kind of work should be considered Section 1.3.\nThe History of Artiﬁcial Intelligence\n17\nmathematics, but von Neumann reportedly said, “If it isn’t now, it will be someday.” Minsky\nwas later to prove inﬂuential theorems showing the limitations of neural network research.\nThere were a number of early examples of work that can be characterized as AI, but\nAlan Turing’s vision was perhaps the most inﬂuential. He gave lectures on the topic as early\nas 1947 at the London Mathematical Society and articulated a persuasive agenda in his 1950\narticle “Computing Machinery and Intelligence.” Therein, he introduced the Turing Test,\nmachine learning, genetic algorithms, and reinforcement learning. He proposed the Child\nProgramme idea, explaining “Instead of trying to produce a programme to simulate the adult\nmind, why not rather try to produce one which simulated the child’s?”\n1.3.2\nThe birth of artiﬁcial intelligence (1956)\nPrinceton was home to another inﬂuential ﬁgure in AI, John McCarthy. After receiving his\nPhD there in 1951 and working for two years as an instructor, McCarthy moved to Stan-\nford and then to Dartmouth College, which was to become the ofﬁcial birthplace of the ﬁeld.",
  "PhD there in 1951 and working for two years as an instructor, McCarthy moved to Stan-\nford and then to Dartmouth College, which was to become the ofﬁcial birthplace of the ﬁeld.\nMcCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring\ntogether U.S. researchers interested in automata theory, neural nets, and the study of intel-\nligence. They organized a two-month workshop at Dartmouth in the summer of 1956. The\nproposal states:10\nWe propose that a 2 month, 10 man study of artiﬁcial intelligence be carried\nout during the summer of 1956 at Dartmouth College in Hanover, New Hamp-\nshire. The study is to proceed on the basis of the conjecture that every aspect of\nlearning or any other feature of intelligence can in principle be so precisely de-\nscribed that a machine can be made to simulate it. An attempt will be made to ﬁnd\nhow to make machines use language, form abstractions and concepts, solve kinds\nof problems now reserved for humans, and improve themselves. We think that a\nsigniﬁcant advance can be made in one or more of these problems if a carefully\nselected group of scientists work on it together for a summer.\nThere were 10 attendees in all, including Trenchard More from Princeton, Arthur Samuel\nfrom IBM, and Ray Solomonoff and Oliver Selfridge from MIT.\nTwo researchers from Carnegie Tech,11 Allen Newell and Herbert Simon, rather stole\nthe show. Although the others had ideas and in some cases programs for particular appli-\ncations such as checkers, Newell and Simon already had a reasoning program, the Logic\nTheorist (LT), about which Simon claimed, “We have invented a computer program capable\nof thinking non-numerically, and thereby solved the venerable mind–body problem.”12 Soon\nafter the workshop, the program was able to prove most of the theorems in Chapter 2 of Rus-\n10 This was the ﬁrst ofﬁcial usage of McCarthy’s term artiﬁcial intelligence. Perhaps “computational rationality”\nwould have been more precise and less threatening, but “AI” has stuck. At the 50th anniversary of the Dartmouth\nconference, McCarthy stated that he resisted the terms “computer” or “computational” in deference to Norbert\nWeiner, who was promoting analog cybernetic devices rather than digital computers.\n11 Now Carnegie Mellon University (CMU).\n12 Newell and Simon also invented a list-processing language, IPL, to write LT. They had no compiler and",
  "Weiner, who was promoting analog cybernetic devices rather than digital computers.\n11 Now Carnegie Mellon University (CMU).\n12 Newell and Simon also invented a list-processing language, IPL, to write LT. They had no compiler and\ntranslated it into machine code by hand. To avoid errors, they worked in parallel, calling out binary numbers to\neach other as they wrote each instruction to make sure they agreed. 18\nChapter\n1.\nIntroduction\nsell and Whitehead’s Principia Mathematica. Russell was reportedly delighted when Simon\nshowed him that the program had come up with a proof for one theorem that was shorter than\nthe one in Principia. The editors of the Journal of Symbolic Logic were less impressed; they\nrejected a paper coauthored by Newell, Simon, and Logic Theorist.\nThe Dartmouth workshop did not lead to any new breakthroughs, but it did introduce\nall the major ﬁgures to each other. For the next 20 years, the ﬁeld would be dominated by\nthese people and their students and colleagues at MIT, CMU, Stanford, and IBM.\nLooking at the proposal for the Dartmouth workshop (McCarthy et al., 1955), we can\nsee why it was necessary for AI to become a separate ﬁeld. Why couldn’t all the work done\nin AI have taken place under the name of control theory or operations research or decision\ntheory, which, after all, have objectives similar to those of AI? Or why isn’t AI a branch\nof mathematics? The ﬁrst answer is that AI from the start embraced the idea of duplicating\nhuman faculties such as creativity, self-improvement, and language use. None of the other\nﬁelds were addressing these issues. The second answer is methodology. AI is the only one\nof these ﬁelds that is clearly a branch of computer science (although operations research does\nshare an emphasis on computer simulations), and AI is the only ﬁeld to attempt to build\nmachines that will function autonomously in complex, changing environments.\n1.3.3\nEarly enthusiasm, great expectations (1952–1969)\nThe early years of AI were full of successes—in a limited way. Given the primitive comput-\ners and programming tools of the time and the fact that only a few years earlier computers\nwere seen as things that could do arithmetic and no more, it was astonishing whenever a com-\nputer did anything remotely clever. The intellectual establishment, by and large, preferred to\nbelieve that “a machine can never do X.” (See Chapter 26 for a long list of X’s gathered",
  "puter did anything remotely clever. The intellectual establishment, by and large, preferred to\nbelieve that “a machine can never do X.” (See Chapter 26 for a long list of X’s gathered\nby Turing.) AI researchers naturally responded by demonstrating one X after another. John\nMcCarthy referred to this period as the “Look, Ma, no hands!” era.\nNewell and Simon’s early success was followed up with the General Problem Solver,\nor GPS. Unlike Logic Theorist, this program was designed from the start to imitate human\nproblem-solving protocols. Within the limited class of puzzles it could handle, it turned out\nthat the order in which the program considered subgoals and possible actions was similar to\nthat in which humans approached the same problems. Thus, GPS was probably the ﬁrst pro-\ngram to embody the “thinking humanly” approach. The success of GPS and subsequent pro-\ngrams as models of cognition led Newell and Simon (1976) to formulate the famous physical\nsymbol system hypothesis, which states that “a physical symbol system has the necessary and\nPHYSICAL SYMBOL\nSYSTEM\nsufﬁcient means for general intelligent action.” What they meant is that any system (human\nor machine) exhibiting intelligence must operate by manipulating data structures composed\nof symbols. We will see later that this hypothesis has been challenged from many directions.\nAt IBM, Nathaniel Rochester and his colleagues produced some of the ﬁrst AI pro-\ngrams.\nHerbert Gelernter (1959) constructed the Geometry Theorem Prover, which was\nable to prove theorems that many students of mathematics would ﬁnd quite tricky. Starting\nin 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that eventually\nlearned to play at a strong amateur level. Along the way, he disproved the idea that comput- Section 1.3.\nThe History of Artiﬁcial Intelligence\n19\ners can do only what they are told to: his program quickly learned to play a better game than\nits creator. The program was demonstrated on television in February 1956, creating a strong\nimpression. Like Turing, Samuel had trouble ﬁnding computer time. Working at night, he\nused machines that were still on the testing ﬂoor at IBM’s manufacturing plant. Chapter 5\ncovers game playing, and Chapter 21 explains the learning techniques used by Samuel.\nJohn McCarthy moved from Dartmouth to MIT and there made three crucial contribu-\ntions in one historic year: 1958. In MIT AI Lab Memo No. 1, McCarthy deﬁned the high-level",
  "John McCarthy moved from Dartmouth to MIT and there made three crucial contribu-\ntions in one historic year: 1958. In MIT AI Lab Memo No. 1, McCarthy deﬁned the high-level\nlanguage Lisp, which was to become the dominant AI programming language for the next 30\nLISP\nyears. With Lisp, McCarthy had the tool he needed, but access to scarce and expensive com-\nputing resources was also a serious problem. In response, he and others at MIT invented time\nsharing. Also in 1958, McCarthy published a paper entitled Programs with Common Sense,\nin which he described the Advice Taker, a hypothetical program that can be seen as the ﬁrst\ncomplete AI system. Like the Logic Theorist and Geometry Theorem Prover, McCarthy’s\nprogram was designed to use knowledge to search for solutions to problems. But unlike the\nothers, it was to embody general knowledge of the world. For example, he showed how\nsome simple axioms would enable the program to generate a plan to drive to the airport. The\nprogram was also designed to accept new axioms in the normal course of operation, thereby\nallowing it to achieve competence in new areas without being reprogrammed. The Advice\nTaker thus embodied the central principles of knowledge representation and reasoning: that\nit is useful to have a formal, explicit representation of the world and its workings and to be\nable to manipulate that representation with deductive processes. It is remarkable how much\nof the 1958 paper remains relevant today.\n1958 also marked the year that Marvin Minsky moved to MIT. His initial collaboration\nwith McCarthy did not last, however. McCarthy stressed representation and reasoning in for-\nmal logic, whereas Minsky was more interested in getting programs to work and eventually\ndeveloped an anti-logic outlook. In 1963, McCarthy started the AI lab at Stanford. His plan\nto use logic to build the ultimate Advice Taker was advanced by J. A. Robinson’s discov-\nery in 1965 of the resolution method (a complete theorem-proving algorithm for ﬁrst-order\nlogic; see Chapter 9). Work at Stanford emphasized general-purpose methods for logical\nreasoning. Applications of logic included Cordell Green’s question-answering and planning\nsystems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute\n(SRI). The latter project, discussed further in Chapter 25, was the ﬁrst to demonstrate the\ncomplete integration of logical reasoning and physical activity.",
  "systems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute\n(SRI). The latter project, discussed further in Chapter 25, was the ﬁrst to demonstrate the\ncomplete integration of logical reasoning and physical activity.\nMinsky supervised a series of students who chose limited problems that appeared to\nrequire intelligence to solve. These limited domains became known as microworlds. James\nMICROWORLD\nSlagle’s SAINT program (1963) was able to solve closed-form calculus integration problems\ntypical of ﬁrst-year college courses. Tom Evans’s ANALOGY program (1968) solved geo-\nmetric analogy problems that appear in IQ tests. Daniel Bobrow’s STUDENT program (1967)\nsolved algebra story problems, such as the following:\nIf the number of customers Tom gets is twice the square of 20 percent of the number\nof advertisements he runs, and the number of advertisements he runs is 45, what is the\nnumber of customers Tom gets? 20\nChapter\n1.\nIntroduction\nRed\nGreen\nRed\nGreen\nGreen\nBlue\nBlue\nRed\nFigure 1.4\nA scene from the blocks world. SHRDLU (Winograd, 1972) has just completed\nthe command “Find a block which is taller than the one you are holding and put it in the box.”\nThe most famous microworld was the blocks world, which consists of a set of solid blocks\nplaced on a tabletop (or more often, a simulation of a tabletop), as shown in Figure 1.4.\nA typical task in this world is to rearrange the blocks in a certain way, using a robot hand\nthat can pick up one block at a time. The blocks world was home to the vision project of\nDavid Huffman (1971), the vision and constraint-propagation work of David Waltz (1975),\nthe learning theory of Patrick Winston (1970), the natural-language-understanding program\nof Terry Winograd (1972), and the planner of Scott Fahlman (1974).\nEarly work building on the neural networks of McCulloch and Pitts also ﬂourished.\nThe work of Winograd and Cowan (1963) showed how a large number of elements could\ncollectively represent an individual concept, with a corresponding increase in robustness and\nparallelism. Hebb’s learning methods were enhanced by Bernie Widrow (Widrow and Hoff,\n1960; Widrow, 1962), who called his networks adalines, and by Frank Rosenblatt (1962)\nwith his perceptrons. The perceptron convergence theorem (Block et al., 1962) says that\nthe learning algorithm can adjust the connection strengths of a perceptron to match any input\ndata, provided such a match exists. These topics are covered in Chapter 20.\n1.3.4",
  "the learning algorithm can adjust the connection strengths of a perceptron to match any input\ndata, provided such a match exists. These topics are covered in Chapter 20.\n1.3.4\nA dose of reality (1966–1973)\nFrom the beginning, AI researchers were not shy about making predictions of their coming\nsuccesses. The following statement by Herbert Simon in 1957 is often quoted:\nIt is not my aim to surprise or shock you—but the simplest way I can summarize is to say\nthat there are now in the world machines that think, that learn and that create. Moreover, Section 1.3.\nThe History of Artiﬁcial Intelligence\n21\ntheir ability to do these things is going to increase rapidly until—in a visible future—the\nrange of problems they can handle will be coextensive with the range to which the human\nmind has been applied.\nTerms such as “visible future” can be interpreted in various ways, but Simon also made\nmore concrete predictions: that within 10 years a computer would be chess champion, and\na signiﬁcant mathematical theorem would be proved by machine. These predictions came\ntrue (or approximately true) within 40 years rather than 10. Simon’s overconﬁdence was due\nto the promising performance of early AI systems on simple examples. In almost all cases,\nhowever, these early systems turned out to fail miserably when tried out on wider selections\nof problems and on more difﬁcult problems.\nThe ﬁrst kind of difﬁculty arose because most early programs knew nothing of their\nsubject matter; they succeeded by means of simple syntactic manipulations. A typical story\noccurred in early machine translation efforts, which were generously funded by the U.S. Na-\ntional Research Council in an attempt to speed up the translation of Russian scientiﬁc papers\nin the wake of the Sputnik launch in 1957. It was thought initially that simple syntactic trans-\nformations based on the grammars of Russian and English, and word replacement from an\nelectronic dictionary, would sufﬁce to preserve the exact meanings of sentences. The fact is\nthat accurate translation requires background knowledge in order to resolve ambiguity and\nestablish the content of the sentence. The famous retranslation of “the spirit is willing but\nthe ﬂesh is weak” as “the vodka is good but the meat is rotten” illustrates the difﬁculties en-\ncountered. In 1966, a report by an advisory committee found that “there has been no machine\ntranslation of general scientiﬁc text, and none is in immediate prospect.” All U.S. government",
  "countered. In 1966, a report by an advisory committee found that “there has been no machine\ntranslation of general scientiﬁc text, and none is in immediate prospect.” All U.S. government\nfunding for academic translation projects was canceled. Today, machine translation is an im-\nperfect but widely used tool for technical, commercial, government, and Internet documents.\nThe second kind of difﬁculty was the intractability of many of the problems that AI was\nattempting to solve. Most of the early AI programs solved problems by trying out different\ncombinations of steps until the solution was found. This strategy worked initially because\nmicroworlds contained very few objects and hence very few possible actions and very short\nsolution sequences. Before the theory of computational complexity was developed, it was\nwidely thought that “scaling up” to larger problems was simply a matter of faster hardware\nand larger memories. The optimism that accompanied the development of resolution theorem\nproving, for example, was soon dampened when researchers failed to prove theorems involv-\ning more than a few dozen facts. The fact that a program can ﬁnd a solution in principle does\nnot mean that the program contains any of the mechanisms needed to ﬁnd it in practice.\nThe illusion of unlimited computational power was not conﬁned to problem-solving\nprograms. Early experiments in machine evolution (now called genetic algorithms) (Fried-\nMACHINE EVOLUTION\nGENETIC\nALGORITHM\nberg, 1958; Friedberg et al., 1959) were based on the undoubtedly correct belief that by\nmaking an appropriate series of small mutations to a machine-code program, one can gen-\nerate a program with good performance for any particular task. The idea, then, was to try\nrandom mutations with a selection process to preserve mutations that seemed useful. De-\nspite thousands of hours of CPU time, almost no progress was demonstrated. Modern genetic\nalgorithms use better representations and have shown more success. 22\nChapter\n1.\nIntroduction\nFailure to come to grips with the “combinatorial explosion” was one of the main criti-\ncisms of AI contained in the Lighthill report (Lighthill, 1973), which formed the basis for the\ndecision by the British government to end support for AI research in all but two universities.\n(Oral tradition paints a somewhat different and more colorful picture, with political ambitions\nand personal animosities whose description is beside the point.)",
  "(Oral tradition paints a somewhat different and more colorful picture, with political ambitions\nand personal animosities whose description is beside the point.)\nA third difﬁculty arose because of some fundamental limitations on the basic structures\nbeing used to generate intelligent behavior. For example, Minsky and Papert’s book Percep-\ntrons (1969) proved that, although perceptrons (a simple form of neural network) could be\nshown to learn anything they were capable of representing, they could represent very little. In\nparticular, a two-input perceptron (restricted to be simpler than the form Rosenblatt originally\nstudied) could not be trained to recognize when its two inputs were different. Although their\nresults did not apply to more complex, multilayer networks, research funding for neural-net\nresearch soon dwindled to almost nothing. Ironically, the new back-propagation learning al-\ngorithms for multilayer networks that were to cause an enormous resurgence in neural-net\nresearch in the late 1980s were actually discovered ﬁrst in 1969 (Bryson and Ho, 1969).\n1.3.5\nKnowledge-based systems: The key to power? (1969–1979)\nThe picture of problem solving that had arisen during the ﬁrst decade of AI research was of\na general-purpose search mechanism trying to string together elementary reasoning steps to\nﬁnd complete solutions. Such approaches have been called weak methods because, although\nWEAK METHOD\ngeneral, they do not scale up to large or difﬁcult problem instances. The alternative to weak\nmethods is to use more powerful, domain-speciﬁc knowledge that allows larger reasoning\nsteps and can more easily handle typically occurring cases in narrow areas of expertise. One\nmight say that to solve a hard problem, you have to almost know the answer already.\nThe DENDRAL program (Buchanan et al., 1969) was an early example of this approach.\nIt was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon),\nBruce Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobel\nlaureate geneticist) teamed up to solve the problem of inferring molecular structure from the\ninformation provided by a mass spectrometer. The input to the program consists of the ele-\nmentary formula of the molecule (e.g., C6H13NO2) and the mass spectrum giving the masses\nof the various fragments of the molecule generated when it is bombarded by an electron beam.\nFor example, the mass spectrum might contain a peak at m = 15, corresponding to the mass",
  "of the various fragments of the molecule generated when it is bombarded by an electron beam.\nFor example, the mass spectrum might contain a peak at m = 15, corresponding to the mass\nof a methyl (CH3) fragment.\nThe naive version of the program generated all possible structures consistent with the\nformula, and then predicted what mass spectrum would be observed for each, comparing this\nwith the actual spectrum. As one might expect, this is intractable for even moderate-sized\nmolecules. The DENDRAL researchers consulted analytical chemists and found that they\nworked by looking for well-known patterns of peaks in the spectrum that suggested common\nsubstructures in the molecule. For example, the following rule is used to recognize a ketone\n(C=O) subgroup (which weighs 28):\nif there are two peaks at x1 and x2 such that\n(a) x1 + x2 = M + 28 (M is the mass of the whole molecule); Section 1.3.\nThe History of Artiﬁcial Intelligence\n23\n(b) x1 −28 is a high peak;\n(c) x2 −28 is a high peak;\n(d) At least one of x1 and x2 is high.\nthen there is a ketone subgroup\nRecognizing that the molecule contains a particular substructure reduces the number of pos-\nsible candidates enormously. DENDRAL was powerful because\nAll the relevant theoretical knowledge to solve these problems has been mapped over from\nits general form in the [spectrum prediction component] (“ﬁrst principles”) to efﬁcient\nspecial forms (“cookbook recipes”). (Feigenbaum et al., 1971)\nThe signiﬁcance of DENDRAL was that it was the ﬁrst successful knowledge-intensive sys-\ntem: its expertise derived from large numbers of special-purpose rules. Later systems also\nincorporated the main theme of McCarthy’s Advice Taker approach—the clean separation of\nthe knowledge (in the form of rules) from the reasoning component.\nWith this lesson in mind, Feigenbaum and others at Stanford began the Heuristic Pro-\ngramming Project (HPP) to investigate the extent to which the new methodology of expert\nsystems could be applied to other areas of human expertise. The next major effort was in\nEXPERT SYSTEMS\nthe area of medical diagnosis. Feigenbaum, Buchanan, and Dr. Edward Shortliffe developed\nMYCIN to diagnose blood infections. With about 450 rules, MYCIN was able to perform\nas well as some experts, and considerably better than junior doctors. It also contained two\nmajor differences from DENDRAL. First, unlike the DENDRAL rules, no general theoretical",
  "as well as some experts, and considerably better than junior doctors. It also contained two\nmajor differences from DENDRAL. First, unlike the DENDRAL rules, no general theoretical\nmodel existed from which the MYCIN rules could be deduced. They had to be acquired from\nextensive interviewing of experts, who in turn acquired them from textbooks, other experts,\nand direct experience of cases. Second, the rules had to reﬂect the uncertainty associated with\nmedical knowledge. MYCIN incorporated a calculus of uncertainty called certainty factors\nCERTAINTY FACTOR\n(see Chapter 14), which seemed (at the time) to ﬁt well with how doctors assessed the impact\nof evidence on the diagnosis.\nThe importance of domain knowledge was also apparent in the area of understanding\nnatural language. Although Winograd’s SHRDLU system for understanding natural language\nhad engendered a good deal of excitement, its dependence on syntactic analysis caused some\nof the same problems as occurred in the early machine translation work. It was able to\novercome ambiguity and understand pronoun references, but this was mainly because it was\ndesigned speciﬁcally for one area—the blocks world. Several researchers, including Eugene\nCharniak, a fellow graduate student of Winograd’s at MIT, suggested that robust language\nunderstanding would require general knowledge about the world and a general method for\nusing that knowledge.\nAt Yale, linguist-turned-AI-researcher Roger Schank emphasized this point, claiming,\n“There is no such thing as syntax,” which upset a lot of linguists but did serve to start a useful\ndiscussion. Schank and his students built a series of programs (Schank and Abelson, 1977;\nWilensky, 1978; Schank and Riesbeck, 1981; Dyer, 1983) that all had the task of under-\nstanding natural language. The emphasis, however, was less on language per se and more on\nthe problems of representing and reasoning with the knowledge required for language under-\nstanding. The problems included representing stereotypical situations (Cullingford, 1981), 24\nChapter\n1.\nIntroduction\ndescribing human memory organization (Rieger, 1976; Kolodner, 1983), and understanding\nplans and goals (Wilensky, 1983).\nThe widespread growth of applications to real-world problems caused a concurrent in-\ncrease in the demands for workable knowledge representation schemes. A large number\nof different representation and reasoning languages were developed. Some were based on",
  "crease in the demands for workable knowledge representation schemes. A large number\nof different representation and reasoning languages were developed. Some were based on\nlogic—for example, the Prolog language became popular in Europe, and the PLANNER fam-\nily in the United States. Others, following Minsky’s idea of frames (1975), adopted a more\nFRAMES\nstructured approach, assembling facts about particular object and event types and arranging\nthe types into a large taxonomic hierarchy analogous to a biological taxonomy.\n1.3.6\nAI becomes an industry (1980–present)\nThe ﬁrst successful commercial expert system, R1, began operation at the Digital Equipment\nCorporation (McDermott, 1982). The program helped conﬁgure orders for new computer\nsystems; by 1986, it was saving the company an estimated $40 million a year. By 1988,\nDEC’s AI group had 40 expert systems deployed, with more on the way. DuPont had 100 in\nuse and 500 in development, saving an estimated $10 million a year. Nearly every major U.S.\ncorporation had its own AI group and was either using or investigating expert systems.\nIn 1981, the Japanese announced the “Fifth Generation” project, a 10-year plan to build\nintelligent computers running Prolog. In response, the United States formed the Microelec-\ntronics and Computer Technology Corporation (MCC) as a research consortium designed to\nassure national competitiveness. In both cases, AI was part of a broad effort, including chip\ndesign and human-interface research. In Britain, the Alvey report reinstated the funding that\nwas cut by the Lighthill report.13 In all three countries, however, the projects never met their\nambitious goals.\nOverall, the AI industry boomed from a few million dollars in 1980 to billions of dollars\nin 1988, including hundreds of companies building expert systems, vision systems, robots,\nand software and hardware specialized for these purposes. Soon after that came a period\ncalled the “AI Winter,” in which many companies fell by the wayside as they failed to deliver\non extravagant promises.\n1.3.7\nThe return of neural networks (1986–present)\nIn the mid-1980s at least four different groups reinvented the back-propagation learning\nBACK-PROPAGATION\nalgorithm ﬁrst found in 1969 by Bryson and Ho. The algorithm was applied to many learn-\ning problems in computer science and psychology, and the widespread dissemination of the\nresults in the collection Parallel Distributed Processing (Rumelhart and McClelland, 1986)",
  "ing problems in computer science and psychology, and the widespread dissemination of the\nresults in the collection Parallel Distributed Processing (Rumelhart and McClelland, 1986)\ncaused great excitement.\nThese so-called connectionist models of intelligent systems were seen by some as di-\nCONNECTIONIST\nrect competitors both to the symbolic models promoted by Newell and Simon and to the\nlogicist approach of McCarthy and others (Smolensky, 1988). It might seem obvious that\nat some level humans manipulate symbols—in fact, Terrence Deacon’s book The Symbolic\n13 To save embarrassment, a new ﬁeld called IKBS (Intelligent Knowledge-Based Systems) was invented because\nArtiﬁcial Intelligence had been ofﬁcially canceled. Section 1.3.\nThe History of Artiﬁcial Intelligence\n25\nSpecies (1997) suggests that this is the deﬁning characteristic of humans—but the most ar-\ndent connectionists questioned whether symbol manipulation had any real explanatory role in\ndetailed models of cognition. This question remains unanswered, but the current view is that\nconnectionist and symbolic approaches are complementary, not competing. As occurred with\nthe separation of AI and cognitive science, modern neural network research has bifurcated\ninto two ﬁelds, one concerned with creating effective network architectures and algorithms\nand understanding their mathematical properties, the other concerned with careful modeling\nof the empirical properties of actual neurons and ensembles of neurons.\n1.3.8\nAI adopts the scientiﬁc method (1987–present)\nRecent years have seen a revolution in both the content and the methodology of work in\nartiﬁcial intelligence.14 It is now more common to build on existing theories than to propose\nbrand-new ones, to base claims on rigorous theorems or hard experimental evidence rather\nthan on intuition, and to show relevance to real-world applications rather than toy examples.\nAI was founded in part as a rebellion against the limitations of existing ﬁelds like control\ntheory and statistics, but now it is embracing those ﬁelds. As David McAllester (1998) put it:\nIn the early period of AI it seemed plausible that new forms of symbolic computation,\ne.g., frames and semantic networks, made much of classical theory obsolete. This led to\na form of isolationism in which AI became largely separated from the rest of computer\nscience. This isolationism is currently being abandoned. There is a recognition that",
  "a form of isolationism in which AI became largely separated from the rest of computer\nscience. This isolationism is currently being abandoned. There is a recognition that\nmachine learning should not be isolated from information theory, that uncertain reasoning\nshould not be isolated from stochastic modeling, that search should not be isolated from\nclassical optimization and control, and that automated reasoning should not be isolated\nfrom formal methods and static analysis.\nIn terms of methodology, AI has ﬁnally come ﬁrmly under the scientiﬁc method. To be ac-\ncepted, hypotheses must be subjected to rigorous empirical experiments, and the results must\nbe analyzed statistically for their importance (Cohen, 1995). It is now possible to replicate\nexperiments by using shared repositories of test data and code.\nThe ﬁeld of speech recognition illustrates the pattern. In the 1970s, a wide variety of\ndifferent architectures and approaches were tried. Many of these were rather ad hoc and\nfragile, and were demonstrated on only a few specially selected examples. In recent years,\napproaches based on hidden Markov models (HMMs) have come to dominate the area. Two\nHIDDEN MARKOV\nMODELS\naspects of HMMs are relevant. First, they are based on a rigorous mathematical theory. This\nhas allowed speech researchers to build on several decades of mathematical results developed\nin other ﬁelds. Second, they are generated by a process of training on a large corpus of\nreal speech data. This ensures that the performance is robust, and in rigorous blind tests the\nHMMs have been improving their scores steadily. Speech technology and the related ﬁeld of\nhandwritten character recognition are already making the transition to widespread industrial\n14 Some have characterized this change as a victory of the neats—those who think that AI theories should be\ngrounded in mathematical rigor—over the scrufﬁes—those who would rather try out lots of ideas, write some\nprograms, and then assess what seems to be working. Both approaches are important. A shift toward neatness\nimplies that the ﬁeld has reached a level of stability and maturity. Whether that stability will be disrupted by a\nnew scruffy idea is another question. 26\nChapter\n1.\nIntroduction\nand consumer applications. Note that there is no scientiﬁc claim that humans use HMMs to\nrecognize speech; rather, HMMs provide a mathematical framework for understanding the\nproblem and support the engineering claim that they work well in practice.",
  "and consumer applications. Note that there is no scientiﬁc claim that humans use HMMs to\nrecognize speech; rather, HMMs provide a mathematical framework for understanding the\nproblem and support the engineering claim that they work well in practice.\nMachine translation follows the same course as speech recognition. In the 1950s there\nwas initial enthusiasm for an approach based on sequences of words, with models learned\naccording to the principles of information theory. That approach fell out of favor in the\n1960s, but returned in the late 1990s and now dominates the ﬁeld.\nNeural networks also ﬁt this trend. Much of the work on neural nets in the 1980s was\ndone in an attempt to scope out what could be done and to learn how neural nets differ from\n“traditional” techniques. Using improved methodology and theoretical frameworks, the ﬁeld\narrived at an understanding in which neural nets can now be compared with corresponding\ntechniques from statistics, pattern recognition, and machine learning, and the most promising\ntechnique can be applied to each application. As a result of these developments, so-called\ndata mining technology has spawned a vigorous new industry.\nDATA MINING\nJudea Pearl’s (1988) Probabilistic Reasoning in Intelligent Systems led to a new accep-\ntance of probability and decision theory in AI, following a resurgence of interest epitomized\nby Peter Cheeseman’s (1985) article “In Defense of Probability.” The Bayesian network\nBAYESIAN NETWORK\nformalism was invented to allow efﬁcient representation of, and rigorous reasoning with,\nuncertain knowledge. This approach largely overcomes many problems of the probabilistic\nreasoning systems of the 1960s and 1970s; it now dominates AI research on uncertain reason-\ning and expert systems. The approach allows for learning from experience, and it combines\nthe best of classical AI and neural nets. Work by Judea Pearl (1982a) and by Eric Horvitz and\nDavid Heckerman (Horvitz and Heckerman, 1986; Horvitz et al., 1986) promoted the idea of\nnormative expert systems: ones that act rationally according to the laws of decision theory\nand do not try to imitate the thought steps of human experts. The WindowsTM operating sys-\ntem includes several normative diagnostic expert systems for correcting problems. Chapters\n13 to 16 cover this area.\nSimilar gentle revolutions have occurred in robotics, computer vision, and knowledge\nrepresentation. A better understanding of the problems and their complexity properties, com-",
  "13 to 16 cover this area.\nSimilar gentle revolutions have occurred in robotics, computer vision, and knowledge\nrepresentation. A better understanding of the problems and their complexity properties, com-\nbined with increased mathematical sophistication, has led to workable research agendas and\nrobust methods. Although increased formalization and specialization led ﬁelds such as vision\nand robotics to become somewhat isolated from “mainstream” AI in the 1990s, this trend has\nreversed in recent years as tools from machine learning in particular have proved effective for\nmany problems. The process of reintegration is already yielding signiﬁcant beneﬁts\n1.3.9\nThe emergence of intelligent agents (1995–present)\nPerhaps encouraged by the progress in solving the subproblems of AI, researchers have also\nstarted to look at the “whole agent” problem again. The work of Allen Newell, John Laird,\nand Paul Rosenbloom on SOAR (Newell, 1990; Laird et al., 1987) is the best-known example\nof a complete agent architecture. One of the most important environments for intelligent\nagents is the Internet. AI systems have become so common in Web-based applications that\nthe “-bot” sufﬁx has entered everyday language. Moreover, AI technologies underlie many Section 1.3.\nThe History of Artiﬁcial Intelligence\n27\nInternet tools, such as search engines, recommender systems, and Web site aggregators.\nOne consequence of trying to build complete agents is the realization that the previously\nisolated subﬁelds of AI might need to be reorganized somewhat when their results are to be\ntied together. In particular, it is now widely appreciated that sensory systems (vision, sonar,\nspeech recognition, etc.) cannot deliver perfectly reliable information about the environment.\nHence, reasoning and planning systems must be able to handle uncertainty. A second major\nconsequence of the agent perspective is that AI has been drawn into much closer contact\nwith other ﬁelds, such as control theory and economics, that also deal with agents. Recent\nprogress in the control of robotic cars has derived from a mixture of approaches ranging from\nbetter sensors, control-theoretic integration of sensing, localization and mapping, as well as\na degree of high-level planning.\nDespite these successes, some inﬂuential founders of AI, including John McCarthy\n(2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005) and Patrick Winston (Beal and",
  "a degree of high-level planning.\nDespite these successes, some inﬂuential founders of AI, including John McCarthy\n(2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005) and Patrick Winston (Beal and\nWinston, 2009), have expressed discontent with the progress of AI. They think that AI should\nput less emphasis on creating ever-improved versions of applications that are good at a spe-\nciﬁc task, such as driving a car, playing chess, or recognizing speech. Instead, they believe\nAI should return to its roots of striving for, in Simon’s words, “machines that think, that learn\nand that create.” They call the effort human-level AI or HLAI; their ﬁrst symposium was in\nHUMAN-LEVEL AI\n2004 (Minsky et al., 2004). The effort will require very large knowledge bases; Hendler et al.\n(1995) discuss where these knowledge bases might come from.\nA related idea is the subﬁeld of Artiﬁcial General Intelligence or AGI (Goertzel and\nARTIFICIAL GENERAL\nINTELLIGENCE\nPennachin, 2007), which held its ﬁrst conference and organized the Journal of Artiﬁcial Gen-\neral Intelligence in 2008. AGI looks for a universal algorithm for learning and acting in\nany environment, and has its roots in the work of Ray Solomonoff (1964), one of the atten-\ndees of the original 1956 Dartmouth conference. Guaranteeing that what we create is really\nFriendly AI is also a concern (Yudkowsky, 2008; Omohundro, 2008), one we will return to\nFRIENDLY AI\nin Chapter 26.\n1.3.10\nThe availability of very large data sets (2001–present)\nThroughout the 60-year history of computer science, the emphasis has been on the algorithm\nas the main subject of study. But some recent work in AI suggests that for many problems, it\nmakes more sense to worry about the data and be less picky about what algorithm to apply.\nThis is true because of the increasing availability of very large data sources: for example,\ntrillions of words of English and billions of images from the Web (Kilgarriff and Grefenstette,\n2006); or billions of base pairs of genomic sequences (Collins et al., 2003).\nOne inﬂuential paper in this line was Yarowsky’s (1995) work on word-sense disam-\nbiguation: given the use of the word “plant” in a sentence, does that refer to ﬂora or factory?\nPrevious approaches to the problem had relied on human-labeled examples combined with\nmachine learning algorithms. Yarowsky showed that the task can be done, with accuracy\nabove 96%, with no labeled examples at all. Instead, given a very large corpus of unanno-",
  "machine learning algorithms. Yarowsky showed that the task can be done, with accuracy\nabove 96%, with no labeled examples at all. Instead, given a very large corpus of unanno-\ntated text and just the dictionary deﬁnitions of the two senses—“works, industrial plant” and\n“ﬂora, plant life”—one can label examples in the corpus, and from there bootstrap to learn 28\nChapter\n1.\nIntroduction\nnew patterns that help label new examples. Banko and Brill (2001) show that techniques\nlike this perform even better as the amount of available text goes from a million words to a\nbillion and that the increase in performance from using more data exceeds any difference in\nalgorithm choice; a mediocre algorithm with 100 million words of unlabeled training data\noutperforms the best known algorithm with 1 million words.\nAs another example, Hays and Efros (2007) discuss the problem of ﬁlling in holes in a\nphotograph. Suppose you use Photoshop to mask out an ex-friend from a group photo, but\nnow you need to ﬁll in the masked area with something that matches the background. Hays\nand Efros deﬁned an algorithm that searches through a collection of photos to ﬁnd something\nthat will match. They found the performance of their algorithm was poor when they used\na collection of only ten thousand photos, but crossed a threshold into excellent performance\nwhen they grew the collection to two million photos.\nWork like this suggests that the “knowledge bottleneck” in AI—the problem of how to\nexpress all the knowledge that a system needs—may be solved in many applications by learn-\ning methods rather than hand-coded knowledge engineering, provided the learning algorithms\nhave enough data to go on (Halevy et al., 2009). Reporters have noticed the surge of new ap-\nplications and have written that “AI Winter” may be yielding to a new Spring (Havenstein,\n2005). As Kurzweil (2005) writes, “today, many thousands of AI applications are deeply\nembedded in the infrastructure of every industry.”\n1.4\nTHE STATE OF THE ART\nWhat can AI do today? A concise answer is difﬁcult because there are so many activities in\nso many subﬁelds. Here we sample a few applications; others appear throughout the book.\nRobotic vehicles: A driverless robotic car named STANLEY sped through the rough\nterrain of the Mojave dessert at 22 mph, ﬁnishing the 132-mile course ﬁrst to win the 2005\nDARPA Grand Challenge. STANLEY is a Volkswagen Touareg outﬁtted with cameras, radar,",
  "terrain of the Mojave dessert at 22 mph, ﬁnishing the 132-mile course ﬁrst to win the 2005\nDARPA Grand Challenge. STANLEY is a Volkswagen Touareg outﬁtted with cameras, radar,\nand laser rangeﬁnders to sense the environment and onboard software to command the steer-\ning, braking, and acceleration (Thrun, 2006). The following year CMU’s BOSS won the Ur-\nban Challenge, safely driving in trafﬁc through the streets of a closed Air Force base, obeying\ntrafﬁc rules and avoiding pedestrians and other vehicles.\nSpeech recognition: A traveler calling United Airlines to book a ﬂight can have the en-\ntire conversation guided by an automated speech recognition and dialog management system.\nAutonomous planning and scheduling: A hundred million miles from Earth, NASA’s\nRemote Agent program became the ﬁrst on-board autonomous planning program to control\nthe scheduling of operations for a spacecraft (Jonsson et al., 2000). REMOTE AGENT gen-\nerated plans from high-level goals speciﬁed from the ground and monitored the execution of\nthose plans—detecting, diagnosing, and recovering from problems as they occurred. Succes-\nsor program MAPGEN (Al-Chang et al., 2004) plans the daily operations for NASA’s Mars\nExploration Rovers, and MEXAR2 (Cesta et al., 2007) did mission planning—both logistics\nand science planning—for the European Space Agency’s Mars Express mission in 2008. Section 1.5.\nSummary\n29\nGame playing: IBM’s DEEP BLUE became the ﬁrst computer program to defeat the\nworld champion in a chess match when it bested Garry Kasparov by a score of 3.5 to 2.5 in\nan exhibition match (Goodman and Keene, 1997). Kasparov said that he felt a “new kind of\nintelligence” across the board from him. Newsweek magazine described the match as “The\nbrain’s last stand.” The value of IBM’s stock increased by $18 billion. Human champions\nstudied Kasparov’s loss and were able to draw a few matches in subsequent years, but the\nmost recent human-computer matches have been won convincingly by the computer.\nSpam ﬁghting: Each day, learning algorithms classify over a billion messages as spam,\nsaving the recipient from having to waste time deleting what, for many users, could comprise\n80% or 90% of all messages, if not classiﬁed away by algorithms. Because the spammers are\ncontinually updating their tactics, it is difﬁcult for a static programmed approach to keep up,\nand learning algorithms work best (Sahami et al., 1998; Goodman and Heckerman, 2004).",
  "continually updating their tactics, it is difﬁcult for a static programmed approach to keep up,\nand learning algorithms work best (Sahami et al., 1998; Goodman and Heckerman, 2004).\nLogistics planning: During the Persian Gulf crisis of 1991, U.S. forces deployed a\nDynamic Analysis and Replanning Tool, DART (Cross and Walker, 1994), to do automated\nlogistics planning and scheduling for transportation. This involved up to 50,000 vehicles,\ncargo, and people at a time, and had to account for starting points, destinations, routes, and\nconﬂict resolution among all parameters. The AI planning techniques generated in hours\na plan that would have taken weeks with older methods. The Defense Advanced Research\nProject Agency (DARPA) stated that this single application more than paid back DARPA’s\n30-year investment in AI.\nRobotics: The iRobot Corporation has sold over two million Roomba robotic vacuum\ncleaners for home use. The company also deploys the more rugged PackBot to Iraq and\nAfghanistan, where it is used to handle hazardous materials, clear explosives, and identify\nthe location of snipers.\nMachine Translation: A computer program automatically translates from Arabic to\nEnglish, allowing an English speaker to see the headline “Ardogan Conﬁrms That Turkey\nWould Not Accept Any Pressure, Urging Them to Recognize Cyprus.” The program uses a\nstatistical model built from examples of Arabic-to-English translations and from examples of\nEnglish text totaling two trillion words (Brants et al., 2007). None of the computer scientists\non the team speak Arabic, but they do understand statistics and machine learning algorithms.\nThese are just a few examples of artiﬁcial intelligence systems that exist today. Not\nmagic or science ﬁction—but rather science, engineering, and mathematics, to which this\nbook provides an introduction.\n1.5\nSUMMARY\nThis chapter deﬁnes AI and establishes the cultural background against which it has devel-\noped. Some of the important points are as follows:\n• Different people approach AI with different goals in mind. Two important questions to\nask are: Are you concerned with thinking or behavior? Do you want to model humans\nor work from an ideal standard? 30\nChapter\n1.\nIntroduction\n• In this book, we adopt the view that intelligence is concerned mainly with rational\naction. Ideally, an intelligent agent takes the best possible action in a situation. We\nstudy the problem of building agents that are intelligent in this sense.",
  "• In this book, we adopt the view that intelligence is concerned mainly with rational\naction. Ideally, an intelligent agent takes the best possible action in a situation. We\nstudy the problem of building agents that are intelligent in this sense.\n• Philosophers (going back to 400 B.C.) made AI conceivable by considering the ideas\nthat the mind is in some ways like a machine, that it operates on knowledge encoded in\nsome internal language, and that thought can be used to choose what actions to take.\n• Mathematicians provided the tools to manipulate statements of logical certainty as well\nas uncertain, probabilistic statements. They also set the groundwork for understanding\ncomputation and reasoning about algorithms.\n• Economists formalized the problem of making decisions that maximize the expected\noutcome to the decision maker.\n• Neuroscientists discovered some facts about how the brain works and the ways in which\nit is similar to and different from computers.\n• Psychologists adopted the idea that humans and animals can be considered information-\nprocessing machines. Linguists showed that language use ﬁts into this model.\n• Computer engineers provided the ever-more-powerful machines that make AI applica-\ntions possible.\n• Control theory deals with designing devices that act optimally on the basis of feedback\nfrom the environment. Initially, the mathematical tools of control theory were quite\ndifferent from AI, but the ﬁelds are coming closer together.\n• The history of AI has had cycles of success, misplaced optimism, and resulting cutbacks\nin enthusiasm and funding. There have also been cycles of introducing new creative\napproaches and systematically reﬁning the best ones.\n• AI has advanced more rapidly in the past decade because of greater use of the scientiﬁc\nmethod in experimenting with and comparing approaches.\n• Recent progress in understanding the theoretical basis for intelligence has gone hand in\nhand with improvements in the capabilities of real systems. The subﬁelds of AI have\nbecome more integrated, and AI has found common ground with other disciplines.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe methodological status of artiﬁcial intelligence is investigated in The Sciences of the Artiﬁ-\ncial, by Herb Simon (1981), which discusses research areas concerned with complex artifacts.\nIt explains how AI can be viewed as both science and mathematics. Cohen (1995) gives an\noverview of experimental methodology within AI.",
  "cial, by Herb Simon (1981), which discusses research areas concerned with complex artifacts.\nIt explains how AI can be viewed as both science and mathematics. Cohen (1995) gives an\noverview of experimental methodology within AI.\nThe Turing Test (Turing, 1950) is discussed by Shieber (1994), who severely criticizes\nthe usefulness of its instantiation in the Loebner Prize competition, and by Ford and Hayes\n(1995), who argue that the test itself is not helpful for AI. Bringsjord (2008) gives advice for\na Turing Test judge. Shieber (2004) and Epstein et al. (2008) collect a number of essays on\nthe Turing Test. Artiﬁcial Intelligence: The Very Idea, by John Haugeland (1985), gives a Exercises\n31\nreadable account of the philosophical and practical problems of AI. Signiﬁcant early papers\nin AI are anthologized in the collections by Webber and Nilsson (1981) and by Luger (1995).\nThe Encyclopedia of AI (Shapiro, 1992) contains survey articles on almost every topic in\nAI, as does Wikipedia. These articles usually provide a good entry point into the research\nliterature on each topic. An insightful and comprehensive history of AI is given by Nils\nNillson (2009), one of the early pioneers of the ﬁeld.\nThe most recent work appears in the proceedings of the major AI conferences: the bi-\nennial International Joint Conference on AI (IJCAI), the annual European Conference on AI\n(ECAI), and the National Conference on AI, more often known as AAAI, after its sponsoring\norganization. The major journals for general AI are Artiﬁcial Intelligence, Computational\nIntelligence, the IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE In-\ntelligent Systems, and the electronic Journal of Artiﬁcial Intelligence Research. There are also\nmany conferences and journals devoted to speciﬁc areas, which we cover in the appropriate\nchapters. The main professional societies for AI are the American Association for Artiﬁcial\nIntelligence (AAAI), the ACM Special Interest Group in Artiﬁcial Intelligence (SIGART),\nand the Society for Artiﬁcial Intelligence and Simulation of Behaviour (AISB). AAAI’s AI\nMagazine contains many topical and tutorial articles, and its Web site, aaai.org, contains\nnews, tutorials, and background information.\nEXERCISES\nThese exercises are intended to stimulate discussion, and some might be set as term projects.\nAlternatively, preliminary attempts can be made now, and these attempts can be reviewed\nafter the completion of the book.\n1.1",
  "EXERCISES\nThese exercises are intended to stimulate discussion, and some might be set as term projects.\nAlternatively, preliminary attempts can be made now, and these attempts can be reviewed\nafter the completion of the book.\n1.1\nDeﬁne in your own words: (a) intelligence, (b) artiﬁcial intelligence, (c) agent, (d)\nrationality, (e) logical reasoning.\n1.2\nRead Turing’s original paper on AI (Turing, 1950). In the paper, he discusses several\nobjections to his proposed enterprise and his test for intelligence. Which objections still carry\nweight? Are his refutations valid? Can you think of new objections arising from develop-\nments since he wrote the paper? In the paper, he predicts that, by the year 2000, a computer\nwill have a 30% chance of passing a ﬁve-minute Turing Test with an unskilled interrogator.\nWhat chance do you think a computer would have today? In another 50 years?\n1.3\nAre reﬂex actions (such as ﬂinching from a hot stove) rational? Are they intelligent?\n1.4\nSuppose we extend Evans’s ANALOGY program so that it can score 200 on a standard\nIQ test. Would we then have a program more intelligent than a human? Explain.\n1.5\nThe neural structure of the sea slug Aplysia has been widely studied (ﬁrst by Nobel\nLaureate Eric Kandel) because it has only about 20,000 neurons, most of them large and\neasily manipulated. Assuming that the cycle time for an Aplysia neuron is roughly the same\nas for a human neuron, how does the computational power, in terms of memory updates per\nsecond, compare with the high-end computer described in Figure 1.3? 32\nChapter\n1.\nIntroduction\n1.6\nHow could introspection—reporting on one’s inner thoughts—be inaccurate? Could I\nbe wrong about what I’m thinking? Discuss.\n1.7\nTo what extent are the following computer systems instances of artiﬁcial intelligence:\n• Supermarket bar code scanners.\n• Web search engines.\n• Voice-activated telephone menus.\n• Internet routing algorithms that respond dynamically to the state of the network.\n1.8\nMany of the computational models of cognitive activities that have been proposed in-\nvolve quite complex mathematical operations, such as convolving an image with a Gaussian\nor ﬁnding a minimum of the entropy function. Most humans (and certainly all animals) never\nlearn this kind of mathematics at all, almost no one learns it before college, and almost no\none can compute the convolution of a function with a Gaussian in their head. What sense",
  "learn this kind of mathematics at all, almost no one learns it before college, and almost no\none can compute the convolution of a function with a Gaussian in their head. What sense\ndoes it make to say that the “vision system” is doing this kind of mathematics, whereas the\nactual person has no idea how to do it?\n1.9\nWhy would evolution tend to result in systems that act rationally? What goals are such\nsystems designed to achieve?\n1.10\nIs AI a science, or is it engineering? Or neither or both? Explain.\n1.11\n“Surely computers cannot be intelligent—they can do only what their programmers\ntell them.” Is the latter statement true, and does it imply the former?\n1.12\n“Surely animals cannot be intelligent—they can do only what their genes tell them.”\nIs the latter statement true, and does it imply the former?\n1.13\n“Surely animals, humans, and computers cannot be intelligent—they can do only what\ntheir constituent atoms are told to do by the laws of physics.” Is the latter statement true, and\ndoes it imply the former?\n1.14\nExamine the AI literature to discover whether the following tasks can currently be\nsolved by computers:\na. Playing a decent game of table tennis (Ping-Pong).\nb. Driving in the center of Cairo, Egypt.\nc. Driving in Victorville, California.\nd. Buying a week’s worth of groceries at the market.\ne. Buying a week’s worth of groceries on the Web.\nf. Playing a decent game of bridge at a competitive level.\ng. Discovering and proving new mathematical theorems.\nh. Writing an intentionally funny story.\ni. Giving competent legal advice in a specialized area of law.\nj. Translating spoken English into spoken Swedish in real time.\nk. Performing a complex surgical operation. Exercises\n33\nFor the currently infeasible tasks, try to ﬁnd out what the difﬁculties are and predict when, if\never, they will be overcome.\n1.15\nVarious subﬁelds of AI have held contests by deﬁning a standard task and inviting re-\nsearchers to do their best. Examples include the DARPA Grand Challenge for robotic cars,\nThe International Planning Competition, the Robocup robotic soccer league, the TREC infor-\nmation retrieval event, and contests in machine translation, speech recognition. Investigate\nﬁve of these contests, and describe the progress made over the years. To what degree have the\ncontests advanced toe state of the art in AI? Do what degree do they hurt the ﬁeld by drawing\nenergy away from new ideas? 2\nINTELLIGENT AGENTS",
  "ﬁve of these contests, and describe the progress made over the years. To what degree have the\ncontests advanced toe state of the art in AI? Do what degree do they hurt the ﬁeld by drawing\nenergy away from new ideas? 2\nINTELLIGENT AGENTS\nIn which we discuss the nature of agents, perfect or otherwise, the diversity of\nenvironments, and the resulting menagerie of agent types.\nChapter 1 identiﬁed the concept of rational agents as central to our approach to artiﬁcial\nintelligence. In this chapter, we make this notion more concrete. We will see that the concept\nof rationality can be applied to a wide variety of agents operating in any imaginable environ-\nment. Our plan in this book is to use this concept to develop a small set of design principles\nfor building successful agents—systems that can reasonably be called intelligent.\nWe begin by examining agents, environments, and the coupling between them. The\nobservation that some agents behave better than others leads naturally to the idea of a rational\nagent—one that behaves as well as possible. How well an agent can behave depends on\nthe nature of the environment; some environments are more difﬁcult than others. We give a\ncrude categorization of environments and show how properties of an environment inﬂuence\nthe design of suitable agents for that environment. We describe a number of basic “skeleton”\nagent designs, which we ﬂesh out in the rest of the book.\n2.1\nAGENTS AND ENVIRONMENTS\nAn agent is anything that can be viewed as perceiving its environment through sensors and\nENVIRONMENT\nSENSOR\nacting upon that environment through actuators. This simple idea is illustrated in Figure 2.1.\nACTUATOR\nA human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract, and so\non for actuators. A robotic agent might have cameras and infrared range ﬁnders for sensors\nand various motors for actuators. A software agent receives keystrokes, ﬁle contents, and\nnetwork packets as sensory inputs and acts on the environment by displaying on the screen,\nwriting ﬁles, and sending network packets.\nWe use the term percept to refer to the agent’s perceptual inputs at any given instant. An\nPERCEPT\nagent’s percept sequence is the complete history of everything the agent has ever perceived.\nPERCEPT SEQUENCE\nIn general, an agent’s choice of action at any given instant can depend on the entire percept\nsequence observed to date, but not on anything it hasn’t perceived. By specifying the agent’s",
  "PERCEPT SEQUENCE\nIn general, an agent’s choice of action at any given instant can depend on the entire percept\nsequence observed to date, but not on anything it hasn’t perceived. By specifying the agent’s\nchoice of action for every possible percept sequence, we have said more or less everything\n34 Section 2.1.\nAgents and Environments\n35\nAgent\nSensors\nActuators\nEnvironment\nPercepts\nActions\n?\nFigure 2.1\nAgents interact with environments through sensors and actuators.\nthere is to say about the agent. Mathematically speaking, we say that an agent’s behavior is\ndescribed by the agent function that maps any given percept sequence to an action.\nAGENT FUNCTION\nWe can imagine tabulating the agent function that describes any given agent; for most\nagents, this would be a very large table—inﬁnite, in fact, unless we place a bound on the\nlength of percept sequences we want to consider. Given an agent to experiment with, we can,\nin principle, construct this table by trying out all possible percept sequences and recording\nwhich actions the agent does in response.1 The table is, of course, an external characterization\nof the agent. Internally, the agent function for an artiﬁcial agent will be implemented by an\nagent program. It is important to keep these two ideas distinct. The agent function is an\nAGENT PROGRAM\nabstract mathematical description; the agent program is a concrete implementation, running\nwithin some physical system.\nTo illustrate these ideas, we use a very simple example—the vacuum-cleaner world\nshown in Figure 2.2. This world is so simple that we can describe everything that happens;\nit’s also a made-up world, so we can invent many variations. This particular world has just two\nlocations: squares A and B. The vacuum agent perceives which square it is in and whether\nthere is dirt in the square. It can choose to move left, move right, suck up the dirt, or do\nnothing. One very simple agent function is the following: if the current square is dirty, then\nsuck; otherwise, move to the other square. A partial tabulation of this agent function is shown\nin Figure 2.3 and an agent program that implements it appears in Figure 2.8 on page 48.\nLooking at Figure 2.3, we see that various vacuum-world agents can be deﬁned simply\nby ﬁlling in the right-hand column in various ways. The obvious question, then, is this: What\nis the right way to ﬁll out the table? In other words, what makes an agent good or bad,",
  "by ﬁlling in the right-hand column in various ways. The obvious question, then, is this: What\nis the right way to ﬁll out the table? In other words, what makes an agent good or bad,\nintelligent or stupid? We answer these questions in the next section.\n1 If the agent uses some randomization to choose its actions, then we would have to try each sequence many\ntimes to identify the probability of each action. One might imagine that acting randomly is rather silly, but we\nshow later in this chapter that it can be very intelligent. 36\nChapter\n2.\nIntelligent Agents\nA\nB\nFigure 2.2\nA vacuum-cleaner world with just two locations.\nPercept sequence\nAction\n[A, Clean]\nRight\n[A, Dirty]\nSuck\n[B, Clean]\nLeft\n[B, Dirty]\nSuck\n[A, Clean], [A, Clean]\nRight\n[A, Clean], [A, Dirty]\nSuck\n...\n...\n[A, Clean], [A, Clean], [A, Clean]\nRight\n[A, Clean], [A, Clean], [A, Dirty]\nSuck\n...\n...\nFigure 2.3\nPartial tabulation of a simple agent function for the vacuum-cleaner world\nshown in Figure 2.2.\nBefore closing this section, we should emphasize that the notion of an agent is meant to\nbe a tool for analyzing systems, not an absolute characterization that divides the world into\nagents and non-agents. One could view a hand-held calculator as an agent that chooses the\naction of displaying “4” when given the percept sequence “2 + 2 =,” but such an analysis\nwould hardly aid our understanding of the calculator. In a sense, all areas of engineering can\nbe seen as designing artifacts that interact with the world; AI operates at (what the authors\nconsider to be) the most interesting end of the spectrum, where the artifacts have signiﬁcant\ncomputational resources and the task environment requires nontrivial decision making.\n2.2\nGOOD BEHAVIOR: THE CONCEPT OF RATIONALITY\nA rational agent is one that does the right thing—conceptually speaking, every entry in the\nRATIONAL AGENT\ntable for the agent function is ﬁlled out correctly. Obviously, doing the right thing is better\nthan doing the wrong thing, but what does it mean to do the right thing? Section 2.2.\nGood Behavior: The Concept of Rationality\n37\nWe answer this age-old question in an age-old way: by considering the consequences\nof the agent’s behavior. When an agent is plunked down in an environment, it generates a\nsequence of actions according to the percepts it receives. This sequence of actions causes the\nenvironment to go through a sequence of states. If the sequence is desirable, then the agent",
  "sequence of actions according to the percepts it receives. This sequence of actions causes the\nenvironment to go through a sequence of states. If the sequence is desirable, then the agent\nhas performed well. This notion of desirability is captured by a performance measure that\nPERFORMANCE\nMEASURE\nevaluates any given sequence of environment states.\nNotice that we said environment states, not agent states. If we deﬁne success in terms\nof agent’s opinion of its own performance, an agent could achieve perfect rationality simply\nby deluding itself that its performance was perfect. Human agents in particular are notorious\nfor “sour grapes”—believing they did not really want something (e.g., a Nobel Prize) after\nnot getting it.\nObviously, there is not one ﬁxed performance measure for all tasks and agents; typically,\na designer will devise one appropriate to the circumstances. This is not as easy as it sounds.\nConsider, for example, the vacuum-cleaner agent from the preceding section. We might\npropose to measure performance by the amount of dirt cleaned up in a single eight-hour shift.\nWith a rational agent, of course, what you ask for is what you get. A rational agent can\nmaximize this performance measure by cleaning up the dirt, then dumping it all on the ﬂoor,\nthen cleaning it up again, and so on. A more suitable performance measure would reward the\nagent for having a clean ﬂoor. For example, one point could be awarded for each clean square\nat each time step (perhaps with a penalty for electricity consumed and noise generated). As\na general rule, it is better to design performance measures according to what one actually\nwants in the environment, rather than according to how one thinks the agent should behave.\nEven when the obvious pitfalls are avoided, there remain some knotty issues to untangle.\nFor example, the notion of “clean ﬂoor” in the preceding paragraph is based on average\ncleanliness over time. Yet the same average cleanliness can be achieved by two different\nagents, one of which does a mediocre job all the time while the other cleans energetically but\ntakes long breaks. Which is preferable might seem to be a ﬁne point of janitorial science, but\nin fact it is a deep philosophical question with far-reaching implications. Which is better—\na reckless life of highs and lows, or a safe but humdrum existence? Which is better—an\neconomy where everyone lives in moderate poverty, or one in which some live in plenty",
  "a reckless life of highs and lows, or a safe but humdrum existence? Which is better—an\neconomy where everyone lives in moderate poverty, or one in which some live in plenty\nwhile others are very poor? We leave these questions as an exercise for the diligent reader.\n2.2.1\nRationality\nWhat is rational at any given time depends on four things:\n• The performance measure that deﬁnes the criterion of success.\n• The agent’s prior knowledge of the environment.\n• The actions that the agent can perform.\n• The agent’s percept sequence to date.\nThis leads to a deﬁnition of a rational agent:\nDEFINITION OF A\nRATIONAL AGENT\nFor each possible percept sequence, a rational agent should select an action that is ex-\npected to maximize its performance measure, given the evidence provided by the percept\nsequence and whatever built-in knowledge the agent has. 38\nChapter\n2.\nIntelligent Agents\nConsider the simple vacuum-cleaner agent that cleans a square if it is dirty and moves to the\nother square if not; this is the agent function tabulated in Figure 2.3. Is this a rational agent?\nThat depends! First, we need to say what the performance measure is, what is known about\nthe environment, and what sensors and actuators the agent has. Let us assume the following:\n• The performance measure awards one point for each clean square at each time step,\nover a “lifetime” of 1000 time steps.\n• The “geography” of the environment is known a priori (Figure 2.2) but the dirt distri-\nbution and the initial location of the agent are not. Clean squares stay clean and sucking\ncleans the current square. The Left and Right actions move the agent left and right\nexcept when this would take the agent outside the environment, in which case the agent\nremains where it is.\n• The only available actions are Left, Right, and Suck.\n• The agent correctly perceives its location and whether that location contains dirt.\nWe claim that under these circumstances the agent is indeed rational; its expected perfor-\nmance is at least as high as any other agent’s. Exercise 2.2 asks you to prove this.\nOne can see easily that the same agent would be irrational under different circum-\nstances. For example, once all the dirt is cleaned up, the agent will oscillate needlessly back\nand forth; if the performance measure includes a penalty of one point for each movement left\nor right, the agent will fare poorly. A better agent for this case would do nothing once it is",
  "and forth; if the performance measure includes a penalty of one point for each movement left\nor right, the agent will fare poorly. A better agent for this case would do nothing once it is\nsure that all the squares are clean. If clean squares can become dirty again, the agent should\noccasionally check and re-clean them if needed. If the geography of the environment is un-\nknown, the agent will need to explore it rather than stick to squares A and B. Exercise 2.2\nasks you to design agents for these cases.\n2.2.2\nOmniscience, learning, and autonomy\nWe need to be careful to distinguish between rationality and omniscience. An omniscient\nOMNISCIENCE\nagent knows the actual outcome of its actions and can act accordingly; but omniscience is\nimpossible in reality. Consider the following example: I am walking along the Champs\nElys´ees one day and I see an old friend across the street. There is no trafﬁc nearby and I’m\nnot otherwise engaged, so, being rational, I start to cross the street. Meanwhile, at 33,000\nfeet, a cargo door falls off a passing airliner,2 and before I make it to the other side of the\nstreet I am ﬂattened. Was I irrational to cross the street? It is unlikely that my obituary would\nread “Idiot attempts to cross street.”\nThis example shows that rationality is not the same as perfection. Rationality max-\nimizes expected performance, while perfection maximizes actual performance. Retreating\nfrom a requirement of perfection is not just a question of being fair to agents. The point is\nthat if we expect an agent to do what turns out to be the best action after the fact, it will be\nimpossible to design an agent to fulﬁll this speciﬁcation—unless we improve the performance\nof crystal balls or time machines.\n2 See N. Henderson, “New door latches urged for Boeing 747 jumbo jets,” Washington Post, August 24, 1989. Section 2.2.\nGood Behavior: The Concept of Rationality\n39\nOur deﬁnition of rationality does not require omniscience, then, because the rational\nchoice depends only on the percept sequence to date. We must also ensure that we haven’t\ninadvertently allowed the agent to engage in decidedly underintelligent activities. For exam-\nple, if an agent does not look both ways before crossing a busy road, then its percept sequence\nwill not tell it that there is a large truck approaching at high speed. Does our deﬁnition of\nrationality say that it’s now OK to cross the road? Far from it! First, it would not be rational",
  "will not tell it that there is a large truck approaching at high speed. Does our deﬁnition of\nrationality say that it’s now OK to cross the road? Far from it! First, it would not be rational\nto cross the road given this uninformative percept sequence: the risk of accident from cross-\ning without looking is too great. Second, a rational agent should choose the “looking” action\nbefore stepping into the street, because looking helps maximize the expected performance.\nDoing actions in order to modify future percepts—sometimes called information gather-\ning—is an important part of rationality and is covered in depth in Chapter 16. A second\nINFORMATION\nGATHERING\nexample of information gathering is provided by the exploration that must be undertaken by\nEXPLORATION\na vacuum-cleaning agent in an initially unknown environment.\nOur deﬁnition requires a rational agent not only to gather information but also to learn\nLEARNING\nas much as possible from what it perceives. The agent’s initial conﬁguration could reﬂect\nsome prior knowledge of the environment, but as the agent gains experience this may be\nmodiﬁed and augmented. There are extreme cases in which the environment is completely\nknown a priori. In such cases, the agent need not perceive or learn; it simply acts correctly.\nOf course, such agents are fragile. Consider the lowly dung beetle. After digging its nest and\nlaying its eggs, it fetches a ball of dung from a nearby heap to plug the entrance. If the ball of\ndung is removed from its grasp en route, the beetle continues its task and pantomimes plug-\nging the nest with the nonexistent dung ball, never noticing that it is missing. Evolution has\nbuilt an assumption into the beetle’s behavior, and when it is violated, unsuccessful behavior\nresults. Slightly more intelligent is the sphex wasp. The female sphex will dig a burrow, go\nout and sting a caterpillar and drag it to the burrow, enter the burrow again to check all is\nwell, drag the caterpillar inside, and lay its eggs. The caterpillar serves as a food source when\nthe eggs hatch. So far so good, but if an entomologist moves the caterpillar a few inches\naway while the sphex is doing the check, it will revert to the “drag” step of its plan and will\ncontinue the plan without modiﬁcation, even after dozens of caterpillar-moving interventions.\nThe sphex is unable to learn that its innate plan is failing, and thus will not change it.",
  "continue the plan without modiﬁcation, even after dozens of caterpillar-moving interventions.\nThe sphex is unable to learn that its innate plan is failing, and thus will not change it.\nTo the extent that an agent relies on the prior knowledge of its designer rather than\non its own percepts, we say that the agent lacks autonomy. A rational agent should be\nAUTONOMY\nautonomous—it should learn what it can to compensate for partial or incorrect prior knowl-\nedge. For example, a vacuum-cleaning agent that learns to foresee where and when additional\ndirt will appear will do better than one that does not. As a practical matter, one seldom re-\nquires complete autonomy from the start: when the agent has had little or no experience, it\nwould have to act randomly unless the designer gave some assistance. So, just as evolution\nprovides animals with enough built-in reﬂexes to survive long enough to learn for themselves,\nit would be reasonable to provide an artiﬁcial intelligent agent with some initial knowledge\nas well as an ability to learn. After sufﬁcient experience of its environment, the behavior\nof a rational agent can become effectively independent of its prior knowledge. Hence, the\nincorporation of learning allows one to design a single rational agent that will succeed in a\nvast variety of environments. 40\nChapter\n2.\nIntelligent Agents\n2.3\nTHE NATURE OF ENVIRONMENTS\nNow that we have a deﬁnition of rationality, we are almost ready to think about building\nrational agents. First, however, we must think about task environments, which are essen-\nTASK ENVIRONMENT\ntially the “problems” to which rational agents are the “solutions.” We begin by showing how\nto specify a task environment, illustrating the process with a number of examples. We then\nshow that task environments come in a variety of ﬂavors. The ﬂavor of the task environment\ndirectly affects the appropriate design for the agent program.\n2.3.1\nSpecifying the task environment\nIn our discussion of the rationality of the simple vacuum-cleaner agent, we had to specify\nthe performance measure, the environment, and the agent’s actuators and sensors. We group\nall these under the heading of the task environment. For the acronymically minded, we call\nthis the PEAS (Performance, Environment, Actuators, Sensors) description. In designing an\nPEAS\nagent, the ﬁrst step must always be to specify the task environment as fully as possible.\nThe vacuum world was a simple example; let us consider a more complex problem: an",
  "PEAS\nagent, the ﬁrst step must always be to specify the task environment as fully as possible.\nThe vacuum world was a simple example; let us consider a more complex problem: an\nautomated taxi driver. We should point out, before the reader becomes alarmed, that a fully\nautomated taxi is currently somewhat beyond the capabilities of existing technology. (page 28\ndescribes an existing driving robot.) The full driving task is extremely open-ended. There is\nno limit to the novel combinations of circumstances that can arise—another reason we chose\nit as a focus for discussion. Figure 2.4 summarizes the PEAS description for the taxi’s task\nenvironment. We discuss each element in more detail in the following paragraphs.\nAgent Type\nPerformance\nMeasure\nEnvironment\nActuators\nSensors\nTaxi driver\nSafe, fast, legal,\ncomfortable trip,\nmaximize proﬁts\nRoads, other\ntrafﬁc,\npedestrians,\ncustomers\nSteering,\naccelerator,\nbrake, signal,\nhorn, display\nCameras, sonar,\nspeedometer,\nGPS, odometer,\naccelerometer,\nengine sensors,\nkeyboard\nFigure 2.4\nPEAS description of the task environment for an automated taxi.\nFirst, what is the performance measure to which we would like our automated driver\nto aspire? Desirable qualities include getting to the correct destination; minimizing fuel con-\nsumption and wear and tear; minimizing the trip time or cost; minimizing violations of trafﬁc\nlaws and disturbances to other drivers; maximizing safety and passenger comfort; maximiz-\ning proﬁts. Obviously, some of these goals conﬂict, so tradeoffs will be required.\nNext, what is the driving environment that the taxi will face? Any taxi driver must\ndeal with a variety of roads, ranging from rural lanes and urban alleys to 12-lane freeways.\nThe roads contain other trafﬁc, pedestrians, stray animals, road works, police cars, puddles, Section 2.3.\nThe Nature of Environments\n41\nand potholes. The taxi must also interact with potential and actual passengers. There are also\nsome optional choices. The taxi might need to operate in Southern California, where snow\nis seldom a problem, or in Alaska, where it seldom is not. It could always be driving on the\nright, or we might want it to be ﬂexible enough to drive on the left when in Britain or Japan.\nObviously, the more restricted the environment, the easier the design problem.\nThe actuators for an automated taxi include those available to a human driver: control\nover the engine through the accelerator and control over steering and braking. In addition, it",
  "The actuators for an automated taxi include those available to a human driver: control\nover the engine through the accelerator and control over steering and braking. In addition, it\nwill need output to a display screen or voice synthesizer to talk back to the passengers, and\nperhaps some way to communicate with other vehicles, politely or otherwise.\nThe basic sensors for the taxi will include one or more controllable video cameras so\nthat it can see the road; it might augment these with infrared or sonar sensors to detect dis-\ntances to other cars and obstacles. To avoid speeding tickets, the taxi should have a speedome-\nter, and to control the vehicle properly, especially on curves, it should have an accelerometer.\nTo determine the mechanical state of the vehicle, it will need the usual array of engine, fuel,\nand electrical system sensors. Like many human drivers, it might want a global positioning\nsystem (GPS) so that it doesn’t get lost. Finally, it will need a keyboard or microphone for\nthe passenger to request a destination.\nIn Figure 2.5, we have sketched the basic PEAS elements for a number of additional\nagent types. Further examples appear in Exercise 2.4. It may come as a surprise to some read-\ners that our list of agent types includes some programs that operate in the entirely artiﬁcial\nenvironment deﬁned by keyboard input and character output on a screen. “Surely,” one might\nsay, “this is not a real environment, is it?” In fact, what matters is not the distinction between\n“real” and “artiﬁcial” environments, but the complexity of the relationship among the behav-\nior of the agent, the percept sequence generated by the environment, and the performance\nmeasure. Some “real” environments are actually quite simple. For example, a robot designed\nto inspect parts as they come by on a conveyor belt can make use of a number of simplifying\nassumptions: that the lighting is always just so, that the only thing on the conveyor belt will\nbe parts of a kind that it knows about, and that only two actions (accept or reject) are possible.\nIn contrast, some software agents (or software robots or softbots) exist in rich, unlim-\nSOFTWARE AGENT\nSOFTBOT\nited domains. Imagine a softbot Web site operator designed to scan Internet news sources and\nshow the interesting items to its users, while selling advertising space to generate revenue.\nTo do well, that operator will need some natural language processing abilities, it will need",
  "show the interesting items to its users, while selling advertising space to generate revenue.\nTo do well, that operator will need some natural language processing abilities, it will need\nto learn what each user and advertiser is interested in, and it will need to change its plans\ndynamically—for example, when the connection for one news source goes down or when a\nnew one comes online. The Internet is an environment whose complexity rivals that of the\nphysical world and whose inhabitants include many artiﬁcial and human agents.\n2.3.2\nProperties of task environments\nThe range of task environments that might arise in AI is obviously vast. We can, however,\nidentify a fairly small number of dimensions along which task environments can be catego-\nrized. These dimensions determine, to a large extent, the appropriate agent design and the\napplicability of each of the principal families of techniques for agent implementation. First, 42\nChapter\n2.\nIntelligent Agents\nAgent Type\nPerformance\nMeasure\nEnvironment\nActuators\nSensors\nMedical\ndiagnosis system\nHealthy patient,\nreduced costs\nPatient, hospital,\nstaff\nDisplay of\nquestions, tests,\ndiagnoses,\ntreatments,\nreferrals\nKeyboard entry\nof symptoms,\nﬁndings, patient’s\nanswers\nSatellite image\nanalysis system\nCorrect image\ncategorization\nDownlink from\norbiting satellite\nDisplay of scene\ncategorization\nColor pixel\narrays\nPart-picking\nrobot\nPercentage of\nparts in correct\nbins\nConveyor belt\nwith parts; bins\nJointed arm and\nhand\nCamera, joint\nangle sensors\nReﬁnery\ncontroller\nPurity, yield,\nsafety\nReﬁnery,\noperators\nValves, pumps,\nheaters, displays\nTemperature,\npressure,\nchemical sensors\nInteractive\nEnglish tutor\nStudent’s score\non test\nSet of students,\ntesting agency\nDisplay of\nexercises,\nsuggestions,\ncorrections\nKeyboard entry\nFigure 2.5\nExamples of agent types and their PEAS descriptions.\nwe list the dimensions, then we analyze several task environments to illustrate the ideas. The\ndeﬁnitions here are informal; later chapters provide more precise statements and examples of\neach kind of environment.\nFully observable vs. partially observable: If an agent’s sensors give it access to the\nFULLY OBSERVABLE\nPARTIALLY\nOBSERVABLE\ncomplete state of the environment at each point in time, then we say that the task environ-\nment is fully observable. A task environment is effectively fully observable if the sensors\ndetect all aspects that are relevant to the choice of action; relevance, in turn, depends on the",
  "ment is fully observable. A task environment is effectively fully observable if the sensors\ndetect all aspects that are relevant to the choice of action; relevance, in turn, depends on the\nperformance measure. Fully observable environments are convenient because the agent need\nnot maintain any internal state to keep track of the world. An environment might be partially\nobservable because of noisy and inaccurate sensors or because parts of the state are simply\nmissing from the sensor data—for example, a vacuum agent with only a local dirt sensor\ncannot tell whether there is dirt in other squares, and an automated taxi cannot see what other\ndrivers are thinking. If the agent has no sensors at all then the environment is unobserv-\nable. One might think that in such cases the agent’s plight is hopeless, but, as we discuss in\nUNOBSERVABLE\nChapter 4, the agent’s goals may still be achievable, sometimes with certainty.\nSingle agent vs. multiagent: The distinction between single-agent and multiagent en-\nSINGLE AGENT\nMULTIAGENT Section 2.3.\nThe Nature of Environments\n43\nvironments may seem simple enough. For example, an agent solving a crossword puzzle by\nitself is clearly in a single-agent environment, whereas an agent playing chess is in a two-\nagent environment. There are, however, some subtle issues. First, we have described how an\nentity may be viewed as an agent, but we have not explained which entities must be viewed\nas agents. Does an agent A (the taxi driver for example) have to treat an object B (another\nvehicle) as an agent, or can it be treated merely as an object behaving according to the laws of\nphysics, analogous to waves at the beach or leaves blowing in the wind? The key distinction\nis whether B’s behavior is best described as maximizing a performance measure whose value\ndepends on agent A’s behavior. For example, in chess, the opponent entity B is trying to\nmaximize its performance measure, which, by the rules of chess, minimizes agent A’s per-\nformance measure. Thus, chess is a competitive multiagent environment. In the taxi-driving\nCOMPETITIVE\nenvironment, on the other hand, avoiding collisions maximizes the performance measure of\nall agents, so it is a partially cooperative multiagent environment. It is also partially com-\nCOOPERATIVE\npetitive because, for example, only one car can occupy a parking space. The agent-design\nproblems in multiagent environments are often quite different from those in single-agent en-",
  "COOPERATIVE\npetitive because, for example, only one car can occupy a parking space. The agent-design\nproblems in multiagent environments are often quite different from those in single-agent en-\nvironments; for example, communication often emerges as a rational behavior in multiagent\nenvironments; in some competitive environments, randomized behavior is rational because\nit avoids the pitfalls of predictability.\nDeterministic vs. stochastic. If the next state of the environment is completely deter-\nDETERMINISTIC\nSTOCHASTIC\nmined by the current state and the action executed by the agent, then we say the environment\nis deterministic; otherwise, it is stochastic. In principle, an agent need not worry about uncer-\ntainty in a fully observable, deterministic environment. (In our deﬁnition, we ignore uncer-\ntainty that arises purely from the actions of other agents in a multiagent environment; thus,\na game can be deterministic even though each agent may be unable to predict the actions of\nthe others.) If the environment is partially observable, however, then it could appear to be\nstochastic. Most real situations are so complex that it is impossible to keep track of all the\nunobserved aspects; for practical purposes, they must be treated as stochastic. Taxi driving is\nclearly stochastic in this sense, because one can never predict the behavior of trafﬁc exactly;\nmoreover, one’s tires blow out and one’s engine seizes up without warning. The vacuum\nworld as we described it is deterministic, but variations can include stochastic elements such\nas randomly appearing dirt and an unreliable suction mechanism (Exercise 2.13). We say an\nenvironment is uncertain if it is not fully observable or not deterministic. One ﬁnal note:\nUNCERTAIN\nour use of the word “stochastic” generally implies that uncertainty about outcomes is quan-\ntiﬁed in terms of probabilities; a nondeterministic environment is one in which actions are\nNONDETERMINISTIC\ncharacterized by their possible outcomes, but no probabilities are attached to them. Nonde-\nterministic environment descriptions are usually associated with performance measures that\nrequire the agent to succeed for all possible outcomes of its actions.\nEpisodic vs. sequential: In an episodic task environment, the agent’s experience is\nEPISODIC\nSEQUENTIAL\ndivided into atomic episodes. In each episode the agent receives a percept and then performs\na single action. Crucially, the next episode does not depend on the actions taken in previous",
  "EPISODIC\nSEQUENTIAL\ndivided into atomic episodes. In each episode the agent receives a percept and then performs\na single action. Crucially, the next episode does not depend on the actions taken in previous\nepisodes. Many classiﬁcation tasks are episodic. For example, an agent that has to spot\ndefective parts on an assembly line bases each decision on the current part, regardless of\nprevious decisions; moreover, the current decision doesn’t affect whether the next part is 44\nChapter\n2.\nIntelligent Agents\ndefective. In sequential environments, on the other hand, the current decision could affect\nall future decisions.3 Chess and taxi driving are sequential: in both cases, short-term actions\ncan have long-term consequences. Episodic environments are much simpler than sequential\nenvironments because the agent does not need to think ahead.\nStatic vs. dynamic: If the environment can change while an agent is deliberating, then\nSTATIC\nDYNAMIC\nwe say the environment is dynamic for that agent; otherwise, it is static. Static environments\nare easy to deal with because the agent need not keep looking at the world while it is deciding\non an action, nor need it worry about the passage of time. Dynamic environments, on the\nother hand, are continuously asking the agent what it wants to do; if it hasn’t decided yet,\nthat counts as deciding to do nothing. If the environment itself does not change with the\npassage of time but the agent’s performance score does, then we say the environment is\nsemidynamic. Taxi driving is clearly dynamic: the other cars and the taxi itself keep moving\nSEMIDYNAMIC\nwhile the driving algorithm dithers about what to do next. Chess, when played with a clock,\nis semidynamic. Crossword puzzles are static.\nDiscrete vs. continuous: The discrete/continuous distinction applies to the state of the\nDISCRETE\nCONTINUOUS\nenvironment, to the way time is handled, and to the percepts and actions of the agent. For\nexample, the chess environment has a ﬁnite number of distinct states (excluding the clock).\nChess also has a discrete set of percepts and actions. Taxi driving is a continuous-state and\ncontinuous-time problem: the speed and location of the taxi and of the other vehicles sweep\nthrough a range of continuous values and do so smoothly over time. Taxi-driving actions are\nalso continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speak-",
  "through a range of continuous values and do so smoothly over time. Taxi-driving actions are\nalso continuous (steering angles, etc.). Input from digital cameras is discrete, strictly speak-\ning, but is typically treated as representing continuously varying intensities and locations.\nKnown vs. unknown: Strictly speaking, this distinction refers not to the environment\nKNOWN\nUNKNOWN\nitself but to the agent’s (or designer’s) state of knowledge about the “laws of physics” of\nthe environment. In a known environment, the outcomes (or outcome probabilities if the\nenvironment is stochastic) for all actions are given. Obviously, if the environment is unknown,\nthe agent will have to learn how it works in order to make good decisions. Note that the\ndistinction between known and unknown environments is not the same as the one between\nfully and partially observable environments. It is quite possible for a known environment\nto be partially observable—for example, in solitaire card games, I know the rules but am\nstill unable to see the cards that have not yet been turned over. Conversely, an unknown\nenvironment can be fully observable—in a new video game, the screen may show the entire\ngame state but I still don’t know what the buttons do until I try them.\nAs one might expect, the hardest case is partially observable, multiagent, stochastic,\nsequential, dynamic, continuous, and unknown. Taxi driving is hard in all these senses, except\nthat for the most part the driver’s environment is known. Driving a rented car in a new country\nwith unfamiliar geography and trafﬁc laws is a lot more exciting.\nFigure 2.6 lists the properties of a number of familiar environments. Note that the\nanswers are not always cut and dried. For example, we describe the part-picking robot as\nepisodic, because it normally considers each part in isolation. But if one day there is a large\n3 The word “sequential” is also used in computer science as the antonym of “parallel.” The two meanings are\nlargely unrelated. Section 2.3.\nThe Nature of Environments\n45\nTask Environment\nObservable Agents Deterministic\nEpisodic\nStatic\nDiscrete\nCrossword puzzle\nFully\nSingle Deterministic Sequential\nStatic\nDiscrete\nChess with a clock\nFully\nMulti\nDeterministic Sequential\nSemi\nDiscrete\nPoker\nPartially\nMulti\nStochastic\nSequential\nStatic\nDiscrete\nBackgammon\nFully\nMulti\nStochastic\nSequential\nStatic\nDiscrete\nTaxi driving\nPartially\nMulti\nStochastic\nSequential Dynamic Continuous\nMedical diagnosis\nPartially\nSingle\nStochastic",
  "Discrete\nPoker\nPartially\nMulti\nStochastic\nSequential\nStatic\nDiscrete\nBackgammon\nFully\nMulti\nStochastic\nSequential\nStatic\nDiscrete\nTaxi driving\nPartially\nMulti\nStochastic\nSequential Dynamic Continuous\nMedical diagnosis\nPartially\nSingle\nStochastic\nSequential Dynamic Continuous\nImage analysis\nFully\nSingle Deterministic\nEpisodic\nSemi\nContinuous\nPart-picking robot\nPartially\nSingle\nStochastic\nEpisodic\nDynamic Continuous\nReﬁnery controller\nPartially\nSingle\nStochastic\nSequential Dynamic Continuous\nInteractive English tutor\nPartially\nMulti\nStochastic\nSequential Dynamic\nDiscrete\nFigure 2.6\nExamples of task environments and their characteristics.\nbatch of defective parts, the robot should learn from several observations that the distribution\nof defects has changed, and should modify its behavior for subsequent parts. We have not\nincluded a “known/unknown” column because, as explained earlier, this is not strictly a prop-\nerty of the environment. For some environments, such as chess and poker, it is quite easy to\nsupply the agent with full knowledge of the rules, but it is nonetheless interesting to consider\nhow an agent might learn to play these games without such knowledge.\nSeveral of the answers in the table depend on how the task environment is deﬁned. We\nhave listed the medical-diagnosis task as single-agent because the disease process in a patient\nis not proﬁtably modeled as an agent; but a medical-diagnosis system might also have to\ndeal with recalcitrant patients and skeptical staff, so the environment could have a multiagent\naspect. Furthermore, medical diagnosis is episodic if one conceives of the task as selecting a\ndiagnosis given a list of symptoms; the problem is sequential if the task can include proposing\na series of tests, evaluating progress over the course of treatment, and so on. Also, many\nenvironments are episodic at higher levels than the agent’s individual actions. For example,\na chess tournament consists of a sequence of games; each game is an episode because (by\nand large) the contribution of the moves in one game to the agent’s overall performance is\nnot affected by the moves in its previous game. On the other hand, decision making within a\nsingle game is certainly sequential.\nThe code repository associated with this book (aima.cs.berkeley.edu) includes imple-\nmentations of a number of environments, together with a general-purpose environment simu-\nlator that places one or more agents in a simulated environment, observes their behavior over",
  "mentations of a number of environments, together with a general-purpose environment simu-\nlator that places one or more agents in a simulated environment, observes their behavior over\ntime, and evaluates them according to a given performance measure. Such experiments are\noften carried out not for a single environment but for many environments drawn from an en-\nvironment class. For example, to evaluate a taxi driver in simulated trafﬁc, we would want to\nENVIRONMENT\nCLASS\nrun many simulations with different trafﬁc, lighting, and weather conditions. If we designed\nthe agent for a single scenario, we might be able to take advantage of speciﬁc properties\nof the particular case but might not identify a good design for driving in general. For this 46\nChapter\n2.\nIntelligent Agents\nreason, the code repository also includes an environment generator for each environment\nENVIRONMENT\nGENERATOR\nclass that selects particular environments (with certain likelihoods) in which to run the agent.\nFor example, the vacuum environment generator initializes the dirt pattern and agent location\nrandomly. We are then interested in the agent’s average performance over the environment\nclass. A rational agent for a given environment class maximizes this average performance.\nExercises 2.8 to 2.13 take you through the process of developing an environment class and\nevaluating various agents therein.\n2.4\nTHE STRUCTURE OF AGENTS\nSo far we have talked about agents by describing behavior—the action that is performed after\nany given sequence of percepts. Now we must bite the bullet and talk about how the insides\nwork. The job of AI is to design an agent program that implements the agent function—\nAGENT PROGRAM\nthe mapping from percepts to actions. We assume this program will run on some sort of\ncomputing device with physical sensors and actuators—we call this the architecture:\nARCHITECTURE\nagent = architecture + program .\nObviously, the program we choose has to be one that is appropriate for the architecture. If the\nprogram is going to recommend actions like Walk, the architecture had better have legs. The\narchitecture might be just an ordinary PC, or it might be a robotic car with several onboard\ncomputers, cameras, and other sensors. In general, the architecture makes the percepts from\nthe sensors available to the program, runs the program, and feeds the program’s action choices\nto the actuators as they are generated. Most of this book is about designing agent programs,",
  "the sensors available to the program, runs the program, and feeds the program’s action choices\nto the actuators as they are generated. Most of this book is about designing agent programs,\nalthough Chapters 24 and 25 deal directly with the sensors and actuators.\n2.4.1\nAgent programs\nThe agent programs that we design in this book all have the same skeleton: they take the\ncurrent percept as input from the sensors and return an action to the actuators.4 Notice the\ndifference between the agent program, which takes the current percept as input, and the agent\nfunction, which takes the entire percept history. The agent program takes just the current\npercept as input because nothing more is available from the environment; if the agent’s actions\nneed to depend on the entire percept sequence, the agent will have to remember the percepts.\nWe describe the agent programs in the simple pseudocode language that is deﬁned in\nAppendix B. (The online code repository contains implementations in real programming\nlanguages.) For example, Figure 2.7 shows a rather trivial agent program that keeps track of\nthe percept sequence and then uses it to index into a table of actions to decide what to do.\nThe table—an example of which is given for the vacuum world in Figure 2.3—represents\nexplicitly the agent function that the agent program embodies. To build a rational agent in\n4 There are other choices for the agent program skeleton; for example, we could have the agent programs be\ncoroutines that run asynchronously with the environment. Each such coroutine has an input and output port and\nconsists of a loop that reads the input port for percepts and writes actions to the output port. Section 2.4.\nThe Structure of Agents\n47\nfunction TABLE-DRIVEN-AGENT(percept) returns an action\npersistent: percepts, a sequence, initially empty\ntable, a table of actions, indexed by percept sequences, initially fully speciﬁed\nappend percept to the end of percepts\naction ←LOOKUP(percepts,table)\nreturn action\nFigure 2.7\nThe TABLE-DRIVEN-AGENT program is invoked for each new percept and\nreturns an action each time. It retains the complete percept sequence in memory.\nthis way, we as designers must construct a table that contains the appropriate action for every\npossible percept sequence.\nIt is instructive to consider why the table-driven approach to agent construction is\ndoomed to failure. Let P be the set of possible percepts and let T be the lifetime of the",
  "possible percept sequence.\nIt is instructive to consider why the table-driven approach to agent construction is\ndoomed to failure. Let P be the set of possible percepts and let T be the lifetime of the\nagent (the total number of percepts it will receive). The lookup table will contain \u0002T\nt = 1 |P|t\nentries. Consider the automated taxi: the visual input from a single camera comes in at the\nrate of roughly 27 megabytes per second (30 frames per second, 640 × 480 pixels with 24\nbits of color information). This gives a lookup table with over 10250,000,000,000 entries for an\nhour’s driving. Even the lookup table for chess—a tiny, well-behaved fragment of the real\nworld—would have at least 10150 entries. The daunting size of these tables (the number of\natoms in the observable universe is less than 1080) means that (a) no physical agent in this\nuniverse will have the space to store the table, (b) the designer would not have time to create\nthe table, (c) no agent could ever learn all the right table entries from its experience, and (d)\neven if the environment is simple enough to yield a feasible table size, the designer still has\nno guidance about how to ﬁll in the table entries.\nDespite all this, TABLE-DRIVEN-AGENT does do what we want: it implements the\ndesired agent function. The key challenge for AI is to ﬁnd out how to write programs that,\nto the extent possible, produce rational behavior from a smallish program rather than from\na vast table. We have many examples showing that this can be done successfully in other\nareas: for example, the huge tables of square roots used by engineers and schoolchildren prior\nto the 1970s have now been replaced by a ﬁve-line program for Newton’s method running\non electronic calculators. The question is, can AI do for general intelligent behavior what\nNewton did for square roots? We believe the answer is yes.\nIn the remainder of this section, we outline four basic kinds of agent programs that\nembody the principles underlying almost all intelligent systems:\n• Simple reﬂex agents;\n• Model-based reﬂex agents;\n• Goal-based agents; and\n• Utility-based agents.\nEach kind of agent program combines particular components in particular ways to generate\nactions. Section 2.4.6 explains in general terms how to convert all these agents into learning 48\nChapter\n2.\nIntelligent Agents\nfunction REFLEX-VACUUM-AGENT([location,status]) returns an action\nif status = Dirty then return Suck\nelse if location = A then return Right",
  "Chapter\n2.\nIntelligent Agents\nfunction REFLEX-VACUUM-AGENT([location,status]) returns an action\nif status = Dirty then return Suck\nelse if location = A then return Right\nelse if location = B then return Left\nFigure 2.8\nThe agent program for a simple reﬂex agent in the two-state vacuum environ-\nment. This program implements the agent function tabulated in Figure 2.3.\nagents that can improve the performance of their components so as to generate better actions.\nFinally, Section 2.4.7 describes the variety of ways in which the components themselves can\nbe represented within the agent. This variety provides a major organizing principle for the\nﬁeld and for the book itself.\n2.4.2\nSimple reﬂex agents\nThe simplest kind of agent is the simple reﬂex agent. These agents select actions on the basis\nSIMPLE REFLEX\nAGENT\nof the current percept, ignoring the rest of the percept history. For example, the vacuum agent\nwhose agent function is tabulated in Figure 2.3 is a simple reﬂex agent, because its decision\nis based only on the current location and on whether that location contains dirt. An agent\nprogram for this agent is shown in Figure 2.8.\nNotice that the vacuum agent program is very small indeed compared to the correspond-\ning table. The most obvious reduction comes from ignoring the percept history, which cuts\ndown the number of possibilities from 4T to just 4. A further, small reduction comes from\nthe fact that when the current square is dirty, the action does not depend on the location.\nSimple reﬂex behaviors occur even in more complex environments. Imagine yourself\nas the driver of the automated taxi. If the car in front brakes and its brake lights come on, then\nyou should notice this and initiate braking. In other words, some processing is done on the\nvisual input to establish the condition we call “The car in front is braking.” Then, this triggers\nsome established connection in the agent program to the action “initiate braking.” We call\nsuch a connection a condition–action rule,5 written as\nCONDITION–ACTION\nRULE\nif car-in-front-is-braking then initiate-braking.\nHumans also have many such connections, some of which are learned responses (as for driv-\ning) and some of which are innate reﬂexes (such as blinking when something approaches the\neye). In the course of the book, we show several different ways in which such connections\ncan be learned and implemented.\nThe program in Figure 2.8 is speciﬁc to one particular vacuum environment. A more",
  "eye). In the course of the book, we show several different ways in which such connections\ncan be learned and implemented.\nThe program in Figure 2.8 is speciﬁc to one particular vacuum environment. A more\ngeneral and ﬂexible approach is ﬁrst to build a general-purpose interpreter for condition–\naction rules and then to create rule sets for speciﬁc task environments. Figure 2.9 gives the\nstructure of this general program in schematic form, showing how the condition–action rules\nallow the agent to make the connection from percept to action. (Do not worry if this seems\n5 Also called situation–action rules, productions, or if–then rules. Section 2.4.\nThe Structure of Agents\n49\nAgent\nEnvironment\nSensors\nWhat action I\nshould do now\nCondition-action rules\nActuators\nWhat the world\nis like now\nFigure 2.9\nSchematic diagram of a simple reﬂex agent.\nfunction SIMPLE-REFLEX-AGENT(percept) returns an action\npersistent: rules, a set of condition–action rules\nstate ←INTERPRET-INPUT(percept)\nrule ←RULE-MATCH(state,rules)\naction ←rule.ACTION\nreturn action\nFigure 2.10\nA simple reﬂex agent. It acts according to a rule whose condition matches\nthe current state, as deﬁned by the percept.\ntrivial; it gets more interesting shortly.) We use rectangles to denote the current internal state\nof the agent’s decision process, and ovals to represent the background information used in\nthe process. The agent program, which is also very simple, is shown in Figure 2.10. The\nINTERPRET-INPUT function generates an abstracted description of the current state from the\npercept, and the RULE-MATCH function returns the ﬁrst rule in the set of rules that matches\nthe given state description. Note that the description in terms of “rules” and “matching” is\npurely conceptual; actual implementations can be as simple as a collection of logic gates\nimplementing a Boolean circuit.\nSimple reﬂex agents have the admirable property of being simple, but they turn out to be\nof limited intelligence. The agent in Figure 2.10 will work only if the correct decision can be\nmade on the basis of only the current percept—that is, only if the environment is fully observ-\nable. Even a little bit of unobservability can cause serious trouble. For example, the braking\nrule given earlier assumes that the condition car-in-front-is-braking can be determined from\nthe current percept—a single frame of video. This works if the car in front has a centrally",
  "rule given earlier assumes that the condition car-in-front-is-braking can be determined from\nthe current percept—a single frame of video. This works if the car in front has a centrally\nmounted brake light. Unfortunately, older models have different conﬁgurations of taillights, 50\nChapter\n2.\nIntelligent Agents\nbrake lights, and turn-signal lights, and it is not always possible to tell from a single image\nwhether the car is braking. A simple reﬂex agent driving behind such a car would either brake\ncontinuously and unnecessarily, or, worse, never brake at all.\nWe can see a similar problem arising in the vacuum world. Suppose that a simple reﬂex\nvacuum agent is deprived of its location sensor and has only a dirt sensor. Such an agent\nhas just two possible percepts: [Dirty] and [Clean]. It can Suck in response to [Dirty]; what\nshould it do in response to [Clean]? Moving Left fails (forever) if it happens to start in square\nA, and moving Right fails (forever) if it happens to start in square B. Inﬁnite loops are often\nunavoidable for simple reﬂex agents operating in partially observable environments.\nEscape from inﬁnite loops is possible if the agent can randomize its actions. For ex-\nRANDOMIZATION\nample, if the vacuum agent perceives [Clean], it might ﬂip a coin to choose between Left and\nRight. It is easy to show that the agent will reach the other square in an average of two steps.\nThen, if that square is dirty, the agent will clean it and the task will be complete. Hence, a\nrandomized simple reﬂex agent might outperform a deterministic simple reﬂex agent.\nWe mentioned in Section 2.3 that randomized behavior of the right kind can be rational\nin some multiagent environments. In single-agent environments, randomization is usually not\nrational. It is a useful trick that helps a simple reﬂex agent in some situations, but in most\ncases we can do much better with more sophisticated deterministic agents.\n2.4.3\nModel-based reﬂex agents\nThe most effective way to handle partial observability is for the agent to keep track of the\npart of the world it can’t see now. That is, the agent should maintain some sort of internal\nstate that depends on the percept history and thereby reﬂects at least some of the unobserved\nINTERNAL STATE\naspects of the current state. For the braking problem, the internal state is not too extensive—\njust the previous frame from the camera, allowing the agent to detect when two red lights at",
  "INTERNAL STATE\naspects of the current state. For the braking problem, the internal state is not too extensive—\njust the previous frame from the camera, allowing the agent to detect when two red lights at\nthe edge of the vehicle go on or off simultaneously. For other driving tasks such as changing\nlanes, the agent needs to keep track of where the other cars are if it can’t see them all at once.\nAnd for any driving to be possible at all, the agent needs to keep track of where its keys are.\nUpdating this internal state information as time goes by requires two kinds of knowl-\nedge to be encoded in the agent program. First, we need some information about how the\nworld evolves independently of the agent—for example, that an overtaking car generally will\nbe closer behind than it was a moment ago. Second, we need some information about how\nthe agent’s own actions affect the world—for example, that when the agent turns the steering\nwheel clockwise, the car turns to the right, or that after driving for ﬁve minutes northbound\non the freeway, one is usually about ﬁve miles north of where one was ﬁve minutes ago. This\nknowledge about “how the world works”—whether implemented in simple Boolean circuits\nor in complete scientiﬁc theories—is called a model of the world. An agent that uses such a\nmodel is called a model-based agent.\nMODEL-BASED\nAGENT\nFigure 2.11 gives the structure of the model-based reﬂex agent with internal state, show-\ning how the current percept is combined with the old internal state to generate the updated\ndescription of the current state, based on the agent’s model of how the world works. The agent\nprogram is shown in Figure 2.12. The interesting part is the function UPDATE-STATE, which Section 2.4.\nThe Structure of Agents\n51\nAgent\nEnvironment\nSensors\nState\nHow the world evolves\nWhat my actions do\nCondition-action rules\nActuators\nWhat the world\nis like now\nWhat action I\nshould do now\nFigure 2.11\nA model-based reﬂex agent.\nfunction MODEL-BASED-REFLEX-AGENT(percept) returns an action\npersistent: state, the agent’s current conception of the world state\nmodel, a description of how the next state depends on current state and action\nrules, a set of condition–action rules\naction, the most recent action, initially none\nstate ←UPDATE-STATE(state,action,percept,model)\nrule ←RULE-MATCH(state,rules)\naction ←rule.ACTION\nreturn action\nFigure 2.12\nA model-based reﬂex agent. It keeps track of the current state of the world,",
  "action, the most recent action, initially none\nstate ←UPDATE-STATE(state,action,percept,model)\nrule ←RULE-MATCH(state,rules)\naction ←rule.ACTION\nreturn action\nFigure 2.12\nA model-based reﬂex agent. It keeps track of the current state of the world,\nusing an internal model. It then chooses an action in the same way as the reﬂex agent.\nis responsible for creating the new internal state description. The details of how models and\nstates are represented vary widely depending on the type of environment and the particular\ntechnology used in the agent design. Detailed examples of models and updating algorithms\nappear in Chapters 4, 12, 11, 15, 17, and 25.\nRegardless of the kind of representation used, it is seldom possible for the agent to\ndetermine the current state of a partially observable environment exactly. Instead, the box\nlabeled “what the world is like now” (Figure 2.11) represents the agent’s “best guess” (or\nsometimes best guesses). For example, an automated taxi may not be able to see around the\nlarge truck that has stopped in front of it and can only guess about what may be causing the\nhold-up. Thus, uncertainty about the current state may be unavoidable, but the agent still has\nto make a decision.\nA perhaps less obvious point about the internal “state” maintained by a model-based\nagent is that it does not have to describe “what the world is like now” in a literal sense. For 52\nChapter\n2.\nIntelligent Agents\nAgent\nEnvironment\nSensors\nWhat action I\nshould do now\nState\nHow the world evolves\nWhat my actions do\nActuators\nWhat the world\nis like now\nWhat it will be like\n  if I do action A\nGoals\nFigure 2.13\nA model-based, goal-based agent. It keeps track of the world state as well as\na set of goals it is trying to achieve, and chooses an action that will (eventually) lead to the\nachievement of its goals.\nexample, the taxi may be driving back home, and it may have a rule telling it to ﬁll up with\ngas on the way home unless it has at least half a tank. Although “driving back home” may\nseem to an aspect of the world state, the fact of the taxi’s destination is actually an aspect of\nthe agent’s internal state. If you ﬁnd this puzzling, consider that the taxi could be in exactly\nthe same place at the same time, but intending to reach a different destination.\n2.4.4\nGoal-based agents\nKnowing something about the current state of the environment is not always enough to decide",
  "the same place at the same time, but intending to reach a different destination.\n2.4.4\nGoal-based agents\nKnowing something about the current state of the environment is not always enough to decide\nwhat to do. For example, at a road junction, the taxi can turn left, turn right, or go straight\non. The correct decision depends on where the taxi is trying to get to. In other words, as well\nas a current state description, the agent needs some sort of goal information that describes\nGOAL\nsituations that are desirable—for example, being at the passenger’s destination. The agent\nprogram can combine this with the model (the same information as was used in the model-\nbased reﬂex agent) to choose actions that achieve the goal. Figure 2.13 shows the goal-based\nagent’s structure.\nSometimes goal-based action selection is straightforward—for example, when goal sat-\nisfaction results immediately from a single action. Sometimes it will be more tricky—for\nexample, when the agent has to consider long sequences of twists and turns in order to ﬁnd a\nway to achieve the goal. Search (Chapters 3 to 5) and planning (Chapters 10 and 11) are the\nsubﬁelds of AI devoted to ﬁnding action sequences that achieve the agent’s goals.\nNotice that decision making of this kind is fundamentally different from the condition–\naction rules described earlier, in that it involves consideration of the future—both “What will\nhappen if I do such-and-such?” and “Will that make me happy?” In the reﬂex agent designs,\nthis information is not explicitly represented, because the built-in rules map directly from Section 2.4.\nThe Structure of Agents\n53\npercepts to actions. The reﬂex agent brakes when it sees brake lights. A goal-based agent, in\nprinciple, could reason that if the car in front has its brake lights on, it will slow down. Given\nthe way the world usually evolves, the only action that will achieve the goal of not hitting\nother cars is to brake.\nAlthough the goal-based agent appears less efﬁcient, it is more ﬂexible because the\nknowledge that supports its decisions is represented explicitly and can be modiﬁed. If it starts\nto rain, the agent can update its knowledge of how effectively its brakes will operate; this will\nautomatically cause all of the relevant behaviors to be altered to suit the new conditions.\nFor the reﬂex agent, on the other hand, we would have to rewrite many condition–action\nrules. The goal-based agent’s behavior can easily be changed to go to a different destination,",
  "For the reﬂex agent, on the other hand, we would have to rewrite many condition–action\nrules. The goal-based agent’s behavior can easily be changed to go to a different destination,\nsimply by specifying that destination as the goal. The reﬂex agent’s rules for when to turn\nand when to go straight will work only for a single destination; they must all be replaced to\ngo somewhere new.\n2.4.5\nUtility-based agents\nGoals alone are not enough to generate high-quality behavior in most environments. For\nexample, many action sequences will get the taxi to its destination (thereby achieving the\ngoal) but some are quicker, safer, more reliable, or cheaper than others. Goals just provide a\ncrude binary distinction between “happy” and “unhappy” states. A more general performance\nmeasure should allow a comparison of different world states according to exactly how happy\nthey would make the agent. Because “happy” does not sound very scientiﬁc, economists and\ncomputer scientists use the term utility instead.6\nUTILITY\nWe have already seen that a performance measure assigns a score to any given sequence\nof environment states, so it can easily distinguish between more and less desirable ways of\ngetting to the taxi’s destination. An agent’s utility function is essentially an internalization\nUTILITY FUNCTION\nof the performance measure. If the internal utility function and the external performance\nmeasure are in agreement, then an agent that chooses actions to maximize its utility will be\nrational according to the external performance measure.\nLet us emphasize again that this is not the only way to be rational—we have already\nseen a rational agent program for the vacuum world (Figure 2.8) that has no idea what its\nutility function is—but, like goal-based agents, a utility-based agent has many advantages in\nterms of ﬂexibility and learning. Furthermore, in two kinds of cases, goals are inadequate but\na utility-based agent can still make rational decisions. First, when there are conﬂicting goals,\nonly some of which can be achieved (for example, speed and safety), the utility function\nspeciﬁes the appropriate tradeoff. Second, when there are several goals that the agent can\naim for, none of which can be achieved with certainty, utility provides a way in which the\nlikelihood of success can be weighed against the importance of the goals.\nPartial observability and stochasticity are ubiquitous in the real world, and so, therefore,",
  "likelihood of success can be weighed against the importance of the goals.\nPartial observability and stochasticity are ubiquitous in the real world, and so, therefore,\nis decision making under uncertainty. Technically speaking, a rational utility-based agent\nchooses the action that maximizes the expected utility of the action outcomes—that is, the\nEXPECTED UTILITY\nutility the agent expects to derive, on average, given the probabilities and utilities of each\n6 The word “utility” here refers to “the quality of being useful,” not to the electric company or waterworks. 54\nChapter\n2.\nIntelligent Agents\nAgent\nEnvironment\nSensors\nHow happy I will be\nin such a state\nState\nHow the world evolves\nWhat my actions do\nUtility\nActuators\nWhat action I\nshould do now\nWhat it will be like\nif I do action A\nWhat the world\nis like now\nFigure 2.14\nA model-based, utility-based agent. It uses a model of the world, along with\na utility function that measures its preferences among states of the world. Then it chooses the\naction that leads to the best expected utility, where expected utility is computed by averaging\nover all possible outcome states, weighted by the probability of the outcome.\noutcome. (Appendix A deﬁnes expectation more precisely.) In Chapter 16, we show that any\nrational agent must behave as if it possesses a utility function whose expected value it tries\nto maximize. An agent that possesses an explicit utility function can make rational decisions\nwith a general-purpose algorithm that does not depend on the speciﬁc utility function being\nmaximized. In this way, the “global” deﬁnition of rationality—designating as rational those\nagent functions that have the highest performance—is turned into a “local” constraint on\nrational-agent designs that can be expressed in a simple program.\nThe utility-based agent structure appears in Figure 2.14. Utility-based agent programs\nappear in Part IV, where we design decision-making agents that must handle the uncertainty\ninherent in stochastic or partially observable environments.\nAt this point, the reader may be wondering, “Is it that simple? We just build agents that\nmaximize expected utility, and we’re done?” It’s true that such agents would be intelligent,\nbut it’s not simple. A utility-based agent has to model and keep track of its environment,\ntasks that have involved a great deal of research on perception, representation, reasoning,\nand learning. The results of this research ﬁll many of the chapters of this book. Choosing",
  "tasks that have involved a great deal of research on perception, representation, reasoning,\nand learning. The results of this research ﬁll many of the chapters of this book. Choosing\nthe utility-maximizing course of action is also a difﬁcult task, requiring ingenious algorithms\nthat ﬁll several more chapters. Even with these algorithms, perfect rationality is usually\nunachievable in practice because of computational complexity, as we noted in Chapter 1.\n2.4.6\nLearning agents\nWe have described agent programs with various methods for selecting actions. We have\nnot, so far, explained how the agent programs come into being. In his famous early paper,\nTuring (1950) considers the idea of actually programming his intelligent machines by hand. Section 2.4.\nThe Structure of Agents\n55\nPerformance standard\nAgent\nEnvironment\nSensors\nPerformance\nelement\nchanges\nknowledge\nlearning\n  goals\nProblem\ngenerator\nfeedback\n  Learning\nelement\nCritic\nActuators\nFigure 2.15\nA general learning agent.\nHe estimates how much work this might take and concludes “Some more expeditious method\nseems desirable.” The method he proposes is to build learning machines and then to teach\nthem. In many areas of AI, this is now the preferred method for creating state-of-the-art\nsystems. Learning has another advantage, as we noted earlier: it allows the agent to operate\nin initially unknown environments and to become more competent than its initial knowledge\nalone might allow. In this section, we brieﬂy introduce the main ideas of learning agents.\nThroughout the book, we comment on opportunities and methods for learning in particular\nkinds of agents. Part V goes into much more depth on the learning algorithms themselves.\nA learning agent can be divided into four conceptual components, as shown in Fig-\nure 2.15. The most important distinction is between the learning element, which is re-\nLEARNING ELEMENT\nsponsible for making improvements, and the performance element, which is responsible for\nPERFORMANCE\nELEMENT\nselecting external actions. The performance element is what we have previously considered\nto be the entire agent: it takes in percepts and decides on actions. The learning element uses\nfeedback from the critic on how the agent is doing and determines how the performance\nCRITIC\nelement should be modiﬁed to do better in the future.\nThe design of the learning element depends very much on the design of the performance",
  "feedback from the critic on how the agent is doing and determines how the performance\nCRITIC\nelement should be modiﬁed to do better in the future.\nThe design of the learning element depends very much on the design of the performance\nelement. When trying to design an agent that learns a certain capability, the ﬁrst question is\nnot “How am I going to get it to learn this?” but “What kind of performance element will my\nagent need to do this once it has learned how?” Given an agent design, learning mechanisms\ncan be constructed to improve every part of the agent.\nThe critic tells the learning element how well the agent is doing with respect to a ﬁxed\nperformance standard. The critic is necessary because the percepts themselves provide no\nindication of the agent’s success. For example, a chess program could receive a percept\nindicating that it has checkmated its opponent, but it needs a performance standard to know\nthat this is a good thing; the percept itself does not say so. It is important that the performance 56\nChapter\n2.\nIntelligent Agents\nstandard be ﬁxed. Conceptually, one should think of it as being outside the agent altogether\nbecause the agent must not modify it to ﬁt its own behavior.\nThe last component of the learning agent is the problem generator. It is responsible\nPROBLEM\nGENERATOR\nfor suggesting actions that will lead to new and informative experiences. The point is that\nif the performance element had its way, it would keep doing the actions that are best, given\nwhat it knows. But if the agent is willing to explore a little and do some perhaps suboptimal\nactions in the short run, it might discover much better actions for the long run. The problem\ngenerator’s job is to suggest these exploratory actions. This is what scientists do when they\ncarry out experiments. Galileo did not think that dropping rocks from the top of a tower in\nPisa was valuable in itself. He was not trying to break the rocks or to modify the brains of\nunfortunate passers-by. His aim was to modify his own brain by identifying a better theory\nof the motion of objects.\nTo make the overall design more concrete, let us return to the automated taxi example.\nThe performance element consists of whatever collection of knowledge and procedures the\ntaxi has for selecting its driving actions. The taxi goes out on the road and drives, using\nthis performance element. The critic observes the world and passes information along to the",
  "taxi has for selecting its driving actions. The taxi goes out on the road and drives, using\nthis performance element. The critic observes the world and passes information along to the\nlearning element. For example, after the taxi makes a quick left turn across three lanes of traf-\nﬁc, the critic observes the shocking language used by other drivers. From this experience, the\nlearning element is able to formulate a rule saying this was a bad action, and the performance\nelement is modiﬁed by installation of the new rule. The problem generator might identify\ncertain areas of behavior in need of improvement and suggest experiments, such as trying out\nthe brakes on different road surfaces under different conditions.\nThe learning element can make changes to any of the “knowledge” components shown\nin the agent diagrams (Figures 2.9, 2.11, 2.13, and 2.14). The simplest cases involve learning\ndirectly from the percept sequence. Observation of pairs of successive states of the environ-\nment can allow the agent to learn “How the world evolves,” and observation of the results of\nits actions can allow the agent to learn “What my actions do.” For example, if the taxi exerts\na certain braking pressure when driving on a wet road, then it will soon ﬁnd out how much\ndeceleration is actually achieved. Clearly, these two learning tasks are more difﬁcult if the\nenvironment is only partially observable.\nThe forms of learning in the preceding paragraph do not need to access the external\nperformance standard—in a sense, the standard is the universal one of making predictions\nthat agree with experiment. The situation is slightly more complex for a utility-based agent\nthat wishes to learn utility information. For example, suppose the taxi-driving agent receives\nno tips from passengers who have been thoroughly shaken up during the trip. The external\nperformance standard must inform the agent that the loss of tips is a negative contribution to\nits overall performance; then the agent might be able to learn that violent maneuvers do not\ncontribute to its own utility. In a sense, the performance standard distinguishes part of the\nincoming percept as a reward (or penalty) that provides direct feedback on the quality of the\nagent’s behavior. Hard-wired performance standards such as pain and hunger in animals can\nbe understood in this way. This issue is discussed further in Chapter 21.\nIn summary, agents have a variety of components, and those components can be repre-",
  "agent’s behavior. Hard-wired performance standards such as pain and hunger in animals can\nbe understood in this way. This issue is discussed further in Chapter 21.\nIn summary, agents have a variety of components, and those components can be repre-\nsented in many ways within the agent program, so there appears to be great variety among Section 2.4.\nThe Structure of Agents\n57\nlearning methods. There is, however, a single unifying theme. Learning in intelligent agents\ncan be summarized as a process of modiﬁcation of each component of the agent to bring the\ncomponents into closer agreement with the available feedback information, thereby improv-\ning the overall performance of the agent.\n2.4.7\nHow the components of agent programs work\nWe have described agent programs (in very high-level terms) as consisting of various compo-\nnents, whose function it is to answer questions such as: “What is the world like now?” “What\naction should I do now?” “What do my actions do?” The next question for a student of AI\nis, “How on earth do these components work?” It takes about a thousand pages to begin to\nanswer that question properly, but here we want to draw the reader’s attention to some basic\ndistinctions among the various ways that the components can represent the environment that\nthe agent inhabits.\nRoughly speaking, we can place the representations along an axis of increasing com-\nplexity and expressive power—atomic, factored, and structured. To illustrate these ideas,\nit helps to consider a particular agent component, such as the one that deals with “What my\nactions do.” This component describes the changes that might occur in the environment as\nthe result of taking an action, and Figure 2.16 provides schematic depictions of how those\ntransitions might be represented.\nB\nC\n(a) Atomic\n(b) Factored\n(b) Structured\nB\nC\nFigure 2.16\nThree ways to represent states and the transitions between them. (a) Atomic\nrepresentation: a state (such as B or C) is a black box with no internal structure; (b) Factored\nrepresentation: a state consists of a vector of attribute values; values can be Boolean, real-\nvalued, or one of a ﬁxed set of symbols. (c) Structured representation: a state includes\nobjects, each of which may have attributes of its own as well as relationships to other objects.\nIn an atomic representation each state of the world is indivisible—it has no internal\nATOMIC\nREPRESENTATION\nstructure. Consider the problem of ﬁnding a driving route from one end of a country to the",
  "In an atomic representation each state of the world is indivisible—it has no internal\nATOMIC\nREPRESENTATION\nstructure. Consider the problem of ﬁnding a driving route from one end of a country to the\nother via some sequence of cities (we address this problem in Figure 3.2 on page 68). For the\npurposes of solving this problem, it may sufﬁce to reduce the state of world to just the name\nof the city we are in—a single atom of knowledge; a “black box” whose only discernible\nproperty is that of being identical to or different from another black box. The algorithms 58\nChapter\n2.\nIntelligent Agents\nunderlying search and game-playing (Chapters 3–5), Hidden Markov models (Chapter 15),\nand Markov decision processes (Chapter 17) all work with atomic representations—or, at\nleast, they treat representations as if they were atomic.\nNow consider a higher-ﬁdelity description for the same problem, where we need to be\nconcerned with more than just atomic location in one city or another; we might need to pay\nattention to how much gas is in the tank, our current GPS coordinates, whether or not the oil\nwarning light is working, how much spare change we have for toll crossings, what station is\non the radio, and so on. A factored representation splits up each state into a ﬁxed set of\nFACTORED\nREPRESENTATION\nvariables or attributes, each of which can have a value. While two different atomic states\nVARIABLE\nATTRIBUTE\nVALUE\nhave nothing in common—they are just different black boxes—two different factored states\ncan share some attributes (such as being at some particular GPS location) and not others (such\nas having lots of gas or having no gas); this makes it much easier to work out how to turn\none state into another. With factored representations, we can also represent uncertainty—for\nexample, ignorance about the amount of gas in the tank can be represented by leaving that\nattribute blank. Many important areas of AI are based on factored representations, including\nconstraint satisfaction algorithms (Chapter 6), propositional logic (Chapter 7), planning\n(Chapters 10 and 11), Bayesian networks (Chapters 13–16), and the machine learning al-\ngorithms in Chapters 18, 20, and 21.\nFor many purposes, we need to understand the world as having things in it that are\nrelated to each other, not just variables with values. For example, we might notice that a\nlarge truck ahead of us is reversing into the driveway of a dairy farm but a cow has got loose",
  "related to each other, not just variables with values. For example, we might notice that a\nlarge truck ahead of us is reversing into the driveway of a dairy farm but a cow has got loose\nand is blocking the truck’s path. A factored representation is unlikely to be pre-equipped\nwith the attribute TruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCow with\nvalue true or false. Instead, we would need a structured representation, in which ob-\nSTRUCTURED\nREPRESENTATION\njects such as cows and trucks and their various and varying relationships can be described\nexplicitly. (See Figure 2.16(c).) Structured representations underlie relational databases\nand ﬁrst-order logic (Chapters 8, 9, and 12), ﬁrst-order probability models (Chapter 14),\nknowledge-based learning (Chapter 19) and much of natural language understanding\n(Chapters 22 and 23). In fact, almost everything that humans express in natural language\nconcerns objects and their relationships.\nAs we mentioned earlier, the axis along which atomic, factored, and structured repre-\nsentations lie is the axis of increasing expressiveness. Roughly speaking, a more expressive\nEXPRESSIVENESS\nrepresentation can capture, at least as concisely, everything a less expressive one can capture,\nplus some more. Often, the more expressive language is much more concise; for example, the\nrules of chess can be written in a page or two of a structured-representation language such\nas ﬁrst-order logic but require thousands of pages when written in a factored-representation\nlanguage such as propositional logic. On the other hand, reasoning and learning become\nmore complex as the expressive power of the representation increases. To gain the beneﬁts\nof expressive representations while avoiding their drawbacks, intelligent systems for the real\nworld may need to operate at all points along the axis simultaneously. Section 2.5.\nSummary\n59\n2.5\nSUMMARY\nThis chapter has been something of a whirlwind tour of AI, which we have conceived of as\nthe science of agent design. The major points to recall are as follows:\n• An agent is something that perceives and acts in an environment. The agent function\nfor an agent speciﬁes the action taken by the agent in response to any percept sequence.\n• The performance measure evaluates the behavior of the agent in an environment. A\nrational agent acts so as to maximize the expected value of the performance measure,\ngiven the percept sequence it has seen so far.",
  "• The performance measure evaluates the behavior of the agent in an environment. A\nrational agent acts so as to maximize the expected value of the performance measure,\ngiven the percept sequence it has seen so far.\n• A task environment speciﬁcation includes the performance measure, the external en-\nvironment, the actuators, and the sensors. In designing an agent, the ﬁrst step must\nalways be to specify the task environment as fully as possible.\n• Task environments vary along several signiﬁcant dimensions. They can be fully or\npartially observable, single-agent or multiagent, deterministic or stochastic, episodic or\nsequential, static or dynamic, discrete or continuous, and known or unknown.\n• The agent program implements the agent function. There exists a variety of basic\nagent-program designs reﬂecting the kind of information made explicit and used in the\ndecision process. The designs vary in efﬁciency, compactness, and ﬂexibility. The\nappropriate design of the agent program depends on the nature of the environment.\n• Simple reﬂex agents respond directly to percepts, whereas model-based reﬂex agents\nmaintain internal state to track aspects of the world that are not evident in the current\npercept. Goal-based agents act to achieve their goals, and utility-based agents try to\nmaximize their own expected “happiness.”\n• All agents can improve their performance through learning.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe central role of action in intelligence—the notion of practical reasoning—goes back at\nleast as far as Aristotle’s Nicomachean Ethics. Practical reasoning was also the subject of\nMcCarthy’s (1958) inﬂuential paper “Programs with Common Sense.” The ﬁelds of robotics\nand control theory are, by their very nature, concerned principally with physical agents. The\nconcept of a controller in control theory is identical to that of an agent in AI. Perhaps sur-\nCONTROLLER\nprisingly, AI has concentrated for most of its history on isolated components of agents—\nquestion-answering systems, theorem-provers, vision systems, and so on—rather than on\nwhole agents. The discussion of agents in the text by Genesereth and Nilsson (1987) was an\ninﬂuential exception. The whole-agent view is now widely accepted and is a central theme in\nrecent texts (Poole et al., 1998; Nilsson, 1998; Padgham and Winikoff, 2004; Jones, 2007).\nChapter 1 traced the roots of the concept of rationality in philosophy and economics. In",
  "recent texts (Poole et al., 1998; Nilsson, 1998; Padgham and Winikoff, 2004; Jones, 2007).\nChapter 1 traced the roots of the concept of rationality in philosophy and economics. In\nAI, the concept was of peripheral interest until the mid-1980s, when it began to suffuse many 60\nChapter\n2.\nIntelligent Agents\ndiscussions about the proper technical foundations of the ﬁeld. A paper by Jon Doyle (1983)\npredicted that rational agent design would come to be seen as the core mission of AI, while\nother popular topics would spin off to form new disciplines.\nCareful attention to the properties of the environment and their consequences for ra-\ntional agent design is most apparent in the control theory tradition—for example, classical\ncontrol systems (Dorf and Bishop, 2004; Kirk, 2004) handle fully observable, deterministic\nenvironments; stochastic optimal control (Kumar and Varaiya, 1986; Bertsekas and Shreve,\n2007) handles partially observable, stochastic environments; and hybrid control (Henzinger\nand Sastry, 1998; Cassandras and Lygeros, 2006) deals with environments containing both\ndiscrete and continuous elements. The distinction between fully and partially observable en-\nvironments is also central in the dynamic programming literature developed in the ﬁeld of\noperations research (Puterman, 1994), which we discuss in Chapter 17.\nReﬂex agents were the primary model for psychological behaviorists such as Skinner\n(1953), who attempted to reduce the psychology of organisms strictly to input/output or stim-\nulus/response mappings. The advance from behaviorism to functionalism in psychology,\nwhich was at least partly driven by the application of the computer metaphor to agents (Put-\nnam, 1960; Lewis, 1966), introduced the internal state of the agent into the picture. Most\nwork in AI views the idea of pure reﬂex agents with state as too simple to provide much\nleverage, but work by Rosenschein (1985) and Brooks (1986) questioned this assumption\n(see Chapter 25). In recent years, a great deal of work has gone into ﬁnding efﬁcient algo-\nrithms for keeping track of complex environments (Hamscher et al., 1992; Simon, 2006). The\nRemote Agent program (described on page 28) that controlled the Deep Space One spacecraft\nis a particularly impressive example (Muscettola et al., 1998; Jonsson et al., 2000).\nGoal-based agents are presupposed in everything from Aristotle’s view of practical rea-\nsoning to McCarthy’s early papers on logical AI. Shakey the Robot (Fikes and Nilsson,",
  "Goal-based agents are presupposed in everything from Aristotle’s view of practical rea-\nsoning to McCarthy’s early papers on logical AI. Shakey the Robot (Fikes and Nilsson,\n1971; Nilsson, 1984) was the ﬁrst robotic embodiment of a logical, goal-based agent. A\nfull logical analysis of goal-based agents appeared in Genesereth and Nilsson (1987), and a\ngoal-based programming methodology called agent-oriented programming was developed by\nShoham (1993). The agent-based approach is now extremely popular in software engineer-\ning (Ciancarini and Wooldridge, 2001). It has also inﬁltrated the area of operating systems,\nwhere autonomic computing refers to computer systems and networks that monitor and con-\nAUTONOMIC\nCOMPUTING\ntrol themselves with a perceive–act loop and machine learning methods (Kephart and Chess,\n2003). Noting that a collection of agent programs designed to work well together in a true\nmultiagent environment necessarily exhibits modularity—the programs share no internal state\nand communicate with each other only through the environment—it is common within the\nﬁeld of multiagent systems to design the agent program of a single agent as a collection of\nMULTIAGENT\nSYSTEMS\nautonomous sub-agents. In some cases, one can even prove that the resulting system gives\nthe same optimal solutions as a monolithic design.\nThe goal-based view of agents also dominates the cognitive psychology tradition in the\narea of problem solving, beginning with the enormously inﬂuential Human Problem Solv-\ning (Newell and Simon, 1972) and running through all of Newell’s later work (Newell, 1990).\nGoals, further analyzed as desires (general) and intentions (currently pursued), are central to\nthe theory of agents developed by Bratman (1987). This theory has been inﬂuential both in Exercises\n61\nnatural language understanding and multiagent systems.\nHorvitz et al. (1988) speciﬁcally suggest the use of rationality conceived as the maxi-\nmization of expected utility as a basis for AI. The text by Pearl (1988) was the ﬁrst in AI to\ncover probability and utility theory in depth; its exposition of practical methods for reasoning\nand decision making under uncertainty was probably the single biggest factor in the rapid\nshift towards utility-based agents in the 1990s (see Part IV).\nThe general design for learning agents portrayed in Figure 2.15 is classic in the machine\nlearning literature (Buchanan et al., 1978; Mitchell, 1997). Examples of the design, as em-",
  "shift towards utility-based agents in the 1990s (see Part IV).\nThe general design for learning agents portrayed in Figure 2.15 is classic in the machine\nlearning literature (Buchanan et al., 1978; Mitchell, 1997). Examples of the design, as em-\nbodied in programs, go back at least as far as Arthur Samuel’s (1959, 1967) learning program\nfor playing checkers. Learning agents are discussed in depth in Part V.\nInterest in agents and in agent design has risen rapidly in recent years, partly because of\nthe growth of the Internet and the perceived need for automated and mobile softbot (Etzioni\nand Weld, 1994). Relevant papers are collected in Readings in Agents (Huhns and Singh,\n1998) and Foundations of Rational Agency (Wooldridge and Rao, 1999). Texts on multiagent\nsystems usually provide a good introduction to many aspects of agent design (Weiss, 2000a;\nWooldridge, 2002). Several conference series devoted to agents began in the 1990s, including\nthe International Workshop on Agent Theories, Architectures, and Languages (ATAL), the\nInternational Conference on Autonomous Agents (AGENTS), and the International Confer-\nence on Multi-Agent Systems (ICMAS). In 2002, these three merged to form the International\nJoint Conference on Autonomous Agents and Multi-Agent Systems (AAMAS). The journal\nAutonomous Agents and Multi-Agent Systems was founded in 1998. Finally, Dung Beetle\nEcology (Hanski and Cambefort, 1991) provides a wealth of interesting information on the\nbehavior of dung beetles. YouTube features inspiring video recordings of their activities.\nEXERCISES\n2.1\nSuppose that the performance measure is concerned with just the ﬁrst T time steps of\nthe environment and ignores everything thereafter. Show that a rational agent’s action may\ndepend not just on the state of the environment but also on the time step it has reached.\n2.2\nLet us examine the rationality of various vacuum-cleaner agent functions.\na. Show that the simple vacuum-cleaner agent function described in Figure 2.3 is indeed\nrational under the assumptions listed on page 38.\nb. Describe a rational agent function for the case in which each movement costs one point.\nDoes the corresponding agent program require internal state?\nc. Discuss possible agent designs for the cases in which clean squares can become dirty\nand the geography of the environment is unknown. Does it make sense for the agent to\nlearn from its experience in these cases? If so, what should it learn? If not, why not?\n2.3",
  "and the geography of the environment is unknown. Does it make sense for the agent to\nlearn from its experience in these cases? If so, what should it learn? If not, why not?\n2.3\nFor each of the following assertions, say whether it is true or false and support your\nanswer with examples or counterexamples where appropriate.\na. An agent that senses only partial information about the state cannot be perfectly rational. 62\nChapter\n2.\nIntelligent Agents\nb. There exist task environments in which no pure reﬂex agent can behave rationally.\nc. There exists a task environment in which every agent is rational.\nd. The input to an agent program is the same as the input to the agent function.\ne. Every agent function is implementable by some program/machine combination.\nf. Suppose an agent selects its action uniformly at random from the set of possible actions.\nThere exists a deterministic task environment in which this agent is rational.\ng. It is possible for a given agent to be perfectly rational in two distinct task environments.\nh. Every agent is rational in an unobservable environment.\ni. A perfectly rational poker-playing agent never loses.\n2.4\nFor each of the following activities, give a PEAS description of the task environment\nand characterize it in terms of the properties listed in Section 2.3.2.\n• Playing soccer.\n• Exploring the subsurface oceans of Titan.\n• Shopping for used AI books on the Internet.\n• Playing a tennis match.\n• Practicing tennis against a wall.\n• Performing a high jump.\n• Knitting a sweater.\n• Bidding on an item at an auction.\n2.5\nDeﬁne in your own words the following terms: agent, agent function, agent program,\nrationality, autonomy, reﬂex agent, model-based agent, goal-based agent, utility-based agent,\nlearning agent.\n2.6\nThis exercise explores the differences between agent functions and agent programs.\na. Can there be more than one agent program that implements a given agent function?\nGive an example, or show why one is not possible.\nb. Are there agent functions that cannot be implemented by any agent program?\nc. Given a ﬁxed machine architecture, does each agent program implement exactly one\nagent function?\nd. Given an architecture with n bits of storage, how many different possible agent pro-\ngrams are there?\ne. Suppose we keep the agent program ﬁxed but speed up the machine by a factor of two.\nDoes that change the agent function?\n2.7\nWrite pseudocode agent programs for the goal-based and utility-based agents.",
  "grams are there?\ne. Suppose we keep the agent program ﬁxed but speed up the machine by a factor of two.\nDoes that change the agent function?\n2.7\nWrite pseudocode agent programs for the goal-based and utility-based agents.\nThe following exercises all concern the implementation of environments and agents for the\nvacuum-cleaner world. Exercises\n63\n2.8\nImplement a performance-measuring environment simulator for the vacuum-cleaner\nworld depicted in Figure 2.2 and speciﬁed on page 38. Your implementation should be modu-\nlar so that the sensors, actuators, and environment characteristics (size, shape, dirt placement,\netc.) can be changed easily. (Note: for some choices of programming language and operating\nsystem there are already implementations in the online code repository.)\n2.9\nImplement a simple reﬂex agent for the vacuum environment in Exercise 2.8. Run the\nenvironment with this agent for all possible initial dirt conﬁgurations and agent locations.\nRecord the performance score for each conﬁguration and the overall average score.\n2.10\nConsider a modiﬁed version of the vacuum environment in Exercise 2.8, in which the\nagent is penalized one point for each movement.\na. Can a simple reﬂex agent be perfectly rational for this environment? Explain.\nb. What about a reﬂex agent with state? Design such an agent.\nc. How do your answers to a and b change if the agent’s percepts give it the clean/dirty\nstatus of every square in the environment?\n2.11\nConsider a modiﬁed version of the vacuum environment in Exercise 2.8, in which the\ngeography of the environment—its extent, boundaries, and obstacles—is unknown, as is the\ninitial dirt conﬁguration. (The agent can go Up and Down as well as Left and Right.)\na. Can a simple reﬂex agent be perfectly rational for this environment? Explain.\nb. Can a simple reﬂex agent with a randomized agent function outperform a simple reﬂex\nagent? Design such an agent and measure its performance on several environments.\nc. Can you design an environment in which your randomized agent will perform poorly?\nShow your results.\nd. Can a reﬂex agent with state outperform a simple reﬂex agent? Design such an agent\nand measure its performance on several environments. Can you design a rational agent\nof this type?\n2.12\nRepeat Exercise 2.11 for the case in which the location sensor is replaced with a\n“bump” sensor that detects the agent’s attempts to move into an obstacle or to cross the",
  "of this type?\n2.12\nRepeat Exercise 2.11 for the case in which the location sensor is replaced with a\n“bump” sensor that detects the agent’s attempts to move into an obstacle or to cross the\nboundaries of the environment. Suppose the bump sensor stops working; how should the\nagent behave?\n2.13\nThe vacuum environments in the preceding exercises have all been deterministic. Dis-\ncuss possible agent programs for each of the following stochastic versions:\na. Murphy’s law: twenty-ﬁve percent of the time, the Suck action fails to clean the ﬂoor if\nit is dirty and deposits dirt onto the ﬂoor if the ﬂoor is clean. How is your agent program\naffected if the dirt sensor gives the wrong answer 10% of the time?\nb. Small children: At each time step, each clean square has a 10% chance of becoming\ndirty. Can you come up with a rational agent design for this case? 3\nSOLVING PROBLEMS BY\nSEARCHING\nIn which we see how an agent can ﬁnd a sequence of actions that achieves its\ngoals when no single action will do.\nThe simplest agents discussed in Chapter 2 were the reﬂex agents, which base their actions on\na direct mapping from states to actions. Such agents cannot operate well in environments for\nwhich this mapping would be too large to store and would take too long to learn. Goal-based\nagents, on the other hand, consider future actions and the desirability of their outcomes.\nThis chapter describes one kind of goal-based agent called a problem-solving agent.\nPROBLEM-SOLVING\nAGENT\nProblem-solving agents use atomic representations, as described in Section 2.4.7—that is,\nstates of the world are considered as wholes, with no internal structure visible to the problem-\nsolving algorithms. Goal-based agents that use more advanced factored or structured rep-\nresentations are usually called planning agents and are discussed in Chapters 7 and 10.\nOur discussion of problem solving begins with precise deﬁnitions of problems and their\nsolutions and give several examples to illustrate these deﬁnitions. We then describe several\ngeneral-purpose search algorithms that can be used to solve these problems. We will see\nseveral uninformed search algorithms—algorithms that are given no information about the\nproblem other than its deﬁnition. Although some of these algorithms can solve any solvable\nproblem, none of them can do so efﬁciently. Informed search algorithms, on the other hand,\ncan do quite well given some guidance on where to look for solutions.",
  "problem, none of them can do so efﬁciently. Informed search algorithms, on the other hand,\ncan do quite well given some guidance on where to look for solutions.\nIn this chapter, we limit ourselves to the simplest kind of task environment, for which\nthe solution to a problem is always a ﬁxed sequence of actions. The more general case—where\nthe agent’s future actions may vary depending on future percepts—is handled in Chapter 4.\nThis chapter uses the concepts of asymptotic complexity (that is, O() notation) and\nNP-completeness. Readers unfamiliar with these concepts should consult Appendix A.\n3.1\nPROBLEM-SOLVING AGENTS\nIntelligent agents are supposed to maximize their performance measure. As we mentioned\nin Chapter 2, achieving this is sometimes simpliﬁed if the agent can adopt a goal and aim at\nsatisfying it. Let us ﬁrst look at why and how an agent might do this.\n64 Section 3.1.\nProblem-Solving Agents\n65\nImagine an agent in the city of Arad, Romania, enjoying a touring holiday. The agent’s\nperformance measure contains many factors: it wants to improve its suntan, improve its Ro-\nmanian, take in the sights, enjoy the nightlife (such as it is), avoid hangovers, and so on. The\ndecision problem is a complex one involving many tradeoffs and careful reading of guide-\nbooks. Now, suppose the agent has a nonrefundable ticket to ﬂy out of Bucharest the follow-\ning day. In that case, it makes sense for the agent to adopt the goal of getting to Bucharest.\nCourses of action that don’t reach Bucharest on time can be rejected without further consid-\neration and the agent’s decision problem is greatly simpliﬁed. Goals help organize behavior\nby limiting the objectives that the agent is trying to achieve and hence the actions it needs\nto consider. Goal formulation, based on the current situation and the agent’s performance\nGOAL FORMULATION\nmeasure, is the ﬁrst step in problem solving.\nWe will consider a goal to be a set of world states—exactly those states in which the\ngoal is satisﬁed. The agent’s task is to ﬁnd out how to act, now and in the future, so that it\nreaches a goal state. Before it can do this, it needs to decide (or we need to decide on its\nbehalf) what sorts of actions and states it should consider. If it were to consider actions at\nthe level of “move the left foot forward an inch” or “turn the steering wheel one degree left,”\nthe agent would probably never ﬁnd its way out of the parking lot, let alone to Bucharest,",
  "the level of “move the left foot forward an inch” or “turn the steering wheel one degree left,”\nthe agent would probably never ﬁnd its way out of the parking lot, let alone to Bucharest,\nbecause at that level of detail there is too much uncertainty in the world and there would be\ntoo many steps in a solution. Problem formulation is the process of deciding what actions\nPROBLEM\nFORMULATION\nand states to consider, given a goal. We discuss this process in more detail later. For now, let\nus assume that the agent will consider actions at the level of driving from one major town to\nanother. Each state therefore corresponds to being in a particular town.\nOur agent has now adopted the goal of driving to Bucharest and is considering where\nto go from Arad. Three roads lead out of Arad, one toward Sibiu, one to Timisoara, and one\nto Zerind. None of these achieves the goal, so unless the agent is familiar with the geography\nof Romania, it will not know which road to follow.1 In other words, the agent will not know\nwhich of its possible actions is best, because it does not yet know enough about the state\nthat results from taking each action. If the agent has no additional information—i.e., if the\nenvironment is unknown in the sense deﬁned in Section 2.3—then it is has no choice but to\ntry one of the actions at random. This sad situation is discussed in Chapter 4.\nBut suppose the agent has a map of Romania. The point of a map is to provide the\nagent with information about the states it might get itself into and the actions it can take. The\nagent can use this information to consider subsequent stages of a hypothetical journey via\neach of the three towns, trying to ﬁnd a journey that eventually gets to Bucharest. Once it has\nfound a path on the map from Arad to Bucharest, it can achieve its goal by carrying out the\ndriving actions that correspond to the legs of the journey. In general, an agent with several\nimmediate options of unknown value can decide what to do by ﬁrst examining future actions\nthat eventually lead to states of known value.\nTo be more speciﬁc about what we mean by “examining future actions,” we have to\nbe more speciﬁc about properties of the environment, as deﬁned in Section 2.3. For now,\n1 We are assuming that most readers are in the same position and can easily imagine themselves to be as clueless\nas our agent. We apologize to Romanian readers who are unable to take advantage of this pedagogical device. 66\nChapter\n3.\nSolving Problems by Searching",
  "as our agent. We apologize to Romanian readers who are unable to take advantage of this pedagogical device. 66\nChapter\n3.\nSolving Problems by Searching\nwe assume that the environment is observable, so the agent always knows the current state.\nFor the agent driving in Romania, it’s reasonable to suppose that each city on the map has a\nsign indicating its presence to arriving drivers. We also assume the environment is discrete,\nso at any given state there are only ﬁnitely many actions to choose from. This is true for\nnavigating in Romania because each city is connected to a small number of other cities. We\nwill assume the environment is known, so the agent knows which states are reached by each\naction. (Having an accurate map sufﬁces to meet this condition for navigation problems.)\nFinally, we assume that the environment is deterministic, so each action has exactly one\noutcome. Under ideal conditions, this is true for the agent in Romania—it means that if it\nchooses to drive from Arad to Sibiu, it does end up in Sibiu. Of course, conditions are not\nalways ideal, as we show in Chapter 4.\nUnder these assumptions, the solution to any problem is a ﬁxed sequence of actions.\n“Of course!” one might say, “What else could it be?” Well, in general it could be a branching\nstrategy that recommends different actions in the future depending on what percepts arrive.\nFor example, under less than ideal conditions, the agent might plan to drive from Arad to\nSibiu and then to Rimnicu Vilcea but may also need to have a contingency plan in case it\narrives by accident in Zerind instead of Sibiu. Fortunately, if the agent knows the initial state\nand the environment is known and deterministic, it knows exactly where it will be after the\nﬁrst action and what it will perceive. Since only one percept is possible after the ﬁrst action,\nthe solution can specify only one possible second action, and so on.\nThe process of looking for a sequence of actions that reaches the goal is called search.\nSEARCH\nA search algorithm takes a problem as input and returns a solution in the form of an action\nSOLUTION\nsequence. Once a solution is found, the actions it recommends can be carried out. This\nis called the execution phase. Thus, we have a simple “formulate, search, execute” design\nEXECUTION\nfor the agent, as shown in Figure 3.1. After formulating a goal and a problem to solve,\nthe agent calls a search procedure to solve it. It then uses the solution to guide its actions,",
  "EXECUTION\nfor the agent, as shown in Figure 3.1. After formulating a goal and a problem to solve,\nthe agent calls a search procedure to solve it. It then uses the solution to guide its actions,\ndoing whatever the solution recommends as the next thing to do—typically, the ﬁrst action of\nthe sequence—and then removing that step from the sequence. Once the solution has been\nexecuted, the agent will formulate a new goal.\nNotice that while the agent is executing the solution sequence it ignores its percepts\nwhen choosing an action because it knows in advance what they will be. An agent that\ncarries out its plans with its eyes closed, so to speak, must be quite certain of what is going\non. Control theorists call this an open-loop system, because ignoring the percepts breaks the\nOPEN-LOOP\nloop between agent and environment.\nWe ﬁrst describe the process of problem formulation, and then devote the bulk of the\nchapter to various algorithms for the SEARCH function. We do not discuss the workings of\nthe UPDATE-STATE and FORMULATE-GOAL functions further in this chapter.\n3.1.1\nWell-deﬁned problems and solutions\nA problem can be deﬁned formally by ﬁve components:\nPROBLEM\n• The initial state that the agent starts in. For example, the initial state for our agent in\nINITIAL STATE\nRomania might be described as In(Arad). Section 3.1.\nProblem-Solving Agents\n67\nfunction SIMPLE-PROBLEM-SOLVING-AGENT(percept) returns an action\npersistent: seq, an action sequence, initially empty\nstate, some description of the current world state\ngoal, a goal, initially null\nproblem, a problem formulation\nstate ←UPDATE-STATE(state,percept)\nif seq is empty then\ngoal ←FORMULATE-GOAL(state)\nproblem ←FORMULATE-PROBLEM(state,goal)\nseq ←SEARCH(problem)\nif seq = failure then return a null action\naction ←FIRST(seq)\nseq ←REST(seq)\nreturn action\nFigure 3.1\nA simple problem-solving agent. It ﬁrst formulates a goal and a problem,\nsearches for a sequence of actions that would solve the problem, and then executes the actions\none at a time. When this is complete, it formulates another goal and starts over.\n• A description of the possible actions available to the agent. Given a particular state s,\nACTIONS\nACTIONS(s) returns the set of actions that can be executed in s. We say that each of\nthese actions is applicable in s. For example, from the state In(Arad), the applicable\nAPPLICABLE\nactions are {Go(Sibiu), Go(Timisoara), Go(Zerind)}.",
  "ACTIONS\nACTIONS(s) returns the set of actions that can be executed in s. We say that each of\nthese actions is applicable in s. For example, from the state In(Arad), the applicable\nAPPLICABLE\nactions are {Go(Sibiu), Go(Timisoara), Go(Zerind)}.\n• A description of what each action does; the formal name for this is the transition\nmodel, speciﬁed by a function RESULT(s, a) that returns the state that results from\nTRANSITION MODEL\ndoing action a in state s. We also use the term successor to refer to any state reachable\nSUCCESSOR\nfrom a given state by a single action.2 For example, we have\nRESULT(In(Arad), Go(Zerind)) = In(Zerind) .\nTogether, the initial state, actions, and transition model implicitly deﬁne the state space\nSTATE SPACE\nof the problem—the set of all states reachable from the initial state by any sequence\nof actions. The state space forms a directed network or graph in which the nodes\nGRAPH\nare states and the links between nodes are actions. (The map of Romania shown in\nFigure 3.2 can be interpreted as a state-space graph if we view each road as standing\nfor two driving actions, one in each direction.) A path in the state space is a sequence\nPATH\nof states connected by a sequence of actions.\n• The goal test, which determines whether a given state is a goal state. Sometimes there\nGOAL TEST\nis an explicit set of possible goal states, and the test simply checks whether the given\nstate is one of them. The agent’s goal in Romania is the singleton set {In(Bucharest)}.\n2 Many treatments of problem solving, including previous editions of this book, use a successor function, which\nreturns the set of all successors, instead of separate ACTIONS and RESULT functions. The successor function\nmakes it difﬁcult to describe an agent that knows what actions it can try but not what they achieve. Also, note\nsome author use RESULT(a, s) instead of RESULT(s, a), and some use DO instead of RESULT. 68\nChapter\n3.\nSolving Problems by Searching\nGiurgiu\nUrziceni\nHirsova\nEforie\nNeamt\nOradea\nZerind\nArad\nTimisoara\nLugoj\nMehadia\nDrobeta\nCraiova\nSibiu\nFagaras\nPitesti\nVaslui\nIasi\nRimnicu Vilcea\nBucharest\n71\n75\n118\n111\n70\n75\n120\n151\n140\n99\n80\n97\n101\n211\n138\n146\n85\n90\n98\n142\n92\n87\n86\nFigure 3.2\nA simpliﬁed road map of part of Romania.\nSometimes the goal is speciﬁed by an abstract property rather than an explicitly enumer-\nated set of states. For example, in chess, the goal is to reach a state called “checkmate,”\nwhere the opponent’s king is under attack and can’t escape.",
  "Sometimes the goal is speciﬁed by an abstract property rather than an explicitly enumer-\nated set of states. For example, in chess, the goal is to reach a state called “checkmate,”\nwhere the opponent’s king is under attack and can’t escape.\n• A path cost function that assigns a numeric cost to each path. The problem-solving\nPATH COST\nagent chooses a cost function that reﬂects its own performance measure. For the agent\ntrying to get to Bucharest, time is of the essence, so the cost of a path might be its length\nin kilometers. In this chapter, we assume that the cost of a path can be described as the\nsum of the costs of the individual actions along the path.3 The step cost of taking action\nSTEP COST\na in state s to reach state s′ is denoted by c(s, a, s′). The step costs for Romania are\nshown in Figure 3.2 as route distances. We assume that step costs are nonnegative.4\nThe preceding elements deﬁne a problem and can be gathered into a single data structure\nthat is given as input to a problem-solving algorithm. A solution to a problem is an action\nsequence that leads from the initial state to a goal state. Solution quality is measured by the\npath cost function, and an optimal solution has the lowest path cost among all solutions.\nOPTIMAL SOLUTION\n3.1.2\nFormulating problems\nIn the preceding section we proposed a formulation of the problem of getting to Bucharest in\nterms of the initial state, actions, transition model, goal test, and path cost. This formulation\nseems reasonable, but it is still a model—an abstract mathematical description—and not the\n3 This assumption is algorithmically convenient but also theoretically justiﬁable—see page 649 in Chapter 17.\n4 The implications of negative costs are explored in Exercise 3.8. Section 3.2.\nExample Problems\n69\nreal thing. Compare the simple state description we have chosen, In(Arad), to an actual cross-\ncountry trip, where the state of the world includes so many things: the traveling companions,\nthe current radio program, the scenery out of the window, the proximity of law enforcement\nofﬁcers, the distance to the next rest stop, the condition of the road, the weather, and so on.\nAll these considerations are left out of our state descriptions because they are irrelevant to the\nproblem of ﬁnding a route to Bucharest. The process of removing detail from a representation\nis called abstraction.\nABSTRACTION\nIn addition to abstracting the state description, we must abstract the actions themselves.",
  "problem of ﬁnding a route to Bucharest. The process of removing detail from a representation\nis called abstraction.\nABSTRACTION\nIn addition to abstracting the state description, we must abstract the actions themselves.\nA driving action has many effects. Besides changing the location of the vehicle and its oc-\ncupants, it takes up time, consumes fuel, generates pollution, and changes the agent (as they\nsay, travel is broadening). Our formulation takes into account only the change in location.\nAlso, there are many actions that we omit altogether: turning on the radio, looking out of\nthe window, slowing down for law enforcement ofﬁcers, and so on. And of course, we don’t\nspecify actions at the level of “turn steering wheel to the left by one degree.”\nCan we be more precise about deﬁning the appropriate level of abstraction? Think of the\nabstract states and actions we have chosen as corresponding to large sets of detailed world\nstates and detailed action sequences. Now consider a solution to the abstract problem: for\nexample, the path from Arad to Sibiu to Rimnicu Vilcea to Pitesti to Bucharest. This abstract\nsolution corresponds to a large number of more detailed paths. For example, we could drive\nwith the radio on between Sibiu and Rimnicu Vilcea, and then switch it off for the rest of\nthe trip. The abstraction is valid if we can expand any abstract solution into a solution in the\nmore detailed world; a sufﬁcient condition is that for every detailed state that is “in Arad,”\nthere is a detailed path to some state that is “in Sibiu,” and so on.5 The abstraction is useful\nif carrying out each of the actions in the solution is easier than the original problem; in this\ncase they are easy enough that they can be carried out without further search or planning by\nan average driving agent. The choice of a good abstraction thus involves removing as much\ndetail as possible while retaining validity and ensuring that the abstract actions are easy to\ncarry out. Were it not for the ability to construct useful abstractions, intelligent agents would\nbe completely swamped by the real world.\n3.2\nEXAMPLE PROBLEMS\nThe problem-solving approach has been applied to a vast array of task environments. We\nlist some of the best known here, distinguishing between toy and real-world problems. A\ntoy problem is intended to illustrate or exercise various problem-solving methods. It can be\nTOY PROBLEM",
  "list some of the best known here, distinguishing between toy and real-world problems. A\ntoy problem is intended to illustrate or exercise various problem-solving methods. It can be\nTOY PROBLEM\ngiven a concise, exact description and hence is usable by different researchers to compare the\nperformance of algorithms. A real-world problem is one whose solutions people actually\nREAL-WORLD\nPROBLEM\ncare about. Such problems tend not to have a single agreed-upon description, but we can give\nthe general ﬂavor of their formulations.\n5 See Section 11.2 for a more complete set of deﬁnitions and algorithms. 70\nChapter\n3.\nSolving Problems by Searching\nR\nL\nS\nS\nS\nS\nR\nL\nR\nL\nR\nL\nS\nS\nS\nS\nL\nL\nL\nL\nR\nR\nR\nR\nFigure 3.3\nThe state space for the vacuum world. Links denote actions: L = Left, R =\nRight, S = Suck.\n3.2.1\nToy problems\nThe ﬁrst example we examine is the vacuum world ﬁrst introduced in Chapter 2. (See\nFigure 2.2.) This can be formulated as a problem as follows:\n• States: The state is determined by both the agent location and the dirt locations. The\nagent is in one of two locations, each of which might or might not contain dirt. Thus,\nthere are 2 × 22 = 8 possible world states. A larger environment with n locations has\nn · 2n states.\n• Initial state: Any state can be designated as the initial state.\n• Actions: In this simple environment, each state has just three actions: Left, Right, and\nSuck. Larger environments might also include Up and Down.\n• Transition model: The actions have their expected effects, except that moving Left in\nthe leftmost square, moving Right in the rightmost square, and Sucking in a clean square\nhave no effect. The complete state space is shown in Figure 3.3.\n• Goal test: This checks whether all the squares are clean.\n• Path cost: Each step costs 1, so the path cost is the number of steps in the path.\nCompared with the real world, this toy problem has discrete locations, discrete dirt, reliable\ncleaning, and it never gets any dirtier. Chapter 4 relaxes some of these assumptions.\nThe 8-puzzle, an instance of which is shown in Figure 3.4, consists of a 3×3 board with\n8-PUZZLE\neight numbered tiles and a blank space. A tile adjacent to the blank space can slide into the\nspace. The object is to reach a speciﬁed goal state, such as the one shown on the right of the\nﬁgure. The standard formulation is as follows: Section 3.2.\nExample Problems\n71\n2\nStart State\nGoal State\n1\n3\n4\n6\n7\n5\n1\n2\n3\n4\n6\n7\n8\n5\n8\nFigure 3.4\nA typical instance of the 8-puzzle.",
  "ﬁgure. The standard formulation is as follows: Section 3.2.\nExample Problems\n71\n2\nStart State\nGoal State\n1\n3\n4\n6\n7\n5\n1\n2\n3\n4\n6\n7\n8\n5\n8\nFigure 3.4\nA typical instance of the 8-puzzle.\n• States: A state description speciﬁes the location of each of the eight tiles and the blank\nin one of the nine squares.\n• Initial state: Any state can be designated as the initial state. Note that any given goal\ncan be reached from exactly half of the possible initial states (Exercise 3.4).\n• Actions: The simplest formulation deﬁnes the actions as movements of the blank space\nLeft, Right, Up, or Down. Different subsets of these are possible depending on where\nthe blank is.\n• Transition model: Given a state and action, this returns the resulting state; for example,\nif we apply Left to the start state in Figure 3.4, the resulting state has the 5 and the blank\nswitched.\n• Goal test: This checks whether the state matches the goal conﬁguration shown in Fig-\nure 3.4. (Other goal conﬁgurations are possible.)\n• Path cost: Each step costs 1, so the path cost is the number of steps in the path.\nWhat abstractions have we included here? The actions are abstracted to their beginning and\nﬁnal states, ignoring the intermediate locations where the block is sliding. We have abstracted\naway actions such as shaking the board when pieces get stuck and ruled out extracting the\npieces with a knife and putting them back again. We are left with a description of the rules of\nthe puzzle, avoiding all the details of physical manipulations.\nThe 8-puzzle belongs to the family of sliding-block puzzles, which are often used as\nSLIDING-BLOCK\nPUZZLES\ntest problems for new search algorithms in AI. This family is known to be NP-complete,\nso one does not expect to ﬁnd methods signiﬁcantly better in the worst case than the search\nalgorithms described in this chapter and the next. The 8-puzzle has 9!/2 = 181, 440 reachable\nstates and is easily solved. The 15-puzzle (on a 4×4 board) has around 1.3 trillion states, and\nrandom instances can be solved optimally in a few milliseconds by the best search algorithms.\nThe 24-puzzle (on a 5 × 5 board) has around 1025 states, and random instances take several\nhours to solve optimally.\nThe goal of the 8-queens problem is to place eight queens on a chessboard such that\n8-QUEENS PROBLEM\nno queen attacks any other. (A queen attacks any piece in the same row, column or diago-\nnal.) Figure 3.5 shows an attempted solution that fails: the queen in the rightmost column is",
  "8-QUEENS PROBLEM\nno queen attacks any other. (A queen attacks any piece in the same row, column or diago-\nnal.) Figure 3.5 shows an attempted solution that fails: the queen in the rightmost column is\nattacked by the queen at the top left. 72\nChapter\n3.\nSolving Problems by Searching\nFigure 3.5\nAlmost a solution to the 8-queens problem. (Solution is left as an exercise.)\nAlthough efﬁcient special-purpose algorithms exist for this problem and for the whole\nn-queens family, it remains a useful test problem for search algorithms. There are two main\nkinds of formulation. An incremental formulation involves operators that augment the state\nINCREMENTAL\nFORMULATION\ndescription, starting with an empty state; for the 8-queens problem, this means that each\naction adds a queen to the state. A complete-state formulation starts with all 8 queens on\nCOMPLETE-STATE\nFORMULATION\nthe board and moves them around. In either case, the path cost is of no interest because only\nthe ﬁnal state counts. The ﬁrst incremental formulation one might try is the following:\n• States: Any arrangement of 0 to 8 queens on the board is a state.\n• Initial state: No queens on the board.\n• Actions: Add a queen to any empty square.\n• Transition model: Returns the board with a queen added to the speciﬁed square.\n• Goal test: 8 queens are on the board, none attacked.\nIn this formulation, we have 64 · 63 · · · 57 ≈1.8 × 1014 possible sequences to investigate. A\nbetter formulation would prohibit placing a queen in any square that is already attacked:\n• States: All possible arrangements of n queens (0 ≤n ≤8), one per column in the\nleftmost n columns, with no queen attacking another.\n• Actions: Add a queen to any square in the leftmost empty column such that it is not\nattacked by any other queen.\nThis formulation reduces the 8-queens state space from 1.8 × 1014 to just 2,057, and solutions\nare easy to ﬁnd. On the other hand, for 100 queens the reduction is from roughly 10400 states\nto about 1052 states (Exercise 3.5)—a big improvement, but not enough to make the problem\ntractable. Section 4.1 describes the complete-state formulation, and Chapter 6 gives a simple\nalgorithm that solves even the million-queens problem with ease. Section 3.2.\nExample Problems\n73\nOur ﬁnal toy problem was devised by Donald Knuth (1964) and illustrates how inﬁnite\nstate spaces can arise. Knuth conjectured that, starting with the number 4, a sequence of fac-",
  "Example Problems\n73\nOur ﬁnal toy problem was devised by Donald Knuth (1964) and illustrates how inﬁnite\nstate spaces can arise. Knuth conjectured that, starting with the number 4, a sequence of fac-\ntorial, square root, and ﬂoor operations will reach any desired positive integer. For example,\nwe can reach 5 from 4 as follows:\n\u0003\n\u0004\n\u0005\n\u0005\n\u0006\n\u0007\b\t\n(4!)!\n\u000b\n= 5 .\nThe problem deﬁnition is very simple:\n• States: Positive numbers.\n• Initial state: 4.\n• Actions: Apply factorial, square root, or ﬂoor operation (factorial for integers only).\n• Transition model: As given by the mathematical deﬁnitions of the operations.\n• Goal test: State is the desired positive integer.\nTo our knowledge there is no bound on how large a number might be constructed in the pro-\ncess of reaching a given target—for example, the number 620,448,401,733,239,439,360,000\nis generated in the expression for 5—so the state space for this problem is inﬁnite. Such\nstate spaces arise frequently in tasks involving the generation of mathematical expressions,\ncircuits, proofs, programs, and other recursively deﬁned objects.\n3.2.2\nReal-world problems\nWe have already seen how the route-ﬁnding problem is deﬁned in terms of speciﬁed loca-\nROUTE-FINDING\nPROBLEM\ntions and transitions along links between them. Route-ﬁnding algorithms are used in a variety\nof applications. Some, such as Web sites and in-car systems that provide driving directions,\nare relatively straightforward extensions of the Romania example. Others, such as routing\nvideo streams in computer networks, military operations planning, and airline travel-planning\nsystems, involve much more complex speciﬁcations. Consider the airline travel problems that\nmust be solved by a travel-planning Web site:\n• States: Each state obviously includes a location (e.g., an airport) and the current time.\nFurthermore, because the cost of an action (a ﬂight segment) may depend on previous\nsegments, their fare bases, and their status as domestic or international, the state must\nrecord extra information about these “historical” aspects.\n• Initial state: This is speciﬁed by the user’s query.\n• Actions: Take any ﬂight from the current location, in any seat class, leaving after the\ncurrent time, leaving enough time for within-airport transfer if needed.\n• Transition model: The state resulting from taking a ﬂight will have the ﬂight’s desti-\nnation as the current location and the ﬂight’s arrival time as the current time.",
  "current time, leaving enough time for within-airport transfer if needed.\n• Transition model: The state resulting from taking a ﬂight will have the ﬂight’s desti-\nnation as the current location and the ﬂight’s arrival time as the current time.\n• Goal test: Are we at the ﬁnal destination speciﬁed by the user?\n• Path cost: This depends on monetary cost, waiting time, ﬂight time, customs and im-\nmigration procedures, seat quality, time of day, type of airplane, frequent-ﬂyer mileage\nawards, and so on. 74\nChapter\n3.\nSolving Problems by Searching\nCommercial travel advice systems use a problem formulation of this kind, with many addi-\ntional complications to handle the byzantine fare structures that airlines impose. Any sea-\nsoned traveler knows, however, that not all air travel goes according to plan. A really good\nsystem should include contingency plans—such as backup reservations on alternate ﬂights—\nto the extent that these are justiﬁed by the cost and likelihood of failure of the original plan.\nTouring problems are closely related to route-ﬁnding problems, but with an impor-\nTOURING PROBLEM\ntant difference. Consider, for example, the problem “Visit every city in Figure 3.2 at least\nonce, starting and ending in Bucharest.”\nAs with route ﬁnding, the actions correspond\nto trips between adjacent cities. The state space, however, is quite different. Each state\nmust include not just the current location but also the set of cities the agent has visited.\nSo the initial state would be In(Bucharest), Visited({Bucharest}), a typical intermedi-\nate state would be In(Vaslui), Visited({Bucharest, Urziceni, Vaslui}), and the goal test\nwould check whether the agent is in Bucharest and all 20 cities have been visited.\nThe traveling salesperson problem (TSP) is a touring problem in which each city\nTRAVELING\nSALESPERSON\nPROBLEM\nmust be visited exactly once. The aim is to ﬁnd the shortest tour. The problem is known to\nbe NP-hard, but an enormous amount of effort has been expended to improve the capabilities\nof TSP algorithms. In addition to planning trips for traveling salespersons, these algorithms\nhave been used for tasks such as planning movements of automatic circuit-board drills and of\nstocking machines on shop ﬂoors.\nA VLSI layout problem requires positioning millions of components and connections\nVLSI LAYOUT\non a chip to minimize area, minimize circuit delays, minimize stray capacitances, and max-",
  "stocking machines on shop ﬂoors.\nA VLSI layout problem requires positioning millions of components and connections\nVLSI LAYOUT\non a chip to minimize area, minimize circuit delays, minimize stray capacitances, and max-\nimize manufacturing yield. The layout problem comes after the logical design phase and is\nusually split into two parts: cell layout and channel routing. In cell layout, the primitive\ncomponents of the circuit are grouped into cells, each of which performs some recognized\nfunction. Each cell has a ﬁxed footprint (size and shape) and requires a certain number of\nconnections to each of the other cells. The aim is to place the cells on the chip so that they do\nnot overlap and so that there is room for the connecting wires to be placed between the cells.\nChannel routing ﬁnds a speciﬁc route for each wire through the gaps between the cells. These\nsearch problems are extremely complex, but deﬁnitely worth solving. Later in this chapter,\nwe present some algorithms capable of solving them.\nRobot navigation is a generalization of the route-ﬁnding problem described earlier.\nROBOT NAVIGATION\nRather than following a discrete set of routes, a robot can move in a continuous space with\n(in principle) an inﬁnite set of possible actions and states. For a circular robot moving on a\nﬂat surface, the space is essentially two-dimensional. When the robot has arms and legs or\nwheels that must also be controlled, the search space becomes many-dimensional. Advanced\ntechniques are required just to make the search space ﬁnite. We examine some of these\nmethods in Chapter 25. In addition to the complexity of the problem, real robots must also\ndeal with errors in their sensor readings and motor controls.\nAutomatic assembly sequencing of complex objects by a robot was ﬁrst demonstrated\nAUTOMATIC\nASSEMBLY\nSEQUENCING\nby FREDDY (Michie, 1972). Progress since then has been slow but sure, to the point where\nthe assembly of intricate objects such as electric motors is economically feasible. In assembly\nproblems, the aim is to ﬁnd an order in which to assemble the parts of some object. If the\nwrong order is chosen, there will be no way to add some part later in the sequence without Section 3.3.\nSearching for Solutions\n75\nundoing some of the work already done. Checking a step in the sequence for feasibility is a\ndifﬁcult geometrical search problem closely related to robot navigation. Thus, the generation",
  "Searching for Solutions\n75\nundoing some of the work already done. Checking a step in the sequence for feasibility is a\ndifﬁcult geometrical search problem closely related to robot navigation. Thus, the generation\nof legal actions is the expensive part of assembly sequencing. Any practical algorithm must\navoid exploring all but a tiny fraction of the state space. Another important assembly problem\nis protein design, in which the goal is to ﬁnd a sequence of amino acids that will fold into a\nPROTEIN DESIGN\nthree-dimensional protein with the right properties to cure some disease.\n3.3\nSEARCHING FOR SOLUTIONS\nHaving formulated some problems, we now need to solve them. A solution is an action\nsequence, so search algorithms work by considering various possible action sequences. The\npossible action sequences starting at the initial state form a search tree with the initial state\nSEARCH TREE\nat the root; the branches are actions and the nodes correspond to states in the state space of\nNODE\nthe problem. Figure 3.6 shows the ﬁrst few steps in growing the search tree for ﬁnding a route\nfrom Arad to Bucharest. The root node of the tree corresponds to the initial state, In(Arad).\nThe ﬁrst step is to test whether this is a goal state. (Clearly it is not, but it is important to\ncheck so that we can solve trick problems like “starting in Arad, get to Arad.”) Then we\nneed to consider taking various actions. We do this by expanding the current state; that is,\nEXPANDING\napplying each legal action to the current state, thereby generating a new set of states. In\nGENERATING\nthis case, we add three branches from the parent node In(Arad) leading to three new child\nPARENT NODE\nnodes: In(Sibiu), In(Timisoara), and In(Zerind). Now we must choose which of these three\nCHILD NODE\npossibilities to consider further.\nThis is the essence of search—following up one option now and putting the others aside\nfor later, in case the ﬁrst choice does not lead to a solution. Suppose we choose Sibiu ﬁrst.\nWe check to see whether it is a goal state (it is not) and then expand it to get In(Arad),\nIn(Fagaras), In(Oradea), and In(RimnicuVilcea). We can then choose any of these four or go\nback and choose Timisoara or Zerind. Each of these six nodes is a leaf node, that is, a node\nLEAF NODE\nwith no children in the tree. The set of all leaf nodes available for expansion at any given\npoint is called the frontier. (Many authors call it the open list, which is both geographically\nFRONTIER\nOPEN LIST",
  "LEAF NODE\nwith no children in the tree. The set of all leaf nodes available for expansion at any given\npoint is called the frontier. (Many authors call it the open list, which is both geographically\nFRONTIER\nOPEN LIST\nless evocative and less accurate, because other data structures are better suited than a list.) In\nFigure 3.6, the frontier of each tree consists of those nodes with bold outlines.\nThe process of expanding nodes on the frontier continues until either a solution is found\nor there are no more states to expand. The general TREE-SEARCH algorithm is shown infor-\nmally in Figure 3.7. Search algorithms all share this basic structure; they vary primarily\naccording to how they choose which state to expand next—the so-called search strategy.\nSEARCH STRATEGY\nThe eagle-eyed reader will notice one peculiar thing about the search tree shown in Fig-\nure 3.6: it includes the path from Arad to Sibiu and back to Arad again! We say that In(Arad)\nis a repeated state in the search tree, generated in this case by a loopy path. Considering\nREPEATED STATE\nLOOPY PATH\nsuch loopy paths means that the complete search tree for Romania is inﬁnite because there\nis no limit to how often one can traverse a loop. On the other hand, the state space—the\nmap shown in Figure 3.2—has only 20 states. As we discuss in Section 3.4, loops can cause 76\nChapter\n3.\nSolving Problems by Searching\ncertain algorithms to fail, making otherwise solvable problems unsolvable. Fortunately, there\nis no need to consider loopy paths. We can rely on more than intuition for this: because path\ncosts are additive and step costs are nonnegative, a loopy path to any given state is never\nbetter than the same path with the loop removed.\nLoopy paths are a special case of the more general concept of redundant paths, which\nREDUNDANT PATH\nexist whenever there is more than one way to get from one state to another. Consider the paths\nArad–Sibiu (140 km long) and Arad–Zerind–Oradea–Sibiu (297 km long). Obviously, the\nsecond path is redundant—it’s just a worse way to get to the same state. If you are concerned\nabout reaching the goal, there’s never any reason to keep more than one path to any given\nstate, because any goal state that is reachable by extending one path is also reachable by\nextending the other.\nIn some cases, it is possible to deﬁne the problem itself so as to eliminate redundant\npaths. For example, if we formulate the 8-queens problem (page 71) so that a queen can be",
  "extending the other.\nIn some cases, it is possible to deﬁne the problem itself so as to eliminate redundant\npaths. For example, if we formulate the 8-queens problem (page 71) so that a queen can be\nplaced in any column, then each state with n queens can be reached by n! different paths; but\nif we reformulate the problem so that each new queen is placed in the leftmost empty column,\nthen each state can be reached only through one path.\n(a) The initial state\n(b) After expanding Arad\n(c) After expanding Sibiu\nRimnicu Vilcea\nLugoj\nArad\nFagaras\nOradea\nArad\nArad\nOradea\nRimnicu Vilcea\nLugoj\nZerind\nSibiu\nArad\nFagaras\nOradea\nTimisoara\nArad\nArad\nOradea\nLugoj\nArad\nArad\nOradea\nZerind\nArad\nSibiu\nTimisoara\nArad\nRimnicu Vilcea\nZerind\nArad\nSibiu\nArad\nFagaras\nOradea\nTimisoara\nFigure 3.6\nPartial search trees for ﬁnding a route from Arad to Bucharest. Nodes that\nhave been expanded are shaded; nodes that have been generated but not yet expanded are\noutlined in bold; nodes that have not yet been generated are shown in faint dashed lines. Section 3.3.\nSearching for Solutions\n77\nfunction TREE-SEARCH(problem) returns a solution, or failure\ninitialize the frontier using the initial state of problem\nloop do\nif the frontier is empty then return failure\nchoose a leaf node and remove it from the frontier\nif the node contains a goal state then return the corresponding solution\nexpand the chosen node, adding the resulting nodes to the frontier\nfunction GRAPH-SEARCH(problem) returns a solution, or failure\ninitialize the frontier using the initial state of problem\ninitialize the explored set to be empty\nloop do\nif the frontier is empty then return failure\nchoose a leaf node and remove it from the frontier\nif the node contains a goal state then return the corresponding solution\nadd the node to the explored set\nexpand the chosen node, adding the resulting nodes to the frontier\nonly if not in the frontier or explored set\nFigure 3.7\nAn informal description of the general tree-search and graph-search algo-\nrithms. The parts of GRAPH-SEARCH marked in bold italic are the additions needed to\nhandle repeated states.\nIn other cases, redundant paths are unavoidable.\nThis includes all problems where\nthe actions are reversible, such as route-ﬁnding problems and sliding-block puzzles. Route-\nﬁnding on a rectangular grid (like the one used later for Figure 3.9) is a particularly impor-\nRECTANGULAR GRID\ntant example in computer games. In such a grid, each state has four successors, so a search",
  "ﬁnding on a rectangular grid (like the one used later for Figure 3.9) is a particularly impor-\nRECTANGULAR GRID\ntant example in computer games. In such a grid, each state has four successors, so a search\ntree of depth d that includes repeated states has 4d leaves; but there are only about 2d2 distinct\nstates within d steps of any given state. For d = 20, this means about a trillion nodes but only\nabout 800 distinct states. Thus, following redundant paths can cause a tractable problem to\nbecome intractable. This is true even for algorithms that know how to avoid inﬁnite loops.\nAs the saying goes, algorithms that forget their history are doomed to repeat it.\nThe\nway to avoid exploring redundant paths is to remember where one has been. To do this, we\naugment the TREE-SEARCH algorithm with a data structure called the explored set (also\nEXPLORED SET\nknown as the closed list), which remembers every expanded node. Newly generated nodes\nCLOSED LIST\nthat match previously generated nodes—ones in the explored set or the frontier—can be dis-\ncarded instead of being added to the frontier. The new algorithm, called GRAPH-SEARCH, is\nshown informally in Figure 3.7. The speciﬁc algorithms in this chapter draw on this general\ndesign.\nClearly, the search tree constructed by the GRAPH-SEARCH algorithm contains at most\none copy of each state, so we can think of it as growing a tree directly on the state-space graph,\nas shown in Figure 3.8. The algorithm has another nice property: the frontier separates the\nSEPARATOR\nstate-space graph into the explored region and the unexplored region, so that every path from 78\nChapter\n3.\nSolving Problems by Searching\nFigure 3.8\nA sequence of search trees generated by a graph search on the Romania prob-\nlem of Figure 3.2. At each stage, we have extended each path by one step. Notice that at the\nthird stage, the northernmost city (Oradea) has become a dead end: both of its successors are\nalready explored via other paths.\n(c)\n(b)\n(a)\nFigure 3.9\nThe separation property of GRAPH-SEARCH, illustrated on a rectangular-grid\nproblem. The frontier (white nodes) always separates the explored region of the state space\n(black nodes) from the unexplored region (gray nodes). In (a), just the root has been ex-\npanded. In (b), one leaf node has been expanded. In (c), the remaining successors of the root\nhave been expanded in clockwise order.\nthe initial state to an unexplored state has to pass through a state in the frontier. (If this",
  "panded. In (b), one leaf node has been expanded. In (c), the remaining successors of the root\nhave been expanded in clockwise order.\nthe initial state to an unexplored state has to pass through a state in the frontier. (If this\nseems completely obvious, try Exercise 3.13 now.) This property is illustrated in Figure 3.9.\nAs every step moves a state from the frontier into the explored region while moving some\nstates from the unexplored region into the frontier, we see that the algorithm is systematically\nexamining the states in the state space, one by one, until it ﬁnds a solution.\n3.3.1\nInfrastructure for search algorithms\nSearch algorithms require a data structure to keep track of the search tree that is being con-\nstructed. For each node n of the tree, we have a structure that contains four components:\n• n.STATE: the state in the state space to which the node corresponds;\n• n.PARENT: the node in the search tree that generated this node;\n• n.ACTION: the action that was applied to the parent to generate the node;\n• n.PATH-COST: the cost, traditionally denoted by g(n), of the path from the initial state\nto the node, as indicated by the parent pointers. Section 3.3.\nSearching for Solutions\n79\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\nNode\nSTATE\nPARENT\nACTION = Right\nPATH-COST = 6\nFigure 3.10\nNodes are the data structures from which the search tree is constructed. Each\nhas a parent, a state, and various bookkeeping ﬁelds. Arrows point from child to parent.\nGiven the components for a parent node, it is easy to see how to compute the necessary\ncomponents for a child node. The function CHILD-NODE takes a parent node and an action\nand returns the resulting child node:\nfunction CHILD-NODE(problem,parent,action) returns a node\nreturn a node with\nSTATE = problem.RESULT(parent.STATE,action),\nPARENT = parent, ACTION = action,\nPATH-COST = parent.PATH-COST + problem.STEP-COST(parent.STATE,action)\nThe node data structure is depicted in Figure 3.10. Notice how the PARENT pointers\nstring the nodes together into a tree structure. These pointers also allow the solution path to be\nextracted when a goal node is found; we use the SOLUTION function to return the sequence\nof actions obtained by following parent pointers back to the root.\nUp to now, we have not been very careful to distinguish between nodes and states, but in\nwriting detailed algorithms it’s important to make that distinction. A node is a bookkeeping",
  "of actions obtained by following parent pointers back to the root.\nUp to now, we have not been very careful to distinguish between nodes and states, but in\nwriting detailed algorithms it’s important to make that distinction. A node is a bookkeeping\ndata structure used to represent the search tree. A state corresponds to a conﬁguration of the\nworld. Thus, nodes are on particular paths, as deﬁned by PARENT pointers, whereas states\nare not. Furthermore, two different nodes can contain the same world state if that state is\ngenerated via two different search paths.\nNow that we have nodes, we need somewhere to put them. The frontier needs to be\nstored in such a way that the search algorithm can easily choose the next node to expand\naccording to its preferred strategy. The appropriate data structure for this is a queue. The\nQUEUE\noperations on a queue are as follows:\n• EMPTY?(queue) returns true only if there are no more elements in the queue.\n• POP(queue) removes the ﬁrst element of the queue and returns it.\n• INSERT(element, queue) inserts an element and returns the resulting queue. 80\nChapter\n3.\nSolving Problems by Searching\nQueues are characterized by the order in which they store the inserted nodes. Three common\nvariants are the ﬁrst-in, ﬁrst-out or FIFO queue, which pops the oldest element of the queue;\nFIFO QUEUE\nthe last-in, ﬁrst-out or LIFO queue (also known as a stack), which pops the newest element\nLIFO QUEUE\nof the queue; and the priority queue, which pops the element of the queue with the highest\nPRIORITY QUEUE\npriority according to some ordering function.\nThe explored set can be implemented with a hash table to allow efﬁcient checking for\nrepeated states. With a good implementation, insertion and lookup can be done in roughly\nconstant time no matter how many states are stored. One must take care to implement the\nhash table with the right notion of equality between states. For example, in the traveling\nsalesperson problem (page 74), the hash table needs to know that the set of visited cities\n{Bucharest,Urziceni,Vaslui} is the same as {Urziceni,Vaslui,Bucharest}. Sometimes this can\nbe achieved most easily by insisting that the data structures for states be in some canonical\nform; that is, logically equivalent states should map to the same data structure. In the case\nCANONICAL FORM\nof states described by sets, for example, a bit-vector representation or a sorted list without\nrepetition would be canonical, whereas an unsorted list would not.\n3.3.2",
  "CANONICAL FORM\nof states described by sets, for example, a bit-vector representation or a sorted list without\nrepetition would be canonical, whereas an unsorted list would not.\n3.3.2\nMeasuring problem-solving performance\nBefore we get into the design of speciﬁc search algorithms, we need to consider the criteria\nthat might be used to choose among them. We can evaluate an algorithm’s performance in\nfour ways:\n• Completeness: Is the algorithm guaranteed to ﬁnd a solution when there is one?\nCOMPLETENESS\n• Optimality: Does the strategy ﬁnd the optimal solution, as deﬁned on page 68?\nOPTIMALITY\n• Time complexity: How long does it take to ﬁnd a solution?\nTIME COMPLEXITY\n• Space complexity: How much memory is needed to perform the search?\nSPACE COMPLEXITY\nTime and space complexity are always considered with respect to some measure of the prob-\nlem difﬁculty. In theoretical computer science, the typical measure is the size of the state\nspace graph, |V | + |E|, where V is the set of vertices (nodes) of the graph and E is the set\nof edges (links). This is appropriate when the graph is an explicit data structure that is input\nto the search program. (The map of Romania is an example of this.) In AI, the graph is often\nrepresented implicitly by the initial state, actions, and transition model and is frequently inﬁ-\nnite. For these reasons, complexity is expressed in terms of three quantities: b, the branching\nfactor or maximum number of successors of any node; d, the depth of the shallowest goal\nBRANCHING FACTOR\nDEPTH\nnode (i.e., the number of steps along the path from the root); and m, the maximum length of\nany path in the state space. Time is often measured in terms of the number of nodes generated\nduring the search, and space in terms of the maximum number of nodes stored in memory.\nFor the most part, we describe time and space complexity for search on a tree; for a graph,\nthe answer depends on how “redundant” the paths in the state space are.\nTo assess the effectiveness of a search algorithm, we can consider just the search cost—\nSEARCH COST\nwhich typically depends on the time complexity but can also include a term for memory\nusage—or we can use the total cost, which combines the search cost and the path cost of the\nTOTAL COST\nsolution found. For the problem of ﬁnding a route from Arad to Bucharest, the search cost\nis the amount of time taken by the search and the solution cost is the total length of the path Section 3.4.\nUninformed Search Strategies\n81",
  "TOTAL COST\nsolution found. For the problem of ﬁnding a route from Arad to Bucharest, the search cost\nis the amount of time taken by the search and the solution cost is the total length of the path Section 3.4.\nUninformed Search Strategies\n81\nin kilometers. Thus, to compute the total cost, we have to add milliseconds and kilometers.\nThere is no “ofﬁcial exchange rate” between the two, but it might be reasonable in this case to\nconvert kilometers into milliseconds by using an estimate of the car’s average speed (because\ntime is what the agent cares about). This enables the agent to ﬁnd an optimal tradeoff point\nat which further computation to ﬁnd a shorter path becomes counterproductive. The more\ngeneral problem of tradeoffs between different goods is taken up in Chapter 16.\n3.4\nUNINFORMED SEARCH STRATEGIES\nThis section covers several search strategies that come under the heading of uninformed\nsearch (also called blind search). The term means that the strategies have no additional\nUNINFORMED\nSEARCH\nBLIND SEARCH\ninformation about states beyond that provided in the problem deﬁnition. All they can do is\ngenerate successors and distinguish a goal state from a non-goal state. All search strategies\nare distinguished by the order in which nodes are expanded. Strategies that know whether\none non-goal state is “more promising” than another are called informed search or heuristic\nINFORMED SEARCH\nsearch strategies; they are covered in Section 3.5.\nHEURISTIC SEARCH\n3.4.1\nBreadth-ﬁrst search\nBreadth-ﬁrst search is a simple strategy in which the root node is expanded ﬁrst, then all the\nBREADTH-FIRST\nSEARCH\nsuccessors of the root node are expanded next, then their successors, and so on. In general,\nall the nodes are expanded at a given depth in the search tree before any nodes at the next\nlevel are expanded.\nBreadth-ﬁrst search is an instance of the general graph-search algorithm (Figure 3.7) in\nwhich the shallowest unexpanded node is chosen for expansion. This is achieved very simply\nby using a FIFO queue for the frontier. Thus, new nodes (which are always deeper than their\nparents) go to the back of the queue, and old nodes, which are shallower than the new nodes,\nget expanded ﬁrst. There is one slight tweak on the general graph-search algorithm, which is\nthat the goal test is applied to each node when it is generated rather than when it is selected for\nexpansion. This decision is explained below, where we discuss time complexity. Note also",
  "that the goal test is applied to each node when it is generated rather than when it is selected for\nexpansion. This decision is explained below, where we discuss time complexity. Note also\nthat the algorithm, following the general template for graph search, discards any new path to\na state already in the frontier or explored set; it is easy to see that any such path must be at\nleast as deep as the one already found. Thus, breadth-ﬁrst search always has the shallowest\npath to every node on the frontier.\nPseudocode is given in Figure 3.11. Figure 3.12 shows the progress of the search on a\nsimple binary tree.\nHow does breadth-ﬁrst search rate according to the four criteria from the previous sec-\ntion? We can easily see that it is complete—if the shallowest goal node is at some ﬁnite depth\nd, breadth-ﬁrst search will eventually ﬁnd it after generating all shallower nodes (provided\nthe branching factor b is ﬁnite). Note that as soon as a goal node is generated, we know it\nis the shallowest goal node because all shallower nodes must have been generated already\nand failed the goal test. Now, the shallowest goal node is not necessarily the optimal one; 82\nChapter\n3.\nSolving Problems by Searching\nfunction BREADTH-FIRST-SEARCH(problem) returns a solution, or failure\nnode ←a node with STATE = problem.INITIAL-STATE, PATH-COST = 0\nif problem.GOAL-TEST(node.STATE) then return SOLUTION(node)\nfrontier ←a FIFO queue with node as the only element\nexplored ←an empty set\nloop do\nif EMPTY?(frontier) then return failure\nnode ←POP(frontier) /* chooses the shallowest node in frontier */\nadd node.STATE to explored\nfor each action in problem.ACTIONS(node.STATE) do\nchild ←CHILD-NODE(problem,node,action)\nif child.STATE is not in explored or frontier then\nif problem.GOAL-TEST(child.STATE) then return SOLUTION(child)\nfrontier ←INSERT(child,frontier)\nFigure 3.11\nBreadth-ﬁrst search on a graph.\ntechnically, breadth-ﬁrst search is optimal if the path cost is a nondecreasing function of the\ndepth of the node. The most common such scenario is that all actions have the same cost.\nSo far, the news about breadth-ﬁrst search has been good. The news about time and\nspace is not so good. Imagine searching a uniform tree where every state has b successors.\nThe root of the search tree generates b nodes at the ﬁrst level, each of which generates b more\nnodes, for a total of b2 at the second level. Each of these generates b more nodes, yielding b3",
  "The root of the search tree generates b nodes at the ﬁrst level, each of which generates b more\nnodes, for a total of b2 at the second level. Each of these generates b more nodes, yielding b3\nnodes at the third level, and so on. Now suppose that the solution is at depth d. In the worst\ncase, it is the last node generated at that level. Then the total number of nodes generated is\nb + b2 + b3 + · · · + bd = O(bd) .\n(If the algorithm were to apply the goal test to nodes when selected for expansion, rather than\nwhen generated, the whole layer of nodes at depth d would be expanded before the goal was\ndetected and the time complexity would be O(bd+1).)\nAs for space complexity: for any kind of graph search, which stores every expanded\nnode in the explored set, the space complexity is always within a factor of b of the time\ncomplexity. For breadth-ﬁrst graph search in particular, every node generated remains in\nmemory. There will be O(bd−1) nodes in the explored set and O(bd) nodes in the frontier,\nA\nB\nC\nE\nF\nG\nD\nA\nB\nD\nE\nF\nG\nC\nA\nC\nD\nE\nF\nG\nB\nB\nC\nD\nE\nF\nG\nA\nFigure 3.12\nBreadth-ﬁrst search on a simple binary tree. At each stage, the node to be\nexpanded next is indicated by a marker. Section 3.4.\nUninformed Search Strategies\n83\nso the space complexity is O(bd), i.e., it is dominated by the size of the frontier. Switching\nto a tree search would not save much space, and in a state space with many redundant paths,\nswitching could cost a great deal of time.\nAn exponential complexity bound such as O(bd) is scary. Figure 3.13 shows why. It\nlists, for various values of the solution depth d, the time and memory required for a breadth-\nﬁrst search with branching factor b = 10. The table assumes that 1 million nodes can be\ngenerated per second and that a node requires 1000 bytes of storage. Many search problems\nﬁt roughly within these assumptions (give or take a factor of 100) when run on a modern\npersonal computer.\nDepth\nNodes\nTime\nMemory\n2\n110\n.11 milliseconds\n107 kilobytes\n4\n11,110\n11 milliseconds\n10.6 megabytes\n6\n106\n1.1 seconds\n1 gigabyte\n8\n108\n2 minutes\n103 gigabytes\n10\n1010\n3 hours\n10 terabytes\n12\n1012\n13 days\n1 petabyte\n14\n1014\n3.5 years\n99 petabytes\n16\n1016\n350 years\n10 exabytes\nFigure 3.13\nTime and memory requirements for breadth-ﬁrst search. The numbers shown\nassume branching factor b = 10; 1 million nodes/second; 1000 bytes/node.\nTwo lessons can be learned from Figure 3.13. First, the memory requirements are a",
  "Figure 3.13\nTime and memory requirements for breadth-ﬁrst search. The numbers shown\nassume branching factor b = 10; 1 million nodes/second; 1000 bytes/node.\nTwo lessons can be learned from Figure 3.13. First, the memory requirements are a\nbigger problem for breadth-ﬁrst search than is the execution time. One might wait 13 days\nfor the solution to an important problem with search depth 12, but no personal computer has\nthe petabyte of memory it would take. Fortunately, other strategies require less memory.\nThe second lesson is that time is still a major factor. If your problem has a solution at\ndepth 16, then (given our assumptions) it will take about 350 years for breadth-ﬁrst search (or\nindeed any uninformed search) to ﬁnd it. In general, exponential-complexity search problems\ncannot be solved by uninformed methods for any but the smallest instances.\n3.4.2\nUniform-cost search\nWhen all step costs are equal, breadth-ﬁrst search is optimal because it always expands the\nshallowest unexpanded node. By a simple extension, we can ﬁnd an algorithm that is optimal\nwith any step-cost function. Instead of expanding the shallowest node, uniform-cost search\nUNIFORM-COST\nSEARCH\nexpands the node n with the lowest path cost g(n). This is done by storing the frontier as a\npriority queue ordered by g. The algorithm is shown in Figure 3.14.\nIn addition to the ordering of the queue by path cost, there are two other signiﬁcant\ndifferences from breadth-ﬁrst search. The ﬁrst is that the goal test is applied to a node when\nit is selected for expansion (as in the generic graph-search algorithm shown in Figure 3.7)\nrather than when it is ﬁrst generated. The reason is that the ﬁrst goal node that is generated 84\nChapter\n3.\nSolving Problems by Searching\nfunction UNIFORM-COST-SEARCH(problem) returns a solution, or failure\nnode ←a node with STATE = problem.INITIAL-STATE, PATH-COST = 0\nfrontier ←a priority queue ordered by PATH-COST, with node as the only element\nexplored ←an empty set\nloop do\nif EMPTY?(frontier) then return failure\nnode ←POP(frontier) /* chooses the lowest-cost node in frontier */\nif problem.GOAL-TEST(node.STATE) then return SOLUTION(node)\nadd node.STATE to explored\nfor each action in problem.ACTIONS(node.STATE) do\nchild ←CHILD-NODE(problem,node,action)\nif child.STATE is not in explored or frontier then\nfrontier ←INSERT(child,frontier)\nelse if child.STATE is in frontier with higher PATH-COST then\nreplace that frontier node with child\nFigure 3.14",
  "child ←CHILD-NODE(problem,node,action)\nif child.STATE is not in explored or frontier then\nfrontier ←INSERT(child,frontier)\nelse if child.STATE is in frontier with higher PATH-COST then\nreplace that frontier node with child\nFigure 3.14\nUniform-cost search on a graph. The algorithm is identical to the general\ngraph search algorithm in Figure 3.7, except for the use of a priority queue and the addition\nof an extra check in case a shorter path to a frontier state is discovered. The data structure for\nfrontier needs to support efﬁcient membership testing, so it should combine the capabilities\nof a priority queue and a hash table.\nSibiu\nFagaras\nPitesti\nRimnicu Vilcea\nBucharest\n99\n80\n97\n101\n211\nFigure 3.15\nPart of the Romania state space, selected to illustrate uniform-cost search.\nmay be on a suboptimal path. The second difference is that a test is added in case a better\npath is found to a node currently on the frontier.\nBoth of these modiﬁcations come into play in the example shown in Figure 3.15, where\nthe problem is to get from Sibiu to Bucharest. The successors of Sibiu are Rimnicu Vilcea and\nFagaras, with costs 80 and 99, respectively. The least-cost node, Rimnicu Vilcea, is expanded\nnext, adding Pitesti with cost 80 + 97 = 177. The least-cost node is now Fagaras, so it is\nexpanded, adding Bucharest with cost 99 + 211 = 310. Now a goal node has been generated,\nbut uniform-cost search keeps going, choosing Pitesti for expansion and adding a second path Section 3.4.\nUninformed Search Strategies\n85\nto Bucharest with cost 80+97+101 = 278. Now the algorithm checks to see if this new path\nis better than the old one; it is, so the old one is discarded. Bucharest, now with g-cost 278,\nis selected for expansion and the solution is returned.\nIt is easy to see that uniform-cost search is optimal in general. First, we observe that\nwhenever uniform-cost search selects a node n for expansion, the optimal path to that node\nhas been found. (Were this not the case, there would have to be another frontier node n′ on\nthe optimal path from the start node to n, by the graph separation property of Figure 3.9;\nby deﬁnition, n′ would have lower g-cost than n and would have been selected ﬁrst.) Then,\nbecause step costs are nonnegative, paths never get shorter as nodes are added. These two\nfacts together imply that uniform-cost search expands nodes in order of their optimal path\ncost. Hence, the ﬁrst goal node selected for expansion must be the optimal solution.",
  "facts together imply that uniform-cost search expands nodes in order of their optimal path\ncost. Hence, the ﬁrst goal node selected for expansion must be the optimal solution.\nUniform-cost search does not care about the number of steps a path has, but only about\ntheir total cost. Therefore, it will get stuck in an inﬁnite loop if there is a path with an inﬁnite\nsequence of zero-cost actions—for example, a sequence of NoOp actions.6 Completeness is\nguaranteed provided the cost of every step exceeds some small positive constant ϵ.\nUniform-cost search is guided by path costs rather than depths, so its complexity is not\neasily characterized in terms of b and d. Instead, let C∗be the cost of the optimal solution,7\nand assume that every action costs at least ϵ. Then the algorithm’s worst-case time and space\ncomplexity is O(b1+⌊C∗/ϵ⌋), which can be much greater than bd. This is because uniform-\ncost search can explore large trees of small steps before exploring paths involving large and\nperhaps useful steps. When all step costs are equal, b1+⌊C∗/ϵ⌋is just bd+1. When all step\ncosts are the same, uniform-cost search is similar to breadth-ﬁrst search, except that the latter\nstops as soon as it generates a goal, whereas uniform-cost search examines all the nodes at\nthe goal’s depth to see if one has a lower cost; thus uniform-cost search does strictly more\nwork by expanding nodes at depth d unnecessarily.\n3.4.3\nDepth-ﬁrst search\nDepth-ﬁrst search always expands the deepest node in the current frontier of the search tree.\nDEPTH-FIRST\nSEARCH\nThe progress of the search is illustrated in Figure 3.16. The search proceeds immediately\nto the deepest level of the search tree, where the nodes have no successors. As those nodes\nare expanded, they are dropped from the frontier, so then the search “backs up” to the next\ndeepest node that still has unexplored successors.\nThe depth-ﬁrst search algorithm is an instance of the graph-search algorithm in Fig-\nure 3.7; whereas breadth-ﬁrst-search uses a FIFO queue, depth-ﬁrst search uses a LIFO queue.\nA LIFO queue means that the most recently generated node is chosen for expansion. This\nmust be the deepest unexpanded node because it is one deeper than its parent—which, in turn,\nwas the deepest unexpanded node when it was selected.\nAs an alternative to the GRAPH-SEARCH-style implementation, it is common to im-\nplement depth-ﬁrst search with a recursive function that calls itself on each of its children in",
  "was the deepest unexpanded node when it was selected.\nAs an alternative to the GRAPH-SEARCH-style implementation, it is common to im-\nplement depth-ﬁrst search with a recursive function that calls itself on each of its children in\nturn. (A recursive depth-ﬁrst algorithm incorporating a depth limit is shown in Figure 3.17.)\n6 NoOp, or “no operation,” is the name of an assembly language instruction that does nothing.\n7 Here, and throughout the book, the “star” in C∗means an optimal value for C. 86\nChapter\n3.\nSolving Problems by Searching\nA\nC\nF\nG\nM\nN\nO\nA\nC\nF\nG\nL\nM\nN\nO\nA\nC\nF\nG\nL\nM\nN\nO\nC\nF\nG\nL\nM\nN\nO\nA\nB\nC\nE\nF\nG\nK\nL\nM\nN\nO\nA\nC\nE\nF\nG\nJ\nK\nL\nM\nN\nO\nA\nC\nE\nF\nG\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nFigure 3.16\nDepth-ﬁrst search on a binary tree. The unexplored region is shown in light\ngray. Explored nodes with no descendants in the frontier are removed from memory. Nodes\nat depth 3 have no successors and M is the only goal node.\nThe properties of depth-ﬁrst search depend strongly on whether the graph-search or\ntree-search version is used. The graph-search version, which avoids repeated states and re-\ndundant paths, is complete in ﬁnite state spaces because it will eventually expand every node.\nThe tree-search version, on the other hand, is not complete—for example, in Figure 3.6 the\nalgorithm will follow the Arad–Sibiu–Arad–Sibiu loop forever. Depth-ﬁrst tree search can be\nmodiﬁed at no extra memory cost so that it checks new states against those on the path from\nthe root to the current node; this avoids inﬁnite loops in ﬁnite state spaces but does not avoid\nthe proliferation of redundant paths. In inﬁnite state spaces, both versions fail if an inﬁnite\nnon-goal path is encountered. For example, in Knuth’s 4 problem, depth-ﬁrst search would\nkeep applying the factorial operator forever.\nFor similar reasons, both versions are nonoptimal. For example, in Figure 3.16, depth-\nﬁrst search will explore the entire left subtree even if node C is a goal node. If node J were\nalso a goal node, then depth-ﬁrst search would return it as a solution instead of C, which\nwould be a better solution; hence, depth-ﬁrst search is not optimal. Section 3.4.\nUninformed Search Strategies\n87\nThe time complexity of depth-ﬁrst graph search is bounded by the size of the state space\n(which may be inﬁnite, of course). A depth-ﬁrst tree search, on the other hand, may generate",
  "Uninformed Search Strategies\n87\nThe time complexity of depth-ﬁrst graph search is bounded by the size of the state space\n(which may be inﬁnite, of course). A depth-ﬁrst tree search, on the other hand, may generate\nall of the O(bm) nodes in the search tree, where m is the maximum depth of any node; this\ncan be much greater than the size of the state space. Note that m itself can be much larger\nthan d (the depth of the shallowest solution) and is inﬁnite if the tree is unbounded.\nSo far, depth-ﬁrst search seems to have no clear advantage over breadth-ﬁrst search,\nso why do we include it? The reason is the space complexity. For a graph search, there is\nno advantage, but a depth-ﬁrst tree search needs to store only a single path from the root\nto a leaf node, along with the remaining unexpanded sibling nodes for each node on the\npath. Once a node has been expanded, it can be removed from memory as soon as all its\ndescendants have been fully explored. (See Figure 3.16.) For a state space with branching\nfactor b and maximum depth m, depth-ﬁrst search requires storage of only O(bm) nodes.\nUsing the same assumptions as for Figure 3.13 and assuming that nodes at the same depth as\nthe goal node have no successors, we ﬁnd that depth-ﬁrst search would require 156 kilobytes\ninstead of 10 exabytes at depth d = 16, a factor of 7 trillion times less space. This has\nled to the adoption of depth-ﬁrst tree search as the basic workhorse of many areas of AI,\nincluding constraint satisfaction (Chapter 6), propositional satisﬁability (Chapter 7), and logic\nprogramming (Chapter 9). For the remainder of this section, we focus primarily on the tree-\nsearch version of depth-ﬁrst search.\nA variant of depth-ﬁrst search called backtracking search uses still less memory. (See\nBACKTRACKING\nSEARCH\nChapter 6 for more details.) In backtracking, only one successor is generated at a time rather\nthan all successors; each partially expanded node remembers which successor to generate\nnext. In this way, only O(m) memory is needed rather than O(bm). Backtracking search\nfacilitates yet another memory-saving (and time-saving) trick: the idea of generating a suc-\ncessor by modifying the current state description directly rather than copying it ﬁrst. This\nreduces the memory requirements to just one state description and O(m) actions. For this to\nwork, we must be able to undo each modiﬁcation when we go back to generate the next suc-",
  "reduces the memory requirements to just one state description and O(m) actions. For this to\nwork, we must be able to undo each modiﬁcation when we go back to generate the next suc-\ncessor. For problems with large state descriptions, such as robotic assembly, these techniques\nare critical to success.\n3.4.4\nDepth-limited search\nThe embarrassing failure of depth-ﬁrst search in inﬁnite state spaces can be alleviated by\nsupplying depth-ﬁrst search with a predetermined depth limit ℓ. That is, nodes at depth ℓare\ntreated as if they have no successors. This approach is called depth-limited search. The\nDEPTH-LIMITED\nSEARCH\ndepth limit solves the inﬁnite-path problem. Unfortunately, it also introduces an additional\nsource of incompleteness if we choose ℓ< d, that is, the shallowest goal is beyond the depth\nlimit. (This is likely when d is unknown.) Depth-limited search will also be nonoptimal if\nwe choose ℓ> d. Its time complexity is O(bℓ) and its space complexity is O(bℓ). Depth-ﬁrst\nsearch can be viewed as a special case of depth-limited search with ℓ= ∞.\nSometimes, depth limits can be based on knowledge of the problem. For example, on\nthe map of Romania there are 20 cities. Therefore, we know that if there is a solution, it must\nbe of length 19 at the longest, so ℓ= 19 is a possible choice. But in fact if we studied the 88\nChapter\n3.\nSolving Problems by Searching\nfunction DEPTH-LIMITED-SEARCH(problem,limit) returns a solution, or failure/cutoff\nreturn RECURSIVE-DLS(MAKE-NODE(problem.INITIAL-STATE),problem,limit)\nfunction RECURSIVE-DLS(node,problem,limit) returns a solution, or failure/cutoff\nif problem.GOAL-TEST(node.STATE) then return SOLUTION(node)\nelse if limit = 0 then return cutoﬀ\nelse\ncutoﬀoccurred? ←false\nfor each action in problem.ACTIONS(node.STATE) do\nchild ←CHILD-NODE(problem,node,action)\nresult ←RECURSIVE-DLS(child,problem,limit −1)\nif result = cutoﬀthen cutoﬀoccurred? ←true\nelse if result ̸= failure then return result\nif cutoﬀoccurred? then return cutoﬀelse return failure\nFigure 3.17\nA recursive implementation of depth-limited tree search.\nmap carefully, we would discover that any city can be reached from any other city in at most\n9 steps. This number, known as the diameter of the state space, gives us a better depth limit,\nDIAMETER\nwhich leads to a more efﬁcient depth-limited search. For most problems, however, we will\nnot know a good depth limit until we have solved the problem.",
  "DIAMETER\nwhich leads to a more efﬁcient depth-limited search. For most problems, however, we will\nnot know a good depth limit until we have solved the problem.\nDepth-limited search can be implemented as a simple modiﬁcation to the general tree-\nor graph-search algorithm. Alternatively, it can be implemented as a simple recursive al-\ngorithm as shown in Figure 3.17. Notice that depth-limited search can terminate with two\nkinds of failure: the standard failure value indicates no solution; the cutoﬀvalue indicates\nno solution within the depth limit.\n3.4.5\nIterative deepening depth-ﬁrst search\nIterative deepening search (or iterative deepening depth-ﬁrst search) is a general strategy,\nITERATIVE\nDEEPENING SEARCH\noften used in combination with depth-ﬁrst tree search, that ﬁnds the best depth limit. It does\nthis by gradually increasing the limit—ﬁrst 0, then 1, then 2, and so on—until a goal is found.\nThis will occur when the depth limit reaches d, the depth of the shallowest goal node. The\nalgorithm is shown in Figure 3.18. Iterative deepening combines the beneﬁts of depth-ﬁrst\nand breadth-ﬁrst search. Like depth-ﬁrst search, its memory requirements are modest: O(bd)\nto be precise. Like breadth-ﬁrst search, it is complete when the branching factor is ﬁnite and\noptimal when the path cost is a nondecreasing function of the depth of the node. Figure 3.19\nshows four iterations of ITERATIVE-DEEPENING-SEARCH on a binary search tree, where the\nsolution is found on the fourth iteration.\nIterative deepening search may seem wasteful because states are generated multiple\ntimes. It turns out this is not too costly. The reason is that in a search tree with the same (or\nnearly the same) branching factor at each level, most of the nodes are in the bottom level,\nso it does not matter much that the upper levels are generated multiple times. In an iterative\ndeepening search, the nodes on the bottom level (depth d) are generated once, those on the Section 3.4.\nUninformed Search Strategies\n89\nfunction ITERATIVE-DEEPENING-SEARCH(problem) returns a solution, or failure\nfor depth = 0 to ∞do\nresult ←DEPTH-LIMITED-SEARCH(problem,depth)\nif result ̸= cutoff then return result\nFigure 3.18\nThe iterative deepening search algorithm, which repeatedly applies depth-\nlimited search with increasing limits. It terminates when a solution is found or if the depth-\nlimited search returns failure, meaning that no solution exists.\nLimit = 3\nLimit = 2\nLimit = 1\nLimit = 0\nA\nA\nA\nB\nC\nA\nB\nC\nA\nB\nC\nA\nB\nC\nA\nB",
  "limited search with increasing limits. It terminates when a solution is found or if the depth-\nlimited search returns failure, meaning that no solution exists.\nLimit = 3\nLimit = 2\nLimit = 1\nLimit = 0\nA\nA\nA\nB\nC\nA\nB\nC\nA\nB\nC\nA\nB\nC\nA\nB\nC\nD\nE\nF\nG\nA\nB\nC\nD\nE\nF\nG\nA\nB\nC\nD\nE\nF\nG\nA\nB\nC\nD\nE\nF\nG\nA\nB\nC\nD\nE\nF\nG\nA\nB\nC\nD\nE\nF\nG\nA\nB\nC\nD\nE\nF\nG\nA\nB\nC\nD\nE\nF\nG\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nA\nB\nC\nD\nE\nF\nG\nH\nJ\nK\nL\nM\nN\nO\nI\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nFigure 3.19\nFour iterations of iterative deepening search on a binary tree. 90\nChapter\n3.\nSolving Problems by Searching\nnext-to-bottom level are generated twice, and so on, up to the children of the root, which are\ngenerated d times. So the total number of nodes generated in the worst case is\nN(IDS) = (d)b + (d −1)b2 + · · · + (1)bd ,\nwhich gives a time complexity of O(bd)—asymptotically the same as breadth-ﬁrst search.\nThere is some extra cost for generating the upper levels multiple times, but it is not large. For\nexample, if b = 10 and d = 5, the numbers are\nN(IDS) = 50 + 400 + 3, 000 + 20, 000 + 100, 000 = 123, 450\nN(BFS) = 10 + 100 + 1, 000 + 10, 000 + 100, 000 = 111, 110 .\nIf you are really concerned about repeating the repetition, you can use a hybrid approach\nthat runs breadth-ﬁrst search until almost all the available memory is consumed, and then\nruns iterative deepening from all the nodes in the frontier. In general, iterative deepening is\nthe preferred uninformed search method when the search space is large and the depth of the\nsolution is not known.\nIterative deepening search is analogous to breadth-ﬁrst search in that it explores a com-\nplete layer of new nodes at each iteration before going on to the next layer. It would seem\nworthwhile to develop an iterative analog to uniform-cost search, inheriting the latter algo-\nrithm’s optimality guarantees while avoiding its memory requirements. The idea is to use\nincreasing path-cost limits instead of increasing depth limits. The resulting algorithm, called\niterative lengthening search, is explored in Exercise 3.17. It turns out, unfortunately, that\nITERATIVE\nLENGTHENING\nSEARCH\niterative lengthening incurs substantial overhead compared to uniform-cost search.\n3.4.6\nBidirectional search",
  "iterative lengthening search, is explored in Exercise 3.17. It turns out, unfortunately, that\nITERATIVE\nLENGTHENING\nSEARCH\niterative lengthening incurs substantial overhead compared to uniform-cost search.\n3.4.6\nBidirectional search\nThe idea behind bidirectional search is to run two simultaneous searches—one forward from\nthe initial state and the other backward from the goal—hoping that the two searches meet in\nthe middle (Figure 3.20). The motivation is that bd/2 + bd/2 is much less than bd, or in the\nﬁgure, the area of the two small circles is less than the area of one big circle centered on the\nstart and reaching to the goal.\nBidirectional search is implemented by replacing the goal test with a check to see\nwhether the frontiers of the two searches intersect; if they do, a solution has been found.\n(It is important to realize that the ﬁrst such solution found may not be optimal, even if the\ntwo searches are both breadth-ﬁrst; some additional search is required to make sure there\nisn’t another short-cut across the gap.) The check can be done when each node is generated\nor selected for expansion and, with a hash table, will take constant time. For example, if a\nproblem has solution depth d = 6, and each direction runs breadth-ﬁrst search one node at a\ntime, then in the worst case the two searches meet when they have generated all of the nodes\nat depth 3. For b = 10, this means a total of 2,220 node generations, compared with 1,111,110\nfor a standard breadth-ﬁrst search. Thus, the time complexity of bidirectional search using\nbreadth-ﬁrst searches in both directions is O(bd/2). The space complexity is also O(bd/2).\nWe can reduce this by roughly half if one of the two searches is done by iterative deepening,\nbut at least one of the frontiers must be kept in memory so that the intersection check can be\ndone. This space requirement is the most signiﬁcant weakness of bidirectional search. Section 3.4.\nUninformed Search Strategies\n91\nGoal\nStart\nFigure 3.20\nA schematic view of a bidirectional search that is about to succeed when a\nbranch from the start node meets a branch from the goal node.\nThe reduction in time complexity makes bidirectional search attractive, but how do we\nsearch backward? This is not as easy as it sounds. Let the predecessors of a state x be all\nPREDECESSOR\nthose states that have x as a successor. Bidirectional search requires a method for computing",
  "search backward? This is not as easy as it sounds. Let the predecessors of a state x be all\nPREDECESSOR\nthose states that have x as a successor. Bidirectional search requires a method for computing\npredecessors. When all the actions in the state space are reversible, the predecessors of x are\njust its successors. Other cases may require substantial ingenuity.\nConsider the question of what we mean by “the goal” in searching “backward from the\ngoal.” For the 8-puzzle and for ﬁnding a route in Romania, there is just one goal state, so the\nbackward search is very much like the forward search. If there are several explicitly listed\ngoal states—for example, the two dirt-free goal states in Figure 3.3—then we can construct a\nnew dummy goal state whose immediate predecessors are all the actual goal states. But if the\ngoal is an abstract description, such as the goal that “no queen attacks another queen” in the\nn-queens problem, then bidirectional search is difﬁcult to use.\n3.4.7\nComparing uninformed search strategies\nFigure 3.21 compares search strategies in terms of the four evaluation criteria set forth in\nSection 3.3.2. This comparison is for tree-search versions. For graph searches, the main\ndifferences are that depth-ﬁrst search is complete for ﬁnite state spaces and that the space and\ntime complexities are bounded by the size of the state space.\nCriterion\nBreadth-\nUniform-\nDepth-\nDepth-\nIterative\nBidirectional\nFirst\nCost\nFirst\nLimited\nDeepening\n(if applicable)\nComplete?\nYesa\nYesa,b\nNo\nNo\nYesa\nYesa,d\nTime\nO(bd)\nO(b1+⌊C∗/ϵ⌋)\nO(bm)\nO(bℓ)\nO(bd)\nO(bd/2)\nSpace\nO(bd)\nO(b1+⌊C∗/ϵ⌋)\nO(bm)\nO(bℓ)\nO(bd)\nO(bd/2)\nOptimal?\nYesc\nYes\nNo\nNo\nYesc\nYesc,d\nFigure 3.21\nEvaluation of tree-search strategies. b is the branching factor; d is the depth\nof the shallowest solution; m is the maximum depth of the search tree; l is the depth limit.\nSuperscript caveats are as follows: a complete if b is ﬁnite; b complete if step costs ≥ϵ for\npositive ϵ; c optimal if step costs are all identical; d if both directions use breadth-ﬁrst search. 92\nChapter\n3.\nSolving Problems by Searching\n3.5\nINFORMED (HEURISTIC) SEARCH STRATEGIES\nThis section shows how an informed search strategy—one that uses problem-speciﬁc knowl-\nINFORMED SEARCH\nedge beyond the deﬁnition of the problem itself—can ﬁnd solutions more efﬁciently than can\nan uninformed strategy.\nThe general approach we consider is called best-ﬁrst search. Best-ﬁrst search is an\nBEST-FIRST SEARCH",
  "INFORMED SEARCH\nedge beyond the deﬁnition of the problem itself—can ﬁnd solutions more efﬁciently than can\nan uninformed strategy.\nThe general approach we consider is called best-ﬁrst search. Best-ﬁrst search is an\nBEST-FIRST SEARCH\ninstance of the general TREE-SEARCH or GRAPH-SEARCH algorithm in which a node is\nselected for expansion based on an evaluation function, f(n). The evaluation function is\nEVALUATION\nFUNCTION\nconstrued as a cost estimate, so the node with the lowest evaluation is expanded ﬁrst. The\nimplementation of best-ﬁrst graph search is identical to that for uniform-cost search (Fig-\nure 3.14), except for the use of f instead of g to order the priority queue.\nThe choice of f determines the search strategy. (For example, as Exercise 3.21 shows,\nbest-ﬁrst tree search includes depth-ﬁrst search as a special case.) Most best-ﬁrst algorithms\ninclude as a component of f a heuristic function, denoted h(n):\nHEURISTIC\nFUNCTION\nh(n) = estimated cost of the cheapest path from the state at node n to a goal state.\n(Notice that h(n) takes a node as input, but, unlike g(n), it depends only on the state at that\nnode.) For example, in Romania, one might estimate the cost of the cheapest path from Arad\nto Bucharest via the straight-line distance from Arad to Bucharest.\nHeuristic functions are the most common form in which additional knowledge of the\nproblem is imparted to the search algorithm. We study heuristics in more depth in Section 3.6.\nFor now, we consider them to be arbitrary, nonnegative, problem-speciﬁc functions, with one\nconstraint: if n is a goal node, then h(n) = 0. The remainder of this section covers two ways\nto use heuristic information to guide search.\n3.5.1\nGreedy best-ﬁrst search\nGreedy best-ﬁrst search8 tries to expand the node that is closest to the goal, on the grounds\nGREEDY BEST-FIRST\nSEARCH\nthat this is likely to lead to a solution quickly. Thus, it evaluates nodes by using just the\nheuristic function; that is, f(n) = h(n).\nLet us see how this works for route-ﬁnding problems in Romania; we use the straight-\nline distance heuristic, which we will call hSLD.\nIf the goal is Bucharest, we need to\nSTRAIGHT-LINE\nDISTANCE\nknow the straight-line distances to Bucharest, which are shown in Figure 3.22. For exam-\nple, hSLD(In(Arad)) = 366. Notice that the values of hSLD cannot be computed from the\nproblem description itself. Moreover, it takes a certain amount of experience to know that",
  "ple, hSLD(In(Arad)) = 366. Notice that the values of hSLD cannot be computed from the\nproblem description itself. Moreover, it takes a certain amount of experience to know that\nhSLD is correlated with actual road distances and is, therefore, a useful heuristic.\nFigure 3.23 shows the progress of a greedy best-ﬁrst search using hSLD to ﬁnd a path\nfrom Arad to Bucharest. The ﬁrst node to be expanded from Arad will be Sibiu because it\nis closer to Bucharest than either Zerind or Timisoara. The next node to be expanded will\nbe Fagaras because it is closest. Fagaras in turn generates Bucharest, which is the goal. For\nthis particular problem, greedy best-ﬁrst search using hSLD ﬁnds a solution without ever\n8 Our ﬁrst edition called this greedy search; other authors have called it best-ﬁrst search. Our more general\nusage of the latter term follows Pearl (1984). Section 3.5.\nInformed (Heuristic) Search Strategies\n93\nUrziceni\nNeamt\nOradea\nZerind\nTimisoara\nMehadia\nSibiu\nPitesti\nRimnicu Vilcea\nVaslui\nBucharest\nGiurgiu\nHirsova\nEforie\nArad\nLugoj\nDrobeta\nCraiova\nFagaras\nIasi\n 0\n160\n242\n161\n77\n151\n366\n244\n226\n176\n241\n253\n329\n80\n199\n380\n234\n374\n100\n193\nFigure 3.22\nValues of hSLD—straight-line distances to Bucharest.\nexpanding a node that is not on the solution path; hence, its search cost is minimal. It is\nnot optimal, however: the path via Sibiu and Fagaras to Bucharest is 32 kilometers longer\nthan the path through Rimnicu Vilcea and Pitesti. This shows why the algorithm is called\n“greedy”—at each step it tries to get as close to the goal as it can.\nGreedy best-ﬁrst tree search is also incomplete even in a ﬁnite state space, much like\ndepth-ﬁrst search. Consider the problem of getting from Iasi to Fagaras. The heuristic sug-\ngests that Neamt be expanded ﬁrst because it is closest to Fagaras, but it is a dead end. The\nsolution is to go ﬁrst to Vaslui—a step that is actually farther from the goal according to\nthe heuristic—and then to continue to Urziceni, Bucharest, and Fagaras. The algorithm will\nnever ﬁnd this solution, however, because expanding Neamt puts Iasi back into the frontier,\nIasi is closer to Fagaras than Vaslui is, and so Iasi will be expanded again, leading to an inﬁ-\nnite loop. (The graph search version is complete in ﬁnite spaces, but not in inﬁnite ones.) The\nworst-case time and space complexity for the tree version is O(bm), where m is the maximum\ndepth of the search space. With a good heuristic function, however, the complexity can be",
  "worst-case time and space complexity for the tree version is O(bm), where m is the maximum\ndepth of the search space. With a good heuristic function, however, the complexity can be\nreduced substantially. The amount of the reduction depends on the particular problem and on\nthe quality of the heuristic.\n3.5.2\nA* search: Minimizing the total estimated solution cost\nThe most widely known form of best-ﬁrst search is called A∗search (pronounced “A-star\nA∗SEARCH\nsearch”). It evaluates nodes by combining g(n), the cost to reach the node, and h(n), the cost\nto get from the node to the goal:\nf(n) = g(n) + h(n) .\nSince g(n) gives the path cost from the start node to node n, and h(n) is the estimated cost\nof the cheapest path from n to the goal, we have\nf(n) = estimated cost of the cheapest solution through n .\nThus, if we are trying to ﬁnd the cheapest solution, a reasonable thing to try ﬁrst is the\nnode with the lowest value of g(n) + h(n). It turns out that this strategy is more than just\nreasonable: provided that the heuristic function h(n) satisﬁes certain conditions, A∗search is\nboth complete and optimal. The algorithm is identical to UNIFORM-COST-SEARCH except\nthat A∗uses g + h instead of g. 94\nChapter\n3.\nSolving Problems by Searching\nRimnicu Vilcea\nZerind\nArad\nSibiu\nArad\nFagaras\nOradea\nTimisoara\nSibiu\nBucharest\n329\n374\n366\n380\n193\n253\n0\nRimnicu Vilcea\nArad\nSibiu\nArad\nFagaras\nOradea\nTimisoara\n329\nZerind\n374\n366\n176\n380\n193\nZerind\nArad\nSibiu\nTimisoara\n253\n329\n374\nArad\n366\n(a) The initial state\n(b) After expanding Arad\n(c) After expanding Sibiu\n(d) After expanding Fagaras\nFigure 3.23\nStages in a greedy best-ﬁrst tree search for Bucharest with the straight-line\ndistance heuristic hSLD. Nodes are labeled with their h-values.\nConditions for optimality: Admissibility and consistency\nThe ﬁrst condition we require for optimality is that h(n) be an admissible heuristic. An\nADMISSIBLE\nHEURISTIC\nadmissible heuristic is one that never overestimates the cost to reach the goal. Because g(n)\nis the actual cost to reach n along the current path, and f(n) = g(n) + h(n), we have as an\nimmediate consequence that f(n) never overestimates the true cost of a solution along the\ncurrent path through n.\nAdmissible heuristics are by nature optimistic because they think the cost of solving\nthe problem is less than it actually is. An obvious example of an admissible heuristic is the\nstraight-line distance hSLD that we used in getting to Bucharest. Straight-line distance is",
  "the problem is less than it actually is. An obvious example of an admissible heuristic is the\nstraight-line distance hSLD that we used in getting to Bucharest. Straight-line distance is\nadmissible because the shortest path between any two points is a straight line, so the straight Section 3.5.\nInformed (Heuristic) Search Strategies\n95\nline cannot be an overestimate. In Figure 3.24, we show the progress of an A∗tree search for\nBucharest. The values of g are computed from the step costs in Figure 3.2, and the values of\nhSLD are given in Figure 3.22. Notice in particular that Bucharest ﬁrst appears on the frontier\nat step (e), but it is not selected for expansion because its f-cost (450) is higher than that of\nPitesti (417). Another way to say this is that there might be a solution through Pitesti whose\ncost is as low as 417, so the algorithm will not settle for a solution that costs 450.\nA second, slightly stronger condition called consistency (or sometimes monotonicity)\nCONSISTENCY\nMONOTONICITY\nis required only for applications of A∗to graph search.9 A heuristic h(n) is consistent if, for\nevery node n and every successor n′ of n generated by any action a, the estimated cost of\nreaching the goal from n is no greater than the step cost of getting to n′ plus the estimated\ncost of reaching the goal from n′:\nh(n) ≤c(n, a, n′) + h(n′) .\nThis is a form of the general triangle inequality, which stipulates that each side of a triangle\nTRIANGLE\nINEQUALITY\ncannot be longer than the sum of the other two sides. Here, the triangle is formed by n, n′,\nand the goal Gn closest to n. For an admissible heuristic, the inequality makes perfect sense:\nif there were a route from n to Gn via n′ that was cheaper than h(n), that would violate the\nproperty that h(n) is a lower bound on the cost to reach Gn.\nIt is fairly easy to show (Exercise 3.29) that every consistent heuristic is also admissible.\nConsistency is therefore a stricter requirement than admissibility, but one has to work quite\nhard to concoct heuristics that are admissible but not consistent. All the admissible heuristics\nwe discuss in this chapter are also consistent. Consider, for example, hSLD. We know that\nthe general triangle inequality is satisﬁed when each side is measured by the straight-line\ndistance and that the straight-line distance between n and n′ is no greater than c(n, a, n′).\nHence, hSLD is a consistent heuristic.\nOptimality of A*",
  "the general triangle inequality is satisﬁed when each side is measured by the straight-line\ndistance and that the straight-line distance between n and n′ is no greater than c(n, a, n′).\nHence, hSLD is a consistent heuristic.\nOptimality of A*\nAs we mentioned earlier, A∗has the following properties: the tree-search version of A∗is\noptimal if h(n) is admissible, while the graph-search version is optimal if h(n) is consistent.\nWe show the second of these two claims since it is more useful. The argument es-\nsentially mirrors the argument for the optimality of uniform-cost search, with g replaced by\nf—just as in the A∗algorithm itself.\nThe ﬁrst step is to establish the following: if h(n) is consistent, then the values of\nf(n) along any path are nondecreasing. The proof follows directly from the deﬁnition of\nconsistency. Suppose n′ is a successor of n; then g(n′) = g(n) + c(n, a, n′) for some action\na, and we have\nf(n′) = g(n′) + h(n′) = g(n) + c(n, a, n′) + h(n′) ≥g(n) + h(n) = f(n) .\nThe next step is to prove that whenever A∗selects a node n for expansion, the optimal path\nto that node has been found. Were this not the case, there would have to be another frontier\nnode n′ on the optimal path from the start node to n, by the graph separation property of\n9 With an admissible but inconsistent heuristic, A∗requires some extra bookkeeping to ensure optimality. 96\nChapter\n3.\nSolving Problems by Searching\n(a) The initial state\n(b) After expanding Arad\n(c) After expanding Sibiu\nArad\nSibiu\nTimisoara\n447=118+329\nZerind\n449=75+374\n393=140+253\nArad\n366=0+366\n(d) After expanding Rimnicu Vilcea\n(e) After expanding Fagaras\n(f) After expanding Pitesti\nZerind\nArad\nSibiu\nArad\nTimisoara\nRimnicu Vilcea\nFagaras\nOradea\n447=118+329\n449=75+374\n646=280+366\n413=220+193\n415=239+176 671=291+380\nZerind\nArad\nSibiu\nTimisoara\n447=118+329\n449=75+374\nRimnicu Vilcea\nCraiova\nPitesti\nSibiu\n526=366+160\n553=300+253\n417=317+100\nZerind\nArad\nSibiu\nArad\nTimisoara\nSibiu\nBucharest\nFagaras\nOradea\nCraiova\nPitesti\nSibiu\n447=118+329\n449=75+374\n646=280+366\n591=338+253\n450=450+0\n526=366+160\n553=300+253\n417=317+100\n671=291+380\nZerind\nArad\nSibiu\nArad\nTimisoara\nSibiu\nBucharest\nOradea\nCraiova\nPitesti\nSibiu\nBucharest\nCraiova\nRimnicu Vilcea\n418=418+0\n447=118+329\n449=75+374\n646=280+366\n591=338+253\n450=450+0\n526=366+160\n553=300+253\n615=455+160 607=414+193\n671=291+380\nRimnicu Vilcea\nFagaras\nRimnicu Vilcea\nArad\nFagaras\nOradea\n646=280+366 415=239+176 671=291+380\nFigure 3.24",
  "Rimnicu Vilcea\n418=418+0\n447=118+329\n449=75+374\n646=280+366\n591=338+253\n450=450+0\n526=366+160\n553=300+253\n615=455+160 607=414+193\n671=291+380\nRimnicu Vilcea\nFagaras\nRimnicu Vilcea\nArad\nFagaras\nOradea\n646=280+366 415=239+176 671=291+380\nFigure 3.24\nStages in an A∗search for Bucharest. Nodes are labeled with f = g + h. The\nh values are the straight-line distances to Bucharest taken from Figure 3.22. Section 3.5.\nInformed (Heuristic) Search Strategies\n97\nO\nZ\nA\nT\nL\nM\nD\nC\nR\nF\nP\nG\nB\nU\nH\nE\nV\nI\nN\n380\n400\n420\nS\nFigure 3.25\nMap of Romania showing contours at f = 380, f = 400, and f = 420, with\nArad as the start state. Nodes inside a given contour have f-costs less than or equal to the\ncontour value.\nFigure 3.9; because f is nondecreasing along any path, n′ would have lower f-cost than n\nand would have been selected ﬁrst.\nFrom the two preceding observations, it follows that the sequence of nodes expanded\nby A∗using GRAPH-SEARCH is in nondecreasing order of f(n). Hence, the ﬁrst goal node\nselected for expansion must be an optimal solution because f is the true cost for goal nodes\n(which have h = 0) and all later goal nodes will be at least as expensive.\nThe fact that f-costs are nondecreasing along any path also means that we can draw\ncontours in the state space, just like the contours in a topographic map. Figure 3.25 shows\nCONTOUR\nan example. Inside the contour labeled 400, all nodes have f(n) less than or equal to 400,\nand so on. Then, because A∗expands the frontier node of lowest f-cost, we can see that an\nA∗search fans out from the start node, adding nodes in concentric bands of increasing f-cost.\nWith uniform-cost search (A∗search using h(n) = 0), the bands will be “circular”\naround the start state. With more accurate heuristics, the bands will stretch toward the goal\nstate and become more narrowly focused around the optimal path. If C∗is the cost of the\noptimal solution path, then we can say the following:\n• A∗expands all nodes with f(n) < C∗.\n• A∗might then expand some of the nodes right on the “goal contour” (where f(n) = C∗)\nbefore selecting a goal node.\nCompleteness requires that there be only ﬁnitely many nodes with cost less than or equal to\nC∗, a condition that is true if all step costs exceed some ﬁnite ϵ and if b is ﬁnite.\nNotice that A∗expands no nodes with f(n) > C∗—for example, Timisoara is not\nexpanded in Figure 3.24 even though it is a child of the root. We say that the subtree below 98\nChapter\n3.\nSolving Problems by Searching",
  "Notice that A∗expands no nodes with f(n) > C∗—for example, Timisoara is not\nexpanded in Figure 3.24 even though it is a child of the root. We say that the subtree below 98\nChapter\n3.\nSolving Problems by Searching\nTimisoara is pruned; because hSLD is admissible, the algorithm can safely ignore this subtree\nPRUNING\nwhile still guaranteeing optimality. The concept of pruning—eliminating possibilities from\nconsideration without having to examine them—is important for many areas of AI.\nOne ﬁnal observation is that among optimal algorithms of this type—algorithms that\nextend search paths from the root and use the same heuristic information—A∗is optimally\nefﬁcient for any given consistent heuristic. That is, no other optimal algorithm is guaran-\nOPTIMALLY\nEFFICIENT\nteed to expand fewer nodes than A∗(except possibly through tie-breaking among nodes with\nf(n) = C∗). This is because any algorithm that does not expand all nodes with f(n) < C∗\nruns the risk of missing the optimal solution.\nThat A∗search is complete, optimal, and optimally efﬁcient among all such algorithms\nis rather satisfying. Unfortunately, it does not mean that A∗is the answer to all our searching\nneeds. The catch is that, for most problems, the number of states within the goal contour\nsearch space is still exponential in the length of the solution. The details of the analysis are\nbeyond the scope of this book, but the basic results are as follows. For problems with constant\nstep costs, the growth in run time as a function of the optimal solution depth d is analyzed in\nterms of the the absolute error or the relative error of the heuristic. The absolute error is\nABSOLUTE ERROR\nRELATIVE ERROR\ndeﬁned as Δ ≡h∗−h, where h∗is the actual cost of getting from the root to the goal, and\nthe relative error is deﬁned as ϵ ≡(h∗−h)/h∗.\nThe complexity results depend very strongly on the assumptions made about the state\nspace. The simplest model studied is a state space that has a single goal and is essentially a\ntree with reversible actions. (The 8-puzzle satisﬁes the ﬁrst and third of these assumptions.)\nIn this case, the time complexity of A∗is exponential in the maximum absolute error, that is,\nO(bΔ). For constant step costs, we can write this as O(bϵd), where d is the solution depth.\nFor almost all heuristics in practical use, the absolute error is at least proportional to the path\ncost h∗, so ϵ is constant or growing and the time complexity is exponential in d. We can",
  "For almost all heuristics in practical use, the absolute error is at least proportional to the path\ncost h∗, so ϵ is constant or growing and the time complexity is exponential in d. We can\nalso see the effect of a more accurate heuristic: O(bϵd) = O((bϵ)d), so the effective branching\nfactor (deﬁned more formally in the next section) is bϵ.\nWhen the state space has many goal states—particularly near-optimal goal states—the\nsearch process can be led astray from the optimal path and there is an extra cost proportional\nto the number of goals whose cost is within a factor ϵ of the optimal cost. Finally, in the\ngeneral case of a graph, the situation is even worse. There can be exponentially many states\nwith f(n) < C∗even if the absolute error is bounded by a constant. For example, consider\na version of the vacuum world where the agent can clean up any square for unit cost without\neven having to visit it: in that case, squares can be cleaned in any order. With N initially dirty\nsquares, there are 2N states where some subset has been cleaned and all of them are on an\noptimal solution path—and hence satisfy f(n) < C∗—even if the heuristic has an error of 1.\nThe complexity of A∗often makes it impractical to insist on ﬁnding an optimal solution.\nOne can use variants of A∗that ﬁnd suboptimal solutions quickly, or one can sometimes\ndesign heuristics that are more accurate but not strictly admissible. In any case, the use of a\ngood heuristic still provides enormous savings compared to the use of an uninformed search.\nIn Section 3.6, we look at the question of designing good heuristics.\nComputation time is not, however, A∗’s main drawback. Because it keeps all generated\nnodes in memory (as do all GRAPH-SEARCH algorithms), A∗usually runs out of space long Section 3.5.\nInformed (Heuristic) Search Strategies\n99\nfunction RECURSIVE-BEST-FIRST-SEARCH(problem) returns a solution, or failure\nreturn RBFS(problem, MAKE-NODE(problem.INITIAL-STATE),∞)\nfunction RBFS(problem,node,f limit) returns a solution, or failure and a new f-cost limit\nif problem.GOAL-TEST(node.STATE) then return SOLUTION(node)\nsuccessors ←[ ]\nfor each action in problem.ACTIONS(node.STATE) do\nadd CHILD-NODE(problem,node,action) into successors\nif successors is empty then return failure, ∞\nfor each s in successors do /* update f with value from previous search, if any */\ns.f ←max(s.g + s.h, node.f ))\nloop do\nbest ←the lowest f-value node in successors\nif best.f > f limit then return failure, best.f",
  "for each s in successors do /* update f with value from previous search, if any */\ns.f ←max(s.g + s.h, node.f ))\nloop do\nbest ←the lowest f-value node in successors\nif best.f > f limit then return failure, best.f\nalternative ←the second-lowest f-value among successors\nresult,best.f ←RBFS(problem,best,min(f limit, alternative))\nif result ̸= failure then return result\nFigure 3.26\nThe algorithm for recursive best-ﬁrst search.\nbefore it runs out of time. For this reason, A∗is not practical for many large-scale prob-\nlems. There are, however, algorithms that overcome the space problem without sacriﬁcing\noptimality or completeness, at a small cost in execution time. We discuss these next.\n3.5.3\nMemory-bounded heuristic search\nThe simplest way to reduce memory requirements for A∗is to adapt the idea of iterative\ndeepening to the heuristic search context, resulting in the iterative-deepening A∗(IDA∗) al-\nITERATIVE-\nDEEPENING\nA∗\ngorithm. The main difference between IDA∗and standard iterative deepening is that the cutoff\nused is the f-cost (g +h) rather than the depth; at each iteration, the cutoff value is the small-\nest f-cost of any node that exceeded the cutoff on the previous iteration. IDA∗is practical\nfor many problems with unit step costs and avoids the substantial overhead associated with\nkeeping a sorted queue of nodes. Unfortunately, it suffers from the same difﬁculties with real-\nvalued costs as does the iterative version of uniform-cost search described in Exercise 3.17.\nThis section brieﬂy examines two other memory-bounded algorithms, called RBFS and MA∗.\nRecursive best-ﬁrst search (RBFS) is a simple recursive algorithm that attempts to\nRECURSIVE\nBEST-FIRST SEARCH\nmimic the operation of standard best-ﬁrst search, but using only linear space. The algorithm\nis shown in Figure 3.26. Its structure is similar to that of a recursive depth-ﬁrst search, but\nrather than continuing indeﬁnitely down the current path, it uses the f limit variable to keep\ntrack of the f-value of the best alternative path available from any ancestor of the current\nnode. If the current node exceeds this limit, the recursion unwinds back to the alternative\npath. As the recursion unwinds, RBFS replaces the f-value of each node along the path\nwith a backed-up value—the best f-value of its children. In this way, RBFS remembers the\nBACKED-UP VALUE\nf-value of the best leaf in the forgotten subtree and can therefore decide whether it’s worth 100\nChapter\n3.\nSolving Problems by Searching",
  "with a backed-up value—the best f-value of its children. In this way, RBFS remembers the\nBACKED-UP VALUE\nf-value of the best leaf in the forgotten subtree and can therefore decide whether it’s worth 100\nChapter\n3.\nSolving Problems by Searching\nZerind\nArad\nSibiu\nArad\nFagaras\nOradea\nCraiova\nSibiu\nBucharest\nCraiova\nRimnicu Vilcea\nZerind\nArad\nSibiu\nArad\nSibiu\nBucharest\nRimnicu Vilcea\nOradea\nZerind\nArad\nSibiu\nArad\nTimisoara\nTimisoara\nTimisoara\nFagaras\nOradea\nRimnicu Vilcea\nCraiova\nPitesti\nSibiu\n646\n415\n671\n526\n553\n646\n671\n450\n591\n646\n671\n526\n553\n418\n615\n607\n447\n449\n447\n447\n449\n449\n366\n393\n366\n393\n413\n413 417\n415\n366\n393\n415 450\n417\nRimnicu Vilcea\nFagaras\n447\n415\n447\n447\n417\n(a) After expanding Arad, Sibiu, \n      and Rimnicu Vilcea\n(c) After switching back to Rimnicu Vilcea\n      and expanding Pitesti\n(b) After unwinding back to Sibiu \n      and expanding Fagaras\n447\n447\n∞\n∞\n∞\n417\n417\nPitesti\nFigure 3.27\nStages in an RBFS search for the shortest route to Bucharest. The f-limit\nvalue for each recursive call is shown on top of each current node, and every node is labeled\nwith its f-cost. (a) The path via Rimnicu Vilcea is followed until the current best leaf (Pitesti)\nhas a value that is worse than the best alternative path (Fagaras). (b) The recursion unwinds\nand the best leaf value of the forgotten subtree (417) is backed up to Rimnicu Vilcea; then\nFagaras is expanded, revealing a best leaf value of 450. (c) The recursion unwinds and the\nbest leaf value of the forgotten subtree (450) is backed up to Fagaras; then Rimnicu Vilcea is\nexpanded. This time, because the best alternative path (through Timisoara) costs at least 447,\nthe expansion continues to Bucharest.\nreexpanding the subtree at some later time. Figure 3.27 shows how RBFS reaches Bucharest.\nRBFS is somewhat more efﬁcient than IDA∗, but still suffers from excessive node re-\ngeneration. In the example in Figure 3.27, RBFS follows the path via Rimnicu Vilcea, then Section 3.5.\nInformed (Heuristic) Search Strategies\n101\n“changes its mind” and tries Fagaras, and then changes its mind back again. These mind\nchanges occur because every time the current best path is extended, its f-value is likely to\nincrease—h is usually less optimistic for nodes closer to the goal. When this happens, the\nsecond-best path might become the best path, so the search has to backtrack to follow it.\nEach mind change corresponds to an iteration of IDA∗and could require many reexpansions",
  "second-best path might become the best path, so the search has to backtrack to follow it.\nEach mind change corresponds to an iteration of IDA∗and could require many reexpansions\nof forgotten nodes to recreate the best path and extend it one more node.\nLike A∗tree search, RBFS is an optimal algorithm if the heuristic function h(n) is\nadmissible. Its space complexity is linear in the depth of the deepest optimal solution, but\nits time complexity is rather difﬁcult to characterize: it depends both on the accuracy of the\nheuristic function and on how often the best path changes as nodes are expanded.\nIDA∗and RBFS suffer from using too little memory. Between iterations, IDA∗retains\nonly a single number: the current f-cost limit. RBFS retains more information in memory,\nbut it uses only linear space: even if more memory were available, RBFS has no way to make\nuse of it. Because they forget most of what they have done, both algorithms may end up reex-\npanding the same states many times over. Furthermore, they suffer the potentially exponential\nincrease in complexity associated with redundant paths in graphs (see Section 3.3).\nIt seems sensible, therefore, to use all available memory. Two algorithms that do this\nare MA∗(memory-bounded A∗) and SMA∗(simpliﬁed MA∗). SMA∗is—well—simpler, so\nMA*\nSMA*\nwe will describe it. SMA∗proceeds just like A∗, expanding the best leaf until memory is full.\nAt this point, it cannot add a new node to the search tree without dropping an old one. SMA∗\nalways drops the worst leaf node—the one with the highest f-value. Like RBFS, SMA∗\nthen backs up the value of the forgotten node to its parent. In this way, the ancestor of a\nforgotten subtree knows the quality of the best path in that subtree. With this information,\nSMA∗regenerates the subtree only when all other paths have been shown to look worse than\nthe path it has forgotten. Another way of saying this is that, if all the descendants of a node n\nare forgotten, then we will not know which way to go from n, but we will still have an idea\nof how worthwhile it is to go anywhere from n.\nThe complete algorithm is too complicated to reproduce here,10 but there is one subtlety\nworth mentioning. We said that SMA∗expands the best leaf and deletes the worst leaf. What\nif all the leaf nodes have the same f-value? To avoid selecting the same node for deletion\nand expansion, SMA∗expands the newest best leaf and deletes the oldest worst leaf. These",
  "if all the leaf nodes have the same f-value? To avoid selecting the same node for deletion\nand expansion, SMA∗expands the newest best leaf and deletes the oldest worst leaf. These\ncoincide when there is only one leaf, but in that case, the current search tree must be a single\npath from root to leaf that ﬁlls all of memory. If the leaf is not a goal node, then even if it is on\nan optimal solution path, that solution is not reachable with the available memory. Therefore,\nthe node can be discarded exactly as if it had no successors.\nSMA∗is complete if there is any reachable solution—that is, if d, the depth of the\nshallowest goal node, is less than the memory size (expressed in nodes). It is optimal if any\noptimal solution is reachable; otherwise, it returns the best reachable solution. In practical\nterms, SMA∗is a fairly robust choice for ﬁnding optimal solutions, particularly when the state\nspace is a graph, step costs are not uniform, and node generation is expensive compared to\nthe overhead of maintaining the frontier and the explored set.\n10 A rough sketch appeared in the ﬁrst edition of this book. 102\nChapter\n3.\nSolving Problems by Searching\nOn very hard problems, however, it will often be the case that SMA∗is forced to switch\nback and forth continually among many candidate solution paths, only a small subset of which\ncan ﬁt in memory. (This resembles the problem of thrashing in disk paging systems.) Then\nTHRASHING\nthe extra time required for repeated regeneration of the same nodes means that problems\nthat would be practically solvable by A∗, given unlimited memory, become intractable for\nSMA∗. That is to say, memory limitations can make a problem intractable from the point\nof view of computation time. Although no current theory explains the tradeoff between time\nand memory, it seems that this is an inescapable problem. The only way out is to drop the\noptimality requirement.\n3.5.4\nLearning to search better\nWe have presented several ﬁxed strategies—breadth-ﬁrst, greedy best-ﬁrst, and so on—that\nhave been designed by computer scientists. Could an agent learn how to search better? The\nanswer is yes, and the method rests on an important concept called the metalevel state space.\nMETALEVEL STATE\nSPACE\nEach state in a metalevel state space captures the internal (computational) state of a program\nthat is searching in an object-level state space such as Romania. For example, the internal\nOBJECT-LEVEL STATE\nSPACE",
  "METALEVEL STATE\nSPACE\nEach state in a metalevel state space captures the internal (computational) state of a program\nthat is searching in an object-level state space such as Romania. For example, the internal\nOBJECT-LEVEL STATE\nSPACE\nstate of the A∗algorithm consists of the current search tree. Each action in the metalevel state\nspace is a computation step that alters the internal state; for example, each computation step\nin A∗expands a leaf node and adds its successors to the tree. Thus, Figure 3.24, which shows\na sequence of larger and larger search trees, can be seen as depicting a path in the metalevel\nstate space where each state on the path is an object-level search tree.\nNow, the path in Figure 3.24 has ﬁve steps, including one step, the expansion of Fagaras,\nthat is not especially helpful. For harder problems, there will be many such missteps, and a\nmetalevel learning algorithm can learn from these experiences to avoid exploring unpromis-\nMETALEVEL\nLEARNING\ning subtrees. The techniques used for this kind of learning are described in Chapter 21. The\ngoal of learning is to minimize the total cost of problem solving, trading off computational\nexpense and path cost.\n3.6\nHEURISTIC FUNCTIONS\nIn this section, we look at heuristics for the 8-puzzle, in order to shed light on the nature of\nheuristics in general.\nThe 8-puzzle was one of the earliest heuristic search problems. As mentioned in Sec-\ntion 3.2, the object of the puzzle is to slide the tiles horizontally or vertically into the empty\nspace until the conﬁguration matches the goal conﬁguration (Figure 3.28).\nThe average solution cost for a randomly generated 8-puzzle instance is about 22 steps.\nThe branching factor is about 3. (When the empty tile is in the middle, four moves are\npossible; when it is in a corner, two; and when it is along an edge, three.) This means\nthat an exhaustive tree search to depth 22 would look at about 322 ≈3.1 × 1010 states.\nA graph search would cut this down by a factor of about 170,000 because only 9!/2 =\n181, 440 distinct states are reachable. (See Exercise 3.4.) This is a manageable number, but Section 3.6.\nHeuristic Functions\n103\n2\nStart State\nGoal State\n1\n3\n4\n6\n7\n5\n1\n2\n3\n4\n6\n7\n8\n5\n8\nFigure 3.28\nA typical instance of the 8-puzzle. The solution is 26 steps long.\nthe corresponding number for the 15-puzzle is roughly 1013, so the next order of business is\nto ﬁnd a good heuristic function. If we want to ﬁnd the shortest solutions by using A∗, we",
  "the corresponding number for the 15-puzzle is roughly 1013, so the next order of business is\nto ﬁnd a good heuristic function. If we want to ﬁnd the shortest solutions by using A∗, we\nneed a heuristic function that never overestimates the number of steps to the goal. There is a\nlong history of such heuristics for the 15-puzzle; here are two commonly used candidates:\n• h1 = the number of misplaced tiles. For Figure 3.28, all of the eight tiles are out of\nposition, so the start state would have h1 = 8. h1 is an admissible heuristic because it\nis clear that any tile that is out of place must be moved at least once.\n• h2 = the sum of the distances of the tiles from their goal positions. Because tiles\ncannot move along diagonals, the distance we will count is the sum of the horizontal\nand vertical distances. This is sometimes called the city block distance or Manhattan\ndistance. h2 is also admissible because all any move can do is move one tile one step\nMANHATTAN\nDISTANCE\ncloser to the goal. Tiles 1 to 8 in the start state give a Manhattan distance of\nh2 = 3 + 1 + 2 + 2 + 2 + 3 + 3 + 2 = 18 .\nAs expected, neither of these overestimates the true solution cost, which is 26.\n3.6.1\nThe effect of heuristic accuracy on performance\nOne way to characterize the quality of a heuristic is the effective branching factor b∗. If the\nEFFECTIVE\nBRANCHING FACTOR\ntotal number of nodes generated by A∗for a particular problem is N and the solution depth is\nd, then b∗is the branching factor that a uniform tree of depth d would have to have in order\nto contain N + 1 nodes. Thus,\nN + 1 = 1 + b∗+ (b∗)2 + · · · + (b∗)d .\nFor example, if A∗ﬁnds a solution at depth 5 using 52 nodes, then the effective branching\nfactor is 1.92. The effective branching factor can vary across problem instances, but usually\nit is fairly constant for sufﬁciently hard problems. (The existence of an effective branching\nfactor follows from the result, mentioned earlier, that the number of nodes expanded by A∗\ngrows exponentially with solution depth.) Therefore, experimental measurements of b∗on a\nsmall set of problems can provide a good guide to the heuristic’s overall usefulness. A well-\ndesigned heuristic would have a value of b∗close to 1, allowing fairly large problems to be\nsolved at reasonable computational cost. 104\nChapter\n3.\nSolving Problems by Searching\nTo test the heuristic functions h1 and h2, we generated 1200 random problems with",
  "solved at reasonable computational cost. 104\nChapter\n3.\nSolving Problems by Searching\nTo test the heuristic functions h1 and h2, we generated 1200 random problems with\nsolution lengths from 2 to 24 (100 for each even number) and solved them with iterative\ndeepening search and with A∗tree search using both h1 and h2. Figure 3.29 gives the average\nnumber of nodes generated by each strategy and the effective branching factor. The results\nsuggest that h2 is better than h1, and is far better than using iterative deepening search. Even\nfor small problems with d = 12, A∗with h2 is 50,000 times more efﬁcient than uninformed\niterative deepening search.\nSearch Cost (nodes generated)\nEffective Branching Factor\nd\nIDS\nA∗(h1)\nA∗(h2)\nIDS\nA∗(h1)\nA∗(h2)\n2\n10\n6\n6\n2.45\n1.79\n1.79\n4\n112\n13\n12\n2.87\n1.48\n1.45\n6\n680\n20\n18\n2.73\n1.34\n1.30\n8\n6384\n39\n25\n2.80\n1.33\n1.24\n10\n47127\n93\n39\n2.79\n1.38\n1.22\n12\n3644035\n227\n73\n2.78\n1.42\n1.24\n14\n–\n539\n113\n–\n1.44\n1.23\n16\n–\n1301\n211\n–\n1.45\n1.25\n18\n–\n3056\n363\n–\n1.46\n1.26\n20\n–\n7276\n676\n–\n1.47\n1.27\n22\n–\n18094\n1219\n–\n1.48\n1.28\n24\n–\n39135\n1641\n–\n1.48\n1.26\nFigure 3.29\nComparison of the search costs and effective branching factors for the\nITERATIVE-DEEPENING-SEARCH and A∗algorithms with h1, h2. Data are averaged over\n100 instances of the 8-puzzle for each of various solution lengths d.\nOne might ask whether h2 is always better than h1. The answer is “Essentially, yes.” It\nis easy to see from the deﬁnitions of the two heuristics that, for any node n, h2(n) ≥h1(n).\nWe thus say that h2 dominates h1. Domination translates directly into efﬁciency: A∗using\nDOMINATION\nh2 will never expand more nodes than A∗using h1 (except possibly for some nodes with\nf(n) = C∗). The argument is simple. Recall the observation on page 97 that every node\nwith f(n) < C∗will surely be expanded. This is the same as saying that every node with\nh(n) < C∗−g(n) will surely be expanded. But because h2 is at least as big as h1 for all\nnodes, every node that is surely expanded by A∗search with h2 will also surely be expanded\nwith h1, and h1 might cause other nodes to be expanded as well. Hence, it is generally\nbetter to use a heuristic function with higher values, provided it is consistent and that the\ncomputation time for the heuristic is not too long.\n3.6.2\nGenerating admissible heuristics from relaxed problems\nWe have seen that both h1 (misplaced tiles) and h2 (Manhattan distance) are fairly good",
  "computation time for the heuristic is not too long.\n3.6.2\nGenerating admissible heuristics from relaxed problems\nWe have seen that both h1 (misplaced tiles) and h2 (Manhattan distance) are fairly good\nheuristics for the 8-puzzle and that h2 is better. How might one have come up with h2? Is it\npossible for a computer to invent such a heuristic mechanically?\nh1 and h2 are estimates of the remaining path length for the 8-puzzle, but they are also\nperfectly accurate path lengths for simpliﬁed versions of the puzzle. If the rules of the puzzle Section 3.6.\nHeuristic Functions\n105\nwere changed so that a tile could move anywhere instead of just to the adjacent empty square,\nthen h1 would give the exact number of steps in the shortest solution. Similarly, if a tile could\nmove one square in any direction, even onto an occupied square, then h2 would give the exact\nnumber of steps in the shortest solution. A problem with fewer restrictions on the actions is\ncalled a relaxed problem. The state-space graph of the relaxed problem is a supergraph of\nRELAXED PROBLEM\nthe original state space because the removal of restrictions creates added edges in the graph.\nBecause the relaxed problem adds edges to the state space, any optimal solution in the\noriginal problem is, by deﬁnition, also a solution in the relaxed problem; but the relaxed\nproblem may have better solutions if the added edges provide short cuts. Hence, the cost of\nan optimal solution to a relaxed problem is an admissible heuristic for the original problem.\nFurthermore, because the derived heuristic is an exact cost for the relaxed problem, it must\nobey the triangle inequality and is therefore consistent (see page 95).\nIf a problem deﬁnition is written down in a formal language, it is possible to construct\nrelaxed problems automatically.11 For example, if the 8-puzzle actions are described as\nA tile can move from square A to square B if\nA is horizontally or vertically adjacent to B and B is blank,\nwe can generate three relaxed problems by removing one or both of the conditions:\n(a) A tile can move from square A to square B if A is adjacent to B.\n(b) A tile can move from square A to square B if B is blank.\n(c) A tile can move from square A to square B.\nFrom (a), we can derive h2 (Manhattan distance). The reasoning is that h2 would be the\nproper score if we moved each tile in turn to its destination. The heuristic derived from (b) is",
  "(c) A tile can move from square A to square B.\nFrom (a), we can derive h2 (Manhattan distance). The reasoning is that h2 would be the\nproper score if we moved each tile in turn to its destination. The heuristic derived from (b) is\ndiscussed in Exercise 3.31. From (c), we can derive h1 (misplaced tiles) because it would be\nthe proper score if tiles could move to their intended destination in one step. Notice that it is\ncrucial that the relaxed problems generated by this technique can be solved essentially without\nsearch, because the relaxed rules allow the problem to be decomposed into eight independent\nsubproblems. If the relaxed problem is hard to solve, then the values of the corresponding\nheuristic will be expensive to obtain.12\nA program called ABSOLVER can generate heuristics automatically from problem def-\ninitions, using the “relaxed problem” method and various other techniques (Prieditis, 1993).\nABSOLVER generated a new heuristic for the 8-puzzle that was better than any preexisting\nheuristic and found the ﬁrst useful heuristic for the famous Rubik’s Cube puzzle.\nOne problem with generating new heuristic functions is that one often fails to get a\nsingle “clearly best” heuristic. If a collection of admissible heuristics h1 . . . hm is available\nfor a problem and none of them dominates any of the others, which should we choose? As it\nturns out, we need not make a choice. We can have the best of all worlds, by deﬁning\nh(n) = max{h1(n), . . . , hm(n)} .\n11 In Chapters 8 and 10, we describe formal languages suitable for this task; with formal descriptions that can be\nmanipulated, the construction of relaxed problems can be automated. For now, we use English.\n12 Note that a perfect heuristic can be obtained simply by allowing h to run a full breadth-ﬁrst search “on the\nsly.” Thus, there is a tradeoff between accuracy and computation time for heuristic functions. 106\nChapter\n3.\nSolving Problems by Searching\nStart State\nGoal State\n1\n2\n3\n4\n6\n8\n5\n2\n1\n3\n6\n7\n8\n54\nFigure 3.30\nA subproblem of the 8-puzzle instance given in Figure 3.28. The task is to\nget tiles 1, 2, 3, and 4 into their correct positions, without worrying about what happens to\nthe other tiles.\nThis composite heuristic uses whichever function is most accurate on the node in question.\nBecause the component heuristics are admissible, h is admissible; it is also easy to prove that\nh is consistent. Furthermore, h dominates all of its component heuristics.\n3.6.3",
  "Because the component heuristics are admissible, h is admissible; it is also easy to prove that\nh is consistent. Furthermore, h dominates all of its component heuristics.\n3.6.3\nGenerating admissible heuristics from subproblems: Pattern databases\nAdmissible heuristics can also be derived from the solution cost of a subproblem of a given\nSUBPROBLEM\nproblem. For example, Figure 3.30 shows a subproblem of the 8-puzzle instance in Fig-\nure 3.28. The subproblem involves getting tiles 1, 2, 3, 4 into their correct positions. Clearly,\nthe cost of the optimal solution of this subproblem is a lower bound on the cost of the com-\nplete problem. It turns out to be more accurate than Manhattan distance in some cases.\nThe idea behind pattern databases is to store these exact solution costs for every pos-\nPATTERN DATABASE\nsible subproblem instance—in our example, every possible conﬁguration of the four tiles\nand the blank. (The locations of the other four tiles are irrelevant for the purposes of solv-\ning the subproblem, but moves of those tiles do count toward the cost.) Then we compute\nan admissible heuristic hDB for each complete state encountered during a search simply by\nlooking up the corresponding subproblem conﬁguration in the database. The database itself is\nconstructed by searching back13 from the goal and recording the cost of each new pattern en-\ncountered; the expense of this search is amortized over many subsequent problem instances.\nThe choice of 1-2-3-4 is fairly arbitrary; we could also construct databases for 5-6-7-8,\nfor 2-4-6-8, and so on. Each database yields an admissible heuristic, and these heuristics can\nbe combined, as explained earlier, by taking the maximum value. A combined heuristic of\nthis kind is much more accurate than the Manhattan distance; the number of nodes generated\nwhen solving random 15-puzzles can be reduced by a factor of 1000.\nOne might wonder whether the heuristics obtained from the 1-2-3-4 database and the\n5-6-7-8 could be added, since the two subproblems seem not to overlap. Would this still give\nan admissible heuristic? The answer is no, because the solutions of the 1-2-3-4 subproblem\nand the 5-6-7-8 subproblem for a given state will almost certainly share some moves—it is\n13 By working backward from the goal, the exact solution cost of every instance encountered is immediately\navailable. This is an example of dynamic programming, which we discuss further in Chapter 17. Section 3.6.\nHeuristic Functions\n107",
  "13 By working backward from the goal, the exact solution cost of every instance encountered is immediately\navailable. This is an example of dynamic programming, which we discuss further in Chapter 17. Section 3.6.\nHeuristic Functions\n107\nunlikely that 1-2-3-4 can be moved into place without touching 5-6-7-8, and vice versa. But\nwhat if we don’t count those moves? That is, we record not the total cost of solving the 1-2-\n3-4 subproblem, but just the number of moves involving 1-2-3-4. Then it is easy to see that\nthe sum of the two costs is still a lower bound on the cost of solving the entire problem. This\nis the idea behind disjoint pattern databases. With such databases, it is possible to solve\nDISJOINT PATTERN\nDATABASES\nrandom 15-puzzles in a few milliseconds—the number of nodes generated is reduced by a\nfactor of 10,000 compared with the use of Manhattan distance. For 24-puzzles, a speedup of\nroughly a factor of a million can be obtained.\nDisjoint pattern databases work for sliding-tile puzzles because the problem can be\ndivided up in such a way that each move affects only one subproblem—because only one tile\nis moved at a time. For a problem such as Rubik’s Cube, this kind of subdivision is difﬁcult\nbecause each move affects 8 or 9 of the 26 cubies. More general ways of deﬁning additive,\nadmissible heuristics have been proposed that do apply to Rubik’s cube (Yang et al., 2008),\nbut they have not yielded a heuristic better than the best nonadditive heuristic for the problem.\n3.6.4\nLearning heuristics from experience\nA heuristic function h(n) is supposed to estimate the cost of a solution beginning from the\nstate at node n. How could an agent construct such a function? One solution was given in\nthe preceding sections—namely, to devise relaxed problems for which an optimal solution\ncan be found easily. Another solution is to learn from experience. “Experience” here means\nsolving lots of 8-puzzles, for instance. Each optimal solution to an 8-puzzle problem provides\nexamples from which h(n) can be learned. Each example consists of a state from the solu-\ntion path and the actual cost of the solution from that point. From these examples, a learning\nalgorithm can be used to construct a function h(n) that can (with luck) predict solution costs\nfor other states that arise during search. Techniques for doing just this using neural nets, de-\ncision trees, and other methods are demonstrated in Chapter 18. (The reinforcement learning",
  "for other states that arise during search. Techniques for doing just this using neural nets, de-\ncision trees, and other methods are demonstrated in Chapter 18. (The reinforcement learning\nmethods described in Chapter 21 are also applicable.)\nInductive learning methods work best when supplied with features of a state that are\nFEATURE\nrelevant to predicting the state’s value, rather than with just the raw state description. For\nexample, the feature “number of misplaced tiles” might be helpful in predicting the actual\ndistance of a state from the goal. Let’s call this feature x1(n). We could take 100 randomly\ngenerated 8-puzzle conﬁgurations and gather statistics on their actual solution costs. We\nmight ﬁnd that when x1(n) is 5, the average solution cost is around 14, and so on. Given\nthese data, the value of x1 can be used to predict h(n). Of course, we can use several features.\nA second feature x2(n) might be “number of pairs of adjacent tiles that are not adjacent in the\ngoal state.” How should x1(n) and x2(n) be combined to predict h(n)? A common approach\nis to use a linear combination:\nh(n) = c1x1(n) + c2x2(n) .\nThe constants c1 and c2 are adjusted to give the best ﬁt to the actual data on solution costs.\nOne expects both c1 and c2 to be positive because misplaced tiles and incorrect adjacent pairs\nmake the problem harder to solve. Notice that this heuristic does satisfy the condition that\nh(n) = 0 for goal states, but it is not necessarily admissible or consistent. 108\nChapter\n3.\nSolving Problems by Searching\n3.7\nSUMMARY\nThis chapter has introduced methods that an agent can use to select actions in environments\nthat are deterministic, observable, static, and completely known. In such cases, the agent can\nconstruct sequences of actions that achieve its goals; this process is called search.\n• Before an agent can start searching for solutions, a goal must be identiﬁed and a well-\ndeﬁned problem must be formulated.\n• A problem consists of ﬁve parts: the initial state, a set of actions, a transition model\ndescribing the results of those actions, a goal test function, and a path cost function.\nThe environment of the problem is represented by a state space. A path through the\nstate space from the initial state to a goal state is a solution.\n• Search algorithms treat states and actions as atomic: they do not consider any internal\nstructure they might possess.\n• A general TREE-SEARCH algorithm considers all possible paths to ﬁnd a solution,",
  "• Search algorithms treat states and actions as atomic: they do not consider any internal\nstructure they might possess.\n• A general TREE-SEARCH algorithm considers all possible paths to ﬁnd a solution,\nwhereas a GRAPH-SEARCH algorithm avoids consideration of redundant paths.\n• Search algorithms are judged on the basis of completeness, optimality, time complex-\nity, and space complexity. Complexity depends on b, the branching factor in the state\nspace, and d, the depth of the shallowest solution.\n• Uninformed search methods have access only to the problem deﬁnition. The basic\nalgorithms are as follows:\n– Breadth-ﬁrst search expands the shallowest nodes ﬁrst; it is complete, optimal\nfor unit step costs, but has exponential space complexity.\n– Uniform-cost search expands the node with lowest path cost, g(n), and is optimal\nfor general step costs.\n– Depth-ﬁrst search expands the deepest unexpanded node ﬁrst. It is neither com-\nplete nor optimal, but has linear space complexity. Depth-limited search adds a\ndepth bound.\n– Iterative deepening search calls depth-ﬁrst search with increasing depth limits\nuntil a goal is found. It is complete, optimal for unit step costs, has time complexity\ncomparable to breadth-ﬁrst search, and has linear space complexity.\n– Bidirectional search can enormously reduce time complexity, but it is not always\napplicable and may require too much space.\n• Informed search methods may have access to a heuristic function h(n) that estimates\nthe cost of a solution from n.\n– The generic best-ﬁrst search algorithm selects a node for expansion according to\nan evaluation function.\n– Greedy best-ﬁrst search expands nodes with minimal h(n). It is not optimal but\nis often efﬁcient. Bibliographical and Historical Notes\n109\n– A∗search expands nodes with minimal f(n) = g(n) + h(n). A∗is complete and\noptimal, provided that h(n) is admissible (for TREE-SEARCH) or consistent (for\nGRAPH-SEARCH). The space complexity of A∗is still prohibitive.\n– RBFS (recursive best-ﬁrst search) and SMA∗(simpliﬁed memory-bounded A∗)\nare robust, optimal search algorithms that use limited amounts of memory; given\nenough time, they can solve problems that A∗cannot solve because it runs out of\nmemory.\n• The performance of heuristic search algorithms depends on the quality of the heuristic\nfunction. One can sometimes construct good heuristics by relaxing the problem deﬁ-\nnition, by storing precomputed solution costs for subproblems in a pattern database, or",
  "function. One can sometimes construct good heuristics by relaxing the problem deﬁ-\nnition, by storing precomputed solution costs for subproblems in a pattern database, or\nby learning from experience with the problem class.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe topic of state-space search originated in more or less its current form in the early years of\nAI. Newell and Simon’s work on the Logic Theorist (1957) and GPS (1961) led to the estab-\nlishment of search algorithms as the primary weapons in the armory of 1960s AI researchers\nand to the establishment of problem solving as the canonical AI task. Work in operations\nresearch by Richard Bellman (1957) showed the importance of additive path costs in sim-\nplifying optimization algorithms. The text on Automated Problem Solving by Nils Nilsson\n(1971) established the area on a solid theoretical footing.\nMost of the state-space search problems analyzed in this chapter have a long history\nin the literature and are less trivial than they might seem. The missionaries and cannibals\nproblem used in Exercise 3.9 was analyzed in detail by Amarel (1968). It had been consid-\nered earlier—in AI by Simon and Newell (1961) and in operations research by Bellman and\nDreyfus (1962).\nThe 8-puzzle is a smaller cousin of the 15-puzzle, whose history is recounted at length\nby Slocum and Sonneveld (2006). It was widely believed to have been invented by the fa-\nmous American game designer Sam Loyd, based on his claims to that effect from 1891 on-\nward (Loyd, 1959). Actually it was invented by Noyes Chapman, a postmaster in Canastota,\nNew York, in the mid-1870s. (Chapman was unable to patent his invention, as a generic\npatent covering sliding blocks with letters, numbers, or pictures was granted to Ernest Kinsey\nin 1878.) It quickly attracted the attention of the public and of mathematicians (Johnson and\nStory, 1879; Tait, 1880). The editors of the American Journal of Mathematics stated, “The\n‘15’ puzzle for the last few weeks has been prominently before the American public, and may\nsafely be said to have engaged the attention of nine out of ten persons of both sexes and all\nages and conditions of the community.” Ratner and Warmuth (1986) showed that the general\nn × n version of the 15-puzzle belongs to the class of NP-complete problems.\nThe 8-queens problem was ﬁrst published anonymously in the German chess maga-\nzine Schach in 1848; it was later attributed to one Max Bezzel. It was republished in 1850",
  "n × n version of the 15-puzzle belongs to the class of NP-complete problems.\nThe 8-queens problem was ﬁrst published anonymously in the German chess maga-\nzine Schach in 1848; it was later attributed to one Max Bezzel. It was republished in 1850\nand at that time drew the attention of the eminent mathematician Carl Friedrich Gauss, who 110\nChapter\n3.\nSolving Problems by Searching\nattempted to enumerate all possible solutions; initially he found only 72, but eventually he\nfound the correct answer of 92, although Nauck published all 92 solutions ﬁrst, in 1850.\nNetto (1901) generalized the problem to n queens, and Abramson and Yung (1989) found an\nO(n) algorithm.\nEach of the real-world search problems listed in the chapter has been the subject of a\ngood deal of research effort. Methods for selecting optimal airline ﬂights remain proprietary\nfor the most part, but Carl de Marcken (personal communication) has shown that airline ticket\npricing and restrictions have become so convoluted that the problem of selecting an optimal\nﬂight is formally undecidable. The traveling-salesperson problem is a standard combinato-\nrial problem in theoretical computer science (Lawler et al., 1992). Karp (1972) proved the\nTSP to be NP-hard, but effective heuristic approximation methods were developed (Lin and\nKernighan, 1973). Arora (1998) devised a fully polynomial approximation scheme for Eu-\nclidean TSPs. VLSI layout methods are surveyed by Shahookar and Mazumder (1991), and\nmany layout optimization papers appear in VLSI journals. Robotic navigation and assembly\nproblems are discussed in Chapter 25.\nUninformed search algorithms for problem solving are a central topic of classical com-\nputer science (Horowitz and Sahni, 1978) and operations research (Dreyfus, 1969). Breadth-\nﬁrst search was formulated for solving mazes by Moore (1959). The method of dynamic\nprogramming (Bellman, 1957; Bellman and Dreyfus, 1962), which systematically records\nsolutions for all subproblems of increasing lengths, can be seen as a form of breadth-ﬁrst\nsearch on graphs. The two-point shortest-path algorithm of Dijkstra (1959) is the origin\nof uniform-cost search. These works also introduced the idea of explored and frontier sets\n(closed and open lists).\nA version of iterative deepening designed to make efﬁcient use of the chess clock was\nﬁrst used by Slate and Atkin (1977) in the CHESS 4.5 game-playing program. Martelli’s",
  "(closed and open lists).\nA version of iterative deepening designed to make efﬁcient use of the chess clock was\nﬁrst used by Slate and Atkin (1977) in the CHESS 4.5 game-playing program. Martelli’s\nalgorithm B (1977) includes an iterative deepening aspect and also dominates A∗’s worst-case\nperformance with admissible but inconsistent heuristics. The iterative deepening technique\ncame to the fore in work by Korf (1985a). Bidirectional search, which was introduced by\nPohl (1971), can also be effective in some cases.\nThe use of heuristic information in problem solving appears in an early paper by Simon\nand Newell (1958), but the phrase “heuristic search” and the use of heuristic functions that\nestimate the distance to the goal came somewhat later (Newell and Ernst, 1965; Lin, 1965).\nDoran and Michie (1966) conducted extensive experimental studies of heuristic search. Al-\nthough they analyzed path length and “penetrance” (the ratio of path length to the total num-\nber of nodes examined so far), they appear to have ignored the information provided by the\npath cost g(n). The A∗algorithm, incorporating the current path cost into heuristic search,\nwas developed by Hart, Nilsson, and Raphael (1968), with some later corrections (Hart et al.,\n1972). Dechter and Pearl (1985) demonstrated the optimal efﬁciency of A∗.\nThe original A∗paper introduced the consistency condition on heuristic functions. The\nmonotone condition was introduced by Pohl (1977) as a simpler replacement, but Pearl (1984)\nshowed that the two were equivalent.\nPohl (1977) pioneered the study of the relationship between the error in heuristic func-\ntions and the time complexity of A∗. Basic results were obtained for tree search with unit step Bibliographical and Historical Notes\n111\ncosts and a single goal node (Pohl, 1977; Gaschnig, 1979; Huyn et al., 1980; Pearl, 1984) and\nwith multiple goal nodes (Dinh et al., 2007). The “effective branching factor” was proposed\nby Nilsson (1971) as an empirical measure of the efﬁciency; it is equivalent to assuming a\ntime cost of O((b∗)d). For tree search applied to a graph, Korf et al. (2001) argue that the time\ncost is better modeled as O(bd−k), where k depends on the heuristic accuracy; this analysis\nhas elicited some controversy, however. For graph search, Helmert and R¨oger (2008) noted\nthat several well-known problems contained exponentially many nodes on optimal solution",
  "has elicited some controversy, however. For graph search, Helmert and R¨oger (2008) noted\nthat several well-known problems contained exponentially many nodes on optimal solution\npaths, implying exponential time complexity for A∗even with constant absolute error in h.\nThere are many variations on the A∗algorithm. Pohl (1973) proposed the use of dynamic\nweighting, which uses a weighted sum fw(n) = wgg(n) + whh(n) of the current path length\nand the heuristic function as an evaluation function, rather than the simple sum f(n) = g(n)+\nh(n) used in A∗. The weights wg and wh are adjusted dynamically as the search progresses.\nPohl’s algorithm can be shown to be ϵ-admissible—that is, guaranteed to ﬁnd solutions within\na factor 1 + ϵ of the optimal solution, where ϵ is a parameter supplied to the algorithm. The\nsame property is exhibited by the A∗\nϵ algorithm (Pearl, 1984), which can select any node from\nthe frontier provided its f-cost is within a factor 1+ ϵ of the lowest-f-cost frontier node. The\nselection can be done so as to minimize search cost.\nBidirectional versions of A∗have been investigated; a combination of bidirectional A∗\nand known landmarks was used to efﬁciently ﬁnd driving routes for Microsoft’s online map\nservice (Goldberg et al., 2006). After caching a set of paths between landmarks, the algorithm\ncan ﬁnd an optimal path between any pair of points in a 24 million point graph of the United\nStates, searching less than 0.1% of the graph. Others approaches to bidirectional search\ninclude a breadth-ﬁrst search backward from the goal up to a ﬁxed depth, followed by a\nforward IDA∗search (Dillenburg and Nelson, 1994; Manzini, 1995).\nA∗and other state-space search algorithms are closely related to the branch-and-bound\ntechniques that are widely used in operations research\n(Lawler and Wood, 1966).\nThe\nrelationships between state-space search and branch-and-bound have been investigated in\ndepth (Kumar and Kanal, 1983; Nau et al., 1984; Kumar et al., 1988). Martelli and Monta-\nnari (1978) demonstrate a connection between dynamic programming (see Chapter 17) and\ncertain types of state-space search. Kumar and Kanal (1988) attempt a “grand uniﬁcation” of\nheuristic search, dynamic programming, and branch-and-bound techniques under the name\nof CDP—the “composite decision process.”\nBecause computers in the late 1950s and early 1960s had at most a few thousand words\nof main memory, memory-bounded heuristic search was an early research topic. The Graph",
  "of CDP—the “composite decision process.”\nBecause computers in the late 1950s and early 1960s had at most a few thousand words\nof main memory, memory-bounded heuristic search was an early research topic. The Graph\nTraverser (Doran and Michie, 1966), one of the earliest search programs, commits to an\noperator after searching best-ﬁrst up to the memory limit. IDA∗(Korf, 1985a, 1985b) was the\nﬁrst widely used optimal, memory-bounded heuristic search algorithm, and a large number\nof variants have been developed. An analysis of the efﬁciency of IDA∗and of its difﬁculties\nwith real-valued heuristics appears in Patrick et al. (1992).\nRBFS (Korf, 1993) is actually somewhat more complicated than the algorithm shown\nin Figure 3.26, which is closer to an independently developed algorithm called iterative ex-\npansion (Russell, 1992). RBFS uses a lower bound as well as the upper bound; the two al-\nITERATIVE\nEXPANSION\ngorithms behave identically with admissible heuristics, but RBFS expands nodes in best-ﬁrst 112\nChapter\n3.\nSolving Problems by Searching\norder even with an inadmissible heuristic. The idea of keeping track of the best alternative\npath appeared earlier in Bratko’s (1986) elegant Prolog implementation of A∗and in the DTA∗\nalgorithm (Russell and Wefald, 1991). The latter work also discusses metalevel state spaces\nand metalevel learning.\nThe MA∗algorithm appeared in Chakrabarti et al. (1989). SMA∗, or Simpliﬁed MA∗,\nemerged from an attempt to implement MA∗as a comparison algorithm for IE (Russell, 1992).\nKaindl and Khorsand (1994) have applied SMA∗to produce a bidirectional search algorithm\nthat is substantially faster than previous algorithms. Korf and Zhang (2000) describe a divide-\nand-conquer approach, and Zhou and Hansen (2002) introduce memory-bounded A∗graph\nsearch and a strategy for switching to breadth-ﬁrst search to increase memory-efﬁciency\n(Zhou and Hansen, 2006). Korf (1995) surveys memory-bounded search techniques.\nThe idea that admissible heuristics can be derived by problem relaxation appears in the\nseminal paper by Held and Karp (1970), who used the minimum-spanning-tree heuristic to\nsolve the TSP. (See Exercise 3.30.)\nThe automation of the relaxation process was implemented successfully by Priedi-\ntis (1993), building on earlier work with Mostow (Mostow and Prieditis, 1989). Holte and\nHernadvolgyi (2001) describe more recent steps towards automating the process. The use of",
  "tis (1993), building on earlier work with Mostow (Mostow and Prieditis, 1989). Holte and\nHernadvolgyi (2001) describe more recent steps towards automating the process. The use of\npattern databases to derive admissible heuristics is due to Gasser (1995) and Culberson and\nSchaeffer (1996, 1998); disjoint pattern databases are described by Korf and Felner (2002);\na similar method using symbolic patterns is due to Edelkamp (2009). Felner et al. (2007)\nshow how to compress pattern databases to save space. The probabilistic interpretation of\nheuristics was investigated in depth by Pearl (1984) and Hansson and Mayer (1989).\nBy far the most comprehensive source on heuristics and heuristic search algorithms\nis Pearl’s (1984) Heuristics text. This book provides especially good coverage of the wide\nvariety of offshoots and variations of A∗, including rigorous proofs of their formal properties.\nKanal and Kumar (1988) present an anthology of important articles on heuristic search, and\nRayward-Smith et al. (1996) cover approaches from Operations Research. Papers about new\nsearch algorithms—which, remarkably, continue to be discovered—appear in journals such\nas Artiﬁcial Intelligence and Journal of the ACM.\nThe topic of parallel search algorithms was not covered in the chapter, partly because\nPARALLEL SEARCH\nit requires a lengthy discussion of parallel computer architectures. Parallel search became a\npopular topic in the 1990s in both AI and theoretical computer science (Mahanti and Daniels,\n1993; Grama and Kumar, 1995; Crauser et al., 1998) and is making a comeback in the era\nof new multicore and cluster architectures (Ralphs et al., 2004; Korf and Schultze, 2005).\nAlso of increasing importance are search algorithms for very large graphs that require disk\nstorage (Korf, 2008).\nEXERCISES\n3.1\nExplain why problem formulation must follow goal formulation.\n3.2\nYour goal is to navigate a robot out of a maze. The robot starts in the center of the maze Exercises\n113\nfacing north. You can turn the robot to face north, east, south, or west. You can direct the\nrobot to move forward a certain distance, although it will stop before hitting a wall.\na. Formulate this problem. How large is the state space?\nb. In navigating a maze, the only place we need to turn is at the intersection of two or\nmore corridors. Reformulate this problem using this observation. How large is the state\nspace now?\nc. From each point in the maze, we can move in any of the four directions until we reach a",
  "more corridors. Reformulate this problem using this observation. How large is the state\nspace now?\nc. From each point in the maze, we can move in any of the four directions until we reach a\nturning point, and this is the only action we need to do. Reformulate the problem using\nthese actions. Do we need to keep track of the robot’s orientation now?\nd. In our initial description of the problem we already abstracted from the real world,\nrestricting actions and removing details. List three such simpliﬁcations we made.\n3.3\nSuppose two friends live in different cities on a map, such as the Romania map shown\nin Figure 3.2. On every turn, we can simultaneously move each friend to a neighboring city\non the map. The amount of time needed to move from city i to neighbor j is equal to the road\ndistance d(i, j) between the cities, but on each turn the friend that arrives ﬁrst must wait until\nthe other one arrives (and calls the ﬁrst on his/her cell phone) before the next turn can begin.\nWe want the two friends to meet as quickly as possible.\na. Write a detailed formulation for this search problem. (You will ﬁnd it helpful to deﬁne\nsome formal notation here.)\nb. Let D(i, j) be the straight-line distance between cities i and j. Which of the following\nheuristic functions are admissible? (i) D(i, j); (ii) 2 · D(i, j); (iii) D(i, j)/2.\nc. Are there completely connected maps for which no solution exists?\nd. Are there maps in which all solutions require one friend to visit the same city twice?\n3.4\nShow that the 8-puzzle states are divided into two disjoint sets, such that any state is\nreachable from any other state in the same set, while no state is reachable from any state in\nthe other set. (Hint: See Berlekamp et al. (1982).) Devise a procedure to decide which set a\ngiven state is in, and explain why this is useful for generating random states.\n3.5\nConsider the n-queens problem using the “efﬁcient” incremental formulation given on\npage 72. Explain why the state space has at least\n3√\nn! states and estimate the largest n for\nwhich exhaustive exploration is feasible. (Hint: Derive a lower bound on the branching factor\nby considering the maximum number of squares that a queen can attack in any column.)\n3.6\nGive a complete problem formulation for each of the following. Choose a formulation\nthat is precise enough to be implemented.\na. Using only four colors, you have to color a planar map in such a way that no two\nadjacent regions have the same color.",
  "Give a complete problem formulation for each of the following. Choose a formulation\nthat is precise enough to be implemented.\na. Using only four colors, you have to color a planar map in such a way that no two\nadjacent regions have the same color.\nb. A 3-foot-tall monkey is in a room where some bananas are suspended from the 8-foot\nceiling. He would like to get the bananas. The room contains two stackable, movable,\nclimbable 3-foot-high crates. 114\nChapter\n3.\nSolving Problems by Searching\nS\nG\nFigure 3.31\nA scene with polygonal obstacles. S and G are the start and goal states.\nc. You have a program that outputs the message “illegal input record” when fed a certain\nﬁle of input records. You know that processing of each record is independent of the\nother records. You want to discover what record is illegal.\nd. You have three jugs, measuring 12 gallons, 8 gallons, and 3 gallons, and a water faucet.\nYou can ﬁll the jugs up or empty them out from one to another or onto the ground. You\nneed to measure out exactly one gallon.\n3.7\nConsider the problem of ﬁnding the shortest path between two points on a plane that has\nconvex polygonal obstacles as shown in Figure 3.31. This is an idealization of the problem\nthat a robot has to solve to navigate in a crowded environment.\na. Suppose the state space consists of all positions (x, y) in the plane. How many states\nare there? How many paths are there to the goal?\nb. Explain brieﬂy why the shortest path from one polygon vertex to any other in the scene\nmust consist of straight-line segments joining some of the vertices of the polygons.\nDeﬁne a good state space now. How large is this state space?\nc. Deﬁne the necessary functions to implement the search problem, including an ACTIONS\nfunction that takes a vertex as input and returns a set of vectors, each of which maps the\ncurrent vertex to one of the vertices that can be reached in a straight line. (Do not forget\nthe neighbors on the same polygon.) Use the straight-line distance for the heuristic\nfunction.\nd. Apply one or more of the algorithms in this chapter to solve a range of problems in the\ndomain, and comment on their performance.\n3.8\nOn page 68, we said that we would not consider problems with negative path costs. In\nthis exercise, we explore this decision in more depth.\na. Suppose that actions can have arbitrarily large negative costs; explain why this possi-\nbility would force any optimal algorithm to explore the entire state space. Exercises\n115",
  "this exercise, we explore this decision in more depth.\na. Suppose that actions can have arbitrarily large negative costs; explain why this possi-\nbility would force any optimal algorithm to explore the entire state space. Exercises\n115\nb. Does it help if we insist that step costs must be greater than or equal to some negative\nconstant c? Consider both trees and graphs.\nc. Suppose that a set of actions forms a loop in the state space such that executing the set in\nsome order results in no net change to the state. If all of these actions have negative cost,\nwhat does this imply about the optimal behavior for an agent in such an environment?\nd. One can easily imagine actions with high negative cost, even in domains such as route\nﬁnding. For example, some stretches of road might have such beautiful scenery as to\nfar outweigh the normal costs in terms of time and fuel. Explain, in precise terms,\nwithin the context of state-space search, why humans do not drive around scenic loops\nindeﬁnitely, and explain how to deﬁne the state space and actions for route ﬁnding so\nthat artiﬁcial agents can also avoid looping.\ne. Can you think of a real domain in which step costs are such as to cause looping?\n3.9\nThe missionaries and cannibals problem is usually stated as follows. Three mission-\naries and three cannibals are on one side of a river, along with a boat that can hold one or\ntwo people. Find a way to get everyone to the other side without ever leaving a group of mis-\nsionaries in one place outnumbered by the cannibals in that place. This problem is famous in\nAI because it was the subject of the ﬁrst paper that approached problem formulation from an\nanalytical viewpoint (Amarel, 1968).\na. Formulate the problem precisely, making only those distinctions necessary to ensure a\nvalid solution. Draw a diagram of the complete state space.\nb. Implement and solve the problem optimally using an appropriate search algorithm. Is it\na good idea to check for repeated states?\nc. Why do you think people have a hard time solving this puzzle, given that the state space\nis so simple?\n3.10\nDeﬁne in your own words the following terms: state, state space, search tree, search\nnode, goal, action, transition model, and branching factor.\n3.11\nWhat’s the difference between a world state, a state description, and a search node?\nWhy is this distinction useful?\n3.12\nAn action such as Go(Sibiu) really consists of a long sequence of ﬁner-grained actions:",
  "3.11\nWhat’s the difference between a world state, a state description, and a search node?\nWhy is this distinction useful?\n3.12\nAn action such as Go(Sibiu) really consists of a long sequence of ﬁner-grained actions:\nturn on the car, release the brake, accelerate forward, etc. Having composite actions of this\nkind reduces the number of steps in a solution sequence, thereby reducing the search time.\nSuppose we take this to the logical extreme, by making super-composite actions out of every\npossible sequence of Go actions. Then every problem instance is solved by a single super-\ncomposite action, such as Go(Sibiu)Go(Rimnicu Vilcea)Go(Pitesti)Go(Bucharest). Explain\nhow search would work in this formulation. Is this a practical approach for speeding up\nproblem solving?\n3.13\nProve that GRAPH-SEARCH satisﬁes the graph separation property illustrated in Fig-\nure 3.9. (Hint: Begin by showing that the property holds at the start, then show that if it holds\nbefore an iteration of the algorithm, it holds afterwards.) Describe a search algorithm that\nviolates the property. 116\nChapter\n3.\nSolving Problems by Searching\nx 12\nx 16\nx 2\nx 2\nFigure 3.32\nThe track pieces in a wooden railway set; each is labeled with the number of\ncopies in the set. Note that curved pieces and “fork” pieces (“switches” or “points”) can be\nﬂipped over so they can curve in either direction. Each curve subtends 45 degrees.\n3.14\nWhich of the following are true and which are false? Explain your answers.\na. Depth-ﬁrst search always expands at least as many nodes as A∗search with an admissi-\nble heuristic.\nb. h(n) = 0 is an admissible heuristic for the 8-puzzle.\nc. A∗is of no use in robotics because percepts, states, and actions are continuous.\nd. Breadth-ﬁrst search is complete even if zero step costs are allowed.\ne. Assume that a rook can move on a chessboard any number of squares in a straight line,\nvertically or horizontally, but cannot jump over other pieces. Manhattan distance is an\nadmissible heuristic for the problem of moving the rook from square A to square B in\nthe smallest number of moves.\n3.15\nConsider a state space where the start state is number 1 and each state k has two\nsuccessors: numbers 2k and 2k + 1.\na. Draw the portion of the state space for states 1 to 15.\nb. Suppose the goal state is 11. List the order in which nodes will be visited for breadth-\nﬁrst search, depth-limited search with limit 3, and iterative deepening search.",
  "a. Draw the portion of the state space for states 1 to 15.\nb. Suppose the goal state is 11. List the order in which nodes will be visited for breadth-\nﬁrst search, depth-limited search with limit 3, and iterative deepening search.\nc. How well would bidirectional search work on this problem? What is the branching\nfactor in each direction of the bidirectional search?\nd. Does the answer to (c) suggest a reformulation of the problem that would allow you to\nsolve the problem of getting from state 1 to a given goal state with almost no search?\ne. Call the action going from k to 2k Left, and the action going to 2k + 1 Right. Can you\nﬁnd an algorithm that outputs the solution to this problem without any search at all?\n3.16\nA basic wooden railway set contains the pieces shown in Figure 3.32. The task is to\nconnect these pieces into a railway that has no overlapping tracks and no loose ends where a\ntrain could run off onto the ﬂoor.\na. Suppose that the pieces ﬁt together exactly with no slack. Give a precise formulation of\nthe task as a search problem.\nb. Identify a suitable uninformed search algorithm for this task and explain your choice.\nc. Explain why removing any one of the “fork” pieces makes the problem unsolvable. Exercises\n117\nd. Give an upper bound on the total size of the state space deﬁned by your formulation.\n(Hint: think about the maximum branching factor for the construction process and the\nmaximum depth, ignoring the problem of overlapping pieces and loose ends. Begin by\npretending that every piece is unique.)\n3.17\nOn page 90, we mentioned iterative lengthening search, an iterative analog of uni-\nform cost search. The idea is to use increasing limits on path cost. If a node is generated\nwhose path cost exceeds the current limit, it is immediately discarded. For each new itera-\ntion, the limit is set to the lowest path cost of any node discarded in the previous iteration.\na. Show that this algorithm is optimal for general path costs.\nb. Consider a uniform tree with branching factor b, solution depth d, and unit step costs.\nHow many iterations will iterative lengthening require?\nc. Now consider step costs drawn from the continuous range [ϵ, 1], where 0 < ϵ < 1. How\nmany iterations are required in the worst case?\nd. Implement the algorithm and apply it to instances of the 8-puzzle and traveling sales-\nperson problems. Compare the algorithm’s performance to that of uniform-cost search,\nand comment on your results.\n3.18",
  "d. Implement the algorithm and apply it to instances of the 8-puzzle and traveling sales-\nperson problems. Compare the algorithm’s performance to that of uniform-cost search,\nand comment on your results.\n3.18\nDescribe a state space in which iterative deepening search performs much worse than\ndepth-ﬁrst search (for example, O(n2) vs. O(n)).\n3.19\nWrite a program that will take as input two Web page URLs and ﬁnd a path of links\nfrom one to the other. What is an appropriate search strategy? Is bidirectional search a good\nidea? Could a search engine be used to implement a predecessor function?\n3.20\nConsider the vacuum-world problem deﬁned in Figure 2.2.\na. Which of the algorithms deﬁned in this chapter would be appropriate for this problem?\nShould the algorithm use tree search or graph search?\nb. Apply your chosen algorithm to compute an optimal sequence of actions for a 3 × 3\nworld whose initial state has dirt in the three top squares and the agent in the center.\nc. Construct a search agent for the vacuum world, and evaluate its performance in a set of\n3 × 3 worlds with probability 0.2 of dirt in each square. Include the search cost as well\nas path cost in the performance measure, using a reasonable exchange rate.\nd. Compare your best search agent with a simple randomized reﬂex agent that sucks if\nthere is dirt and otherwise moves randomly.\ne. Consider what would happen if the world were enlarged to n × n. How does the per-\nformance of the search agent and of the reﬂex agent vary with n?\n3.21\nProve each of the following statements, or give a counterexample:\na. Breadth-ﬁrst search is a special case of uniform-cost search.\nb. Depth-ﬁrst search is a special case of best-ﬁrst tree search.\nc. Uniform-cost search is a special case of A∗search. 118\nChapter\n3.\nSolving Problems by Searching\n3.22\nCompare the performance of A∗and RBFS on a set of randomly generated problems\nin the 8-puzzle (with Manhattan distance) and TSP (with MST—see Exercise 3.30) domains.\nDiscuss your results. What happens to the performance of RBFS when a small random num-\nber is added to the heuristic values in the 8-puzzle domain?\n3.23\nTrace the operation of A∗search applied to the problem of getting to Bucharest from\nLugoj using the straight-line distance heuristic. That is, show the sequence of nodes that the\nalgorithm will consider and the f, g, and h score for each node.\n3.24\nDevise a state space in which A∗using GRAPH-SEARCH returns a suboptimal solution",
  "Lugoj using the straight-line distance heuristic. That is, show the sequence of nodes that the\nalgorithm will consider and the f, g, and h score for each node.\n3.24\nDevise a state space in which A∗using GRAPH-SEARCH returns a suboptimal solution\nwith an h(n) function that is admissible but inconsistent.\n3.25\nThe heuristic path algorithm (Pohl, 1977) is a best-ﬁrst search in which the evalu-\nHEURISTIC PATH\nALGORITHM\nation function is f(n) = (2 −w)g(n) + wh(n). For what values of w is this complete?\nFor what values is it optimal, assuming that h is admissible? What kind of search does this\nperform for w = 0, w = 1, and w = 2?\n3.26\nConsider the unbounded version of the regular 2D grid shown in Figure 3.9. The start\nstate is at the origin, (0,0), and the goal state is at (x, y).\na. What is the branching factor b in this state space?\nb. How many distinct states are there at depth k (for k > 0)?\nc. What is the maximum number of nodes expanded by breadth-ﬁrst tree search?\nd. What is the maximum number of nodes expanded by breadth-ﬁrst graph search?\ne. Is h = |u −x| + |v −y| an admissible heuristic for a state at (u, v)? Explain.\nf. How many nodes are expanded by A∗graph search using h?\ng. Does h remain admissible if some links are removed?\nh. Does h remain admissible if some links are added between nonadjacent states?\n3.27\nn vehicles occupy squares (1, 1) through (n, 1) (i.e., the bottom row) of an n × n grid.\nThe vehicles must be moved to the top row but in reverse order; so the vehicle i that starts in\n(i, 1) must end up in (n −i + 1, n). On each time step, every one of the n vehicles can move\none square up, down, left, or right, or stay put; but if a vehicle stays put, one other adjacent\nvehicle (but not more than one) can hop over it. Two vehicles cannot occupy the same square.\na. Calculate the size of the state space as a function of n.\nb. Calculate the branching factor as a function of n.\nc. Suppose that vehicle i is at (xi, yi); write a nontrivial admissible heuristic hi for the\nnumber of moves it will require to get to its goal location (n −i + 1, n), assuming no\nother vehicles are on the grid.\nd. Which of the following heuristics are admissible for the problem of moving all n vehi-\ncles to their destinations? Explain.\n(i) \u0002n\ni = 1 hi.\n(ii) max{h1, . . . , hn}.\n(iii) min{h1, . . . , hn}. Exercises\n119\n3.28\nInvent a heuristic function for the 8-puzzle that sometimes overestimates, and show",
  "cles to their destinations? Explain.\n(i) \u0002n\ni = 1 hi.\n(ii) max{h1, . . . , hn}.\n(iii) min{h1, . . . , hn}. Exercises\n119\n3.28\nInvent a heuristic function for the 8-puzzle that sometimes overestimates, and show\nhow it can lead to a suboptimal solution on a particular problem. (You can use a computer to\nhelp if you want.) Prove that if h never overestimates by more than c, A∗using h returns a\nsolution whose cost exceeds that of the optimal solution by no more than c.\n3.29\nProve that if a heuristic is consistent, it must be admissible. Construct an admissible\nheuristic that is not consistent.\n3.30\nThe traveling salesperson problem (TSP) can be solved with the minimum-spanning-\ntree (MST) heuristic, which estimates the cost of completing a tour, given that a partial tour\nhas already been constructed. The MST cost of a set of cities is the smallest sum of the link\ncosts of any tree that connects all the cities.\na. Show how this heuristic can be derived from a relaxed version of the TSP.\nb. Show that the MST heuristic dominates straight-line distance.\nc. Write a problem generator for instances of the TSP where cities are represented by\nrandom points in the unit square.\nd. Find an efﬁcient algorithm in the literature for constructing the MST, and use it with A∗\ngraph search to solve instances of the TSP.\n3.31\nOn page 105, we deﬁned the relaxation of the 8-puzzle in which a tile can move from\nsquare A to square B if B is blank. The exact solution of this problem deﬁnes Gaschnig’s\nheuristic (Gaschnig, 1979). Explain why Gaschnig’s heuristic is at least as accurate as h1\n(misplaced tiles), and show cases where it is more accurate than both h1 and h2 (Manhattan\ndistance). Explain how to calculate Gaschnig’s heuristic efﬁciently.\n3.32\nWe gave two simple heuristics for the 8-puzzle: Manhattan distance and misplaced\ntiles. Several heuristics in the literature purport to improve on this—see, for example, Nils-\nson (1971), Mostow and Prieditis (1989), and Hansson et al. (1992). Test these claims by\nimplementing the heuristics and comparing the performance of the resulting algorithms. 4\nBEYOND CLASSICAL\nSEARCH\nIn which we relax the simplifying assumptions of the previous chapter, thereby\ngetting closer to the real world.\nChapter 3 addressed a single category of problems: observable, deterministic, known envi-\nronments where the solution is a sequence of actions. In this chapter, we look at what happens",
  "getting closer to the real world.\nChapter 3 addressed a single category of problems: observable, deterministic, known envi-\nronments where the solution is a sequence of actions. In this chapter, we look at what happens\nwhen these assumptions are relaxed. We begin with a fairly simple case: Sections 4.1 and 4.2\ncover algorithms that perform purely local search in the state space, evaluating and modify-\ning one or more current states rather than systematically exploring paths from an initial state.\nThese algorithms are suitable for problems in which all that matters is the solution state, not\nthe path cost to reach it. The family of local search algorithms includes methods inspired by\nstatistical physics (simulated annealing) and evolutionary biology (genetic algorithms).\nThen, in Sections 4.3–4.4, we examine what happens when we relax the assumptions\nof determinism and observability. The key idea is that if an agent cannot predict exactly what\npercept it will receive, then it will need to consider what to do under each contingency that\nits percepts may reveal. With partial observability, the agent will also need to keep track of\nthe states it might be in.\nFinally, Section 4.5 investigates online search, in which the agent is faced with a state\nspace that is initially unknown and must be explored.\n4.1\nLOCAL SEARCH ALGORITHMS AND OPTIMIZATION PROBLEMS\nThe search algorithms that we have seen so far are designed to explore search spaces sys-\ntematically. This systematicity is achieved by keeping one or more paths in memory and by\nrecording which alternatives have been explored at each point along the path. When a goal is\nfound, the path to that goal also constitutes a solution to the problem. In many problems, how-\never, the path to the goal is irrelevant. For example, in the 8-queens problem (see page 71),\nwhat matters is the ﬁnal conﬁguration of queens, not the order in which they are added. The\nsame general property holds for many important applications such as integrated-circuit de-\nsign, factory-ﬂoor layout, job-shop scheduling, automatic programming, telecommunications\nnetwork optimization, vehicle routing, and portfolio management.\n120 Section 4.1.\nLocal Search Algorithms and Optimization Problems\n121\nIf the path to the goal does not matter, we might consider a different class of algo-\nrithms, ones that do not worry about paths at all. Local search algorithms operate using\nLOCAL SEARCH",
  "Local Search Algorithms and Optimization Problems\n121\nIf the path to the goal does not matter, we might consider a different class of algo-\nrithms, ones that do not worry about paths at all. Local search algorithms operate using\nLOCAL SEARCH\na single current node (rather than multiple paths) and generally move only to neighbors\nCURRENT NODE\nof that node. Typically, the paths followed by the search are not retained. Although local\nsearch algorithms are not systematic, they have two key advantages: (1) they use very little\nmemory—usually a constant amount; and (2) they can often ﬁnd reasonable solutions in large\nor inﬁnite (continuous) state spaces for which systematic algorithms are unsuitable.\nIn addition to ﬁnding goals, local search algorithms are useful for solving pure op-\ntimization problems, in which the aim is to ﬁnd the best state according to an objective\nOPTIMIZATION\nPROBLEM\nfunction. Many optimization problems do not ﬁt the “standard” search model introduced in\nOBJECTIVE\nFUNCTION\nChapter 3. For example, nature provides an objective function—reproductive ﬁtness—that\nDarwinian evolution could be seen as attempting to optimize, but there is no “goal test” and\nno “path cost” for this problem.\nTo understand local search, we ﬁnd it useful to consider the state-space landscape (as\nSTATE-SPACE\nLANDSCAPE\nin Figure 4.1). A landscape has both “location” (deﬁned by the state) and “elevation” (deﬁned\nby the value of the heuristic cost function or objective function). If elevation corresponds to\ncost, then the aim is to ﬁnd the lowest valley—a global minimum; if elevation corresponds\nGLOBAL MINIMUM\nto an objective function, then the aim is to ﬁnd the highest peak—a global maximum. (You\nGLOBAL MAXIMUM\ncan convert from one to the other just by inserting a minus sign.) Local search algorithms\nexplore this landscape. A complete local search algorithm always ﬁnds a goal if one exists;\nan optimal algorithm always ﬁnds a global minimum/maximum.\ncurrent\nstate\nobjective function\nstate space\nglobal maximum\nlocal maximum\n“flat” local maximum\nshoulder\nFigure 4.1\nA one-dimensional state-space landscape in which elevation corresponds to the\nobjective function. The aim is to ﬁnd the global maximum. Hill-climbing search modiﬁes\nthe current state to try to improve it, as shown by the arrow. The various topographic features\nare deﬁned in the text. 122\nChapter\n4.\nBeyond Classical Search\nfunction HILL-CLIMBING(problem) returns a state that is a local maximum",
  "the current state to try to improve it, as shown by the arrow. The various topographic features\nare deﬁned in the text. 122\nChapter\n4.\nBeyond Classical Search\nfunction HILL-CLIMBING(problem) returns a state that is a local maximum\ncurrent ←MAKE-NODE(problem.INITIAL-STATE)\nloop do\nneighbor ←a highest-valued successor of current\nif neighbor.VALUE ≤current.VALUE then return current.STATE\ncurrent ←neighbor\nFigure 4.2\nThe hill-climbing search algorithm, which is the most basic local search tech-\nnique. At each step the current node is replaced by the best neighbor; in this version, that\nmeans the neighbor with the highest VALUE, but if a heuristic cost estimate h is used, we\nwould ﬁnd the neighbor with the lowest h.\n4.1.1\nHill-climbing search\nThe hill-climbing search algorithm (steepest-ascent version) is shown in Figure 4.2. It is\nHILL CLIMBING\nSTEEPEST ASCENT\nsimply a loop that continually moves in the direction of increasing value—that is, uphill. It\nterminates when it reaches a “peak” where no neighbor has a higher value. The algorithm\ndoes not maintain a search tree, so the data structure for the current node need only record\nthe state and the value of the objective function. Hill climbing does not look ahead beyond\nthe immediate neighbors of the current state. This resembles trying to ﬁnd the top of Mount\nEverest in a thick fog while suffering from amnesia.\nTo illustrate hill climbing, we will use the 8-queens problem introduced on page 71.\nLocal search algorithms typically use a complete-state formulation, where each state has\n8 queens on the board, one per column. The successors of a state are all possible states\ngenerated by moving a single queen to another square in the same column (so each state has\n8 × 7 = 56 successors). The heuristic cost function h is the number of pairs of queens that\nare attacking each other, either directly or indirectly. The global minimum of this function\nis zero, which occurs only at perfect solutions. Figure 4.3(a) shows a state with h = 17. The\nﬁgure also shows the values of all its successors, with the best successors having h = 12.\nHill-climbing algorithms typically choose randomly among the set of best successors if there\nis more than one.\nHill climbing is sometimes called greedy local search because it grabs a good neighbor\nGREEDY LOCAL\nSEARCH\nstate without thinking ahead about where to go next. Although greed is considered one of the",
  "is more than one.\nHill climbing is sometimes called greedy local search because it grabs a good neighbor\nGREEDY LOCAL\nSEARCH\nstate without thinking ahead about where to go next. Although greed is considered one of the\nseven deadly sins, it turns out that greedy algorithms often perform quite well. Hill climbing\noften makes rapid progress toward a solution because it is usually quite easy to improve a bad\nstate. For example, from the state in Figure 4.3(a), it takes just ﬁve steps to reach the state\nin Figure 4.3(b), which has h = 1 and is very nearly a solution. Unfortunately, hill climbing\noften gets stuck for the following reasons:\n• Local maxima: a local maximum is a peak that is higher than each of its neighboring\nLOCAL MAXIMUM\nstates but lower than the global maximum. Hill-climbing algorithms that reach the\nvicinity of a local maximum will be drawn upward toward the peak but will then be\nstuck with nowhere else to go. Figure 4.1 illustrates the problem schematically. More Section 4.1.\nLocal Search Algorithms and Optimization Problems\n123\n14\n18\n17\n15\n14\n18\n14\n14\n14\n14\n14\n12\n16\n12\n13\n16\n17\n14\n18\n13\n14\n17\n15\n18\n15\n13\n15\n13\n12\n15\n15\n13\n15\n12\n13\n14\n14\n14\n16\n12\n14\n12\n12\n15\n16\n13\n14\n12\n14\n18\n16\n16\n16\n14\n16\n14\n(a)\n(b)\nFigure 4.3\n(a) An 8-queens state with heuristic cost estimate h = 17, showing the value of\nh for each possible successor obtained by moving a queen within its column. The best moves\nare marked. (b) A local minimum in the 8-queens state space; the state has h = 1 but every\nsuccessor has a higher cost.\nconcretely, the state in Figure 4.3(b) is a local maximum (i.e., a local minimum for the\ncost h); every move of a single queen makes the situation worse.\n• Ridges: a ridge is shown in Figure 4.4. Ridges result in a sequence of local maxima\nRIDGE\nthat is very difﬁcult for greedy algorithms to navigate.\n• Plateaux: a plateau is a ﬂat area of the state-space landscape. It can be a ﬂat local\nPLATEAU\nmaximum, from which no uphill exit exists, or a shoulder, from which progress is\nSHOULDER\npossible. (See Figure 4.1.) A hill-climbing search might get lost on the plateau.\nIn each case, the algorithm reaches a point at which no progress is being made. Starting from\na randomly generated 8-queens state, steepest-ascent hill climbing gets stuck 86% of the time,\nsolving only 14% of problem instances. It works quickly, taking just 4 steps on average when\nit succeeds and 3 when it gets stuck—not bad for a state space with 88 ≈17 million states.",
  "solving only 14% of problem instances. It works quickly, taking just 4 steps on average when\nit succeeds and 3 when it gets stuck—not bad for a state space with 88 ≈17 million states.\nThe algorithm in Figure 4.2 halts if it reaches a plateau where the best successor has\nthe same value as the current state. Might it not be a good idea to keep going—to allow a\nsideways move in the hope that the plateau is really a shoulder, as shown in Figure 4.1? The\nSIDEWAYS MOVE\nanswer is usually yes, but we must take care. If we always allow sideways moves when there\nare no uphill moves, an inﬁnite loop will occur whenever the algorithm reaches a ﬂat local\nmaximum that is not a shoulder. One common solution is to put a limit on the number of con-\nsecutive sideways moves allowed. For example, we could allow up to, say, 100 consecutive\nsideways moves in the 8-queens problem. This raises the percentage of problem instances\nsolved by hill climbing from 14% to 94%. Success comes at a cost: the algorithm averages\nroughly 21 steps for each successful instance and 64 for each failure. 124\nChapter\n4.\nBeyond Classical Search\nFigure 4.4\nIllustration of why ridges cause difﬁculties for hill climbing. The grid of states\n(dark circles) is superimposed on a ridge rising from left to right, creating a sequence of local\nmaxima that are not directly connected to each other. From each local maximum, all the\navailable actions point downhill.\nMany variants of hill climbing have been invented. Stochastic hill climbing chooses at\nSTOCHASTIC HILL\nCLIMBING\nrandom from among the uphill moves; the probability of selection can vary with the steepness\nof the uphill move. This usually converges more slowly than steepest ascent, but in some\nstate landscapes, it ﬁnds better solutions. First-choice hill climbing implements stochastic\nFIRST-CHOICE HILL\nCLIMBING\nhill climbing by generating successors randomly until one is generated that is better than the\ncurrent state. This is a good strategy when a state has many (e.g., thousands) of successors.\nThe hill-climbing algorithms described so far are incomplete—they often fail to ﬁnd\na goal when one exists because they can get stuck on local maxima. Random-restart hill\nclimbing adopts the well-known adage, “If at ﬁrst you don’t succeed, try, try again.” It con-\nRANDOM-RESTART\nHILL CLIMBING\nducts a series of hill-climbing searches from randomly generated initial states,1 until a goal",
  "climbing adopts the well-known adage, “If at ﬁrst you don’t succeed, try, try again.” It con-\nRANDOM-RESTART\nHILL CLIMBING\nducts a series of hill-climbing searches from randomly generated initial states,1 until a goal\nis found. It is trivially complete with probability approaching 1, because it will eventually\ngenerate a goal state as the initial state. If each hill-climbing search has a probability p of\nsuccess, then the expected number of restarts required is 1/p. For 8-queens instances with\nno sideways moves allowed, p ≈0.14, so we need roughly 7 iterations to ﬁnd a goal (6 fail-\nures and 1 success). The expected number of steps is the cost of one successful iteration plus\n(1−p)/p times the cost of failure, or roughly 22 steps in all. When we allow sideways moves,\n1/0.94 ≈1.06 iterations are needed on average and (1 × 21) + (0.06/0.94) × 64 ≈25 steps.\nFor 8-queens, then, random-restart hill climbing is very effective indeed. Even for three mil-\nlion queens, the approach can ﬁnd solutions in under a minute.2\n1 Generating a random state from an implicitly speciﬁed state space can be a hard problem in itself.\n2 Luby et al. (1993) prove that it is best, in some cases, to restart a randomized search algorithm after a particular,\nﬁxed amount of time and that this can be much more efﬁcient than letting each search continue indeﬁnitely.\nDisallowing or limiting the number of sideways moves is an example of this idea. Section 4.1.\nLocal Search Algorithms and Optimization Problems\n125\nThe success of hill climbing depends very much on the shape of the state-space land-\nscape: if there are few local maxima and plateaux, random-restart hill climbing will ﬁnd a\ngood solution very quickly. On the other hand, many real problems have a landscape that\nlooks more like a widely scattered family of balding porcupines on a ﬂat ﬂoor, with miniature\nporcupines living on the tip of each porcupine needle, ad inﬁnitum. NP-hard problems typi-\ncally have an exponential number of local maxima to get stuck on. Despite this, a reasonably\ngood local maximum can often be found after a small number of restarts.\n4.1.2\nSimulated annealing\nA hill-climbing algorithm that never makes “downhill” moves toward states with lower value\n(or higher cost) is guaranteed to be incomplete, because it can get stuck on a local maxi-\nmum. In contrast, a purely random walk—that is, moving to a successor chosen uniformly\nat random from the set of successors—is complete but extremely inefﬁcient. Therefore, it",
  "mum. In contrast, a purely random walk—that is, moving to a successor chosen uniformly\nat random from the set of successors—is complete but extremely inefﬁcient. Therefore, it\nseems reasonable to try to combine hill climbing with a random walk in some way that yields\nboth efﬁciency and completeness. Simulated annealing is such an algorithm. In metallurgy,\nSIMULATED\nANNEALING\nannealing is the process used to temper or harden metals and glass by heating them to a\nhigh temperature and then gradually cooling them, thus allowing the material to reach a low-\nenergy crystalline state. To explain simulated annealing, we switch our point of view from\nhill climbing to gradient descent (i.e., minimizing cost) and imagine the task of getting a\nGRADIENT DESCENT\nping-pong ball into the deepest crevice in a bumpy surface. If we just let the ball roll, it will\ncome to rest at a local minimum. If we shake the surface, we can bounce the ball out of the\nlocal minimum. The trick is to shake just hard enough to bounce the ball out of local min-\nima but not hard enough to dislodge it from the global minimum. The simulated-annealing\nsolution is to start by shaking hard (i.e., at a high temperature) and then gradually reduce the\nintensity of the shaking (i.e., lower the temperature).\nThe innermost loop of the simulated-annealing algorithm (Figure 4.5) is quite similar to\nhill climbing. Instead of picking the best move, however, it picks a random move. If the move\nimproves the situation, it is always accepted. Otherwise, the algorithm accepts the move with\nsome probability less than 1. The probability decreases exponentially with the “badness” of\nthe move—the amount ΔE by which the evaluation is worsened. The probability also de-\ncreases as the “temperature” T goes down: “bad” moves are more likely to be allowed at the\nstart when T is high, and they become more unlikely as T decreases. If the schedule lowers\nT slowly enough, the algorithm will ﬁnd a global optimum with probability approaching 1.\nSimulated annealing was ﬁrst used extensively to solve VLSI layout problems in the\nearly 1980s. It has been applied widely to factory scheduling and other large-scale optimiza-\ntion tasks. In Exercise 4.4, you are asked to compare its performance to that of random-restart\nhill climbing on the 8-queens puzzle.\n4.1.3\nLocal beam search\nKeeping just one node in memory might seem to be an extreme reaction to the problem of",
  "tion tasks. In Exercise 4.4, you are asked to compare its performance to that of random-restart\nhill climbing on the 8-queens puzzle.\n4.1.3\nLocal beam search\nKeeping just one node in memory might seem to be an extreme reaction to the problem of\nmemory limitations. The local beam search algorithm3 keeps track of k states rather than\nLOCAL BEAM\nSEARCH\n3 Local beam search is an adaptation of beam search, which is a path-based algorithm. 126\nChapter\n4.\nBeyond Classical Search\nfunction SIMULATED-ANNEALING(problem,schedule) returns a solution state\ninputs: problem, a problem\nschedule, a mapping from time to “temperature”\ncurrent ←MAKE-NODE(problem.INITIAL-STATE)\nfor t = 1 to ∞do\nT ←schedule(t)\nif T = 0 then return current\nnext ←a randomly selected successor of current\nΔE ←next.VALUE – current.VALUE\nif ΔE > 0 then current ←next\nelse current ←next only with probability eΔE/T\nFigure 4.5\nThe simulated annealing algorithm, a version of stochastic hill climbing where\nsome downhill moves are allowed. Downhill moves are accepted readily early in the anneal-\ning schedule and then less often as time goes on. The schedule input determines the value of\nthe temperature T as a function of time.\njust one. It begins with k randomly generated states. At each step, all the successors of all k\nstates are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best\nsuccessors from the complete list and repeats.\nAt ﬁrst sight, a local beam search with k states might seem to be nothing more than\nrunning k random restarts in parallel instead of in sequence. In fact, the two algorithms\nare quite different. In a random-restart search, each search process runs independently of\nthe others. In a local beam search, useful information is passed among the parallel search\nthreads. In effect, the states that generate the best successors say to the others, “Come over\nhere, the grass is greener!” The algorithm quickly abandons unfruitful searches and moves\nits resources to where the most progress is being made.\nIn its simplest form, local beam search can suffer from a lack of diversity among the\nk states—they can quickly become concentrated in a small region of the state space, making\nthe search little more than an expensive version of hill climbing. A variant called stochastic\nbeam search, analogous to stochastic hill climbing, helps alleviate this problem. Instead\nSTOCHASTIC BEAM\nSEARCH",
  "the search little more than an expensive version of hill climbing. A variant called stochastic\nbeam search, analogous to stochastic hill climbing, helps alleviate this problem. Instead\nSTOCHASTIC BEAM\nSEARCH\nof choosing the best k from the the pool of candidate successors, stochastic beam search\nchooses k successors at random, with the probability of choosing a given successor being\nan increasing function of its value. Stochastic beam search bears some resemblance to the\nprocess of natural selection, whereby the “successors” (offspring) of a “state” (organism)\npopulate the next generation according to its “value” (ﬁtness).\n4.1.4\nGenetic algorithms\nA genetic algorithm (or GA) is a variant of stochastic beam search in which successor states\nGENETIC\nALGORITHM\nare generated by combining two parent states rather than by modifying a single state. The\nanalogy to natural selection is the same as in stochastic beam search, except that now we are\ndealing with sexual rather than asexual reproduction. Section 4.1.\nLocal Search Algorithms and Optimization Problems\n127\n(a)\nInitial Population\n(b)\nFitness Function\n(c)\nSelection\n(d)\nCrossover\n(e)\nMutation\n24\n23\n20\n11\n29%\n31%\n26%\n14%\n32752411\n24748552\n32752411\n24415124\n32748552\n24752411\n32752124\n24415411\n32252124\n24752411\n32748152\n24415417\n24748552\n32752411\n24415124\n32543213\nFigure 4.6\nThe genetic algorithm, illustrated for digit strings representing 8-queens states.\nThe initial population in (a) is ranked by the ﬁtness function in (b), resulting in pairs for\nmating in (c). They produce offspring in (d), which are subject to mutation in (e).\n+\n=\nFigure 4.7\nThe 8-queens states corresponding to the ﬁrst two parents in Figure 4.6(c) and\nthe ﬁrst offspring in Figure 4.6(d). The shaded columns are lost in the crossover step and the\nunshaded columns are retained.\nLike beam searches, GAs begin with a set of k randomly generated states, called the\npopulation. Each state, or individual, is represented as a string over a ﬁnite alphabet—most\nPOPULATION\nINDIVIDUAL\ncommonly, a string of 0s and 1s. For example, an 8-queens state must specify the positions of\n8 queens, each in a column of 8 squares, and so requires 8 × log2 8 = 24 bits. Alternatively,\nthe state could be represented as 8 digits, each in the range from 1 to 8. (We demonstrate later\nthat the two encodings behave differently.) Figure 4.6(a) shows a population of four 8-digit\nstrings representing 8-queens states.",
  "the state could be represented as 8 digits, each in the range from 1 to 8. (We demonstrate later\nthat the two encodings behave differently.) Figure 4.6(a) shows a population of four 8-digit\nstrings representing 8-queens states.\nThe production of the next generation of states is shown in Figure 4.6(b)–(e). In (b),\neach state is rated by the objective function, or (in GA terminology) the ﬁtness function. A\nFITNESS FUNCTION\nﬁtness function should return higher values for better states, so, for the 8-queens problem\nwe use the number of nonattacking pairs of queens, which has a value of 28 for a solution.\nThe values of the four states are 24, 23, 20, and 11. In this particular variant of the genetic\nalgorithm, the probability of being chosen for reproducing is directly proportional to the\nﬁtness score, and the percentages are shown next to the raw scores.\nIn (c), two pairs are selected at random for reproduction, in accordance with the prob- 128\nChapter\n4.\nBeyond Classical Search\nabilities in (b). Notice that one individual is selected twice and one not at all.4 For each\npair to be mated, a crossover point is chosen randomly from the positions in the string. In\nCROSSOVER\nFigure 4.6, the crossover points are after the third digit in the ﬁrst pair and after the ﬁfth digit\nin the second pair.5\nIn (d), the offspring themselves are created by crossing over the parent strings at the\ncrossover point. For example, the ﬁrst child of the ﬁrst pair gets the ﬁrst three digits from the\nﬁrst parent and the remaining digits from the second parent, whereas the second child gets\nthe ﬁrst three digits from the second parent and the rest from the ﬁrst parent. The 8-queens\nstates involved in this reproduction step are shown in Figure 4.7. The example shows that\nwhen two parent states are quite different, the crossover operation can produce a state that is\na long way from either parent state. It is often the case that the population is quite diverse\nearly on in the process, so crossover (like simulated annealing) frequently takes large steps in\nthe state space early in the search process and smaller steps later on when most individuals\nare quite similar.\nFinally, in (e), each location is subject to random mutation with a small independent\nMUTATION\nprobability. One digit was mutated in the ﬁrst, third, and fourth offspring. In the 8-queens\nproblem, this corresponds to choosing a queen at random and moving it to a random square",
  "MUTATION\nprobability. One digit was mutated in the ﬁrst, third, and fourth offspring. In the 8-queens\nproblem, this corresponds to choosing a queen at random and moving it to a random square\nin its column. Figure 4.8 describes an algorithm that implements all these steps.\nLike stochastic beam search, genetic algorithms combine an uphill tendency with ran-\ndom exploration and exchange of information among parallel search threads. The primary\nadvantage, if any, of genetic algorithms comes from the crossover operation. Yet it can be\nshown mathematically that, if the positions of the genetic code are permuted initially in a\nrandom order, crossover conveys no advantage. Intuitively, the advantage comes from the\nability of crossover to combine large blocks of letters that have evolved independently to per-\nform useful functions, thus raising the level of granularity at which the search operates. For\nexample, it could be that putting the ﬁrst three queens in positions 2, 4, and 6 (where they do\nnot attack each other) constitutes a useful block that can be combined with other blocks to\nconstruct a solution.\nThe theory of genetic algorithms explains how this works using the idea of a schema,\nSCHEMA\nwhich is a substring in which some of the positions can be left unspeciﬁed. For example,\nthe schema 246***** describes all 8-queens states in which the ﬁrst three queens are in\npositions 2, 4, and 6, respectively. Strings that match the schema (such as 24613578) are\ncalled instances of the schema. It can be shown that if the average ﬁtness of the instances of\nINSTANCE\na schema is above the mean, then the number of instances of the schema within the population\nwill grow over time. Clearly, this effect is unlikely to be signiﬁcant if adjacent bits are totally\nunrelated to each other, because then there will be few contiguous blocks that provide a\nconsistent beneﬁt. Genetic algorithms work best when schemata correspond to meaningful\ncomponents of a solution. For example, if the string is a representation of an antenna, then the\nschemata may represent components of the antenna, such as reﬂectors and deﬂectors. A good\n4 There are many variants of this selection rule. The method of culling, in which all individuals below a given\nthreshold are discarded, can be shown to converge faster than the random version (Baum et al., 1995).\n5 It is here that the encoding matters. If a 24-bit encoding is used instead of 8 digits, then the crossover point",
  "threshold are discarded, can be shown to converge faster than the random version (Baum et al., 1995).\n5 It is here that the encoding matters. If a 24-bit encoding is used instead of 8 digits, then the crossover point\nhas a 2/3 chance of being in the middle of a digit, which results in an essentially arbitrary mutation of that digit. Section 4.2.\nLocal Search in Continuous Spaces\n129\nfunction GENETIC-ALGORITHM(population, FITNESS-FN) returns an individual\ninputs: population, a set of individuals\nFITNESS-FN, a function that measures the ﬁtness of an individual\nrepeat\nnew population ←empty set\nfor i = 1 to SIZE(population) do\nx ←RANDOM-SELECTION(population, FITNESS-FN)\ny ←RANDOM-SELECTION(population, FITNESS-FN)\nchild ←REPRODUCE(x,y)\nif (small random probability) then child ←MUTATE(child)\nadd child to new population\npopulation ←new population\nuntil some individual is ﬁt enough, or enough time has elapsed\nreturn the best individual in population, according to FITNESS-FN\nfunction REPRODUCE(x,y) returns an individual\ninputs: x,y, parent individuals\nn ←LENGTH(x); c ←random number from 1 to n\nreturn APPEND(SUBSTRING(x,1,c), SUBSTRING(y,c + 1,n))\nFigure 4.8\nA genetic algorithm. The algorithm is the same as the one diagrammed in\nFigure 4.6, with one variation: in this more popular version, each mating of two parents\nproduces only one offspring, not two.\ncomponent is likely to be good in a variety of different designs. This suggests that successful\nuse of genetic algorithms requires careful engineering of the representation.\nIn practice, genetic algorithms have had a widespread impact on optimization problems,\nsuch as circuit layout and job-shop scheduling. At present, it is not clear whether the appeal\nof genetic algorithms arises from their performance or from their æsthetically pleasing origins\nin the theory of evolution. Much work remains to be done to identify the conditions under\nwhich genetic algorithms perform well.\n4.2\nLOCAL SEARCH IN CONTINUOUS SPACES\nIn Chapter 2, we explained the distinction between discrete and continuous environments,\npointing out that most real-world environments are continuous. Yet none of the algorithms\nwe have described (except for ﬁrst-choice hill climbing and simulated annealing) can handle\ncontinuous state and action spaces, because they have inﬁnite branching factors. This section\nprovides a very brief introduction to some local search techniques for ﬁnding optimal solu-",
  "continuous state and action spaces, because they have inﬁnite branching factors. This section\nprovides a very brief introduction to some local search techniques for ﬁnding optimal solu-\ntions in continuous spaces. The literature on this topic is vast; many of the basic techniques 130\nChapter\n4.\nBeyond Classical Search\nEVOLUTION AND SEARCH\nThe theory of evolution was developed in Charles Darwin’s On the Origin of\nSpecies by Means of Natural Selection (1859) and independently by Alfred Russel\nWallace (1858). The central idea is simple: variations occur in reproduction and\nwill be preserved in successive generations approximately in proportion to their\neffect on reproductive ﬁtness.\nDarwin’s theory was developed with no knowledge of how the traits of organ-\nisms can be inherited and modiﬁed. The probabilistic laws governing these pro-\ncesses were ﬁrst identiﬁed by Gregor Mendel (1866), a monk who experimented\nwith sweet peas. Much later, Watson and Crick (1953) identiﬁed the structure of the\nDNA molecule and its alphabet, AGTC (adenine, guanine, thymine, cytosine). In\nthe standard model, variation occurs both by point mutations in the letter sequence\nand by “crossover” (in which the DNA of an offspring is generated by combining\nlong sections of DNA from each parent).\nThe analogy to local search algorithms has already been described; the princi-\npal difference between stochastic beam search and evolution is the use of sexual re-\nproduction, wherein successors are generated from multiple organisms rather than\njust one. The actual mechanisms of evolution are, however, far richer than most\ngenetic algorithms allow. For example, mutations can involve reversals, duplica-\ntions, and movement of large chunks of DNA; some viruses borrow DNA from one\norganism and insert it in another; and there are transposable genes that do nothing\nbut copy themselves many thousands of times within the genome. There are even\ngenes that poison cells from potential mates that do not carry the gene, thereby in-\ncreasing their own chances of replication. Most important is the fact that the genes\nthemselves encode the mechanisms whereby the genome is reproduced and trans-\nlated into an organism. In genetic algorithms, those mechanisms are a separate\nprogram that is not represented within the strings being manipulated.\nDarwinian evolution may appear inefﬁcient, having generated blindly some\n1045 or so organisms without improving its search heuristics one iota.\nFifty",
  "program that is not represented within the strings being manipulated.\nDarwinian evolution may appear inefﬁcient, having generated blindly some\n1045 or so organisms without improving its search heuristics one iota.\nFifty\nyears before Darwin, however, the otherwise great French naturalist Jean Lamarck\n(1809) proposed a theory of evolution whereby traits acquired by adaptation dur-\ning an organism’s lifetime would be passed on to its offspring. Such a process\nwould be effective but does not seem to occur in nature. Much later, James Bald-\nwin (1896) proposed a superﬁcially similar theory: that behavior learned during an\norganism’s lifetime could accelerate the rate of evolution. Unlike Lamarck’s, Bald-\nwin’s theory is entirely consistent with Darwinian evolution because it relies on se-\nlection pressures operating on individuals that have found local optima among the\nset of possible behaviors allowed by their genetic makeup. Computer simulations\nconﬁrm that the “Baldwin effect” is real, once “ordinary” evolution has created\norganisms whose internal performance measure correlates with actual ﬁtness. Section 4.2.\nLocal Search in Continuous Spaces\n131\noriginated in the 17th century, after the development of calculus by Newton and Leibniz.6 We\nﬁnd uses for these techniques at several places in the book, including the chapters on learning,\nvision, and robotics.\nWe begin with an example. Suppose we want to place three new airports anywhere\nin Romania, such that the sum of squared distances from each city on the map (Figure 3.2)\nto its nearest airport is minimized. The state space is then deﬁned by the coordinates of\nthe airports: (x1, y1), (x2, y2), and (x3, y3). This is a six-dimensional space; we also say\nthat states are deﬁned by six variables. (In general, states are deﬁned by an n-dimensional\nVARIABLE\nvector of variables, x.) Moving around in this space corresponds to moving one or more of\nthe airports on the map. The objective function f(x1, y1, x2, y2, x3, y3) is relatively easy to\ncompute for any particular state once we compute the closest cities. Let Ci be the set of\ncities whose closest airport (in the current state) is airport i. Then, in the neighborhood of the\ncurrent state, where the Cis remain constant, we have\nf(x1, y1, x2, y2, x3, y3) =\n3\n\f\ni = 1\n\f\nc∈Ci\n(xi −xc)2 + (yi −yc)2 .\n(4.1)\nThis expression is correct locally, but not globally because the sets Ci are (discontinuous)\nfunctions of the state.",
  "current state, where the Cis remain constant, we have\nf(x1, y1, x2, y2, x3, y3) =\n3\n\f\ni = 1\n\f\nc∈Ci\n(xi −xc)2 + (yi −yc)2 .\n(4.1)\nThis expression is correct locally, but not globally because the sets Ci are (discontinuous)\nfunctions of the state.\nOne way to avoid continuous problems is simply to discretize the neighborhood of each\nDISCRETIZATION\nstate. For example, we can move only one airport at a time in either the x or y direction by\na ﬁxed amount ±δ. With 6 variables, this gives 12 possible successors for each state. We\ncan then apply any of the local search algorithms described previously. We could also ap-\nply stochastic hill climbing and simulated annealing directly, without discretizing the space.\nThese algorithms choose successors randomly, which can be done by generating random vec-\ntors of length δ.\nMany methods attempt to use the gradient of the landscape to ﬁnd a maximum. The\nGRADIENT\ngradient of the objective function is a vector ∇f that gives the magnitude and direction of the\nsteepest slope. For our problem, we have\n∇f =\n\r ∂f\n∂x1\n, ∂f\n∂y1\n, ∂f\n∂x2\n, ∂f\n∂y2\n, ∂f\n∂x3\n, ∂f\n∂y3\n\u000e\n.\nIn some cases, we can ﬁnd a maximum by solving the equation ∇f = 0. (This could be done,\nfor example, if we were placing just one airport; the solution is the arithmetic mean of all the\ncities’ coordinates.) In many cases, however, this equation cannot be solved in closed form.\nFor example, with three airports, the expression for the gradient depends on what cities are\nclosest to each airport in the current state. This means we can compute the gradient locally\n(but not globally); for example,\n∂f\n∂x1\n= 2\n\f\nc∈C1\n(xi −xc) .\n(4.2)\nGiven a locally correct expression for the gradient, we can perform steepest-ascent hill climb-\n6 A basic knowledge of multivariate calculus and vector arithmetic is useful for reading this section. 132\nChapter\n4.\nBeyond Classical Search\ning by updating the current state according to the formula\nx ←x + α∇f(x) ,\nwhere α is a small constant often called the step size. In other cases, the objective function\nSTEP SIZE\nmight not be available in a differentiable form at all—for example, the value of a particular set\nof airport locations might be determined by running some large-scale economic simulation\npackage. In those cases, we can calculate a so-called empirical gradient by evaluating the\nEMPIRICAL\nGRADIENT\nresponse to small increments and decrements in each coordinate. Empirical gradient search",
  "package. In those cases, we can calculate a so-called empirical gradient by evaluating the\nEMPIRICAL\nGRADIENT\nresponse to small increments and decrements in each coordinate. Empirical gradient search\nis the same as steepest-ascent hill climbing in a discretized version of the state space.\nHidden beneath the phrase “α is a small constant” lies a huge variety of methods for\nadjusting α. The basic problem is that, if α is too small, too many steps are needed; if α\nis too large, the search could overshoot the maximum. The technique of line search tries to\nLINE SEARCH\novercome this dilemma by extending the current gradient direction—usually by repeatedly\ndoubling α—until f starts to decrease again. The point at which this occurs becomes the new\ncurrent state. There are several schools of thought about how the new direction should be\nchosen at this point.\nFor many problems, the most effective algorithm is the venerable Newton–Raphson\nNEWTON–RAPHSON\nmethod. This is a general technique for ﬁnding roots of functions—that is, solving equations\nof the form g(x) = 0. It works by computing a new estimate for the root x according to\nNewton’s formula\nx ←x −g(x)/g′(x) .\nTo ﬁnd a maximum or minimum of f, we need to ﬁnd x such that the gradient is zero (i.e.,\n∇f(x) = 0). Thus, g(x) in Newton’s formula becomes ∇f(x), and the update equation can\nbe written in matrix–vector form as\nx ←x −H−1\nf (x)∇f(x) ,\nwhere Hf(x) is the Hessian matrix of second derivatives, whose elements Hij are given\nHESSIAN\nby ∂2f/∂xi∂xj. For our airport example, we can see from Equation (4.2) that Hf(x) is\nparticularly simple: the off-diagonal elements are zero and the diagonal elements for airport\ni are just twice the number of cities in Ci. A moment’s calculation shows that one step of\nthe update moves airport i directly to the centroid of Ci, which is the minimum of the local\nexpression for f from Equation (4.1).7 For high-dimensional problems, however, computing\nthe n2 entries of the Hessian and inverting it may be expensive, so many approximate versions\nof the Newton–Raphson method have been developed.\nLocal search methods suffer from local maxima, ridges, and plateaux in continuous\nstate spaces just as much as in discrete spaces. Random restarts and simulated annealing can\nbe used and are often helpful. High-dimensional continuous spaces are, however, big places\nin which it is easy to get lost.\nA ﬁnal topic with which a passing acquaintance is useful is constrained optimization.\nCONSTRAINED",
  "be used and are often helpful. High-dimensional continuous spaces are, however, big places\nin which it is easy to get lost.\nA ﬁnal topic with which a passing acquaintance is useful is constrained optimization.\nCONSTRAINED\nOPTIMIZATION\nAn optimization problem is constrained if solutions must satisfy some hard constraints on the\nvalues of the variables. For example, in our airport-siting problem, we might constrain sites\n7 In general, the Newton–Raphson update can be seen as ﬁtting a quadratic surface to f at x and then moving\ndirectly to the minimum of that surface—which is also the minimum of f if f is quadratic. Section 4.3.\nSearching with Nondeterministic Actions\n133\nto be inside Romania and on dry land (rather than in the middle of lakes). The difﬁculty of\nconstrained optimization problems depends on the nature of the constraints and the objective\nfunction. The best-known category is that of linear programming problems, in which con-\nLINEAR\nPROGRAMMING\nstraints must be linear inequalities forming a convex set 8 and the objective function is also\nCONVEX SET\nlinear. The time complexity of linear programming is polynomial in the number of variables.\nLinear programming is probably the most widely studied and broadly useful class of\noptimization problems. It is a special case of the more general problem of convex opti-\nmization, which allows the constraint region to be any convex region and the objective to\nCONVEX\nOPTIMIZATION\nbe any function that is convex within the constraint region. Under certain conditions, convex\noptimization problems are also polynomially solvable and may be feasible in practice with\nthousands of variables. Several important problems in machine learning and control theory\ncan be formulated as convex optimization problems (see Chapter 20).\n4.3\nSEARCHING WITH NONDETERMINISTIC ACTIONS\nIn Chapter 3, we assumed that the environment is fully observable and deterministic and that\nthe agent knows what the effects of each action are. Therefore, the agent can calculate exactly\nwhich state results from any sequence of actions and always knows which state it is in. Its\npercepts provide no new information after each action, although of course they tell the agent\nthe initial state.\nWhen the environment is either partially observable or nondeterministic (or both), per-\ncepts become useful. In a partially observable environment, every percept helps narrow down",
  "the initial state.\nWhen the environment is either partially observable or nondeterministic (or both), per-\ncepts become useful. In a partially observable environment, every percept helps narrow down\nthe set of possible states the agent might be in, thus making it easier for the agent to achieve\nits goals. When the environment is nondeterministic, percepts tell the agent which of the pos-\nsible outcomes of its actions has actually occurred. In both cases, the future percepts cannot\nbe determined in advance and the agent’s future actions will depend on those future percepts.\nSo the solution to a problem is not a sequence but a contingency plan (also known as a strat-\nCONTINGENCY PLAN\negy) that speciﬁes what to do depending on what percepts are received. In this section, we\nSTRATEGY\nexamine the case of nondeterminism, deferring partial observability to Section 4.4.\n4.3.1\nThe erratic vacuum world\nAs an example, we use the vacuum world, ﬁrst introduced in Chapter 2 and deﬁned as a\nsearch problem in Section 3.2.1. Recall that the state space has eight states, as shown in\nFigure 4.9. There are three actions—Left, Right, and Suck—and the goal is to clean up all\nthe dirt (states 7 and 8). If the environment is observable, deterministic, and completely\nknown, then the problem is trivially solvable by any of the algorithms in Chapter 3 and the\nsolution is an action sequence. For example, if the initial state is 1, then the action sequence\n[Suck,Right,Suck] will reach a goal state, 8.\n8 A set of points S is convex if the line joining any two points in S is also contained in S. A convex function is\none for which the space “above” it forms a convex set; by deﬁnition, convex functions have no local (as opposed\nto global) minima. 134\nChapter\n4.\nBeyond Classical Search\n1\n2\n8\n7\n5\n6\n3\n4\nFigure 4.9\nThe eight possible states of the vacuum world; states 7 and 8 are goal states.\nNow suppose that we introduce nondeterminism in the form of a powerful but erratic\nvacuum cleaner. In the erratic vacuum world, the Suck action works as follows:\nERRATIC VACUUM\nWORLD\n• When applied to a dirty square the action cleans the square and sometimes cleans up\ndirt in an adjacent square, too.\n• When applied to a clean square the action sometimes deposits dirt on the carpet.9\nTo provide a precise formulation of this problem, we need to generalize the notion of a tran-\nsition model from Chapter 3. Instead of deﬁning the transition model by a RESULT function",
  "To provide a precise formulation of this problem, we need to generalize the notion of a tran-\nsition model from Chapter 3. Instead of deﬁning the transition model by a RESULT function\nthat returns a single state, we use a RESULTS function that returns a set of possible outcome\nstates. For example, in the erratic vacuum world, the Suck action in state 1 leads to a state in\nthe set {5, 7}—the dirt in the right-hand square may or may not be vacuumed up.\nWe also need to generalize the notion of a solution to the problem. For example, if we\nstart in state 1, there is no single sequence of actions that solves the problem. Instead, we\nneed a contingency plan such as the following:\n[Suck, if State = 5 then [Right, Suck] else [ ]] .\n(4.3)\nThus, solutions for nondeterministic problems can contain nested if–then–else statements;\nthis means that they are trees rather than sequences. This allows the selection of actions\nbased on contingencies arising during execution. Many problems in the real, physical world\nare contingency problems because exact prediction is impossible. For this reason, many\npeople keep their eyes open while walking around or driving.\n9 We assume that most readers face similar problems and can sympathize with our agent. We apologize to\nowners of modern, efﬁcient home appliances who cannot take advantage of this pedagogical device. Section 4.3.\nSearching with Nondeterministic Actions\n135\n4.3.2\nAND–OR search trees\nThe next question is how to ﬁnd contingent solutions to nondeterministic problems. As in\nChapter 3, we begin by constructing search trees, but here the trees have a different character.\nIn a deterministic environment, the only branching is introduced by the agent’s own choices\nin each state. We call these nodes OR nodes. In the vacuum world, for example, at an OR\nOR NODE\nnode the agent chooses Left or Right or Suck. In a nondeterministic environment, branching\nis also introduced by the environment’s choice of outcome for each action. We call these\nnodes AND nodes. For example, the Suck action in state 1 leads to a state in the set {5, 7},\nAND NODE\nso the agent would need to ﬁnd a plan for state 5 and for state 7. These two kinds of nodes\nalternate, leading to an AND–OR tree as illustrated in Figure 4.10.\nAND–OR TREE\nA solution for an AND–OR search problem is a subtree that (1) has a goal node at every\nleaf, (2) speciﬁes one action at each of its OR nodes, and (3) includes every outcome branch",
  "AND–OR TREE\nA solution for an AND–OR search problem is a subtree that (1) has a goal node at every\nleaf, (2) speciﬁes one action at each of its OR nodes, and (3) includes every outcome branch\nat each of its AND nodes. The solution is shown in bold lines in the ﬁgure; it corresponds\nto the plan given in Equation (4.3). (The plan uses if–then–else notation to handle the AND\nbranches, but when there are more than two branches at a node, it might be better to use a case\nLeft\nSuck\nRight\nSuck\nRight\nSuck\n6 \nGOAL\n8 \nGOAL\n7 \n1 \n2 \n5 \n1 \nLOOP\n5 \nLOOP\n5 \nLOOP\nLeft\nSuck\n1 \nLOOP\nGOAL\n8 \n4 \nFigure 4.10\nThe ﬁrst two levels of the search tree for the erratic vacuum world. State\nnodes are OR nodes where some action must be chosen. At the AND nodes, shown as circles,\nevery outcome must be handled, as indicated by the arc linking the outgoing branches. The\nsolution found is shown in bold lines. 136\nChapter\n4.\nBeyond Classical Search\nfunction AND-OR-GRAPH-SEARCH(problem) returns a conditional plan, or failure\nOR-SEARCH(problem.INITIAL-STATE,problem,[ ])\nfunction OR-SEARCH(state,problem,path) returns a conditional plan, or failure\nif problem.GOAL-TEST(state) then return the empty plan\nif state is on path then return failure\nfor each action in problem.ACTIONS(state) do\nplan ←AND-SEARCH(RESULTS(state,action),problem,[state | path])\nif plan ̸= failure then return [action | plan]\nreturn failure\nfunction AND-SEARCH(states,problem,path) returns a conditional plan, or failure\nfor each si in states do\nplani ←OR-SEARCH(si,problem,path)\nif plani = failure then return failure\nreturn [if s1 then plan1 else if s2 then plan2 else . . . if sn−1 then plann−1 else plann]\nFigure 4.11\nAn algorithm for searching AND–OR graphs generated by nondeterministic\nenvironments. It returns a conditional plan that reaches a goal state in all circumstances. (The\nnotation [x | l] refers to the list formed by adding object x to the front of list l.)\nconstruct.) Modifying the basic problem-solving agent shown in Figure 3.1 to execute con-\ntingent solutions of this kind is straightforward. One may also consider a somewhat different\nagent design, in which the agent can act before it has found a guaranteed plan and deals with\nsome contingencies only as they arise during execution. This type of interleaving of search\nINTERLEAVING\nand execution is also useful for exploration problems (see Section 4.5) and for game playing\n(see Chapter 5).",
  "some contingencies only as they arise during execution. This type of interleaving of search\nINTERLEAVING\nand execution is also useful for exploration problems (see Section 4.5) and for game playing\n(see Chapter 5).\nFigure 4.11 gives a recursive, depth-ﬁrst algorithm for AND–OR graph search. One\nkey aspect of the algorithm is the way in which it deals with cycles, which often arise in\nnondeterministic problems (e.g., if an action sometimes has no effect or if an unintended\neffect can be corrected). If the current state is identical to a state on the path from the root,\nthen it returns with failure. This doesn’t mean that there is no solution from the current state;\nit simply means that if there is a noncyclic solution, it must be reachable from the earlier\nincarnation of the current state, so the new incarnation can be discarded. With this check, we\nensure that the algorithm terminates in every ﬁnite state space, because every path must reach\na goal, a dead end, or a repeated state. Notice that the algorithm does not check whether the\ncurrent state is a repetition of a state on some other path from the root, which is important for\nefﬁciency. Exercise 4.5 investigates this issue.\nAND–OR graphs can also be explored by breadth-ﬁrst or best-ﬁrst methods. The concept\nof a heuristic function must be modiﬁed to estimate the cost of a contingent solution rather\nthan a sequence, but the notion of admissibility carries over and there is an analog of the A∗\nalgorithm for ﬁnding optimal solutions. Pointers are given in the bibliographical notes at the\nend of the chapter. Section 4.3.\nSearching with Nondeterministic Actions\n137\nSuck\nRight\n6 \n1 \n2 \n5 \nRight\nFigure 4.12\nPart of the search graph for the slippery vacuum world, where we have shown\n(some) cycles explicitly. All solutions for this problem are cyclic plans because there is no\nway to move reliably.\n4.3.3\nTry, try again\nConsider the slippery vacuum world, which is identical to the ordinary (non-erratic) vac-\nuum world except that movement actions sometimes fail, leaving the agent in the same loca-\ntion. For example, moving Right in state 1 leads to the state set {1, 2}. Figure 4.12 shows\npart of the search graph; clearly, there are no longer any acyclic solutions from state 1, and\nAND-OR-GRAPH-SEARCH would return with failure. There is, however, a cyclic solution,\nCYCLIC SOLUTION\nwhich is to keep trying Right until it works. We can express this solution by adding a label to\nLABEL",
  "AND-OR-GRAPH-SEARCH would return with failure. There is, however, a cyclic solution,\nCYCLIC SOLUTION\nwhich is to keep trying Right until it works. We can express this solution by adding a label to\nLABEL\ndenote some portion of the plan and using that label later instead of repeating the plan itself.\nThus, our cyclic solution is\n[Suck, L1 : Right, if State = 5 then L1 else Suck] .\n(A better syntax for the looping part of this plan would be “while State = 5 do Right.”)\nIn general a cyclic plan may be considered a solution provided that every leaf is a goal\nstate and that a leaf is reachable from every point in the plan. The modiﬁcations needed\nto AND-OR-GRAPH-SEARCH are covered in Exercise 4.6. The key realization is that a loop\nin the state space back to a state L translates to a loop in the plan back to the point where the\nsubplan for state L is executed.\nGiven the deﬁnition of a cyclic solution, an agent executing such a solution will eventu-\nally reach the goal provided that each outcome of a nondeterministic action eventually occurs.\nIs this condition reasonable? It depends on the reason for the nondeterminism. If the action\nrolls a die, then it’s reasonable to suppose that eventually a six will be rolled. If the action is\nto insert a hotel card key into the door lock, but it doesn’t work the ﬁrst time, then perhaps it\nwill eventually work, or perhaps one has the wrong key (or the wrong room!). After seven or 138\nChapter\n4.\nBeyond Classical Search\neight tries, most people will assume the problem is with the key and will go back to the front\ndesk to get a new one. One way to understand this decision is to say that the initial problem\nformulation (observable, nondeterministic) is abandoned in favor of a different formulation\n(partially observable, deterministic) where the failure is attributed to an unobservable prop-\nerty of the key. We have more to say on this issue in Chapter 13.\n4.4\nSEARCHING WITH PARTIAL OBSERVATIONS\nWe now turn to the problem of partial observability, where the agent’s percepts do not suf-\nﬁce to pin down the exact state. As noted at the beginning of the previous section, if the\nagent is in one of several possible states, then an action may lead to one of several possible\noutcomes—even if the environment is deterministic. The key concept required for solving\npartially observable problems is the belief state, representing the agent’s current belief about\nBELIEF STATE",
  "outcomes—even if the environment is deterministic. The key concept required for solving\npartially observable problems is the belief state, representing the agent’s current belief about\nBELIEF STATE\nthe possible physical states it might be in, given the sequence of actions and percepts up to\nthat point. We begin with the simplest scenario for studying belief states, which is when the\nagent has no sensors at all; then we add in partial sensing as well as nondeterministic actions.\n4.4.1\nSearching with no observation\nWhen the agent’s percepts provide no information at all, we have what is called a sensor-\nless problem or sometimes a conformant problem. At ﬁrst, one might think the sensorless\nSENSORLESS\nCONFORMANT\nagent has no hope of solving a problem if it has no idea what state it’s in; in fact, sensorless\nproblems are quite often solvable. Moreover, sensorless agents can be surprisingly useful,\nprimarily because they don’t rely on sensors working properly. In manufacturing systems,\nfor example, many ingenious methods have been developed for orienting parts correctly from\nan unknown initial position by using a sequence of actions with no sensing at all. The high\ncost of sensing is another reason to avoid it: for example, doctors often prescribe a broad-\nspectrum antibiotic rather than using the contingent plan of doing an expensive blood test,\nthen waiting for the results to come back, and then prescribing a more speciﬁc antibiotic and\nperhaps hospitalization because the infection has progressed too far.\nWe can make a sensorless version of the vacuum world. Assume that the agent knows\nthe geography of its world, but doesn’t know its location or the distribution of dirt. In that\ncase, its initial state could be any element of the set {1, 2, 3, 4, 5, 6, 7, 8}. Now, consider what\nhappens if it tries the action Right. This will cause it to be in one of the states {2, 4, 6, 8}—the\nagent now has more information! Furthermore, the action sequence [Right,Suck] will always\nend up in one of the states {4, 8}. Finally, the sequence [Right,Suck,Left,Suck] is guaranteed\nto reach the goal state 7 no matter what the start state. We say that the agent can coerce the\nCOERCION\nworld into state 7.\nTo solve sensorless problems, we search in the space of belief states rather than physical\nstates.10 Notice that in belief-state space, the problem is fully observable because the agent",
  "COERCION\nworld into state 7.\nTo solve sensorless problems, we search in the space of belief states rather than physical\nstates.10 Notice that in belief-state space, the problem is fully observable because the agent\n10 In a fully observable environment, each belief state contains one physical state. Thus, we can view the algo-\nrithms in Chapter 3 as searching in a belief-state space of singleton belief states. Section 4.4.\nSearching with Partial Observations\n139\nalways knows its own belief state. Furthermore, the solution (if any) is always a sequence of\nactions. This is because, as in the ordinary problems of Chapter 3, the percepts received after\neach action are completely predictable—they’re always empty! So there are no contingencies\nto plan for. This is true even if the environment is nondeterminstic.\nIt is instructive to see how the belief-state search problem is constructed. Suppose\nthe underlying physical problem P is deﬁned by ACTIONSP , RESULTP , GOAL-TESTP, and\nSTEP-COSTP. Then we can deﬁne the corresponding sensorless problem as follows:\n• Belief states: The entire belief-state space contains every possible set of physical states.\nIf P has N states, then the sensorless problem has up to 2N states, although many may\nbe unreachable from the initial state.\n• Initial state: Typically the set of all states in P, although in some cases the agent will\nhave more knowledge than this.\n• Actions: This is slightly tricky. Suppose the agent is in belief state b = {s1, s2}, but\nACTIONSP (s1) ̸= ACTIONSP(s2); then the agent is unsure of which actions are legal.\nIf we assume that illegal actions have no effect on the environment, then it is safe to\ntake the union of all the actions in any of the physical states in the current belief state b:\nACTIONS(b) =\n\u000f\ns∈b\nACTIONSP(s) .\nOn the other hand, if an illegal action might be the end of the world, it is safer to allow\nonly the intersection, that is, the set of actions legal in all the states. For the vacuum\nworld, every state has the same legal actions, so both methods give the same result.\n• Transition model: The agent doesn’t know which state in the belief state is the right\none; so as far as it knows, it might get to any of the states resulting from applying the\naction to one of the physical states in the belief state. For deterministic actions, the set\nof states that might be reached is\nb′ = RESULT(b, a) = {s′ : s′ = RESULTP(s, a) and s ∈b} .\n(4.4)",
  "action to one of the physical states in the belief state. For deterministic actions, the set\nof states that might be reached is\nb′ = RESULT(b, a) = {s′ : s′ = RESULTP(s, a) and s ∈b} .\n(4.4)\nWith deterministic actions, b′ is never larger than b. With nondeterminism, we have\nb′ = RESULT(b, a) = {s′ : s′ ∈RESULTSP(s, a) and s ∈b}\n=\n\u000f\ns∈b\nRESULTSP(s, a) ,\nwhich may be larger than b, as shown in Figure 4.13.\nThe process of generating\nthe new belief state after the action is called the prediction step; the notation b′ =\nPREDICTION\nPREDICTP (b, a) will come in handy.\n• Goal test: The agent wants a plan that is sure to work, which means that a belief state\nsatisﬁes the goal only if all the physical states in it satisfy GOAL-TESTP . The agent\nmay accidentally achieve the goal earlier, but it won’t know that it has done so.\n• Path cost: This is also tricky. If the same action can have different costs in different\nstates, then the cost of taking an action in a given belief state could be one of several\nvalues. (This gives rise to a new class of problems, which we explore in Exercise 4.9.)\nFor now we assume that the cost of an action is the same in all states and so can be\ntransferred directly from the underlying physical problem. 140\nChapter\n4.\nBeyond Classical Search\n2 \n4 \n1 \n3 \n2 \n4 \n1 \n3 \n1 \n3 \n(b)\n(a)\nFigure 4.13\n(a) Predicting the next belief state for the sensorless vacuum world with a\ndeterministic action, Right. (b) Prediction for the same belief state and action in the slippery\nversion of the sensorless vacuum world.\nFigure 4.14 shows the reachable belief-state space for the deterministic, sensorless vacuum\nworld. There are only 12 reachable belief states out of 28 = 256 possible belief states.\nThe preceding deﬁnitions enable the automatic construction of the belief-state problem\nformulation from the deﬁnition of the underlying physical problem. Once this is done, we\ncan apply any of the search algorithms of Chapter 3. In fact, we can do a little bit more\nthan that. In “ordinary” graph search, newly generated states are tested to see if they are\nidentical to existing states. This works for belief states, too; for example, in Figure 4.14, the\naction sequence [Suck,Left,Suck] starting at the initial state reaches the same belief state as\n[Right,Left,Suck], namely, {5, 7}. Now, consider the belief state reached by [Left], namely,\n{1, 3, 5, 7}. Obviously, this is not identical to {5, 7}, but it is a superset. It is easy to prove",
  "[Right,Left,Suck], namely, {5, 7}. Now, consider the belief state reached by [Left], namely,\n{1, 3, 5, 7}. Obviously, this is not identical to {5, 7}, but it is a superset. It is easy to prove\n(Exercise 4.8) that if an action sequence is a solution for a belief state b, it is also a solution for\nany subset of b. Hence, we can discard a path reaching {1, 3, 5, 7} if {5, 7} has already been\ngenerated. Conversely, if {1, 3, 5, 7} has already been generated and found to be solvable,\nthen any subset, such as {5, 7}, is guaranteed to be solvable. This extra level of pruning may\ndramatically improve the efﬁciency of sensorless problem solving.\nEven with this improvement, however, sensorless problem-solving as we have described\nit is seldom feasible in practice. The difﬁculty is not so much the vastness of the belief-state\nspace—even though it is exponentially larger than the underlying physical state space; in\nmost cases the branching factor and solution length in the belief-state space and physical\nstate space are not so different. The real difﬁculty lies with the size of each belief state. For\nexample, the initial belief state for the 10 × 10 vacuum world contains 100 × 2100 or around\n1032 physical states—far too many if we use the atomic representation, which is an explicit\nlist of states.\nOne solution is to represent the belief state by some more compact description. In\nEnglish, we could say the agent knows “Nothing” in the initial state; after moving Left, we\ncould say, “Not in the rightmost column,” and so on. Chapter 7 explains how to do this in a\nformal representation scheme. Another approach is to avoid the standard search algorithms,\nwhich treat belief states as black boxes just like any other problem state. Instead, we can look Section 4.4.\nSearching with Partial Observations\n141\nL\nR\nS\nL\nR\nS\nL\nR\nS\nL\nR\nS\nL\nR\nS\nL\nR\nS\nL\nR\nS\n1\n1\n3\n5\n7\n2\n4\n6\n8\n2\n3\n4\n5\n6\n7\n8\n4\n5\n7\n8\n5\n3\n7\n6\n4\n8\n4\n8\n5\n7\n6\n8\n8\n7\n3\n7\nFigure 4.14\nThe reachable portion of the belief-state space for the deterministic, sensor-\nless vacuum world. Each shaded box corresponds to a single belief state. At any given point,\nthe agent is in a particular belief state but does not know which physical state it is in. The\ninitial belief state (complete ignorance) is the top center box. Actions are represented by\nlabeled links. Self-loops are omitted for clarity.\ninside the belief states and develop incremental belief-state search algorithms that build up\nINCREMENTAL\nBELIEF-STATE\nSEARCH",
  "labeled links. Self-loops are omitted for clarity.\ninside the belief states and develop incremental belief-state search algorithms that build up\nINCREMENTAL\nBELIEF-STATE\nSEARCH\nthe solution one physical state at a time. For example, in the sensorless vacuum world, the\ninitial belief state is {1, 2, 3, 4, 5, 6, 7, 8}, and we have to ﬁnd an action sequence that works\nin all 8 states. We can do this by ﬁrst ﬁnding a solution that works for state 1; then we check\nif it works for state 2; if not, go back and ﬁnd a different solution for state 1, and so on. Just\nas an AND–OR search has to ﬁnd a solution for every branch at an AND node, this algorithm\nhas to ﬁnd a solution for every state in the belief state; the difference is that AND–OR search\ncan ﬁnd a different solution for each branch, whereas an incremental belief-state search has\nto ﬁnd one solution that works for all the states.\nThe main advantage of the incremental approach is that it is typically able to detect\nfailure quickly—when a belief state is unsolvable, it is usually the case that a small subset of\nthe belief state, consisting of the ﬁrst few states examined, is also unsolvable. In some cases, 142\nChapter\n4.\nBeyond Classical Search\nthis leads to a speedup proportional to the size of the belief states, which may themselves be\nas large as the physical state space itself.\nEven the most efﬁcient solution algorithm is not of much use when no solutions exist.\nMany things just cannot be done without sensing. For example, the sensorless 8-puzzle is\nimpossible. On the other hand, a little bit of sensing can go a long way. For example, every\n8-puzzle instance is solvable if just one square is visible—the solution involves moving each\ntile in turn into the visible square and then keeping track of its location.\n4.4.2\nSearching with observations\nFor a general partially observable problem, we have to specify how the environment generates\npercepts for the agent. For example, we might deﬁne the local-sensing vacuum world to be\none in which the agent has a position sensor and a local dirt sensor but has no sensor capable\nof detecting dirt in other squares. The formal problem speciﬁcation includes a PERCEPT(s)\nfunction that returns the percept received in a given state. (If sensing is nondeterministic,\nthen we use a PERCEPTS function that returns a set of possible percepts.) For example, in the\nlocal-sensing vacuum world, the PERCEPT in state 1 is [A, Dirty]. Fully observable problems",
  "then we use a PERCEPTS function that returns a set of possible percepts.) For example, in the\nlocal-sensing vacuum world, the PERCEPT in state 1 is [A, Dirty]. Fully observable problems\nare a special case in which PERCEPT(s) = s for every state s, while sensorless problems are\na special case in which PERCEPT(s) = null.\nWhen observations are partial, it will usually be the case that several states could have\nproduced any given percept. For example, the percept [A, Dirty] is produced by state 3 as\nwell as by state 1. Hence, given this as the initial percept, the initial belief state for the\nlocal-sensing vacuum world will be {1, 3}. The ACTIONS, STEP-COST, and GOAL-TEST\nare constructed from the underlying physical problem just as for sensorless problems, but the\ntransition model is a bit more complicated. We can think of transitions from one belief state\nto the next for a particular action as occurring in three stages, as shown in Figure 4.15:\n• The prediction stage is the same as for sensorless problems: given the action a in belief\nstate b, the predicted belief state is ˆb = PREDICT(b, a).11\n• The observation prediction stage determines the set of percepts o that could be ob-\nserved in the predicted belief state:\nPOSSIBLE-PERCEPTS(ˆb) = {o : o = PERCEPT(s) and s ∈ˆb} .\n• The update stage determines, for each possible percept, the belief state that would\nresult from the percept. The new belief state bo is just the set of states in ˆb that could\nhave produced the percept:\nbo = UPDATE(ˆb, o) = {s : o = PERCEPT(s) and s ∈ˆb} .\nNotice that each updated belief state bo can be no larger than the predicted belief state ˆb;\nobservations can only help reduce uncertainty compared to the sensorless case. More-\nover, for deterministic sensing, the belief states for the different possible percepts will\nbe disjoint, forming a partition of the original predicted belief state.\n11 Here, and throughout the book, the “hat” in ˆb means an estimated or predicted value for b. Section 4.4.\nSearching with Partial Observations\n143\n2 \n4 \n4 \n1 \n2 \n4 \n1 \n3 \n2 \n1 \n3 \n3 \n(b)\n(a)\n4 \n2 \n1 \n3 \nRight\n[A,Dirty]\n[B,Dirty]\n[B,Clean]\nRight\n[B,Dirty]\n[B,Clean]\nFigure 4.15\nTwo example of transitions in local-sensing vacuum worlds. (a) In the de-\nterministic world, Right is applied in the initial belief state, resulting in a new belief state\nwith two possible physical states; for those states, the possible percepts are [B, Dirty] and",
  "terministic world, Right is applied in the initial belief state, resulting in a new belief state\nwith two possible physical states; for those states, the possible percepts are [B, Dirty] and\n[B, Clean], leading to two belief states, each of which is a singleton. (b) In the slippery\nworld, Right is applied in the initial belief state, giving a new belief state with four physi-\ncal states; for those states, the possible percepts are [A, Dirty], [B, Dirty], and [B, Clean],\nleading to three belief states as shown.\nPutting these three stages together, we obtain the possible belief states resulting from a given\naction and the subsequent possible percepts:\nRESULTS(b, a) = {bo : bo = UPDATE(PREDICT(b, a), o) and\no ∈POSSIBLE-PERCEPTS(PREDICT(b, a))} .\n(4.5)\nAgain, the nondeterminism in the partially observable problem comes from the inability\nto predict exactly which percept will be received after acting; underlying nondeterminism in\nthe physical environment may contribute to this inability by enlarging the belief state at the\nprediction stage, leading to more percepts at the observation stage.\n4.4.3\nSolving partially observable problems\nThe preceding section showed how to derive the RESULTS function for a nondeterministic\nbelief-state problem from an underlying physical problem and the PERCEPT function. Given 144\nChapter\n4.\nBeyond Classical Search\n7 \n5 \n1 \n3 \n4 \n2 \nSuck\n[B,Dirty]\n[B,Clean]\nRight\n[A,Clean]\nFigure 4.16\nThe ﬁrst level of the AND–OR search tree for a problem in the local-sensing\nvacuum world; Suck is the ﬁrst step of the solution.\nsuch a formulation, the AND–OR search algorithm of Figure 4.11 can be applied directly to\nderive a solution. Figure 4.16 shows part of the search tree for the local-sensing vacuum\nworld, assuming an initial percept [A, Dirty]. The solution is the conditional plan\n[Suck, Right, if Bstate = {6} then Suck else [ ]] .\nNotice that, because we supplied a belief-state problem to the AND–OR search algorithm, it\nreturned a conditional plan that tests the belief state rather than the actual state. This is as it\nshould be: in a partially observable environment the agent won’t be able to execute a solution\nthat requires testing the actual state.\nAs in the case of standard search algorithms applied to sensorless problems, the AND–\nOR search algorithm treats belief states as black boxes, just like any other states. One can\nimprove on this by checking for previously generated belief states that are subsets or supersets",
  "OR search algorithm treats belief states as black boxes, just like any other states. One can\nimprove on this by checking for previously generated belief states that are subsets or supersets\nof the current state, just as for sensorless problems. One can also derive incremental search\nalgorithms, analogous to those described for sensorless problems, that provide substantial\nspeedups over the black-box approach.\n4.4.4\nAn agent for partially observable environments\nThe design of a problem-solving agent for partially observable environments is quite similar\nto the simple problem-solving agent in Figure 3.1: the agent formulates a problem, calls a\nsearch algorithm (such as AND-OR-GRAPH-SEARCH) to solve it, and executes the solution.\nThere are two main differences. First, the solution to a problem will be a conditional plan\nrather than a sequence; if the ﬁrst step is an if–then–else expression, the agent will need to\ntest the condition in the if-part and execute the then-part or the else-part accordingly. Second,\nthe agent will need to maintain its belief state as it performs actions and receives percepts.\nThis process resembles the prediction–observation–update process in Equation (4.5) but is\nactually simpler because the percept is given by the environment rather than calculated by the Section 4.4.\nSearching with Partial Observations\n145\n7 \n5 \n6 \n2 \n1 \n3 \n6 \n4 \n8 \n2 \n[B,Dirty]\nRight\n[A,Clean]\n7 \n5 \nSuck\nFigure 4.17\nTwo prediction–update cycles of belief-state maintenance in the kindergarten\nvacuum world with local sensing.\nagent. Given an initial belief state b, an action a, and a percept o, the new belief state is:\nb′ = UPDATE(PREDICT(b, a), o) .\n(4.6)\nFigure 4.17 shows the belief state being maintained in the kindergarten vacuum world with\nlocal sensing, wherein any square may become dirty at any time unless the agent is actively\ncleaning it at that moment.12\nIn partially observable environments—which include the vast majority of real-world\nenvironments—maintaining one’s belief state is a core function of any intelligent system.\nThis function goes under various names, including monitoring, ﬁltering and state estima-\nMONITORING\nFILTERING\ntion. Equation (4.6) is called a recursive state estimator because it computes the new belief\nSTATE ESTIMATION\nRECURSIVE\nstate from the previous one rather than by examining the entire percept sequence. If the agent\nis not to “fall behind,” the computation has to happen as fast as percepts are coming in. As",
  "STATE ESTIMATION\nRECURSIVE\nstate from the previous one rather than by examining the entire percept sequence. If the agent\nis not to “fall behind,” the computation has to happen as fast as percepts are coming in. As\nthe environment becomes more complex, the exact update computation becomes infeasible\nand the agent will have to compute an approximate belief state, perhaps focusing on the im-\nplications of the percept for the aspects of the environment that are of current interest. Most\nwork on this problem has been done for stochastic, continuous-state environments with the\ntools of probability theory, as explained in Chapter 15. Here we will show an example in a\ndiscrete environment with detrministic sensors and nondeterministic actions.\nThe example concerns a robot with the task of localization: working out where it is,\nLOCALIZATION\ngiven a map of the world and a sequence of percepts and actions. Our robot is placed in the\nmaze-like environment of Figure 4.18. The robot is equipped with four sonar sensors that\ntell whether there is an obstacle—the outer wall or a black square in the ﬁgure—in each of\nthe four compass directions. We assume that the sensors give perfectly correct data, and that\nthe robot has a correct map of the enviornment. But unfortunately the robot’s navigational\nsystem is broken, so when it executes a Move action, it moves randomly to one of the adjacent\nsquares. The robot’s task is to determine its current location.\nSuppose the robot has just been switched on, so it does not know where it is. Thus its\ninitial belief state b consists of the set of all locations. The the robot receives the percept\n12 The usual apologies to those who are unfamiliar with the effect of small children on the environment. 146\nChapter\n4.\nBeyond Classical Search\n(a) Possible locations of robot after E1 = NSW\n(b) Possible locations of robot After E1 = NSW, E2 = NS\nFigure 4.18\nPossible positions of the robot, ⊙, (a) after one observation E1 = NSW and\n(b) after a second observation E2 = NS. When sensors are noiseless and the transition model\nis accurate, there are no other possible locations for the robot consistent with this sequence\nof two observations.\nNSW, meaning there are obstacles to the north, west, and south, and does an update using the\nequation bo = UPDATE(b), yielding the 4 locations shown in Figure 4.18(a). You can inspect\nthe maze to see that those are the only four locations that yield the percept NWS.",
  "equation bo = UPDATE(b), yielding the 4 locations shown in Figure 4.18(a). You can inspect\nthe maze to see that those are the only four locations that yield the percept NWS.\nNext the robot executes a Move action, but the result is nondeterministic. The new be-\nlief state, ba = PREDICT(bo, Move), contains all the locations that are one step away from the\nlocations in bo. When the second percept, NS, arrives, the robot does UPDATE(ba, NS) and\nﬁnds that the belief state has collapsed down to the single location shown in Figure 4.18(b).\nThat’s the only location that could be the result of\nUPDATE(PREDICT(UPDATE(b, NSW ), Move), NS) .\nWith nondetermnistic actions the PREDICT step grows the belief state, but the UPDATE step\nshrinks it back down—as long as the percepts provide some useful identifying information.\nSometimes the percepts don’t help much for localization: If there were one or more long\neast-west corridors, then a robot could receive a long sequence of NS percepts, but never\nknow where in the corridor(s) it was. Section 4.5.\nOnline Search Agents and Unknown Environments\n147\n4.5\nONLINE SEARCH AGENTS AND UNKNOWN ENVIRONMENTS\nSo far we have concentrated on agents that use ofﬂine search algorithms. They compute\nOFFLINE SEARCH\na complete solution before setting foot in the real world and then execute the solution. In\ncontrast, an online search13 agent interleaves computation and action: ﬁrst it takes an action,\nONLINE SEARCH\nthen it observes the environment and computes the next action. Online search is a good idea\nin dynamic or semidynamic domains—domains where there is a penalty for sitting around\nand computing too long. Online search is also helpful in nondeterministic domains because\nit allows the agent to focus its computational efforts on the contingencies that actually arise\nrather than those that might happen but probably won’t. Of course, there is a tradeoff: the\nmore an agent plans ahead, the less often it will ﬁnd itself up the creek without a paddle.\nOnline search is a necessary idea for unknown environments, where the agent does not\nknow what states exist or what its actions do. In this state of ignorance, the agent faces an\nexploration problem and must use its actions as experiments in order to learn enough to\nEXPLORATION\nPROBLEM\nmake deliberation worthwhile.\nThe canonical example of online search is a robot that is placed in a new building and\nmust explore it to build a map that it can use for getting from A to B. Methods for escaping",
  "EXPLORATION\nPROBLEM\nmake deliberation worthwhile.\nThe canonical example of online search is a robot that is placed in a new building and\nmust explore it to build a map that it can use for getting from A to B. Methods for escaping\nfrom labyrinths—required knowledge for aspiring heroes of antiquity—are also examples of\nonline search algorithms. Spatial exploration is not the only form of exploration, however.\nConsider a newborn baby: it has many possible actions but knows the outcomes of none of\nthem, and it has experienced only a few of the possible states that it can reach. The baby’s\ngradual discovery of how the world works is, in part, an online search process.\n4.5.1\nOnline search problems\nAn online search problem must be solved by an agent executing actions, rather than by pure\ncomputation. We assume a deterministic and fully observable environment (Chapter 17 re-\nlaxes these assumptions), but we stipulate that the agent knows only the following:\n• ACTIONS(s), which returns a list of actions allowed in state s;\n• The step-cost function c(s, a, s′)—note that this cannot be used until the agent knows\nthat s′ is the outcome; and\n• GOAL-TEST(s).\nNote in particular that the agent cannot determine RESULT(s, a) except by actually being\nin s and doing a. For example, in the maze problem shown in Figure 4.19, the agent does\nnot know that going Up from (1,1) leads to (1,2); nor, having done that, does it know that\ngoing Down will take it back to (1,1). This degree of ignorance can be reduced in some\napplications—for example, a robot explorer might know how its movement actions work and\nbe ignorant only of the locations of obstacles.\n13 The term “online” is commonly used in computer science to refer to algorithms that must process input data\nas they are received rather than waiting for the entire input data set to become available. 148\nChapter\n4.\nBeyond Classical Search\nG\nS\n1\n2\n3\n1\n2\n3\nFigure 4.19\nA simple maze problem. The agent starts at S and must reach G but knows\nnothing of the environment.\nS\nG\nS\nG\nA\nA\nS\nG\n(a)\n(b)\nFigure 4.20\n(a) Two state spaces that might lead an online search agent into a dead end.\nAny given agent will fail in at least one of these spaces. (b) A two-dimensional environment\nthat can cause an online search agent to follow an arbitrarily inefﬁcient route to the goal.\nWhichever choice the agent makes, the adversary blocks that route with another long, thin\nwall, so that the path followed is much longer than the best possible path.",
  "Whichever choice the agent makes, the adversary blocks that route with another long, thin\nwall, so that the path followed is much longer than the best possible path.\nFinally, the agent might have access to an admissible heuristic function h(s) that es-\ntimates the distance from the current state to a goal state. For example, in Figure 4.19, the\nagent might know the location of the goal and be able to use the Manhattan-distance heuristic.\nTypically, the agent’s objective is to reach a goal state while minimizing cost. (Another\npossible objective is simply to explore the entire environment.) The cost is the total path cost\nof the path that the agent actually travels. It is common to compare this cost with the path\ncost of the path the agent would follow if it knew the search space in advance—that is, the\nactual shortest path (or shortest complete exploration). In the language of online algorithms,\nthis is called the competitive ratio; we would like it to be as small as possible.\nCOMPETITIVE RATIO Section 4.5.\nOnline Search Agents and Unknown Environments\n149\nAlthough this sounds like a reasonable request, it is easy to see that the best achievable\ncompetitive ratio is inﬁnite in some cases. For example, if some actions are irreversible—\nIRREVERSIBLE\ni.e., they lead to a state from which no action leads back to the previous state—the online\nsearch might accidentally reach a dead-end state from which no goal state is reachable. Per-\nDEAD END\nhaps the term “accidentally” is unconvincing—after all, there might be an algorithm that\nhappens not to take the dead-end path as it explores. Our claim, to be more precise, is that no\nalgorithm can avoid dead ends in all state spaces. Consider the two dead-end state spaces in\nFigure 4.20(a). To an online search algorithm that has visited states S and A, the two state\nspaces look identical, so it must make the same decision in both. Therefore, it will fail in\none of them. This is an example of an adversary argument—we can imagine an adversary\nADVERSARY\nARGUMENT\nconstructing the state space while the agent explores it and putting the goals and dead ends\nwherever it chooses.\nDead ends are a real difﬁculty for robot exploration—staircases, ramps, cliffs, one-way\nstreets, and all kinds of natural terrain present opportunities for irreversible actions. To make\nprogress, we simply assume that the state space is safely explorable—that is, some goal state\nSAFELY EXPLORABLE",
  "streets, and all kinds of natural terrain present opportunities for irreversible actions. To make\nprogress, we simply assume that the state space is safely explorable—that is, some goal state\nSAFELY EXPLORABLE\nis reachable from every reachable state. State spaces with reversible actions, such as mazes\nand 8-puzzles, can be viewed as undirected graphs and are clearly safely explorable.\nEven in safely explorable environments, no bounded competitive ratio can be guaran-\nteed if there are paths of unbounded cost. This is easy to show in environments with irre-\nversible actions, but in fact it remains true for the reversible case as well, as Figure 4.20(b)\nshows. For this reason, it is common to describe the performance of online search algorithms\nin terms of the size of the entire state space rather than just the depth of the shallowest goal.\n4.5.2\nOnline search agents\nAfter each action, an online agent receives a percept telling it what state it has reached; from\nthis information, it can augment its map of the environment. The current map is used to\ndecide where to go next. This interleaving of planning and action means that online search\nalgorithms are quite different from the ofﬂine search algorithms we have seen previously. For\nexample, ofﬂine algorithms such as A∗can expand a node in one part of the space and then\nimmediately expand a node in another part of the space, because node expansion involves\nsimulated rather than real actions. An online algorithm, on the other hand, can discover\nsuccessors only for a node that it physically occupies. To avoid traveling all the way across\nthe tree to expand the next node, it seems better to expand nodes in a local order. Depth-ﬁrst\nsearch has exactly this property because (except when backtracking) the next node expanded\nis a child of the previous node expanded.\nAn online depth-ﬁrst search agent is shown in Figure 4.21. This agent stores its map\nin a table, RESULT[s, a], that records the state resulting from executing action a in state s.\nWhenever an action from the current state has not been explored, the agent tries that action.\nThe difﬁculty comes when the agent has tried all the actions in a state. In ofﬂine depth-ﬁrst\nsearch, the state is simply dropped from the queue; in an online search, the agent has to\nbacktrack physically. In depth-ﬁrst search, this means going back to the state from which the\nagent most recently entered the current state. To achieve that, the algorithm keeps a table that 150\nChapter\n4.",
  "backtrack physically. In depth-ﬁrst search, this means going back to the state from which the\nagent most recently entered the current state. To achieve that, the algorithm keeps a table that 150\nChapter\n4.\nBeyond Classical Search\nfunction ONLINE-DFS-AGENT(s′) returns an action\ninputs: s′, a percept that identiﬁes the current state\npersistent: result, a table indexed by state and action, initially empty\nuntried, a table that lists, for each state, the actions not yet tried\nunbacktracked, a table that lists, for each state, the backtracks not yet tried\ns, a, the previous state and action, initially null\nif GOAL-TEST(s′) then return stop\nif s′ is a new state (not in untried) then untried[s′] ←ACTIONS(s′)\nif s is not null then\nresult[s,a] ←s′\nadd s to the front of unbacktracked[s′]\nif untried[s′] is empty then\nif unbacktracked[s′] is empty then return stop\nelse a ←an action b such that result[s′,b] = POP(unbacktracked[s′])\nelse a ←POP(untried[s′])\ns ←s′\nreturn a\nFigure 4.21\nAn online search agent that uses depth-ﬁrst exploration. The agent is appli-\ncable only in state spaces in which every action can be “undone” by some other action.\nlists, for each state, the predecessor states to which the agent has not yet backtracked. If the\nagent has run out of states to which it can backtrack, then its search is complete.\nWe recommend that the reader trace through the progress of O NLINE-DFS-AGENT\nwhen applied to the maze given in Figure 4.19. It is fairly easy to see that the agent will, in\nthe worst case, end up traversing every link in the state space exactly twice. For exploration,\nthis is optimal; for ﬁnding a goal, on the other hand, the agent’s competitive ratio could be\narbitrarily bad if it goes off on a long excursion when there is a goal right next to the initial\nstate. An online variant of iterative deepening solves this problem; for an environment that is\na uniform tree, the competitive ratio of such an agent is a small constant.\nBecause of its method of backtracking, ONLINE-DFS-AGENT works only in state\nspaces where the actions are reversible. There are slightly more complex algorithms that\nwork in general state spaces, but no such algorithm has a bounded competitive ratio.\n4.5.3\nOnline local search\nLike depth-ﬁrst search, hill-climbing search has the property of locality in its node expan-\nsions. In fact, because it keeps just one current state in memory, hill-climbing search is",
  "4.5.3\nOnline local search\nLike depth-ﬁrst search, hill-climbing search has the property of locality in its node expan-\nsions. In fact, because it keeps just one current state in memory, hill-climbing search is\nalready an online search algorithm! Unfortunately, it is not very useful in its simplest form\nbecause it leaves the agent sitting at local maxima with nowhere to go. Moreover, random\nrestarts cannot be used, because the agent cannot transport itself to a new state.\nInstead of random restarts, one might consider using a random walk to explore the\nRANDOM WALK\nenvironment. A random walk simply selects at random one of the available actions from the Section 4.5.\nOnline Search Agents and Unknown Environments\n151\nS\nG\nFigure 4.22\nAn environment in which a random walk will take exponentially many steps\nto ﬁnd the goal.\ncurrent state; preference can be given to actions that have not yet been tried. It is easy to\nprove that a random walk will eventually ﬁnd a goal or complete its exploration, provided\nthat the space is ﬁnite.14 On the other hand, the process can be very slow. Figure 4.22 shows\nan environment in which a random walk will take exponentially many steps to ﬁnd the goal\nbecause, at each step, backward progress is twice as likely as forward progress. The example\nis contrived, of course, but there are many real-world state spaces whose topology causes\nthese kinds of “traps” for random walks.\nAugmenting hill climbing with memory rather than randomness turns out to be a more\neffective approach. The basic idea is to store a “current best estimate” H(s) of the cost to\nreach the goal from each state that has been visited. H(s) starts out being just the heuristic\nestimate h(s) and is updated as the agent gains experience in the state space. Figure 4.23\nshows a simple example in a one-dimensional state space. In (a), the agent seems to be\nstuck in a ﬂat local minimum at the shaded state. Rather than staying where it is, the agent\nshould follow what seems to be the best path to the goal given the current cost estimates for\nits neighbors. The estimated cost to reach the goal through a neighbor s′ is the cost to get\nto s′ plus the estimated cost to get to a goal from there—that is, c(s, a, s′) + H(s′). In the\nexample, there are two actions, with estimated costs 1+9 and 1+2, so it seems best to move\nright. Now, it is clear that the cost estimate of 2 for the shaded state was overly optimistic.",
  "example, there are two actions, with estimated costs 1+9 and 1+2, so it seems best to move\nright. Now, it is clear that the cost estimate of 2 for the shaded state was overly optimistic.\nSince the best move cost 1 and led to a state that is at least 2 steps from a goal, the shaded\nstate must be at least 3 steps from a goal, so its H should be updated accordingly, as shown\nin Figure 4.23(b). Continuing this process, the agent will move back and forth twice more,\nupdating H each time and “ﬂattening out” the local minimum until it escapes to the right.\nAn agent implementing this scheme, which is called learning real-time A∗(LRTA∗), is\nLRTA*\nshown in Figure 4.24. Like ONLINE-DFS-AGENT, it builds a map of the environment in\nthe result table. It updates the cost estimate for the state it has just left and then chooses the\n“apparently best” move according to its current cost estimates. One important detail is that\nactions that have not yet been tried in a state s are always assumed to lead immediately to the\ngoal with the least possible cost, namely h(s). This optimism under uncertainty encourages\nOPTIMISM UNDER\nUNCERTAINTY\nthe agent to explore new, possibly promising paths.\nAn LRTA∗agent is guaranteed to ﬁnd a goal in any ﬁnite, safely explorable environment.\nUnlike A∗, however, it is not complete for inﬁnite state spaces—there are cases where it can be\nled inﬁnitely astray. It can explore an environment of n states in O(n2) steps in the worst case,\n14 Random walks are complete on inﬁnite one-dimensional and two-dimensional grids. On a three-dimensional\ngrid, the probability that the walk ever returns to the starting point is only about 0.3405 (Hughes, 1995). 152\nChapter\n4.\nBeyond Classical Search\n1\n2\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n2\n2\n3\n4\n4\n4\n3\n3\n3\n1\n1\n1\n1\n1\n1\n1\n3\n1\n1\n1\n1\n1\n1\n1\n5\n3\n5\n5\n4\n(a)\n(b)\n(c)\n(d)\n(e)\n8\n9\n8\n9\n8\n9\n8\n9\n8\n9\n4\n4\n3\n4\nFigure 4.23\nFive iterations of LRTA∗on a one-dimensional state space. Each state is\nlabeled with H(s), the current cost estimate to reach a goal, and each link is labeled with its\nstep cost. The shaded state marks the location of the agent, and the updated cost estimates at\neach iteration are circled.\nfunction LRTA*-AGENT(s′) returns an action\ninputs: s′, a percept that identiﬁes the current state\npersistent: result, a table, indexed by state and action, initially empty\nH , a table of cost estimates indexed by state, initially empty\ns, a, the previous state and action, initially null\nif GOAL-TEST(s′) then return stop",
  "persistent: result, a table, indexed by state and action, initially empty\nH , a table of cost estimates indexed by state, initially empty\ns, a, the previous state and action, initially null\nif GOAL-TEST(s′) then return stop\nif s′ is a new state (not in H ) then H [s′] ←h(s′)\nif s is not null\nresult[s,a] ←s′\nH [s] ←\nmin\nb ∈ACTIONS(s)LRTA*-COST(s,b,result[s,b],H )\na ←an action b in ACTIONS(s′) that minimizes LRTA*-COST(s′,b,result[s′,b],H )\ns ←s′\nreturn a\nfunction LRTA*-COST(s,a,s′,H ) returns a cost estimate\nif s′ is undeﬁned then return h(s)\nelse return c(s, a, s′) + H[s′]\nFigure 4.24\nLRTA*-AGENT selects an action according to the values of neighboring\nstates, which are updated as the agent moves about the state space. Section 4.6.\nSummary\n153\nbut often does much better. The LRTA∗agent is just one of a large family of online agents that\none can deﬁne by specifying the action selection rule and the update rule in different ways.\nWe discuss this family, developed originally for stochastic environments, in Chapter 21.\n4.5.4\nLearning in online search\nThe initial ignorance of online search agents provides several opportunities for learning. First,\nthe agents learn a “map” of the environment—more precisely, the outcome of each action in\neach state—simply by recording each of their experiences. (Notice that the assumption of\ndeterministic environments means that one experience is enough for each action.) Second,\nthe local search agents acquire more accurate estimates of the cost of each state by using local\nupdating rules, as in LRTA∗. In Chapter 21, we show that these updates eventually converge\nto exact values for every state, provided that the agent explores the state space in the right\nway. Once exact values are known, optimal decisions can be taken simply by moving to the\nlowest-cost successor—that is, pure hill climbing is then an optimal strategy.\nIf you followed our suggestion to trace the behavior of ONLINE-DFS-AGENT in the\nenvironment of Figure 4.19, you will have noticed that the agent is not very bright. For\nexample, after it has seen that the Up action goes from (1,1) to (1,2), the agent still has no\nidea that the Down action goes back to (1,1) or that the Up action also goes from (2,1) to\n(2,2), from (2,2) to (2,3), and so on. In general, we would like the agent to learn that Up\nincreases the y-coordinate unless there is a wall in the way, that Down reduces it, and so on.",
  "(2,2), from (2,2) to (2,3), and so on. In general, we would like the agent to learn that Up\nincreases the y-coordinate unless there is a wall in the way, that Down reduces it, and so on.\nFor this to happen, we need two things. First, we need a formal and explicitly manipulable\nrepresentation for these kinds of general rules; so far, we have hidden the information inside\nthe black box called the RESULT function. Part III is devoted to this issue. Second, we need\nalgorithms that can construct suitable general rules from the speciﬁc observations made by\nthe agent. These are covered in Chapter 18.\n4.6\nSUMMARY\nThis chapter has examined search algorithms for problems beyond the “classical” case of\nﬁnding the shortest path to a goal in an observable, deterministic, discrete environment.\n• Local search methods such as hill climbing operate on complete-state formulations,\nkeeping only a small number of nodes in memory. Several stochastic algorithms have\nbeen developed, including simulated annealing, which returns optimal solutions when\ngiven an appropriate cooling schedule.\n• Many local search methods apply also to problems in continuous spaces. Linear pro-\ngramming and convex optimization problems obey certain restrictions on the shape\nof the state space and the nature of the objective function, and admit polynomial-time\nalgorithms that are often extremely efﬁcient in practice.\n• A genetic algorithm is a stochastic hill-climbing search in which a large population of\nstates is maintained. New states are generated by mutation and by crossover, which\ncombines pairs of states from the population. 154\nChapter\n4.\nBeyond Classical Search\n• In nondeterministic environments, agents can apply AND–OR search to generate con-\ntingent plans that reach the goal regardless of which outcomes occur during execution.\n• When the environment is partially observable, the belief state represents the set of\npossible states that the agent might be in.\n• Standard search algorithms can be applied directly to belief-state space to solve sensor-\nless problems, and belief-state AND–OR search can solve general partially observable\nproblems. Incremental algorithms that construct solutions state-by-state within a belief\nstate are often more efﬁcient.\n• Exploration problems arise when the agent has no idea about the states and actions of\nits environment. For safely explorable environments, online search agents can build a",
  "state are often more efﬁcient.\n• Exploration problems arise when the agent has no idea about the states and actions of\nits environment. For safely explorable environments, online search agents can build a\nmap and ﬁnd a goal if one exists. Updating heuristic estimates from experience provides\nan effective method to escape from local minima.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nLocal search techniques have a long history in mathematics and computer science. Indeed,\nthe Newton–Raphson method (Newton, 1671; Raphson, 1690) can be seen as a very efﬁ-\ncient local search method for continuous spaces in which gradient information is available.\nBrent (1973) is a classic reference for optimization algorithms that do not require such in-\nformation. Beam search, which we have presented as a local search algorithm, originated\nas a bounded-width variant of dynamic programming for speech recognition in the HARPY\nsystem (Lowerre, 1976). A related algorithm is analyzed in depth by Pearl (1984, Ch. 5).\nThe topic of local search was reinvigorated in the early 1990s by surprisingly good re-\nsults for large constraint-satisfaction problems such as n-queens (Minton et al., 1992) and\nlogical reasoning (Selman et al., 1992) and by the incorporation of randomness, multiple\nsimultaneous searches, and other improvements. This renaissance of what Christos Papadim-\nitriou has called “New Age” algorithms also sparked increased interest among theoretical\ncomputer scientists (Koutsoupias and Papadimitriou, 1992; Aldous and Vazirani, 1994). In\nthe ﬁeld of operations research, a variant of hill climbing called tabu search has gained popu-\nTABU SEARCH\nlarity (Glover and Laguna, 1997). This algorithm maintains a tabu list of k previously visited\nstates that cannot be revisited; as well as improving efﬁciency when searching graphs, this list\ncan allow the algorithm to escape from some local minima. Another useful improvement on\nhill climbing is the STAGE algorithm (Boyan and Moore, 1998). The idea is to use the local\nmaxima found by random-restart hill climbing to get an idea of the overall shape of the land-\nscape. The algorithm ﬁts a smooth surface to the set of local maxima and then calculates the\nglobal maximum of that surface analytically. This becomes the new restart point. The algo-\nrithm has been shown to work in practice on hard problems. Gomes et al. (1998) showed that\nthe run times of systematic backtracking algorithms often have a heavy-tailed distribution,\nHEAVY-TAILED",
  "rithm has been shown to work in practice on hard problems. Gomes et al. (1998) showed that\nthe run times of systematic backtracking algorithms often have a heavy-tailed distribution,\nHEAVY-TAILED\nDISTRIBUTION\nwhich means that the probability of a very long run time is more than would be predicted if\nthe run times were exponentially distributed. When the run time distribution is heavy-tailed,\nrandom restarts ﬁnd a solution faster, on average, than a single run to completion. Bibliographical and Historical Notes\n155\nSimulated annealing was ﬁrst described by Kirkpatrick et al. (1983), who borrowed\ndirectly from the Metropolis algorithm (which is used to simulate complex systems in\nphysics (Metropolis et al., 1953) and was supposedly invented at a Los Alamos dinner party).\nSimulated annealing is now a ﬁeld in itself, with hundreds of papers published every year.\nFinding optimal solutions in continuous spaces is the subject matter of several ﬁelds,\nincluding optimization theory, optimal control theory, and the calculus of variations. The\nbasic techniques are explained well by Bishop (1995); Press et al. (2007) cover a wide range\nof algorithms and provide working software.\nAs Andrew Moore points out, researchers have taken inspiration for search and opti-\nmization algorithms from a wide variety of ﬁelds of study: metallurgy (simulated annealing),\nbiology (genetic algorithms), economics (market-based algorithms), entomology (ant colony\noptimization), neurology (neural networks), animal behavior (reinforcement learning), moun-\ntaineering (hill climbing), and others.\nLinear programming (LP) was ﬁrst studied systematically by the Russian mathemati-\ncian Leonid Kantorovich (1939). It was one of the ﬁrst applications of computers; the sim-\nplex algorithm (Dantzig, 1949) is still used despite worst-case exponential complexity. Kar-\nmarkar (1984) developed the far more efﬁcient family of interior-point methods, which was\nshown to have polynomial complexity for the more general class of convex optimization prob-\nlems by Nesterov and Nemirovski (1994). Excellent introductions to convex optimization are\nprovided by Ben-Tal and Nemirovski (2001) and Boyd and Vandenberghe (2004).\nWork by Sewall Wright (1931) on the concept of a ﬁtness landscape was an impor-\ntant precursor to the development of genetic algorithms. In the 1950s, several statisticians,\nincluding Box (1957) and Friedman (1959), used evolutionary techniques for optimization",
  "tant precursor to the development of genetic algorithms. In the 1950s, several statisticians,\nincluding Box (1957) and Friedman (1959), used evolutionary techniques for optimization\nproblems, but it wasn’t until Rechenberg (1965) introduced evolution strategies to solve op-\nEVOLUTION\nSTRATEGY\ntimization problems for airfoils that the approach gained popularity. In the 1960s and 1970s,\nJohn Holland (1975) championed genetic algorithms, both as a useful tool and as a method\nto expand our understanding of adaptation, biological or otherwise (Holland, 1995). The ar-\ntiﬁcial life movement (Langton, 1995) takes this idea one step further, viewing the products\nARTIFICIAL LIFE\nof genetic algorithms as organisms rather than solutions to problems. Work in this ﬁeld by\nHinton and Nowlan (1987) and Ackley and Littman (1991) has done much to clarify the im-\nplications of the Baldwin effect. For general background on evolution, we recommend Smith\nand Szathm´ary (1999), Ridley (2004), and Carroll (2007).\nMost comparisons of genetic algorithms to other approaches (especially stochastic hill\nclimbing) have found that the genetic algorithms are slower to converge (O’Reilly and Op-\npacher, 1994; Mitchell et al., 1996; Juels and Wattenberg, 1996; Baluja, 1997). Such ﬁndings\nare not universally popular within the GA community, but recent attempts within that com-\nmunity to understand population-based search as an approximate form of Bayesian learning\n(see Chapter 20) might help close the gap between the ﬁeld and its critics (Pelikan et al.,\n1999). The theory of quadratic dynamical systems may also explain the performance of\nGAs (Rabani et al., 1998). See Lohn et al. (2001) for an example of GAs applied to antenna\ndesign, and Renner and Ekart (2003) for an application to computer-aided design.\nThe ﬁeld of genetic programming is closely related to genetic algorithms. The princi-\nGENETIC\nPROGRAMMING\npal difference is that the representations that are mutated and combined are programs rather 156\nChapter\n4.\nBeyond Classical Search\nthan bit strings. The programs are represented in the form of expression trees; the expressions\ncan be in a standard language such as Lisp or can be specially designed to represent circuits,\nrobot controllers, and so on. Crossover involves splicing together subtrees rather than sub-\nstrings. This form of mutation guarantees that the offspring are well-formed expressions,\nwhich would not be the case if programs were manipulated as strings.",
  "strings. This form of mutation guarantees that the offspring are well-formed expressions,\nwhich would not be the case if programs were manipulated as strings.\nInterest in genetic programming was spurred by John Koza’s work (Koza, 1992, 1994),\nbut it goes back at least to early experiments with machine code by Friedberg (1958) and\nwith ﬁnite-state automata by Fogel et al. (1966). As with genetic algorithms, there is debate\nabout the effectiveness of the technique. Koza et al. (1999) describe experiments in the use\nof genetic programming to design circuit devices.\nThe journals Evolutionary Computation and IEEE Transactions on Evolutionary Com-\nputation cover genetic algorithms and genetic programming; articles are also found in Com-\nplex Systems, Adaptive Behavior, and Artiﬁcial Life. The main conference is the Genetic\nand Evolutionary Computation Conference (GECCO). Good overview texts on genetic algo-\nrithms are given by Mitchell (1996), Fogel (2000), and Langdon and Poli (2002), and by the\nfree online book by Poli et al. (2008).\nThe unpredictability and partial observability of real environments were recognized\nearly on in robotics projects that used planning techniques, including Shakey (Fikes et al.,\n1972) and FREDDY (Michie, 1974). The problems received more attention after the publica-\ntion of McDermott’s (1978a) inﬂuential article, Planning and Acting.\nThe ﬁrst work to make explicit use of AND–OR trees seems to have been Slagle’s SAINT\nprogram for symbolic integration, mentioned in Chapter 1. Amarel (1967) applied the idea\nto propositional theorem proving, a topic discussed in Chapter 7, and introduced a search\nalgorithm similar to AND-OR-GRAPH-SEARCH. The algorithm was further developed and\nformalized by Nilsson (1971), who also described AO∗—which, as its name suggests, ﬁnds\noptimal solutions given an admissible heuristic. AO∗was analyzed and improved by Martelli\nand Montanari (1973). AO∗is a top-down algorithm; a bottom-up generalization of A∗is\nA∗LD, for A∗Lightest Derivation (Felzenszwalb and McAllester, 2007). Interest in AND–OR\nsearch has undergone a revival in recent years, with new algorithms for ﬁnding cyclic solu-\ntions (Jimenez and Torras, 2000; Hansen and Zilberstein, 2001) and new techniques inspired\nby dynamic programming (Bonet and Geffner, 2005).\nThe idea of transforming partially observable problems into belief-state problems orig-\ninated with Astrom (1965) for the much more complex case of probabilistic uncertainty (see",
  "by dynamic programming (Bonet and Geffner, 2005).\nThe idea of transforming partially observable problems into belief-state problems orig-\ninated with Astrom (1965) for the much more complex case of probabilistic uncertainty (see\nChapter 17). Erdmann and Mason (1988) studied the problem of robotic manipulation with-\nout sensors, using a continuous form of belief-state search. They showed that it was possible\nto orient a part on a table from an arbitrary initial position by a well-designed sequence of tilt-\ning actions. More practical methods, based on a series of precisely oriented diagonal barriers\nacross a conveyor belt, use the same algorithmic insights (Wiegley et al., 1996).\nThe belief-state approach was reinvented in the context of sensorless and partially ob-\nservable search problems by Genesereth and Nourbakhsh (1993). Additional work was done\non sensorless problems in the logic-based planning community (Goldman and Boddy, 1996;\nSmith and Weld, 1998). This work has emphasized concise representations for belief states,\nas explained in Chapter 11. Bonet and Geffner (2000) introduced the ﬁrst effective heuristics Exercises\n157\nfor belief-state search; these were reﬁned by Bryce et al. (2006). The incremental approach\nto belief-state search, in which solutions are constructed incrementally for subsets of states\nwithin each belief state, was studied in the planning literature by Kurien et al. (2002); several\nnew incremental algorithms were introduced for nondeterministic, partially observable prob-\nlems by Russell and Wolfe (2005). Additional references for planning in stochastic, partially\nobservable environments appear in Chapter 17.\nAlgorithms for exploring unknown state spaces have been of interest for many centuries.\nDepth-ﬁrst search in a maze can be implemented by keeping one’s left hand on the wall; loops\ncan be avoided by marking each junction. Depth-ﬁrst search fails with irreversible actions;\nthe more general problem of exploring Eulerian graphs (i.e., graphs in which each node has\nEULERIAN GRAPH\nequal numbers of incoming and outgoing edges) was solved by an algorithm due to Hierholzer\n(1873). The ﬁrst thorough algorithmic study of the exploration problem for arbitrary graphs\nwas carried out by Deng and Papadimitriou (1990), who developed a completely general\nalgorithm but showed that no bounded competitive ratio is possible for exploring a general\ngraph. Papadimitriou and Yannakakis (1991) examined the question of ﬁnding paths to a goal",
  "algorithm but showed that no bounded competitive ratio is possible for exploring a general\ngraph. Papadimitriou and Yannakakis (1991) examined the question of ﬁnding paths to a goal\nin geometric path-planning environments (where all actions are reversible). They showed that\na small competitive ratio is achievable with square obstacles, but with general rectangular\nobstacles no bounded ratio can be achieved. (See Figure 4.20.)\nThe LRTA∗algorithm was developed by Korf (1990) as part of an investigation into\nreal-time search for environments in which the agent must act after searching for only a\nREAL-TIME SEARCH\nﬁxed amount of time (a common situation in two-player games). LRTA∗is in fact a special\ncase of reinforcement learning algorithms for stochastic environments (Barto et al., 1995). Its\npolicy of optimism under uncertainty—always head for the closest unvisited state—can result\nin an exploration pattern that is less efﬁcient in the uninformed case than simple depth-ﬁrst\nsearch (Koenig, 2000). Dasgupta et al. (1994) show that online iterative deepening search is\noptimally efﬁcient for ﬁnding a goal in a uniform tree with no heuristic information. Sev-\neral informed variants on the LRTA∗theme have been developed with different methods for\nsearching and updating within the known portion of the graph (Pemberton and Korf, 1992).\nAs yet, there is no good understanding of how to ﬁnd goals with optimal efﬁciency when\nusing heuristic information.\nEXERCISES\n4.1\nGive the name of the algorithm that results from each of the following special cases:\na. Local beam search with k = 1.\nb. Local beam search with one initial state and no limit on the number of states retained.\nc. Simulated annealing with T = 0 at all times (and omitting the termination test).\nd. Simulated annealing with T = ∞at all times.\ne. Genetic algorithm with population size N = 1. 158\nChapter\n4.\nBeyond Classical Search\n4.2\nExercise 3.16 considers the problem of building railway tracks under the assumption\nthat pieces ﬁt exactly with no slack. Now consider the real problem, in which pieces don’t\nﬁt exactly but allow for up to 10 degrees of rotation to either side of the “proper” alignment.\nExplain how to formulate the problem so it could be solved by simulated annealing.\n4.3\nIn this exercise, we explore the use of local search methods to solve TSPs of the type\ndeﬁned in Exercise 3.30.\na. Implement and test a hill-climbing method to solve TSPs. Compare the results with op-",
  "4.3\nIn this exercise, we explore the use of local search methods to solve TSPs of the type\ndeﬁned in Exercise 3.30.\na. Implement and test a hill-climbing method to solve TSPs. Compare the results with op-\ntimal solutions obtained from the A∗algorithm with the MST heuristic (Exercise 3.30).\nb. Repeat part (a) using a genetic algorithm instead of hill climbing. You may want to\nconsult Larra˜naga et al. (1999) for some suggestions for representations.\n4.4\nGenerate a large number of 8-puzzle and 8-queens instances and solve them (where pos-\nsible) by hill climbing (steepest-ascent and ﬁrst-choice variants), hill climbing with random\nrestart, and simulated annealing. Measure the search cost and percentage of solved problems\nand graph these against the optimal solution cost. Comment on your results.\n4.5\nThe AND-OR-GRAPH-SEARCH algorithm in Figure 4.11 checks for repeated states\nonly on the path from the root to the current state. Suppose that, in addition, the algorithm\nwere to store every visited state and check against that list. (See BREADTH-FIRST-SEARCH\nin Figure 3.11 for an example.) Determine the information that should be stored and how the\nalgorithm should use that information when a repeated state is found. (Hint: You will need to\ndistinguish at least between states for which a successful subplan was constructed previously\nand states for which no subplan could be found.) Explain how to use labels, as deﬁned in\nSection 4.3.3, to avoid having multiple copies of subplans.\n4.6\nExplain precisely how to modify the AND-OR-GRAPH-SEARCH algorithm to generate\na cyclic plan if no acyclic plan exists. You will need to deal with three issues: labeling the plan\nsteps so that a cyclic plan can point back to an earlier part of the plan, modifying OR-SEARCH\nso that it continues to look for acyclic plans after ﬁnding a cyclic plan, and augmenting the\nplan representation to indicate whether a plan is cyclic. Show how your algorithm works on\n(a) the slippery vacuum world, and (b) the slippery, erratic vacuum world. You might wish to\nuse a computer implementation to check your results.\n4.7\nIn Section 4.4.1 we introduced belief states to solve sensorless search problems. A\nsequence of actions solves a sensorless problem if it maps every physical state in the initial\nbelief state b to a goal state. Suppose the agent knows h∗(s), the true optimal cost of solving\nthe physical state s in the fully observable problem, for every state s in b. Find an admissible",
  "belief state b to a goal state. Suppose the agent knows h∗(s), the true optimal cost of solving\nthe physical state s in the fully observable problem, for every state s in b. Find an admissible\nheuristic h(b) for the sensorless problem in terms of these costs, and prove its admissibilty.\nComment on the accuracy of this heuristic on the sensorless vacuum problem of Figure 4.14.\nHow well does A∗perform?\n4.8\nThis exercise explores subset–superset relations between belief states in sensorless or\npartially observable environments.\na. Prove that if an action sequence is a solution for a belief state b, it is also a solution for\nany subset of b. Can anything be said about supersets of b? Exercises\n159\nb. Explain in detail how to modify graph search for sensorless problems to take advantage\nof your answers in (a).\nc. Explain in detail how to modify AND–OR search for partially observable problems,\nbeyond the modiﬁcations you describe in (b).\n4.9\nOn page 139 it was assumed that a given action would have the same cost when ex-\necuted in any physical state within a given belief state. (This leads to a belief-state search\nproblem with well-deﬁned step costs.) Now consider what happens when the assumption\ndoes not hold. Does the notion of optimality still make sense in this context, or does it require\nmodiﬁcation? Consider also various possible deﬁnitions of the “cost” of executing an action\nin a belief state; for example, we could use the minimum of the physical costs; or the maxi-\nmum; or a cost interval with the lower bound being the minimum cost and the upper bound\nbeing the maximum; or just keep the set of all possible costs for that action. For each of these,\nexplore whether A∗(with modiﬁcations if necessary) can return optimal solutions.\n4.10\nConsider the sensorless version of the erratic vacuum world. Draw the belief-state\nspace reachable from the initial belief state {1, 2, 3, 4, 5, 6, 7, 8}, and explain why the problem\nis unsolvable.\n4.11\nWe can turn the navigation problem in Exercise 3.7 into an environment as follows:\n• The percept will be a list of the positions, relative to the agent, of the visible vertices.\nThe percept does not include the position of the robot! The robot must learn its own po-\nsition from the map; for now, you can assume that each location has a different “view.”\n• Each action will be a vector describing a straight-line path to follow. If the path is\nunobstructed, the action succeeds; otherwise, the robot stops at the point where its",
  "• Each action will be a vector describing a straight-line path to follow. If the path is\nunobstructed, the action succeeds; otherwise, the robot stops at the point where its\npath ﬁrst intersects an obstacle. If the agent returns a zero motion vector and is at the\ngoal (which is ﬁxed and known), then the environment teleports the agent to a random\nlocation (not inside an obstacle).\n• The performance measure charges the agent 1 point for each unit of distance traversed\nand awards 1000 points each time the goal is reached.\na. Implement this environment and a problem-solving agent for it. After each teleporta-\ntion, the agent will need to formulate a new problem, which will involve discovering its\ncurrent location.\nb. Document your agent’s performance (by having the agent generate suitable commentary\nas it moves around) and report its performance over 100 episodes.\nc. Modify the environment so that 30% of the time the agent ends up at an unintended\ndestination (chosen randomly from the other visible vertices if any; otherwise, no move\nat all). This is a crude model of the motion errors of a real robot. Modify the agent\nso that when such an error is detected, it ﬁnds out where it is and then constructs a\nplan to get back to where it was and resume the old plan. Remember that sometimes\ngetting back to where it was might also fail! Show an example of the agent successfully\novercoming two successive motion errors and still reaching the goal. 160\nChapter\n4.\nBeyond Classical Search\nd. Now try two different recovery schemes after an error: (1) head for the closest vertex on\nthe original route; and (2) replan a route to the goal from the new location. Compare the\nperformance of the three recovery schemes. Would the inclusion of search costs affect\nthe comparison?\ne. Now suppose that there are locations from which the view is identical. (For example,\nsuppose the world is a grid with square obstacles.) What kind of problem does the agent\nnow face? What do solutions look like?\n4.12\nSuppose that an agent is in a 3 × 3 maze environment like the one shown in Fig-\nure 4.19. The agent knows that its initial location is (1,1), that the goal is at (3,3), and that the\nactions Up, Down, Left, Right have their usual effects unless blocked by a wall. The agent\ndoes not know where the internal walls are. In any given state, the agent perceives the set of\nlegal actions; it can also tell whether the state is one it has visited before.",
  "does not know where the internal walls are. In any given state, the agent perceives the set of\nlegal actions; it can also tell whether the state is one it has visited before.\na. Explain how this online search problem can be viewed as an ofﬂine search in belief-state\nspace, where the initial belief state includes all possible environment conﬁgurations.\nHow large is the initial belief state? How large is the space of belief states?\nb. How many distinct percepts are possible in the initial state?\nc. Describe the ﬁrst few branches of a contingency plan for this problem. How large\n(roughly) is the complete plan?\nNotice that this contingency plan is a solution for every possible environment ﬁtting the given\ndescription. Therefore, interleaving of search and execution is not strictly necessary even in\nunknown environments.\n4.13\nIn this exercise, we examine hill climbing in the context of robot navigation, using the\nenvironment in Figure 3.31 as an example.\na. Repeat Exercise 4.11 using hill climbing. Does your agent ever get stuck in a local\nminimum? Is it possible for it to get stuck with convex obstacles?\nb. Construct a nonconvex polygonal environment in which the agent gets stuck.\nc. Modify the hill-climbing algorithm so that, instead of doing a depth-1 search to decide\nwhere to go next, it does a depth-k search. It should ﬁnd the best k-step path and do\none step along it, and then repeat the process.\nd. Is there some k for which the new algorithm is guaranteed to escape from local minima?\ne. Explain how LRTA∗enables the agent to escape from local minima in this case.\n4.14\nLike DFS, online DFS is incomplete for reversible state spaces with inﬁnite paths. For\nexample, suppose that states are points on the inﬁnite two-dimensional grid and actions are\nunit vectors (1, 0), (0, 1), (−1, 0), (0, −1), tried in that order. Show that online DFS starting\nat (0, 0) will not reach (1, −1). Suppose the agent can observe, in addition to its current\nstate, all successor states and the actions that would lead to them. Write an algorithm that\nis complete even for bidirected state spaces with inﬁnite paths. What states does it visit in\nreaching (1, −1)? 5\nADVERSARIAL SEARCH\nIn which we examine the problems that arise when we try to plan ahead in a world\nwhere other agents are planning against us.\n5.1\nGAMES\nChapter 2 introduced multiagent environments, in which each agent needs to consider the",
  "ADVERSARIAL SEARCH\nIn which we examine the problems that arise when we try to plan ahead in a world\nwhere other agents are planning against us.\n5.1\nGAMES\nChapter 2 introduced multiagent environments, in which each agent needs to consider the\nactions of other agents and how they affect its own welfare. The unpredictability of these\nother agents can introduce contingencies into the agent’s problem-solving process, as dis-\ncussed in Chapter 4. In this chapter we cover competitive environments, in which the agents’\ngoals are in conﬂict, giving rise to adversarial search problems—often known as games.\nGAME\nMathematical game theory, a branch of economics, views any multiagent environment\nas a game, provided that the impact of each agent on the others is “signiﬁcant,” regardless\nof whether the agents are cooperative or competitive.1 In AI, the most common games are\nof a rather specialized kind—what game theorists call deterministic, turn-taking, two-player,\nzero-sum games of perfect information (such as chess). In our terminology, this means\nZERO-SUM GAMES\nPERFECT\nINFORMATION\ndeterministic, fully observable environments in which two agents act alternately and in which\nthe utility values at the end of the game are always equal and opposite. For example, if one\nplayer wins a game of chess, the other player necessarily loses. It is this opposition between\nthe agents’ utility functions that makes the situation adversarial.\nGames have engaged the intellectual faculties of humans—sometimes to an alarming\ndegree—for as long as civilization has existed. For AI researchers, the abstract nature of\ngames makes them an appealing subject for study. The state of a game is easy to represent,\nand agents are usually restricted to a small number of actions whose outcomes are deﬁned by\nprecise rules. Physical games, such as croquet and ice hockey, have much more complicated\ndescriptions, a much larger range of possible actions, and rather imprecise rules deﬁning\nthe legality of actions. With the exception of robot soccer, these physical games have not\nattracted much interest in the AI community.\n1 Environments with very many agents are often viewed as economies rather than games.\n161 162\nChapter\n5.\nAdversarial Search\nGames, unlike most of the toy problems studied in Chapter 3, are interesting because\nthey are too hard to solve. For example, chess has an average branching factor of about 35,\nand games often go to 50 moves by each player, so the search tree has about 35100 or 10154",
  "they are too hard to solve. For example, chess has an average branching factor of about 35,\nand games often go to 50 moves by each player, so the search tree has about 35100 or 10154\nnodes (although the search graph has “only” about 1040 distinct nodes). Games, like the real\nworld, therefore require the ability to make some decision even when calculating the optimal\ndecision is infeasible. Games also penalize inefﬁciency severely. Whereas an implementation\nof A∗search that is half as efﬁcient will simply take twice as long to run to completion, a chess\nprogram that is half as efﬁcient in using its available time probably will be beaten into the\nground, other things being equal. Game-playing research has therefore spawned a number of\ninteresting ideas on how to make the best possible use of time.\nWe begin with a deﬁnition of the optimal move and an algorithm for ﬁnding it. We\nthen look at techniques for choosing a good move when time is limited. Pruning allows us\nPRUNING\nto ignore portions of the search tree that make no difference to the ﬁnal choice, and heuristic\nevaluation functions allow us to approximate the true utility of a state without doing a com-\nplete search. Section 5.5 discusses games such as backgammon that include an element of\nchance; we also discuss bridge, which includes elements of imperfect information because\nIMPERFECT\nINFORMATION\nnot all cards are visible to each player. Finally, we look at how state-of-the-art game-playing\nprograms fare against human opposition and at directions for future developments.\nWe ﬁrst consider games with two players, whom we call MAX and MIN for reasons that\nwill soon become obvious. MAX moves ﬁrst, and then they take turns moving until the game\nis over. At the end of the game, points are awarded to the winning player and penalties are\ngiven to the loser. A game can be formally deﬁned as a kind of search problem with the\nfollowing elements:\n• S0: The initial state, which speciﬁes how the game is set up at the start.\n• PLAYER(s): Deﬁnes which player has the move in a state.\n• ACTIONS(s): Returns the set of legal moves in a state.\n• RESULT(s, a): The transition model, which deﬁnes the result of a move.\n• TERMINAL-TEST(s): A terminal test, which is true when the game is over and false\nTERMINAL TEST\notherwise. States where the game has ended are called terminal states.\nTERMINAL STATES\n• UTILITY(s, p): A utility function (also called an objective function or payoff function),",
  "TERMINAL TEST\notherwise. States where the game has ended are called terminal states.\nTERMINAL STATES\n• UTILITY(s, p): A utility function (also called an objective function or payoff function),\ndeﬁnes the ﬁnal numeric value for a game that ends in terminal state s for a player p. In\nchess, the outcome is a win, loss, or draw, with values +1, 0, or 1\n2. Some games have a\nwider variety of possible outcomes; the payoffs in backgammon range from 0 to +192.\nA zero-sum game is (confusingly) deﬁned as one where the total payoff to all players\nis the same for every instance of the game. Chess is zero-sum because every game has\npayoff of either 0 + 1, 1 + 0 or 1\n2 + 1\n2. “Constant-sum” would have been a better term,\nbut zero-sum is traditional and makes sense if you imagine each player is charged an\nentry fee of 1\n2.\nThe initial state, ACTIONS function, and RESULT function deﬁne the game tree for the\nGAME TREE\ngame—a tree where the nodes are game states and the edges are moves. Figure 5.1 shows\npart of the game tree for tic-tac-toe (noughts and crosses). From the initial state, MAX has\nnine possible moves. Play alternates between MAX’s placing an X and MIN’s placing an O Section 5.2.\nOptimal Decisions in Games\n163\nuntil we reach leaf nodes corresponding to terminal states such that one player has three in\na row or all the squares are ﬁlled. The number on each leaf node indicates the utility value\nof the terminal state from the point of view of MAX; high values are assumed to be good for\nMAX and bad for MIN (which is how the players get their names).\nFor tic-tac-toe the game tree is relatively small—fewer than 9! = 362, 880 terminal\nnodes. But for chess there are over 1040 nodes, so the game tree is best thought of as a\ntheoretical construct that we cannot realize in the physical world. But regardless of the size\nof the game tree, it is MAX’s job to search for a good move. We use the term search tree for a\nSEARCH TREE\ntree that is superimposed on the full game tree, and examines enough nodes to allow a player\nto determine what move to make.\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nO\nO\nX O\nO\nX O\nX O\nX\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\n. . .\nX\nX\n–1\n 0\n+1\nX\nX\nX\nX\nO\nX\nX\nO\nX\nX\nO\nO\nO\nX\nX\nX\nO\nO\nO\nO O X\nX\nMAX (X)\nMIN (O)\nMAX (X)\nMIN (O)\nTERMINAL\nUtility\nFigure 5.1\nA (partial) game tree for the game of tic-tac-toe. The top node is the initial\nstate, and MAX moves ﬁrst, placing an X in an empty square. We show part of the tree, giving",
  "X\nX\nO\nO\nO\nO O X\nX\nMAX (X)\nMIN (O)\nMAX (X)\nMIN (O)\nTERMINAL\nUtility\nFigure 5.1\nA (partial) game tree for the game of tic-tac-toe. The top node is the initial\nstate, and MAX moves ﬁrst, placing an X in an empty square. We show part of the tree, giving\nalternating moves by MIN (O) and MAX (X), until we eventually reach terminal states, which\ncan be assigned utilities according to the rules of the game.\n5.2\nOPTIMAL DECISIONS IN GAMES\nIn a normal search problem, the optimal solution would be a sequence of actions leading to\na goal state—a terminal state that is a win. In adversarial search, MIN has something to say\nabout it.\nMAX therefore must ﬁnd a contingent strategy, which speciﬁes MAX’s move in\nSTRATEGY\nthe initial state, then MAX’s moves in the states resulting from every possible response by 164\nChapter\n5.\nAdversarial Search\nMAX\nA\nB\nC\nD\n3\n12\n8\n2\n4\n6\n14\n5\n2\n3\n2\n2\n3\na1\na2\na3\nb1\nb2\nb3\nc1\nc2\nc3\nd1\nd2\nd3\nMIN\nFigure 5.2\nA two-ply game tree. The △nodes are “MAX nodes,” in which it is MAX’s\nturn to move, and the ▽nodes are “MIN nodes.” The terminal nodes show the utility values\nfor MAX; the other nodes are labeled with their minimax values. MAX’s best move at the root\nis a1, because it leads to the state with the highest minimax value, and MIN’s best reply is b1,\nbecause it leads to the state with the lowest minimax value.\nMIN, then MAX’s moves in the states resulting from every possible response by MIN to those\nmoves, and so on. This is exactly analogous to the AND–OR search algorithm (Figure 4.11)\nwith MAX playing the role of OR and MIN equivalent to AND. Roughly speaking, an optimal\nstrategy leads to outcomes at least as good as any other strategy when one is playing an\ninfallible opponent. We begin by showing how to ﬁnd this optimal strategy.\nEven a simple game like tic-tac-toe is too complex for us to draw the entire game tree\non one page, so we will switch to the trivial game in Figure 5.2. The possible moves for MAX\nat the root node are labeled a1, a2, and a3. The possible replies to a1 for MIN are b1, b2,\nb3, and so on. This particular game ends after one move each by MAX and MIN. (In game\nparlance, we say that this tree is one move deep, consisting of two half-moves, each of which\nis called a ply.) The utilities of the terminal states in this game range from 2 to 14.\nPLY\nGiven a game tree, the optimal strategy can be determined from the minimax value\nMINIMAX VALUE\nof each node, which we write as MINIMAX(n). The minimax value of a node is the utility",
  "PLY\nGiven a game tree, the optimal strategy can be determined from the minimax value\nMINIMAX VALUE\nof each node, which we write as MINIMAX(n). The minimax value of a node is the utility\n(for MAX) of being in the corresponding state, assuming that both players play optimally\nfrom there to the end of the game. Obviously, the minimax value of a terminal state is just\nits utility. Furthermore, given a choice, MAX prefers to move to a state of maximum value,\nwhereas MIN prefers a state of minimum value. So we have the following:\nMINIMAX(s) =\n⎧\n⎨\n⎩\nUTILITY(s)\nif TERMINAL-TEST(s)\nmaxa∈Actions(s) MINIMAX(RESULT(s, a)) if PLAYER(s) = MAX\nmina∈Actions(s) MINIMAX(RESULT(s,a))\nif PLAYER(s) = MIN\nLet us apply these deﬁnitions to the game tree in Figure 5.2. The terminal nodes on the bottom\nlevel get their utility values from the game’s UTILITY function. The ﬁrst MIN node, labeled\nB, has three successor states with values 3, 12, and 8, so its minimax value is 3. Similarly,\nthe other two MIN nodes have minimax value 2. The root node is a MAX node; its successor\nstates have minimax values 3, 2, and 2; so it has a minimax value of 3. We can also identify Section 5.2.\nOptimal Decisions in Games\n165\nthe minimax decision at the root: action a1 is the optimal choice for MAX because it leads to\nMINIMAX DECISION\nthe state with the highest minimax value.\nThis deﬁnition of optimal play for MAX assumes that MIN also plays optimally—it\nmaximizes the worst-case outcome for MAX. What if MIN does not play optimally? Then it is\neasy to show (Exercise 5.7) that MAX will do even better. Other strategies against suboptimal\nopponents may do better than the minimax strategy, but these strategies necessarily do worse\nagainst optimal opponents.\n5.2.1\nThe minimax algorithm\nThe minimax algorithm (Figure 5.3) computes the minimax decision from the current state.\nMINIMAX ALGORITHM\nIt uses a simple recursive computation of the minimax values of each successor state, directly\nimplementing the deﬁning equations. The recursion proceeds all the way down to the leaves\nof the tree, and then the minimax values are backed up through the tree as the recursion\nunwinds. For example, in Figure 5.2, the algorithm ﬁrst recurses down to the three bottom-\nleft nodes and uses the UTILITY function on them to discover that their values are 3, 12, and\n8, respectively. Then it takes the minimum of these values, 3, and returns it as the backed-",
  "left nodes and uses the UTILITY function on them to discover that their values are 3, 12, and\n8, respectively. Then it takes the minimum of these values, 3, and returns it as the backed-\nup value of node B. A similar process gives the backed-up values of 2 for C and 2 for D.\nFinally, we take the maximum of 3, 2, and 2 to get the backed-up value of 3 for the root node.\nThe minimax algorithm performs a complete depth-ﬁrst exploration of the game tree.\nIf the maximum depth of the tree is m and there are b legal moves at each point, then the\ntime complexity of the minimax algorithm is O(b m). The space complexity is O(bm) for an\nalgorithm that generates all actions at once, or O(m) for an algorithm that generates actions\none at a time (see page 87). For real games, of course, the time cost is totally impractical,\nbut this algorithm serves as the basis for the mathematical analysis of games and for more\npractical algorithms.\n5.2.2\nOptimal decisions in multiplayer games\nMany popular games allow more than two players. Let us examine how to extend the minimax\nidea to multiplayer games. This is straightforward from the technical viewpoint, but raises\nsome interesting new conceptual issues.\nFirst, we need to replace the single value for each node with a vector of values. For\nexample, in a three-player game with players A, B, and C, a vector ⟨vA, vB, vC⟩is associated\nwith each node. For terminal states, this vector gives the utility of the state from each player’s\nviewpoint. (In two-player, zero-sum games, the two-element vector can be reduced to a single\nvalue because the values are always opposite.) The simplest way to implement this is to have\nthe UTILITY function return a vector of utilities.\nNow we have to consider nonterminal states. Consider the node marked X in the game\ntree shown in Figure 5.4. In that state, player C chooses what to do. The two choices lead\nto terminal states with utility vectors ⟨vA = 1, vB = 2, vC = 6⟩and ⟨vA = 4, vB = 2, vC = 3⟩.\nSince 6 is bigger than 3, C should choose the ﬁrst move. This means that if state X is reached,\nsubsequent play will lead to a terminal state with utilities ⟨vA = 1, vB = 2, vC = 6⟩. Hence,\nthe backed-up value of X is this vector. The backed-up value of a node n is always the utility 166\nChapter\n5.\nAdversarial Search\nfunction MINIMAX-DECISION(state) returns an action\nreturn arg maxa ∈ACTIONS(s) MIN-VALUE(RESULT(state,a))\nfunction MAX-VALUE(state) returns a utility value",
  "Chapter\n5.\nAdversarial Search\nfunction MINIMAX-DECISION(state) returns an action\nreturn arg maxa ∈ACTIONS(s) MIN-VALUE(RESULT(state,a))\nfunction MAX-VALUE(state) returns a utility value\nif TERMINAL-TEST(state) then return UTILITY(state)\nv ←−∞\nfor each a in ACTIONS(state) do\nv ←MAX(v, MIN-VALUE(RESULT(s, a)))\nreturn v\nfunction MIN-VALUE(state) returns a utility value\nif TERMINAL-TEST(state) then return UTILITY(state)\nv ←∞\nfor each a in ACTIONS(state) do\nv ←MIN(v, MAX-VALUE(RESULT(s, a)))\nreturn v\nFigure 5.3\nAn algorithm for calculating minimax decisions. It returns the action corre-\nsponding to the best possible move, that is, the move that leads to the outcome with the\nbest utility, under the assumption that the opponent plays to minimize utility. The functions\nMAX-VALUE and MIN-VALUE go through the whole game tree, all the way to the leaves,\nto determine the backed-up value of a state. The notation argmaxa ∈S f(a) computes the\nelement a of set S that has the maximum value of f(a).\nto move\nA\nB\nC\nA\n(1, 2, 6)\n(4, 2, 3)\n(6, 1, 2)\n(7, 4,1)\n(5,1,1)\n(1, 5, 2)\n(7, 7,1)\n(5, 4, 5)\n(1, 2, 6)\n(6, 1, 2)\n(1, 5, 2)\n(5, 4, 5)\n(1, 2, 6)\n(1, 5, 2)\n(1, 2, 6)\nX\nFigure 5.4\nThe ﬁrst three plies of a game tree with three players (A, B, C). Each node is\nlabeled with values from the viewpoint of each player. The best move is marked at the root.\nvector of the successor state with the highest value for the player choosing at n. Anyone\nwho plays multiplayer games, such as Diplomacy, quickly becomes aware that much more\nis going on than in two-player games. Multiplayer games usually involve alliances, whether\nALLIANCE\nformal or informal, among the players. Alliances are made and broken as the game proceeds.\nHow are we to understand such behavior? Are alliances a natural consequence of optimal\nstrategies for each player in a multiplayer game? It turns out that they can be. For example, Section 5.3.\nAlpha–Beta Pruning\n167\nsuppose A and B are in weak positions and C is in a stronger position. Then it is often\noptimal for both A and B to attack C rather than each other, lest C destroy each of them\nindividually. In this way, collaboration emerges from purely selﬁsh behavior. Of course,\nas soon as C weakens under the joint onslaught, the alliance loses its value, and either A\nor B could violate the agreement. In some cases, explicit alliances merely make concrete\nwhat would have happened anyway. In other cases, a social stigma attaches to breaking an",
  "or B could violate the agreement. In some cases, explicit alliances merely make concrete\nwhat would have happened anyway. In other cases, a social stigma attaches to breaking an\nalliance, so players must balance the immediate advantage of breaking an alliance against the\nlong-term disadvantage of being perceived as untrustworthy. See Section 17.5 for more on\nthese complications.\nIf the game is not zero-sum, then collaboration can also occur with just two players.\nSuppose, for example, that there is a terminal state with utilities ⟨vA = 1000, vB = 1000⟩and\nthat 1000 is the highest possible utility for each player. Then the optimal strategy is for both\nplayers to do everything possible to reach this state—that is, the players will automatically\ncooperate to achieve a mutually desirable goal.\n5.3\nALPHA–BETA PRUNING\nThe problem with minimax search is that the number of game states it has to examine is\nexponential in the depth of the tree. Unfortunately, we can’t eliminate the exponent, but it\nturns out we can effectively cut it in half. The trick is that it is possible to compute the correct\nminimax decision without looking at every node in the game tree. That is, we can borrow the\nidea of pruning from Chapter 3 to eliminate large parts of the tree from consideration. The\nparticular technique we examine is called alpha–beta pruning. When applied to a standard\nALPHA–BETA\nPRUNING\nminimax tree, it returns the same move as minimax would, but prunes away branches that\ncannot possibly inﬂuence the ﬁnal decision.\nConsider again the two-ply game tree from Figure 5.2. Let’s go through the calculation\nof the optimal decision once more, this time paying careful attention to what we know at\neach point in the process. The steps are explained in Figure 5.5. The outcome is that we can\nidentify the minimax decision without ever evaluating two of the leaf nodes.\nAnother way to look at this is as a simpliﬁcation of the formula for MINIMAX. Let the\ntwo unevaluated successors of node C in Figure 5.5 have values x and y. Then the value of\nthe root node is given by\nMINIMAX(root) = max(min(3, 12, 8), min(2, x, y), min(14, 5, 2))\n= max(3, min(2, x, y), 2)\n= max(3, z, 2)\nwhere z = min(2, x, y) ≤2\n= 3.\nIn other words, the value of the root and hence the minimax decision are independent of the\nvalues of the pruned leaves x and y.\nAlpha–beta pruning can be applied to trees of any depth, and it is often possible to",
  "where z = min(2, x, y) ≤2\n= 3.\nIn other words, the value of the root and hence the minimax decision are independent of the\nvalues of the pruned leaves x and y.\nAlpha–beta pruning can be applied to trees of any depth, and it is often possible to\nprune entire subtrees rather than just leaves. The general principle is this: consider a node n 168\nChapter\n5.\nAdversarial Search\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n3\n3\n12\n3\n12\n8\n3\n12\n8\n2\n3\n12\n8\n2\n14\n3\n12\n8\n2\n14\n5\n2\nA\nB\nA\nB\nA\nB\nC\nD\nA\nB\nC\nD\nA\nB\nA\nB\nC\n[−∞, +∞]\n[−∞, +∞]\n[3, +∞]\n[3, +∞]\n[3, 3]\n[3, 14]\n[−∞, 2]\n[−∞, 2]\n[2, 2]\n[3, 3]\n[3, 3]\n[3, 3]\n[3, 3]\n[−∞, 3]\n[−∞, 3]\n[−∞, 2]\n[−∞, 14]\nFigure 5.5\nStages in the calculation of the optimal decision for the game tree in Figure 5.2.\nAt each point, we show the range of possible values for each node. (a) The ﬁrst leaf below B\nhas the value 3. Hence, B, which is a MIN node, has a value of at most 3. (b) The second leaf\nbelow B has a value of 12; MIN would avoid this move, so the value of B is still at most 3.\n(c) The third leaf below B has a value of 8; we have seen all B’s successor states, so the\nvalue of B is exactly 3. Now, we can infer that the value of the root is at least 3, because\nMAX has a choice worth 3 at the root. (d) The ﬁrst leaf below C has the value 2. Hence,\nC, which is a MIN node, has a value of at most 2. But we know that B is worth 3, so MAX\nwould never choose C. Therefore, there is no point in looking at the other successor states\nof C. This is an example of alpha–beta pruning. (e) The ﬁrst leaf below D has the value 14,\nso D is worth at most 14. This is still higher than MAX’s best alternative (i.e., 3), so we need\nto keep exploring D’s successor states. Notice also that we now have bounds on all of the\nsuccessors of the root, so the root’s value is also at most 14. (f) The second successor of D\nis worth 5, so again we need to keep exploring. The third successor is worth 2, so now D is\nworth exactly 2. MAX’s decision at the root is to move to B, giving a value of 3.\nsomewhere in the tree (see Figure 5.6), such that Player has a choice of moving to that node.\nIf Player has a better choice m either at the parent node of n or at any choice point further up,\nthen n will never be reached in actual play. So once we have found out enough about n (by\nexamining some of its descendants) to reach this conclusion, we can prune it.\nRemember that minimax search is depth-ﬁrst, so at any one time we just have to con-",
  "examining some of its descendants) to reach this conclusion, we can prune it.\nRemember that minimax search is depth-ﬁrst, so at any one time we just have to con-\nsider the nodes along a single path in the tree. Alpha–beta pruning gets its name from the\nfollowing two parameters that describe bounds on the backed-up values that appear anywhere\nalong the path: Section 5.3.\nAlpha–Beta Pruning\n169\nPlayer\nOpponent\nPlayer\nOpponent\nm\nn\n•\n•\n•\nFigure 5.6\nThe general case for alpha–beta pruning. If m is better than n for Player, we\nwill never get to n in play.\nα = the value of the best (i.e., highest-value) choice we have found so far at any choice point\nalong the path for MAX.\nβ = the value of the best (i.e., lowest-value) choice we have found so far at any choice point\nalong the path for MIN.\nAlpha–beta search updates the values of α and β as it goes along and prunes the remaining\nbranches at a node (i.e., terminates the recursive call) as soon as the value of the current\nnode is known to be worse than the current α or β value for MAX or MIN, respectively. The\ncomplete algorithm is given in Figure 5.7. We encourage you to trace its behavior when\napplied to the tree in Figure 5.5.\n5.3.1\nMove ordering\nThe effectiveness of alpha–beta pruning is highly dependent on the order in which the states\nare examined. For example, in Figure 5.5(e) and (f), we could not prune any successors of D\nat all because the worst successors (from the point of view of MIN) were generated ﬁrst. If\nthe third successor of D had been generated ﬁrst, we would have been able to prune the other\ntwo. This suggests that it might be worthwhile to try to examine ﬁrst the successors that are\nlikely to be best.\nIf this can be done,2 then it turns out that alpha–beta needs to examine only O(bm/2)\nnodes to pick the best move, instead of O(bm) for minimax. This means that the effective\nbranching factor becomes\n√\nb instead of b—for chess, about 6 instead of 35. Put another\nway, alpha–beta can solve a tree roughly twice as deep as minimax in the same amount of\ntime. If successors are examined in random order rather than best-ﬁrst, the total number of\nnodes examined will be roughly O(b3m/4) for moderate b. For chess, a fairly simple ordering\nfunction (such as trying captures ﬁrst, then threats, then forward moves, and then backward\nmoves) gets you to within about a factor of 2 of the best-case O(bm/2) result.",
  "function (such as trying captures ﬁrst, then threats, then forward moves, and then backward\nmoves) gets you to within about a factor of 2 of the best-case O(bm/2) result.\n2 Obviously, it cannot be done perfectly; otherwise, the ordering function could be used to play a perfect game! 170\nChapter\n5.\nAdversarial Search\nfunction ALPHA-BETA-SEARCH(state) returns an action\nv ←MAX-VALUE(state,−∞,+∞)\nreturn the action in ACTIONS(state) with value v\nfunction MAX-VALUE(state,α,β) returns a utility value\nif TERMINAL-TEST(state) then return UTILITY(state)\nv ←−∞\nfor each a in ACTIONS(state) do\nv ←MAX(v, MIN-VALUE(RESULT(s,a),α,β))\nif v ≥β then return v\nα ←MAX(α, v)\nreturn v\nfunction MIN-VALUE(state,α,β) returns a utility value\nif TERMINAL-TEST(state) then return UTILITY(state)\nv ←+∞\nfor each a in ACTIONS(state) do\nv ←MIN(v, MAX-VALUE(RESULT(s,a) ,α,β))\nif v ≤α then return v\nβ ←MIN(β, v)\nreturn v\nFigure 5.7\nThe alpha–beta search algorithm. Notice that these routines are the same as\nthe MINIMAX functions in Figure 5.3, except for the two lines in each of MIN-VALUE and\nMAX-VALUE that maintain α and β (and the bookkeeping to pass these parameters along).\nAdding dynamic move-ordering schemes, such as trying ﬁrst the moves that were found\nto be best in the past, brings us quite close to the theoretical limit. The past could be the\nprevious move—often the same threats remain—or it could come from previous exploration\nof the current move. One way to gain information from the current move is with iterative\ndeepening search. First, search 1 ply deep and record the best path of moves. Then search\n1 ply deeper, but use the recorded path to inform move ordering. As we saw in Chapter 3,\niterative deepening on an exponential game tree adds only a constant fraction to the total\nsearch time, which can be more than made up from better move ordering. The best moves are\noften called killer moves and to try them ﬁrst is called the killer move heuristic.\nKILLER MOVES\nIn Chapter 3, we noted that repeated states in the search tree can cause an exponential\nincrease in search cost. In many games, repeated states occur frequently because of transpo-\nsitions—different permutations of the move sequence that end up in the same position. For\nTRANSPOSITION\nexample, if White has one move, a1, that can be answered by Black with b1 and an unre-\nlated move a2 on the other side of the board that can be answered by b2, then the sequences",
  "TRANSPOSITION\nexample, if White has one move, a1, that can be answered by Black with b1 and an unre-\nlated move a2 on the other side of the board that can be answered by b2, then the sequences\n[a1, b1, a2, b2] and [a2, b2, a1, b1] both end up in the same position. It is worthwhile to store\nthe evaluation of the resulting position in a hash table the ﬁrst time it is encountered so that\nwe don’t have to recompute it on subsequent occurrences. The hash table of previously seen\npositions is traditionally called a transposition table; it is essentially identical to the explored\nTRANSPOSITION\nTABLE Section 5.4.\nImperfect Real-Time Decisions\n171\nlist in GRAPH-SEARCH (Section 3.3). Using a transposition table can have a dramatic effect,\nsometimes as much as doubling the reachable search depth in chess. On the other hand, if we\nare evaluating a million nodes per second, at some point it is not practical to keep all of them\nin the transposition table. Various strategies have been used to choose which nodes to keep\nand which to discard.\n5.4\nIMPERFECT REAL-TIME DECISIONS\nThe minimax algorithm generates the entire game search space, whereas the alpha–beta algo-\nrithm allows us to prune large parts of it. However, alpha–beta still has to search all the way\nto terminal states for at least a portion of the search space. This depth is usually not practical,\nbecause moves must be made in a reasonable amount of time—typically a few minutes at\nmost. Claude Shannon’s paper Programming a Computer for Playing Chess (1950) proposed\ninstead that programs should cut off the search earlier and apply a heuristic evaluation func-\ntion to states in the search, effectively turning nonterminal nodes into terminal leaves. In\nEVALUATION\nFUNCTION\nother words, the suggestion is to alter minimax or alpha–beta in two ways: replace the utility\nfunction by a heuristic evaluation function EVAL, which estimates the position’s utility, and\nreplace the terminal test by a cutoff test that decides when to apply EVAL. That gives us the\nCUTOFF TEST\nfollowing for heuristic minimax for state s and maximum depth d:\nH-MINIMAX(s, d) =\n⎧\n⎨\n⎩\nEVAL(s)\nif CUTOFF-TEST(s, d)\nmaxa∈Actions(s) H-MINIMAX(RESULT(s, a), d + 1)\nif PLAYER(s) = MAX\nmina∈Actions(s) H-MINIMAX(RESULT(s,a), d + 1)\nif PLAYER(s) = MIN.\n5.4.1\nEvaluation functions\nAn evaluation function returns an estimate of the expected utility of the game from a given\nposition, just as the heuristic functions of Chapter 3 return an estimate of the distance to",
  "if PLAYER(s) = MIN.\n5.4.1\nEvaluation functions\nAn evaluation function returns an estimate of the expected utility of the game from a given\nposition, just as the heuristic functions of Chapter 3 return an estimate of the distance to\nthe goal. The idea of an estimator was not new when Shannon proposed it. For centuries,\nchess players (and aﬁcionados of other games) have developed ways of judging the value of\na position because humans are even more limited in the amount of search they can do than\nare computer programs. It should be clear that the performance of a game-playing program\ndepends strongly on the quality of its evaluation function. An inaccurate evaluation function\nwill guide an agent toward positions that turn out to be lost. How exactly do we design good\nevaluation functions?\nFirst, the evaluation function should order the terminal states in the same way as the\ntrue utility function: states that are wins must evaluate better than draws, which in turn must\nbe better than losses. Otherwise, an agent using the evaluation function might err even if it\ncan see ahead all the way to the end of the game. Second, the computation must not take\ntoo long! (The whole point is to search faster.) Third, for nonterminal states, the evaluation\nfunction should be strongly correlated with the actual chances of winning. 172\nChapter\n5.\nAdversarial Search\nOne might well wonder about the phrase “chances of winning.” After all, chess is not a\ngame of chance: we know the current state with certainty, and no dice are involved. But if the\nsearch must be cut off at nonterminal states, then the algorithm will necessarily be uncertain\nabout the ﬁnal outcomes of those states. This type of uncertainty is induced by computational,\nrather than informational, limitations. Given the limited amount of computation that the\nevaluation function is allowed to do for a given state, the best it can do is make a guess about\nthe ﬁnal outcome.\nLet us make this idea more concrete. Most evaluation functions work by calculating\nvarious features of the state—for example, in chess, we would have features for the number\nof white pawns, black pawns, white queens, black queens, and so on. The features, taken\ntogether, deﬁne various categories or equivalence classes of states: the states in each category\nhave the same values for all the features. For example, one category contains all two-pawn\nvs. one-pawn endgames. Any given category, generally speaking, will contain some states",
  "have the same values for all the features. For example, one category contains all two-pawn\nvs. one-pawn endgames. Any given category, generally speaking, will contain some states\nthat lead to wins, some that lead to draws, and some that lead to losses. The evaluation\nfunction cannot know which states are which, but it can return a single value that reﬂects the\nproportion of states with each outcome. For example, suppose our experience suggests that\n72% of the states encountered in the two-pawns vs. one-pawn category lead to a win (utility\n+1); 20% to a loss (0), and 8% to a draw (1/2). Then a reasonable evaluation for states in\nthe category is the expected value: (0.72 × +1) + (0.20 × 0) + (0.08 × 1/2) = 0.76. In\nEXPECTED VALUE\nprinciple, the expected value can be determined for each category, resulting in an evaluation\nfunction that works for any state. As with terminal states, the evaluation function need not\nreturn actual expected values as long as the ordering of the states is the same.\nIn practice, this kind of analysis requires too many categories and hence too much\nexperience to estimate all the probabilities of winning. Instead, most evaluation functions\ncompute separate numerical contributions from each feature and then combine them to ﬁnd\nthe total value. For example, introductory chess books give an approximate material value\nMATERIAL VALUE\nfor each piece: each pawn is worth 1, a knight or bishop is worth 3, a rook 5, and the queen 9.\nOther features such as “good pawn structure” and “king safety” might be worth half a pawn,\nsay. These feature values are then simply added up to obtain the evaluation of the position.\nA secure advantage equivalent to a pawn gives a substantial likelihood of winning, and\na secure advantage equivalent to three pawns should give almost certain victory, as illustrated\nin Figure 5.8(a). Mathematically, this kind of evaluation function is called a weighted linear\nfunction because it can be expressed as\nWEIGHTED LINEAR\nFUNCTION\nEVAL(s) = w1f1(s) + w2f2(s) + · · · + wnfn(s) =\nn\n\f\ni=1\nwifi(s) ,\nwhere each wi is a weight and each fi is a feature of the position. For chess, the fi could be\nthe numbers of each kind of piece on the board, and the wi could be the values of the pieces\n(1 for pawn, 3 for bishop, etc.).\nAdding up the values of features seems like a reasonable thing to do, but in fact it\ninvolves a strong assumption: that the contribution of each feature is independent of the",
  "(1 for pawn, 3 for bishop, etc.).\nAdding up the values of features seems like a reasonable thing to do, but in fact it\ninvolves a strong assumption: that the contribution of each feature is independent of the\nvalues of the other features. For example, assigning the value 3 to a bishop ignores the fact\nthat bishops are more powerful in the endgame, when they have a lot of space to maneuver. Section 5.4.\nImperfect Real-Time Decisions\n173\n(b) White to move\n(a) White to move\nFigure 5.8\nTwo chess positions that differ only in the position of the rook at lower right.\nIn (a), Black has an advantage of a knight and two pawns, which should be enough to win\nthe game. In (b), White will capture the queen, giving it an advantage that should be strong\nenough to win.\nFor this reason, current programs for chess and other games also use nonlinear combinations\nof features. For example, a pair of bishops might be worth slightly more than twice the value\nof a single bishop, and a bishop is worth more in the endgame (that is, when the move number\nfeature is high or the number of remaining pieces feature is low).\nThe astute reader will have noticed that the features and weights are not part of the rules\nof chess! They come from centuries of human chess-playing experience. In games where this\nkind of experience is not available, the weights of the evaluation function can be estimated\nby the machine learning techniques of Chapter 18. Reassuringly, applying these techniques\nto chess has conﬁrmed that a bishop is indeed worth about three pawns.\n5.4.2\nCutting off search\nThe next step is to modify ALPHA-BETA-SEARCH so that it will call the heuristic EVAL\nfunction when it is appropriate to cut off the search. We replace the two lines in Figure 5.7\nthat mention TERMINAL-TEST with the following line:\nif CUTOFF-TEST(state, depth) then return EVAL(state)\nWe also must arrange for some bookkeeping so that the current depth is incremented on each\nrecursive call. The most straightforward approach to controlling the amount of search is to set\na ﬁxed depth limit so that CUTOFF-TEST(state, depth) returns true for all depth greater than\nsome ﬁxed depth d. (It must also return true for all terminal states, just as TERMINAL-TEST\ndid.) The depth d is chosen so that a move is selected within the allocated time. A more\nrobust approach is to apply iterative deepening. (See Chapter 3.) When time runs out, the\nprogram returns the move selected by the deepest completed search. As a bonus, iterative",
  "robust approach is to apply iterative deepening. (See Chapter 3.) When time runs out, the\nprogram returns the move selected by the deepest completed search. As a bonus, iterative\ndeepening also helps with move ordering. 174\nChapter\n5.\nAdversarial Search\nThese simple approaches can lead to errors due to the approximate nature of the eval-\nuation function. Consider again the simple evaluation function for chess based on material\nadvantage. Suppose the program searches to the depth limit, reaching the position in Fig-\nure 5.8(b), where Black is ahead by a knight and two pawns. It would report this as the\nheuristic value of the state, thereby declaring that the state is a probable win by Black. But\nWhite’s next move captures Black’s queen with no compensation. Hence, the position is\nreally won for White, but this can be seen only by looking ahead one more ply.\nObviously, a more sophisticated cutoff test is needed. The evaluation function should be\napplied only to positions that are quiescent—that is, unlikely to exhibit wild swings in value\nQUIESCENCE\nin the near future. In chess, for example, positions in which favorable captures can be made\nare not quiescent for an evaluation function that just counts material. Nonquiescent positions\ncan be expanded further until quiescent positions are reached. This extra search is called a\nquiescence search; sometimes it is restricted to consider only certain types of moves, such\nQUIESCENCE\nSEARCH\nas capture moves, that will quickly resolve the uncertainties in the position.\nThe horizon effect is more difﬁcult to eliminate. It arises when the program is facing\nHORIZON EFFECT\nan opponent’s move that causes serious damage and is ultimately unavoidable, but can be\ntemporarily avoided by delaying tactics. Consider the chess game in Figure 5.9. It is clear\nthat there is no way for the black bishop to escape. For example, the white rook can capture\nit by moving to h1, then a1, then a2; a capture at depth 6 ply. But Black does have a sequence\nof moves that pushes the capture of the bishop “over the horizon.” Suppose Black searches\nto depth 8 ply. Most moves by Black will lead to the eventual capture of the bishop, and thus\nwill be marked as “bad” moves. But Black will consider checking the white king with the\npawn at e4. This will lead to the king capturing the pawn. Now Black will consider checking\nagain, with the pawn at f5, leading to another pawn capture. That takes up 4 ply, and from",
  "pawn at e4. This will lead to the king capturing the pawn. Now Black will consider checking\nagain, with the pawn at f5, leading to another pawn capture. That takes up 4 ply, and from\nthere the remaining 4 ply is not enough to capture the bishop. Black thinks that the line of\nplay has saved the bishop at the price of two pawns, when actually all it has done is push the\ninevitable capture of the bishop beyond the horizon that Black can see.\nOne strategy to mitigate the horizon effect is the singular extension, a move that is\nSINGULAR\nEXTENSION\n“clearly better” than all other moves in a given position. Once discovered anywhere in the\ntree in the course of a search, this singular move is remembered. When the search reaches the\nnormal depth limit, the algorithm checks to see if the singular extension is a legal move; if it\nis, the algorithm allows the move to be considered. This makes the tree deeper, but because\nthere will be few singular extensions, it does not add many total nodes to the tree.\n5.4.3\nForward pruning\nSo far, we have talked about cutting off search at a certain level and about doing alpha–\nbeta pruning that provably has no effect on the result (at least with respect to the heuristic\nevaluation values). It is also possible to do forward pruning, meaning that some moves at\nFORWARD PRUNING\na given node are pruned immediately without further consideration. Clearly, most humans\nplaying chess consider only a few moves from each position (at least consciously). One\napproach to forward pruning is beam search: on each ply, consider only a “beam” of the n\nBEAM SEARCH\nbest moves (according to the evaluation function) rather than considering all possible moves. Section 5.4.\nImperfect Real-Time Decisions\n175\na     b    c    d    e     f     g    h\n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8\nFigure 5.9\nThe horizon effect. With Black to move, the black bishop is surely doomed.\nBut Black can forestall that event by checking the white king with its pawns, forcing the king\nto capture the pawns. This pushes the inevitable loss of the bishop over the horizon, and thus\nthe pawn sacriﬁces are seen by the search algorithm as good moves rather than bad ones.\nUnfortunately, this approach is rather dangerous because there is no guarantee that the best\nmove will not be pruned away.\nThe PROBCUT, or probabilistic cut, algorithm (Buro, 1995) is a forward-pruning ver-\nsion of alpha–beta search that uses statistics gained from prior experience to lessen the chance",
  "move will not be pruned away.\nThe PROBCUT, or probabilistic cut, algorithm (Buro, 1995) is a forward-pruning ver-\nsion of alpha–beta search that uses statistics gained from prior experience to lessen the chance\nthat the best move will be pruned. Alpha–beta search prunes any node that is provably out-\nside the current (α, β) window. PROBCUT also prunes nodes that are probably outside the\nwindow. It computes this probability by doing a shallow search to compute the backed-up\nvalue v of a node and then using past experience to estimate how likely it is that a score of v\nat depth d in the tree would be outside (α, β). Buro applied this technique to his Othello pro-\ngram, LOGISTELLO, and found that a version of his program with PROBCUT beat the regular\nversion 64% of the time, even when the regular version was given twice as much time.\nCombining all the techniques described here results in a program that can play cred-\nitable chess (or other games). Let us assume we have implemented an evaluation function for\nchess, a reasonable cutoff test with a quiescence search, and a large transposition table. Let\nus also assume that, after months of tedious bit-bashing, we can generate and evaluate around\na million nodes per second on the latest PC, allowing us to search roughly 200 million nodes\nper move under standard time controls (three minutes per move). The branching factor for\nchess is about 35, on average, and 355 is about 50 million, so if we used minimax search,\nwe could look ahead only about ﬁve plies. Though not incompetent, such a program can be\nfooled easily by an average human chess player, who can occasionally plan six or eight plies\nahead. With alpha–beta search we get to about 10 plies, which results in an expert level of\nplay. Section 5.8 describes additional pruning techniques that can extend the effective search\ndepth to roughly 14 plies. To reach grandmaster status we would need an extensively tuned\nevaluation function and a large database of optimal opening and endgame moves. 176\nChapter\n5.\nAdversarial Search\n5.4.4\nSearch versus lookup\nSomehow it seems like overkill for a chess program to start a game by considering a tree of a\nbillion game states, only to conclude that it will move its pawn to e4. Books describing good\nplay in the opening and endgame in chess have been available for about a century (Tattersall,\n1911). It is not surprising, therefore, that many game-playing programs use table lookup",
  "play in the opening and endgame in chess have been available for about a century (Tattersall,\n1911). It is not surprising, therefore, that many game-playing programs use table lookup\nrather than search for the opening and ending of games.\nFor the openings, the computer is mostly relying on the expertise of humans. The best\nadvice of human experts on how to play each opening is copied from books and entered into\ntables for the computer’s use. However, computers can also gather statistics from a database\nof previously played games to see which opening sequences most often lead to a win. In\nthe early moves there are few choices, and thus much expert commentary and past games on\nwhich to draw. Usually after ten moves we end up in a rarely seen position, and the program\nmust switch from table lookup to search.\nNear the end of the game there are again fewer possible positions, and thus more chance\nto do lookup.\nBut here it is the computer that has the expertise: computer analysis of\nendgames goes far beyond anything achieved by humans. A human can tell you the gen-\neral strategy for playing a king-and-rook-versus-king (KRK) endgame: reduce the opposing\nking’s mobility by squeezing it toward one edge of the board, using your king to prevent the\nopponent from escaping the squeeze. Other endings, such as king, bishop, and knight versus\nking (KBNK), are difﬁcult to master and have no succinct strategy description. A computer,\non the other hand, can completely solve the endgame by producing a policy, which is a map-\nPOLICY\nping from every possible state to the best move in that state. Then we can just look up the best\nmove rather than recompute it anew. How big will the KBNK lookup table be? It turns out\nthere are 462 ways that two kings can be placed on the board without being adjacent. After\nthe kings are placed, there are 62 empty squares for the bishop, 61 for the knight, and two\npossible players to move next, so there are just 462 × 62 × 61 × 2 = 3, 494, 568 possible\npositions. Some of these are checkmates; mark them as such in a table. Then do a retrograde\nRETROGRADE\nminimax search: reverse the rules of chess to do unmoves rather than moves. Any move by\nWhite that, no matter what move Black responds with, ends up in a position marked as a win,\nmust also be a win. Continue this search until all 3,494,568 positions are resolved as win,\nloss, or draw, and you have an infallible lookup table for all KBNK endgames.",
  "must also be a win. Continue this search until all 3,494,568 positions are resolved as win,\nloss, or draw, and you have an infallible lookup table for all KBNK endgames.\nUsing this technique and a tour de force of optimization tricks, Ken Thompson (1986,\n1996) and Lewis Stiller (1992, 1996) solved all chess endgames with up to ﬁve pieces and\nsome with six pieces, making them available on the Internet. Stiller discovered one case\nwhere a forced mate existed but required 262 moves; this caused some consternation because\nthe rules of chess require a capture or pawn move to occur within 50 moves. Later work by\nMarc Bourzutschky and Yakov Konoval (Bourzutschky, 2006) solved all pawnless six-piece\nand some seven-piece endgames; there is a KQNKRBN endgame that with best play requires\n517 moves until a capture, which then leads to a mate.\nIf we could extend the chess endgame tables from 6 pieces to 32, then White would\nknow on the opening move whether it would be a win, loss, or draw. This has not happened\nso far for chess, but it has happened for checkers, as explained in the historical notes section. Section 5.5.\nStochastic Games\n177\n5.5\nSTOCHASTIC GAMES\nIn real life, many unpredictable external events can put us into unforeseen situations. Many\ngames mirror this unpredictability by including a random element, such as the throwing of\ndice. We call these stochastic games. Backgammon is a typical game that combines luck\nSTOCHASTIC GAMES\nand skill. Dice are rolled at the beginning of a player’s turn to determine the legal moves. In\nthe backgammon position of Figure 5.10, for example, White has rolled a 6–5 and has four\npossible moves.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n24\n23\n22\n21\n20\n19\n18\n17\n16\n15\n14\n13\n0\n25\nFigure 5.10\nA typical backgammon position. The goal of the game is to move all one’s\npieces off the board. White moves clockwise toward 25, and Black moves counterclockwise\ntoward 0. A piece can move to any position unless multiple opponent pieces are there; if there\nis one opponent, it is captured and must start over. In the position shown, White has rolled\n6–5 and must choose among four legal moves: (5–10,5–11), (5–11,19–24), (5–10,10–16),\nand (5–11,11–16), where the notation (5–11,11–16) means move one piece from position 5\nto 11, and then move a piece from 11 to 16.\nAlthough White knows what his or her own legal moves are, White does not know what\nBlack is going to roll and thus does not know what Black’s legal moves will be. That means",
  "to 11, and then move a piece from 11 to 16.\nAlthough White knows what his or her own legal moves are, White does not know what\nBlack is going to roll and thus does not know what Black’s legal moves will be. That means\nWhite cannot construct a standard game tree of the sort we saw in chess and tic-tac-toe. A\ngame tree in backgammon must include chance nodes in addition to MAX and MIN nodes.\nCHANCE NODES\nChance nodes are shown as circles in Figure 5.11. The branches leading from each chance\nnode denote the possible dice rolls; each branch is labeled with the roll and its probability.\nThere are 36 ways to roll two dice, each equally likely; but because a 6–5 is the same as a 5–6,\nthere are only 21 distinct rolls. The six doubles (1–1 through 6–6) each have a probability of\n1/36, so we say P(1–1) = 1/36. The other 15 distinct rolls each have a 1/18 probability. 178\nChapter\n5.\nAdversarial Search\nCHANCE\nMIN\nMAX\nCHANCE\nMAX\n. . .\n. . .\nB\n1\n. . .\n1,1\n1/36\n1,2\n1/18\nTERMINAL\n1,2\n1/18\n...\n...\n...\n...\n...\n...\n...\n1,1\n1/36\n...\n...\n...\n...\n...\n...\nC\n. . .\n1/18\n6,5\n6,6\n1/36\n1/18\n6,5\n6,6\n1/36\n2\n–1\n1\n–1\nFigure 5.11\nSchematic game tree for a backgammon position.\nThe next step is to understand how to make correct decisions. Obviously, we still want\nto pick the move that leads to the best position. However, positions do not have deﬁnite\nminimax values. Instead, we can only calculate the expected value of a position: the average\nEXPECTED VALUE\nover all possible outcomes of the chance nodes.\nThis leads us to generalize the minimax value for deterministic games to an expecti-\nminimax value for games with chance nodes. Terminal nodes and MAX and MIN nodes (for\nEXPECTIMINIMAX\nVALUE\nwhich the dice roll is known) work exactly the same way as before. For chance nodes we\ncompute the expected value, which is the sum of the value over all outcomes, weighted by\nthe probability of each chance action:\nEXPECTIMINIMAX(s) =\n⎧\n⎪\n⎪\n⎨\n⎪\n⎪\n⎩\nUTILITY(s)\nif TERMINAL-TEST(s)\nmaxa EXPECTIMINIMAX(RESULT(s, a))\nif PLAYER(s) = MAX\nmina EXPECTIMINIMAX(RESULT(s, a))\nif PLAYER(s) = MIN\n\u0002\nr P(r)EXPECTIMINIMAX(RESULT(s, r)) if PLAYER(s) = CHANCE\nwhere r represents a possible dice roll (or other chance event) and RESULT(s, r) is the same\nstate as s, with the additional fact that the result of the dice roll is r.\n5.5.1\nEvaluation functions for games of chance\nAs with minimax, the obvious approximation to make with expectiminimax is to cut the",
  "state as s, with the additional fact that the result of the dice roll is r.\n5.5.1\nEvaluation functions for games of chance\nAs with minimax, the obvious approximation to make with expectiminimax is to cut the\nsearch off at some point and apply an evaluation function to each leaf. One might think that\nevaluation functions for games such as backgammon should be just like evaluation functions Section 5.5.\nStochastic Games\n179\nfor chess—they just need to give higher scores to better positions. But in fact, the presence of\nchance nodes means that one has to be more careful about what the evaluation values mean.\nFigure 5.12 shows what happens: with an evaluation function that assigns the values [1, 2,\n3, 4] to the leaves, move a1 is best; with values [1, 20, 30, 400], move a2 is best. Hence,\nthe program behaves totally differently if we make a change in the scale of some evaluation\nvalues! It turns out that to avoid this sensitivity, the evaluation function must be a positive\nlinear transformation of the probability of winning from a position (or, more generally, of the\nexpected utility of the position). This is an important and general property of situations in\nwhich uncertainty is involved, and we discuss it further in Chapter 16.\nCHANCE\nMIN\nMAX\n2\n2\n3\n3\n1\n1\n4\n4\n2\n3\n1\n4\n.9\n.1\n.9\n.1\n2.1\n1.3\n20\n20\n30\n30\n1\n1\n400\n400\n20\n30\n1\n400\n.9\n.1\n.9\n.1\n21\n40.9\na1\na2\na1\na2\nFigure 5.12\nAn order-preserving transformation on leaf values changes the best move.\nIf the program knew in advance all the dice rolls that would occur for the rest of the\ngame, solving a game with dice would be just like solving a game without dice, which mini-\nmax does in O(bm) time, where b is the branching factor and m is the maximum depth of the\ngame tree. Because expectiminimax is also considering all the possible dice-roll sequences,\nit will take O(bmnm), where n is the number of distinct rolls.\nEven if the search depth is limited to some small depth d, the extra cost compared with\nthat of minimax makes it unrealistic to consider looking ahead very far in most games of\nchance. In backgammon n is 21 and b is usually around 20, but in some situations can be as\nhigh as 4000 for dice rolls that are doubles. Three plies is probably all we could manage.\nAnother way to think about the problem is this: the advantage of alpha–beta is that\nit ignores future developments that just are not going to happen, given best play. Thus, it",
  "Another way to think about the problem is this: the advantage of alpha–beta is that\nit ignores future developments that just are not going to happen, given best play. Thus, it\nconcentrates on likely occurrences. In games with dice, there are no likely sequences of\nmoves, because for those moves to take place, the dice would ﬁrst have to come out the right\nway to make them legal. This is a general problem whenever uncertainty enters the picture:\nthe possibilities are multiplied enormously, and forming detailed plans of action becomes\npointless because the world probably will not play along.\nIt may have occurred to you that something like alpha–beta pruning could be applied 180\nChapter\n5.\nAdversarial Search\nto game trees with chance nodes. It turns out that it can. The analysis for MIN and MAX\nnodes is unchanged, but we can also prune chance nodes, using a bit of ingenuity. Consider\nthe chance node C in Figure 5.11 and what happens to its value as we examine and evaluate\nits children. Is it possible to ﬁnd an upper bound on the value of C before we have looked\nat all its children? (Recall that this is what alpha–beta needs in order to prune a node and its\nsubtree.) At ﬁrst sight, it might seem impossible because the value of C is the average of its\nchildren’s values, and in order to compute the average of a set of numbers, we must look at\nall the numbers. But if we put bounds on the possible values of the utility function, then we\ncan arrive at bounds for the average without looking at every number. For example, say that\nall utility values are between −2 and +2; then the value of leaf nodes is bounded, and in turn\nwe can place an upper bound on the value of a chance node without looking at all its children.\nAn alternative is to do Monte Carlo simulation to evaluate a position. Start with\nMONTE CARLO\nSIMULATION\nan alpha–beta (or other) search algorithm. From a start position, have the algorithm play\nthousands of games against itself, using random dice rolls. In the case of backgammon, the\nresulting win percentage has been shown to be a good approximation of the value of the\nposition, even if the algorithm has an imperfect heuristic and is searching only a few plies\n(Tesauro, 1995). For games with dice, this type of simulation is called a rollout.\nROLLOUT\n5.6\nPARTIALLY OBSERVABLE GAMES\nChess has often been described as war in miniature, but it lacks at least one major charac-",
  "(Tesauro, 1995). For games with dice, this type of simulation is called a rollout.\nROLLOUT\n5.6\nPARTIALLY OBSERVABLE GAMES\nChess has often been described as war in miniature, but it lacks at least one major charac-\nteristic of real wars, namely, partial observability. In the “fog of war,” the existence and\ndisposition of enemy units is often unknown until revealed by direct contact. As a result,\nwarfare includes the use of scouts and spies to gather information and the use of concealment\nand bluff to confuse the enemy. Partially observable games share these characteristics and\nare thus qualitatively different from the games described in the preceding sections.\n5.6.1\nKriegspiel: Partially observable chess\nIn deterministic partially observable games, uncertainty about the state of the board arises en-\ntirely from lack of access to the choices made by the opponent. This class includes children’s\ngames such as Battleships (where each player’s ships are placed in locations hidden from the\nopponent but do not move) and Stratego (where piece locations are known but piece types are\nhidden). We will examine the game of Kriegspiel, a partially observable variant of chess in\nKRIEGSPIEL\nwhich pieces can move but are completely invisible to the opponent.\nThe rules of Kriegspiel are as follows: White and Black each see a board containing\nonly their own pieces. A referee, who can see all the pieces, adjudicates the game and period-\nically makes announcements that are heard by both players. On his turn, White proposes to\nthe referee any move that would be legal if there were no black pieces. If the move is in fact\nnot legal (because of the black pieces), the referee announces “illegal.” In this case, White\nmay keep proposing moves until a legal one is found—and learns more about the location of\nBlack’s pieces in the process. Once a legal move is proposed, the referee announces one or Section 5.6.\nPartially Observable Games\n181\nmore of the following: “Capture on square X” if there is a capture, and “Check by D” if the\nblack king is in check, where D is the direction of the check, and can be one of “Knight,”\n“Rank,” “File,” “Long diagonal,” or “Short diagonal.” (In case of discovered check, the ref-\neree may make two “Check” announcements.) If Black is checkmated or stalemated, the\nreferee says so; otherwise, it is Black’s turn to move.\nKriegspiel may seem terrifyingly impossible, but humans manage it quite well and com-",
  "eree may make two “Check” announcements.) If Black is checkmated or stalemated, the\nreferee says so; otherwise, it is Black’s turn to move.\nKriegspiel may seem terrifyingly impossible, but humans manage it quite well and com-\nputer programs are beginning to catch up. It helps to recall the notion of a belief state as\ndeﬁned in Section 4.4 and illustrated in Figure 4.14—the set of all logically possible board\nstates given the complete history of percepts to date. Initially, White’s belief state is a sin-\ngleton because Black’s pieces haven’t moved yet. After White makes a move and Black re-\nsponds, White’s belief state contains 20 positions because Black has 20 replies to any White\nmove. Keeping track of the belief state as the game progresses is exactly the problem of state\nestimation, for which the update step is given in Equation (4.6). We can map Kriegspiel\nstate estimation directly onto the partially observable, nondeterministic framework of Sec-\ntion 4.4 if we consider the opponent as the source of nondeterminism; that is, the RESULTS\nof White’s move are composed from the (predictable) outcome of White’s own move and the\nunpredictable outcome given by Black’s reply.3\nGiven a current belief state, White may ask, “Can I win the game?” For a partially\nobservable game, the notion of a strategy is altered; instead of specifying a move to make\nfor each possible move the opponent might make, we need a move for every possible percept\nsequence that might be received. For Kriegspiel, a winning strategy, or guaranteed check-\nmate, is one that, for each possible percept sequence, leads to an actual checkmate for every\nGUARANTEED\nCHECKMATE\npossible board state in the current belief state, regardless of how the opponent moves. With\nthis deﬁnition, the opponent’s belief state is irrelevant—the strategy has to work even if the\nopponent can see all the pieces. This greatly simpliﬁes the computation. Figure 5.13 shows\npart of a guaranteed checkmate for the KRK (king and rook against king) endgame. In this\ncase, Black has just one piece (the king), so a belief state for White can be shown in a single\nboard by marking each possible position of the Black king.\nThe general AND-OR search algorithm can be applied to the belief-state space to ﬁnd\nguaranteed checkmates, just as in Section 4.4. The incremental belief-state algorithm men-\ntioned in that section often ﬁnds midgame checkmates up to depth 9—probably well beyond\nthe abilities of human players.",
  "guaranteed checkmates, just as in Section 4.4. The incremental belief-state algorithm men-\ntioned in that section often ﬁnds midgame checkmates up to depth 9—probably well beyond\nthe abilities of human players.\nIn addition to guaranteed checkmates, Kriegspiel admits an entirely new concept that\nmakes no sense in fully observable games: probabilistic checkmate. Such checkmates are\nPROBABILISTIC\nCHECKMATE\nstill required to work in every board state in the belief state; they are probabilistic with respect\nto randomization of the winning player’s moves. To get the basic idea, consider the problem\nof ﬁnding a lone black king using just the white king. Simply by moving randomly, the\nwhite king will eventually bump into the black king even if the latter tries to avoid this fate,\nsince Black cannot keep guessing the right evasive moves indeﬁnitely. In the terminology of\nprobability theory, detection occurs with probability 1. The KBNK endgame—king, bishop\n3 Sometimes, the belief state will become too large to represent just as a list of board states, but we will ignore\nthis issue for now; Chapters 7 and 8 suggest methods for compactly representing very large belief states. 182\nChapter\n5.\nAdversarial Search\na\n1\n2\n3\n4\nd\nb\nc\nKc3 ?\n“Illegal”\n“OK”\nRc3 ?\n“OK”\n“Check”\nFigure 5.13\nPart of a guaranteed checkmate in the KRK endgame, shown on a reduced\nboard. In the initial belief state, Black’s king is in one of three possible locations. By a\ncombination of probing moves, the strategy narrows this down to one. Completion of the\ncheckmate is left as an exercise.\nand knight against king—is won in this sense; White presents Black with an inﬁnite random\nsequence of choices, for one of which Black will guess incorrectly and reveal his position,\nleading to checkmate. The KBBK endgame, on the other hand, is won with probability 1−ϵ.\nWhite can force a win only by leaving one of his bishops unprotected for one move. If\nBlack happens to be in the right place and captures the bishop (a move that would lose if the\nbishops are protected), the game is drawn. White can choose to make the risky move at some\nrandomly chosen point in the middle of a very long sequence, thus reducing ϵ to an arbitrarily\nsmall constant, but cannot reduce ϵ to zero.\nIt is quite rare that a guaranteed or probabilistic checkmate can be found within any\nreasonable depth, except in the endgame. Sometimes a checkmate strategy works for some of",
  "small constant, but cannot reduce ϵ to zero.\nIt is quite rare that a guaranteed or probabilistic checkmate can be found within any\nreasonable depth, except in the endgame. Sometimes a checkmate strategy works for some of\nthe board states in the current belief state but not others. Trying such a strategy may succeed,\nleading to an accidental checkmate—accidental in the sense that White could not know that\nACCIDENTAL\nCHECKMATE\nit would be checkmate—if Black’s pieces happen to be in the right places. (Most checkmates\nin games between humans are of this accidental nature.) This idea leads naturally to the\nquestion of how likely it is that a given strategy will win, which leads in turn to the question\nof how likely it is that each board state in the current belief state is the true board state. Section 5.6.\nPartially Observable Games\n183\nOne’s ﬁrst inclination might be to propose that all board states in the current belief state\nare equally likely—but this can’t be right. Consider, for example, White’s belief state after\nBlack’s ﬁrst move of the game. By deﬁnition (assuming that Black plays optimally), Black\nmust have played an optimal move, so all board states resulting from suboptimal moves ought\nto be assigned zero probability. This argument is not quite right either, because each player’s\ngoal is not just to move pieces to the right squares but also to minimize the information that\nthe opponent has about their location. Playing any predictable “optimal” strategy provides\nthe opponent with information. Hence, optimal play in partially observable games requires\na willingness to play somewhat randomly. (This is why restaurant hygiene inspectors do\nrandom inspection visits.) This means occasionally selecting moves that may seem “intrinsi-\ncally” weak—but they gain strength from their very unpredictability, because the opponent is\nunlikely to have prepared any defense against them.\nFrom these considerations, it seems that the probabilities associated with the board\nstates in the current belief state can only be calculated given an optimal randomized strat-\negy; in turn, computing that strategy seems to require knowing the probabilities of the var-\nious states the board might be in. This conundrum can be resolved by adopting the game-\ntheoretic notion of an equilibrium solution, which we pursue further in Chapter 17. An\nequilibrium speciﬁes an optimal randomized strategy for each player. Computing equilib-",
  "theoretic notion of an equilibrium solution, which we pursue further in Chapter 17. An\nequilibrium speciﬁes an optimal randomized strategy for each player. Computing equilib-\nria is prohibitively expensive, however, even for small games, and is out of the question for\nKriegspiel. At present, the design of effective algorithms for general Kriegspiel play is an\nopen research topic. Most systems perform bounded-depth lookahead in their own belief-\nstate space, ignoring the opponent’s belief state. Evaluation functions resemble those for the\nobservable game but include a component for the size of the belief state—smaller is better!\n5.6.2\nCard games\nCard games provide many examples of stochastic partial observability, where the missing\ninformation is generated randomly. For example, in many games, cards are dealt randomly at\nthe beginning of the game, with each player receiving a hand that is not visible to the other\nplayers. Such games include bridge, whist, hearts, and some forms of poker.\nAt ﬁrst sight, it might seem that these card games are just like dice games: the cards are\ndealt randomly and determine the moves available to each player, but all the “dice” are rolled\nat the beginning! Even though this analogy turns out to be incorrect, it suggests an effective\nalgorithm: consider all possible deals of the invisible cards; solve each one as if it were a\nfully observable game; and then choose the move that has the best outcome averaged over all\nthe deals. Suppose that each deal s occurs with probability P(s); then the move we want is\nargmax\na\n\f\ns\nP(s) MINIMAX(RESULT(s,a)) .\n(5.1)\nHere, we run exact MINIMAX if computationally feasible; otherwise, we run H-MINIMAX.\nNow, in most card games, the number of possible deals is rather large. For example,\nin bridge play, each player sees just two of the four hands; there are two unseen hands of 13\ncards each, so the number of deals is\n\u001426\n13\n\u0015\n= 10, 400, 600. Solving even one deal is quite\ndifﬁcult, so solving ten million is out of the question. Instead, we resort to a Monte Carlo 184\nChapter\n5.\nAdversarial Search\napproximation: instead of adding up all the deals, we take a random sample of N deals,\nwhere the probability of deal s appearing in the sample is proportional to P(s):\nargmax\na\n1\nN\nN\n\f\ni = 1\nMINIMAX(RESULT(si, a)) .\n(5.2)\n(Notice that P(s) does not appear explicitly in the summation, because the samples are al-\nready drawn according to P(s).) As N grows large, the sum over the random sample tends",
  "argmax\na\n1\nN\nN\n\f\ni = 1\nMINIMAX(RESULT(si, a)) .\n(5.2)\n(Notice that P(s) does not appear explicitly in the summation, because the samples are al-\nready drawn according to P(s).) As N grows large, the sum over the random sample tends\nto the exact value, but even for fairly small N—say, 100 to 1,000—the method gives a good\napproximation. It can also be applied to deterministic games such as Kriegspiel, given some\nreasonable estimate of P(s).\nFor games like whist and hearts, where there is no bidding or betting phase before play\ncommences, each deal will be equally likely and so the values of P(s) are all equal. For\nbridge, play is preceded by a bidding phase in which each team indicates how many tricks it\nexpects to win. Since players bid based on the cards they hold, the other players learn more\nabout the probability of each deal. Taking this into account in deciding how to play the hand\nis tricky, for the reasons mentioned in our description of Kriegspiel: players may bid in such\na way as to minimize the information conveyed to their opponents. Even so, the approach is\nquite effective for bridge, as we show in Section 5.7.\nThe strategy described in Equations 5.1 and 5.2 is sometimes called averaging over\nclairvoyance because it assumes that the game will become observable to both players im-\nmediately after the ﬁrst move. Despite its intuitive appeal, the strategy can lead one astray.\nConsider the following story:\nDay 1: Road A leads to a heap of gold; Road B leads to a fork. Take the left fork and\nyou’ll ﬁnd a bigger heap of gold, but take the right fork and you’ll be run over by a bus.\nDay 2: Road A leads to a heap of gold; Road B leads to a fork. Take the right fork and\nyou’ll ﬁnd a bigger heap of gold, but take the left fork and you’ll be run over by a bus.\nDay 3: Road A leads to a heap of gold; Road B leads to a fork. One branch of the\nfork leads to a bigger heap of gold, but take the wrong fork and you’ll be hit by a bus.\nUnfortunately you don’t know which fork is which.\nAveraging over clairvoyance leads to the following reasoning: on Day 1, B is the right choice;\non Day 2, B is the right choice; on Day 3, the situation is the same as either Day 1 or Day 2,\nso B must still be the right choice.\nNow we can see how averaging over clairvoyance fails: it does not consider the belief\nstate that the agent will be in after acting. A belief state of total ignorance is not desirable, es-",
  "so B must still be the right choice.\nNow we can see how averaging over clairvoyance fails: it does not consider the belief\nstate that the agent will be in after acting. A belief state of total ignorance is not desirable, es-\npecially when one possibility is certain death. Because it assumes that every future state will\nautomatically be one of perfect knowledge, the approach never selects actions that gather in-\nformation (like the ﬁrst move in Figure 5.13); nor will it choose actions that hide information\nfrom the opponent or provide information to a partner because it assumes that they already\nknow the information; and it will never bluff in poker,4 because it assumes the opponent can\nBLUFF\nsee its cards. In Chapter 17, we show how to construct algorithms that do all these things by\nvirtue of solving the true partially observable decision problem.\n4 Blufﬁng—betting as if one’s hand is good, even when it’s not—is a core part of poker strategy. Section 5.7.\nState-of-the-Art Game Programs\n185\n5.7\nSTATE-OF-THE-ART GAME PROGRAMS\nIn 1965, the Russian mathematician Alexander Kronrod called chess “the Drosophila of ar-\ntiﬁcial intelligence.” John McCarthy disagrees: whereas geneticists use fruit ﬂies to make\ndiscoveries that apply to biology more broadly, AI has used chess to do the equivalent of\nbreeding very fast fruit ﬂies. Perhaps a better analogy is that chess is to AI as Grand Prix\nmotor racing is to the car industry: state-of-the-art game programs are blindingly fast, highly\noptimized machines that incorporate the latest engineering advances, but they aren’t much\nuse for doing the shopping or driving off-road. Nonetheless, racing and game-playing gen-\nerate excitement and a steady stream of innovations that have been adopted by the wider\ncommunity. In this section we look at what it takes to come out on top in various games.\nChess: IBM’s DEEP BLUE chess program, now retired, is well known for defeating world\nCHESS\nchampion Garry Kasparov in a widely publicized exhibition match. Deep Blue ran on a par-\nallel computer with 30 IBM RS/6000 processors doing alpha–beta search. The unique part\nwas a conﬁguration of 480 custom VLSI chess processors that performed move generation\nand move ordering for the last few levels of the tree, and evaluated the leaf nodes. Deep Blue\nsearched up to 30 billion positions per move, reaching depth 14 routinely. The key to its\nsuccess seems to have been its ability to generate singular extensions beyond the depth limit",
  "searched up to 30 billion positions per move, reaching depth 14 routinely. The key to its\nsuccess seems to have been its ability to generate singular extensions beyond the depth limit\nfor sufﬁciently interesting lines of forcing/forced moves. In some cases the search reached a\ndepth of 40 plies. The evaluation function had over 8000 features, many of them describing\nhighly speciﬁc patterns of pieces. An “opening book” of about 4000 positions was used, as\nwell as a database of 700,000 grandmaster games from which consensus recommendations\ncould be extracted. The system also used a large endgame database of solved positions con-\ntaining all positions with ﬁve pieces and many with six pieces. This database had the effect\nof substantially extending the effective search depth, allowing Deep Blue to play perfectly in\nsome cases even when it was many moves away from checkmate.\nThe success of DEEP BLUE reinforced the widely held belief that progress in computer\ngame-playing has come primarily from ever-more-powerful hardware—a view encouraged\nby IBM. But algorithmic improvements have allowed programs running on standard PCs\nto win World Computer Chess Championships. A variety of pruning heuristics are used to\nreduce the effective branching factor to less than 3 (compared with the actual branching factor\nof about 35). The most important of these is the null move heuristic, which generates a good\nNULL MOVE\nlower bound on the value of a position, using a shallow search in which the opponent gets\nto move twice at the beginning. This lower bound often allows alpha–beta pruning without\nthe expense of a full-depth search. Also important is futility pruning, which helps decide in\nFUTILITY PRUNING\nadvance which moves will cause a beta cutoff in the successor nodes.\nHYDRA can be seen as the successor to DEEP BLUE. HYDRA runs on a 64-processor\ncluster with 1 gigabyte per processor and with custom hardware in the form of FPGA (Field\nProgrammable Gate Array) chips. HYDRA reaches 200 million evaluations per second, about\nthe same as Deep Blue, but HYDRA reaches 18 plies deep rather than just 14 because of\naggressive use of the null move heuristic and forward pruning. 186\nChapter\n5.\nAdversarial Search\nRYBKA, winner of the 2008 and 2009 World Computer Chess Championships, is con-\nsidered the strongest current computer player. It uses an off-the-shelf 8-core 3.2 GHz Intel\nXeon processor, but little is known about the design of the program. RYBKA’s main ad-",
  "sidered the strongest current computer player. It uses an off-the-shelf 8-core 3.2 GHz Intel\nXeon processor, but little is known about the design of the program. RYBKA’s main ad-\nvantage appears to be its evaluation function, which has been tuned by its main developer,\nInternational Master Vasik Rajlich, and at least three other grandmasters.\nThe most recent matches suggest that the top computer chess programs have pulled\nahead of all human contenders. (See the historical notes for details.)\nCheckers: Jonathan Schaeffer and colleagues developed CHINOOK, which runs on regular\nCHECKERS\nPCs and uses alpha–beta search. Chinook defeated the long-running human champion in an\nabbreviated match in 1990, and since 2007 CHINOOK has been able to play perfectly by using\nalpha–beta search combined with a database of 39 trillion endgame positions.\nOthello, also called Reversi, is probably more popular as a computer game than as a board\nOTHELLO\ngame. It has a smaller search space than chess, usually 5 to 15 legal moves, but evaluation\nexpertise had to be developed from scratch. In 1997, the LOGISTELLO program (Buro, 2002)\ndefeated the human world champion, Takeshi Murakami, by six games to none. It is generally\nacknowledged that humans are no match for computers at Othello.\nBackgammon: Section 5.5 explained why the inclusion of uncertainty from dice rolls makes\nBACKGAMMON\ndeep search an expensive luxury. Most work on backgammon has gone into improving the\nevaluation function.\nGerry Tesauro (1992) combined reinforcement learning with neural\nnetworks to develop a remarkably accurate evaluator that is used with a search to depth 2\nor 3. After playing more than a million training games against itself, Tesauro’s program,\nTD-GAMMON, is competitive with top human players. The program’s opinions on the open-\ning moves of the game have in some cases radically altered the received wisdom.\nGo is the most popular board game in Asia. Because the board is 19 × 19 and moves are\nGO\nallowed into (almost) every empty square, the branching factor starts at 361, which is too\ndaunting for regular alpha–beta search methods. In addition, it is difﬁcult to write an eval-\nuation function because control of territory is often very unpredictable until the endgame.\nTherefore the top programs, such as MOGO, avoid alpha–beta search and instead use Monte\nCarlo rollouts. The trick is to decide what moves to make in the course of the rollout. There is",
  "Therefore the top programs, such as MOGO, avoid alpha–beta search and instead use Monte\nCarlo rollouts. The trick is to decide what moves to make in the course of the rollout. There is\nno aggressive pruning; all moves are possible. The UCT (upper conﬁdence bounds on trees)\nmethod works by making random moves in the ﬁrst few iterations, and over time guiding\nthe sampling process to prefer moves that have led to wins in previous samples. Some tricks\nare added, including knowledge-based rules that suggest particular moves whenever a given\npattern is detected and limited local search to decide tactical questions. Some programs also\ninclude special techniques from combinatorial game theory to analyze endgames. These\nCOMBINATORIAL\nGAME THEORY\ntechniques decompose a position into sub-positions that can be analyzed separately and then\ncombined (Berlekamp and Wolfe, 1994; M¨uller, 2003). The optimal solutions obtained in\nthis way have surprised many professional Go players, who thought they had been playing\noptimally all along. Current Go programs play at the master level on a reduced 9 × 9 board,\nbut are still at advanced amateur level on a full board.\nBridge is a card game of imperfect information: a player’s cards are hidden from the other\nBRIDGE\nplayers. Bridge is also a multiplayer game with four players instead of two, although the Section 5.8.\nAlternative Approaches\n187\nplayers are paired into two teams. As in Section 5.6, optimal play in partially observable\ngames like bridge can include elements of information gathering, communication, and careful\nweighing of probabilities. Many of these techniques are used in the Bridge Baron program\n(Smith et al., 1998), which won the 1997 computer bridge championship. While it does\nnot play optimally, Bridge Baron is one of the few successful game-playing systems to use\ncomplex, hierarchical plans (see Chapter 11) involving high-level ideas, such as ﬁnessing and\nsqueezing, that are familiar to bridge players.\nThe GIB program (Ginsberg, 1999) won the 2000 computer bridge championship quite\ndecisively using the Monte Carlo method. Since then, other winning programs have followed\nGIB’s lead. GIB’s major innovation is using explanation-based generalization to compute\nEXPLANATION-\nBASED\nGENERALIZATION\nand cache general rules for optimal play in various standard classes of situations rather than\nevaluating each situation individually. For example, in a situation where one player has the",
  "EXPLANATION-\nBASED\nGENERALIZATION\nand cache general rules for optimal play in various standard classes of situations rather than\nevaluating each situation individually. For example, in a situation where one player has the\ncards A-K-Q-J-4-3-2 of one suit and another player has 10-9-8-7-6-5, there are 7 × 6 = 42\nways that the ﬁrst player can lead from that suit and the second player can follow. But GIB\ntreats these situations as just two: the ﬁrst player can lead either a high card or a low card;\nthe exact cards played don’t matter. With this optimization (and a few others), GIB can solve\na 52-card, fully observable deal exactly in about a second. GIB’s tactical accuracy makes up\nfor its inability to reason about information. It ﬁnished 12th in a ﬁeld of 35 in the par contest\n(involving just play of the hand, not bidding) at the 1998 human world championship, far\nexceeding the expectations of many human experts.\nThere are several reasons why GIB plays at expert level with Monte Carlo simulation,\nwhereas Kriegspiel programs do not. First, GIB’s evaluation of the fully observable version\nof the game is exact, searching the full game tree, while Kriegspiel programs rely on inexact\nheuristics. But far more important is the fact that in bridge, most of the uncertainty in the\npartially observable information comes from the randomness of the deal, not from the adver-\nsarial play of the opponent. Monte Carlo simulation handles randomness well, but does not\nalways handle strategy well, especially when the strategy involves the value of information.\nScrabble: Most people think the hard part about Scrabble is coming up with good words, but\nSCRABBLE\ngiven the ofﬁcial dictionary, it turns out to be rather easy to program a move generator to ﬁnd\nthe highest-scoring move (Gordon, 1994). That doesn’t mean the game is solved, however:\nmerely taking the top-scoring move each turn results in a good but not expert player. The\nproblem is that Scrabble is both partially observable and stochastic: you don’t know what\nletters the other player has or what letters you will draw next. So playing Scrabble well\ncombines the difﬁculties of backgammon and bridge. Nevertheless, in 2006, the QUACKLE\nprogram defeated the former world champion, David Boys, 3–2.\n5.8\nALTERNATIVE APPROACHES\nBecause calculating optimal decisions in games is intractable in most cases, all algorithms\nmust make some assumptions and approximations. The standard approach, based on mini-",
  "5.8\nALTERNATIVE APPROACHES\nBecause calculating optimal decisions in games is intractable in most cases, all algorithms\nmust make some assumptions and approximations. The standard approach, based on mini-\nmax, evaluation functions, and alpha–beta, is just one way to do this. Probably because it has 188\nChapter\n5.\nAdversarial Search\nMAX\n99\n1000\n1000\n1000\n100\n101\n102\n100\n100\n99\nMIN\nFigure 5.14\nA two-ply game tree for which heuristic minimax may make an error.\nbeen worked on for so long, the standard approach dominates other methods in tournament\nplay. Some believe that this has caused game playing to become divorced from the main-\nstream of AI research: the standard approach no longer provides much room for new insight\ninto general questions of decision making. In this section, we look at the alternatives.\nFirst, let us consider heuristic minimax. It selects an optimal move in a given search\ntree provided that the leaf node evaluations are exactly correct. In reality, evaluations are\nusually crude estimates of the value of a position and can be considered to have large errors\nassociated with them. Figure 5.14 shows a two-ply game tree for which minimax suggests\ntaking the right-hand branch because 100 > 99. That is the correct move if the evaluations\nare all correct. But of course the evaluation function is only approximate. Suppose that\nthe evaluation of each node has an error that is independent of other nodes and is randomly\ndistributed with mean zero and standard deviation of σ. Then when σ = 5, the left-hand\nbranch is actually better 71% of the time, and 58% of the time when σ = 2. The intuition\nbehind this is that the right-hand branch has four nodes that are close to 99; if an error in\nthe evaluation of any one of the four makes the right-hand branch slip below 99, then the\nleft-hand branch is better.\nIn reality, circumstances are actually worse than this because the error in the evaluation\nfunction is not independent. If we get one node wrong, the chances are high that nearby nodes\nin the tree will also be wrong. The fact that the node labeled 99 has siblings labeled 1000\nsuggests that in fact it might have a higher true value. We can use an evaluation function\nthat returns a probability distribution over possible values, but it is difﬁcult to combine these\ndistributions properly, because we won’t have a good model of the very strong dependencies\nthat exist between the values of sibling nodes",
  "that returns a probability distribution over possible values, but it is difﬁcult to combine these\ndistributions properly, because we won’t have a good model of the very strong dependencies\nthat exist between the values of sibling nodes\nNext, we consider the search algorithm that generates the tree. The aim of an algorithm\ndesigner is to specify a computation that runs quickly and yields a good move. The alpha–beta\nalgorithm is designed not just to select a good move but also to calculate bounds on the values\nof all the legal moves. To see why this extra information is unnecessary, consider a position\nin which there is only one legal move. Alpha–beta search still will generate and evaluate a\nlarge search tree, telling us that the only move is the best move and assigning it a value. But\nsince we have to make the move anyway, knowing the move’s value is useless. Similarly, if\nthere is one obviously good move and several moves that are legal but lead to a quick loss, we Section 5.9.\nSummary\n189\nwould not want alpha–beta to waste time determining a precise value for the lone good move.\nBetter to just make the move quickly and save the time for later. This leads to the idea of the\nutility of a node expansion. A good search algorithm should select node expansions of high\nutility—that is, ones that are likely to lead to the discovery of a signiﬁcantly better move. If\nthere are no node expansions whose utility is higher than their cost (in terms of time), then\nthe algorithm should stop searching and make a move. Notice that this works not only for\nclear-favorite situations but also for the case of symmetrical moves, for which no amount of\nsearch will show that one move is better than another.\nThis kind of reasoning about what computations to do is called metareasoning (rea-\nMETAREASONING\nsoning about reasoning). It applies not just to game playing but to any kind of reasoning\nat all. All computations are done in the service of trying to reach better decisions, all have\ncosts, and all have some likelihood of resulting in a certain improvement in decision quality.\nAlpha–beta incorporates the simplest kind of metareasoning, namely, a theorem to the effect\nthat certain branches of the tree can be ignored without loss. It is possible to do much better.\nIn Chapter 16, we see how these ideas can be made precise and implementable.\nFinally, let us reexamine the nature of search itself. Algorithms for heuristic search",
  "In Chapter 16, we see how these ideas can be made precise and implementable.\nFinally, let us reexamine the nature of search itself. Algorithms for heuristic search\nand for game playing generate sequences of concrete states, starting from the initial state\nand then applying an evaluation function. Clearly, this is not how humans play games. In\nchess, one often has a particular goal in mind—for example, trapping the opponent’s queen—\nand can use this goal to selectively generate plausible plans for achieving it. This kind of\ngoal-directed reasoning or planning sometimes eliminates combinatorial search altogether.\nDavid Wilkins’ (1980) PARADISE is the only program to have used goal-directed reasoning\nsuccessfully in chess: it was capable of solving some chess problems requiring an 18-move\ncombination. As yet there is no good understanding of how to combine the two kinds of\nalgorithms into a robust and efﬁcient system, although Bridge Baron might be a step in the\nright direction. A fully integrated system would be a signiﬁcant achievement not just for\ngame-playing research but also for AI research in general, because it would be a good basis\nfor a general intelligent agent.\n5.9\nSUMMARY\nWe have looked at a variety of games to understand what optimal play means and to under-\nstand how to play well in practice. The most important ideas are as follows:\n• A game can be deﬁned by the initial state (how the board is set up), the legal actions\nin each state, the result of each action, a terminal test (which says when the game is\nover), and a utility function that applies to terminal states.\n• In two-player zero-sum games with perfect information, the minimax algorithm can\nselect optimal moves by a depth-ﬁrst enumeration of the game tree.\n• The alpha–beta search algorithm computes the same optimal move as minimax, but\nachieves much greater efﬁciency by eliminating subtrees that are provably irrelevant.\n• Usually, it is not feasible to consider the whole game tree (even with alpha–beta), so we 190\nChapter\n5.\nAdversarial Search\nneed to cut the search off at some point and apply a heuristic evaluation function that\nestimates the utility of a state.\n• Many game programs precompute tables of best moves in the opening and endgame so\nthat they can look up a move rather than search.\n• Games of chance can be handled by an extension to the minimax algorithm that eval-\nuates a chance node by taking the average utility of all its children, weighted by the",
  "that they can look up a move rather than search.\n• Games of chance can be handled by an extension to the minimax algorithm that eval-\nuates a chance node by taking the average utility of all its children, weighted by the\nprobability of each child.\n• Optimal play in games of imperfect information, such as Kriegspiel and bridge, re-\nquires reasoning about the current and future belief states of each player. A simple\napproximation can be obtained by averaging the value of an action over each possible\nconﬁguration of missing information.\n• Programs have bested even champion human players at games such as chess, checkers,\nand Othello. Humans retain the edge in several games of imperfect information, such\nas poker, bridge, and Kriegspiel, and in games with very large branching factors and\nlittle good heuristic knowledge, such as Go.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe early history of mechanical game playing was marred by numerous frauds. The most\nnotorious of these was Baron Wolfgang von Kempelen’s (1734–1804) “The Turk,” a supposed\nchess-playing automaton that defeated Napoleon before being exposed as a magician’s trick\ncabinet housing a human chess expert (see Levitt, 2000). It played from 1769 to 1854. In\n1846, Charles Babbage (who had been fascinated by the Turk) appears to have contributed\nthe ﬁrst serious discussion of the feasibility of computer chess and checkers (Morrison and\nMorrison, 1961). He did not understand the exponential complexity of search trees, claiming\n“the combinations involved in the Analytical Engine enormously surpassed any required,\neven by the game of chess.” Babbage also designed, but did not build, a special-purpose\nmachine for playing tic-tac-toe. The ﬁrst true game-playing machine was built around 1890\nby the Spanish engineer Leonardo Torres y Quevedo. It specialized in the “KRK” (king and\nrook vs. king) chess endgame, guaranteeing a win with king and rook from any position.\nThe minimax algorithm is traced to a 1912 paper by Ernst Zermelo, the developer of\nmodern set theory. The paper unfortunately contained several errors and did not describe min-\nimax correctly. On the other hand, it did lay out the ideas of retrograde analysis and proposed\n(but did not prove) what became known as Zermelo’s theorem: that chess is determined—\nWhite can force a win or Black can or it is a draw; we just don’t know which. Zermelo says\nthat should we eventually know, “Chess would of course lose the character of a game at all.”",
  "White can force a win or Black can or it is a draw; we just don’t know which. Zermelo says\nthat should we eventually know, “Chess would of course lose the character of a game at all.”\nA solid foundation for game theory was developed in the seminal work Theory of Games\nand Economic Behavior (von Neumann and Morgenstern, 1944), which included an analysis\nshowing that some games require strategies that are randomized (or otherwise unpredictable).\nSee Chapter 17 for more information. Bibliographical and Historical Notes\n191\nJohn McCarthy conceived the idea of alpha–beta search in 1956, although he did not\npublish it. The NSS chess program (Newell et al., 1958) used a simpliﬁed version of alpha–\nbeta; it was the ﬁrst chess program to do so. Alpha–beta pruning was described by Hart and\nEdwards (1961) and Hart et al. (1972). Alpha–beta was used by the “Kotok–McCarthy” chess\nprogram written by a student of John McCarthy (Kotok, 1962). Knuth and Moore (1975)\nproved the correctness of alpha–beta and analysed its time complexity. Pearl (1982b) shows\nalpha–beta to be asymptotically optimal among all ﬁxed-depth game-tree search algorithms.\nSeveral attempts have been made to overcome the problems with the “standard ap-\nproach” that were outlined in Section 5.8. The ﬁrst nonexhaustive heuristic search algorithm\nwith some theoretical grounding was probably B∗(Berliner, 1979), which attempts to main-\ntain interval bounds on the possible value of a node in the game tree rather than giving it\na single point-valued estimate. Leaf nodes are selected for expansion in an attempt to re-\nﬁne the top-level bounds until one move is “clearly best.” Palay (1985) extends the B∗idea\nusing probability distributions on values in place of intervals. David McAllester’s (1988)\nconspiracy number search expands leaf nodes that, by changing their values, could cause\nthe program to prefer a new move at the root. MGSS∗(Russell and Wefald, 1989) uses the\ndecision-theoretic techniques of Chapter 16 to estimate the value of expanding each leaf in\nterms of the expected improvement in decision quality at the root. It outplayed an alpha–\nbeta algorithm at Othello despite searching an order of magnitude fewer nodes. The MGSS∗\napproach is, in principle, applicable to the control of any form of deliberation.\nAlpha–beta search is in many ways the two-player analog of depth-ﬁrst branch-and-\nbound, which is dominated by A∗in the single-agent case. The SSS∗algorithm (Stockman,",
  "approach is, in principle, applicable to the control of any form of deliberation.\nAlpha–beta search is in many ways the two-player analog of depth-ﬁrst branch-and-\nbound, which is dominated by A∗in the single-agent case. The SSS∗algorithm (Stockman,\n1979) can be viewed as a two-player A∗and never expands more nodes than alpha–beta to\nreach the same decision. The memory requirements and computational overhead of the queue\nmake SSS∗in its original form impractical, but a linear-space version has been developed\nfrom the RBFS algorithm (Korf and Chickering, 1996). Plaat et al. (1996) developed a new\nview of SSS∗as a combination of alpha–beta and transposition tables, showing how to over-\ncome the drawbacks of the original algorithm and developing a new variant called MTD(f)\nthat has been adopted by a number of top programs.\nD. F. Beal (1980) and Dana Nau (1980, 1983) studied the weaknesses of minimax ap-\nplied to approximate evaluations. They showed that under certain assumptions about the dis-\ntribution of leaf values in the tree, minimaxing can yield values at the root that are actually less\nreliable than the direct use of the evaluation function itself. Pearl’s book Heuristics (1984)\npartially explains this apparent paradox and analyzes many game-playing algorithms. Baum\nand Smith (1997) propose a probability-based replacement for minimax, showing that it re-\nsults in better choices in certain games. The expectiminimax algorithm was proposed by\nDonald Michie (1966). Bruce Ballard (1983) extended alpha–beta pruning to cover trees\nwith chance nodes and Hauk (2004) reexamines this work and provides empirical results.\nKoller and Pfeffer (1997) describe a system for completely solving partially observ-\nable games. The system is quite general, handling games whose optimal strategy requires\nrandomized moves and games that are more complex than those handled by any previous\nsystem. Still, it can’t handle games as complex as poker, bridge, and Kriegspiel. Frank\net al. (1998) describe several variants of Monte Carlo search, including one where MIN has 192\nChapter\n5.\nAdversarial Search\ncomplete information but MAX does not. Among deterministic, partially observable games,\nKriegspiel has received the most attention. Ferguson demonstrated hand-derived random-\nized strategies for winning Kriegspiel with a bishop and knight (1992) or two bishops (1995)\nagainst a king. The ﬁrst Kriegspiel programs concentrated on ﬁnding endgame checkmates",
  "ized strategies for winning Kriegspiel with a bishop and knight (1992) or two bishops (1995)\nagainst a king. The ﬁrst Kriegspiel programs concentrated on ﬁnding endgame checkmates\nand performed AND–OR search in belief-state space (Sakuta and Iida, 2002; Bolognesi and\nCiancarini, 2003). Incremental belief-state algorithms enabled much more complex midgame\ncheckmates to be found (Russell and Wolfe, 2005; Wolfe and Russell, 2007), but efﬁcient\nstate estimation remains the primary obstacle to effective general play (Parker et al., 2005).\nChess was one of the ﬁrst tasks undertaken in AI, with early efforts by many of the pio-\nneers of computing, including Konrad Zuse in 1945, Norbert Wiener in his book Cybernetics\n(1948), and Alan Turing in 1950 (see Turing et al., 1953). But it was Claude Shannon’s\narticle Programming a Computer for Playing Chess (1950) that had the most complete set\nof ideas, describing a representation for board positions, an evaluation function, quiescence\nsearch, and some ideas for selective (nonexhaustive) game-tree search. Slater (1950) and the\ncommentators on his article also explored the possibilities for computer chess play.\nD. G. Prinz (1952) completed a program that solved chess endgame problems but did\nnot play a full game. Stan Ulam and a group at the Los Alamos National Lab produced a\nprogram that played chess on a 6 × 6 board with no bishops (Kister et al., 1957). It could\nsearch 4 plies deep in about 12 minutes. Alex Bernstein wrote the ﬁrst documented program\nto play a full game of standard chess (Bernstein and Roberts, 1958).5\nThe ﬁrst computer chess match featured the Kotok–McCarthy program from MIT (Ko-\ntok, 1962) and the ITEP program written in the mid-1960s at Moscow’s Institute of Theo-\nretical and Experimental Physics (Adelson-Velsky et al., 1970). This intercontinental match\nwas played by telegraph. It ended with a 3–1 victory for the ITEP program in 1967. The ﬁrst\nchess program to compete successfully with humans was MIT’s MACHACK-6 (Greenblatt\net al., 1967). Its Elo rating of approximately 1400 was well above the novice level of 1000.\nThe Fredkin Prize, established in 1980, offered awards for progressive milestones in\nchess play. The $5,000 prize for the ﬁrst program to achieve a master rating went to BELLE\n(Condon and Thompson, 1982), which achieved a rating of 2250. The $10,000 prize for the\nﬁrst program to achieve a USCF (United States Chess Federation) rating of 2500 (near the",
  "(Condon and Thompson, 1982), which achieved a rating of 2250. The $10,000 prize for the\nﬁrst program to achieve a USCF (United States Chess Federation) rating of 2500 (near the\ngrandmaster level) was awarded to DEEP THOUGHT (Hsu et al., 1990) in 1989. The grand\nprize, $100,000, went to DEEP BLUE (Campbell et al., 2002; Hsu, 2004) for its landmark\nvictory over world champion Garry Kasparov in a 1997 exhibition match. Kasparov wrote:\nThe decisive game of the match was Game 2, which left a scar in my memory . . . we saw\nsomething that went well beyond our wildest expectations of how well a computer would\nbe able to foresee the long-term positional consequences of its decisions. The machine\nrefused to move to a position that had a decisive short-term advantage—showing a very\nhuman sense of danger. (Kasparov, 1997)\nProbably the most complete description of a modern chess program is provided by Ernst\nHeinz (2000), whose DARKTHOUGHT program was the highest-ranked noncommercial PC\nprogram at the 1999 world championships.\n5 A Russian program, BESM may have predated Bernstein’s program. Bibliographical and Historical Notes\n193\n(a)\n(b)\nFigure 5.15\nPioneers in computer chess: (a) Herbert Simon and Allen Newell, developers\nof the NSS program (1958); (b) John McCarthy and the Kotok–McCarthy program on an\nIBM 7090 (1967).\nIn recent years, chess programs are pulling ahead of even the world’s best humans.\nIn 2004–2005 HYDRA defeated grand master Evgeny Vladimirov 3.5–0.5, world champion\nRuslan Ponomariov 2–0, and seventh-ranked Michael Adams 5.5–0.5. In 2006, DEEP FRITZ\nbeat world champion Vladimir Kramnik 4–2, and in 2007 RYBKA defeated several grand\nmasters in games in which it gave odds (such as a pawn) to the human players. As of 2009,\nthe highest Elo rating ever recorded was Kasparov’s 2851. HYDRA (Donninger and Lorenz,\n2004) is rated somewhere between 2850 and 3000, based mostly on its trouncing of Michael\nAdams. The RYBKA program is rated between 2900 and 3100, but this is based on a small\nnumber of games and is not considered reliable. Ross (2004) shows how human players have\nlearned to exploit some of the weaknesses of the computer programs.\nCheckers was the ﬁrst of the classic games fully played by a computer. Christopher\nStrachey (1952) wrote the ﬁrst working program for checkers. Beginning in 1952, Arthur\nSamuel of IBM, working in his spare time, developed a checkers program that learned its",
  "Strachey (1952) wrote the ﬁrst working program for checkers. Beginning in 1952, Arthur\nSamuel of IBM, working in his spare time, developed a checkers program that learned its\nown evaluation function by playing itself thousands of times (Samuel, 1959, 1967). We\ndescribe this idea in more detail in Chapter 21. Samuel’s program began as a novice but\nafter only a few days’ self-play had improved itself beyond Samuel’s own level. In 1962 it\ndefeated Robert Nealy, a champion at “blind checkers,” through an error on his part. When\none considers that Samuel’s computing equipment (an IBM 704) had 10,000 words of main\nmemory, magnetic tape for long-term storage, and a .000001 GHz processor, the win remains\na great accomplishment.\nThe challenge started by Samuel was taken up by Jonathan Schaeffer of the University\nof Alberta. His CHINOOK program came in second in the 1990 U.S. Open and earned the\nright to challenge for the world championship. It then ran up against a problem, in the form\nof Marion Tinsley. Dr. Tinsley had been world champion for over 40 years, losing only\nthree games in all that time. In the ﬁrst match against CHINOOK, Tinsley suffered his fourth 194\nChapter\n5.\nAdversarial Search\nand ﬁfth losses, but won the match 20.5–18.5. A rematch at the 1994 world championship\nended prematurely when Tinsley had to withdraw for health reasons. CHINOOK became the\nofﬁcial world champion. Schaeffer kept on building on his database of endgames, and in\n2007 “solved” checkers (Schaeffer et al., 2007; Schaeffer, 2008). This had been predicted by\nRichard Bellman (1965). In the paper that introduced the dynamic programming approach\nto retrograde analysis, he wrote, “In checkers, the number of possible moves in any given\nsituation is so small that we can conﬁdently expect a complete digital computer solution to\nthe problem of optimal play in this game.” Bellman did not, however, fully appreciate the\nsize of the checkers game tree. There are about 500 quadrillion positions. After 18 years\nof computation on a cluster of 50 or more machines, Jonathan Schaeffer’s team completed\nan endgame table for all checkers positions with 10 or fewer pieces: over 39 trillion entries.\nFrom there, they were able to do forward alpha–beta search to derive a policy that proves\nthat checkers is in fact a draw with best play by both sides. Note that this is an application\nof bidirectional search (Section 3.4.6). Building an endgame table for all of checkers would",
  "that checkers is in fact a draw with best play by both sides. Note that this is an application\nof bidirectional search (Section 3.4.6). Building an endgame table for all of checkers would\nbe impractical: it would require a billion gigabytes of storage. Searching without any table\nwould also be impractical: the search tree has about 847 positions, and would take thousands\nof years to search with today’s technology. Only a combination of clever search, endgame\ndata, and a drop in the price of processors and memory could solve checkers. Thus, checkers\njoins Qubic (Patashnik, 1980), Connect Four (Allis, 1988), and Nine-Men’s Morris (Gasser,\n1998) as games that have been solved by computer analysis.\nBackgammon, a game of chance, was analyzed mathematically by Gerolamo Cardano\n(1663), but only taken up for computer play in the late 1970s, ﬁrst with the BKG pro-\ngram (Berliner, 1980b); it used a complex, manually constructed evaluation function and\nsearched only to depth 1. It was the ﬁrst program to defeat a human world champion at a ma-\njor classic game (Berliner, 1980a). Berliner readily acknowledged that BKG was very lucky\nwith the dice. Gerry Tesauro’s (1995) TD-GAMMON played consistently at world champion\nlevel. The BGBLITZ program was the winner of the 2008 Computer Olympiad.\nGo is a deterministic game, but the large branching factor makes it challeging. The key\nissues and early literature in computer Go are summarized by Bouzy and Cazenave (2001) and\nM¨uller (2002). Up to 1997 there were no competent Go programs. Now the best programs\nplay most of their moves at the master level; the only problem is that over the course of a\ngame they usually make at least one serious blunder that allows a strong opponent to win.\nWhereas alpha–beta search reigns in most games, many recent Go programs have adopted\nMonte Carlo methods based on the UCT (upper conﬁdence bounds on trees) scheme (Kocsis\nand Szepesvari, 2006). The strongest Go program as of 2009 is Gelly and Silver’s MOGO\n(Wang and Gelly, 2007; Gelly and Silver, 2008). In August 2008, MOGO scored a surprising\nwin against top professional Myungwan Kim, albeit with MOGO receiving a handicap of\nnine stones (about the equivalent of a queen handicap in chess). Kim estimated MOGO’s\nstrength at 2–3 dan, the low end of advanced amateur. For this match, MOGO was run on\nan 800-processor 15 teraﬂop supercomputer (1000 times Deep Blue). A few weeks later,",
  "strength at 2–3 dan, the low end of advanced amateur. For this match, MOGO was run on\nan 800-processor 15 teraﬂop supercomputer (1000 times Deep Blue). A few weeks later,\nMOGO, with only a ﬁve-stone handicap, won against a 6-dan professional. In the 9 × 9 form\nof Go, MOGO is at approximately the 1-dan professional level. Rapid advances are likely\nas experimentation continues with new forms of Monte Carlo search. The Computer Go Exercises\n195\nNewsletter, published by the Computer Go Association, describes current developments.\nBridge: Smith et al. (1998) report on how their planning-based program won the 1998\ncomputer bridge championship, and (Ginsberg, 2001) describes how his GIB program, based\non Monte Carlo simulation, won the following computer championship and did surprisingly\nwell against human players and standard book problem sets. From 2001–2007, the computer\nbridge championship was won ﬁve times by JACK and twice by WBRIDGE5. Neither has\nhad academic articles explaining their structure, but both are rumored to use the Monte Carlo\ntechnique, which was ﬁrst proposed for bridge by Levy (1989).\nScrabble: A good description of a top program, MAVEN, is given by its creator, Brian\nSheppard (2002). Generating the highest-scoring move is described by Gordon (1994), and\nmodeling opponents is covered by Richards and Amir (2007).\nSoccer (Kitano et al., 1997b; Visser et al., 2008) and billiards (Lam and Greenspan,\n2008; Archibald et al., 2009) and other stochastic games with a continuous space of actions\nare beginning to attract attention in AI, both in simulation and with physical robot players.\nComputer game competitions occur annually, and papers appear in a variety of venues.\nThe rather misleadingly named conference proceedings Heuristic Programming in Artiﬁcial\nIntelligence report on the Computer Olympiads, which include a wide variety of games. The\nGeneral Game Competition (Love et al., 2006) tests programs that must learn to play an un-\nknown game given only a logical description of the rules of the game. There are also several\nedited collections of important papers on game-playing research (Levy, 1988a, 1988b; Mars-\nland and Schaeffer, 1990). The International Computer Chess Association (ICCA), founded\nin 1977, publishes the ICGA Journal (formerly the ICCA Journal). Important papers have\nbeen published in the serial anthology Advances in Computer Chess, starting with Clarke",
  "in 1977, publishes the ICGA Journal (formerly the ICCA Journal). Important papers have\nbeen published in the serial anthology Advances in Computer Chess, starting with Clarke\n(1977). Volume 134 of the journal Artiﬁcial Intelligence (2002) contains descriptions of\nstate-of-the-art programs for chess, Othello, Hex, shogi, Go, backgammon, poker, Scrabble,\nand other games. Since 1998, a biennial Computers and Games conference has been held.\nEXERCISES\n5.1\nSuppose you have an oracle, OM(s), that correctly predicts the opponent’s move in\nany state. Using this, formulate the deﬁnition of a game as a (single-agent) search problem.\nDescribe an algorithm for ﬁnding the optimal move.\n5.2\nConsider the problem of solving two 8-puzzles.\na. Give a complete problem formulation in the style of Chapter 3.\nb. How large is the reachable state space? Give an exact numerical expression.\nc. Suppose we make the problem adversarial as follows: the two players take turns mov-\ning; a coin is ﬂipped to determine the puzzle on which to make a move in that turn; and\nthe winner is the ﬁrst to solve one puzzle. Which algorithm can be used to choose a\nmove in this setting?\nd. Give an informal proof that someone will eventually win if both play perfectly. 196\nChapter\n5.\nAdversarial Search\n(b)\n(a)\na\nf\ne\nd\nc\nb\nbd\ncd\nad\nce\ncf\ncc\nae\naf\nac\nde\ndf\ndd\ndd\n?\n?\n?\n?\n?\nP\nE\nFigure 5.16\n(a) A map where the cost of every edge is 1. Initially the pursuer P is at node\nb and the evader E is at node d. (b) A partial game tree for this map. Each node is labeled\nwith the P, E positions. P moves ﬁrst. Branches marked “?” have yet to be explored.\n5.3\nImagine that, in Exercise 3.3, one of the friends wants to avoid the other. The problem\nthen becomes a two-player pursuit–evasion game. We assume now that the players take\nPURSUIT–EVASION\nturns moving. The game ends only when the players are on the same node; the terminal\npayoff to the pursuer is minus the total time taken. (The evader “wins” by never losing.) An\nexample is shown in Figure 5.16.\na. Copy the game tree and mark the values of the terminal nodes.\nb. Next to each internal node, write the strongest fact you can infer about its value (a\nnumber, one or more inequalities such as “≥14”, or a “?”).\nc. Beneath each question mark, write the name of the node reached by that branch.\nd. Explain how a bound on the value of the nodes in (c) can be derived from consideration\nof shortest-path lengths on the map, and derive such bounds for these nodes. Remember",
  "d. Explain how a bound on the value of the nodes in (c) can be derived from consideration\nof shortest-path lengths on the map, and derive such bounds for these nodes. Remember\nthe cost to get to each leaf as well as the cost to solve it.\ne. Now suppose that the tree as given, with the leaf bounds from (d), is evaluated from left\nto right. Circle those “?” nodes that would not need to be expanded further, given the\nbounds from part (d), and cross out those that need not be considered at all.\nf. Can you prove anything in general about who wins the game on a map that is a tree? Exercises\n197\n5.4\nDescribe and implement state descriptions, move generators, terminal tests, utility func-\ntions, and evaluation functions for one or more of the following stochastic games: Monopoly,\nScrabble, bridge play with a given contract, or Texas hold’em poker.\n5.5\nDescribe and implement a real-time, multiplayer game-playing environment, where\ntime is part of the environment state and players are given ﬁxed time allocations.\n5.6\nDiscuss how well the standard approach to game playing would apply to games such as\ntennis, pool, and croquet, which take place in a continuous physical state space.\n5.7\nProve the following assertion: For every game tree, the utility obtained by MAX using\nminimax decisions against a suboptimal MIN will be never be lower than the utility obtained\nplaying against an optimal MIN. Can you come up with a game tree in which MAX can do\nstill better using a suboptimal strategy against a suboptimal MIN?\nA\nB\n1\n4\n3\n2\nFigure 5.17\nThe starting position of a simple game. Player A moves ﬁrst. The two players\ntake turns moving, and each player must move his token to an open adjacent space in either\ndirection. If the opponent occupies an adjacent space, then a player may jump over the\nopponent to the next open space if any. (For example, if A is on 3 and B is on 2, then A may\nmove back to 1.) The game ends when one player reaches the opposite end of the board. If\nplayer A reaches space 4 ﬁrst, then the value of the game to A is +1; if player B reaches\nspace 1 ﬁrst, then the value of the game to A is −1.\n5.8\nConsider the two-player game described in Figure 5.17.\na. Draw the complete game tree, using the following conventions:\n• Write each state as (sA, sB), where sA and sB denote the token locations.\n• Put each terminal state in a square box and write its game value in a circle.\n• Put loop states (states that already appear on the path to the root) in double square",
  "• Write each state as (sA, sB), where sA and sB denote the token locations.\n• Put each terminal state in a square box and write its game value in a circle.\n• Put loop states (states that already appear on the path to the root) in double square\nboxes. Since their value is unclear, annotate each with a “?” in a circle.\nb. Now mark each node with its backed-up minimax value (also in a circle). Explain how\nyou handled the “?” values and why.\nc. Explain why the standard minimax algorithm would fail on this game tree and brieﬂy\nsketch how you might ﬁx it, drawing on your answer to (b). Does your modiﬁed algo-\nrithm give optimal decisions for all games with loops?\nd. This 4-square game can be generalized to n squares for any n > 2. Prove that A wins\nif n is even and loses if n is odd.\n5.9\nThis problem exercises the basic concepts of game playing, using tic-tac-toe (noughts\nand crosses) as an example. We deﬁne Xn as the number of rows, columns, or diagonals 198\nChapter\n5.\nAdversarial Search\nwith exactly n X’s and no O’s. Similarly, On is the number of rows, columns, or diagonals\nwith just n O’s. The utility function assigns +1 to any position with X3 = 1 and −1 to any\nposition with O3 = 1. All other terminal positions have utility 0. For nonterminal positions,\nwe use a linear evaluation function deﬁned as Eval(s) = 3X2(s)+X1(s)−(3O2(s)+O1(s)).\na. Approximately how many possible games of tic-tac-toe are there?\nb. Show the whole game tree starting from an empty board down to depth 2 (i.e., one X\nand one O on the board), taking symmetry into account.\nc. Mark on your tree the evaluations of all the positions at depth 2.\nd. Using the minimax algorithm, mark on your tree the backed-up values for the positions\nat depths 1 and 0, and use those values to choose the best starting move.\ne. Circle the nodes at depth 2 that would not be evaluated if alpha–beta pruning were\napplied, assuming the nodes are generated in the optimal order for alpha–beta pruning.\n5.10\nConsider the family of generalized tic-tac-toe games, deﬁned as follows. Each partic-\nular game is speciﬁed by a set S of squares and a collection W of winning positions. Each\nwinning position is a subset of S. For example, in standard tic-tac-toe, S is a set of 9 squares\nand W is a collection of 8 subsets of W: the three rows, the three columns, and the two diag-\nonals. In other respects, the game is identical to standard tic-tac-toe. Starting from an empty",
  "and W is a collection of 8 subsets of W: the three rows, the three columns, and the two diag-\nonals. In other respects, the game is identical to standard tic-tac-toe. Starting from an empty\nboard, players alternate placing their marks on an empty square. A player who marks every\nsquare in a winning position wins the game. It is a tie if all squares are marked and neither\nplayer has won.\na. Let N = |S|, the number of squares. Give an upper bound on the number of nodes in\nthe complete game tree for generalized tic-tac-toe as a function of N.\nb. Give a lower bound on the size of the game tree for the worst case, where W = { }.\nc. Propose a plausible evaluation function that can be used for any instance of generalized\ntic-tac-toe. The function may depend on S and W.\nd. Assume that it is possible to generate a new board and check whether it is a winning\nposition in 100N machine instructions and assume a 2 gigahertz processor. Ignore\nmemory limitations. Using your estimate in (a), roughly how large a game tree can be\ncompletely solved by alpha–beta in a second of CPU time? a minute? an hour?\n5.11\nDevelop a general game-playing program, capable of playing a variety of games.\na. Implement move generators and evaluation functions for one or more of the following\ngames: Kalah, Othello, checkers, and chess.\nb. Construct a general alpha–beta game-playing agent.\nc. Compare the effect of increasing search depth, improving move ordering, and improv-\ning the evaluation function. How close does your effective branching factor come to the\nideal case of perfect move ordering?\nd. Implement a selective search algorithm, such as B* (Berliner, 1979), conspiracy number\nsearch (McAllester, 1988), or MGSS* (Russell and Wefald, 1989) and compare its\nperformance to A*. Exercises\n199\nn1\nn2\nnj\nFigure 5.18\nSituation when considering whether to prune node nj.\n5.12\nDescribe how the minimax and alpha–beta algorithms change for two-player, non-\nzero-sum games in which each player has a distinct utility function and both utility functions\nare known to both players. If there are no constraints on the two terminal utilities, is it possible\nfor any node to be pruned by alpha–beta? What if the player’s utility functions on any state\ndiffer by at most a constant k, making the game almost cooperative?\n5.13\nDevelop a formal proof of correctness for alpha–beta pruning. To do this, consider the\nsituation shown in Figure 5.18. The question is whether to prune node nj, which is a max-",
  "differ by at most a constant k, making the game almost cooperative?\n5.13\nDevelop a formal proof of correctness for alpha–beta pruning. To do this, consider the\nsituation shown in Figure 5.18. The question is whether to prune node nj, which is a max-\nnode and a descendant of node n1. The basic idea is to prune it if and only if the minimax\nvalue of n1 can be shown to be independent of the value of nj.\na. Mode n1 takes on the minimum value among its children: n1 = min(n2, n21, . . . , n2b2).\nFind a similar expression for n2 and hence an expression for n1 in terms of nj.\nb. Let li be the minimum (or maximum) value of the nodes to the left of node ni at depth i,\nwhose minimax value is already known. Similarly, let ri be the minimum (or maximum)\nvalue of the unexplored nodes to the right of ni at depth i. Rewrite your expression for\nn1 in terms of the li and ri values.\nc. Now reformulate the expression to show that in order to affect n1, nj must not exceed\na certain bound derived from the li values.\nd. Repeat the process for the case where nj is a min-node.\n5.14\nProve that alpha–beta pruning takes time O(2m/2) with optimal move ordering, where\nm is the maximum depth of the game tree.\n5.15\nSuppose you have a chess program that can evaluate 10 million nodes per second.\nDecide on a compact representation of a game state for storage in a transposition table. About\nhow many entries can you ﬁt in a 2-gigabyte in-memory table? Will that be enough for the 200\nChapter\n5.\nAdversarial Search\n0.5\n0.5\n0.5\n0.5\n2\n2\n1\n2\n0\n2\n-1\n0\nFigure 5.19\nThe complete game tree for a trivial game with chance nodes.\nthree minutes of search allocated for one move? How many table lookups can you do in the\ntime it would take to do one evaluation? Now suppose the transposition table is stored on\ndisk. About how many evaluations could you do in the time it takes to do one disk seek with\nstandard disk hardware?\n5.16\nThis question considers pruning in games with chance nodes. Figure 5.19 shows the\ncomplete game tree for a trivial game. Assume that the leaf nodes are to be evaluated in left-\nto-right order, and that before a leaf node is evaluated, we know nothing about its value—the\nrange of possible values is −∞to ∞.\na. Copy the ﬁgure, mark the value of all the internal nodes, and indicate the best move at\nthe root with an arrow.\nb. Given the values of the ﬁrst six leaves, do we need to evaluate the seventh and eighth",
  "range of possible values is −∞to ∞.\na. Copy the ﬁgure, mark the value of all the internal nodes, and indicate the best move at\nthe root with an arrow.\nb. Given the values of the ﬁrst six leaves, do we need to evaluate the seventh and eighth\nleaves? Given the values of the ﬁrst seven leaves, do we need to evaluate the eighth\nleaf? Explain your answers.\nc. Suppose the leaf node values are known to lie between –2 and 2 inclusive. After the\nﬁrst two leaves are evaluated, what is the value range for the left-hand chance node?\nd. Circle all the leaves that need not be evaluated under the assumption in (c).\n5.17\nImplement the expectiminimax algorithm and the *-alpha–beta algorithm, which is\ndescribed by Ballard (1983), for pruning game trees with chance nodes. Try them on a game\nsuch as backgammon and measure the pruning effectiveness of *-alpha–beta.\n5.18\nProve that with a positive linear transformation of leaf values (i.e., transforming a\nvalue x to ax + b where a > 0), the choice of move remains unchanged in a game tree, even\nwhen there are chance nodes.\n5.19\nConsider the following procedure for choosing moves in games with chance nodes:\n• Generate some dice-roll sequences (say, 50) down to a suitable depth (say, 8).\n• With known dice rolls, the game tree becomes deterministic. For each dice-roll se-\nquence, solve the resulting deterministic game tree using alpha–beta. Exercises\n201\n• Use the results to estimate the value of each move and to choose the best.\nWill this procedure work well? Why (or why not)?\n5.20\nIn the following, a “max” tree consists only of max nodes, whereas an “expectimax”\ntree consists of a max node at the root with alternating layers of chance and max nodes. At\nchance nodes, all outcome probabilities are nonzero. The goal is to ﬁnd the value of the root\nwith a bounded-depth search. For each of (a)–(f), either give an example or explain why this\nis impossible.\na. Assuming that leaf values are ﬁnite but unbounded, is pruning (as in alpha–beta) ever\npossible in a max tree?\nb. Is pruning ever possible in an expectimax tree under the same conditions?\nc. If leaf values are all nonnegative, is pruning ever possible in a max tree? Give an\nexample, or explain why not.\nd. If leaf values are all nonnegative, is pruning ever possible in an expectimax tree? Give\nan example, or explain why not.\ne. If leaf values are all in the range [0, 1], is pruning ever possible in a max tree? Give an\nexample, or explain why not.",
  "d. If leaf values are all nonnegative, is pruning ever possible in an expectimax tree? Give\nan example, or explain why not.\ne. If leaf values are all in the range [0, 1], is pruning ever possible in a max tree? Give an\nexample, or explain why not.\nf. If leaf values are all in the range [0, 1], is pruning ever possible in an expectimax tree?\ng. Consider the outcomes of a chance node in an expectimax tree. Which of the following\nevaluation orders is most likely to yield pruning opportunities?\n(i) Lowest probability ﬁrst\n(ii) Highest probability ﬁrst\n(iii) Doesn’t make any difference\n5.21\nWhich of the following are true and which are false? Give brief explanations.\na. In a fully observable, turn-taking, zero-sum game between two perfectly rational play-\ners, it does not help the ﬁrst player to know what strategy the second player is using—\nthat is, what move the second player will make, given the ﬁrst player’s move.\nb. In a partially observable, turn-taking, zero-sum game between two perfectly rational\nplayers, it does not help the ﬁrst player to know what move the second player will\nmake, given the ﬁrst player’s move.\nc. A perfectly rational backgammon agent never loses.\n5.22\nConsider carefully the interplay of chance events and partial information in each of the\ngames in Exercise 5.4.\na. For which is the standard expectiminimax model appropriate? Implement the algorithm\nand run it in your game-playing agent, with appropriate modiﬁcations to the game-\nplaying environment.\nb. For which would the scheme described in Exercise 5.19 be appropriate?\nc. Discuss how you might deal with the fact that in some of the games, the players do not\nhave the same knowledge of the current state. 6\nCONSTRAINT\nSATISFACTION PROBLEMS\nIn which we see how treating states as more than just little black boxes leads to the\ninvention of a range of powerful new search methods and a deeper understanding\nof problem structure and complexity.\nChapters 3 and 4 explored the idea that problems can be solved by searching in a space of\nstates. These states can be evaluated by domain-speciﬁc heuristics and tested to see whether\nthey are goal states. From the point of view of the search algorithm, however, each state is\natomic, or indivisible—a black box with no internal structure.\nThis chapter describes a way to solve a wide variety of problems more efﬁciently. We\nuse a factored representation for each state: a set of variables, each of which has a value.",
  "atomic, or indivisible—a black box with no internal structure.\nThis chapter describes a way to solve a wide variety of problems more efﬁciently. We\nuse a factored representation for each state: a set of variables, each of which has a value.\nA problem is solved when each variable has a value that satisﬁes all the constraints on the\nvariable. A problem described this way is called a constraint satisfaction problem, or CSP.\nCONSTRAINT\nSATISFACTION\nPROBLEM\nCSP search algorithms take advantage of the structure of states and use general-purpose\nrather than problem-speciﬁc heuristics to enable the solution of complex problems. The main\nidea is to eliminate large portions of the search space all at once by identifying variable/value\ncombinations that violate the constraints.\n6.1\nDEFINING CONSTRAINT SATISFACTION PROBLEMS\nA constraint satisfaction problem consists of three components, X, D, and C:\nX is a set of variables, {X1, . . . , Xn}.\nD is a set of domains, {D1, . . . , Dn}, one for each variable.\nC is a set of constraints that specify allowable combinations of values.\nEach domain Di consists of a set of allowable values, {v1, . . . , vk} for variable Xi. Each\nconstraint Ci consists of a pair ⟨scope, rel⟩, where scope is a tuple of variables that participate\nin the constraint and rel is a relation that deﬁnes the values that those variables can take on. A\nrelation can be represented as an explicit list of all tuples of values that satisfy the constraint,\nor as an abstract relation that supports two operations: testing if a tuple is a member of the\nrelation and enumerating the members of the relation. For example, if X1 and X2 both have\n202 Section 6.1.\nDeﬁning Constraint Satisfaction Problems\n203\nthe domain {A,B}, then the constraint saying the two variables must have different values\ncan be written as ⟨(X1, X2), [(A, B), (B, A)]⟩or as ⟨(X1, X2), X1 ̸= X2⟩.\nTo solve a CSP, we need to deﬁne a state space and the notion of a solution. Each\nstate in a CSP is deﬁned by an assignment of values to some or all of the variables, {Xi =\nASSIGNMENT\nvi, Xj = vj, . . .}. An assignment that does not violate any constraints is called a consistent\nCONSISTENT\nor legal assignment. A complete assignment is one in which every variable is assigned, and\nCOMPLETE\nASSIGNMENT\na solution to a CSP is a consistent, complete assignment. A partial assignment is one that\nSOLUTION\nPARTIAL\nASSIGNMENT\nassigns values to only some of the variables.\n6.1.1\nExample problem: Map coloring",
  "COMPLETE\nASSIGNMENT\na solution to a CSP is a consistent, complete assignment. A partial assignment is one that\nSOLUTION\nPARTIAL\nASSIGNMENT\nassigns values to only some of the variables.\n6.1.1\nExample problem: Map coloring\nSuppose that, having tired of Romania, we are looking at a map of Australia showing each\nof its states and territories (Figure 6.1(a)). We are given the task of coloring each region\neither red, green, or blue in such a way that no neighboring regions have the same color. To\nformulate this as a CSP, we deﬁne the variables to be the regions\nX = {WA, NT, Q, NSW , V, SA, T} .\nThe domain of each variable is the set Di = {red, green, blue}. The constraints require\nneighboring regions to have distinct colors. Since there are nine places where regions border,\nthere are nine constraints:\nC = {SA ̸= WA, SA ̸= NT, SA ̸= Q, SA ̸= NSW , SA ̸= V,\nWA ̸= NT, NT ̸= Q, Q ̸= NSW , NSW ̸= V } .\nHere we are using abbreviations; SA ̸= WA is a shortcut for ⟨(SA, WA), SA ̸= WA⟩, where\nSA ̸= WA can be fully enumerated in turn as\n{(red, green), (red, blue), (green, red), (green, blue), (blue, red), (blue, green)} .\nThere are many possible solutions to this problem, such as\n{WA = red, NT = green, Q = red, NSW = green, V = red, SA = blue, T = red }.\nIt can be helpful to visualize a CSP as a constraint graph, as shown in Figure 6.1(b). The\nCONSTRAINT GRAPH\nnodes of the graph correspond to variables of the problem, and a link connects any two vari-\nables that participate in a constraint.\nWhy formulate a problem as a CSP? One reason is that the CSPs yield a natural rep-\nresentation for a wide variety of problems; if you already have a CSP-solving system, it is\noften easier to solve a problem using it than to design a custom solution using another search\ntechnique. In addition, CSP solvers can be faster than state-space searchers because the CSP\nsolver can quickly eliminate large swatches of the search space. For example, once we have\nchosen {SA = blue} in the Australia problem, we can conclude that none of the ﬁve neighbor-\ning variables can take on the value blue. Without taking advantage of constraint propagation,\na search procedure would have to consider 35 = 243 assignments for the ﬁve neighboring\nvariables; with constraint propagation we never have to consider blue as a value, so we have\nonly 25 = 32 assignments to look at, a reduction of 87%.\nIn regular state-space search we can only ask: is this speciﬁc state a goal? No? What",
  "variables; with constraint propagation we never have to consider blue as a value, so we have\nonly 25 = 32 assignments to look at, a reduction of 87%.\nIn regular state-space search we can only ask: is this speciﬁc state a goal? No? What\nabout this one? With CSPs, once we ﬁnd out that a partial assignment is not a solution, we can 204\nChapter\n6.\nConstraint Satisfaction Problems\nWestern\nAustralia\nNorthern\nTerritory\nSouth\nAustralia\nQueensland\nNew\nSouth\nWales\nVictoria\nTasmania\nWA\nNT\nSA\nQ\nNSW\nV\nT\n(a)\n(b)\nFigure 6.1\n(a) The principal states and territories of Australia. Coloring this map can\nbe viewed as a constraint satisfaction problem (CSP). The goal is to assign colors to each\nregion so that no neighboring regions have the same color. (b) The map-coloring problem\nrepresented as a constraint graph.\nimmediately discard further reﬁnements of the partial assignment. Furthermore, we can see\nwhy the assignment is not a solution—we see which variables violate a constraint—so we can\nfocus attention on the variables that matter. As a result, many problems that are intractable\nfor regular state-space search can be solved quickly when formulated as a CSP.\n6.1.2\nExample problem: Job-shop scheduling\nFactories have the problem of scheduling a day’s worth of jobs, subject to various constraints.\nIn practice, many of these problems are solved with CSP techniques. Consider the problem of\nscheduling the assembly of a car. The whole job is composed of tasks, and we can model each\ntask as a variable, where the value of each variable is the time that the task starts, expressed\nas an integer number of minutes. Constraints can assert that one task must occur before\nanother—for example, a wheel must be installed before the hubcap is put on—and that only\nso many tasks can go on at once. Constraints can also specify that a task takes a certain\namount of time to complete.\nWe consider a small part of the car assembly, consisting of 15 tasks: install axles (front\nand back), afﬁx all four wheels (right and left, front and back), tighten nuts for each wheel,\nafﬁx hubcaps, and inspect the ﬁnal assembly. We can represent the tasks with 15 variables:\nX = {AxleF , AxleB, Wheel RF , WheelLF , WheelRB, WheelLB, NutsRF ,\nNutsLF , NutsRB, NutsLB, CapRF , CapLF , CapRB, CapLB, Inspect} .\nThe value of each variable is the time that the task starts. Next we represent precedence\nconstraints between individual tasks. Whenever a task T1 must occur before task T2, and\nPRECEDENCE\nCONSTRAINTS",
  "The value of each variable is the time that the task starts. Next we represent precedence\nconstraints between individual tasks. Whenever a task T1 must occur before task T2, and\nPRECEDENCE\nCONSTRAINTS\ntask T1 takes duration d1 to complete, we add an arithmetic constraint of the form\nT1 + d1 ≤T2 . Section 6.1.\nDeﬁning Constraint Satisfaction Problems\n205\nIn our example, the axles have to be in place before the wheels are put on, and it takes 10\nminutes to install an axle, so we write\nAxleF + 10 ≤WheelRF ;\nAxleF + 10 ≤WheelLF ;\nAxleB + 10 ≤WheelRB;\nAxleB + 10 ≤WheelLB .\nNext we say that, for each wheel, we must afﬁx the wheel (which takes 1 minute), then tighten\nthe nuts (2 minutes), and ﬁnally attach the hubcap (1 minute, but not represented yet):\nWheelRF + 1 ≤NutsRF ;\nNutsRF + 2 ≤CapRF ;\nWheelLF + 1 ≤NutsLF;\nNutsLF + 2 ≤CapLF;\nWheelRB + 1 ≤NutsRB;\nNutsRB + 2 ≤CapRB;\nWheelLB + 1 ≤NutsLB;\nNutsLB + 2 ≤CapLB .\nSuppose we have four workers to install wheels, but they have to share one tool that helps put\nthe axle in place. We need a disjunctive constraint to say that AxleF and AxleB must not\nDISJUNCTIVE\nCONSTRAINT\noverlap in time; either one comes ﬁrst or the other does:\n(AxleF + 10 ≤AxleB)\nor\n(AxleB + 10 ≤AxleF) .\nThis looks like a more complicated constraint, combining arithmetic and logic. But it still\nreduces to a set of pairs of values that AxleF and AxleF can take on.\nWe also need to assert that the inspection comes last and takes 3 minutes. For every\nvariable except Inspect we add a constraint of the form X + dX ≤Inspect. Finally, suppose\nthere is a requirement to get the whole assembly done in 30 minutes. We can achieve that by\nlimiting the domain of all variables:\nDi = {1, 2, 3, . . . , 27} .\nThis particular problem is trivial to solve, but CSPs have been applied to job-shop schedul-\ning problems like this with thousands of variables. In some cases, there are complicated\nconstraints that are difﬁcult to specify in the CSP formalism, and more advanced planning\ntechniques are used, as discussed in Chapter 11.\n6.1.3\nVariations on the CSP formalism\nThe simplest kind of CSP involves variables that have discrete, ﬁnite domains.\nMap-\nDISCRETE DOMAIN\nFINITE DOMAIN\ncoloring problems and scheduling with time limits are both of this kind. The 8-queens prob-\nlem described in Chapter 3 can also be viewed as a ﬁnite-domain CSP, where the variables\nQ1, . . . , Q8 are the positions of each queen in columns 1, . . . , 8 and each variable has the",
  "lem described in Chapter 3 can also be viewed as a ﬁnite-domain CSP, where the variables\nQ1, . . . , Q8 are the positions of each queen in columns 1, . . . , 8 and each variable has the\ndomain Di = {1, 2, 3, 4, 5, 6, 7, 8}.\nA discrete domain can be inﬁnite, such as the set of integers or strings. (If we didn’t put\nINFINITE\na deadline on the job-scheduling problem, there would be an inﬁnite number of start times\nfor each variable.) With inﬁnite domains, it is no longer possible to describe constraints by\nenumerating all allowed combinations of values. Instead, a constraint language must be\nCONSTRAINT\nLANGUAGE\nused that understands constraints such as T1 + d1 ≤T2 directly, without enumerating the\nset of pairs of allowable values for (T1, T2). Special solution algorithms (which we do not\ndiscuss here) exist for linear constraints on integer variables—that is, constraints, such as\nLINEAR\nCONSTRAINTS\nthe one just given, in which each variable appears only in linear form. It can be shown that\nno algorithm exists for solving general nonlinear constraints on integer variables.\nNONLINEAR\nCONSTRAINTS 206\nChapter\n6.\nConstraint Satisfaction Problems\nConstraint satisfaction problems with continuous domains are common in the real\nCONTINUOUS\nDOMAINS\nworld and are widely studied in the ﬁeld of operations research. For example, the scheduling\nof experiments on the Hubble Space Telescope requires very precise timing of observations;\nthe start and ﬁnish of each observation and maneuver are continuous-valued variables that\nmust obey a variety of astronomical, precedence, and power constraints. The best-known\ncategory of continuous-domain CSPs is that of linear programming problems, where con-\nstraints must be linear equalities or inequalities. Linear programming problems can be solved\nin time polynomial in the number of variables. Problems with different types of constraints\nand objective functions have also been studied—quadratic programming, second-order conic\nprogramming, and so on.\nIn addition to examining the types of variables that can appear in CSPs, it is useful to\nlook at the types of constraints. The simplest type is the unary constraint, which restricts\nUNARY CONSTRAINT\nthe value of a single variable. For example, in the map-coloring problem it could be the case\nthat South Australians won’t tolerate the color green; we can express that with the unary\nconstraint ⟨(SA), SA ̸= green⟩\nA binary constraint relates two variables. For example, SA ̸= NSW is a binary",
  "that South Australians won’t tolerate the color green; we can express that with the unary\nconstraint ⟨(SA), SA ̸= green⟩\nA binary constraint relates two variables. For example, SA ̸= NSW is a binary\nBINARY CONSTRAINT\nconstraint. A binary CSP is one with only binary constraints; it can be represented as a\nconstraint graph, as in Figure 6.1(b).\nWe can also describe higher-order constraints, such as asserting that the value of Y is\nbetween X and Z, with the ternary constraint Between(X, Y, Z).\nA constraint involving an arbitrary number of variables is called a global constraint.\nGLOBAL\nCONSTRAINT\n(The name is traditional but confusing because it need not involve all the variables in a prob-\nlem). One of the most common global constraints is Alldiﬀ, which says that all of the\nvariables involved in the constraint must have different values. In Sudoku problems (see\nSection 6.2.6), all variables in a row or column must satisfy an Alldiﬀconstraint.\nAn-\nother example is provided by cryptarithmetic puzzles. (See Figure 6.2(a).) Each letter in a\nCRYPTARITHMETIC\ncryptarithmetic puzzle represents a different digit. For the case in Figure 6.2(a), this would\nbe represented as the global constraint Alldiﬀ(F, T, U, W, R, O). The addition constraints\non the four columns of the puzzle can be written as the following n-ary constraints:\nO + O = R + 10 · C10\nC10 + W + W = U + 10 · C100\nC100 + T + T = O + 10 · C1000\nC1000 = F ,\nwhere C10, C100, and C1000 are auxiliary variables representing the digit carried over into the\ntens, hundreds, or thousands column. These constraints can be represented in a constraint\nhypergraph, such as the one shown in Figure 6.2(b). A hypergraph consists of ordinary nodes\nCONSTRAINT\nHYPERGRAPH\n(the circles in the ﬁgure) and hypernodes (the squares), which represent n-ary constraints.\nAlternatively, as Exercise 6.6 asks you to prove, every ﬁnite-domain constraint can be\nreduced to a set of binary constraints if enough auxiliary variables are introduced, so we could\ntransform any CSP into one with only binary constraints; this makes the algorithms simpler.\nAnother way to convert an n-ary CSP to a binary one is the dual graph transformation: create\nDUAL GRAPH\na new graph in which there will be one variable for each constraint in the original graph, and Section 6.1.\nDeﬁning Constraint Satisfaction Problems\n207\n(a)\nO\nW\nT\nF\nU\nR\n(b)\n+\nF\nT\nT\nO\nW\nW\nU\nO\nO\nR\nC3\nC1\nC2\nFigure 6.2\n(a) A cryptarithmetic problem. Each letter stands for a distinct digit; the aim is",
  "Deﬁning Constraint Satisfaction Problems\n207\n(a)\nO\nW\nT\nF\nU\nR\n(b)\n+\nF\nT\nT\nO\nW\nW\nU\nO\nO\nR\nC3\nC1\nC2\nFigure 6.2\n(a) A cryptarithmetic problem. Each letter stands for a distinct digit; the aim is\nto ﬁnd a substitution of digits for letters such that the resulting sum is arithmetically correct,\nwith the added restriction that no leading zeroes are allowed. (b) The constraint hypergraph\nfor the cryptarithmetic problem, showing the Alldiﬀconstraint (square box at the top) as\nwell as the column addition constraints (four square boxes in the middle). The variables C1,\nC2, and C3 represent the carry digits for the three columns.\none binary constraint for each pair of constraints in the original graph that share variables. For\nexample, if the original graph has variables {X, Y, Z} and constraints ⟨(X, Y, Z), C1⟩and\n⟨(X, Y ), C2⟩then the dual graph would have variables {C1, C2} with the binary constraint\n⟨(X, Y ), R1⟩, where (X, Y ) are the shared variables and R1 is a new relation that deﬁnes the\nconstraint between the shared variables, as speciﬁed by the original C1 and C2.\nThere are however two reasons why we might prefer a global constraint such as Alldiﬀ\nrather than a set of binary constraints. First, it is easier and less error-prone to write the\nproblem description using Alldiﬀ. Second, it is possible to design special-purpose inference\nalgorithms for global constraints that are not available for a set of more primitive constraints.\nWe describe these inference algorithms in Section 6.2.5.\nThe constraints we have described so far have all been absolute constraints, violation of\nwhich rules out a potential solution. Many real-world CSPs include preference constraints\nPREFERENCE\nCONSTRAINTS\nindicating which solutions are preferred. For example, in a university class-scheduling prob-\nlem there are absolute constraints that no professor can teach two classes at the same time.\nBut we also may allow preference constraints: Prof. R might prefer teaching in the morning,\nwhereas Prof. N prefers teaching in the afternoon. A schedule that has Prof. R teaching at\n2 p.m. would still be an allowable solution (unless Prof. R happens to be the department chair)\nbut would not be an optimal one. Preference constraints can often be encoded as costs on in-\ndividual variable assignments—for example, assigning an afternoon slot for Prof. R costs\n2 points against the overall objective function, whereas a morning slot costs 1. With this",
  "dividual variable assignments—for example, assigning an afternoon slot for Prof. R costs\n2 points against the overall objective function, whereas a morning slot costs 1. With this\nformulation, CSPs with preferences can be solved with optimization search methods, either\npath-based or local. We call such a problem a constraint optimization problem, or COP.\nCONSTRAINT\nOPTIMIZATION\nPROBLEM\nLinear programming problems do this kind of optimization. 208\nChapter\n6.\nConstraint Satisfaction Problems\n6.2\nCONSTRAINT PROPAGATION: INFERENCE IN CSPS\nIn regular state-space search, an algorithm can do only one thing: search. In CSPs there is a\nchoice: an algorithm can search (choose a new variable assignment from several possibilities)\nor do a speciﬁc type of inference called constraint propagation: using the constraints to\nINFERENCE\nCONSTRAINT\nPROPAGATION\nreduce the number of legal values for a variable, which in turn can reduce the legal values\nfor another variable, and so on. Constraint propagation may be intertwined with search, or it\nmay be done as a preprocessing step, before search starts. Sometimes this preprocessing can\nsolve the whole problem, so no search is required at all.\nThe key idea is local consistency. If we treat each variable as a node in a graph (see\nLOCAL\nCONSISTENCY\nFigure 6.1(b)) and each binary constraint as an arc, then the process of enforcing local con-\nsistency in each part of the graph causes inconsistent values to be eliminated throughout the\ngraph. There are different types of local consistency, which we now cover in turn.\n6.2.1\nNode consistency\nA single variable (corresponding to a node in the CSP network) is node-consistent if all\nNODE CONSISTENCY\nthe values in the variable’s domain satisfy the variable’s unary constraints. For example,\nin the variant of the Australia map-coloring problem (Figure 6.1) where South Australians\ndislike green, the variable SA starts with domain {red, green, blue}, and we can make it\nnode consistent by eliminating green, leaving SA with the reduced domain {red, blue}. We\nsay that a network is node-consistent if every variable in the network is node-consistent.\nIt is always possible to eliminate all the unary constraints in a CSP by running node\nconsistency. It is also possible to transform all n-ary constraints into binary ones (see Ex-\nercise 6.6). Because of this, it is common to deﬁne CSP solvers that work with only binary",
  "consistency. It is also possible to transform all n-ary constraints into binary ones (see Ex-\nercise 6.6). Because of this, it is common to deﬁne CSP solvers that work with only binary\nconstraints; we make that assumption for the rest of this chapter, except where noted.\n6.2.2\nArc consistency\nA variable in a CSP is arc-consistent if every value in its domain satisﬁes the variable’s\nARC CONSISTENCY\nbinary constraints. More formally, Xi is arc-consistent with respect to another variable Xj if\nfor every value in the current domain Di there is some value in the domain Dj that satisﬁes\nthe binary constraint on the arc (Xi, Xj). A network is arc-consistent if every variable is arc\nconsistent with every other variable. For example, consider the constraint Y = X2 where the\ndomain of both X and Y is the set of digits. We can write this constraint explicitly as\n⟨(X, Y ), {(0, 0), (1, 1), (2, 4), (3, 9))}⟩.\nTo make X arc-consistent with respect to Y , we reduce X’s domain to {0, 1, 2, 3}. If we\nalso make Y arc-consistent with respect to X, then Y ’s domain becomes {0, 1, 4, 9} and the\nwhole CSP is arc-consistent.\nOn the other hand, arc consistency can do nothing for the Australia map-coloring prob-\nlem. Consider the following inequality constraint on (SA, WA):\n{(red, green), (red, blue), (green, red), (green, blue), (blue, red), (blue, green)} . Section 6.2.\nConstraint Propagation: Inference in CSPs\n209\nfunction AC-3(csp) returns false if an inconsistency is found and true otherwise\ninputs: csp, a binary CSP with components (X, D, C)\nlocal variables: queue, a queue of arcs, initially all the arcs in csp\nwhile queue is not empty do\n(Xi, Xj) ←REMOVE-FIRST(queue)\nif REVISE(csp, Xi, Xj) then\nif size of Di = 0 then return false\nfor each Xk in Xi.NEIGHBORS - {Xj} do\nadd (Xk, Xi) to queue\nreturn true\nfunction REVISE(csp, Xi, Xj) returns true iff we revise the domain of Xi\nrevised ←false\nfor each x in Di do\nif no value y in Dj allows (x,y) to satisfy the constraint between Xi and Xj then\ndelete x from Di\nrevised ←true\nreturn revised\nFigure 6.3\nThe arc-consistency algorithm AC-3. After applying AC-3, either every arc\nis arc-consistent, or some variable has an empty domain, indicating that the CSP cannot be\nsolved. The name “AC-3” was used by the algorithm’s inventor (Mackworth, 1977) because\nit’s the third version developed in the paper.\nNo matter what value you choose for SA (or for WA), there is a valid value for the other",
  "solved. The name “AC-3” was used by the algorithm’s inventor (Mackworth, 1977) because\nit’s the third version developed in the paper.\nNo matter what value you choose for SA (or for WA), there is a valid value for the other\nvariable. So applying arc consistency has no effect on the domains of either variable.\nThe most popular algorithm for arc consistency is called AC-3 (see Figure 6.3). To\nmake every variable arc-consistent, the AC-3 algorithm maintains a queue of arcs to consider.\n(Actually, the order of consideration is not important, so the data structure is really a set, but\ntradition calls it a queue.) Initially, the queue contains all the arcs in the CSP. AC-3 then pops\noff an arbitrary arc (Xi, Xj) from the queue and makes Xi arc-consistent with respect to Xj.\nIf this leaves Di unchanged, the algorithm just moves on to the next arc. But if this revises\nDi (makes the domain smaller), then we add to the queue all arcs (Xk, Xi) where Xk is a\nneighbor of Xi. We need to do that because the change in Di might enable further reductions\nin the domains of Dk, even if we have previously considered Xk. If Di is revised down to\nnothing, then we know the whole CSP has no consistent solution, and AC-3 can immediately\nreturn failure. Otherwise, we keep checking, trying to remove values from the domains of\nvariables until no more arcs are in the queue. At that point, we are left with a CSP that is\nequivalent to the original CSP—they both have the same solutions—but the arc-consistent\nCSP will in most cases be faster to search because its variables have smaller domains.\nThe complexity of AC-3 can be analyzed as follows. Assume a CSP with n variables,\neach with domain size at most d, and with c binary constraints (arcs). Each arc (Xk, Xi) can\nbe inserted in the queue only d times because Xi has at most d values to delete. Checking 210\nChapter\n6.\nConstraint Satisfaction Problems\nconsistency of an arc can be done in O(d2) time, so we get O(cd3) total worst-case time.1\nIt is possible to extend the notion of arc consistency to handle n-ary rather than just\nbinary constraints; this is called generalized arc consistency or sometimes hyperarc consis-\ntency, depending on the author. A variable Xi is generalized arc consistent with respect to\nGENERALIZED ARC\nCONSISTENT\nan n-ary constraint if for every value v in the domain of Xi there exists a tuple of values that\nis a member of the constraint, has all its values taken from the domains of the corresponding",
  "GENERALIZED ARC\nCONSISTENT\nan n-ary constraint if for every value v in the domain of Xi there exists a tuple of values that\nis a member of the constraint, has all its values taken from the domains of the corresponding\nvariables, and has its Xi component equal to v. For example, if all variables have the do-\nmain {0, 1, 2, 3}, then to make the variable X consistent with the constraint X < Y < Z,\nwe would have to eliminate 2 and 3 from the domain of X because the constraint cannot be\nsatisﬁed when X is 2 or 3.\n6.2.3\nPath consistency\nArc consistency can go a long way toward reducing the domains of variables, sometimes\nﬁnding a solution (by reducing every domain to size 1) and sometimes ﬁnding that the CSP\ncannot be solved (by reducing some domain to size 0). But for other networks, arc consistency\nfails to make enough inferences. Consider the map-coloring problem on Australia, but with\nonly two colors allowed, red and blue. Arc consistency can do nothing because every variable\nis already arc consistent: each can be red with blue at the other end of the arc (or vice versa).\nBut clearly there is no solution to the problem: because Western Australia, Northern Territory\nand South Australia all touch each other, we need at least three colors for them alone.\nArc consistency tightens down the domains (unary constraints) using the arcs (binary\nconstraints). To make progress on problems like map coloring, we need a stronger notion of\nconsistency. Path consistency tightens the binary constraints by using implicit constraints\nPATH CONSISTENCY\nthat are inferred by looking at triples of variables.\nA two-variable set {Xi, Xj} is path-consistent with respect to a third variable Xm if,\nfor every assignment {Xi = a, Xj = b} consistent with the constraints on {Xi, Xj}, there is\nan assignment to Xm that satisﬁes the constraints on {Xi, Xm} and {Xm, Xj}. This is called\npath consistency because one can think of it as looking at a path from Xi to Xj with Xm in\nthe middle.\nLet’s see how path consistency fares in coloring the Australia map with two colors. We\nwill make the set {WA, SA} path consistent with respect to NT. We start by enumerating the\nconsistent assignments to the set. In this case, there are only two: {WA = red, SA = blue}\nand {WA = blue, SA = red}. We can see that with both of these assignments NT can be\nneither red nor blue (because it would conﬂict with either WA or SA). Because there is no",
  "and {WA = blue, SA = red}. We can see that with both of these assignments NT can be\nneither red nor blue (because it would conﬂict with either WA or SA). Because there is no\nvalid choice for NT, we eliminate both assignments, and we end up with no valid assignments\nfor {WA, SA}. Therefore, we know that there can be no solution to this problem. The PC-2\nalgorithm (Mackworth, 1977) achieves path consistency in much the same way that AC-3\nachieves arc consistency. Because it is so similar, we do not show it here.\n1 The AC-4 algorithm (Mohr and Henderson, 1986) runs in O(cd2) worst-case time but can be slower than AC-3\non average cases. See Exercise 6.13. Section 6.2.\nConstraint Propagation: Inference in CSPs\n211\n6.2.4\nK-consistency\nStronger forms of propagation can be deﬁned with the notion of k-consistency. A CSP is\nK-CONSISTENCY\nk-consistent if, for any set of k −1 variables and for any consistent assignment to those\nvariables, a consistent value can always be assigned to any kth variable. 1-consistency says\nthat, given the empty set, we can make any set of one variable consistent: this is what we\ncalled node consistency. 2-consistency is the same as arc consistency. For binary constraint\nnetworks, 3-consistency is the same as path consistency.\nA CSP is strongly k-consistent if it is k-consistent and is also (k −1)-consistent,\nSTRONGLY\nK-CONSISTENT\n(k −2)-consistent, . . . all the way down to 1-consistent. Now suppose we have a CSP with\nn nodes and make it strongly n-consistent (i.e., strongly k-consistent for k = n). We can\nthen solve the problem as follows: First, we choose a consistent value for X1. We are then\nguaranteed to be able to choose a value for X2 because the graph is 2-consistent, for X3\nbecause it is 3-consistent, and so on. For each variable Xi, we need only search through the d\nvalues in the domain to ﬁnd a value consistent with X1, . . . , Xi−1. We are guaranteed to ﬁnd\na solution in time O(n2d). Of course, there is no free lunch: any algorithm for establishing\nn-consistency must take time exponential in n in the worst case. Worse, n-consistency also\nrequires space that is exponential in n. The memory issue is even more severe than the time.\nIn practice, determining the appropriate level of consistency checking is mostly an empirical\nscience. It can be said practitioners commonly compute 2-consistency and less commonly\n3-consistency.\n6.2.5\nGlobal constraints",
  "In practice, determining the appropriate level of consistency checking is mostly an empirical\nscience. It can be said practitioners commonly compute 2-consistency and less commonly\n3-consistency.\n6.2.5\nGlobal constraints\nRemember that a global constraint is one involving an arbitrary number of variables (but not\nnecessarily all variables). Global constraints occur frequently in real problems and can be\nhandled by special-purpose algorithms that are more efﬁcient than the general-purpose meth-\nods described so far. For example, the Alldiﬀconstraint says that all the variables involved\nmust have distinct values (as in the cryptarithmetic problem above and Sudoku puzzles be-\nlow). One simple form of inconsistency detection for Alldiﬀconstraints works as follows:\nif m variables are involved in the constraint, and if they have n possible distinct values alto-\ngether, and m > n, then the constraint cannot be satisﬁed.\nThis leads to the following simple algorithm: First, remove any variable in the con-\nstraint that has a singleton domain, and delete that variable’s value from the domains of the\nremaining variables. Repeat as long as there are singleton variables. If at any point an empty\ndomain is produced or there are more variables than domain values left, then an inconsistency\nhas been detected.\nThis method can detect the inconsistency in the assignment {WA = red, NSW = red}\nfor Figure 6.1. Notice that the variables SA, NT, and Q are effectively connected by an\nAlldiﬀconstraint because each pair must have two different colors. After applying AC-3\nwith the partial assignment, the domain of each variable is reduced to {green, blue}. That\nis, we have three variables and only two colors, so the Alldiﬀconstraint is violated. Thus,\na simple consistency procedure for a higher-order constraint is sometimes more effective\nthan applying arc consistency to an equivalent set of binary constraints. There are more 212\nChapter\n6.\nConstraint Satisfaction Problems\ncomplex inference algorithms for Alldiﬀ(see van Hoeve and Katriel, 2006) that propagate\nmore constraints but are more computationally expensive to run.\nAnother important higher-order constraint is the resource constraint, sometimes called\nRESOURCE\nCONSTRAINT\nthe atmost constraint. For example, in a scheduling problem, let P1, . . . , P4 denote the\nnumbers of personnel assigned to each of four tasks. The constraint that no more than 10",
  "RESOURCE\nCONSTRAINT\nthe atmost constraint. For example, in a scheduling problem, let P1, . . . , P4 denote the\nnumbers of personnel assigned to each of four tasks. The constraint that no more than 10\npersonnel are assigned in total is written as Atmost(10, P1, P2, P3, P4). We can detect an\ninconsistency simply by checking the sum of the minimum values of the current domains;\nfor example, if each variable has the domain {3, 4, 5, 6}, the Atmost constraint cannot be\nsatisﬁed. We can also enforce consistency by deleting the maximum value of any domain if it\nis not consistent with the minimum values of the other domains. Thus, if each variable in our\nexample has the domain {2, 3, 4, 5, 6}, the values 5 and 6 can be deleted from each domain.\nFor large resource-limited problems with integer values—such as logistical problems\ninvolving moving thousands of people in hundreds of vehicles—it is usually not possible to\nrepresent the domain of each variable as a large set of integers and gradually reduce that set by\nconsistency-checking methods. Instead, domains are represented by upper and lower bounds\nand are managed by bounds propagation. For example, in an airline-scheduling problem,\nBOUNDS\nPROPAGATION\nlet’s suppose there are two ﬂights, F1 and F2, for which the planes have capacities 165 and\n385, respectively. The initial domains for the numbers of passengers on each ﬂight are then\nD1 = [0, 165]\nand\nD2 = [0, 385] .\nNow suppose we have the additional constraint that the two ﬂights together must carry 420\npeople: F1 + F2 = 420. Propagating bounds constraints, we reduce the domains to\nD1 = [35, 165]\nand\nD2 = [255, 385] .\nWe say that a CSP is bounds consistent if for every variable X, and for both the lower-\nBOUNDS\nCONSISTENT\nbound and upper-bound values of X, there exists some value of Y that satisﬁes the constraint\nbetween X and Y for every variable Y . This kind of bounds propagation is widely used in\npractical constraint problems.\n6.2.6\nSudoku example\nThe popular Sudoku puzzle has introduced millions of people to constraint satisfaction prob-\nSUDOKU\nlems, although they may not recognize it. A Sudoku board consists of 81 squares, some of\nwhich are initially ﬁlled with digits from 1 to 9. The puzzle is to ﬁll in all the remaining\nsquares such that no digit appears twice in any row, column, or 3 × 3 box (see Figure 6.4). A\nrow, column, or box is called a unit.\nThe Sudoku puzzles that are printed in newspapers and puzzle books have the property",
  "squares such that no digit appears twice in any row, column, or 3 × 3 box (see Figure 6.4). A\nrow, column, or box is called a unit.\nThe Sudoku puzzles that are printed in newspapers and puzzle books have the property\nthat there is exactly one solution. Although some can be tricky to solve by hand, taking tens\nof minutes, even the hardest Sudoku problems yield to a CSP solver in less than 0.1 second.\nA Sudoku puzzle can be considered a CSP with 81 variables, one for each square. We\nuse the variable names A1 through A9 for the top row (left to right), down to I1 through I9\nfor the bottom row. The empty squares have the domain {1, 2, 3, 4, 5, 6, 7, 8, 9} and the pre-\nﬁlled squares have a domain consisting of a single value. In addition, there are 27 different Section 6.2.\nConstraint Propagation: Inference in CSPs\n213\n3\n2\n6\n9\n3\n5\n1\n1 8\n6 4\n8 1\n2 9\n7\n8\n6 7\n8 2\n2 6\n9 5\n8\n2\n3\n9\n5\n1\n3\n3\n2\n6\n9\n3\n5\n1\n1 8\n6 4\n8 1\n2 9\n7\n8\n6 7\n8 2\n2 6\n9 5\n8\n2\n3\n9\n5\n1\n3\n4 8\n9\n1\n5 7\n6 7\n4\n8 2\n2 5\n7\n9 3\n5 4\n3\n7 6\n2 9 5 6 4 1 3\n1 3\n9\n4 5\n3 7\n8\n1 4\n1 4\n5\n7 6\n6 9\n4\n7\n8 2\n1\n2\n3\n4\n5\n6\n7\n8\n9\nA\nB\nC\nD\nE\nF\nG\nH\n I\nA\nB\nC\nD\nE\nF\nG\nH\n I\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(a)\n(b)\nFigure 6.4\n(a) A Sudoku puzzle and (b) its solution.\nAlldiﬀconstraints: one for each row, column, and box of 9 squares.\nAlldiﬀ(A1, A2, A3, A4, A5, A6, A7, A8, A9)\nAlldiﬀ(B1, B2, B3, B4, B5, B6, B7, B8, B9)\n· · ·\nAlldiﬀ(A1, B1, C1, D1, E1, F1, G1, H1, I1)\nAlldiﬀ(A2, B2, C2, D2, E2, F2, G2, H2, I2)\n· · ·\nAlldiﬀ(A1, A2, A3, B1, B2, B3, C1, C2, C3)\nAlldiﬀ(A4, A5, A6, B4, B5, B6, C4, C5, C6)\n· · ·\nLet us see how far arc consistency can take us. Assume that the Alldiﬀconstraints have been\nexpanded into binary constraints (such as A1 ̸= A2) so that we can apply the AC-3 algorithm\ndirectly. Consider variable E6 from Figure 6.4(a)—the empty square between the 2 and the\n8 in the middle box. From the constraints in the box, we can remove not only 2 and 8 but also\n1 and 7 from E6’s domain. From the constraints in its column, we can eliminate 5, 6, 2, 8,\n9, and 3. That leaves E6 with a domain of {4}; in other words, we know the answer for E6.\nNow consider variable I6—the square in the bottom middle box surrounded by 1, 3, and 3.\nApplying arc consistency in its column, we eliminate 5, 6, 2, 4 (since we now know E6 must\nbe 4), 8, 9, and 3. We eliminate 1 by arc consistency with I5, and we are left with only the\nvalue 7 in the domain of I6. Now there are 8 known values in column 6, so arc consistency",
  "be 4), 8, 9, and 3. We eliminate 1 by arc consistency with I5, and we are left with only the\nvalue 7 in the domain of I6. Now there are 8 known values in column 6, so arc consistency\ncan infer that A6 must be 1. Inference continues along these lines, and eventually, AC-3 can\nsolve the entire puzzle—all the variables have their domains reduced to a single value, as\nshown in Figure 6.4(b).\nOf course, Sudoku would soon lose its appeal if every puzzle could be solved by a 214\nChapter\n6.\nConstraint Satisfaction Problems\nmechanical application of AC-3, and indeed AC-3 works only for the easiest Sudoku puzzles.\nSlightly harder ones can be solved by PC-2, but at a greater computational cost: there are\n255,960 different path constraints to consider in a Sudoku puzzle. To solve the hardest puzzles\nand to make efﬁcient progress, we will have to be more clever.\nIndeed, the appeal of Sudoku puzzles for the human solver is the need to be resourceful\nin applying more complex inference strategies. Aﬁcionados give them colorful names, such\nas “naked triples.” That strategy works as follows: in any unit (row, column or box), ﬁnd\nthree squares that each have a domain that contains the same three numbers or a subset of\nthose numbers. For example, the three domains might be {1, 8}, {3, 8}, and {1, 3, 8}. From\nthat we don’t know which square contains 1, 3, or 8, but we do know that the three numbers\nmust be distributed among the three squares. Therefore we can remove 1, 3, and 8 from the\ndomains of every other square in the unit.\nIt is interesting to note how far we can go without saying much that is speciﬁc to Su-\ndoku. We do of course have to say that there are 81 variables, that their domains are the digits\n1 to 9, and that there are 27 Alldiﬀconstraints. But beyond that, all the strategies—arc con-\nsistency, path consistency, etc.—apply generally to all CSPs, not just to Sudoku problems.\nEven naked triples is really a strategy for enforcing consistency of Alldiﬀconstraints and\nhas nothing to do with Sudoku per se. This is the power of the CSP formalism: for each new\nproblem area, we only need to deﬁne the problem in terms of constraints; then the general\nconstraint-solving mechanisms can take over.\n6.3\nBACKTRACKING SEARCH FOR CSPS\nSudoku problems are designed to be solved by inference over constraints. But many other\nCSPs cannot be solved by inference alone; there comes a time when we must search for a",
  "6.3\nBACKTRACKING SEARCH FOR CSPS\nSudoku problems are designed to be solved by inference over constraints. But many other\nCSPs cannot be solved by inference alone; there comes a time when we must search for a\nsolution. In this section we look at backtracking search algorithms that work on partial as-\nsignments; in the next section we look at local search algorithms over complete assignments.\nWe could apply a standard depth-limited search (from Chapter 3). A state would be a\npartial assignment, and an action would be adding var = value to the assignment. But for a\nCSP with n variables of domain size d, we quickly notice something terrible: the branching\nfactor at the top level is nd because any of d values can be assigned to any of n variables. At\nthe next level, the branching factor is (n −1)d, and so on for n levels. We generate a tree\nwith n! · dn leaves, even though there are only dn possible complete assignments!\nOur seemingly reasonable but naive formulation ignores crucial property common to\nall CSPs: commutativity. A problem is commutative if the order of application of any given\nCOMMUTATIVITY\nset of actions has no effect on the outcome. CSPs are commutative because when assigning\nvalues to variables, we reach the same partial assignment regardless of order. Therefore, we\nneed only consider a single variable at each node in the search tree. For example, at the root\nnode of a search tree for coloring the map of Australia, we might make a choice between\nSA = red, SA = green, and SA = blue, but we would never choose between SA = red and\nWA = blue. With this restriction, the number of leaves is dn, as we would hope. Section 6.3.\nBacktracking Search for CSPs\n215\nfunction BACKTRACKING-SEARCH(csp) returns a solution, or failure\nreturn BACKTRACK({ },csp)\nfunction BACKTRACK(assignment,csp) returns a solution, or failure\nif assignment is complete then return assignment\nvar ←SELECT-UNASSIGNED-VARIABLE(csp)\nfor each value in ORDER-DOMAIN-VALUES(var,assignment,csp) do\nif value is consistent with assignment then\nadd {var = value} to assignment\ninferences ←INFERENCE(csp,var,value)\nif inferences ̸= failure then\nadd inferences to assignment\nresult ←BACKTRACK(assignment,csp)\nif result ̸= failure then\nreturn result\nremove {var = value} and inferences from assignment\nreturn failure\nFigure 6.5\nA simple backtracking algorithm for constraint satisfaction problems. The al-\ngorithm is modeled on the recursive depth-ﬁrst search of Chapter 3. By varying the functions",
  "remove {var = value} and inferences from assignment\nreturn failure\nFigure 6.5\nA simple backtracking algorithm for constraint satisfaction problems. The al-\ngorithm is modeled on the recursive depth-ﬁrst search of Chapter 3. By varying the functions\nSELECT-UNASSIGNED-VARIABLE and ORDER-DOMAIN-VALUES, we can implement the\ngeneral-purpose heuristics discussed in the text. The function INFERENCE can optionally be\nused to impose arc-, path-, or k-consistency, as desired. If a value choice leads to failure\n(noticed either by INFERENCE or by BACKTRACK), then value assignments (including those\nmade by INFERENCE) are removed from the current assignment and a new value is tried.\nThe term backtracking search is used for a depth-ﬁrst search that chooses values for\nBACKTRACKING\nSEARCH\none variable at a time and backtracks when a variable has no legal values left to assign. The\nalgorithm is shown in Figure 6.5. It repeatedly chooses an unassigned variable, and then tries\nall values in the domain of that variable in turn, trying to ﬁnd a solution. If an inconsistency is\ndetected, then BACKTRACK returns failure, causing the previous call to try another value. Part\nof the search tree for the Australia problem is shown in Figure 6.6, where we have assigned\nvariables in the order WA, NT, Q, . . .. Because the representation of CSPs is standardized,\nthere is no need to supply BACKTRACKING-SEARCH with a domain-speciﬁc initial state,\naction function, transition model, or goal test.\nNotice that BACKTRACKING-SEARCH keeps only a single representation of a state and\nalters that representation rather than creating new ones, as described on page 87.\nIn Chapter 3 we improved the poor performance of uninformed search algorithms by\nsupplying them with domain-speciﬁc heuristic functions derived from our knowledge of the\nproblem. It turns out that we can solve CSPs efﬁciently without such domain-speciﬁc knowl-\nedge. Instead, we can add some sophistication to the unspeciﬁed functions in Figure 6.5,\nusing them to address the following questions:\n1. Which variable should be assigned next (SELECT-UNASSIGNED-VARIABLE), and in\nwhat order should its values be tried (ORDER-DOMAIN-VALUES)? 216\nChapter\n6.\nConstraint Satisfaction Problems\nWA=red\nWA=blue\nWA=green\nWA=red\nNT=blue\nWA=red\nNT=green\nWA=red\nNT=green\nQ=red\nWA=red\nNT=green\nQ=blue\nFigure 6.6\nPart of the search tree for the map-coloring problem in Figure 6.1.\n2. What inferences should be performed at each step in the search (INFERENCE)?",
  "WA=green\nWA=red\nNT=blue\nWA=red\nNT=green\nWA=red\nNT=green\nQ=red\nWA=red\nNT=green\nQ=blue\nFigure 6.6\nPart of the search tree for the map-coloring problem in Figure 6.1.\n2. What inferences should be performed at each step in the search (INFERENCE)?\n3. When the search arrives at an assignment that violates a constraint, can the search avoid\nrepeating this failure?\nThe subsections that follow answer each of these questions in turn.\n6.3.1\nVariable and value ordering\nThe backtracking algorithm contains the line\nvar ←SELECT-UNASSIGNED-VARIABLE(csp) .\nThe simplest strategy for SELECT-UNASSIGNED-VARIABLE is to choose the next unassigned\nvariable in order, {X1, X2, . . .}. This static variable ordering seldom results in the most efﬁ-\ncient search. For example, after the assignments for WA = red and NT = green in Figure 6.6,\nthere is only one possible value for SA, so it makes sense to assign SA = blue next rather than\nassigning Q. In fact, after SA is assigned, the choices for Q, NSW , and V are all forced. This\nintuitive idea—choosing the variable with the fewest “legal” values—is called the minimum-\nremaining-values (MRV) heuristic. It also has been called the “most constrained variable” or\nMINIMUM-\nREMAINING-VALUES\n“fail-ﬁrst” heuristic, the latter because it picks a variable that is most likely to cause a failure\nsoon, thereby pruning the search tree. If some variable X has no legal values left, the MRV\nheuristic will select X and failure will be detected immediately—avoiding pointless searches\nthrough other variables. The MRV heuristic usually performs better than a random or static\nordering, sometimes by a factor of 1,000 or more, although the results vary widely depending\non the problem.\nThe MRV heuristic doesn’t help at all in choosing the ﬁrst region to color in Australia,\nbecause initially every region has three legal colors. In this case, the degree heuristic comes\nDEGREE HEURISTIC\nin handy. It attempts to reduce the branching factor on future choices by selecting the vari-\nable that is involved in the largest number of constraints on other unassigned variables. In\nFigure 6.1, SA is the variable with highest degree, 5; the other variables have degree 2 or 3,\nexcept for T, which has degree 0. In fact, once SA is chosen, applying the degree heuris-\ntic solves the problem without any false steps—you can choose any consistent color at each\nchoice point and still arrive at a solution with no backtracking. The minimum-remaining- Section 6.3.",
  "tic solves the problem without any false steps—you can choose any consistent color at each\nchoice point and still arrive at a solution with no backtracking. The minimum-remaining- Section 6.3.\nBacktracking Search for CSPs\n217\nvalues heuristic is usually a more powerful guide, but the degree heuristic can be useful as a\ntie-breaker.\nOnce a variable has been selected, the algorithm must decide on the order in which to\nexamine its values. For this, the least-constraining-value heuristic can be effective in some\nLEAST-\nCONSTRAINING-\nVALUE\ncases. It prefers the value that rules out the fewest choices for the neighboring variables in\nthe constraint graph. For example, suppose that in Figure 6.1 we have generated the partial\nassignment with WA = red and NT = green and that our next choice is for Q. Blue would\nbe a bad choice because it eliminates the last legal value left for Q’s neighbor, SA. The\nleast-constraining-value heuristic therefore prefers red to blue. In general, the heuristic is\ntrying to leave the maximum ﬂexibility for subsequent variable assignments. Of course, if we\nare trying to ﬁnd all the solutions to a problem, not just the ﬁrst one, then the ordering does\nnot matter because we have to consider every value anyway. The same holds if there are no\nsolutions to the problem.\nWhy should variable selection be fail-ﬁrst, but value selection be fail-last? It turns out\nthat, for a wide variety of problems, a variable ordering that chooses a variable with the\nminimum number of remaining values helps minimize the number of nodes in the search tree\nby pruning larger parts of the tree earlier. For value ordering, the trick is that we only need\none solution; therefore it makes sense to look for the most likely values ﬁrst. If we wanted to\nenumerate all solutions rather than just ﬁnd one, then value ordering would be irrelevant.\n6.3.2\nInterleaving search and inference\nSo far we have seen how AC-3 and other algorithms can infer reductions in the domain of\nvariables before we begin the search. But inference can be even more powerful in the course\nof a search: every time we make a choice of a value for a variable, we have a brand-new\nopportunity to infer new domain reductions on the neighboring variables.\nOne of the simplest forms of inference is called forward checking. Whenever a vari-\nFORWARD\nCHECKING\nable X is assigned, the forward-checking process establishes arc consistency for it: for each",
  "One of the simplest forms of inference is called forward checking. Whenever a vari-\nFORWARD\nCHECKING\nable X is assigned, the forward-checking process establishes arc consistency for it: for each\nunassigned variable Y that is connected to X by a constraint, delete from Y ’s domain any\nvalue that is inconsistent with the value chosen for X. Because forward checking only does\narc consistency inferences, there is no reason to do forward checking if we have already done\narc consistency as a preprocessing step.\nFigure 6.7 shows the progress of backtracking search on the Australia CSP with for-\nward checking. There are two important points to notice about this example. First, notice\nthat after WA = red and Q = green are assigned, the domains of NT and SA are reduced\nto a single value; we have eliminated branching on these variables altogether by propagat-\ning information from WA and Q. A second point to notice is that after V = blue, the do-\nmain of SA is empty. Hence, forward checking has detected that the partial assignment\n{WA = red, Q = green, V = blue} is inconsistent with the constraints of the problem, and\nthe algorithm will therefore backtrack immediately.\nFor many problems the search will be more effective if we combine the MRV heuris-\ntic with forward checking. Consider Figure 6.7 after assigning {WA = red}. Intuitively, it\nseems that that assignment constrains its neighbors, NT and SA, so we should handle those 218\nChapter\n6.\nConstraint Satisfaction Problems\nInitial domains\nAfter WA=red\nAfter Q=green\nAfter V=blue\nR G B\nR\nR\nB\nR G B\nR G B\nB\nR G B\nR G B\nR G B\nR\nR\nR\nR G B\nB\nB\nG B\nR G B\nG\nG\nR G B\nR G B\nB\nG B\nR G B\nR G B\nR G B\nR G B\nWA\nT\nSA\nV\nNSW\nQ\nNT\nFigure 6.7\nThe progress of a map-coloring search with forward checking. WA = red\nis assigned ﬁrst; then forward checking deletes red from the domains of the neighboring\nvariables NT and SA. After Q = green is assigned, green is deleted from the domains of\nNT, SA, and NSW . After V = blue is assigned, blue is deleted from the domains of NSW\nand SA, leaving SA with no legal values.\nvariables next, and then all the other variables will fall into place. That’s exactly what hap-\npens with MRV: NT and SA have two values, so one of them is chosen ﬁrst, then the other,\nthen Q, NSW , and V in order. Finally T still has three values, and any one of them works.\nWe can view forward checking as an efﬁcient way to incrementally compute the information\nthat the MRV heuristic needs to do its job.",
  "then Q, NSW , and V in order. Finally T still has three values, and any one of them works.\nWe can view forward checking as an efﬁcient way to incrementally compute the information\nthat the MRV heuristic needs to do its job.\nAlthough forward checking detects many inconsistencies, it does not detect all of them.\nThe problem is that it makes the current variable arc-consistent, but doesn’t look ahead and\nmake all the other variables arc-consistent. For example, consider the third row of Figure 6.7.\nIt shows that when WA is red and Q is green, both NT and SA are forced to be blue. Forward\nchecking does not look far enough ahead to notice that this is an inconsistency: NT and SA\nare adjacent and so cannot have the same value.\nThe algorithm called MAC (for Maintaining Arc Consistency (MAC)) detects this\nMAINTAINING ARC\nCONSISTENCY (MAC)\ninconsistency. After a variable Xi is assigned a value, the INFERENCE procedure calls AC-3,\nbut instead of a queue of all arcs in the CSP, we start with only the arcs (Xj, Xi) for all\nXj that are unassigned variables that are neighbors of Xi. From there, AC-3 does constraint\npropagation in the usual way, and if any variable has its domain reduced to the empty set, the\ncall to AC-3 fails and we know to backtrack immediately. We can see that MAC is strictly\nmore powerful than forward checking because forward checking does the same thing as MAC\non the initial arcs in MAC’s queue; but unlike MAC, forward checking does not recursively\npropagate constraints when changes are made to the domains of variables.\n6.3.3\nIntelligent backtracking: Looking backward\nThe BACKTRACKING-SEARCH algorithm in Figure 6.5 has a very simple policy for what to\ndo when a branch of the search fails: back up to the preceding variable and try a different\nvalue for it. This is called chronological backtracking because the most recent decision\nCHRONOLOGICAL\nBACKTRACKING\npoint is revisited. In this subsection, we consider better possibilities.\nConsider what happens when we apply simple backtracking in Figure 6.1 with a ﬁxed\nvariable ordering Q, NSW , V , T, SA, WA, NT. Suppose we have generated the partial\nassignment {Q = red, NSW = green, V = blue, T = red}. When we try the next variable,\nSA, we see that every value violates a constraint. We back up to T and try a new color for Section 6.3.\nBacktracking Search for CSPs\n219\nTasmania! Obviously this is silly—recoloring Tasmania cannot possibly resolve the problem\nwith South Australia.",
  "SA, we see that every value violates a constraint. We back up to T and try a new color for Section 6.3.\nBacktracking Search for CSPs\n219\nTasmania! Obviously this is silly—recoloring Tasmania cannot possibly resolve the problem\nwith South Australia.\nA more intelligent approach to backtracking is to backtrack to a variable that might ﬁx\nthe problem—a variable that was responsible for making one of the possible values of SA\nimpossible. To do this, we will keep track of a set of assignments that are in conﬂict with\nsome value for SA. The set (in this case {Q = red, NSW = green, V = blue, }), is called the\nconﬂict set for SA. The backjumping method backtracks to the most recent assignment in\nCONFLICT SET\nBACKJUMPING\nthe conﬂict set; in this case, backjumping would jump over Tasmania and try a new value\nfor V . This method is easily implemented by a modiﬁcation to BACKTRACK such that it\naccumulates the conﬂict set while checking for a legal value to assign. If no legal value is\nfound, the algorithm should return the most recent element of the conﬂict set along with the\nfailure indicator.\nThe sharp-eyed reader will have noticed that forward checking can supply the conﬂict\nset with no extra work: whenever forward checking based on an assignment X = x deletes a\nvalue from Y ’s domain, it should add X = x to Y ’s conﬂict set. If the last value is deleted\nfrom Y ’s domain, then the assignments in the conﬂict set of Y are added to the conﬂict set\nof X. Then, when we get to Y , we know immediately where to backtrack if needed.\nThe eagle-eyed reader will have noticed something odd: backjumping occurs when\nevery value in a domain is in conﬂict with the current assignment; but forward checking\ndetects this event and prevents the search from ever reaching such a node! In fact, it can be\nshown that every branch pruned by backjumping is also pruned by forward checking. Hence,\nsimple backjumping is redundant in a forward-checking search or, indeed, in a search that\nuses stronger consistency checking, such as MAC.\nDespite the observations of the preceding paragraph, the idea behind backjumping re-\nmains a good one: to backtrack based on the reasons for failure. Backjumping notices failure\nwhen a variable’s domain becomes empty, but in many cases a branch is doomed long before\nthis occurs. Consider again the partial assignment {WA = red, NSW = red} (which, from\nour earlier discussion, is inconsistent). Suppose we try T = red next and then assign NT, Q,",
  "this occurs. Consider again the partial assignment {WA = red, NSW = red} (which, from\nour earlier discussion, is inconsistent). Suppose we try T = red next and then assign NT, Q,\nV , SA. We know that no assignment can work for these last four variables, so eventually we\nrun out of values to try at NT. Now, the question is, where to backtrack? Backjumping cannot\nwork, because NT does have values consistent with the preceding assigned variables—NT\ndoesn’t have a complete conﬂict set of preceding variables that caused it to fail. We know,\nhowever, that the four variables NT, Q, V , and SA, taken together, failed because of a set of\npreceding variables, which must be those variables that directly conﬂict with the four. This\nleads to a deeper notion of the conﬂict set for a variable such as NT: it is that set of preced-\ning variables that caused NT, together with any subsequent variables, to have no consistent\nsolution. In this case, the set is WA and NSW , so the algorithm should backtrack to NSW\nand skip over Tasmania. A backjumping algorithm that uses conﬂict sets deﬁned in this way\nis called conﬂict-directed backjumping.\nCONFLICT-DIRECTED\nBACKJUMPING\nWe must now explain how these new conﬂict sets are computed. The method is in\nfact quite simple. The “terminal” failure of a branch of the search always occurs because a\nvariable’s domain becomes empty; that variable has a standard conﬂict set. In our example,\nSA fails, and its conﬂict set is (say) {WA, NT, Q}. We backjump to Q, and Q absorbs 220\nChapter\n6.\nConstraint Satisfaction Problems\nthe conﬂict set from SA (minus Q itself, of course) into its own direct conﬂict set, which is\n{NT, NSW }; the new conﬂict set is {WA, NT, NSW }. That is, there is no solution from\nQ onward, given the preceding assignment to {WA, NT, NSW }. Therefore, we backtrack\nto NT, the most recent of these. NT absorbs {WA, NT, NSW } −{NT} into its own\ndirect conﬂict set {WA}, giving {WA, NSW } (as stated in the previous paragraph). Now\nthe algorithm backjumps to NSW , as we would hope. To summarize: let Xj be the current\nvariable, and let conf (Xj) be its conﬂict set. If every possible value for Xj fails, backjump\nto the most recent variable Xi in conf (Xj), and set\nconf (Xi) ←conf (Xi) ∪conf (Xj) −{Xi} .\nWhen we reach a contradiction, backjumping can tell us how far to back up, so we don’t\nwaste time changing variables that won’t ﬁx the problem. But we would also like to avoid",
  "conf (Xi) ←conf (Xi) ∪conf (Xj) −{Xi} .\nWhen we reach a contradiction, backjumping can tell us how far to back up, so we don’t\nwaste time changing variables that won’t ﬁx the problem. But we would also like to avoid\nrunning into the same problem again. When the search arrives at a contradiction, we know\nthat some subset of the conﬂict set is responsible for the problem. Constraint learning is the\nCONSTRAINT\nLEARNING\nidea of ﬁnding a minimum set of variables from the conﬂict set that causes the problem. This\nset of variables, along with their corresponding values, is called a no-good. We then record\nNO-GOOD\nthe no-good, either by adding a new constraint to the CSP or by keeping a separate cache of\nno-goods.\nFor example, consider the state {WA = red, NT = green, Q = blue} in the bottom\nrow of Figure 6.6. Forward checking can tell us this state is a no-good because there is no\nvalid assignment to SA. In this particular case, recording the no-good would not help, because\nonce we prune this branch from the search tree, we will never encounter this combination\nagain. But suppose that the search tree in Figure 6.6 were actually part of a larger search tree\nthat started by ﬁrst assigning values for V and T. Then it would be worthwhile to record\n{WA = red, NT = green, Q = blue} as a no-good because we are going to run into the\nsame problem again for each possible set of assignments to V and T.\nNo-goods can be effectively used by forward checking or by backjumping. Constraint\nlearning is one of the most important techniques used by modern CSP solvers to achieve\nefﬁciency on complex problems.\n6.4\nLOCAL SEARCH FOR CSPS\nLocal search algorithms (see Section 4.1) turn out to be effective in solving many CSPs. They\nuse a complete-state formulation: the initial state assigns a value to every variable, and the\nsearch changes the value of one variable at a time. For example, in the 8-queens problem (see\nFigure 4.3), the initial state might be a random conﬁguration of 8 queens in 8 columns, and\neach step moves a single queen to a new position in its column. Typically, the initial guess\nviolates several constraints. The point of local search is to eliminate the violated constraints.2\nIn choosing a new value for a variable, the most obvious heuristic is to select the value\nthat results in the minimum number of conﬂicts with other variables—the min-conﬂicts\nMIN-CONFLICTS",
  "In choosing a new value for a variable, the most obvious heuristic is to select the value\nthat results in the minimum number of conﬂicts with other variables—the min-conﬂicts\nMIN-CONFLICTS\n2 Local search can easily be extended to constraint optimization problems (COPs). In that case, all the techniques\nfor hill climbing and simulated annealing can be applied to optimize the objective function. Section 6.4.\nLocal Search for CSPs\n221\nfunction MIN-CONFLICTS(csp,max steps) returns a solution or failure\ninputs: csp, a constraint satisfaction problem\nmax steps, the number of steps allowed before giving up\ncurrent ←an initial complete assignment for csp\nfor i = 1 to max steps do\nif current is a solution for csp then return current\nvar ←a randomly chosen conﬂicted variable from csp.VARIABLES\nvalue ←the value v for var that minimizes CONFLICTS(var,v,current,csp)\nset var = value in current\nreturn failure\nFigure 6.8\nThe MIN-CONFLICTS algorithm for solving CSPs by local search. The initial\nstate may be chosen randomly or by a greedy assignment process that chooses a minimal-\nconﬂict value for each variable in turn. The CONFLICTS function counts the number of\nconstraints violated by a particular value, given the rest of the current assignment.\n2\n2\n1\n2\n3\n1\n2\n3\n3\n2\n3\n2\n3\n0\nFigure 6.9\nA two-step solution using min-conﬂicts for an 8-queens problem. At each\nstage, a queen is chosen for reassignment in its column. The number of conﬂicts (in this\ncase, the number of attacking queens) is shown in each square. The algorithm moves the\nqueen to the min-conﬂicts square, breaking ties randomly.\nheuristic. The algorithm is shown in Figure 6.8 and its application to an 8-queens problem is\ndiagrammed in Figure 6.9.\nMin-conﬂicts is surprisingly effective for many CSPs. Amazingly, on the n-queens\nproblem, if you don’t count the initial placement of queens, the run time of min-conﬂicts is\nroughly independent of problem size. It solves even the million-queens problem in an aver-\nage of 50 steps (after the initial assignment). This remarkable observation was the stimulus\nleading to a great deal of research in the 1990s on local search and the distinction between\neasy and hard problems, which we take up in Chapter 7. Roughly speaking, n-queens is\neasy for local search because solutions are densely distributed throughout the state space.\nMin-conﬂicts also works well for hard problems. For example, it has been used to schedule",
  "easy for local search because solutions are densely distributed throughout the state space.\nMin-conﬂicts also works well for hard problems. For example, it has been used to schedule\nobservations for the Hubble Space Telescope, reducing the time taken to schedule a week of\nobservations from three weeks (!) to around 10 minutes. 222\nChapter\n6.\nConstraint Satisfaction Problems\nAll the local search techniques from Section 4.1 are candidates for application to CSPs,\nand some of those have proved especially effective. The landscape of a CSP under the min-\nconﬂicts heuristic usually has a series of plateaux. There may be millions of variable as-\nsignments that are only one conﬂict away from a solution. Plateau search—allowing side-\nways moves to another state with the same score—can help local search ﬁnd its way off this\nplateau. This wandering on the plateau can be directed with tabu search: keeping a small\nlist of recently visited states and forbidding the algorithm to return to those states. Simulated\nannealing can also be used to escape from plateaux.\nAnother technique, called constraint weighting, can help concentrate the search on the\nCONSTRAINT\nWEIGHTING\nimportant constraints. Each constraint is given a numeric weight, Wi, initially all 1. At each\nstep of the search, the algorithm chooses a variable/value pair to change that will result in the\nlowest total weight of all violated constraints. The weights are then adjusted by incrementing\nthe weight of each constraint that is violated by the current assignment. This has two beneﬁts:\nit adds topography to plateaux, making sure that it is possible to improve from the current\nstate, and it also, over time, adds weight to the constraints that are proving difﬁcult to solve.\nAnother advantage of local search is that it can be used in an online setting when the\nproblem changes. This is particularly important in scheduling problems. A week’s airline\nschedule may involve thousands of ﬂights and tens of thousands of personnel assignments,\nbut bad weather at one airport can render the schedule infeasible. We would like to repair the\nschedule with a minimum number of changes. This can be easily done with a local search\nalgorithm starting from the current schedule. A backtracking search with the new set of\nconstraints usually requires much more time and might ﬁnd a solution with many changes\nfrom the current schedule.\n6.5\nTHE STRUCTURE OF PROBLEMS",
  "algorithm starting from the current schedule. A backtracking search with the new set of\nconstraints usually requires much more time and might ﬁnd a solution with many changes\nfrom the current schedule.\n6.5\nTHE STRUCTURE OF PROBLEMS\nIn this section, we examine ways in which the structure of the problem, as represented by\nthe constraint graph, can be used to ﬁnd solutions quickly. Most of the approaches here also\napply to other problems besides CSPs, such as probabilistic reasoning. After all, the only way\nwe can possibly hope to deal with the real world is to decompose it into many subproblems.\nLooking again at the constraint graph for Australia (Figure 6.1(b), repeated as Figure 6.12(a)),\none fact stands out: Tasmania is not connected to the mainland.3 Intuitively, it is obvious that\ncoloring Tasmania and coloring the mainland are independent subproblems—any solution\nINDEPENDENT\nSUBPROBLEMS\nfor the mainland combined with any solution for Tasmania yields a solution for the whole\nmap. Independence can be ascertained simply by ﬁnding connected components of the\nCONNECTED\nCOMPONENT\nconstraint graph. Each component corresponds to a subproblem CSPi. If assignment Si is\na solution of CSPi, then \u0016\ni Si is a solution of \u0016\ni CSPi. Why is this important? Consider\nthe following: suppose each CSPi has c variables from the total of n variables, where c is\na constant. Then there are n/c subproblems, each of which takes at most dc work to solve,\n3 A careful cartographer or patriotic Tasmanian might object that Tasmania should not be colored the same as\nits nearest mainland neighbor, to avoid the impression that it might be part of that state. Section 6.5.\nThe Structure of Problems\n223\nwhere d is the size of the domain. Hence, the total work is O(dcn/c), which is linear in n;\nwithout the decomposition, the total work is O(dn), which is exponential in n. Let’s make\nthis more concrete: dividing a Boolean CSP with 80 variables into four subproblems reduces\nthe worst-case solution time from the lifetime of the universe down to less than a second.\nCompletely independent subproblems are delicious, then, but rare. Fortunately, some\nother graph structures are also easy to solve. For example, a constraint graph is a tree when\nany two variables are connected by only one path. We show that any tree-structured CSP can\nbe solved in time linear in the number of variables.4 The key is a new notion of consistency,",
  "any two variables are connected by only one path. We show that any tree-structured CSP can\nbe solved in time linear in the number of variables.4 The key is a new notion of consistency,\ncalled directed arc consistency or DAC. A CSP is deﬁned to be directed arc-consistent under\nDIRECTED ARC\nCONSISTENCY\nan ordering of variables X1, X2, . . . , Xn if and only if every Xi is arc-consistent with each\nXj for j > i.\nTo solve a tree-structured CSP, ﬁrst pick any variable to be the root of the tree, and\nchoose an ordering of the variables such that each variable appears after its parent in the tree.\nSuch an ordering is called a topological sort. Figure 6.10(a) shows a sample tree and (b)\nTOPOLOGICAL SORT\nshows one possible ordering. Any tree with n nodes has n−1 arcs, so we can make this graph\ndirected arc-consistent in O(n) steps, each of which must compare up to d possible domain\nvalues for two variables, for a total time of O(nd2). Once we have a directed arc-consistent\ngraph, we can just march down the list of variables and choose any remaining value. Since\neach link from a parent to its child is arc consistent, we know that for any value we choose for\nthe parent, there will be a valid value left to choose for the child. That means we won’t have\nto backtrack; we can move linearly through the variables. The complete algorithm is shown\nin Figure 6.11.\nA\nC\nB\nD\nE\nF\n(a)\nA\nC\nB\nD\nE\nF\n(b)\nFigure 6.10\n(a) The constraint graph of a tree-structured CSP. (b) A linear ordering of the\nvariables consistent with the tree with A as the root. This is known as a topological sort of\nthe variables.\nNow that we have an efﬁcient algorithm for trees, we can consider whether more general\nconstraint graphs can be reduced to trees somehow. There are two primary ways to do this,\none based on removing nodes and one based on collapsing nodes together.\nThe ﬁrst approach involves assigning values to some variables so that the remaining\nvariables form a tree.\nConsider the constraint graph for Australia, shown again in Fig-\nure 6.12(a). If we could delete South Australia, the graph would become a tree, as in (b).\nFortunately, we can do this (in the graph, not the continent) by ﬁxing a value for SA and\n4 Sadly, very few regions of the world have tree-structured maps, although Sulawesi comes close. 224\nChapter\n6.\nConstraint Satisfaction Problems\nfunction TREE-CSP-SOLVER(csp) returns a solution, or failure\ninputs: csp, a CSP with components X, D, C\nn ←number of variables in X",
  "Chapter\n6.\nConstraint Satisfaction Problems\nfunction TREE-CSP-SOLVER(csp) returns a solution, or failure\ninputs: csp, a CSP with components X, D, C\nn ←number of variables in X\nassignment ←an empty assignment\nroot ←any variable in X\nX ←TOPOLOGICALSORT(X ,root)\nfor j = n down to 2 do\nMAKE-ARC-CONSISTENT(PARENT(Xj),Xj)\nif it cannot be made consistent then return failure\nfor i = 1 to n do\nassignment[Xi] ←any consistent value from Di\nif there is no consistent value then return failure\nreturn assignment\nFigure 6.11\nThe TREE-CSP-SOLVER algorithm for solving tree-structured CSPs. If the\nCSP has a solution, we will ﬁnd it in linear time; if not, we will detect a contradiction.\nWA\nNT\nSA\nQ\nNSW\nV\nT\nWA\nNT\nQ\nNSW\nV\nT\n(a)\n(b)\nFigure 6.12\n(a) The original constraint graph from Figure 6.1. (b) The constraint graph\nafter the removal of SA.\ndeleting from the domains of the other variables any values that are inconsistent with the\nvalue chosen for SA.\nNow, any solution for the CSP after SA and its constraints are removed will be con-\nsistent with the value chosen for SA. (This works for binary CSPs; the situation is more\ncomplicated with higher-order constraints.) Therefore, we can solve the remaining tree with\nthe algorithm given above and thus solve the whole problem. Of course, in the general case\n(as opposed to map coloring), the value chosen for SA could be the wrong one, so we would\nneed to try each possible value. The general algorithm is as follows: Section 6.5.\nThe Structure of Problems\n225\n1. Choose a subset S of the CSP’s variables such that the constraint graph becomes a tree\nafter removal of S. S is called a cycle cutset.\nCYCLE CUTSET\n2. For each possible assignment to the variables in S that satisﬁes all constraints on S,\n(a) remove from the domains of the remaining variables any values that are inconsis-\ntent with the assignment for S, and\n(b) If the remaining CSP has a solution, return it together with the assignment for S.\nIf the cycle cutset has size c, then the total run time is O(dc · (n −c)d2): we have to try each\nof the dc combinations of values for the variables in S, and for each combination we must\nsolve a tree problem of size n −c. If the graph is “nearly a tree,” then c will be small and the\nsavings over straight backtracking will be huge. In the worst case, however, c can be as large\nas (n −2). Finding the smallest cycle cutset is NP-hard, but several efﬁcient approximation",
  "savings over straight backtracking will be huge. In the worst case, however, c can be as large\nas (n −2). Finding the smallest cycle cutset is NP-hard, but several efﬁcient approximation\nalgorithms are known. The overall algorithmic approach is called cutset conditioning; it\nCUTSET\nCONDITIONING\ncomes up again in Chapter 14, where it is used for reasoning about probabilities.\nThe second approach is based on constructing a tree decomposition of the constraint\nTREE\nDECOMPOSITION\ngraph into a set of connected subproblems. Each subproblem is solved independently, and the\nresulting solutions are then combined. Like most divide-and-conquer algorithms, this works\nwell if no subproblem is too large. Figure 6.13 shows a tree decomposition of the map-\ncoloring problem into ﬁve subproblems. A tree decomposition must satisfy the following\nthree requirements:\n• Every variable in the original problem appears in at least one of the subproblems.\n• If two variables are connected by a constraint in the original problem, they must appear\ntogether (along with the constraint) in at least one of the subproblems.\n• If a variable appears in two subproblems in the tree, it must appear in every subproblem\nalong the path connecting those subproblems.\nThe ﬁrst two conditions ensure that all the variables and constraints are represented in the\ndecomposition. The third condition seems rather technical, but simply reﬂects the constraint\nthat any given variable must have the same value in every subproblem in which it appears;\nthe links joining subproblems in the tree enforce this constraint. For example, SA appears in\nall four of the connected subproblems in Figure 6.13. You can verify from Figure 6.12 that\nthis decomposition makes sense.\nWe solve each subproblem independently; if any one has no solution, we know the en-\ntire problem has no solution. If we can solve all the subproblems, then we attempt to construct\na global solution as follows. First, we view each subproblem as a “mega-variable” whose do-\nmain is the set of all solutions for the subproblem. For example, the leftmost subproblem in\nFigure 6.13 is a map-coloring problem with three variables and hence has six solutions—one\nis {WA = red, SA = blue, NT = green}. Then, we solve the constraints connecting the\nsubproblems, using the efﬁcient algorithm for trees given earlier. The constraints between\nsubproblems simply insist that the subproblem solutions agree on their shared variables. For",
  "subproblems, using the efﬁcient algorithm for trees given earlier. The constraints between\nsubproblems simply insist that the subproblem solutions agree on their shared variables. For\nexample, given the solution {WA = red, SA = blue, NT = green} for the ﬁrst subproblem,\nthe only consistent solution for the next subproblem is {SA = blue, NT = green, Q = red}.\nA given constraint graph admits many tree decompositions; in choosing a decompo-\nsition, the aim is to make the subproblems as small as possible. The tree width of a tree\nTREE WIDTH 226\nChapter\n6.\nConstraint Satisfaction Problems\nT\nWA\nNT\nSA\nNT\nSA\nQ\nSA\nQ\nNSW\nSA\nNSW\nV\nFigure 6.13\nA tree decomposition of the constraint graph in Figure 6.12(a).\ndecomposition of a graph is one less than the size of the largest subproblem; the tree width\nof the graph itself is deﬁned to be the minimum tree width among all its tree decompositions.\nIf a graph has tree width w and we are given the corresponding tree decomposition, then the\nproblem can be solved in O(ndw+1) time. Hence, CSPs with constraint graphs of bounded\ntree width are solvable in polynomial time. Unfortunately, ﬁnding the decomposition with\nminimal tree width is NP-hard, but there are heuristic methods that work well in practice.\nSo far, we have looked at the structure of the constraint graph. There can be important\nstructure in the values of variables as well. Consider the map-coloring problem with n colors.\nFor every consistent solution, there is actually a set of n! solutions formed by permuting the\ncolor names. For example, on the Australia map we know that WA, NT, and SA must all have\ndifferent colors, but there are 3! = 6 ways to assign the three colors to these three regions.\nThis is called value symmetry. We would like to reduce the search space by a factor of\nVALUE SYMMETRY\nn! by breaking the symmetry. We do this by introducing a symmetry-breaking constraint.\nSYMMETRY-\nBREAKING\nCONSTRAINT\nFor our example, we might impose an arbitrary ordering constraint, NT < SA < WA, that\nrequires the three values to be in alphabetical order. This constraint ensures that only one of\nthe n! solutions is possible: {NT = blue, SA = green, WA = red}.\nFor map coloring, it was easy to ﬁnd a constraint that eliminates the symmetry, and\nin general it is possible to ﬁnd constraints that eliminate all but one symmetric solution in\npolynomial time, but it is NP-hard to eliminate all symmetry among intermediate sets of",
  "in general it is possible to ﬁnd constraints that eliminate all but one symmetric solution in\npolynomial time, but it is NP-hard to eliminate all symmetry among intermediate sets of\nvalues during search. In practice, breaking value symmetry has proved to be important and\neffective on a wide range of problems. Section 6.6.\nSummary\n227\n6.6\nSUMMARY\n• Constraint satisfaction problems (CSPs) represent a state with a set of variable/value\npairs and represent the conditions for a solution by a set of constraints on the variables.\nMany important real-world problems can be described as CSPs.\n• A number of inference techniques use the constraints to infer which variable/value pairs\nare consistent and which are not. These include node, arc, path, and k-consistency.\n• Backtracking search, a form of depth-ﬁrst search, is commonly used for solving CSPs.\nInference can be interwoven with search.\n• The minimum-remaining-values and degree heuristics are domain-independent meth-\nods for deciding which variable to choose next in a backtracking search. The least-\nconstraining-value heuristic helps in deciding which value to try ﬁrst for a given\nvariable. Backtracking occurs when no legal assignment can be found for a variable.\nConﬂict-directed backjumping backtracks directly to the source of the problem.\n• Local search using the min-conﬂicts heuristic has also been applied to constraint satis-\nfaction problems with great success.\n• The complexity of solving a CSP is strongly related to the structure of its constraint\ngraph. Tree-structured problems can be solved in linear time. Cutset conditioning can\nreduce a general CSP to a tree-structured one and is quite efﬁcient if a small cutset can\nbe found. Tree decomposition techniques transform the CSP into a tree of subproblems\nand are efﬁcient if the tree width of the constraint graph is small.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe earliest work related to constraint satisfaction dealt largely with numerical constraints.\nEquational constraints with integer domains were studied by the Indian mathematician Brah-\nmagupta in the seventh century; they are often called Diophantine equations, after the Greek\nDIOPHANTINE\nEQUATIONS\nmathematician Diophantus (c. 200–284), who actually considered the domain of positive ra-\ntionals. Systematic methods for solving linear equations by variable elimination were studied\nby Gauss (1829); the solution of linear inequality constraints goes back to Fourier (1827).",
  "tionals. Systematic methods for solving linear equations by variable elimination were studied\nby Gauss (1829); the solution of linear inequality constraints goes back to Fourier (1827).\nFinite-domain constraint satisfaction problems also have a long history. For example,\ngraph coloring (of which map coloring is a special case) is an old problem in mathematics.\nGRAPH COLORING\nThe four-color conjecture (that every planar graph can be colored with four or fewer colors)\nwas ﬁrst made by Francis Guthrie, a student of De Morgan, in 1852. It resisted solution—\ndespite several published claims to the contrary—until a proof was devised by Appel and\nHaken (1977) (see the book Four Colors Sufﬁce (Wilson, 2004)). Purists were disappointed\nthat part of the proof relied on a computer, so Georges Gonthier (2008), using the COQ\ntheorem prover, derived a formal proof that Appel and Haken’s proof was correct.\nSpeciﬁc classes of constraint satisfaction problems occur throughout the history of\ncomputer science. One of the most inﬂuential early examples was the SKETCHPAD sys- 228\nChapter\n6.\nConstraint Satisfaction Problems\ntem (Sutherland, 1963), which solved geometric constraints in diagrams and was the fore-\nrunner of modern drawing programs and CAD tools. The identiﬁcation of CSPs as a general\nclass is due to Ugo Montanari (1974). The reduction of higher-order CSPs to purely binary\nCSPs with auxiliary variables (see Exercise 6.6) is due originally to the 19th-century logician\nCharles Sanders Peirce. It was introduced into the CSP literature by Dechter (1990b) and\nwas elaborated by Bacchus and van Beek (1998). CSPs with preferences among solutions are\nstudied widely in the optimization literature; see Bistarelli et al. (1997) for a generalization\nof the CSP framework to allow for preferences. The bucket-elimination algorithm (Dechter,\n1999) can also be applied to optimization problems.\nConstraint propagation methods were popularized by Waltz’s (1975) success on poly-\nhedral line-labeling problems for computer vision. Waltz showed that, in many problems,\npropagation completely eliminates the need for backtracking. Montanari (1974) introduced\nthe notion of constraint networks and propagation by path consistency. Alan Mackworth\n(1977) proposed the AC-3 algorithm for enforcing arc consistency as well as the general idea\nof combining backtracking with some degree of consistency enforcement. AC-4, a more",
  "(1977) proposed the AC-3 algorithm for enforcing arc consistency as well as the general idea\nof combining backtracking with some degree of consistency enforcement. AC-4, a more\nefﬁcient arc-consistency algorithm, was developed by Mohr and Henderson (1986). Soon af-\nter Mackworth’s paper appeared, researchers began experimenting with the tradeoff between\nthe cost of consistency enforcement and the beneﬁts in terms of search reduction. Haralick\nand Elliot (1980) favored the minimal forward-checking algorithm described by McGregor\n(1979), whereas Gaschnig (1979) suggested full arc-consistency checking after each vari-\nable assignment—an algorithm later called MAC by Sabin and Freuder (1994). The latter\npaper provides somewhat convincing evidence that, on harder CSPs, full arc-consistency\nchecking pays off. Freuder (1978, 1982) investigated the notion of k-consistency and its\nrelationship to the complexity of solving CSPs. Apt (1999) describes a generic algorithmic\nframework within which consistency propagation algorithms can be analyzed, and Bessi`ere\n(2006) presents a current survey.\nSpecial methods for handling higher-order or global constraints were developed ﬁrst\nwithin the context of constraint logic programming. Marriott and Stuckey (1998) provide\nexcellent coverage of research in this area. The Alldiﬀconstraint was studied by Regin\n(1994), Stergiou and Walsh (1999), and van Hoeve (2001). Bounds constraints were incorpo-\nrated into constraint logic programming by Van Hentenryck et al. (1998). A survey of global\nconstraints is provided by van Hoeve and Katriel (2006).\nSudoku has become the most widely known CSP and was described as such by Simonis\n(2005). Agerbeck and Hansen (2008) describe some of the strategies and show that Sudoku\non an n2 × n2 board is in the class of NP-hard problems. Reeson et al. (2007) show an\ninteractive solver based on CSP techniques.\nThe idea of backtracking search goes back to Golomb and Baumert (1965), and its\napplication to constraint satisfaction is due to Bitner and Reingold (1975), although they trace\nthe basic algorithm back to the 19th century. Bitner and Reingold also introduced the MRV\nheuristic, which they called the most-constrained-variable heuristic. Brelaz (1979) used the\ndegree heuristic as a tiebreaker after applying the MRV heuristic. The resulting algorithm,\ndespite its simplicity, is still the best method for k-coloring arbitrary graphs. Haralick and",
  "degree heuristic as a tiebreaker after applying the MRV heuristic. The resulting algorithm,\ndespite its simplicity, is still the best method for k-coloring arbitrary graphs. Haralick and\nElliot (1980) proposed the least-constraining-value heuristic. Bibliographical and Historical Notes\n229\nThe basic backjumping method is due to John Gaschnig (1977, 1979). Kondrak and\nvan Beek (1997) showed that this algorithm is essentially subsumed by forward checking.\nConﬂict-directed backjumping was devised by Prosser (1993). The most general and pow-\nerful form of intelligent backtracking was actually developed very early on by Stallman and\nSussman (1977). Their technique of dependency-directed backtracking led to the develop-\nDEPENDENCY-\nDIRECTED\nBACKTRACKING\nment of truth maintenance systems (Doyle, 1979), which we discuss in Section 12.6.2. The\nconnection between the two areas is analyzed by de Kleer (1989).\nThe work of Stallman and Sussman also introduced the idea of constraint learning,\nin which partial results obtained by search can be saved and reused later in the search. The\nidea was formalized Dechter (1990a). Backmarking (Gaschnig, 1979) is a particularly sim-\nBACKMARKING\nple method in which consistent and inconsistent pairwise assignments are saved and used\nto avoid rechecking constraints. Backmarking can be combined with conﬂict-directed back-\njumping; Kondrak and van Beek (1997) present a hybrid algorithm that provably subsumes\neither method taken separately. The method of dynamic backtracking (Ginsberg, 1993) re-\nDYNAMIC\nBACKTRACKING\ntains successful partial assignments from later subsets of variables when backtracking over\nan earlier choice that does not invalidate the later success.\nEmpirical studies of several randomized backtracking methods were done by Gomes\net al. (2000) and Gomes and Selman (2001). Van Beek (2006) surveys backtracking.\nLocal search in constraint satisfaction problems was popularized by the work of Kirk-\npatrick et al. (1983) on simulated annealing (see Chapter 4), which is widely used for schedul-\ning problems. The min-conﬂicts heuristic was ﬁrst proposed by Gu (1989) and was developed\nindependently by Minton et al. (1992). Sosic and Gu (1994) showed how it could be applied\nto solve the 3,000,000 queens problem in less than a minute. The astounding success of\nlocal search using min-conﬂicts on the n-queens problem led to a reappraisal of the nature",
  "to solve the 3,000,000 queens problem in less than a minute. The astounding success of\nlocal search using min-conﬂicts on the n-queens problem led to a reappraisal of the nature\nand prevalence of “easy” and “hard” problems. Peter Cheeseman et al. (1991) explored the\ndifﬁculty of randomly generated CSPs and discovered that almost all such problems either\nare trivially easy or have no solutions. Only if the parameters of the problem generator are\nset in a certain narrow range, within which roughly half of the problems are solvable, do we\nﬁnd “hard” problem instances. We discuss this phenomenon further in Chapter 7. Konolige\n(1994) showed that local search is inferior to backtracking search on problems with a certain\ndegree of local structure; this led to work that combined local search and inference, such as\nthat by Pinkas and Dechter (1995). Hoos and Tsang (2006) survey local search techniques.\nWork relating the structure and complexity of CSPs originates with Freuder (1985), who\nshowed that search on arc consistent trees works without any backtracking. A similar result,\nwith extensions to acyclic hypergraphs, was developed in the database community (Beeri\net al., 1983). Bayardo and Miranker (1994) present an algorithm for tree-structured CSPs\nthat runs in linear time without any preprocessing.\nSince those papers were published, there has been a great deal of progress in developing\nmore general results relating the complexity of solving a CSP to the structure of its constraint\ngraph. The notion of tree width was introduced by the graph theorists Robertson and Seymour\n(1986). Dechter and Pearl (1987, 1989), building on the work of Freuder, applied a related\nnotion (which they called induced width) to constraint satisfaction problems and developed\nthe tree decomposition approach sketched in Section 6.5. Drawing on this work and on results 230\nChapter\n6.\nConstraint Satisfaction Problems\nfrom database theory, Gottlob et al. (1999a, 1999b) developed a notion, hypertree width, that\nis based on the characterization of the CSP as a hypergraph. In addition to showing that any\nCSP with hypertree width w can be solved in time O(nw+1 log n), they also showed that\nhypertree width subsumes all previously deﬁned measures of “width” in the sense that there\nare cases where the hypertree width is bounded and the other measures are unbounded.\nInterest in look-back approaches to backtracking was rekindled by the work of Bayardo",
  "are cases where the hypertree width is bounded and the other measures are unbounded.\nInterest in look-back approaches to backtracking was rekindled by the work of Bayardo\nand Schrag (1997), whose RELSAT algorithm combined constraint learning and backjumping\nand was shown to outperform many other algorithms of the time. This led to AND/OR\nsearch algorithms applicable to both CSPs and probabilistic reasoning (Dechter and Ma-\nteescu, 2007). Brown et al. (1988) introduce the idea of symmetry breaking in CSPs, and\nGent et al. (2006) give a recent survey.\nThe ﬁeld of distributed constraint satisfaction looks at solving CSPs when there is a\nDISTRIBUTED\nCONSTRAINT\nSATISFACTION\ncollection of agents, each of which controls a subset of the constraint variables. There have\nbeen annual workshops on this problem since 2000, and good coverage elsewhere (Collin\net al., 1999; Pearce et al., 2008; Shoham and Leyton-Brown, 2009).\nComparing CSP algorithms is mostly an empirical science: few theoretical results show\nthat one algorithm dominates another on all problems; instead, we need to run experiments\nto see which algorithms perform better on typical instances of problems. As Hooker (1995)\npoints out, we need to be careful to distinguish between competitive testing—as occurs in\ncompetitions among algorithms based on run time—and scientiﬁc testing, whose goal is to\nidentify the properties of an algorithm that determine its efﬁcacy on a class of problems.\nThe recent textbooks by Apt (2003) and Dechter (2003), and the collection by Rossi\net al. (2006) are excellent resources on constraint processing. There are several good earlier\nsurveys, including those by Kumar (1992), Dechter and Frost (2002), and Bartak (2001); and\nthe encyclopedia articles by Dechter (1992) and Mackworth (1992). Pearson and Jeavons\n(1997) survey tractable classes of CSPs, covering both structural decomposition methods\nand methods that rely on properties of the domains or constraints themselves. Kondrak and\nvan Beek (1997) give an analytical survey of backtracking search algorithms, and Bacchus\nand van Run (1995) give a more empirical survey. Constraint programming is covered in the\nbooks by Apt (2003) and Fruhwirth and Abdennadher (2003). Several interesting applications\nare described in the collection edited by Freuder and Mackworth (1994). Papers on constraint\nsatisfaction appear regularly in Artiﬁcial Intelligence and in the specialist journal Constraints.",
  "are described in the collection edited by Freuder and Mackworth (1994). Papers on constraint\nsatisfaction appear regularly in Artiﬁcial Intelligence and in the specialist journal Constraints.\nThe primary conference venue is the International Conference on Principles and Practice of\nConstraint Programming, often called CP.\nEXERCISES\n6.1\nHow many solutions are there for the map-coloring problem in Figure 6.1? How many\nsolutions if four colors are allowed? Two colors?\n6.2\nConsider the problem of placing k knights on an n × n chessboard such that no two\nknights are attacking each other, where k is given and k ≤n2. Exercises\n231\na. Choose a CSP formulation. In your formulation, what are the variables?\nb. What are the possible values of each variable?\nc. What sets of variables are constrained, and how?\nd. Now consider the problem of putting as many knights as possible on the board with-\nout any attacks. Explain how to solve this with local search by deﬁning appropriate\nACTIONS and RESULT functions and a sensible objective function.\n6.3\nConsider the problem of constructing (not solving) crossword puzzles:5 ﬁtting words\ninto a rectangular grid. The grid, which is given as part of the problem, speciﬁes which\nsquares are blank and which are shaded. Assume that a list of words (i.e., a dictionary)\nis provided and that the task is to ﬁll in the blank squares by using any subset of the list.\nFormulate this problem precisely in two ways:\na. As a general search problem. Choose an appropriate search algorithm and specify a\nheuristic function. Is it better to ﬁll in blanks one letter at a time or one word at a time?\nb. As a constraint satisfaction problem. Should the variables be words or letters?\nWhich formulation do you think will be better? Why?\n6.4\nGive precise formulations for each of the following as constraint satisfaction problems:\na. Rectilinear ﬂoor-planning: ﬁnd non-overlapping places in a large rectangle for a number\nof smaller rectangles.\nb. Class scheduling: There is a ﬁxed number of professors and classrooms, a list of classes\nto be offered, and a list of possible time slots for classes. Each professor has a set of\nclasses that he or she can teach.\nc. Hamiltonian tour: given a network of cities connected by roads, choose an order to visit\nall cities in a country without repeating any.\n6.5\nSolve the cryptarithmetic problem in Figure 6.2 by hand, using the strategy of back-\ntracking with forward checking and the MRV and least-constraining-value heuristics.",
  "all cities in a country without repeating any.\n6.5\nSolve the cryptarithmetic problem in Figure 6.2 by hand, using the strategy of back-\ntracking with forward checking and the MRV and least-constraining-value heuristics.\n6.6\nShow how a single ternary constraint such as “A + B = C” can be turned into three\nbinary constraints by using an auxiliary variable. You may assume ﬁnite domains. (Hint:\nConsider a new variable that takes on values that are pairs of other values, and consider\nconstraints such as “X is the ﬁrst element of the pair Y .”) Next, show how constraints with\nmore than three variables can be treated similarly. Finally, show how unary constraints can be\neliminated by altering the domains of variables. This completes the demonstration that any\nCSP can be transformed into a CSP with only binary constraints.\n6.7\nConsider the following logic puzzle: In ﬁve houses, each with a different color, live ﬁve\npersons of different nationalities, each of whom prefers a different brand of candy, a different\ndrink, and a different pet. Given the following facts, the questions to answer are “Where does\nthe zebra live, and in which house do they drink water?”\n5 Ginsberg et al. (1990) discuss several methods for constructing crossword puzzles. Littman et al. (1999) tackle\nthe harder problem of solving them. 232\nChapter\n6.\nConstraint Satisfaction Problems\nThe Englishman lives in the red house.\nThe Spaniard owns the dog.\nThe Norwegian lives in the ﬁrst house on the left.\nThe green house is immediately to the right of the ivory house.\nThe man who eats Hershey bars lives in the house next to the man with the fox.\nKit Kats are eaten in the yellow house.\nThe Norwegian lives next to the blue house.\nThe Smarties eater owns snails.\nThe Snickers eater drinks orange juice.\nThe Ukrainian drinks tea.\nThe Japanese eats Milky Ways.\nKit Kats are eaten in a house next to the house where the horse is kept.\nCoffee is drunk in the green house.\nMilk is drunk in the middle house.\nDiscuss different representations of this problem as a CSP. Why would one prefer one repre-\nsentation over another?\n6.8\nConsider the graph with 8 nodes A1, A2, A3, A4, H, T, F1, F2. Ai is connected to\nAi+1 for all i, each Ai is connected to H, H is connected to T, and T is connected to each\nFi. Find a 3-coloring of this graph by hand using the following strategy: backtracking with\nconﬂict-directed backjumping, the variable order A1, H, A4, F1, A2, F2, A3, T, and the\nvalue order R, G, B.\n6.9",
  "Fi. Find a 3-coloring of this graph by hand using the following strategy: backtracking with\nconﬂict-directed backjumping, the variable order A1, H, A4, F1, A2, F2, A3, T, and the\nvalue order R, G, B.\n6.9\nExplain why it is a good heuristic to choose the variable that is most constrained but the\nvalue that is least constraining in a CSP search.\n6.10\nGenerate random instances of map-coloring problems as follows: scatter n points on\nthe unit square; select a point X at random, connect X by a straight line to the nearest point\nY such that X is not already connected to Y and the line crosses no other line; repeat the\nprevious step until no more connections are possible. The points represent regions on the\nmap and the lines connect neighbors. Now try to ﬁnd k-colorings of each map, for both\nk = 3 and k = 4, using min-conﬂicts, backtracking, backtracking with forward checking, and\nbacktracking with MAC. Construct a table of average run times for each algorithm for values\nof n up to the largest you can manage. Comment on your results.\n6.11\nUse the AC-3 algorithm to show that arc consistency can detect the inconsistency of\nthe partial assignment {WA = green, V = red} for the problem shown in Figure 6.1.\n6.12\nWhat is the worst-case complexity of running AC-3 on a tree-structured CSP?\n6.13\nAC-3 puts back on the queue every arc (Xk, Xi) whenever any value is deleted from\nthe domain of Xi, even if each value of Xk is consistent with several remaining values of Xi.\nSuppose that, for every arc (Xk, Xi), we keep track of the number of remaining values of Xi\nthat are consistent with each value of Xk. Explain how to update these numbers efﬁciently\nand hence show that arc consistency can be enforced in total time O(n2d2). Exercises\n233\n6.14\nThe TREE-CSP-SOLVER (Figure 6.10) makes arcs consistent starting at the leaves and\nworking backwards towards the root. Why does it do that? What would happen if it went in\nthe opposite direction?\n6.15\nWe introduced Sudoku as a CSP to be solved by search over partial assignments be-\ncause that is the way people generally undertake solving Sudoku problems. It is also possible,\nof course, to attack these problems with local search over complete assignments. How well\nwould a local solver using the min-conﬂicts heuristic do on Sudoku problems?\n6.16\nDeﬁne in your own words the terms constraint, backtracking search, arc consistency,\nbackjumping, min-conﬂicts, and cycle cutset.\n6.17",
  "would a local solver using the min-conﬂicts heuristic do on Sudoku problems?\n6.16\nDeﬁne in your own words the terms constraint, backtracking search, arc consistency,\nbackjumping, min-conﬂicts, and cycle cutset.\n6.17\nSuppose that a graph is known to have a cycle cutset of no more than k nodes. Describe\na simple algorithm for ﬁnding a minimal cycle cutset whose run time is not much more than\nO(nk) for a CSP with n variables. Search the literature for methods for ﬁnding approximately\nminimal cycle cutsets in time that is polynomial in the size of the cutset. Does the existence\nof such algorithms make the cycle cutset method practical? 7\nLOGICAL AGENTS\nIn which we design agents that can form representations of a complex world, use a\nprocess of inference to derive new representations about the world, and use these\nnew representations to deduce what to do.\nHumans, it seems, know things; and what they know helps them do things.\nThese are\nnot empty statements. They make strong claims about how the intelligence of humans is\nachieved—not by purely reﬂex mechanisms but by processes of reasoning that operate on\nREASONING\ninternal representations of knowledge. In AI, this approach to intelligence is embodied in\nREPRESENTATION\nknowledge-based agents.\nKNOWLEDGE-BASED\nAGENTS\nThe problem-solving agents of Chapters 3 and 4 know things, but only in a very limited,\ninﬂexible sense. For example, the transition model for the 8-puzzle—knowledge of what the\nactions do—is hidden inside the domain-speciﬁc code of the RESULT function. It can be\nused to predict the outcome of actions but not to deduce that two tiles cannot occupy the\nsame space or that states with odd parity cannot be reached from states with even parity. The\natomic representations used by problem-solving agents are also very limiting. In a partially\nobservable environment, an agent’s only choice for representing what it knows about the\ncurrent state is to list all possible concrete states—a hopeless prospect in large environments.\nChapter 6 introduced the idea of representing states as assignments of values to vari-\nables; this is a step in the right direction, enabling some parts of the agent to work in a\ndomain-independent way and allowing for more efﬁcient algorithms. In this chapter and\nthose that follow, we take this step to its logical conclusion, so to speak—we develop logic\nLOGIC\nas a general class of representations to support knowledge-based agents. Such agents can",
  "those that follow, we take this step to its logical conclusion, so to speak—we develop logic\nLOGIC\nas a general class of representations to support knowledge-based agents. Such agents can\ncombine and recombine information to suit myriad purposes. Often, this process can be quite\nfar removed from the needs of the moment—as when a mathematician proves a theorem or\nan astronomer calculates the earth’s life expectancy. Knowledge-based agents can accept new\ntasks in the form of explicitly described goals; they can achieve competence quickly by being\ntold or learning new knowledge about the environment; and they can adapt to changes in the\nenvironment by updating the relevant knowledge.\nWe begin in Section 7.1 with the overall agent design. Section 7.2 introduces a sim-\nple new environment, the wumpus world, and illustrates the operation of a knowledge-based\nagent without going into any technical detail. Then we explain the general principles of logic\n234 Section 7.1.\nKnowledge-Based Agents\n235\nin Section 7.3 and the speciﬁcs of propositional logic in Section 7.4. While less expressive\nthan ﬁrst-order logic (Chapter 8), propositional logic illustrates all the basic concepts of\nlogic; it also comes with well-developed inference technologies, which we describe in sec-\ntions 7.5 and 7.6. Finally, Section 7.7 combines the concept of knowledge-based agents with\nthe technology of propositional logic to build some simple agents for the wumpus world.\n7.1\nKNOWLEDGE-BASED AGENTS\nThe central component of a knowledge-based agent is its knowledge base, or KB. A knowl-\nKNOWLEDGE BASE\nedge base is a set of sentences. (Here “sentence” is used as a technical term. It is related\nSENTENCE\nbut not identical to the sentences of English and other natural languages.) Each sentence is\nexpressed in a language called a knowledge representation language and represents some\nKNOWLEDGE\nREPRESENTATION\nLANGUAGE\nassertion about the world. Sometimes we dignify a sentence with the name axiom, when the\nAXIOM\nsentence is taken as given without being derived from other sentences.\nThere must be a way to add new sentences to the knowledge base and a way to query\nwhat is known. The standard names for these operations are TELL and ASK, respectively.\nBoth operations may involve inference—that is, deriving new sentences from old. Inference\nINFERENCE\nmust obey the requirement that when one ASKs a question of the knowledge base, the answer",
  "Both operations may involve inference—that is, deriving new sentences from old. Inference\nINFERENCE\nmust obey the requirement that when one ASKs a question of the knowledge base, the answer\nshould follow from what has been told (or TELLed) to the knowledge base previously. Later\nin this chapter, we will be more precise about the crucial word “follow.” For now, take it to\nmean that the inference process should not make things up as it goes along.\nFigure 7.1 shows the outline of a knowledge-based agent program. Like all our agents,\nit takes a percept as input and returns an action. The agent maintains a knowledge base, KB,\nwhich may initially contain some background knowledge.\nBACKGROUND\nKNOWLEDGE\nEach time the agent program is called, it does three things. First, it TELLs the knowl-\nedge base what it perceives. Second, it ASKs the knowledge base what action it should\nperform. In the process of answering this query, extensive reasoning may be done about\nthe current state of the world, about the outcomes of possible action sequences, and so on.\nThird, the agent program TELLs the knowledge base which action was chosen, and the agent\nexecutes the action.\nThe details of the representation language are hidden inside three functions that imple-\nment the interface between the sensors and actuators on one side and the core representation\nand reasoning system on the other. MAKE-PERCEPT-SENTENCE constructs a sentence as-\nserting that the agent perceived the given percept at the given time. MAKE-ACTION-QUERY\nconstructs a sentence that asks what action should be done at the current time. Finally,\nMAKE-ACTION-SENTENCE constructs a sentence asserting that the chosen action was ex-\necuted. The details of the inference mechanisms are hidden inside TELL and ASK. Later\nsections will reveal these details.\nThe agent in Figure 7.1 appears quite similar to the agents with internal state described\nin Chapter 2. Because of the deﬁnitions of TELL and ASK, however, the knowledge-based\nagent is not an arbitrary program for calculating actions. It is amenable to a description at 236\nChapter\n7.\nLogical Agents\nfunction KB-AGENT(percept) returns an action\npersistent: KB, a knowledge base\nt, a counter, initially 0, indicating time\nTELL(KB, MAKE-PERCEPT-SENTENCE(percept,t))\naction ←ASK(KB, MAKE-ACTION-QUERY(t))\nTELL(KB, MAKE-ACTION-SENTENCE(action,t))\nt ←t + 1\nreturn action\nFigure 7.1\nA generic knowledge-based agent. Given a percept, the agent adds the percept",
  "TELL(KB, MAKE-PERCEPT-SENTENCE(percept,t))\naction ←ASK(KB, MAKE-ACTION-QUERY(t))\nTELL(KB, MAKE-ACTION-SENTENCE(action,t))\nt ←t + 1\nreturn action\nFigure 7.1\nA generic knowledge-based agent. Given a percept, the agent adds the percept\nto its knowledge base, asks the knowledge base for the best action, and tells the knowledge\nbase that it has in fact taken that action.\nthe knowledge level, where we need specify only what the agent knows and what its goals\nKNOWLEDGE LEVEL\nare, in order to ﬁx its behavior. For example, an automated taxi might have the goal of\ntaking a passenger from San Francisco to Marin County and might know that the Golden\nGate Bridge is the only link between the two locations. Then we can expect it to cross the\nGolden Gate Bridge because it knows that that will achieve its goal. Notice that this analysis\nis independent of how the taxi works at the implementation level. It doesn’t matter whether\nIMPLEMENTATION\nLEVEL\nits geographical knowledge is implemented as linked lists or pixel maps, or whether it reasons\nby manipulating strings of symbols stored in registers or by propagating noisy signals in a\nnetwork of neurons.\nA knowledge-based agent can be built simply by TELLing it what it needs to know.\nStarting with an empty knowledge base, the agent designer can TELL sentences one by one\nuntil the agent knows how to operate in its environment. This is called the declarative ap-\nDECLARATIVE\nproach to system building. In contrast, the procedural approach encodes desired behaviors\ndirectly as program code. In the 1970s and 1980s, advocates of the two approaches engaged\nin heated debates. We now understand that a successful agent often combines both declarative\nand procedural elements in its design, and that declarative knowledge can often be compiled\ninto more efﬁcient procedural code.\nWe can also provide a knowledge-based agent with mechanisms that allow it to learn\nfor itself. These mechanisms, which are discussed in Chapter 18, create general knowledge\nabout the environment from a series of percepts. A learning agent can be fully autonomous.\n7.2\nTHE WUMPUS WORLD\nIn this section we describe an environment in which knowledge-based agents can show their\nworth. The wumpus world is a cave consisting of rooms connected by passageways. Lurking\nWUMPUS WORLD\nsomewhere in the cave is the terrible wumpus, a beast that eats anyone who enters its room.\nThe wumpus can be shot by an agent, but the agent has only one arrow. Some rooms contain Section 7.2.",
  "WUMPUS WORLD\nsomewhere in the cave is the terrible wumpus, a beast that eats anyone who enters its room.\nThe wumpus can be shot by an agent, but the agent has only one arrow. Some rooms contain Section 7.2.\nThe Wumpus World\n237\nbottomless pits that will trap anyone who wanders into these rooms (except for the wumpus,\nwhich is too big to fall in). The only mitigating feature of this bleak environment is the\npossibility of ﬁnding a heap of gold. Although the wumpus world is rather tame by modern\ncomputer game standards, it illustrates some important points about intelligence.\nA sample wumpus world is shown in Figure 7.2. The precise deﬁnition of the task\nenvironment is given, as suggested in Section 2.3, by the PEAS description:\n• Performance measure: +1000 for climbing out of the cave with the gold, –1000 for\nfalling into a pit or being eaten by the wumpus, –1 for each action taken and –10 for\nusing up the arrow. The game ends either when the agent dies or when the agent climbs\nout of the cave.\n• Environment: A 4 × 4 grid of rooms. The agent always starts in the square labeled\n[1,1], facing to the right. The locations of the gold and the wumpus are chosen ran-\ndomly, with a uniform distribution, from the squares other than the start square. In\naddition, each square other than the start can be a pit, with probability 0.2.\n• Actuators: The agent can move Forward, TurnLeft by 90◦, or TurnRight by 90◦. The\nagent dies a miserable death if it enters a square containing a pit or a live wumpus. (It\nis safe, albeit smelly, to enter a square with a dead wumpus.) If an agent tries to move\nforward and bumps into a wall, then the agent does not move. The action Grab can be\nused to pick up the gold if it is in the same square as the agent. The action Shoot can\nbe used to ﬁre an arrow in a straight line in the direction the agent is facing. The arrow\ncontinues until it either hits (and hence kills) the wumpus or hits a wall. The agent has\nonly one arrow, so only the ﬁrst Shoot action has any effect. Finally, the action Climb\ncan be used to climb out of the cave, but only from square [1,1].\n• Sensors: The agent has ﬁve sensors, each of which gives a single bit of information:\n– In the square containing the wumpus and in the directly (not diagonally) adjacent\nsquares, the agent will perceive a Stench.\n– In the squares directly adjacent to a pit, the agent will perceive a Breeze.\n– In the square where the gold is, the agent will perceive a Glitter.",
  "squares, the agent will perceive a Stench.\n– In the squares directly adjacent to a pit, the agent will perceive a Breeze.\n– In the square where the gold is, the agent will perceive a Glitter.\n– When an agent walks into a wall, it will perceive a Bump.\n– When the wumpus is killed, it emits a woeful Scream that can be perceived any-\nwhere in the cave.\nThe percepts will be given to the agent program in the form of a list of ﬁve symbols;\nfor example, if there is a stench and a breeze, but no glitter, bump, or scream, the agent\nprogram will get [Stench, Breeze, None, None, None].\nWe can characterize the wumpus environment along the various dimensions given in Chap-\nter 2. Clearly, it is discrete, static, and single-agent. (The wumpus doesn’t move, fortunately.)\nIt is sequential, because rewards may come only after many actions are taken. It is partially\nobservable, because some aspects of the state are not directly perceivable: the agent’s lo-\ncation, the wumpus’s state of health, and the availability of an arrow. As for the locations\nof the pits and the wumpus: we could treat them as unobserved parts of the state that hap-\npen to be immutable—in which case, the transition model for the environment is completely 238\nChapter\n7.\nLogical Agents\nPIT\n1\n2\n3\n4\n1\n2\n3\n4\nSTART\nStench\nStench\nB\nr\nee\nz\ne\nGold\nPIT\nPIT\nB\nr\nee\nz\ne\nB\nr\nee\nz\ne\nB\nr\nee\nz\ne\nB\nr\nee\nz\ne\nB\nr\nee\nz\ne\nStench\nFigure 7.2\nA typical wumpus world. The agent is in the bottom left corner, facing right.\nknown; or we could say that the transition model itself is unknown because the agent doesn’t\nknow which Forward actions are fatal—in which case, discovering the locations of pits and\nwumpus completes the agent’s knowledge of the transition model.\nFor an agent in the environment, the main challenge is its initial ignorance of the con-\nﬁguration of the environment; overcoming this ignorance seems to require logical reasoning.\nIn most instances of the wumpus world, it is possible for the agent to retrieve the gold safely.\nOccasionally, the agent must choose between going home empty-handed and risking death to\nﬁnd the gold. About 21% of the environments are utterly unfair, because the gold is in a pit\nor surrounded by pits.\nLet us watch a knowledge-based wumpus agent exploring the environment shown in\nFigure 7.2. We use an informal knowledge representation language consisting of writing\ndown symbols in a grid (as in Figures 7.3 and 7.4).",
  "or surrounded by pits.\nLet us watch a knowledge-based wumpus agent exploring the environment shown in\nFigure 7.2. We use an informal knowledge representation language consisting of writing\ndown symbols in a grid (as in Figures 7.3 and 7.4).\nThe agent’s initial knowledge base contains the rules of the environment, as described\npreviously; in particular, it knows that it is in [1,1] and that [1,1] is a safe square; we denote\nthat with an “A” and “OK,” respectively, in square [1,1].\nThe ﬁrst percept is [None, None, None, None, None], from which the agent can con-\nclude that its neighboring squares, [1,2] and [2,1], are free of dangers—they are OK. Fig-\nure 7.3(a) shows the agent’s state of knowledge at this point.\nA cautious agent will move only into a square that it knows to be OK. Let us suppose\nthe agent decides to move forward to [2,1]. The agent perceives a breeze (denoted by “B”) in\n[2,1], so there must be a pit in a neighboring square. The pit cannot be in [1,1], by the rules of\nthe game, so there must be a pit in [2,2] or [3,1] or both. The notation “P?” in Figure 7.3(b)\nindicates a possible pit in those squares. At this point, there is only one known square that is\nOK and that has not yet been visited. So the prudent agent will turn around, go back to [1,1],\nand then proceed to [1,2].\nThe agent perceives a stench in [1,2], resulting in the state of knowledge shown in\nFigure 7.4(a). The stench in [1,2] means that there must be a wumpus nearby. But the Section 7.2.\nThe Wumpus World\n239\nA\nB\nG\nP\nS\nW\n = Agent\n = Breeze\n = Glitter, Gold\n = Pit\n = Stench\n = Wumpus\nOK = Safe square\nV\n = Visited\nA\nOK\n 1,1\n 2,1\n 3,1\n 4,1\n 1,2\n 2,2\n 3,2\n 4,2\n 1,3\n 2,3\n 3,3\n 4,3\n 1,4\n 2,4\n 3,4\n 4,4\nOK\nOK\nB\nP?\nP?\nA\nOK\nOK\nOK\n 1,1\n 2,1\n 3,1\n 4,1\n 1,2\n 2,2\n 3,2\n 4,2\n 1,3\n 2,3\n 3,3\n 4,3\n 1,4\n 2,4\n 3,4\n 4,4\nV\n(a)\n(b)\nFigure 7.3\nThe ﬁrst step taken by the agent in the wumpus world. (a) The initial sit-\nuation, after percept [None, None, None, None, None]. (b) After one move, with percept\n[None, Breeze, None, None, None].\nB\nB\nP!\nA\nOK\nOK\nOK\n 1,1\n 2,1\n 3,1\n 4,1\n 1,2\n 2,2\n 3,2\n 4,2\n 1,3\n 2,3\n 3,3\n 4,3\n 1,4\n 2,4\n 3,4\n 4,4\nV\nOK\nW!\nV\nP!\nA\nOK\nOK\nOK\n 1,1\n 2,1\n 3,1\n 4,1\n 1,2\n 2,2\n 3,2\n 4,2\n 1,3\n 2,3\n 3,3\n 4,3\n 1,4\n 2,4\n 3,4\n 4,4\nV\nS\nOK\nW!\nV\nV\nV\nB\nS G\nP?\nP?\n(b)\n(a)\nS\nA\nB\nG\nP\nS\nW\n = Agent\n = Breeze\n = Glitter, Gold\n = Pit\n = Stench\n = Wumpus\nOK = Safe square\nV\n = Visited\nFigure 7.4\nTwo later stages in the progress of the agent.\n(a) After the third move,",
  "1,4\n 2,4\n 3,4\n 4,4\nV\nS\nOK\nW!\nV\nV\nV\nB\nS G\nP?\nP?\n(b)\n(a)\nS\nA\nB\nG\nP\nS\nW\n = Agent\n = Breeze\n = Glitter, Gold\n = Pit\n = Stench\n = Wumpus\nOK = Safe square\nV\n = Visited\nFigure 7.4\nTwo later stages in the progress of the agent.\n(a) After the third move,\nwith percept [Stench, None, None, None, None]. (b) After the ﬁfth move, with percept\n[Stench, Breeze, Glitter, None, None].\nwumpus cannot be in [1,1], by the rules of the game, and it cannot be in [2,2] (or the agent\nwould have detected a stench when it was in [2,1]). Therefore, the agent can infer that the\nwumpus is in [1,3]. The notation W! indicates this inference. Moreover, the lack of a breeze\nin [1,2] implies that there is no pit in [2,2]. Yet the agent has already inferred that there must\nbe a pit in either [2,2] or [3,1], so this means it must be in [3,1]. This is a fairly difﬁcult\ninference, because it combines knowledge gained at different times in different places and\nrelies on the lack of a percept to make one crucial step. 240\nChapter\n7.\nLogical Agents\nThe agent has now proved to itself that there is neither a pit nor a wumpus in [2,2], so it\nis OK to move there. We do not show the agent’s state of knowledge at [2,2]; we just assume\nthat the agent turns and moves to [2,3], giving us Figure 7.4(b). In [2,3], the agent detects a\nglitter, so it should grab the gold and then return home.\nNote that in each case for which the agent draws a conclusion from the available in-\nformation, that conclusion is guaranteed to be correct if the available information is correct.\nThis is a fundamental property of logical reasoning. In the rest of this chapter, we describe\nhow to build logical agents that can represent information and draw conclusions such as those\ndescribed in the preceding paragraphs.\n7.3\nLOGIC\nThis section summarizes the fundamental concepts of logical representation and reasoning.\nThese beautiful ideas are independent of any of logic’s particular forms. We therefore post-\npone the technical details of those forms until the next section, using instead the familiar\nexample of ordinary arithmetic.\nIn Section 7.1, we said that knowledge bases consist of sentences. These sentences\nare expressed according to the syntax of the representation language, which speciﬁes all the\nSYNTAX\nsentences that are well formed. The notion of syntax is clear enough in ordinary arithmetic:\n“x + y = 4” is a well-formed sentence, whereas “x4y+ =” is not.",
  "SYNTAX\nsentences that are well formed. The notion of syntax is clear enough in ordinary arithmetic:\n“x + y = 4” is a well-formed sentence, whereas “x4y+ =” is not.\nA logic must also deﬁne the semantics or meaning of sentences. The semantics deﬁnes\nSEMANTICS\nthe truth of each sentence with respect to each possible world. For example, the semantics\nTRUTH\nPOSSIBLE WORLD\nfor arithmetic speciﬁes that the sentence “x + y = 4” is true in a world where x is 2 and y\nis 2, but false in a world where x is 1 and y is 1. In standard logics, every sentence must be\neither true or false in each possible world—there is no “in between.”1\nWhen we need to be precise, we use the term model in place of “possible world.”\nMODEL\nWhereas possible worlds might be thought of as (potentially) real environments that the agent\nmight or might not be in, models are mathematical abstractions, each of which simply ﬁxes\nthe truth or falsehood of every relevant sentence. Informally, we may think of a possible world\nas, for example, having x men and y women sitting at a table playing bridge, and the sentence\nx + y = 4 is true when there are four people in total. Formally, the possible models are just\nall possible assignments of real numbers to the variables x and y. Each such assignment ﬁxes\nthe truth of any sentence of arithmetic whose variables are x and y. If a sentence α is true in\nmodel m, we say that m satisﬁes α or sometimes m is a model of α. We use the notation\nSATISFACTION\nM(α) to mean the set of all models of α.\nNow that we have a notion of truth, we are ready to talk about logical reasoning. This\ninvolves the relation of logical entailment between sentences—the idea that a sentence fol-\nENTAILMENT\nlows logically from another sentence. In mathematical notation, we write\nα |= β\n1 Fuzzy logic, discussed in Chapter 14, allows for degrees of truth. Section 7.3.\nLogic\n241\n1\n2\n3\n1\n2\nPIT\n1\n2\n3\n1\n2\nPIT\n1\n2\n3\n1\n2\nPIT\nPIT\nPIT\n1\n2\n3\n1\n2\nPIT\nPIT\n1\n2\n3\n1\n2\nPIT\n1\n2\n3\n1\n2\nPIT\nPIT\n1\n2\n3\n1\n2\nPIT\nPIT\n1\n2\n3\n1\n2\nKB\nα1\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\n1\n2\n3\n1\n2\nPIT\n1\n2\n3\n1\n2\nPIT\n1\n2\n3\n1\n2\nPIT\nPIT\nPIT\n1\n2\n3\n1\n2\nPIT\nPIT\n1\n2\n3\n1\n2\nPIT\n1\n2\n3\n1\n2\nPIT\nPIT\n1\n2\n3\n1\n2\nPIT\nPIT\n1\n2\n3\n1\n2\nKB\nB\nr\neez\ne\nα2\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\n(a)\n(b)\nFigure 7.5\nPossible models for the presence of pits in squares [1,2], [2,2], and [3,1]. The\nKB corresponding to the observations of nothing in [1,1] and a breeze in [2,1] is shown by",
  "r\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\nB\nr\neez\ne\n(a)\n(b)\nFigure 7.5\nPossible models for the presence of pits in squares [1,2], [2,2], and [3,1]. The\nKB corresponding to the observations of nothing in [1,1] and a breeze in [2,1] is shown by\nthe solid line. (a) Dotted line shows models of α1 (no pit in [1,2]). (b) Dotted line shows\nmodels of α2 (no pit in [2,2]).\nto mean that the sentence α entails the sentence β. The formal deﬁnition of entailment is this:\nα |= β if and only if, in every model in which α is true, β is also true. Using the notation just\nintroduced, we can write\nα |= β if and only if M(α) ⊆M(β) .\n(Note the direction of the ⊆here: if α |= β, then α is a stronger assertion than β: it rules out\nmore possible worlds.) The relation of entailment is familiar from arithmetic; we are happy\nwith the idea that the sentence x = 0 entails the sentence xy = 0. Obviously, in any model\nwhere x is zero, it is the case that xy is zero (regardless of the value of y).\nWe can apply the same kind of analysis to the wumpus-world reasoning example given\nin the preceding section. Consider the situation in Figure 7.3(b): the agent has detected\nnothing in [1,1] and a breeze in [2,1]. These percepts, combined with the agent’s knowledge\nof the rules of the wumpus world, constitute the KB. The agent is interested (among other\nthings) in whether the adjacent squares [1,2], [2,2], and [3,1] contain pits. Each of the three\nsquares might or might not contain a pit, so (for the purposes of this example) there are 23 = 8\npossible models. These eight models are shown in Figure 7.5.2\nThe KB can be thought of as a set of sentences or as a single sentence that asserts all\nthe individual sentences. The KB is false in models that contradict what the agent knows—\nfor example, the KB is false in any model in which [1,2] contains a pit, because there is\nno breeze in [1,1]. There are in fact just three models in which the KB is true, and these are\n2 Although the ﬁgure shows the models as partial wumpus worlds, they are really nothing more than assignments\nof true and false to the sentences “there is a pit in [1,2]” etc. Models, in the mathematical sense, do not need to\nhave ’orrible ’airy wumpuses in them. 242\nChapter\n7.\nLogical Agents\nshown surrounded by a solid line in Figure 7.5. Now let us consider two possible conclusions:\nα1 = “There is no pit in [1,2].”\nα2 = “There is no pit in [2,2].”",
  "have ’orrible ’airy wumpuses in them. 242\nChapter\n7.\nLogical Agents\nshown surrounded by a solid line in Figure 7.5. Now let us consider two possible conclusions:\nα1 = “There is no pit in [1,2].”\nα2 = “There is no pit in [2,2].”\nWe have surrounded the models of α1 and α2 with dotted lines in Figures 7.5(a) and 7.5(b),\nrespectively. By inspection, we see the following:\nin every model in which KB is true, α1 is also true.\nHence, KB |= α1: there is no pit in [1,2]. We can also see that\nin some models in which KB is true, α2 is false.\nHence, KB ̸|= α2: the agent cannot conclude that there is no pit in [2,2]. (Nor can it conclude\nthat there is a pit in [2,2].)3\nThe preceding example not only illustrates entailment but also shows how the deﬁnition\nof entailment can be applied to derive conclusions—that is, to carry out logical inference.\nLOGICAL INFERENCE\nThe inference algorithm illustrated in Figure 7.5 is called model checking, because it enu-\nMODEL CHECKING\nmerates all possible models to check that α is true in all models in which KB is true, that is,\nthat M(KB) ⊆M(α).\nIn understanding entailment and inference, it might help to think of the set of all conse-\nquences of KB as a haystack and of α as a needle. Entailment is like the needle being in the\nhaystack; inference is like ﬁnding it. This distinction is embodied in some formal notation: if\nan inference algorithm i can derive α from KB, we write\nKB ⊢i α ,\nwhich is pronounced “α is derived from KB by i” or “i derives α from KB.”\nAn inference algorithm that derives only entailed sentences is called sound or truth-\nSOUND\npreserving. Soundness is a highly desirable property. An unsound inference procedure es-\nTRUTH-PRESERVING\nsentially makes things up as it goes along—it announces the discovery of nonexistent needles.\nIt is easy to see that model checking, when it is applicable,4 is a sound procedure.\nThe property of completeness is also desirable: an inference algorithm is complete if\nCOMPLETENESS\nit can derive any sentence that is entailed. For real haystacks, which are ﬁnite in extent,\nit seems obvious that a systematic examination can always decide whether the needle is in\nthe haystack. For many knowledge bases, however, the haystack of consequences is inﬁnite,\nand completeness becomes an important issue.5 Fortunately, there are complete inference\nprocedures for logics that are sufﬁciently expressive to handle many knowledge bases.",
  "and completeness becomes an important issue.5 Fortunately, there are complete inference\nprocedures for logics that are sufﬁciently expressive to handle many knowledge bases.\nWe have described a reasoning process whose conclusions are guaranteed to be true\nin any world in which the premises are true; in particular, if KB is true in the real world,\nthen any sentence α derived from KB by a sound inference procedure is also true in the real\nworld. So, while an inference process operates on “syntax”—internal physical conﬁgurations\nsuch as bits in registers or patterns of electrical blips in brains—the process corresponds\n3 The agent can calculate the probability that there is a pit in [2,2]; Chapter 13 shows how.\n4 Model checking works if the space of models is ﬁnite—for example, in wumpus worlds of ﬁxed size. For\narithmetic, on the other hand, the space of models is inﬁnite: even if we restrict ourselves to the integers, there\nare inﬁnitely many pairs of values for x and y in the sentence x + y = 4.\n5 Compare with the case of inﬁnite search spaces in Chapter 3, where depth-ﬁrst search is not complete. Section 7.4.\nPropositional Logic: A Very Simple Logic\n243\nFollows\nSentences\nSentence\nEntails\nSemantics\nSemantics\nRepresentation\nWorld\nAspects of the\n    real world\nAspect of the\n   real world\nFigure 7.6\nSentences are physical conﬁgurations of the agent, and reasoning is a process\nof constructing new physical conﬁgurations from old ones. Logical reasoning should en-\nsure that the new conﬁgurations represent aspects of the world that actually follow from the\naspects that the old conﬁgurations represent.\nto the real-world relationship whereby some aspect of the real world is the case6 by virtue\nof other aspects of the real world being the case. This correspondence between world and\nrepresentation is illustrated in Figure 7.6.\nThe ﬁnal issue to consider is grounding—the connection between logical reasoning\nGROUNDING\nprocesses and the real environment in which the agent exists. In particular, how do we know\nthat KB is true in the real world? (After all, KB is just “syntax” inside the agent’s head.)\nThis is a philosophical question about which many, many books have been written. (See\nChapter 26.) A simple answer is that the agent’s sensors create the connection. For example,\nour wumpus-world agent has a smell sensor. The agent program creates a suitable sentence\nwhenever there is a smell. Then, whenever that sentence is in the knowledge base, it is",
  "our wumpus-world agent has a smell sensor. The agent program creates a suitable sentence\nwhenever there is a smell. Then, whenever that sentence is in the knowledge base, it is\ntrue in the real world. Thus, the meaning and truth of percept sentences are deﬁned by the\nprocesses of sensing and sentence construction that produce them. What about the rest of the\nagent’s knowledge, such as its belief that wumpuses cause smells in adjacent squares? This\nis not a direct representation of a single percept, but a general rule—derived, perhaps, from\nperceptual experience but not identical to a statement of that experience. General rules like\nthis are produced by a sentence construction process called learning, which is the subject\nof Part V. Learning is fallible. It could be the case that wumpuses cause smells except on\nFebruary 29 in leap years, which is when they take their baths. Thus, KB may not be true in\nthe real world, but with good learning procedures, there is reason for optimism.\n7.4\nPROPOSITIONAL LOGIC: A VERY SIMPLE LOGIC\nWe now present a simple but powerful logic called propositional logic. We cover the syntax\nPROPOSITIONAL\nLOGIC\nof propositional logic and its semantics—the way in which the truth of sentences is deter-\nmined. Then we look at entailment—the relation between a sentence and another sentence\nthat follows from it—and see how this leads to a simple algorithm for logical inference. Ev-\nerything takes place, of course, in the wumpus world.\n6 As Wittgenstein (1922) put it in his famous Tractatus: “The world is everything that is the case.” 244\nChapter\n7.\nLogical Agents\n7.4.1\nSyntax\nThe syntax of propositional logic deﬁnes the allowable sentences. The atomic sentences\nATOMIC SENTENCES\nconsist of a single proposition symbol. Each such symbol stands for a proposition that can\nPROPOSITION\nSYMBOL\nbe true or false. We use symbols that start with an uppercase letter and may contain other\nletters or subscripts, for example: P, Q, R, W1,3 and North. The names are arbitrary but\nare often chosen to have some mnemonic value—we use W1,3 to stand for the proposition\nthat the wumpus is in [1,3]. (Remember that symbols such as W1,3 are atomic, i.e., W, 1,\nand 3 are not meaningful parts of the symbol.) There are two proposition symbols with ﬁxed\nmeanings: True is the always-true proposition and False is the always-false proposition.\nComplex sentences are constructed from simpler sentences, using parentheses and logical\nCOMPLEX\nSENTENCES",
  "meanings: True is the always-true proposition and False is the always-false proposition.\nComplex sentences are constructed from simpler sentences, using parentheses and logical\nCOMPLEX\nSENTENCES\nconnectives. There are ﬁve connectives in common use:\nLOGICAL\nCONNECTIVES\n¬ (not). A sentence such as ¬W1,3 is called the negation of W1,3. A literal is either an\nNEGATION\nLITERAL\natomic sentence (a positive literal) or a negated atomic sentence (a negative literal).\n∧(and). A sentence whose main connective is ∧, such as W1,3 ∧P3,1, is called a con-\njunction; its parts are the conjuncts. (The ∧looks like an “A” for “And.”)\nCONJUNCTION\n∨(or). A sentence using ∨, such as (W1,3∧P3,1)∨W2,2, is a disjunction of the disjuncts\nDISJUNCTION\n(W1,3 ∧P3,1) and W2,2. (Historically, the ∨comes from the Latin “vel,” which means\n“or.” For most people, it is easier to remember ∨as an upside-down ∧.)\n⇒(implies). A sentence such as (W1,3 ∧P3,1) ⇒¬W2,2 is called an implication (or con-\nIMPLICATION\nditional). Its premise or antecedent is (W1,3 ∧P3,1), and its conclusion or consequent\nPREMISE\nCONCLUSION\nis ¬W2,2. Implications are also known as rules or if–then statements. The implication\nRULES\nsymbol is sometimes written in other books as ⊃or →.\n⇔(if and only if). The sentence W1,3 ⇔¬W2,2 is a biconditional. Some other books\nBICONDITIONAL\nwrite this as ≡.\nSentence\n→\nAtomicSentence | ComplexSentence\nAtomicSentence\n→\nTrue | False | P | Q | R | . . .\nComplexSentence\n→\n( Sentence ) | [ Sentence ]\n|\n¬ Sentence\n|\nSentence ∧Sentence\n|\nSentence ∨Sentence\n|\nSentence\n⇒Sentence\n|\nSentence\n⇔\nSentence\nOPERATOR PRECEDENCE\n:\n¬, ∧, ∨, ⇒, ⇔\nFigure 7.7\nA BNF (Backus–Naur Form) grammar of sentences in propositional logic,\nalong with operator precedences, from highest to lowest. Section 7.4.\nPropositional Logic: A Very Simple Logic\n245\nFigure 7.7 gives a formal grammar of propositional logic; see page 1060 if you are not\nfamiliar with the BNF notation. The BNF grammar by itself is ambiguous; a sentence with\nseveral operators can be parsed by the grammar in multiple ways. To eliminate the ambiguity\nwe deﬁne a precedence for each operator. The “not” operator (¬) has the highest precedence,\nwhich means that in the sentence ¬A ∧B the ¬ binds most tightly, giving us the equivalent\nof (¬A)∧B rather than ¬(A∧B). (The notation for ordinary arithmetic is the same: −2+4\nis 2, not –6.) When in doubt, use parentheses to make sure of the right interpretation. Square",
  "of (¬A)∧B rather than ¬(A∧B). (The notation for ordinary arithmetic is the same: −2+4\nis 2, not –6.) When in doubt, use parentheses to make sure of the right interpretation. Square\nbrackets mean the same thing as parentheses; the choice of square brackets or parentheses is\nsolely to make it easier for a human to read a sentence.\n7.4.2\nSemantics\nHaving speciﬁed the syntax of propositional logic, we now specify its semantics. The se-\nmantics deﬁnes the rules for determining the truth of a sentence with respect to a particular\nmodel. In propositional logic, a model simply ﬁxes the truth value—true or false—for ev-\nTRUTH VALUE\nery proposition symbol. For example, if the sentences in the knowledge base make use of the\nproposition symbols P1,2, P2,2, and P3,1, then one possible model is\nm1 = {P1,2 = false, P2,2 = false, P3,1 = true} .\nWith three proposition symbols, there are 23 = 8 possible models—exactly those depicted\nin Figure 7.5. Notice, however, that the models are purely mathematical objects with no\nnecessary connection to wumpus worlds. P1,2 is just a symbol; it might mean “there is a pit\nin [1,2]” or “I’m in Paris today and tomorrow.”\nThe semantics for propositional logic must specify how to compute the truth value of\nany sentence, given a model. This is done recursively. All sentences are constructed from\natomic sentences and the ﬁve connectives; therefore, we need to specify how to compute the\ntruth of atomic sentences and how to compute the truth of sentences formed with each of the\nﬁve connectives. Atomic sentences are easy:\n• True is true in every model and False is false in every model.\n• The truth value of every other proposition symbol must be speciﬁed directly in the\nmodel. For example, in the model m1 given earlier, P1,2 is false.\nFor complex sentences, we have ﬁve rules, which hold for any subsentences P and Q in any\nmodel m (here “iff” means “if and only if”):\n• ¬P is true iff P is false in m.\n• P ∧Q is true iff both P and Q are true in m.\n• P ∨Q is true iff either P or Q is true in m.\n• P ⇒Q is true unless P is true and Q is false in m.\n• P ⇔Q is true iff P and Q are both true or both false in m.\nThe rules can also be expressed with truth tables that specify the truth value of a complex\nTRUTH TABLE\nsentence for each possible assignment of truth values to its components. Truth tables for the\nﬁve connectives are given in Figure 7.8. From these tables, the truth value of any sentence s",
  "TRUTH TABLE\nsentence for each possible assignment of truth values to its components. Truth tables for the\nﬁve connectives are given in Figure 7.8. From these tables, the truth value of any sentence s\ncan be computed with respect to any model m by a simple recursive evaluation. For example, 246\nChapter\n7.\nLogical Agents\nP\nQ\n¬P\nP ∧Q\nP ∨Q\nP ⇒Q\nP ⇔Q\nfalse\nfalse\ntrue\nfalse\nfalse\ntrue\ntrue\nfalse\ntrue\ntrue\nfalse\ntrue\ntrue\nfalse\ntrue\nfalse\nfalse\nfalse\ntrue\nfalse\nfalse\ntrue\ntrue\nfalse\ntrue\ntrue\ntrue\ntrue\nFigure 7.8\nTruth tables for the ﬁve logical connectives. To use the table to compute, for\nexample, the value of P ∨Q when P is true and Q is false, ﬁrst look on the left for the row\nwhere P is true and Q is false (the third row). Then look in that row under the P ∨Q column\nto see the result: true.\nthe sentence ¬P1,2 ∧(P2,2 ∨P3,1), evaluated in m1, gives true ∧(false ∨true) = true ∧\ntrue = true. Exercise 7.3 asks you to write the algorithm PL-TRUE?(s, m), which computes\nthe truth value of a propositional logic sentence s in a model m.\nThe truth tables for “and,” “or,” and “not” are in close accord with our intuitions about\nthe English words. The main point of possible confusion is that P ∨Q is true when P is true\nor Q is true or both. A different connective, called “exclusive or” (“xor” for short), yields\nfalse when both disjuncts are true.7 There is no consensus on the symbol for exclusive or;\nsome choices are ˙∨or ̸= or ⊕.\nThe truth table for ⇒may not quite ﬁt one’s intuitive understanding of “P implies Q”\nor “if P then Q.” For one thing, propositional logic does not require any relation of causation\nor relevance between P and Q. The sentence “5 is odd implies Tokyo is the capital of Japan”\nis a true sentence of propositional logic (under the normal interpretation), even though it is\na decidedly odd sentence of English. Another point of confusion is that any implication is\ntrue whenever its antecedent is false. For example, “5 is even implies Sam is smart” is true,\nregardless of whether Sam is smart. This seems bizarre, but it makes sense if you think of\n“P ⇒Q” as saying, “If P is true, then I am claiming that Q is true. Otherwise I am making\nno claim.” The only way for this sentence to be false is if P is true but Q is false.\nThe biconditional, P ⇔Q, is true whenever both P ⇒Q and Q ⇒P are true. In\nEnglish, this is often written as “P if and only if Q.” Many of the rules of the wumpus world",
  "The biconditional, P ⇔Q, is true whenever both P ⇒Q and Q ⇒P are true. In\nEnglish, this is often written as “P if and only if Q.” Many of the rules of the wumpus world\nare best written using ⇔. For example, a square is breezy if a neighboring square has a pit,\nand a square is breezy only if a neighboring square has a pit. So we need a biconditional,\nB1,1 ⇔(P1,2 ∨P2,1) ,\nwhere B1,1 means that there is a breeze in [1,1].\n7.4.3\nA simple knowledge base\nNow that we have deﬁned the semantics for propositional logic, we can construct a knowledge\nbase for the wumpus world. We focus ﬁrst on the immutable aspects of the wumpus world,\nleaving the mutable aspects for a later section. For now, we need the following symbols for\neach [x, y] location:\n7 Latin has a separate word, aut, for exclusive or. Section 7.4.\nPropositional Logic: A Very Simple Logic\n247\nPx,y is true if there is a pit in [x, y].\nWx,y is true if there is a wumpus in [x, y], dead or alive.\nBx,y is true if the agent perceives a breeze in [x, y].\nSx,y is true if the agent perceives a stench in [x, y].\nThe sentences we write will sufﬁce to derive ¬P1,2 (there is no pit in [1,2]), as was done\ninformally in Section 7.3. We label each sentence Ri so that we can refer to them:\n• There is no pit in [1,1]:\nR1 :\n¬P1,1 .\n• A square is breezy if and only if there is a pit in a neighboring square. This has to be\nstated for each square; for now, we include just the relevant squares:\nR2 :\nB1,1\n⇔\n(P1,2 ∨P2,1) .\nR3 :\nB2,1\n⇔\n(P1,1 ∨P2,2 ∨P3,1) .\n• The preceding sentences are true in all wumpus worlds. Now we include the breeze\npercepts for the ﬁrst two squares visited in the speciﬁc world the agent is in, leading up\nto the situation in Figure 7.3(b).\nR4 :\n¬B1,1 .\nR5 :\nB2,1 .\n7.4.4\nA simple inference procedure\nOur goal now is to decide whether KB |= α for some sentence α. For example, is ¬P1,2\nentailed by our KB? Our ﬁrst algorithm for inference is a model-checking approach that is a\ndirect implementation of the deﬁnition of entailment: enumerate the models, and check that\nα is true in every model in which KB is true. Models are assignments of true or false to\nevery proposition symbol. Returning to our wumpus-world example, the relevant proposi-\ntion symbols are B1,1, B2,1, P1,1, P1,2, P2,1, P2,2, and P3,1. With seven symbols, there are\n27 = 128 possible models; in three of these, KB is true (Figure 7.9). In those three models,",
  "tion symbols are B1,1, B2,1, P1,1, P1,2, P2,1, P2,2, and P3,1. With seven symbols, there are\n27 = 128 possible models; in three of these, KB is true (Figure 7.9). In those three models,\n¬P1,2 is true, hence there is no pit in [1,2]. On the other hand, P2,2 is true in two of the three\nmodels and false in one, so we cannot yet tell whether there is a pit in [2,2].\nFigure 7.9 reproduces in a more precise form the reasoning illustrated in Figure 7.5. A\ngeneral algorithm for deciding entailment in propositional logic is shown in Figure 7.10. Like\nthe BACKTRACKING-SEARCH algorithm on page 215, TT-ENTAILS? performs a recursive\nenumeration of a ﬁnite space of assignments to symbols. The algorithm is sound because it\nimplements directly the deﬁnition of entailment, and complete because it works for any KB\nand α and always terminates—there are only ﬁnitely many models to examine.\nOf course, “ﬁnitely many” is not always the same as “few.” If KB and α contain n\nsymbols in all, then there are 2n models. Thus, the time complexity of the algorithm is\nO(2n). (The space complexity is only O(n) because the enumeration is depth-ﬁrst.) Later in\nthis chapter we show algorithms that are much more efﬁcient in many cases. Unfortunately,\npropositional entailment is co-NP-complete (i.e., probably no easier than NP-complete—see\nAppendix A), so every known inference algorithm for propositional logic has a worst-case\ncomplexity that is exponential in the size of the input. 248\nChapter\n7.\nLogical Agents\nB1,1\nB2,1\nP1,1\nP1,2\nP2,1\nP2,2\nP3,1\nR1\nR2\nR3\nR4\nR5\nKB\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\nfalse\ntrue\nfalse\nfalse\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\nfalse\ntrue\ntrue\nfalse\nfalse\ntrue\nfalse\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\nfalse\ntrue\nfalse\nfalse\nfalse\ntrue\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\nfalse\ntrue\nfalse\nfalse\nfalse\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\nfalse\ntrue\nfalse\nfalse\ntrue\nfalse\nfalse\ntrue\nfalse\nfalse\ntrue\ntrue\nfalse\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\ntrue\nfalse\ntrue\ntrue\nfalse\ntrue\nfalse\nFigure 7.9\nA truth table constructed for the knowledge base given in the text. KB is true\nif R1 through R5 are true, which occurs in just 3 of the 128 rows (the ones underlined in the",
  "true\ntrue\ntrue\ntrue\ntrue\ntrue\nfalse\ntrue\ntrue\nfalse\ntrue\nfalse\nFigure 7.9\nA truth table constructed for the knowledge base given in the text. KB is true\nif R1 through R5 are true, which occurs in just 3 of the 128 rows (the ones underlined in the\nright-hand column). In all 3 rows, P1,2 is false, so there is no pit in [1,2]. On the other hand,\nthere might (or might not) be a pit in [2,2].\nfunction TT-ENTAILS?(KB,α) returns true or false\ninputs: KB, the knowledge base, a sentence in propositional logic\nα, the query, a sentence in propositional logic\nsymbols ←a list of the proposition symbols in KB and α\nreturn TT-CHECK-ALL(KB,α,symbols,{ })\nfunction TT-CHECK-ALL(KB,α,symbols,model) returns true or false\nif EMPTY?(symbols) then\nif PL-TRUE?(KB,model) then return PL-TRUE?(α,model)\nelse return true // when KB is false, always return true\nelse do\nP ←FIRST(symbols)\nrest ←REST(symbols)\nreturn (TT-CHECK-ALL(KB,α,rest,model ∪{P = true})\nand\nTT-CHECK-ALL(KB,α,rest,model ∪{P = false }))\nFigure 7.10\nA truth-table enumeration algorithm for deciding propositional entailment.\n(TT stands for truth table.) PL-TRUE? returns true if a sentence holds within a model. The\nvariable model represents a partial model—an assignment to some of the symbols. The key-\nword “and” is used here as a logical operation on its two arguments, returning true or false. Section 7.5.\nPropositional Theorem Proving\n249\n(α ∧β) ≡(β ∧α)\ncommutativity of ∧\n(α ∨β) ≡(β ∨α)\ncommutativity of ∨\n((α ∧β) ∧γ) ≡(α ∧(β ∧γ))\nassociativity of ∧\n((α ∨β) ∨γ) ≡(α ∨(β ∨γ))\nassociativity of ∨\n¬(¬α) ≡α\ndouble-negation elimination\n(α ⇒β) ≡(¬β ⇒¬α)\ncontraposition\n(α ⇒β) ≡(¬α ∨β)\nimplication elimination\n(α ⇔β) ≡((α ⇒β) ∧(β ⇒α))\nbiconditional elimination\n¬(α ∧β) ≡(¬α ∨¬β)\nDe Morgan\n¬(α ∨β) ≡(¬α ∧¬β)\nDe Morgan\n(α ∧(β ∨γ)) ≡((α ∧β) ∨(α ∧γ))\ndistributivity of ∧over ∨\n(α ∨(β ∧γ)) ≡((α ∨β) ∧(α ∨γ))\ndistributivity of ∨over ∧\nFigure 7.11\nStandard logical equivalences. The symbols α, β, and γ stand for arbitrary\nsentences of propositional logic.\n7.5\nPROPOSITIONAL THEOREM PROVING\nSo far, we have shown how to determine entailment by model checking: enumerating models\nand showing that the sentence must hold in all models. In this section, we show how entail-\nment can be done by theorem proving—applying rules of inference directly to the sentences\nTHEOREM PROVING\nin our knowledge base to construct a proof of the desired sentence without consulting models.",
  "ment can be done by theorem proving—applying rules of inference directly to the sentences\nTHEOREM PROVING\nin our knowledge base to construct a proof of the desired sentence without consulting models.\nIf the number of models is large but the length of the proof is short, then theorem proving can\nbe more efﬁcient than model checking.\nBefore we plunge into the details of theorem-proving algorithms, we will need some\nadditional concepts related to entailment. The ﬁrst concept is logical equivalence: two sen-\nLOGICAL\nEQUIVALENCE\ntences α and β are logically equivalent if they are true in the same set of models. We write\nthis as α ≡β. For example, we can easily show (using truth tables) that P ∧Q and Q ∧P\nare logically equivalent; other equivalences are shown in Figure 7.11. These equivalences\nplay much the same role in logic as arithmetic identities do in ordinary mathematics. An\nalternative deﬁnition of equivalence is as follows: any two sentences α and β are equivalent\nonly if each of them entails the other:\nα ≡β\nif and only if\nα |= β and β |= α .\nThe second concept we will need is validity. A sentence is valid if it is true in all models. For\nVALIDITY\nexample, the sentence P ∨¬P is valid. Valid sentences are also known as tautologies—they\nTAUTOLOGY\nare necessarily true. Because the sentence True is true in all models, every valid sentence\nis logically equivalent to True. What good are valid sentences? From our deﬁnition of\nentailment, we can derive the deduction theorem, which was known to the ancient Greeks:\nDEDUCTION\nTHEOREM\nFor any sentences α and β, α |= β if and only if the sentence (α ⇒β) is valid.\n(Exercise 7.5 asks for a proof.) Hence, we can decide if α |= β by checking that (α ⇒β) is\ntrue in every model—which is essentially what the inference algorithm in Figure 7.10 does— 250\nChapter\n7.\nLogical Agents\nor by proving that (α ⇒β) is equivalent to True. Conversely, the deduction theorem states\nthat every valid implication sentence describes a legitimate inference.\nThe ﬁnal concept we will need is satisﬁability. A sentence is satisﬁable if it is true\nSATISFIABILITY\nin, or satisﬁed by, some model. For example, the knowledge base given earlier, (R1 ∧R2 ∧\nR3 ∧R4 ∧R5), is satisﬁable because there are three models in which it is true, as shown\nin Figure 7.9. Satisﬁability can be checked by enumerating the possible models until one is\nfound that satisﬁes the sentence. The problem of determining the satisﬁability of sentences",
  "in Figure 7.9. Satisﬁability can be checked by enumerating the possible models until one is\nfound that satisﬁes the sentence. The problem of determining the satisﬁability of sentences\nin propositional logic—the SAT problem—was the ﬁrst problem proved to be NP-complete.\nSAT\nMany problems in computer science are really satisﬁability problems. For example, all the\nconstraint satisfaction problems in Chapter 6 ask whether the constraints are satisﬁable by\nsome assignment.\nValidity and satisﬁability are of course connected: α is valid iff ¬α is unsatisﬁable;\ncontrapositively, α is satisﬁable iff ¬α is not valid. We also have the following useful result:\nα |= β if and only if the sentence (α ∧¬β) is unsatisﬁable.\nProving β from α by checking the unsatisﬁability of (α ∧¬β) corresponds exactly to the\nstandard mathematical proof technique of reductio ad absurdum (literally, “reduction to an\nREDUCTIO AD\nABSURDUM\nabsurd thing”). It is also called proof by refutation or proof by contradiction. One assumes a\nREFUTATION\nCONTRADICTION\nsentence β to be false and shows that this leads to a contradiction with known axioms α. This\ncontradiction is exactly what is meant by saying that the sentence (α ∧¬β) is unsatisﬁable.\n7.5.1\nInference and proofs\nThis section covers inference rules that can be applied to derive a proof—a chain of conclu-\nINFERENCE RULES\nPROOF\nsions that leads to the desired goal. The best-known rule is called Modus Ponens (Latin for\nMODUS PONENS\nmode that afﬁrms) and is written\nα ⇒β,\nα\nβ\n.\nThe notation means that, whenever any sentences of the form α ⇒β and α are given, then\nthe sentence β can be inferred. For example, if (WumpusAhead ∧WumpusAlive) ⇒Shoot\nand (WumpusAhead ∧WumpusAlive) are given, then Shoot can be inferred.\nAnother useful inference rule is And-Elimination, which says that, from a conjunction,\nAND-ELIMINATION\nany of the conjuncts can be inferred:\nα ∧β\nα\n.\nFor example, from (WumpusAhead ∧WumpusAlive), WumpusAlive can be inferred.\nBy considering the possible truth values of α and β, one can show easily that Modus\nPonens and And-Elimination are sound once and for all. These rules can then be used in\nany particular instances where they apply, generating sound inferences without the need for\nenumerating models.\nAll of the logical equivalences in Figure 7.11 can be used as inference rules. For exam-\nple, the equivalence for biconditional elimination yields the two inference rules\nα ⇔β\n(α ⇒β) ∧(β ⇒α)\nand\n(α ⇒β) ∧(β ⇒α)\nα ⇔β\n. Section 7.5.",
  "enumerating models.\nAll of the logical equivalences in Figure 7.11 can be used as inference rules. For exam-\nple, the equivalence for biconditional elimination yields the two inference rules\nα ⇔β\n(α ⇒β) ∧(β ⇒α)\nand\n(α ⇒β) ∧(β ⇒α)\nα ⇔β\n. Section 7.5.\nPropositional Theorem Proving\n251\nNot all inference rules work in both directions like this. For example, we cannot run Modus\nPonens in the opposite direction to obtain α ⇒β and α from β.\nLet us see how these inference rules and equivalences can be used in the wumpus world.\nWe start with the knowledge base containing R1 through R5 and show how to prove ¬P1,2,\nthat is, there is no pit in [1,2]. First, we apply biconditional elimination to R2 to obtain\nR6 :\n(B1,1 ⇒(P1,2 ∨P2,1)) ∧((P1,2 ∨P2,1) ⇒B1,1) .\nThen we apply And-Elimination to R6 to obtain\nR7 :\n((P1,2 ∨P2,1) ⇒B1,1) .\nLogical equivalence for contrapositives gives\nR8 :\n(¬B1,1 ⇒¬(P1,2 ∨P2,1)) .\nNow we can apply Modus Ponens with R8 and the percept R4 (i.e., ¬B1,1), to obtain\nR9 :\n¬(P1,2 ∨P2,1) .\nFinally, we apply De Morgan’s rule, giving the conclusion\nR10 :\n¬P1,2 ∧¬P2,1 .\nThat is, neither [1,2] nor [2,1] contains a pit.\nWe found this proof by hand, but we can apply any of the search algorithms in Chapter 3\nto ﬁnd a sequence of steps that constitutes a proof. We just need to deﬁne a proof problem as\nfollows:\n• INITIAL STATE: the initial knowledge base.\n• ACTIONS: the set of actions consists of all the inference rules applied to all the sen-\ntences that match the top half of the inference rule.\n• RESULT: the result of an action is to add the sentence in the bottom half of the inference\nrule.\n• GOAL: the goal is a state that contains the sentence we are trying to prove.\nThus, searching for proofs is an alternative to enumerating models. In many practical cases\nﬁnding a proof can be more efﬁcient because the proof can ignore irrelevant propositions, no\nmatter how many of them there are. For example, the proof given earlier leading to ¬P1,2 ∧\n¬P2,1 does not mention the propositions B2,1, P1,1, P2,2, or P3,1. They can be ignored\nbecause the goal proposition, P1,2, appears only in sentence R2; the other propositions in R2\nappear only in R4 and R2; so R1, R3, and R5 have no bearing on the proof. The same would\nhold even if we added a million more sentences to the knowledge base; the simple truth-table\nalgorithm, on the other hand, would be overwhelmed by the exponential explosion of models.",
  "hold even if we added a million more sentences to the knowledge base; the simple truth-table\nalgorithm, on the other hand, would be overwhelmed by the exponential explosion of models.\nOne ﬁnal property of logical systems is monotonicity, which says that the set of en-\nMONOTONICITY\ntailed sentences can only increase as information is added to the knowledge base.8 For any\nsentences α and β,\nif\nKB |= α\nthen\nKB ∧β |= α .\n8 Nonmonotonic logics, which violate the monotonicity property, capture a common property of human rea-\nsoning: changing one’s mind. They are discussed in Section 12.6. 252\nChapter\n7.\nLogical Agents\nFor example, suppose the knowledge base contains the additional assertion β stating that there\nare exactly eight pits in the world. This knowledge might help the agent draw additional con-\nclusions, but it cannot invalidate any conclusion α already inferred—such as the conclusion\nthat there is no pit in [1,2]. Monotonicity means that inference rules can be applied whenever\nsuitable premises are found in the knowledge base—the conclusion of the rule must follow\nregardless of what else is in the knowledge base.\n7.5.2\nProof by resolution\nWe have argued that the inference rules covered so far are sound, but we have not discussed\nthe question of completeness for the inference algorithms that use them. Search algorithms\nsuch as iterative deepening search (page 89) are complete in the sense that they will ﬁnd\nany reachable goal, but if the available inference rules are inadequate, then the goal is not\nreachable—no proof exists that uses only those inference rules. For example, if we removed\nthe biconditional elimination rule, the proof in the preceding section would not go through.\nThe current section introduces a single inference rule, resolution, that yields a complete\ninference algorithm when coupled with any complete search algorithm.\nWe begin by using a simple version of the resolution rule in the wumpus world. Let us\nconsider the steps leading up to Figure 7.4(a): the agent returns from [2,1] to [1,1] and then\ngoes to [1,2], where it perceives a stench, but no breeze. We add the following facts to the\nknowledge base:\nR11 :\n¬B1,2 .\nR12 :\nB1,2 ⇔(P1,1 ∨P2,2 ∨P1,3) .\nBy the same process that led to R10 earlier, we can now derive the absence of pits in [2,2]\nand [1,3] (remember that [1,1] is already known to be pitless):\nR13 :\n¬P2,2 .\nR14 :\n¬P1,3 .\nWe can also apply biconditional elimination to R3, followed by Modus Ponens with R5, to",
  "and [1,3] (remember that [1,1] is already known to be pitless):\nR13 :\n¬P2,2 .\nR14 :\n¬P1,3 .\nWe can also apply biconditional elimination to R3, followed by Modus Ponens with R5, to\nobtain the fact that there is a pit in [1,1], [2,2], or [3,1]:\nR15 :\nP1,1 ∨P2,2 ∨P3,1 .\nNow comes the ﬁrst application of the resolution rule: the literal ¬P2,2 in R13 resolves with\nthe literal P2,2 in R15 to give the resolvent\nRESOLVENT\nR16 :\nP1,1 ∨P3,1 .\nIn English; if there’s a pit in one of [1,1], [2,2], and [3,1] and it’s not in [2,2], then it’s in [1,1]\nor [3,1]. Similarly, the literal ¬P1,1 in R1 resolves with the literal P1,1 in R16 to give\nR17 :\nP3,1 .\nIn English: if there’s a pit in [1,1] or [3,1] and it’s not in [1,1], then it’s in [3,1]. These last\ntwo inference steps are examples of the unit resolution inference rule,\nUNIT RESOLUTION\nℓ1 ∨· · · ∨ℓk,\nm\nℓ1 ∨· · · ∨ℓi−1 ∨ℓi+1 ∨· · · ∨ℓk\n,\nwhere each ℓis a literal and ℓi and m are complementary literals (i.e., one is the negation\nCOMPLEMENTARY\nLITERALS Section 7.5.\nPropositional Theorem Proving\n253\nof the other). Thus, the unit resolution rule takes a clause—a disjunction of literals—and a\nCLAUSE\nliteral and produces a new clause. Note that a single literal can be viewed as a disjunction of\none literal, also known as a unit clause.\nUNIT CLAUSE\nThe unit resolution rule can be generalized to the full resolution rule,\nRESOLUTION\nℓ1 ∨· · · ∨ℓk,\nm1 ∨· · · ∨mn\nℓ1 ∨· · · ∨ℓi−1 ∨ℓi+1 ∨· · · ∨ℓk ∨m1 ∨· · · ∨mj−1 ∨mj+1 ∨· · · ∨mn\n,\nwhere ℓi and mj are complementary literals. This says that resolution takes two clauses and\nproduces a new clause containing all the literals of the two original clauses except the two\ncomplementary literals. For example, we have\nP1,1 ∨P3,1,\n¬P1,1 ∨¬P2,2\nP3,1 ∨¬P2,2\n.\nThere is one more technical aspect of the resolution rule: the resulting clause should contain\nonly one copy of each literal.9 The removal of multiple copies of literals is called factoring.\nFACTORING\nFor example, if we resolve (A ∨B) with (A ∨¬B), we obtain (A ∨A), which is reduced to\njust A.\nThe soundness of the resolution rule can be seen easily by considering the literal ℓi that\nis complementary to literal mj in the other clause. If ℓi is true, then mj is false, and hence\nm1 ∨· · · ∨mj−1 ∨mj+1 ∨· · · ∨mn must be true, because m1 ∨· · · ∨mn is given. If ℓi is\nfalse, then ℓ1 ∨· · · ∨ℓi−1 ∨ℓi+1 ∨· · · ∨ℓk must be true because ℓ1 ∨· · · ∨ℓk is given. Now",
  "m1 ∨· · · ∨mj−1 ∨mj+1 ∨· · · ∨mn must be true, because m1 ∨· · · ∨mn is given. If ℓi is\nfalse, then ℓ1 ∨· · · ∨ℓi−1 ∨ℓi+1 ∨· · · ∨ℓk must be true because ℓ1 ∨· · · ∨ℓk is given. Now\nℓi is either true or false, so one or other of these conclusions holds—exactly as the resolution\nrule states.\nWhat is more surprising about the resolution rule is that it forms the basis for a family\nof complete inference procedures. A resolution-based theorem prover can, for any sentences\nα and β in propositional logic, decide whether α |= β. The next two subsections explain\nhow resolution accomplishes this.\nConjunctive normal form\nThe resolution rule applies only to clauses (that is, disjunctions of literals), so it would seem\nto be relevant only to knowledge bases and queries consisting of clauses. How, then, can\nit lead to a complete inference procedure for all of propositional logic? The answer is that\nevery sentence of propositional logic is logically equivalent to a conjunction of clauses. A\nsentence expressed as a conjunction of clauses is said to be in conjunctive normal form or\nCONJUNCTIVE\nNORMAL FORM\nCNF (see Figure 7.14). We now describe a procedure for converting to CNF. We illustrate\nthe procedure by converting the sentence B1,1 ⇔(P1,2 ∨P2,1) into CNF. The steps are as\nfollows:\n1. Eliminate ⇔, replacing α ⇔β with (α ⇒β) ∧(β ⇒α).\n(B1,1 ⇒(P1,2 ∨P2,1)) ∧((P1,2 ∨P2,1) ⇒B1,1) .\n2. Eliminate ⇒, replacing α ⇒β with ¬α ∨β:\n(¬B1,1 ∨P1,2 ∨P2,1) ∧(¬(P1,2 ∨P2,1) ∨B1,1) .\n9 If a clause is viewed as a set of literals, then this restriction is automatically respected. Using set notation for\nclauses makes the resolution rule much cleaner, at the cost of introducing additional notation. 254\nChapter\n7.\nLogical Agents\n3. CNF requires ¬ to appear only in literals, so we “move ¬ inwards” by repeated appli-\ncation of the following equivalences from Figure 7.11:\n¬(¬α) ≡α (double-negation elimination)\n¬(α ∧β) ≡(¬α ∨¬β) (De Morgan)\n¬(α ∨β) ≡(¬α ∧¬β) (De Morgan)\nIn the example, we require just one application of the last rule:\n(¬B1,1 ∨P1,2 ∨P2,1) ∧((¬P1,2 ∧¬P2,1) ∨B1,1) .\n4. Now we have a sentence containing nested ∧and ∨operators applied to literals. We\napply the distributivity law from Figure 7.11, distributing ∨over ∧wherever possible.\n(¬B1,1 ∨P1,2 ∨P2,1) ∧(¬P1,2 ∨B1,1) ∧(¬P2,1 ∨B1,1) .\nThe original sentence is now in CNF, as a conjunction of three clauses. It is much harder to\nread, but it can be used as input to a resolution procedure.\nA resolution algorithm",
  "(¬B1,1 ∨P1,2 ∨P2,1) ∧(¬P1,2 ∨B1,1) ∧(¬P2,1 ∨B1,1) .\nThe original sentence is now in CNF, as a conjunction of three clauses. It is much harder to\nread, but it can be used as input to a resolution procedure.\nA resolution algorithm\nInference procedures based on resolution work by using the principle of proof by contradic-\ntion introduced on page 250. That is, to show that KB |= α, we show that (KB ∧¬α) is\nunsatisﬁable. We do this by proving a contradiction.\nA resolution algorithm is shown in Figure 7.12. First, (KB ∧¬α) is converted into\nCNF. Then, the resolution rule is applied to the resulting clauses. Each pair that contains\ncomplementary literals is resolved to produce a new clause, which is added to the set if it is\nnot already present. The process continues until one of two things happens:\n• there are no new clauses that can be added, in which case KB does not entail α; or,\n• two clauses resolve to yield the empty clause, in which case KB entails α.\nThe empty clause—a disjunction of no disjuncts—is equivalent to False because a disjunction\nis true only if at least one of its disjuncts is true. Another way to see that an empty clause\nrepresents a contradiction is to observe that it arises only from resolving two complementary\nunit clauses such as P and ¬P.\nWe can apply the resolution procedure to a very simple inference in the wumpus world.\nWhen the agent is in [1,1], there is no breeze, so there can be no pits in neighboring squares.\nThe relevant knowledge base is\nKB = R2 ∧R4 = (B1,1 ⇔(P1,2 ∨P2,1)) ∧¬B1,1\nand we wish to prove α which is, say, ¬P1,2. When we convert (KB ∧¬α) into CNF, we\nobtain the clauses shown at the top of Figure 7.13. The second row of the ﬁgure shows\nclauses obtained by resolving pairs in the ﬁrst row. Then, when P1,2 is resolved with ¬P1,2,\nwe obtain the empty clause, shown as a small square. Inspection of Figure 7.13 reveals that\nmany resolution steps are pointless. For example, the clause B1,1 ∨¬B1,1 ∨P1,2 is equivalent\nto True ∨P1,2 which is equivalent to True. Deducing that True is true is not very helpful.\nTherefore, any clause in which two complementary literals appear can be discarded. Section 7.5.\nPropositional Theorem Proving\n255\nfunction PL-RESOLUTION(KB,α) returns true or false\ninputs: KB, the knowledge base, a sentence in propositional logic\nα, the query, a sentence in propositional logic\nclauses ←the set of clauses in the CNF representation of KB ∧¬α\nnew ←{ }\nloop do\nfor each pair of clauses Ci, Cj in clauses do",
  "inputs: KB, the knowledge base, a sentence in propositional logic\nα, the query, a sentence in propositional logic\nclauses ←the set of clauses in the CNF representation of KB ∧¬α\nnew ←{ }\nloop do\nfor each pair of clauses Ci, Cj in clauses do\nresolvents ←PL-RESOLVE(Ci,Cj)\nif resolvents contains the empty clause then return true\nnew ←new ∪resolvents\nif new ⊆clauses then return false\nclauses ←clauses ∪new\nFigure 7.12\nA simple resolution algorithm for propositional logic.\nThe function\nPL-RESOLVE returns the set of all possible clauses obtained by resolving its two inputs.\n¬P2,1     B1,1\n¬B1,1    P1,2     P2,1\n¬P1,2    B1,1\n¬B1,1\nP1,2\n¬P2,1\n¬P1,2\nP1,2    P2,1     ¬P2,1\n¬B1,1     P2,1     B1,1\nP1,2    P2,1    ¬P1,2\n¬B1,1    P1,2     B1,1\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\nFigure 7.13\nPartial application of PL-RESOLUTION to a simple inference in the wumpus\nworld. ¬P1,2 is shown to follow from the ﬁrst four clauses in the top row.\nCompleteness of resolution\nTo conclude our discussion of resolution, we now show why PL-RESOLUTION is complete.\nTo do this, we introduce the resolution closure RC (S) of a set of clauses S, which is the set\nRESOLUTION\nCLOSURE\nof all clauses derivable by repeated application of the resolution rule to clauses in S or their\nderivatives. The resolution closure is what PL-RESOLUTION computes as the ﬁnal value of\nthe variable clauses. It is easy to see that RC (S) must be ﬁnite, because there are only ﬁnitely\nmany distinct clauses that can be constructed out of the symbols P1, . . . , Pk that appear in S.\n(Notice that this would not be true without the factoring step that removes multiple copies of\nliterals.) Hence, PL-RESOLUTION always terminates.\nThe completeness theorem for resolution in propositional logic is called the ground\nresolution theorem:\nGROUND\nRESOLUTION\nTHEOREM\nIf a set of clauses is unsatisﬁable, then the resolution closure of those clauses\ncontains the empty clause.\nThis theorem is proved by demonstrating its contrapositive: if the closure RC(S) does not 256\nChapter\n7.\nLogical Agents\ncontain the empty clause, then S is satisﬁable. In fact, we can construct a model for S with\nsuitable truth values for P1, . . . , Pk. The construction procedure is as follows:\nFor i from 1 to k,\n– If a clause in RC(S) contains the literal ¬Pi and all its other literals are false under\nthe assignment chosen for P1, . . . , Pi−1, then assign false to Pi.\n– Otherwise, assign true to Pi.",
  "For i from 1 to k,\n– If a clause in RC(S) contains the literal ¬Pi and all its other literals are false under\nthe assignment chosen for P1, . . . , Pi−1, then assign false to Pi.\n– Otherwise, assign true to Pi.\nThis assignment to P1, . . . , Pk is a model of S. To see this, assume the opposite—that, at\nsome stage i in the sequence, assigning symbol Pi causes some clause C to become false.\nFor this to happen, it must be the case that all the other literals in C must already have been\nfalsiﬁed by assignments to P1, . . . , Pi−1. Thus, C must now look like either (false ∨false ∨\n· · · false∨Pi) or like (false∨false∨· · · false∨¬Pi). If just one of these two is in RC(S), then\nthe algorithm will assign the appropriate truth value to Pi to make C true, so C can only be\nfalsiﬁed if both of these clauses are in RC(S). Now, since RC(S) is closed under resolution,\nit will contain the resolvent of these two clauses, and that resolvent will have all of its literals\nalready falsiﬁed by the assignments to P1, . . . , Pi−1. This contradicts our assumption that\nthe ﬁrst falsiﬁed clause appears at stage i. Hence, we have proved that the construction never\nfalsiﬁes a clause in RC(S); that is, it produces a model of RC(S) and thus a model of S\nitself (since S is contained in RC(S)).\n7.5.3\nHorn clauses and deﬁnite clauses\nThe completeness of resolution makes it a very important inference method. In many practical\nsituations, however, the full power of resolution is not needed. Some real-world knowledge\nbases satisfy certain restrictions on the form of sentences they contain, which enables them\nto use a more restricted and efﬁcient inference algorithm.\nOne such restricted form is the deﬁnite clause, which is a disjunction of literals of\nDEFINITE CLAUSE\nwhich exactly one is positive. For example, the clause (¬L1,1 ∨¬Breeze ∨B1,1) is a deﬁnite\nclause, whereas (¬B1,1 ∨P1,2 ∨P2,1) is not.\nSlightly more general is the Horn clause, which is a disjunction of literals of which at\nHORN CLAUSE\nmost one is positive. So all deﬁnite clauses are Horn clauses, as are clauses with no positive\nliterals; these are called goal clauses. Horn clauses are closed under resolution: if you resolve\nGOAL CLAUSES\ntwo Horn clauses, you get back a Horn clause.\nKnowledge bases containing only deﬁnite clauses are interesting for three reasons:\n1. Every deﬁnite clause can be written as an implication whose premise is a conjunction",
  "GOAL CLAUSES\ntwo Horn clauses, you get back a Horn clause.\nKnowledge bases containing only deﬁnite clauses are interesting for three reasons:\n1. Every deﬁnite clause can be written as an implication whose premise is a conjunction\nof positive literals and whose conclusion is a single positive literal. (See Exercise 7.13.)\nFor example, the deﬁnite clause (¬L1,1 ∨¬Breeze ∨B1,1) can be written as the im-\nplication (L1,1 ∧Breeze) ⇒B1,1. In the implication form, the sentence is easier to\nunderstand: it says that if the agent is in [1,1] and there is a breeze, then [1,1] is breezy.\nIn Horn form, the premise is called the body and the conclusion is called the head. A\nBODY\nHEAD\nsentence consisting of a single positive literal, such as L1,1, is called a fact. It too can\nFACT\nbe written in implication form as True ⇒L1,1, but it is simpler to write just L1,1. Section 7.5.\nPropositional Theorem Proving\n257\nCNFSentence\n→\nClause1 ∧· · · ∧Clausen\nClause\n→\nLiteral 1 ∨· · · ∨Literal m\nLiteral\n→\nSymbol | ¬Symbol\nSymbol\n→\nP | Q | R | . . .\nHornClauseForm\n→\nDeﬁniteClauseForm | GoalClauseForm\nDeﬁniteClauseForm\n→\n(Symbol1 ∧· · · ∧Symboll) ⇒Symbol\nGoalClauseForm\n→\n(Symbol1 ∧· · · ∧Symboll) ⇒False\nFigure 7.14\nA grammar for conjunctive normal form, Horn clauses, and deﬁnite clauses.\nA clause such as A ∧B ⇒C is still a deﬁnite clause when it is written as ¬A ∨¬B ∨C,\nbut only the former is considered the canonical form for deﬁnite clauses. One more class is\nthe k-CNF sentence, which is a CNF sentence where each clause has at most k literals.\n2. Inference with Horn clauses can be done through the forward-chaining and backward-\nFORWARD-CHAINING\nchaining algorithms, which we explain next. Both of these algorithms are natural,\nBACKWARD-\nCHAINING\nin that the inference steps are obvious and easy for humans to follow. This type of\ninference is the basis for logic programming, which is discussed in Chapter 9.\n3. Deciding entailment with Horn clauses can be done in time that is linear in the size of\nthe knowledge base—a pleasant surprise.\n7.5.4\nForward and backward chaining\nThe forward-chaining algorithm PL-FC-ENTAILS?(KB, q) determines if a single proposi-\ntion symbol q—the query—is entailed by a knowledge base of deﬁnite clauses. It begins\nfrom known facts (positive literals) in the knowledge base. If all the premises of an implica-\ntion are known, then its conclusion is added to the set of known facts. For example, if L1,1",
  "from known facts (positive literals) in the knowledge base. If all the premises of an implica-\ntion are known, then its conclusion is added to the set of known facts. For example, if L1,1\nand Breeze are known and (L1,1 ∧Breeze) ⇒B1,1 is in the knowledge base, then B1,1 can\nbe added. This process continues until the query q is added or until no further inferences can\nbe made. The detailed algorithm is shown in Figure 7.15; the main point to remember is that\nit runs in linear time.\nThe best way to understand the algorithm is through an example and a picture. Fig-\nure 7.16(a) shows a simple knowledge base of Horn clauses with A and B as known facts.\nFigure 7.16(b) shows the same knowledge base drawn as an AND–OR graph (see Chap-\nter 4). In AND–OR graphs, multiple links joined by an arc indicate a conjunction—every\nlink must be proved—while multiple links without an arc indicate a disjunction—any link\ncan be proved. It is easy to see how forward chaining works in the graph. The known leaves\n(here, A and B) are set, and inference propagates up the graph as far as possible. Wher-\never a conjunction appears, the propagation waits until all the conjuncts are known before\nproceeding. The reader is encouraged to work through the example in detail. 258\nChapter\n7.\nLogical Agents\nfunction PL-FC-ENTAILS?(KB,q) returns true or false\ninputs: KB, the knowledge base, a set of propositional deﬁnite clauses\nq, the query, a proposition symbol\ncount ←a table, where count[c] is the number of symbols in c’s premise\ninferred ←a table, where inferred[s] is initially false for all symbols\nagenda ←a queue of symbols, initially symbols known to be true in KB\nwhile agenda is not empty do\np ←POP(agenda)\nif p = q then return true\nif inferred[p] = false then\ninferred[p] ←true\nfor each clause c in KB where p is in c.PREMISE do\ndecrement count[c]\nif count[c] = 0 then add c.CONCLUSION to agenda\nreturn false\nFigure 7.15\nThe forward-chaining algorithm for propositional logic. The agenda keeps\ntrack of symbols known to be true but not yet “processed.” The count table keeps track of\nhow many premises of each implication are as yet unknown. Whenever a new symbol p from\nthe agenda is processed, the count is reduced by one for each implication in whose premise\np appears (easily identiﬁed in constant time with appropriate indexing.) If a count reaches\nzero, all the premises of the implication are known, so its conclusion can be added to the",
  "p appears (easily identiﬁed in constant time with appropriate indexing.) If a count reaches\nzero, all the premises of the implication are known, so its conclusion can be added to the\nagenda. Finally, we need to keep track of which symbols have been processed; a symbol that\nis already in the set of inferred symbols need not be added to the agenda again. This avoids\nredundant work and prevents loops caused by implications such as P ⇒Q and Q ⇒P.\nIt is easy to see that forward chaining is sound: every inference is essentially an appli-\ncation of Modus Ponens. Forward chaining is also complete: every entailed atomic sentence\nwill be derived. The easiest way to see this is to consider the ﬁnal state of the inferred table\n(after the algorithm reaches a ﬁxed point where no new inferences are possible). The table\nFIXED POINT\ncontains true for each symbol inferred during the process, and false for all other symbols.\nWe can view the table as a logical model; moreover, every deﬁnite clause in the original KB is\ntrue in this model. To see this, assume the opposite, namely that some clause a1∧. . .∧ak ⇒b\nis false in the model. Then a1 ∧. . . ∧ak must be true in the model and b must be false in\nthe model. But this contradicts our assumption that the algorithm has reached a ﬁxed point!\nWe can conclude, therefore, that the set of atomic sentences inferred at the ﬁxed point deﬁnes\na model of the original KB. Furthermore, any atomic sentence q that is entailed by the KB\nmust be true in all its models and in this model in particular. Hence, every entailed atomic\nsentence q must be inferred by the algorithm.\nForward chaining is an example of the general concept of data-driven reasoning—that\nDATA-DRIVEN\nis, reasoning in which the focus of attention starts with the known data. It can be used within\nan agent to derive conclusions from incoming percepts, often without a speciﬁc query in\nmind. For example, the wumpus agent might TELL its percepts to the knowledge base using Section 7.6.\nEffective Propositional Model Checking\n259\nP ⇒Q\nL ∧M ⇒P\nB ∧L ⇒M\nA ∧P ⇒L\nA ∧B ⇒L\nA\nB\nQ\nP\nM\nL\nB\nA\n(a)\n(b)\nFigure 7.16\n(a) A set of Horn clauses. (b) The corresponding AND–OR graph.\nan incremental forward-chaining algorithm in which new facts can be added to the agenda to\ninitiate new inferences. In humans, a certain amount of data-driven reasoning occurs as new\ninformation arrives. For example, if I am indoors and hear rain starting to fall, it might occur",
  "initiate new inferences. In humans, a certain amount of data-driven reasoning occurs as new\ninformation arrives. For example, if I am indoors and hear rain starting to fall, it might occur\nto me that the picnic will be canceled. Yet it will probably not occur to me that the seventeenth\npetal on the largest rose in my neighbor’s garden will get wet; humans keep forward chaining\nunder careful control, lest they be swamped with irrelevant consequences.\nThe backward-chaining algorithm, as its name suggests, works backward from the\nquery. If the query q is known to be true, then no work is needed. Otherwise, the algorithm\nﬁnds those implications in the knowledge base whose conclusion is q. If all the premises of\none of those implications can be proved true (by backward chaining), then q is true. When\napplied to the query Q in Figure 7.16, it works back down the graph until it reaches a set of\nknown facts, A and B, that forms the basis for a proof. The algorithm is essentially identical\nto the AND-OR-GRAPH-SEARCH algorithm in Figure 4.11. As with forward chaining, an\nefﬁcient implementation runs in linear time.\nBackward chaining is a form of goal-directed reasoning. It is useful for answering\nGOAL-DIRECTED\nREASONING\nspeciﬁc questions such as “What shall I do now?” and “Where are my keys?” Often, the cost\nof backward chaining is much less than linear in the size of the knowledge base, because the\nprocess touches only relevant facts.\n7.6\nEFFECTIVE PROPOSITIONAL MODEL CHECKING\nIn this section, we describe two families of efﬁcient algorithms for general propositional\ninference based on model checking: One approach based on backtracking search, and one\non local hill-climbing search. These algorithms are part of the “technology” of propositional\nlogic. This section can be skimmed on a ﬁrst reading of the chapter. 260\nChapter\n7.\nLogical Agents\nThe algorithms we describe are for checking satisﬁability: the SAT problem. (As noted\nearlier, testing entailment, α |= β, can be done by testing unsatisﬁability of α ∧¬β.) We\nhave already noted the connection between ﬁnding a satisfying model for a logical sentence\nand ﬁnding a solution for a constraint satisfaction problem, so it is perhaps not surprising that\nthe two families of algorithms closely resemble the backtracking algorithms of Section 6.3\nand the local search algorithms of Section 6.4. They are, however, extremely important in\ntheir own right because so many combinatorial problems in computer science can be reduced",
  "and the local search algorithms of Section 6.4. They are, however, extremely important in\ntheir own right because so many combinatorial problems in computer science can be reduced\nto checking the satisﬁability of a propositional sentence. Any improvement in satisﬁability\nalgorithms has huge consequences for our ability to handle complexity in general.\n7.6.1\nA complete backtracking algorithm\nThe ﬁrst algorithm we consider is often called the Davis–Putnam algorithm, after the sem-\nDAVIS–PUTNAM\nALGORITHM\ninal paper by Martin Davis and Hilary Putnam (1960). The algorithm is in fact the version\ndescribed by Davis, Logemann, and Loveland (1962), so we will call it DPLL after the ini-\ntials of all four authors. DPLL takes as input a sentence in conjunctive normal form—a set\nof clauses. Like BACKTRACKING-SEARCH and TT-ENTAILS?, it is essentially a recursive,\ndepth-ﬁrst enumeration of possible models. It embodies three improvements over the simple\nscheme of TT-ENTAILS?:\n• Early termination: The algorithm detects whether the sentence must be true or false,\neven with a partially completed model. A clause is true if any literal is true, even if\nthe other literals do not yet have truth values; hence, the sentence as a whole could be\njudged true even before the model is complete. For example, the sentence (A ∨B) ∧\n(A ∨C) is true if A is true, regardless of the values of B and C. Similarly, a sentence\nis false if any clause is false, which occurs when each of its literals is false. Again, this\ncan occur long before the model is complete. Early termination avoids examination of\nentire subtrees in the search space.\n• Pure symbol heuristic: A pure symbol is a symbol that always appears with the same\nPURE SYMBOL\n“sign” in all clauses. For example, in the three clauses (A ∨¬B), (¬B ∨¬C), and\n(C ∨A), the symbol A is pure because only the positive literal appears, B is pure\nbecause only the negative literal appears, and C is impure. It is easy to see that if\na sentence has a model, then it has a model with the pure symbols assigned so as to\nmake their literals true, because doing so can never make a clause false. Note that, in\ndetermining the purity of a symbol, the algorithm can ignore clauses that are already\nknown to be true in the model constructed so far. For example, if the model contains\nB = false, then the clause (¬B ∨¬C) is already true, and in the remaining clauses C\nappears only as a positive literal; therefore C becomes pure.",
  "known to be true in the model constructed so far. For example, if the model contains\nB = false, then the clause (¬B ∨¬C) is already true, and in the remaining clauses C\nappears only as a positive literal; therefore C becomes pure.\n• Unit clause heuristic: A unit clause was deﬁned earlier as a clause with just one lit-\neral. In the context of DPLL, it also means clauses in which all literals but one are\nalready assigned false by the model. For example, if the model contains B = true,\nthen (¬B ∨¬C) simpliﬁes to ¬C, which is a unit clause. Obviously, for this clause\nto be true, C must be set to false. The unit clause heuristic assigns all such symbols\nbefore branching on the remainder. One important consequence of the heuristic is that Section 7.6.\nEffective Propositional Model Checking\n261\nfunction DPLL-SATISFIABLE?(s) returns true or false\ninputs: s, a sentence in propositional logic\nclauses ←the set of clauses in the CNF representation of s\nsymbols ←a list of the proposition symbols in s\nreturn DPLL(clauses,symbols,{ })\nfunction DPLL(clauses,symbols,model) returns true or false\nif every clause in clauses is true in model then return true\nif some clause in clauses is false in model then return false\nP,value ←FIND-PURE-SYMBOL(symbols,clauses,model)\nif P is non-null then return DPLL(clauses,symbols – P,model ∪{P=value})\nP,value ←FIND-UNIT-CLAUSE(clauses,model)\nif P is non-null then return DPLL(clauses,symbols – P,model ∪{P=value})\nP ←FIRST(symbols); rest ←REST(symbols)\nreturn DPLL(clauses,rest,model ∪{P=true}) or\nDPLL(clauses,rest,model ∪{P=false}))\nFigure 7.17\nThe DPLL algorithm for checking satisﬁability of a sentence in propositional\nlogic. The ideas behind FIND-PURE-SYMBOL and FIND-UNIT-CLAUSE are described in\nthe text; each returns a symbol (or null) and the truth value to assign to that symbol. Like\nTT-ENTAILS?, DPLL operates over partial models.\nany attempt to prove (by refutation) a literal that is already in the knowledge base will\nsucceed immediately (Exercise 7.23). Notice also that assigning one unit clause can\ncreate another unit clause—for example, when C is set to false, (C ∨A) becomes a\nunit clause, causing true to be assigned to A. This “cascade” of forced assignments\nis called unit propagation. It resembles the process of forward chaining with deﬁnite\nUNIT PROPAGATION\nclauses, and indeed, if the CNF expression contains only deﬁnite clauses then DPLL\nessentially replicates forward chaining. (See Exercise 7.24.)",
  "is called unit propagation. It resembles the process of forward chaining with deﬁnite\nUNIT PROPAGATION\nclauses, and indeed, if the CNF expression contains only deﬁnite clauses then DPLL\nessentially replicates forward chaining. (See Exercise 7.24.)\nThe DPLL algorithm is shown in Figure 7.17, which gives the the essential skeleton of the\nsearch process.\nWhat Figure 7.17 does not show are the tricks that enable SAT solvers to scale up to\nlarge problems. It is interesting that most of these tricks are in fact rather general, and we\nhave seen them before in other guises:\n1. Component analysis (as seen with Tasmania in CSPs): As DPLL assigns truth values\nto variables, the set of clauses may become separated into disjoint subsets, called com-\nponents, that share no unassigned variables. Given an efﬁcient way to detect when this\noccurs, a solver can gain considerable speed by working on each component separately.\n2. Variable and value ordering (as seen in Section 6.3.1 for CSPs): Our simple imple-\nmentation of DPLL uses an arbitrary variable ordering and always tries the value true\nbefore false. The degree heuristic (see page 216) suggests choosing the variable that\nappears most frequently over all remaining clauses. 262\nChapter\n7.\nLogical Agents\n3. Intelligent backtracking (as seen in Section 6.3 for CSPs): Many problems that can-\nnot be solved in hours of run time with chronological backtracking can be solved in\nseconds with intelligent backtracking that backs up all the way to the relevant point of\nconﬂict. All SAT solvers that do intelligent backtracking use some form of conﬂict\nclause learning to record conﬂicts so that they won’t be repeated later in the search.\nUsually a limited-size set of conﬂicts is kept, and rarely used ones are dropped.\n4. Random restarts (as seen on page 124 for hill-climbing): Sometimes a run appears not\nto be making progress. In this case, we can start over from the top of the search tree,\nrather than trying to continue. After restarting, different random choices (in variable\nand value selection) are made. Clauses that are learned in the ﬁrst run are retained after\nthe restart and can help prune the search space. Restarting does not guarantee that a\nsolution will be found faster, but it does reduce the variance on the time to solution.\n5. Clever indexing (as seen in many algorithms): The speedup methods used in DPLL\nitself, as well as the tricks used in modern solvers, require fast indexing of such things",
  "5. Clever indexing (as seen in many algorithms): The speedup methods used in DPLL\nitself, as well as the tricks used in modern solvers, require fast indexing of such things\nas “the set of clauses in which variable Xi appears as a positive literal.” This task is\ncomplicated by the fact that the algorithms are interested only in the clauses that have\nnot yet been satisﬁed by previous assignments to variables, so the indexing structures\nmust be updated dynamically as the computation proceeds.\nWith these enhancements, modern solvers can handle problems with tens of millions of vari-\nables. They have revolutionized areas such as hardware veriﬁcation and security protocol\nveriﬁcation, which previously required laborious, hand-guided proofs.\n7.6.2\nLocal search algorithms\nWe have seen several local search algorithms so far in this book, including HILL-CLIMBING\n(page 122) and SIMULATED-ANNEALING (page 126). These algorithms can be applied di-\nrectly to satisﬁability problems, provided that we choose the right evaluation function. Be-\ncause the goal is to ﬁnd an assignment that satisﬁes every clause, an evaluation function that\ncounts the number of unsatisﬁed clauses will do the job. In fact, this is exactly the measure\nused by the MIN-CONFLICTS algorithm for CSPs (page 221). All these algorithms take steps\nin the space of complete assignments, ﬂipping the truth value of one symbol at a time. The\nspace usually contains many local minima, to escape from which various forms of random-\nness are required. In recent years, there has been a great deal of experimentation to ﬁnd a\ngood balance between greediness and randomness.\nOne of the simplest and most effective algorithms to emerge from all this work is called\nWALKSAT (Figure 7.18). On every iteration, the algorithm picks an unsatisﬁed clause and\npicks a symbol in the clause to ﬂip. It chooses randomly between two ways to pick which\nsymbol to ﬂip: (1) a “min-conﬂicts” step that minimizes the number of unsatisﬁed clauses in\nthe new state and (2) a “random walk” step that picks the symbol randomly.\nWhen WALKSAT returns a model, the input sentence is indeed satisﬁable, but when\nit returns failure, there are two possible causes: either the sentence is unsatisﬁable or we\nneed to give the algorithm more time. If we set max ﬂips = ∞and p > 0, WALKSAT will\neventually return a model (if one exists), because the random-walk steps will eventually hit Section 7.6.\nEffective Propositional Model Checking\n263",
  "need to give the algorithm more time. If we set max ﬂips = ∞and p > 0, WALKSAT will\neventually return a model (if one exists), because the random-walk steps will eventually hit Section 7.6.\nEffective Propositional Model Checking\n263\nfunction WALKSAT(clauses,p,max ﬂips) returns a satisfying model or failure\ninputs: clauses, a set of clauses in propositional logic\np, the probability of choosing to do a “random walk” move, typically around 0.5\nmax ﬂips, number of ﬂips allowed before giving up\nmodel ←a random assignment of true/false to the symbols in clauses\nfor i = 1 to max ﬂips do\nif model satisﬁes clauses then return model\nclause ←a randomly selected clause from clauses that is false in model\nwith probability p ﬂip the value in model of a randomly selected symbol from clause\nelse ﬂip whichever symbol in clause maximizes the number of satisﬁed clauses\nreturn failure\nFigure 7.18\nThe WALKSAT algorithm for checking satisﬁability by randomly ﬂipping\nthe values of variables. Many versions of the algorithm exist.\nupon the solution. Alas, if max ﬂips is inﬁnity and the sentence is unsatisﬁable, then the\nalgorithm never terminates!\nFor this reason, WALKSAT is most useful when we expect a solution to exist—for ex-\nample, the problems discussed in Chapters 3 and 6 usually have solutions. On the other hand,\nWALKSAT cannot always detect unsatisﬁability, which is required for deciding entailment.\nFor example, an agent cannot reliably use WALKSAT to prove that a square is safe in the\nwumpus world. Instead, it can say, “I thought about it for an hour and couldn’t come up with\na possible world in which the square isn’t safe.” This may be a good empirical indicator that\nthe square is safe, but it’s certainly not a proof.\n7.6.3\nThe landscape of random SAT problems\nSome SAT problems are harder than others. Easy problems can be solved by any old algo-\nrithm, but because we know that SAT is NP-complete, at least some problem instances must\nrequire exponential run time. In Chapter 6, we saw some surprising discoveries about certain\nkinds of problems. For example, the n-queens problem—thought to be quite tricky for back-\ntracking search algorithms—turned out to be trivially easy for local search methods, such as\nmin-conﬂicts. This is because solutions are very densely distributed in the space of assign-\nments, and any initial assignment is guaranteed to have a solution nearby. Thus, n-queens is\neasy because it is underconstrained.\nUNDERCONSTRAINED",
  "min-conﬂicts. This is because solutions are very densely distributed in the space of assign-\nments, and any initial assignment is guaranteed to have a solution nearby. Thus, n-queens is\neasy because it is underconstrained.\nUNDERCONSTRAINED\nWhen we look at satisﬁability problems in conjunctive normal form, an undercon-\nstrained problem is one with relatively few clauses constraining the variables. For example,\nhere is a randomly generated 3-CNF sentence with ﬁve symbols and ﬁve clauses:\n(¬D ∨¬B ∨C) ∧(B ∨¬A ∨¬C) ∧(¬C ∨¬B ∨E)\n∧(E ∨¬D ∨B) ∧(B ∨E ∨¬C) .\nSixteen of the 32 possible assignments are models of this sentence, so, on average, it would\ntake just two random guesses to ﬁnd a model. This is an easy satisﬁability problem, as are 264\nChapter\n7.\nLogical Agents\nmost such underconstrained problems. On the other hand, an overconstrained problem has\nmany clauses relative to the number of variables and is likely to have no solutions.\nTo go beyond these basic intuitions, we must deﬁne exactly how random sentences\nare generated. The notation CNFk(m, n) denotes a k-CNF sentence with m clauses and n\nsymbols, where the clauses are chosen uniformly, independently, and without replacement\nfrom among all clauses with k different literals, which are positive or negative at random. (A\nsymbol may not appear twice in a clause, nor may a clause appear twice in a sentence.)\nGiven a source of random sentences, we can measure the probability of satisﬁability.\nFigure 7.19(a) plots the probability for CNF3(m, 50), that is, sentences with 50 variables\nand 3 literals per clause, as a function of the clause/symbol ratio, m/n. As we expect, for\nsmall m/n the probability of satisﬁability is close to 1, and at large m/n the probability\nis close to 0. The probability drops fairly sharply around m/n = 4.3. Empirically, we ﬁnd\nthat the “cliff” stays in roughly the same place (for k = 3) and gets sharper and sharper as n\nincreases. Theoretically, the satisﬁability threshold conjecture says that for every k ≥3,\nSATISFIABILITY\nTHRESHOLD\nCONJECTURE\nthere is a threshold ratio rk such that, as n goes to inﬁnity, the probability that CNFk(n, rn)\nis satisﬁable becomes 1 for all values of r below the threshold, and 0 for all values above.\nThe conjecture remains unproven.\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\nP(satisfiable)\nClause/symbol ratio m/n\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n0\n1\n2\n3\n4\n5\n6\n7\n8\nRuntime\nClause/symbol ratio m/n\nDPLL\nWalkSAT\n(a)\n(b)\nFigure 7.19",
  "The conjecture remains unproven.\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n1\n2\n3\n4\n5\n6\n7\n8\nP(satisfiable)\nClause/symbol ratio m/n\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n0\n1\n2\n3\n4\n5\n6\n7\n8\nRuntime\nClause/symbol ratio m/n\nDPLL\nWalkSAT\n(a)\n(b)\nFigure 7.19\n(a) Graph showing the probability that a random 3-CNF sentence with n = 50\nsymbols is satisﬁable, as a function of the clause/symbol ratio m/n. (b) Graph of the median\nrun time (measured in number of recursive calls to DPLL, a good proxy) on random 3-CNF\nsentences. The most difﬁcult problems have a clause/symbol ratio of about 4.3.\nNow that we have a good idea where the satisﬁable and unsatisﬁable problems are, the\nnext question is, where are the hard problems? It turns out that they are also often at the\nthreshold value. Figure 7.19(b) shows that 50-symbol problems at the threshold value of 4.3\nare about 20 times more difﬁcult to solve than those at a ratio of 3.3. The underconstrained\nproblems are easiest to solve (because it is so easy to guess a solution); the overconstrained\nproblems are not as easy as the underconstrained, but still are much easier than the ones right\nat the threshold. Section 7.7.\nAgents Based on Propositional Logic\n265\n7.7\nAGENTS BASED ON PROPOSITIONAL LOGIC\nIn this section, we bring together what we have learned so far in order to construct wumpus\nworld agents that use propositional logic. The ﬁrst step is to enable the agent to deduce, to the\nextent possible, the state of the world given its percept history. This requires writing down a\ncomplete logical model of the effects of actions. We also show how the agent can keep track of\nthe world efﬁciently without going back into the percept history for each inference. Finally,\nwe show how the agent can use logical inference to construct plans that are guaranteed to\nachieve its goals.\n7.7.1\nThe current state of the world\nAs stated at the beginning of the chapter, a logical agent operates by deducing what to do\nfrom a knowledge base of sentences about the world. The knowledge base is composed of\naxioms—general knowledge about how the world works—and percept sentences obtained\nfrom the agent’s experience in a particular world. In this section, we focus on the problem of\ndeducing the current state of the wumpus world—where am I, is that square safe, and so on.\nWe began collecting axioms in Section 7.4.3. The agent knows that the starting square\ncontains no pit (¬P1,1) and no wumpus (¬W1,1). Furthermore, for each square, it knows that",
  "We began collecting axioms in Section 7.4.3. The agent knows that the starting square\ncontains no pit (¬P1,1) and no wumpus (¬W1,1). Furthermore, for each square, it knows that\nthe square is breezy if and only if a neighboring square has a pit; and a square is smelly if and\nonly if a neighboring square has a wumpus. Thus, we include a large collection of sentences\nof the following form:\nB1,1 ⇔(P1,2 ∨P2,1)\nS1,1 ⇔(W1,2 ∨W2,1)\n· · ·\nThe agent also knows that there is exactly one wumpus. This is expressed in two parts. First,\nwe have to say that there is at least one wumpus:\nW1,1 ∨W1,2 ∨· · · ∨W4,3 ∨W4,4 .\nThen, we have to say that there is at most one wumpus. For each pair of locations, we add a\nsentence saying that at least one of them must be wumpus-free:\n¬W1,1 ∨¬W1,2\n¬W1,1 ∨¬W1,3\n· · ·\n¬W4,3 ∨¬W4,4 .\nSo far, so good. Now let’s consider the agent’s percepts. If there is currently a stench, one\nmight suppose that a proposition Stench should be added to the knowledge base. This is not\nquite right, however: if there was no stench at the previous time step, then ¬Stench would al-\nready be asserted, and the new assertion would simply result in a contradiction. The problem\nis solved when we realize that a percept asserts something only about the current time. Thus,\nif the time step (as supplied to MAKE-PERCEPT-SENTENCE in Figure 7.1) is 4, then we add 266\nChapter\n7.\nLogical Agents\nStench4 to the knowledge base, rather than Stench—neatly avoiding any contradiction with\n¬Stench3. The same goes for the breeze, bump, glitter, and scream percepts.\nThe idea of associating propositions with time steps extends to any aspect of the world\nthat changes over time. For example, the initial knowledge base includes L0\n1,1—the agent is in\nsquare [1, 1] at time 0—as well as FacingEast0, HaveArrow 0, and WumpusAlive0. We use\nthe word ﬂuent (from the Latin ﬂuens, ﬂowing) to refer an aspect of the world that changes.\nFLUENT\n“Fluent” is a synonym for “state variable,” in the sense described in the discussion of factored\nrepresentations in Section 2.4.7 on page 57. Symbols associated with permanent aspects of\nthe world do not need a time superscript and are sometimes called atemporal variables.\nATEMPORAL\nVARIABLE\nWe can connect stench and breeze percepts directly to the properties of the squares\nwhere they are experienced through the location ﬂuent as follows.10 For any time step t and\nany square [x, y], we assert\nLt\nx,y ⇒(Breezet ⇔Bx,y)\nLt\nx,y ⇒(Stencht ⇔Sx,y) .",
  "where they are experienced through the location ﬂuent as follows.10 For any time step t and\nany square [x, y], we assert\nLt\nx,y ⇒(Breezet ⇔Bx,y)\nLt\nx,y ⇒(Stencht ⇔Sx,y) .\nNow, of course, we need axioms that allow the agent to keep track of ﬂuents such as Lt\nx,y.\nThese ﬂuents change as the result of actions taken by the agent, so, in the terminology of\nChapter 3, we need to write down the transition model of the wumpus world as a set of\nlogical sentences.\nFirst, we need proposition symbols for the occurrences of actions. As with percepts,\nthese symbols are indexed by time; thus, Forward 0 means that the agent executes the Forward\naction at time 0. By convention, the percept for a given time step happens ﬁrst, followed by\nthe action for that time step, followed by a transition to the next time step.\nTo describe how the world changes, we can try writing effect axioms that specify the\nEFFECT AXIOM\noutcome of an action at the next time step. For example, if the agent is at location [1, 1] facing\neast at time 0 and goes Forward, the result is that the agent is in square [2, 1] and no longer\nis in [1, 1]:\nL0\n1,1 ∧FacingEast0 ∧Forward 0 ⇒(L1\n2,1 ∧¬L1\n1,1) .\n(7.1)\nWe would need one such sentence for each possible time step, for each of the 16 squares,\nand each of the four orientations. We would also need similar sentences for the other actions:\nGrab, Shoot, Climb, TurnLeft, and TurnRight.\nLet us suppose that the agent does decide to move Forward at time 0 and asserts this\nfact into its knowledge base. Given the effect axiom in Equation (7.1), combined with the\ninitial assertions about the state at time 0, the agent can now deduce that it is in [2, 1]. That\nis, ASK(KB, L1\n2,1) = true. So far, so good. Unfortunately, the news elsewhere is less good:\nif we ASK(KB, HaveArrow 1), the answer is false, that is, the agent cannot prove it still\nhas the arrow; nor can it prove it doesn’t have it! The information has been lost because the\neffect axiom fails to state what remains unchanged as the result of an action. The need to do\nthis gives rise to the frame problem.11 One possible solution to the frame problem would\nFRAME PROBLEM\n10 Section 7.4.3 conveniently glossed over this requirement.\n11 The name “frame problem” comes from “frame of reference” in physics—the assumed stationary background\nwith respect to which motion is measured. It also has an analogy to the frames of a movie, in which normally",
  "11 The name “frame problem” comes from “frame of reference” in physics—the assumed stationary background\nwith respect to which motion is measured. It also has an analogy to the frames of a movie, in which normally\nmost of the background stays constant while changes occur in the foreground. Section 7.7.\nAgents Based on Propositional Logic\n267\nbe to add frame axioms explicitly asserting all the propositions that remain the same. For\nFRAME AXIOM\nexample, for each time t we would have\nForward t ⇒(HaveArrow t ⇔HaveArrow t+1)\nForward t ⇒(WumpusAlivet ⇔WumpusAlivet+1)\n· · ·\nwhere we explicitly mention every proposition that stays unchanged from time t to time\nt + 1 under the action Forward. Although the agent now knows that it still has the arrow\nafter moving forward and that the wumpus hasn’t died or come back to life, the proliferation\nof frame axioms seems remarkably inefﬁcient. In a world with m different actions and n\nﬂuents, the set of frame axioms will be of size O(mn). This speciﬁc manifestation of the\nframe problem is sometimes called the representational frame problem. Historically, the\nREPRESENTATIONAL\nFRAME PROBLEM\nproblem was a signiﬁcant one for AI researchers; we explore it further in the notes at the end\nof the chapter.\nThe representational frame problem is signiﬁcant because the real world has very many\nﬂuents, to put it mildly. Fortunately for us humans, each action typically changes no more\nthan some small number k of those ﬂuents—the world exhibits locality. Solving the repre-\nLOCALITY\nsentational frame problem requires deﬁning the transition model with a set of axioms of size\nO(mk) rather than size O(mn). There is also an inferential frame problem: the problem\nINFERENTIAL FRAME\nPROBLEM\nof projecting forward the results of a t step plan of action in time O(kt) rather than O(nt).\nThe solution to the problem involves changing one’s focus from writing axioms about\nactions to writing axioms about ﬂuents. Thus, for each ﬂuent F, we will have an axiom that\ndeﬁnes the truth value of F t+1 in terms of ﬂuents (including F itself) at time t and the actions\nthat may have occurred at time t. Now, the truth value of F t+1 can be set in one of two ways:\neither the action at time t causes F to be true at t + 1, or F was already true at time t and the\naction at time t does not cause it to be false. An axiom of this form is called a successor-state\naxiom and has this schema:\nSUCCESSOR-STATE\nAXIOM\nF t+1 ⇔ActionCausesF t ∨(F t ∧¬ActionCausesNotF t) .",
  "action at time t does not cause it to be false. An axiom of this form is called a successor-state\naxiom and has this schema:\nSUCCESSOR-STATE\nAXIOM\nF t+1 ⇔ActionCausesF t ∨(F t ∧¬ActionCausesNotF t) .\nOne of the simplest successor-state axioms is the one for HaveArrow. Because there is no\naction for reloading, the ActionCausesF t part goes away and we are left with\nHaveArrow t+1 ⇔(HaveArrow t ∧¬Shoott) .\n(7.2)\nFor the agent’s location, the successor-state axioms are more elaborate. For example, Lt+1\n1,1\nis true if either (a) the agent moved Forward from [1, 2] when facing south, or from [2, 1]\nwhen facing west; or (b) Lt\n1,1 was already true and the action did not cause movement (either\nbecause the action was not Forward or because the action bumped into a wall). Written out\nin propositional logic, this becomes\nLt+1\n1,1\n⇔\n(Lt\n1,1 ∧(¬Forward t ∨Bumpt+1))\n∨(Lt\n1,2 ∧(Southt ∧Forward t))\n(7.3)\n∨(Lt\n2,1 ∧(Westt ∧Forward t)) .\nExercise 7.26 asks you to write out axioms for the remaining wumpus world ﬂuents. 268\nChapter\n7.\nLogical Agents\nGiven a complete set of successor-state axioms and the other axioms listed at the begin-\nning of this section, the agent will be able to ASK and answer any answerable question about\nthe current state of the world. For example, in Section 7.2 the initial sequence of percepts and\nactions is\n¬Stench0 ∧¬Breeze0 ∧¬Glitter 0 ∧¬Bump0 ∧¬Scream0 ; Forward 0\n¬Stench1 ∧Breeze1 ∧¬Glitter 1 ∧¬Bump1 ∧¬Scream1 ; TurnRight1\n¬Stench2 ∧Breeze2 ∧¬Glitter 2 ∧¬Bump2 ∧¬Scream2 ; TurnRight2\n¬Stench3 ∧Breeze3 ∧¬Glitter 3 ∧¬Bump3 ∧¬Scream3 ; Forward 3\n¬Stench4 ∧¬Breeze4 ∧¬Glitter 4 ∧¬Bump4 ∧¬Scream4 ; TurnRight4\n¬Stench5 ∧¬Breeze5 ∧¬Glitter 5 ∧¬Bump5 ∧¬Scream5 ; Forward 5\nStench6 ∧¬Breeze6 ∧¬Glitter 6 ∧¬Bump6 ∧¬Scream6\nAt this point, we have ASK(KB, L6\n1,2) = true, so the agent knows where it is. Moreover,\nASK(KB, W1,3) = true and ASK(KB, P3,1) = true, so the agent has found the wumpus and\none of the pits. The most important question for the agent is whether a square is OK to move\ninto, that is, the square contains no pit nor live wumpus. It’s convenient to add axioms for\nthis, having the form\nOK t\nx,y ⇔¬Px,y ∧¬(Wx,y ∧WumpusAlivet) .\nFinally, ASK(KB, OK 6\n2,2) = true, so the square [2, 2] is OK to move into. In fact, given a\nsound and complete inference algorithm such as DPLL, the agent can answer any answerable\nquestion about which squares are OK—and can do so in just a few milliseconds for small-to-\nmedium wumpus worlds.",
  "sound and complete inference algorithm such as DPLL, the agent can answer any answerable\nquestion about which squares are OK—and can do so in just a few milliseconds for small-to-\nmedium wumpus worlds.\nSolving the representational and inferential frame problems is a big step forward, but\na pernicious problem remains: we need to conﬁrm that all the necessary preconditions of an\naction hold for it to have its intended effect. We said that the Forward action moves the agent\nahead unless there is a wall in the way, but there are many other unusual exceptions that could\ncause the action to fail: the agent might trip and fall, be stricken with a heart attack, be carried\naway by giant bats, etc. Specifying all these exceptions is called the qualiﬁcation problem.\nQUALIFICATION\nPROBLEM\nThere is no complete solution within logic; system designers have to use good judgment in\ndeciding how detailed they want to be in specifying their model, and what details they want\nto leave out. We will see in Chapter 13 that probability theory allows us to summarize all the\nexceptions without explicitly naming them.\n7.7.2\nA hybrid agent\nThe ability to deduce various aspects of the state of the world can be combined fairly straight-\nforwardly with condition–action rules and with problem-solving algorithms from Chapters 3\nand 4 to produce a hybrid agent for the wumpus world. Figure 7.20 shows one possible way\nHYBRID AGENT\nto do this. The agent program maintains and updates a knowledge base as well as a current\nplan. The initial knowledge base contains the atemporal axioms—those that don’t depend\non t, such as the axiom relating the breeziness of squares to the presence of pits. At each\ntime step, the new percept sentence is added along with all the axioms that depend on t, such Section 7.7.\nAgents Based on Propositional Logic\n269\nas the successor-state axioms. (The next section explains why the agent doesn’t need axioms\nfor future time steps.) Then, the agent uses logical inference, by ASKing questions of the\nknowledge base, to work out which squares are safe and which have yet to be visited.\nThe main body of the agent program constructs a plan based on a decreasing priority of\ngoals. First, if there is a glitter, the program constructs a plan to grab the gold, follow a route\nback to the initial location, and climb out of the cave. Otherwise, if there is no current plan,\nthe program plans a route to the closest safe square that it has not visited yet, making sure",
  "back to the initial location, and climb out of the cave. Otherwise, if there is no current plan,\nthe program plans a route to the closest safe square that it has not visited yet, making sure\nthe route goes through only safe squares. Route planning is done with A∗search, not with\nASK. If there are no safe squares to explore, the next step—if the agent still has an arrow—is\nto try to make a safe square by shooting at one of the possible wumpus locations. These are\ndetermined by asking where ASK(KB, ¬Wx,y) is false—that is, where it is not known that\nthere is not a wumpus. The function PLAN-SHOT (not shown) uses PLAN-ROUTE to plan a\nsequence of actions that will line up this shot. If this fails, the program looks for a square to\nexplore that is not provably unsafe—that is, a square for which ASK(KB, ¬OK t\nx,y) returns\nfalse. If there is no such square, then the mission is impossible and the agent retreats to [1, 1]\nand climbs out of the cave.\n7.7.3\nLogical state estimation\nThe agent program in Figure 7.20 works quite well, but it has one major weakness: as time\ngoes by, the computational expense involved in the calls to ASK goes up and up. This happens\nmainly because the required inferences have to go back further and further in time and involve\nmore and more proposition symbols. Obviously, this is unsustainable—we cannot have an\nagent whose time to process each percept grows in proportion to the length of its life! What\nwe really need is a constant update time—that is, independent of t. The obvious answer is to\nsave, or cache, the results of inference, so that the inference process at the next time step can\nCACHING\nbuild on the results of earlier steps instead of having to start again from scratch.\nAs we saw in Section 4.4, the past history of percepts and all their ramiﬁcations can\nbe replaced by the belief state—that is, some representation of the set of all possible current\nstates of the world.12 The process of updating the belief state as new percepts arrive is called\nstate estimation. Whereas in Section 4.4 the belief state was an explicit list of states, here\nwe can use a logical sentence involving the proposition symbols associated with the current\ntime step, as well as the atemporal symbols. For example, the logical sentence\nWumpusAlive1 ∧L1\n2,1 ∧B2,1 ∧(P3,1 ∨P2,2)\n(7.4)\nrepresents the set of all states at time 1 in which the wumpus is alive, the agent is at [2, 1],\nthat square is breezy, and there is a pit in [3, 1] or [2, 2] or both.",
  "WumpusAlive1 ∧L1\n2,1 ∧B2,1 ∧(P3,1 ∨P2,2)\n(7.4)\nrepresents the set of all states at time 1 in which the wumpus is alive, the agent is at [2, 1],\nthat square is breezy, and there is a pit in [3, 1] or [2, 2] or both.\nMaintaining an exact belief state as a logical formula turns out not to be easy. If there\nare n ﬂuent symbols for time t, then there are 2n possible states—that is, assignments of truth\nvalues to those symbols. Now, the set of belief states is the powerset (set of all subsets) of the\nset of physical states. There are 2n physical states, hence 22n belief states. Even if we used\nthe most compact possible encoding of logical formulas, with each belief state represented\n12 We can think of the percept history itself as a representation of the belief state, but one that makes inference\nincreasingly expensive as the history gets longer. 270\nChapter\n7.\nLogical Agents\nfunction HYBRID-WUMPUS-AGENT(percept) returns an action\ninputs: percept, a list, [stench,breeze,glitter,bump,scream]\npersistent: KB, a knowledge base, initially the atemporal “wumpus physics”\nt, a counter, initially 0, indicating time\nplan, an action sequence, initially empty\nTELL(KB, MAKE-PERCEPT-SENTENCE(percept,t))\nTELL the KB the temporal “physics” sentences for time t\nsafe ←{[x, y] : ASK(KB,OK t\nx,y) = true}\nif ASK(KB,Glitter t) = true then\nplan ←[Grab] + PLAN-ROUTE(current,{[1,1]},safe) + [Climb]\nif plan is empty then\nunvisited ←{[x, y] : ASK(KB, Lt′\nx,y) = false for all t′ ≤t}\nplan ←PLAN-ROUTE(current,unvisited ∩safe,safe)\nif plan is empty and ASK(KB,HaveArrow t) = true then\npossible wumpus ←{[x, y] : ASK(KB, ¬ Wx,y) = false}\nplan ←PLAN-SHOT(current,possible wumpus,safe)\nif plan is empty then // no choice but to take a risk\nnot unsafe ←{[x, y] : ASK(KB, ¬ OK t\nx,y) = false}\nplan ←PLAN-ROUTE(current,unvisited ∩not unsafe,safe)\nif plan is empty then\nplan ←PLAN-ROUTE(current,{[1, 1]},safe) + [Climb]\naction ←POP(plan)\nTELL(KB, MAKE-ACTION-SENTENCE(action,t))\nt ←t + 1\nreturn action\nfunction PLAN-ROUTE(current,goals,allowed) returns an action sequence\ninputs: current, the agent’s current position\ngoals, a set of squares; try to plan a route to one of them\nallowed, a set of squares that can form part of the route\nproblem ←ROUTE-PROBLEM(current,goals,allowed)\nreturn A*-GRAPH-SEARCH(problem)\nFigure 7.20\nA hybrid agent program for the wumpus world. It uses a propositional knowl-\nedge base to infer the state of the world, and a combination of problem-solving search and",
  "return A*-GRAPH-SEARCH(problem)\nFigure 7.20\nA hybrid agent program for the wumpus world. It uses a propositional knowl-\nedge base to infer the state of the world, and a combination of problem-solving search and\ndomain-speciﬁc code to decide what actions to take.\nby a unique binary number, we would need numbers with log2(22n) = 2n bits to label the\ncurrent belief state. That is, exact state estimation may require logical formulas whose size is\nexponential in the number of symbols.\nOne very common and natural scheme for approximate state estimation is to represent\nbelief states as conjunctions of literals, that is, 1-CNF formulas. To do this, the agent program\nsimply tries to prove Xt and ¬Xt for each symbol Xt (as well as each atemporal symbol\nwhose truth value is not yet known), given the belief state at t −1. The conjunction of Section 7.7.\nAgents Based on Propositional Logic\n271\nFigure 7.21\nDepiction of a 1-CNF belief state (bold outline) as a simply representable,\nconservative approximation to the exact (wiggly) belief state (shaded region with dashed\noutline). Each possible world is shown as a circle; the shaded ones are consistent with all the\npercepts.\nprovable literals becomes the new belief state, and the previous belief state is discarded.\nIt is important to understand that this scheme may lose some information as time goes\nalong. For example, if the sentence in Equation (7.4) were the true belief state, then neither\nP3,1 nor P2,2 would be provable individually and neither would appear in the 1-CNF belief\nstate. (Exercise 7.27 explores one possible solution to this problem.) On the other hand,\nbecause every literal in the 1-CNF belief state is proved from the previous belief state, and\nthe initial belief state is a true assertion, we know that entire 1-CNF belief state must be\ntrue. Thus, the set of possible states represented by the 1-CNF belief state includes all states\nthat are in fact possible given the full percept history. As illustrated in Figure 7.21, the 1-\nCNF belief state acts as a simple outer envelope, or conservative approximation, around the\nCONSERVATIVE\nAPPROXIMATION\nexact belief state. We see this idea of conservative approximations to complicated sets as a\nrecurring theme in many areas of AI.\n7.7.4\nMaking plans by propositional inference\nThe agent in Figure 7.20 uses logical inference to determine which squares are safe, but uses\nA∗search to make plans. In this section, we show how to make plans by logical inference.",
  "7.7.4\nMaking plans by propositional inference\nThe agent in Figure 7.20 uses logical inference to determine which squares are safe, but uses\nA∗search to make plans. In this section, we show how to make plans by logical inference.\nThe basic idea is very simple:\n1. Construct a sentence that includes\n(a) Init0, a collection of assertions about the initial state;\n(b) Transition1, . . . , Transitiont, the successor-state axioms for all possible actions\nat each time up to some maximum time t;\n(c) the assertion that the goal is achieved at time t: HaveGoldt ∧ClimbedOutt. 272\nChapter\n7.\nLogical Agents\n2. Present the whole sentence to a SAT solver. If the solver ﬁnds a satisfying model, then\nthe goal is achievable; if the sentence is unsatisﬁable, then the planning problem is\nimpossible.\n3. Assuming a model is found, extract from the model those variables that represent ac-\ntions and are assigned true. Together they represent a plan to achieve the goals.\nA propositional planning procedure, SATPLAN, is shown in Figure 7.22. It implements the\nbasic idea just given, with one twist. Because the agent does not know how many steps it\nwill take to reach the goal, the algorithm tries each possible number of steps t, up to some\nmaximum conceivable plan length Tmax. In this way, it is guaranteed to ﬁnd the shortest plan\nif one exists. Because of the way SATPLAN searches for a solution, this approach cannot\nbe used in a partially observable environment; SATPLAN would just set the unobservable\nvariables to the values it needs to create a solution.\nfunction SATPLAN(init, transition, goal,T max) returns solution or failure\ninputs: init, transition, goal, constitute a description of the problem\nT max, an upper limit for plan length\nfor t = 0 to T max do\ncnf ←TRANSLATE-TO-SAT(init, transition, goal,t)\nmodel ←SAT-SOLVER(cnf )\nif model is not null then\nreturn EXTRACT-SOLUTION(model)\nreturn failure\nFigure 7.22\nThe SATPLAN algorithm. The planning problem is translated into a CNF\nsentence in which the goal is asserted to hold at a ﬁxed time step t and axioms are included\nfor each time step up to t. If the satisﬁability algorithm ﬁnds a model, then a plan is extracted\nby looking at those proposition symbols that refer to actions and are assigned true in the\nmodel. If no model exists, then the process is repeated with the goal moved one step later.\nThe key step in using SATPLAN is the construction of the knowledge base. It might",
  "model. If no model exists, then the process is repeated with the goal moved one step later.\nThe key step in using SATPLAN is the construction of the knowledge base. It might\nseem, on casual inspection, that the wumpus world axioms in Section 7.7.1 sufﬁce for steps\n1(a) and 1(b) above. There is, however, a signiﬁcant difference between the requirements for\nentailment (as tested by ASK) and those for satisﬁability. Consider, for example, the agent’s\nlocation, initially [1, 1], and suppose the agent’s unambitious goal is to be in [2, 1] at time 1.\nThe initial knowledge base contains L0\n1,1 and the goal is L1\n2,1. Using ASK, we can prove L1\n2,1\nif Forward0 is asserted, and, reassuringly, we cannot prove L1\n2,1 if, say, Shoot0 is asserted\ninstead. Now, SATPLAN will ﬁnd the plan [Forward0]; so far, so good. Unfortunately,\nSATPLAN also ﬁnds the plan [Shoot0]. How could this be? To ﬁnd out, we inspect the model\nthat SATPLAN constructs: it includes the assignment L0\n2,1, that is, the agent can be in [2, 1]\nat time 1 by being there at time 0 and shooting. One might ask, “Didn’t we say the agent is in\n[1, 1] at time 0?” Yes, we did, but we didn’t tell the agent that it can’t be in two places at once!\nFor entailment, L0\n2,1 is unknown and cannot, therefore, be used in a proof; for satisﬁability, Section 7.7.\nAgents Based on Propositional Logic\n273\non the other hand, L0\n2,1 is unknown and can, therefore, be set to whatever value helps to\nmake the goal true. For this reason, SATPLAN is a good debugging tool for knowledge bases\nbecause it reveals places where knowledge is missing. In this particular case, we can ﬁx the\nknowledge base by asserting that, at each time step, the agent is in exactly one location, using\na collection of sentences similar to those used to assert the existence of exactly one wumpus.\nAlternatively, we can assert ¬L0\nx,y for all locations other than [1, 1]; the successor-state axiom\nfor location takes care of subsequent time steps. The same ﬁxes also work to make sure the\nagent has only one orientation.\nSATPLAN has more surprises in store, however. The ﬁrst is that it ﬁnds models with\nimpossible actions, such as shooting with no arrow. To understand why, we need to look more\ncarefully at what the successor-state axioms (such as Equation (7.3)) say about actions whose\npreconditions are not satisﬁed. The axioms do predict correctly that nothing will happen when",
  "carefully at what the successor-state axioms (such as Equation (7.3)) say about actions whose\npreconditions are not satisﬁed. The axioms do predict correctly that nothing will happen when\nsuch an action is executed (see Exercise 10.14), but they do not say that the action cannot be\nexecuted! To avoid generating plans with illegal actions, we must add precondition axioms\nPRECONDITION\nAXIOMS\nstating that an action occurrence requires the preconditions to be satisﬁed.13 For example, we\nneed to say, for each time t, that\nShoot t ⇒HaveArrow t .\nThis ensures that if a plan selects the Shoot action at any time, it must be the case that the\nagent has an arrow at that time.\nSATPLAN’s second surprise is the creation of plans with multiple simultaneous actions.\nFor example, it may come up with a model in which both Forward 0 and Shoot0 are true,\nwhich is not allowed. To eliminate this problem, we introduce action exclusion axioms: for\nACTION EXCLUSION\nAXIOM\nevery pair of actions At\ni and At\nj we add the axiom\n¬At\ni ∨¬At\nj .\nIt might be pointed out that walking forward and shooting at the same time is not so hard to\ndo, whereas, say, shooting and grabbing at the same time is rather impractical. By imposing\naction exclusion axioms only on pairs of actions that really do interfere with each other, we\ncan allow for plans that include multiple simultaneous actions—and because SATPLAN ﬁnds\nthe shortest legal plan, we can be sure that it will take advantage of this capability.\nTo summarize, SATPLAN ﬁnds models for a sentence containing the initial state, the\ngoal, the successor-state axioms, the precondition axioms, and the action exclusion axioms.\nIt can be shown that this collection of axioms is sufﬁcient, in the sense that there are no\nlonger any spurious “solutions.” Any model satisfying the propositional sentence will be a\nvalid plan for the original problem. Modern SAT-solving technology makes the approach\nquite practical. For example, a DPLL-style solver has no difﬁculty in generating the 11-step\nsolution for the wumpus world instance shown in Figure 7.2.\nThis section has described a declarative approach to agent construction: the agent works\nby a combination of asserting sentences in the knowledge base and performing logical infer-\nence. This approach has some weaknesses hidden in phrases such as “for each time t” and\n13 Notice that the addition of precondition axioms means that we need not include preconditions for actions in\nthe successor-state axioms. 274",
  "ence. This approach has some weaknesses hidden in phrases such as “for each time t” and\n13 Notice that the addition of precondition axioms means that we need not include preconditions for actions in\nthe successor-state axioms. 274\nChapter\n7.\nLogical Agents\n“for each square [x, y].” For any practical agent, these phrases have to be implemented by\ncode that generates instances of the general sentence schema automatically for insertion into\nthe knowledge base. For a wumpus world of reasonable size—one comparable to a smallish\ncomputer game—we might need a 100 × 100 board and 1000 time steps, leading to knowl-\nedge bases with tens or hundreds of millions of sentences. Not only does this become rather\nimpractical, but it also illustrates a deeper problem: we know something about the wum-\npus world—namely, that the “physics” works the same way across all squares and all time\nsteps—that we cannot express directly in the language of propositional logic. To solve this\nproblem, we need a more expressive language, one in which phrases like “for each time t”\nand “for each square [x, y]” can be written in a natural way. First-order logic, described in\nChapter 8, is such a language; in ﬁrst-order logic a wumpus world of any size and duration\ncan be described in about ten sentences rather than ten million or ten trillion.\n7.8\nSUMMARY\nWe have introduced knowledge-based agents and have shown how to deﬁne a logic with\nwhich such agents can reason about the world. The main points are as follows:\n• Intelligent agents need knowledge about the world in order to reach good decisions.\n• Knowledge is contained in agents in the form of sentences in a knowledge represen-\ntation language that are stored in a knowledge base.\n• A knowledge-based agent is composed of a knowledge base and an inference mecha-\nnism. It operates by storing sentences about the world in its knowledge base, using the\ninference mechanism to infer new sentences, and using these sentences to decide what\naction to take.\n• A representation language is deﬁned by its syntax, which speciﬁes the structure of\nsentences, and its semantics, which deﬁnes the truth of each sentence in each possible\nworld or model.\n• The relationship of entailment between sentences is crucial to our understanding of\nreasoning. A sentence α entails another sentence β if β is true in all worlds where\nα is true. Equivalent deﬁnitions include the validity of the sentence α ⇒β and the\nunsatisﬁability of the sentence α ∧¬β.",
  "reasoning. A sentence α entails another sentence β if β is true in all worlds where\nα is true. Equivalent deﬁnitions include the validity of the sentence α ⇒β and the\nunsatisﬁability of the sentence α ∧¬β.\n• Inference is the process of deriving new sentences from old ones. Sound inference algo-\nrithms derive only sentences that are entailed; complete algorithms derive all sentences\nthat are entailed.\n• Propositional logic is a simple language consisting of proposition symbols and logical\nconnectives. It can handle propositions that are known true, known false, or completely\nunknown.\n• The set of possible models, given a ﬁxed propositional vocabulary, is ﬁnite, so en-\ntailment can be checked by enumerating models. Efﬁcient model-checking inference\nalgorithms for propositional logic include backtracking and local search methods and\ncan often solve large problems quickly. Bibliographical and Historical Notes\n275\n• Inference rules are patterns of sound inference that can be used to ﬁnd proofs. The\nresolution rule yields a complete inference algorithm for knowledge bases that are\nexpressed in conjunctive normal form. Forward chaining and backward chaining\nare very natural reasoning algorithms for knowledge bases in Horn form.\n• Local search methods such as WALKSAT can be used to ﬁnd solutions. Such algo-\nrithms are sound but not complete.\n• Logical state estimation involves maintaining a logical sentence that describes the set\nof possible states consistent with the observation history. Each update step requires\ninference using the transition model of the environment, which is built from successor-\nstate axioms that specify how each ﬂuent changes.\n• Decisions within a logical agent can be made by SAT solving: ﬁnding possible models\nspecifying future action sequences that reach the goal. This approach works only for\nfully observable or sensorless environments.\n• Propositional logic does not scale to environments of unbounded size because it lacks\nthe expressive power to deal concisely with time, space, and universal patterns of rela-\ntionships among objects.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nJohn McCarthy’s paper “Programs with Common Sense” (McCarthy, 1958, 1968) promul-\ngated the notion of agents that use logical reasoning to mediate between percepts and actions.\nIt also raised the ﬂag of declarativism, pointing out that telling an agent what it needs to know\nis an elegant way to build software. Allen Newell’s (1982) article “The Knowledge Level”",
  "It also raised the ﬂag of declarativism, pointing out that telling an agent what it needs to know\nis an elegant way to build software. Allen Newell’s (1982) article “The Knowledge Level”\nmakes the case that rational agents can be described and analyzed at an abstract level deﬁned\nby the knowledge they possess rather than the programs they run. The declarative and proce-\ndural approaches to AI are analyzed in depth by Boden (1977). The debate was revived by,\namong others, Brooks (1991) and Nilsson (1991), and continues to this day (Shaparau et al.,\n2008). Meanwhile, the declarative approach has spread into other areas of computer science\nsuch as networking (Loo et al., 2006).\nLogic itself had its origins in ancient Greek philosophy and mathematics. Various log-\nical principles—principles connecting the syntactic structure of sentences with their truth\nand falsity, with their meaning, or with the validity of arguments in which they ﬁgure—are\nscattered in the works of Plato. The ﬁrst known systematic study of logic was carried out\nby Aristotle, whose work was assembled by his students after his death in 322 B.C. as a\ntreatise called the Organon. Aristotle’s syllogisms were what we would now call inference\nSYLLOGISM\nrules. Although the syllogisms included elements of both propositional and ﬁrst-order logic,\nthe system as a whole lacked the compositional properties required to handle sentences of\narbitrary complexity.\nThe closely related Megarian and Stoic schools (originating in the ﬁfth century B.C.\nand continuing for several centuries thereafter) began the systematic study of the basic logical\nconnectives. The use of truth tables for deﬁning connectives is due to Philo of Megara. The 276\nChapter\n7.\nLogical Agents\nStoics took ﬁve basic inference rules as valid without proof, including the rule we now call\nModus Ponens. They derived a number of other rules from these ﬁve, using, among other\nprinciples, the deduction theorem (page 249) and were much clearer about the notion of\nproof than was Aristotle. A good account of the history of Megarian and Stoic logic is given\nby Benson Mates (1953).\nThe idea of reducing logical inference to a purely mechanical process applied to a for-\nmal language is due to Wilhelm Leibniz (1646–1716), although he had limited success in im-\nplementing the ideas. George Boole (1847) introduced the ﬁrst comprehensive and workable\nsystem of formal logic in his book The Mathematical Analysis of Logic. Boole’s logic was",
  "plementing the ideas. George Boole (1847) introduced the ﬁrst comprehensive and workable\nsystem of formal logic in his book The Mathematical Analysis of Logic. Boole’s logic was\nclosely modeled on the ordinary algebra of real numbers and used substitution of logically\nequivalent expressions as its primary inference method. Although Boole’s system still fell\nshort of full propositional logic, it was close enough that other mathematicians could quickly\nﬁll in the gaps. Schr¨oder (1877) described conjunctive normal form, while Horn form was\nintroduced much later by Alfred Horn (1951). The ﬁrst comprehensive exposition of modern\npropositional logic (and ﬁrst-order logic) is found in Gottlob Frege’s (1879) Begriffschrift\n(“Concept Writing” or “Conceptual Notation”).\nThe ﬁrst mechanical device to carry out logical inferences was constructed by the third\nEarl of Stanhope (1753–1816). The Stanhope Demonstrator could handle syllogisms and\ncertain inferences in the theory of probability. William Stanley Jevons, one of those who\nimproved upon and extended Boole’s work, constructed his “logical piano” in 1869 to per-\nform inferences in Boolean logic. An entertaining and instructive history of these and other\nearly mechanical devices for reasoning is given by Martin Gardner (1968). The ﬁrst pub-\nlished computer program for logical inference was the Logic Theorist of Newell, Shaw,\nand Simon (1957). This program was intended to model human thought processes. Mar-\ntin Davis (1957) had actually designed a program that came up with a proof in 1954, but the\nLogic Theorist’s results were published slightly earlier.\nTruth tables as a method of testing validity or unsatisﬁability in propositional logic were\nintroduced independently by Emil Post (1921) and Ludwig Wittgenstein (1922). In the 1930s,\na great deal of progress was made on inference methods for ﬁrst-order logic. In particular,\nG¨odel (1930) showed that a complete procedure for inference in ﬁrst-order logic could be\nobtained via a reduction to propositional logic, using Herbrand’s theorem (Herbrand, 1930).\nWe take up this history again in Chapter 9; the important point here is that the development\nof efﬁcient propositional algorithms in the 1960s was motivated largely by the interest of\nmathematicians in an effective theorem prover for ﬁrst-order logic. The Davis–Putnam algo-\nrithm (Davis and Putnam, 1960) was the ﬁrst effective algorithm for propositional resolution",
  "mathematicians in an effective theorem prover for ﬁrst-order logic. The Davis–Putnam algo-\nrithm (Davis and Putnam, 1960) was the ﬁrst effective algorithm for propositional resolution\nbut was in most cases much less efﬁcient than the DPLL backtracking algorithm introduced\ntwo years later (1962). The full resolution rule and a proof of its completeness appeared in a\nseminal paper by J. A. Robinson (1965), which also showed how to do ﬁrst-order reasoning\nwithout resort to propositional techniques.\nStephen Cook (1971) showed that deciding satisﬁability of a sentence in propositional\nlogic (the SAT problem) is NP-complete. Since deciding entailment is equivalent to decid-\ning unsatisﬁability, it is co-NP-complete. Many subsets of propositional logic are known for\nwhich the satisﬁability problem is polynomially solvable; Horn clauses are one such subset. Bibliographical and Historical Notes\n277\nThe linear-time forward-chaining algorithm for Horn clauses is due to Dowling and Gallier\n(1984), who describe their algorithm as a dataﬂow process similar to the propagation of sig-\nnals in a circuit.\nEarly theoretical investigations showed that DPLL has polynomial average-case com-\nplexity for certain natural distributions of problems. This potentially exciting fact became\nless exciting when Franco and Paull (1983) showed that the same problems could be solved\nin constant time simply by guessing random assignments. The random-generation method\ndescribed in the chapter produces much harder problems. Motivated by the empirical success\nof local search on these problems, Koutsoupias and Papadimitriou (1992) showed that a sim-\nple hill-climbing algorithm can solve almost all satisﬁability problem instances very quickly,\nsuggesting that hard problems are rare. Moreover, Sch¨oning (1999) exhibited a randomized\nhill-climbing algorithm whose worst-case expected run time on 3-SAT problems (that is, sat-\nisﬁability of 3-CNF sentences) is O(1.333n)—still exponential, but substantially faster than\nprevious worst-case bounds. The current record is O(1.324n) (Iwama and Tamaki, 2004).\nAchlioptas et al. (2004) and Alekhnovich et al. (2005) exhibit families of 3-SAT instances\nfor which all known DPLL-like algorithms require exponential running time.\nOn the practical side, efﬁciency gains in propositional solvers have been marked. Given\nten minutes of computing time, the original DPLL algorithm in 1962 could only solve prob-",
  "On the practical side, efﬁciency gains in propositional solvers have been marked. Given\nten minutes of computing time, the original DPLL algorithm in 1962 could only solve prob-\nlems with no more than 10 or 15 variables. By 1995 the SATZ solver (Li and Anbulagan,\n1997) could handle 1,000 variables, thanks to optimized data structures for indexing vari-\nables. Two crucial contributions were the watched literal indexing technique of Zhang and\nStickel (1996), which makes unit propagation very efﬁcient, and the introduction of clause\n(i.e., constraint) learning techniques from the CSP community by Bayardo and Schrag (1997).\nUsing these ideas, and spurred by the prospect of solving industrial-scale circuit veriﬁcation\nproblems, Moskewicz et al. (2001) developed the CHAFF solver, which could handle prob-\nlems with millions of variables. Beginning in 2002, SAT competitions have been held reg-\nularly; most of the winning entries have either been descendants of CHAFF or have used the\nsame general approach. RSAT (Pipatsrisawat and Darwiche, 2007), the 2007 winner, falls in\nthe latter category. Also noteworthy is MINISAT (Een and S¨orensson, 2003), an open-source\nimplementation available at http://minisat.se that is designed to be easily modiﬁed\nand improved. The current landscape of solvers is surveyed by Gomes et al. (2008).\nLocal search algorithms for satisﬁability were tried by various authors throughout the\n1980s; all of the algorithms were based on the idea of minimizing the number of unsatisﬁed\nclauses (Hansen and Jaumard, 1990). A particularly effective algorithm was developed by\nGu (1989) and independently by Selman et al. (1992), who called it GSAT and showed that\nit was capable of solving a wide range of very hard problems very quickly. The WALKSAT\nalgorithm described in the chapter is due to Selman et al. (1996).\nThe “phase transition” in satisﬁability of random k-SAT problems was ﬁrst observed\nby Simon and Dubois (1989) and has given rise to a great deal of theoretical and empirical\nresearch—due, in part, to the obvious connection to phase transition phenomena in statistical\nphysics. Cheeseman et al. (1991) observed phase transitions in several CSPs and conjecture\nthat all NP-hard problems have a phase transition. Crawford and Auton (1993) located the\n3-SAT transition at a clause/variable ratio of around 4.26, noting that this coincides with a 278\nChapter\n7.\nLogical Agents",
  "that all NP-hard problems have a phase transition. Crawford and Auton (1993) located the\n3-SAT transition at a clause/variable ratio of around 4.26, noting that this coincides with a 278\nChapter\n7.\nLogical Agents\nsharp peak in the run time of their SAT solver. Cook and Mitchell (1997) provide an excellent\nsummary of the early literature on the problem.\nThe current state of theoretical understanding is summarized by Achlioptas (2009).\nThe satisﬁability threshold conjecture states that, for each k, there is a sharp satisﬁability\nSATISFIABILITY\nTHRESHOLD\nCONJECTURE\nthreshold rk, such that as the number of variables n →∞, instances below the threshold are\nsatisﬁable with probability 1, while those above the threshold are unsatisﬁable with proba-\nbility 1. The conjecture was not quite proved by Friedgut (1999): a sharp threshold exists but\nits location might depend on n even as n →∞. Despite signiﬁcant progress in asymptotic\nanalysis of the threshold location for large k (Achlioptas and Peres, 2004; Achlioptas et al.,\n2007), all that can be proved for k = 3 is that it lies in the range [3.52,4.51]. Current theory\nsuggests that a peak in the run time of a SAT solver is not necessarily related to the satisﬁa-\nbility threshold, but instead to a phase transition in the solution distribution and structure of\nSAT instances. Empirical results due to Coarfa et al. (2003) support this view. In fact, al-\ngorithms such as survey propagation (Parisi and Zecchina, 2002; Maneva et al., 2007) take\nSURVEY\nPROPAGATION\nadvantage of special properties of random SAT instances near the satisﬁability threshold and\ngreatly outperform general SAT solvers on such instances.\nThe best sources for information on satisﬁability, both theoretical and practical, are the\nHandbook of Satisﬁability (Biere et al., 2009) and the regular International Conferences on\nTheory and Applications of Satisﬁability Testing, known as SAT.\nThe idea of building agents with propositional logic can be traced back to the seminal\npaper of McCulloch and Pitts (1943), which initiated the ﬁeld of neural networks. Con-\ntrary to popular supposition, the paper was concerned with the implementation of a Boolean\ncircuit-based agent design in the brain. Circuit-based agents, which perform computation by\npropagating signals in hardware circuits rather than running algorithms in general-purpose\ncomputers, have received little attention in AI, however. The most notable exception is the",
  "propagating signals in hardware circuits rather than running algorithms in general-purpose\ncomputers, have received little attention in AI, however. The most notable exception is the\nwork of Stan Rosenschein (Rosenschein, 1985; Kaelbling and Rosenschein, 1990), who de-\nveloped ways to compile circuit-based agents from declarative descriptions of the task envi-\nronment. (Rosenschein’s approach is described at some length in the second edition of this\nbook.) The work of Rod Brooks (1986, 1989) demonstrates the effectiveness of circuit-based\ndesigns for controlling robots—a topic we take up in Chapter 25. Brooks (1991) argues\nthat circuit-based designs are all that is needed for AI—that representation and reasoning\nare cumbersome, expensive, and unnecessary. In our view, neither approach is sufﬁcient by\nitself. Williams et al. (2003) show how a hybrid agent design not too different from our\nwumpus agent has been used to control NASA spacecraft, planning sequences of actions and\ndiagnosing and recovering from faults.\nThe general problem of keeping track of a partially observable environment was intro-\nduced for state-based representations in Chapter 4. Its instantiation for propositional repre-\nsentations was studied by Amir and Russell (2003), who identiﬁed several classes of envi-\nronments that admit efﬁcient state-estimation algorithms and showed that for several other\nclasses the problem is intractable. The temporal-projection problem, which involves deter-\nTEMPORAL-\nPROJECTION\nmining what propositions hold true after an action sequence is executed, can be seen as a\nspecial case of state estimation with empty percepts. Many authors have studied this problem\nbecause of its importance in planning; some important hardness results were established by Exercises\n279\nLiberatore (1997). The idea of representing a belief state with propositions can be traced to\nWittgenstein (1922).\nLogical state estimation, of course, requires a logical representation of the effects of\nactions—a key problem in AI since the late 1950s. The dominant proposal has been the sit-\nuation calculus formalism (McCarthy, 1963), which is couched within ﬁrst-order logic. We\ndiscuss situation calculus, and various extensions and alternatives, in Chapters 10 and 12. The\napproach taken in this chapter—using temporal indices on propositional variables—is more\nrestrictive but has the beneﬁt of simplicity. The general approach embodied in the SATPLAN",
  "approach taken in this chapter—using temporal indices on propositional variables—is more\nrestrictive but has the beneﬁt of simplicity. The general approach embodied in the SATPLAN\nalgorithm was proposed by Kautz and Selman (1992). Later generations of SATPLAN were\nable to take advantage of the advances in SAT solvers, described earlier, and remain among\nthe most effective ways of solving difﬁcult problems (Kautz, 2006).\nThe frame problem was ﬁrst recognized by McCarthy and Hayes (1969). Many re-\nsearchers considered the problem unsolvable within ﬁrst-order logic, and it spurred a great\ndeal of research into nonmonotonic logics. Philosophers from Dreyfus (1972) to Crockett\n(1994) have cited the frame problem as one symptom of the inevitable failure of the entire\nAI enterprise. The solution of the frame problem with successor-state axioms is due to Ray\nReiter (1991). Thielscher (1999) identiﬁes the inferential frame problem as a separate idea\nand provides a solution. In retrospect, one can see that Rosenschein’s (1985) agents were\nusing circuits that implemented successor-state axioms, but Rosenschein did not notice that\nthe frame problem was thereby largely solved. Foo (2001) explains why the discrete-event\ncontrol theory models typically used by engineers do not have to explicitly deal with the\nframe problem: because they are dealing with prediction and control, not with explanation\nand reasoning about counterfactual situations.\nModern propositional solvers have wide applicability in industrial applications. The ap-\nplication of propositional inference in the synthesis of computer hardware is now a standard\ntechnique having many large-scale deployments (Nowick et al., 1993). The SATMC satisﬁ-\nability checker was used to detect a previously unknown vulnerability in a Web browser user\nsign-on protocol (Armando et al., 2008).\nThe wumpus world was invented by Gregory Yob (1975). Ironically, Yob developed it\nbecause he was bored with games played on a rectangular grid: the topology of his original\nwumpus world was a dodecahedron, and we put it back in the boring old grid. Michael\nGenesereth was the ﬁrst to suggest that the wumpus world be used as an agent testbed.\nEXERCISES\n7.1\nSuppose the agent has progressed to the point shown in Figure 7.4(a), page 239, having\nperceived nothing in [1,1], a breeze in [2,1], and a stench in [1,2], and is now concerned with\nthe contents of [1,3], [2,2], and [3,1]. Each of these can contain a pit, and at most one can",
  "perceived nothing in [1,1], a breeze in [2,1], and a stench in [1,2], and is now concerned with\nthe contents of [1,3], [2,2], and [3,1]. Each of these can contain a pit, and at most one can\ncontain a wumpus. Following the example of Figure 7.5, construct the set of possible worlds.\n(You should ﬁnd 32 of them.) Mark the worlds in which the KB is true and those in which 280\nChapter\n7.\nLogical Agents\neach of the following sentences is true:\nα2 = “There is no pit in [2,2].”\nα3 = “There is a wumpus in [1,3].”\nHence show that KB |= α2 and KB |= α3.\n7.2\n(Adapted from Barwise and Etchemendy (1993).) Given the following, can you prove\nthat the unicorn is mythical? How about magical? Horned?\nIf the unicorn is mythical, then it is immortal, but if it is not mythical, then it is a\nmortal mammal. If the unicorn is either immortal or a mammal, then it is horned.\nThe unicorn is magical if it is horned.\n7.3\nConsider the problem of deciding whether a propositional logic sentence is true in a\ngiven model.\na. Write a recursive algorithm PL-TRUE?(s, m) that returns true if and only if the sen-\ntence s is true in the model m (where m assigns a truth value for every symbol in s).\nThe algorithm should run in time linear in the size of the sentence. (Alternatively, use a\nversion of this function from the online code repository.)\nb. Give three examples of sentences that can be determined to be true or false in a partial\nmodel that does not specify a truth value for some of the symbols.\nc. Show that the truth value (if any) of a sentence in a partial model cannot be determined\nefﬁciently in general.\nd. Modify your PL-TRUE? algorithm so that it can sometimes judge truth from partial\nmodels, while retaining its recursive structure and linear run time. Give three examples\nof sentences whose truth in a partial model is not detected by your algorithm.\ne. Investigate whether the modiﬁed algorithm makes TT-ENTAILS? more efﬁcient.\n7.4\nWhich of the following are correct?\na. False |= True.\nb. True |= False.\nc. (A ∧B) |= (A ⇔B).\nd. A ⇔B |= A ∨B.\ne. A ⇔B |= ¬A ∨B.\nf. (A ∧B) ⇒C |= (A ⇒C) ∨(B ⇒C).\ng. (C ∨(¬A ∧¬B)) ≡((A ⇒C) ∧(B ⇒C)).\nh. (A ∨B) ∧(¬C ∨¬D ∨E) |= (A ∨B).\ni. (A ∨B) ∧(¬C ∨¬D ∨E) |= (A ∨B) ∧(¬D ∨E).\nj. (A ∨B) ∧¬(A ⇒B) is satisﬁable.\nk. (A ⇔B) ∧(¬A ∨B) is satisﬁable.\nl. (A ⇔B) ⇔C has the same number of models as (A ⇔B) for any ﬁxed set of\nproposition symbols that includes A, B, C. Exercises\n281\n7.5\nProve each of the following assertions:\na. α is valid if and only if True |= α.",
  "k. (A ⇔B) ∧(¬A ∨B) is satisﬁable.\nl. (A ⇔B) ⇔C has the same number of models as (A ⇔B) for any ﬁxed set of\nproposition symbols that includes A, B, C. Exercises\n281\n7.5\nProve each of the following assertions:\na. α is valid if and only if True |= α.\nb. For any α, False |= α.\nc. α |= β if and only if the sentence (α ⇒β) is valid.\nd. α ≡β if and only if the sentence (α ⇔β) is valid.\ne. α |= β if and only if the sentence (α ∧¬β) is unsatisﬁable.\n7.6\nProve, or ﬁnd a counterexample to, each of the following assertions:\na. If α |= γ or β |= γ (or both) then (α ∧β) |= γ\nb. If α |= (β ∧γ) then α |= β and α |= γ.\nc. If α |= (β ∨γ) then α |= β or α |= γ (or both).\n7.7\nConsider a vocabulary with only four propositions, A, B, C, and D. How many models\nare there for the following sentences?\na. B ∨C.\nb. ¬A ∨¬B ∨¬C ∨¬D.\nc. (A ⇒B) ∧A ∧¬B ∧C ∧D.\n7.8\nWe have deﬁned four binary logical connectives.\na. Are there any others that might be useful?\nb. How many binary connectives can there be?\nc. Why are some of them not very useful?\n7.9\nUsing a method of your choice, verify each of the equivalences in Figure 7.11 (page 249).\n7.10\nDecide whether each of the following sentences is valid, unsatisﬁable, or neither. Ver-\nify your decisions using truth tables or the equivalence rules of Figure 7.11 (page 249).\na. Smoke ⇒Smoke\nb. Smoke ⇒Fire\nc. (Smoke ⇒Fire) ⇒(¬Smoke ⇒¬Fire)\nd. Smoke ∨Fire ∨¬Fire\ne. ((Smoke ∧Heat) ⇒Fire) ⇔((Smoke ⇒Fire) ∨(Heat ⇒Fire))\nf. (Smoke ⇒Fire) ⇒((Smoke ∧Heat) ⇒Fire)\ng. Big ∨Dumb ∨(Big ⇒Dumb)\n7.11\nAny propositional logic sentence is logically equivalent to the assertion that each pos-\nsible world in which it would be false is not the case. From this observation, prove that any\nsentence can be written in CNF.\n7.12\nUse resolution to prove the sentence ¬A∧¬B from the clauses in Exercise 7.20.\n7.13\nThis exercise looks into the relationship between clauses and implication sentences. 282\nChapter\n7.\nLogical Agents\na. Show that the clause (¬P1 ∨· · · ∨¬Pm ∨Q) is logically equivalent to the implication\nsentence (P1 ∧· · · ∧Pm) ⇒Q.\nb. Show that every clause (regardless of the number of positive literals) can be written in\nthe form (P1 ∧· · · ∧Pm) ⇒(Q1 ∨· · · ∨Qn), where the Ps and Qs are proposition\nsymbols. A knowledge base consisting of such sentences is in implicative normal\nform or Kowalski form (Kowalski, 1979).\nIMPLICATIVE\nNORMAL FORM\nc. Write down the full resolution rule for sentences in implicative normal form.\n7.14",
  "symbols. A knowledge base consisting of such sentences is in implicative normal\nform or Kowalski form (Kowalski, 1979).\nIMPLICATIVE\nNORMAL FORM\nc. Write down the full resolution rule for sentences in implicative normal form.\n7.14\nAccording to some political pundits, a person who is radical (R) is electable (E) if\nhe/she is conservative (C), but otherwise is not electable.\na. Which of the following are correct representations of this assertion?\n(i) (R ∧E) ⇐⇒C\n(ii) R ⇒(E ⇐⇒C)\n(iii) R ⇒((C ⇒E) ∨¬E)\nb. Which of the sentences in (a) can be expressed in Horn form?\n7.15\nThis question considers representing satisﬁability (SAT) problems as CSPs.\na. Draw the constraint graph corresponding to the SAT problem\n(¬X1 ∨X2) ∧(¬X2 ∨X3) ∧. . . ∧(¬Xn−1 ∨Xn)\nfor the particular case n = 5.\nb. How many solutions are there for this general SAT problem as a function of n?\nc. Suppose we apply BACKTRACKING-SEARCH (page 215) to ﬁnd all solutions to a SAT\nCSP of the type given in (a). (To ﬁnd all solutions to a CSP, we simply modify the\nbasic algorithm so it continues searching after each solution is found.) Assume that\nvariables are ordered X1, . . . , Xn and false is ordered before true. How much time\nwill the algorithm take to terminate? (Write an O(·) expression as a function of n.)\nd. We know that SAT problems in Horn form can be solved in linear time by forward\nchaining (unit propagation). We also know that every tree-structured binary CSP with\ndiscrete, ﬁnite domains can be solved in time linear in the number of variables (Sec-\ntion 6.5). Are these two facts connected? Discuss.\n7.16\nExplain why every nonempty propositional clause, by itself, is satisﬁable. Prove rig-\norously that every set of ﬁve 3-SAT clauses is satisﬁable, provided that each clause mentions\nexactly three distinct variables. What is the smallest set of such clauses that is unsatisﬁable?\nConstruct such a set.\n7.17\nA propositional 2-CNF expression is a conjunction of clauses, each containing exactly\n2 literals, e.g.,\n(A ∨B) ∧(¬A ∨C) ∧(¬B ∨D) ∧(¬C ∨G) ∧(¬D ∨G) .\na. Prove using resolution that the above sentence entails G. Exercises\n283\nb. Two clauses are semantically distinct if they are not logically equivalent. How many\nsemantically distinct 2-CNF clauses can be constructed from n proposition symbols?\nc. Using your answer to (b), prove that propositional resolution always terminates in time\npolynomial in n given a 2-CNF sentence containing no more than n distinct symbols.",
  "c. Using your answer to (b), prove that propositional resolution always terminates in time\npolynomial in n given a 2-CNF sentence containing no more than n distinct symbols.\nd. Explain why your argument in (c) does not apply to 3-CNF.\n7.18\nConsider the following sentence:\n[(Food ⇒Party) ∨(Drinks ⇒Party)] ⇒[(Food ∧Drinks) ⇒Party] .\na. Determine, using enumeration, whether this sentence is valid, satisﬁable (but not valid),\nor unsatisﬁable.\nb. Convert the left-hand and right-hand sides of the main implication into CNF, showing\neach step, and explain how the results conﬁrm your answer to (a).\nc. Prove your answer to (a) using resolution.\n7.19\nA sentence is in disjunctive normal form (DNF) if it is the disjunction of conjunctions\nDISJUNCTIVE\nNORMAL FORM\nof literals. For example, the sentence (A ∧B ∧¬C) ∨(¬A ∧C) ∨(B ∧¬C) is in DNF.\na. Any propositional logic sentence is logically equivalent to the assertion that some pos-\nsible world in which it would be true is in fact the case. From this observation, prove\nthat any sentence can be written in DNF.\nb. Construct an algorithm that converts any sentence in propositional logic into DNF.\n(Hint: The algorithm is similar to the algorithm for conversion to CNF given in Sec-\ntion 7.5.2.)\nc. Construct a simple algorithm that takes as input a sentence in DNF and returns a satis-\nfying assignment if one exists, or reports that no satisfying assignment exists.\nd. Apply the algorithms in (b) and (c) to the following set of sentences:\nA ⇒B\nB ⇒C\nC ⇒¬A .\ne. Since the algorithm in (b) is very similar to the algorithm for conversion to CNF, and\nsince the algorithm in (c) is much simpler than any algorithm for solving a set of sen-\ntences in CNF, why is this technique not used in automated reasoning?\n7.20\nConvert the following set of sentences to clausal form.\nS1: A ⇔(B ∨E).\nS2: E ⇒D.\nS3: C ∧F ⇒¬B.\nS4: E ⇒B.\nS5: B ⇒F.\nS6: B ⇒C\nGive a trace of the execution of DPLL on the conjunction of these clauses. 284\nChapter\n7.\nLogical Agents\n7.21\nIs a randomly generated 4-CNF sentence with n symbols and m clauses more or less\nlikely to be solvable than a randomly generated 3-CNF sentence with n symbols and m\nclauses? Explain.\n7.22\nMinesweeper, the well-known computer game, is closely related to the wumpus world.\nA minesweeper world is a rectangular grid of N squares with M invisible mines scattered\namong them. Any square may be probed by the agent; instant death follows if a mine is",
  "A minesweeper world is a rectangular grid of N squares with M invisible mines scattered\namong them. Any square may be probed by the agent; instant death follows if a mine is\nprobed. Minesweeper indicates the presence of mines by revealing, in each probed square,\nthe number of mines that are directly or diagonally adjacent. The goal is to probe every\nunmined square.\na. Let Xi,j be true iff square [i, j] contains a mine. Write down the assertion that exactly\ntwo mines are adjacent to [1,1] as a sentence involving some logical combination of\nXi,j propositions.\nb. Generalize your assertion from (a) by explaining how to construct a CNF sentence\nasserting that k of n neighbors contain mines.\nc. Explain precisely how an agent can use DPLL to prove that a given square does (or\ndoes not) contain a mine, ignoring the global constraint that there are exactly M mines\nin all.\nd. Suppose that the global constraint is constructed from your method from part (b). How\ndoes the number of clauses depend on M and N? Suggest a way to modify DPLL so\nthat the global constraint does not need to be represented explicitly.\ne. Are any conclusions derived by the method in part (c) invalidated when the global\nconstraint is taken into account?\nf. Give examples of conﬁgurations of probe values that induce long-range dependencies\nsuch that the contents of a given unprobed square would give information about the\ncontents of a far-distant square. (Hint: consider an N × 1 board.)\n7.23\nHow long does it take to prove KB |= α using DPLL when α is a literal already\ncontained in KB? Explain.\n7.24\nTrace the behavior of DPLL on the knowledge base in Figure 7.16 when trying to\nprove Q, and compare this behavior with that of the forward-chaining algorithm.\n7.25\nWrite a successor-state axiom for the Locked predicate, which applies to doors, as-\nsuming the only actions available are Lock and Unlock.\n7.26\nSection 7.7.1 provides some of the successor-state axioms required for the wumpus\nworld. Write down axioms for all remaining ﬂuent symbols.\n7.27\nModify the HYBRID-WUMPUS-AGENT to use the 1-CNF logical state estimation\nmethod described on page 271. We noted on that page that such an agent will not be able\nto acquire, maintain, and use more complex beliefs such as the disjunction P3,1 ∨P2,2. Sug-\ngest a method for overcoming this problem by deﬁning additional proposition symbols, and\ntry it out in the wumpus world. Does it improve the performance of the agent? 8\nFIRST-ORDER LOGIC",
  "gest a method for overcoming this problem by deﬁning additional proposition symbols, and\ntry it out in the wumpus world. Does it improve the performance of the agent? 8\nFIRST-ORDER LOGIC\nIn which we notice that the world is blessed with many objects, some of which are\nrelated to other objects, and in which we endeavor to reason about them.\nIn Chapter 7, we showed how a knowledge-based agent could represent the world in which it\noperates and deduce what actions to take. We used propositional logic as our representation\nlanguage because it sufﬁced to illustrate the basic concepts of logic and knowledge-based\nagents. Unfortunately, propositional logic is too puny a language to represent knowledge\nof complex environments in a concise way. In this chapter, we examine ﬁrst-order logic,1\nFIRST-ORDER LOGIC\nwhich is sufﬁciently expressive to represent a good deal of our commonsense knowledge.\nIt also either subsumes or forms the foundation of many other representation languages and\nhas been studied intensively for many decades. We begin in Section 8.1 with a discussion of\nrepresentation languages in general; Section 8.2 covers the syntax and semantics of ﬁrst-order\nlogic; Sections 8.3 and 8.4 illustrate the use of ﬁrst-order logic for simple representations.\n8.1\nREPRESENTATION REVISITED\nIn this section, we discuss the nature of representation languages. Our discussion motivates\nthe development of ﬁrst-order logic, a much more expressive language than the propositional\nlogic introduced in Chapter 7. We look at propositional logic and at other kinds of languages\nto understand what works and what fails. Our discussion will be cursory, compressing cen-\nturies of thought, trial, and error into a few paragraphs.\nProgramming languages (such as C++ or Java or Lisp) are by far the largest class of\nformal languages in common use. Programs themselves represent, in a direct sense, only\ncomputational processes. Data structures within programs can represent facts; for example,\na program could use a 4 × 4 array to represent the contents of the wumpus world. Thus, the\nprogramming language statement World[2,2] ←Pit is a fairly natural way to assert that there\nis a pit in square [2,2]. (Such representations might be considered ad hoc; database systems\nwere developed precisely to provide a more general, domain-independent way to store and\n1 Also called ﬁrst-order predicate calculus, sometimes abbreviated as FOL or FOPC.\n285 286\nChapter\n8.\nFirst-Order Logic",
  "were developed precisely to provide a more general, domain-independent way to store and\n1 Also called ﬁrst-order predicate calculus, sometimes abbreviated as FOL or FOPC.\n285 286\nChapter\n8.\nFirst-Order Logic\nretrieve facts.) What programming languages lack is any general mechanism for deriving\nfacts from other facts; each update to a data structure is done by a domain-speciﬁc procedure\nwhose details are derived by the programmer from his or her own knowledge of the domain.\nThis procedural approach can be contrasted with the declarative nature of propositional logic,\nin which knowledge and inference are separate, and inference is entirely domain independent.\nA second drawback of data structures in programs (and of databases, for that matter)\nis the lack of any easy way to say, for example, “There is a pit in [2,2] or [3,1]” or “If the\nwumpus is in [1,1] then he is not in [2,2].” Programs can store a single value for each variable,\nand some systems allow the value to be “unknown,” but they lack the expressiveness required\nto handle partial information.\nPropositional logic is a declarative language because its semantics is based on a truth\nrelation between sentences and possible worlds. It also has sufﬁcient expressive power to\ndeal with partial information, using disjunction and negation. Propositional logic has a third\nproperty that is desirable in representation languages, namely, compositionality. In a com-\nCOMPOSITIONALITY\npositional language, the meaning of a sentence is a function of the meaning of its parts. For\nexample, the meaning of “S1,4 ∧S1,2” is related to the meanings of “S1,4” and “S1,2.” It\nwould be very strange if “S1,4” meant that there is a stench in square [1,4] and “S1,2” meant\nthat there is a stench in square [1,2], but “S1,4 ∧S1,2” meant that France and Poland drew 1–1\nin last week’s ice hockey qualifying match. Clearly, noncompositionality makes life much\nmore difﬁcult for the reasoning system.\nAs we saw in Chapter 7, however, propositional logic lacks the expressive power to\nconcisely describe an environment with many objects. For example, we were forced to write\na separate rule about breezes and pits for each square, such as\nB1,1 ⇔(P1,2 ∨P2,1) .\nIn English, on the other hand, it seems easy enough to say, once and for all, “Squares adjacent\nto pits are breezy.” The syntax and semantics of English somehow make it possible to describe\nthe environment concisely.\n8.1.1\nThe language of thought",
  "In English, on the other hand, it seems easy enough to say, once and for all, “Squares adjacent\nto pits are breezy.” The syntax and semantics of English somehow make it possible to describe\nthe environment concisely.\n8.1.1\nThe language of thought\nNatural languages (such as English or Spanish) are very expressive indeed. We managed to\nwrite almost this whole book in natural language, with only occasional lapses into other lan-\nguages (including logic, mathematics, and the language of diagrams). There is a long tradi-\ntion in linguistics and the philosophy of language that views natural language as a declarative\nknowledge representation language. If we could uncover the rules for natural language, we\ncould use it in representation and reasoning systems and gain the beneﬁt of the billions of\npages that have been written in natural language.\nThe modern view of natural language is that it serves a as a medium for communication\nrather than pure representation. When a speaker points and says, “Look!” the listener comes\nto know that, say, Superman has ﬁnally appeared over the rooftops. Yet we would not want\nto say that the sentence “Look!” represents that fact. Rather, the meaning of the sentence\ndepends both on the sentence itself and on the context in which the sentence was spoken.\nClearly, one could not store a sentence such as “Look!” in a knowledge base and expect to Section 8.1.\nRepresentation Revisited\n287\nrecover its meaning without also storing a representation of the context—which raises the\nquestion of how the context itself can be represented. Natural languages also suffer from\nambiguity, a problem for a representation language. As Pinker (1995) puts it: “When people\nAMBIGUITY\nthink about spring, surely they are not confused as to whether they are thinking about a season\nor something that goes boing—and if one word can correspond to two thoughts, thoughts\ncan’t be words.”\nThe famous Sapir–Whorf hypothesis claims that our understanding of the world is\nstrongly inﬂuenced by the language we speak. Whorf (1956) wrote “We cut nature up, orga-\nnize it into concepts, and ascribe signiﬁcances as we do, largely because we are parties to an\nagreement to organize it this way—an agreement that holds throughout our speech commu-\nnity and is codiﬁed in the patterns of our language.” It is certainly true that different speech\ncommunities divide up the world differently. The French have two words “chaise” and “fau-",
  "nity and is codiﬁed in the patterns of our language.” It is certainly true that different speech\ncommunities divide up the world differently. The French have two words “chaise” and “fau-\nteuil,” for a concept that English speakers cover with one: “chair.” But English speakers\ncan easily recognize the category fauteuil and give it a name—roughly “open-arm chair”—so\ndoes language really make a difference? Whorf relied mainly on intuition and speculation,\nbut in the intervening years we actually have real data from anthropological, psychological\nand neurological studies.\nFor example, can you remember which of the following two phrases formed the opening\nof Section 8.1?\n“In this section, we discuss the nature of representation languages . . .”\n“This section covers the topic of knowledge representation languages . . .”\nWanner (1974) did a similar experiment and found that subjects made the right choice at\nchance level—about 50% of the time—but remembered the content of what they read with\nbetter than 90% accuracy. This suggests that people process the words to form some kind of\nnonverbal representation.\nMore interesting is the case in which a concept is completely absent in a language.\nSpeakers of the Australian aboriginal language Guugu Yimithirr have no words for relative\ndirections, such as front, back, right, or left. Instead they use absolute directions, saying,\nfor example, the equivalent of “I have a pain in my north arm.” This difference in language\nmakes a difference in behavior: Guugu Yimithirr speakers are better at navigating in open\nterrain, while English speakers are better at placing the fork to the right of the plate.\nLanguage also seems to inﬂuence thought through seemingly arbitrary grammatical\nfeatures such as the gender of nouns. For example, “bridge” is masculine in Spanish and\nfeminine in German. Boroditsky (2003) asked subjects to choose English adjectives to de-\nscribe a photograph of a particular bridge. Spanish speakers chose big, dangerous, strong,\nand towering, whereas German speakers chose beautiful, elegant, fragile, and slender. Words\ncan serve as anchor points that affect how we perceive the world. Loftus and Palmer (1974)\nshowed experimental subjects a movie of an auto accident. Subjects who were asked “How\nfast were the cars going when they contacted each other?” reported an average of 32 mph,\nwhile subjects who were asked the question with the word “smashed” instead of “contacted”",
  "fast were the cars going when they contacted each other?” reported an average of 32 mph,\nwhile subjects who were asked the question with the word “smashed” instead of “contacted”\nreported 41mph for the same cars in the same movie. 288\nChapter\n8.\nFirst-Order Logic\nIn a ﬁrst-order logic reasoning system that uses CNF, we can see that the linguistic form\n“¬(A ∨B)” and “¬A ∧¬B” are the same because we can look inside the system and see\nthat the two sentences are stored as the same canonical CNF form. Can we do that with the\nhuman brain? Until recently the answer was “no,” but now it is “maybe.” Mitchell et al.\n(2008) put subjects in an fMRI (functional magnetic resonance imaging) machine, showed\nthem words such as “celery,” and imaged their brains. The researchers were then able to train\na computer program to predict, from a brain image, what word the subject had been presented\nwith. Given two choices (e.g., “celery” or “airplane”), the system predicts correctly 77% of\nthe time. The system can even predict at above-chance levels for words it has never seen\nan fMRI image of before (by considering the images of related words) and for people it has\nnever seen before (proving that fMRI reveals some level of common representation across\npeople). This type of work is still in its infancy, but fMRI (and other imaging technology\nsuch as intracranial electrophysiology (Sahin et al., 2009)) promises to give us much more\nconcrete ideas of what human knowledge representations are like.\nFrom the viewpoint of formal logic, representing the same knowledge in two different\nways makes absolutely no difference; the same facts will be derivable from either represen-\ntation. In practice, however, one representation might require fewer steps to derive a conclu-\nsion, meaning that a reasoner with limited resources could get to the conclusion using one\nrepresentation but not the other. For nondeductive tasks such as learning from experience,\noutcomes are necessarily dependent on the form of the representations used. We show in\nChapter 18 that when a learning program considers two possible theories of the world, both\nof which are consistent with all the data, the most common way of breaking the tie is to choose\nthe most succinct theory—and that depends on the language used to represent theories. Thus,\nthe inﬂuence of language on thought is unavoidable for any agent that does learning.\n8.1.2\nCombining the best of formal and natural languages",
  "the most succinct theory—and that depends on the language used to represent theories. Thus,\nthe inﬂuence of language on thought is unavoidable for any agent that does learning.\n8.1.2\nCombining the best of formal and natural languages\nWe can adopt the foundation of propositional logic—a declarative, compositional semantics\nthat is context-independent and unambiguous—and build a more expressive logic on that\nfoundation, borrowing representational ideas from natural language while avoiding its draw-\nbacks. When we look at the syntax of natural language, the most obvious elements are nouns\nand noun phrases that refer to objects (squares, pits, wumpuses) and verbs and verb phrases\nOBJECT\nthat refer to relations among objects (is breezy, is adjacent to, shoots). Some of these rela-\nRELATION\ntions are functions—relations in which there is only one “value” for a given “input.” It is\nFUNCTION\neasy to start listing examples of objects, relations, and functions:\n• Objects: people, houses, numbers, theories, Ronald McDonald, colors, baseball games,\nwars, centuries . . .\n• Relations: these can be unary relations or properties such as red, round, bogus, prime,\nPROPERTY\nmultistoried . . ., or more general n-ary relations such as brother of, bigger than, inside,\npart of, has color, occurred after, owns, comes between, . . .\n• Functions: father of, best friend, third inning of, one more than, beginning of . . .\nIndeed, almost any assertion can be thought of as referring to objects and properties or rela-\ntions. Some examples follow: Section 8.1.\nRepresentation Revisited\n289\n• “One plus two equals three.”\nObjects: one, two, three, one plus two; Relation: equals; Function: plus. (“One plus\ntwo” is a name for the object that is obtained by applying the function “plus” to the\nobjects “one” and “two.” “Three” is another name for this object.)\n• “Squares neighboring the wumpus are smelly.”\nObjects: wumpus, squares; Property: smelly; Relation: neighboring.\n• “Evil King John ruled England in 1200.”\nObjects: John, England, 1200; Relation: ruled; Properties: evil, king.\nThe language of ﬁrst-order logic, whose syntax and semantics we deﬁne in the next section,\nis built around objects and relations. It has been so important to mathematics, philosophy, and\nartiﬁcial intelligence precisely because those ﬁelds—and indeed, much of everyday human\nexistence—can be usefully thought of as dealing with objects and the relations among them.",
  "artiﬁcial intelligence precisely because those ﬁelds—and indeed, much of everyday human\nexistence—can be usefully thought of as dealing with objects and the relations among them.\nFirst-order logic can also express facts about some or all of the objects in the universe. This\nenables one to represent general laws or rules, such as the statement “Squares neighboring\nthe wumpus are smelly.”\nThe primary difference between propositional and ﬁrst-order logic lies in the ontologi-\ncal commitment made by each language—that is, what it assumes about the nature of reality.\nONTOLOGICAL\nCOMMITMENT\nMathematically, this commitment is expressed through the nature of the formal models with\nrespect to which the truth of sentences is deﬁned. For example, propositional logic assumes\nthat there are facts that either hold or do not hold in the world. Each fact can be in one\nof two states: true or false, and each model assigns true or false to each proposition sym-\nbol (see Section 7.4.2).2 First-order logic assumes more; namely, that the world consists of\nobjects with certain relations among them that do or do not hold. The formal models are\ncorrespondingly more complicated than those for propositional logic. Special-purpose logics\nmake still further ontological commitments; for example, temporal logic assumes that facts\nTEMPORAL LOGIC\nhold at particular times and that those times (which may be points or intervals) are ordered.\nThus, special-purpose logics give certain kinds of objects (and the axioms about them) “ﬁrst\nclass” status within the logic, rather than simply deﬁning them within the knowledge base.\nHigher-order logic views the relations and functions referred to by ﬁrst-order logic as ob-\nHIGHER-ORDER\nLOGIC\njects in themselves. This allows one to make assertions about all relations—for example, one\ncould wish to deﬁne what it means for a relation to be transitive. Unlike most special-purpose\nlogics, higher-order logic is strictly more expressive than ﬁrst-order logic, in the sense that\nsome sentences of higher-order logic cannot be expressed by any ﬁnite number of ﬁrst-order\nlogic sentences.\nA logic can also be characterized by its epistemological commitments—the possible\nEPISTEMOLOGICAL\nCOMMITMENT\nstates of knowledge that it allows with respect to each fact. In both propositional and ﬁrst-\norder logic, a sentence represents a fact and the agent either believes the sentence to be true,",
  "EPISTEMOLOGICAL\nCOMMITMENT\nstates of knowledge that it allows with respect to each fact. In both propositional and ﬁrst-\norder logic, a sentence represents a fact and the agent either believes the sentence to be true,\nbelieves it to be false, or has no opinion. These logics therefore have three possible states\nof knowledge regarding any sentence. Systems using probability theory, on the other hand,\n2 In contrast, facts in fuzzy logic have a degree of truth between 0 and 1. For example, the sentence “Vienna is\na large city” might be true in our world only to degree 0.6 in fuzzy logic. 290\nChapter\n8.\nFirst-Order Logic\ncan have any degree of belief, ranging from 0 (total disbelief) to 1 (total belief).3 For ex-\nample, a probabilistic wumpus-world agent might believe that the wumpus is in [1,3] with\nprobability 0.75. The ontological and epistemological commitments of ﬁve different logics\nare summarized in Figure 8.1.\nLanguage\nOntological Commitment\nEpistemological Commitment\n(What exists in the world)\n(What an agent believes about facts)\nPropositional logic\nfacts\ntrue/false/unknown\nFirst-order logic\nfacts, objects, relations\ntrue/false/unknown\nTemporal logic\nfacts, objects, relations, times\ntrue/false/unknown\nProbability theory\nfacts\ndegree of belief ∈[0, 1]\nFuzzy logic\nfacts with degree of truth ∈[0, 1]\nknown interval value\nFigure 8.1\nFormal languages and their ontological and epistemological commitments.\nIn the next section, we will launch into the details of ﬁrst-order logic. Just as a student of\nphysics requires some familiarity with mathematics, a student of AI must develop a talent for\nworking with logical notation. On the other hand, it is also important not to get too concerned\nwith the speciﬁcs of logical notation—after all, there are dozens of different versions. The\nmain things to keep hold of are how the language facilitates concise representations and how\nits semantics leads to sound reasoning procedures.\n8.2\nSYNTAX AND SEMANTICS OF FIRST-ORDER LOGIC\nWe begin this section by specifying more precisely the way in which the possible worlds\nof ﬁrst-order logic reﬂect the ontological commitment to objects and relations. Then we\nintroduce the various elements of the language, explaining their semantics as we go along.\n8.2.1\nModels for ﬁrst-order logic\nRecall from Chapter 7 that the models of a logical language are the formal structures that\nconstitute the possible worlds under consideration. Each model links the vocabulary of the",
  "8.2.1\nModels for ﬁrst-order logic\nRecall from Chapter 7 that the models of a logical language are the formal structures that\nconstitute the possible worlds under consideration. Each model links the vocabulary of the\nlogical sentences to elements of the possible world, so that the truth of any sentence can\nbe determined. Thus, models for propositional logic link proposition symbols to predeﬁned\ntruth values. Models for ﬁrst-order logic are much more interesting. First, they have objects\nin them! The domain of a model is the set of objects or domain elements it contains. The do-\nDOMAIN\nDOMAIN ELEMENTS\nmain is required to be nonempty—every possible world must contain at least one object. (See\nExercise 8.7 for a discussion of empty worlds.) Mathematically speaking, it doesn’t matter\nwhat these objects are—all that matters is how many there are in each particular model—but\nfor pedagogical purposes we’ll use a concrete example. Figure 8.2 shows a model with ﬁve\n3 It is important not to confuse the degree of belief in probability theory with the degree of truth in fuzzy logic.\nIndeed, some fuzzy systems allow uncertainty (degree of belief) about degrees of truth. Section 8.2.\nSyntax and Semantics of First-Order Logic\n291\nobjects: Richard the Lionheart, King of England from 1189 to 1199; his younger brother, the\nevil King John, who ruled from 1199 to 1215; the left legs of Richard and John; and a crown.\nThe objects in the model may be related in various ways. In the ﬁgure, Richard and\nJohn are brothers. Formally speaking, a relation is just the set of tuples of objects that are\nTUPLE\nrelated. (A tuple is a collection of objects arranged in a ﬁxed order and is written with angle\nbrackets surrounding the objects.) Thus, the brotherhood relation in this model is the set\n{ ⟨Richard the Lionheart, King John⟩, ⟨King John, Richard the Lionheart⟩} .\n(8.1)\n(Here we have named the objects in English, but you may, if you wish, mentally substitute the\npictures for the names.) The crown is on King John’s head, so the “on head” relation contains\njust one tuple, ⟨the crown, King John⟩. The “brother” and “on head” relations are binary\nrelations—that is, they relate pairs of objects. The model also contains unary relations, or\nproperties: the “person” property is true of both Richard and John; the “king” property is true\nonly of John (presumably because Richard is dead at this point); and the “crown” property is\ntrue only of the crown.",
  "properties: the “person” property is true of both Richard and John; the “king” property is true\nonly of John (presumably because Richard is dead at this point); and the “crown” property is\ntrue only of the crown.\nCertain kinds of relationships are best considered as functions, in that a given object\nmust be related to exactly one object in this way. For example, each person has one left leg,\nso the model has a unary “left leg” function that includes the following mappings:\n⟨Richard the Lionheart⟩→Richard’s left leg\n⟨King John⟩→John’s left leg .\n(8.2)\nStrictly speaking, models in ﬁrst-order logic require total functions, that is, there must be a\nTOTAL FUNCTIONS\nvalue for every input tuple. Thus, the crown must have a left leg and so must each of the left\nlegs. There is a technical solution to this awkward problem involving an additional “invisible”\nR\nJ\n$\nleft leg\non head\nbrother\nbrother\nperson\nperson\nking\ncrown\nleft leg\nFigure 8.2\nA model containing ﬁve objects, two binary relations, three unary relations\n(indicated by labels on the objects), and one unary function, left-leg. 292\nChapter\n8.\nFirst-Order Logic\nobject that is the left leg of everything that has no left leg, including itself. Fortunately, as\nlong as one makes no assertions about the left legs of things that have no left legs, these\ntechnicalities are of no import.\nSo far, we have described the elements that populate models for ﬁrst-order logic. The\nother essential part of a model is the link between those elements and the vocabulary of the\nlogical sentences, which we explain next.\n8.2.2\nSymbols and interpretations\nWe turn now to the syntax of ﬁrst-order logic. The impatient reader can obtain a complete\ndescription from the formal grammar in Figure 8.3.\nThe basic syntactic elements of ﬁrst-order logic are the symbols that stand for objects,\nrelations, and functions. The symbols, therefore, come in three kinds: constant symbols,\nCONSTANT SYMBOL\nwhich stand for objects; predicate symbols, which stand for relations; and function sym-\nPREDICATE SYMBOL\nbols, which stand for functions. We adopt the convention that these symbols will begin with\nFUNCTION SYMBOL\nuppercase letters. For example, we might use the constant symbols Richard and John; the\npredicate symbols Brother, OnHead, Person, King, and Crown; and the function symbol\nLeftLeg. As with proposition symbols, the choice of names is entirely up to the user. Each\npredicate and function symbol comes with an arity that ﬁxes the number of arguments.",
  "LeftLeg. As with proposition symbols, the choice of names is entirely up to the user. Each\npredicate and function symbol comes with an arity that ﬁxes the number of arguments.\nARITY\nAs in propositional logic, every model must provide the information required to deter-\nmine if any given sentence is true or false. Thus, in addition to its objects, relations, and\nfunctions, each model includes an interpretation that speciﬁes exactly which objects, rela-\nINTERPRETATION\ntions and functions are referred to by the constant, predicate, and function symbols. One\npossible interpretation for our example—which a logician would call the intended interpre-\ntation—is as follows:\nINTENDED\nINTERPRETATION\n• Richard refers to Richard the Lionheart and John refers to the evil King John.\n• Brother refers to the brotherhood relation, that is, the set of tuples of objects given in\nEquation (8.1); OnHead refers to the “on head” relation that holds between the crown\nand King John; Person, King, and Crown refer to the sets of objects that are persons,\nkings, and crowns.\n• LeftLeg refers to the “left leg” function, that is, the mapping given in Equation (8.2).\nThere are many other possible interpretations, of course. For example, one interpretation\nmaps Richard to the crown and John to King John’s left leg. There are ﬁve objects in\nthe model, so there are 25 possible interpretations just for the constant symbols Richard\nand John. Notice that not all the objects need have a name—for example, the intended\ninterpretation does not name the crown or the legs. It is also possible for an object to have\nseveral names; there is an interpretation under which both Richard and John refer to the\ncrown.4 If you ﬁnd this possibility confusing, remember that, in propositional logic, it is\nperfectly possible to have a model in which Cloudy and Sunny are both true; it is the job of\nthe knowledge base to rule out models that are inconsistent with our knowledge.\n4 Later, in Section 8.2.8, we examine a semantics in which every object has exactly one name. Section 8.2.\nSyntax and Semantics of First-Order Logic\n293\nSentence\n→\nAtomicSentence | ComplexSentence\nAtomicSentence\n→\nPredicate | Predicate(Term, . . .) | Term = Term\nComplexSentence\n→\n( Sentence ) | [ Sentence ]\n|\n¬ Sentence\n|\nSentence ∧Sentence\n|\nSentence ∨Sentence\n|\nSentence\n⇒Sentence\n|\nSentence\n⇔\nSentence\n|\nQuantiﬁer Variable, . . . Sentence\nTerm\n→\nFunction(Term, . . .)\n|\nConstant\n|\nVariable\nQuantiﬁer\n→\n∀| ∃\nConstant\n→\nA | X1 | John | · · ·",
  "|\n¬ Sentence\n|\nSentence ∧Sentence\n|\nSentence ∨Sentence\n|\nSentence\n⇒Sentence\n|\nSentence\n⇔\nSentence\n|\nQuantiﬁer Variable, . . . Sentence\nTerm\n→\nFunction(Term, . . .)\n|\nConstant\n|\nVariable\nQuantiﬁer\n→\n∀| ∃\nConstant\n→\nA | X1 | John | · · ·\nVariable\n→\na | x | s | · · ·\nPredicate\n→\nTrue | False | After | Loves | Raining | · · ·\nFunction\n→\nMother | LeftLeg | · · ·\nOPERATOR PRECEDENCE\n:\n¬, =, ∧, ∨, ⇒, ⇔\nFigure 8.3\nThe syntax of ﬁrst-order logic with equality, speciﬁed in Backus–Naur form\n(see page 1060 if you are not familiar with this notation). Operator precedences are speciﬁed,\nfrom highest to lowest. The precedence of quantiﬁers is such that a quantiﬁer holds over\neverything to the right of it.\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\n. . .\n. . .\n. . .\nFigure 8.4\nSome members of the set of all models for a language with two constant sym-\nbols, R and J, and one binary relation symbol. The interpretation of each constant symbol is\nshown by a gray arrow. Within each model, the related objects are connected by arrows. 294\nChapter\n8.\nFirst-Order Logic\nIn summary, a model in ﬁrst-order logic consists of a set of objects and an interpretation\nthat maps constant symbols to objects, predicate symbols to relations on those objects, and\nfunction symbols to functions on those objects. Just as with propositional logic, entailment,\nvalidity, and so on are deﬁned in terms of all possible models. To get an idea of what the\nset of all possible models looks like, see Figure 8.4. It shows that models vary in how many\nobjects they contain—from one up to inﬁnity—and in the way the constant symbols map\nto objects. If there are two constant symbols and one object, then both symbols must refer\nto the same object; but this can still happen even with more objects. When there are more\nobjects than constant symbols, some of the objects will have no names. Because the number\nof possible models is unbounded, checking entailment by the enumeration of all possible\nmodels is not feasible for ﬁrst-order logic (unlike propositional logic). Even if the number of\nobjects is restricted, the number of combinations can be very large. (See Exercise 8.5.) For\nthe example in Figure 8.4, there are 137,506,194,466 models with six or fewer objects.\n8.2.3\nTerms\nA term is a logical expression that refers to an object. Constant symbols are therefore terms,\nTERM\nbut it is not always convenient to have a distinct symbol to name every object. For example,",
  "8.2.3\nTerms\nA term is a logical expression that refers to an object. Constant symbols are therefore terms,\nTERM\nbut it is not always convenient to have a distinct symbol to name every object. For example,\nin English we might use the expression “King John’s left leg” rather than giving a name\nto his leg. This is what function symbols are for: instead of using a constant symbol, we\nuse LeftLeg(John). In the general case, a complex term is formed by a function symbol\nfollowed by a parenthesized list of terms as arguments to the function symbol. It is important\nto remember that a complex term is just a complicated kind of name. It is not a “subroutine\ncall” that “returns a value.” There is no LeftLeg subroutine that takes a person as input and\nreturns a leg. We can reason about left legs (e.g., stating the general rule that everyone has one\nand then deducing that John must have one) without ever providing a deﬁnition of LeftLeg.\nThis is something that cannot be done with subroutines in programming languages.5\nThe formal semantics of terms is straightforward. Consider a term f(t1, . . . , tn). The\nfunction symbol f refers to some function in the model (call it F); the argument terms refer\nto objects in the domain (call them d1, . . . , dn); and the term as a whole refers to the object\nthat is the value of the function F applied to d1, . . . , dn. For example, suppose the LeftLeg\nfunction symbol refers to the function shown in Equation (8.2) and John refers to King John,\nthen LeftLeg(John) refers to King John’s left leg. In this way, the interpretation ﬁxes the\nreferent of every term.\n8.2.4\nAtomic sentences\nNow that we have both terms for referring to objects and predicate symbols for referring to\nrelations, we can put them together to make atomic sentences that state facts. An atomic\n5 λ-expressions provide a useful notation in which new function symbols are constructed “on the ﬂy.” For\nexample, the function that squares its argument can be written as (λx x × x) and can be applied to arguments\njust like any other function symbol. A λ-expression can also be deﬁned and used as a predicate symbol. (See\nChapter 22.) The lambda operator in Lisp plays exactly the same role. Notice that the use of λ in this way does\nnot increase the formal expressive power of ﬁrst-order logic, because any sentence that includes a λ-expression\ncan be rewritten by “plugging in” its arguments to yield an equivalent sentence. Section 8.2.\nSyntax and Semantics of First-Order Logic\n295",
  "can be rewritten by “plugging in” its arguments to yield an equivalent sentence. Section 8.2.\nSyntax and Semantics of First-Order Logic\n295\nsentence (or atom for short) is formed from a predicate symbol optionally followed by a\nATOMIC SENTENCE\nATOM\nparenthesized list of terms, such as\nBrother(Richard, John).\nThis states, under the intended interpretation given earlier, that Richard the Lionheart is the\nbrother of King John.6 Atomic sentences can have complex terms as arguments. Thus,\nMarried(Father(Richard), Mother(John))\nstates that Richard the Lionheart’s father is married to King John’s mother (again, under a\nsuitable interpretation).\nAn atomic sentence is true in a given model if the relation referred to by the predicate\nsymbol holds among the objects referred to by the arguments.\n8.2.5\nComplex sentences\nWe can use logical connectives to construct more complex sentences, with the same syntax\nand semantics as in propositional calculus. Here are four sentences that are true in the model\nof Figure 8.2 under our intended interpretation:\n¬Brother(LeftLeg(Richard), John)\nBrother(Richard, John) ∧Brother(John, Richard)\nKing(Richard) ∨King(John)\n¬King(Richard) ⇒King(John) .\n8.2.6\nQuantiﬁers\nOnce we have a logic that allows objects, it is only natural to want to express properties of\nentire collections of objects, instead of enumerating the objects by name. Quantiﬁers let us\nQUANTIFIER\ndo this. First-order logic contains two standard quantiﬁers, called universal and existential.\nUniversal quantiﬁcation (∀)\nRecall the difﬁculty we had in Chapter 7 with the expression of general rules in proposi-\ntional logic. Rules such as “Squares neighboring the wumpus are smelly” and “All kings\nare persons” are the bread and butter of ﬁrst-order logic. We deal with the ﬁrst of these in\nSection 8.3. The second rule, “All kings are persons,” is written in ﬁrst-order logic as\n∀x King(x) ⇒Person(x) .\n∀is usually pronounced “For all . . .”. (Remember that the upside-down A stands for “all.”)\nThus, the sentence says, “For all x, if x is a king, then x is a person.” The symbol x is called\na variable. By convention, variables are lowercase letters. A variable is a term all by itself,\nVARIABLE\nand as such can also serve as the argument of a function—for example, LeftLeg(x). A term\nwith no variables is called a ground term.\nGROUND TERM\nIntuitively, the sentence ∀x P, where P is any logical expression, says that P is true",
  "VARIABLE\nand as such can also serve as the argument of a function—for example, LeftLeg(x). A term\nwith no variables is called a ground term.\nGROUND TERM\nIntuitively, the sentence ∀x P, where P is any logical expression, says that P is true\nfor every object x. More precisely, ∀x P is true in a given model if P is true in all possible\nextended interpretations constructed from the interpretation given in the model, where each\nEXTENDED\nINTERPRETATION\n6 We usually follow the argument-ordering convention that P(x, y) is read as “x is a P of y.” 296\nChapter\n8.\nFirst-Order Logic\nextended interpretation speciﬁes a domain element to which x refers.\nThis sounds complicated, but it is really just a careful way of stating the intuitive mean-\ning of universal quantiﬁcation. Consider the model shown in Figure 8.2 and the intended\ninterpretation that goes with it. We can extend the interpretation in ﬁve ways:\nx →Richard the Lionheart,\nx →King John,\nx →Richard’s left leg,\nx →John’s left leg,\nx →the crown.\nThe universally quantiﬁed sentence ∀x King(x) ⇒Person(x) is true in the original model\nif the sentence King(x) ⇒Person(x) is true under each of the ﬁve extended interpreta-\ntions. That is, the universally quantiﬁed sentence is equivalent to asserting the following ﬁve\nsentences:\nRichard the Lionheart is a king ⇒Richard the Lionheart is a person.\nKing John is a king ⇒King John is a person.\nRichard’s left leg is a king ⇒Richard’s left leg is a person.\nJohn’s left leg is a king ⇒John’s left leg is a person.\nThe crown is a king ⇒the crown is a person.\nLet us look carefully at this set of assertions. Since, in our model, King John is the only\nking, the second sentence asserts that he is a person, as we would hope. But what about\nthe other four sentences, which appear to make claims about legs and crowns? Is that part\nof the meaning of “All kings are persons”? In fact, the other four assertions are true in the\nmodel, but make no claim whatsoever about the personhood qualiﬁcations of legs, crowns,\nor indeed Richard. This is because none of these objects is a king. Looking at the truth table\nfor ⇒(Figure 7.8 on page 246), we see that the implication is true whenever its premise is\nfalse—regardless of the truth of the conclusion. Thus, by asserting the universally quantiﬁed\nsentence, which is equivalent to asserting a whole list of individual implications, we end\nup asserting the conclusion of the rule just for those objects for whom the premise is true",
  "sentence, which is equivalent to asserting a whole list of individual implications, we end\nup asserting the conclusion of the rule just for those objects for whom the premise is true\nand saying nothing at all about those individuals for whom the premise is false. Thus, the\ntruth-table deﬁnition of ⇒turns out to be perfect for writing general rules with universal\nquantiﬁers.\nA common mistake, made frequently even by diligent readers who have read this para-\ngraph several times, is to use conjunction instead of implication. The sentence\n∀x King(x) ∧Person(x)\nwould be equivalent to asserting\nRichard the Lionheart is a king ∧Richard the Lionheart is a person,\nKing John is a king ∧King John is a person,\nRichard’s left leg is a king ∧Richard’s left leg is a person,\nand so on. Obviously, this does not capture what we want. Section 8.2.\nSyntax and Semantics of First-Order Logic\n297\nExistential quantiﬁcation (∃)\nUniversal quantiﬁcation makes statements about every object. Similarly, we can make a state-\nment about some object in the universe without naming it, by using an existential quantiﬁer.\nTo say, for example, that King John has a crown on his head, we write\n∃x Crown(x) ∧OnHead(x, John) .\n∃x is pronounced “There exists an x such that . . .” or “For some x . . .”.\nIntuitively, the sentence ∃x P says that P is true for at least one object x. More\nprecisely, ∃x P is true in a given model if P is true in at least one extended interpretation\nthat assigns x to a domain element. That is, at least one of the following is true:\nRichard the Lionheart is a crown ∧Richard the Lionheart is on John’s head;\nKing John is a crown ∧King John is on John’s head;\nRichard’s left leg is a crown ∧Richard’s left leg is on John’s head;\nJohn’s left leg is a crown ∧John’s left leg is on John’s head;\nThe crown is a crown ∧the crown is on John’s head.\nThe ﬁfth assertion is true in the model, so the original existentially quantiﬁed sentence is\ntrue in the model. Notice that, by our deﬁnition, the sentence would also be true in a model\nin which King John was wearing two crowns. This is entirely consistent with the original\nsentence “King John has a crown on his head.” 7\nJust as ⇒appears to be the natural connective to use with ∀, ∧is the natural connective\nto use with ∃. Using ∧as the main connective with ∀led to an overly strong statement in\nthe example in the previous section; using ⇒with ∃usually leads to a very weak statement,\nindeed. Consider the following sentence:",
  "to use with ∃. Using ∧as the main connective with ∀led to an overly strong statement in\nthe example in the previous section; using ⇒with ∃usually leads to a very weak statement,\nindeed. Consider the following sentence:\n∃x Crown(x) ⇒OnHead(x, John) .\nOn the surface, this might look like a reasonable rendition of our sentence. Applying the\nsemantics, we see that the sentence says that at least one of the following assertions is true:\nRichard the Lionheart is a crown ⇒Richard the Lionheart is on John’s head;\nKing John is a crown ⇒King John is on John’s head;\nRichard’s left leg is a crown ⇒Richard’s left leg is on John’s head;\nand so on. Now an implication is true if both premise and conclusion are true, or if its premise\nis false. So if Richard the Lionheart is not a crown, then the ﬁrst assertion is true and the\nexistential is satisﬁed. So, an existentially quantiﬁed implication sentence is true whenever\nany object fails to satisfy the premise; hence such sentences really do not say much at all.\nNested quantiﬁers\nWe will often want to express more complex sentences using multiple quantiﬁers. The sim-\nplest case is where the quantiﬁers are of the same type. For example, “Brothers are siblings”\ncan be written as\n∀x ∀y Brother(x, y) ⇒Sibling(x, y) .\n7 There is a variant of the existential quantiﬁer, usually written ∃1 or ∃!, that means “There exists exactly one.”\nThe same meaning can be expressed using equality statements. 298\nChapter\n8.\nFirst-Order Logic\nConsecutive quantiﬁers of the same type can be written as one quantiﬁer with several vari-\nables. For example, to say that siblinghood is a symmetric relationship, we can write\n∀x, y Sibling(x, y) ⇔Sibling(y, x) .\nIn other cases we will have mixtures. “Everybody loves somebody” means that for every\nperson, there is someone that person loves:\n∀x ∃y Loves(x, y) .\nOn the other hand, to say “There is someone who is loved by everyone,” we write\n∃y ∀x Loves(x, y) .\nThe order of quantiﬁcation is therefore very important. It becomes clearer if we insert paren-\ntheses. ∀x (∃y Loves(x, y)) says that everyone has a particular property, namely, the prop-\nerty that they love someone. On the other hand, ∃y (∀x Loves(x, y)) says that someone in\nthe world has a particular property, namely the property of being loved by everybody.\nSome confusion can arise when two quantiﬁers are used with the same variable name.\nConsider the sentence\n∀x (Crown(x) ∨(∃x Brother(Richard, x))) .",
  "the world has a particular property, namely the property of being loved by everybody.\nSome confusion can arise when two quantiﬁers are used with the same variable name.\nConsider the sentence\n∀x (Crown(x) ∨(∃x Brother(Richard, x))) .\nHere the x in Brother(Richard, x) is existentially quantiﬁed. The rule is that the variable\nbelongs to the innermost quantiﬁer that mentions it; then it will not be subject to any other\nquantiﬁcation. Another way to think of it is this: ∃x Brother(Richard, x) is a sentence\nabout Richard (that he has a brother), not about x; so putting a ∀x outside it has no effect. It\ncould equally well have been written ∃z Brother(Richard, z). Because this can be a source\nof confusion, we will always use different variable names with nested quantiﬁers.\nConnections between ∀and ∃\nThe two quantiﬁers are actually intimately connected with each other, through negation. As-\nserting that everyone dislikes parsnips is the same as asserting there does not exist someone\nwho likes them, and vice versa:\n∀x ¬Likes(x, Parsnips)\nis equivalent to\n¬∃x Likes(x, Parsnips) .\nWe can go one step further: “Everyone likes ice cream” means that there is no one who does\nnot like ice cream:\n∀x Likes(x, IceCream)\nis equivalent to\n¬∃x ¬Likes(x, IceCream) .\nBecause ∀is really a conjunction over the universe of objects and ∃is a disjunction, it should\nnot be surprising that they obey De Morgan’s rules. The De Morgan rules for quantiﬁed and\nunquantiﬁed sentences are as follows:\n∀x ¬P\n≡¬∃x P\n¬(P ∨Q) ≡¬P ∧¬Q\n¬∀x P\n≡∃x ¬P\n¬(P ∧Q) ≡¬P ∨¬Q\n∀x P\n≡¬∃x ¬P\nP ∧Q\n≡¬(¬P ∨¬Q)\n∃x P\n≡¬∀x ¬P\nP ∨Q\n≡¬(¬P ∧¬Q) .\nThus, we do not really need both ∀and ∃, just as we do not really need both ∧and ∨. Still,\nreadability is more important than parsimony, so we will keep both of the quantiﬁers. Section 8.2.\nSyntax and Semantics of First-Order Logic\n299\n8.2.7\nEquality\nFirst-order logic includes one more way to make atomic sentences, other than using a predi-\ncate and terms as described earlier. We can use the equality symbol to signify that two terms\nEQUALITY SYMBOL\nrefer to the same object. For example,\nFather(John) = Henry\nsays that the object referred to by Father(John) and the object referred to by Henry are the\nsame. Because an interpretation ﬁxes the referent of any term, determining the truth of an\nequality sentence is simply a matter of seeing that the referents of the two terms are the same\nobject.\nThe equality symbol can be used to state facts about a given function, as we just did for",
  "equality sentence is simply a matter of seeing that the referents of the two terms are the same\nobject.\nThe equality symbol can be used to state facts about a given function, as we just did for\nthe Father symbol. It can also be used with negation to insist that two terms are not the same\nobject. To say that Richard has at least two brothers, we would write\n∃x, y Brother(x, Richard) ∧Brother(y, Richard) ∧¬(x = y) .\nThe sentence\n∃x, y Brother(x, Richard) ∧Brother(y, Richard)\ndoes not have the intended meaning. In particular, it is true in the model of Figure 8.2, where\nRichard has only one brother. To see this, consider the extended interpretation in which both\nx and y are assigned to King John. The addition of ¬(x = y) rules out such models. The\nnotation x ̸= y is sometimes used as an abbreviation for ¬(x = y).\n8.2.8\nAn alternative semantics?\nContinuing the example from the previous section, suppose that we believe that Richard has\ntwo brothers, John and Geoffrey.8 Can we capture this state of affairs by asserting\nBrother(John, Richard) ∧Brother(Geoﬀrey, Richard) ?\n(8.3)\nNot quite. First, this assertion is true in a model where Richard has only one brother—\nwe need to add John ̸= Geoﬀrey. Second, the sentence doesn’t rule out models in which\nRichard has many more brothers besides John and Geoffrey. Thus, the correct translation of\n“Richard’s brothers are John and Geoffrey” is as follows:\nBrother(John, Richard) ∧Brother(Geoﬀrey, Richard) ∧John ̸= Geoﬀrey\n∧∀x Brother(x, Richard) ⇒(x = John ∨x = Geoﬀrey) .\nFor many purposes, this seems much more cumbersome than the corresponding natural-\nlanguage expression. As a consequence, humans may make mistakes in translating their\nknowledge into ﬁrst-order logic, resulting in unintuitive behaviors from logical reasoning\nsystems that use the knowledge. Can we devise a semantics that allows a more straightfor-\nward logical expression?\nOne proposal that is very popular in database systems works as follows. First, we insist\nthat every constant symbol refer to a distinct object—the so-called unique-names assump-\ntion. Second, we assume that atomic sentences not known to be true are in fact false—the\nUNIQUE-NAMES\nASSUMPTION\nclosed-world assumption. Finally, we invoke domain closure, meaning that each model\nCLOSED-WORLD\nASSUMPTION\nDOMAIN CLOSURE\n8 Actually he had four, the others being William and Henry. 300\nChapter\n8.\nFirst-Order Logic\n. . .\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nFigure 8.5",
  "CLOSED-WORLD\nASSUMPTION\nDOMAIN CLOSURE\n8 Actually he had four, the others being William and Henry. 300\nChapter\n8.\nFirst-Order Logic\n. . .\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nFigure 8.5\nSome members of the set of all models for a language with two constant sym-\nbols, R and J, and one binary relation symbol, under database semantics. The interpretation\nof the constant symbols is ﬁxed, and there is a distinct object for each constant symbol.\ncontains no more domain elements than those named by the constant symbols. Under the\nresulting semantics, which we call database semantics to distinguish it from the standard\nDATABASE\nSEMANTICS\nsemantics of ﬁrst-order logic, the sentence Equation (8.3) does indeed state that Richard’s\ntwo brothers are John and Geoffrey. Database semantics is also used in logic programming\nsystems, as explained in Section 9.4.5.\nIt is instructive to consider the set of all possible models under database semantics for\nthe same case as shown in Figure 8.4. Figure 8.5 shows some of the models, ranging from\nthe model with no tuples satisfying the relation to the model with all tuples satisfying the\nrelation. With two objects, there are four possible two-element tuples, so there are 24 = 16\ndifferent subsets of tuples that can satisfy the relation. Thus, there are 16 possible models in\nall—a lot fewer than the inﬁnitely many models for the standard ﬁrst-order semantics. On the\nother hand, the database semantics requires deﬁnite knowledge of what the world contains.\nThis example brings up an important point: there is no one “correct” semantics for\nlogic. The usefulness of any proposed semantics depends on how concise and intuitive it\nmakes the expression of the kinds of knowledge we want to write down, and on how easy\nand natural it is to develop the corresponding rules of inference. Database semantics is most\nuseful when we are certain about the identity of all the objects described in the knowledge\nbase and when we have all the facts at hand; in other cases, it is quite awkward. For the rest\nof this chapter, we assume the standard semantics while noting instances in which this choice\nleads to cumbersome expressions.\n8.3\nUSING FIRST-ORDER LOGIC\nNow that we have deﬁned an expressive logical language, it is time to learn how to use it. The\nbest way to do this is through examples. We have seen some simple sentences illustrating the\nvarious aspects of logical syntax; in this section, we provide more systematic representations",
  "best way to do this is through examples. We have seen some simple sentences illustrating the\nvarious aspects of logical syntax; in this section, we provide more systematic representations\nof some simple domains. In knowledge representation, a domain is just some part of the\nDOMAIN\nworld about which we wish to express some knowledge.\nWe begin with a brief description of the TELL/ASK interface for ﬁrst-order knowledge\nbases. Then we look at the domains of family relationships, numbers, sets, and lists, and at Section 8.3.\nUsing First-Order Logic\n301\nthe wumpus world. The next section contains a more substantial example (electronic circuits)\nand Chapter 12 covers everything in the universe.\n8.3.1\nAssertions and queries in ﬁrst-order logic\nSentences are added to a knowledge base using TELL, exactly as in propositional logic. Such\nsentences are called assertions. For example, we can assert that John is a king, Richard is a\nASSERTION\nperson, and all kings are persons:\nTELL(KB, King(John)) .\nTELL(KB, Person(Richard)) .\nTELL(KB, ∀x King(x) ⇒Person(x)) .\nWe can ask questions of the knowledge base using ASK. For example,\nASK(KB, King(John))\nreturns true. Questions asked with ASK are called queries or goals. Generally speaking, any\nQUERY\nGOAL\nquery that is logically entailed by the knowledge base should be answered afﬁrmatively. For\nexample, given the two preceding assertions, the query\nASK(KB, Person(John))\nshould also return true. We can ask quantiﬁed queries, such as\nASK(KB, ∃x Person(x)) .\nThe answer is true, but this is perhaps not as helpful as we would like. It is rather like\nanswering “Can you tell me the time?” with “Yes.” If we want to know what value of x\nmakes the sentence true, we will need a different function, ASKVARS, which we call with\nASKVARS(KB, Person(x))\nand which yields a stream of answers. In this case there will be two answers: {x/John} and\n{x/Richard}. Such an answer is called a substitution or binding list. ASKVARS is usually\nSUBSTITUTION\nBINDING LIST\nreserved for knowledge bases consisting solely of Horn clauses, because in such knowledge\nbases every way of making the query true will bind the variables to speciﬁc values. That is\nnot the case with ﬁrst-order logic; if KB has been told King(John) ∨King(Richard), then\nthere is no binding to x for the query ∃x King(x), even though the query is true.\n8.3.2\nThe kinship domain\nThe ﬁrst example we consider is the domain of family relationships, or kinship. This domain",
  "there is no binding to x for the query ∃x King(x), even though the query is true.\n8.3.2\nThe kinship domain\nThe ﬁrst example we consider is the domain of family relationships, or kinship. This domain\nincludes facts such as “Elizabeth is the mother of Charles” and “Charles is the father of\nWilliam” and rules such as “One’s grandmother is the mother of one’s parent.”\nClearly, the objects in our domain are people. We have two unary predicates, Male and\nFemale. Kinship relations—parenthood, brotherhood, marriage, and so on—are represented\nby binary predicates: Parent, Sibling, Brother, Sister, Child, Daughter, Son, Spouse,\nWife, Husband, Grandparent, Grandchild, Cousin, Aunt, and Uncle. We use functions\nfor Mother and Father, because every person has exactly one of each of these (at least\naccording to nature’s design). 302\nChapter\n8.\nFirst-Order Logic\nWe can go through each function and predicate, writing down what we know in terms\nof the other symbols. For example, one’s mother is one’s female parent:\n∀m, c Mother(c) = m ⇔Female(m) ∧Parent(m, c) .\nOne’s husband is one’s male spouse:\n∀w, h Husband(h, w) ⇔Male(h) ∧Spouse(h, w) .\nMale and female are disjoint categories:\n∀x Male(x) ⇔¬Female(x) .\nParent and child are inverse relations:\n∀p, c Parent(p, c) ⇔Child(c, p) .\nA grandparent is a parent of one’s parent:\n∀g, c Grandparent(g, c) ⇔∃p Parent(g, p) ∧Parent(p, c) .\nA sibling is another child of one’s parents:\n∀x, y Sibling(x, y) ⇔x ̸= y ∧∃p Parent(p, x) ∧Parent(p, y) .\nWe could go on for several more pages like this, and Exercise 8.14 asks you to do just that.\nEach of these sentences can be viewed as an axiom of the kinship domain, as explained\nin Section 7.1. Axioms are commonly associated with purely mathematical domains—we\nwill see some axioms for numbers shortly—but they are needed in all domains. They provide\nthe basic factual information from which useful conclusions can be derived. Our kinship\naxioms are also deﬁnitions; they have the form ∀x, y P(x, y) ⇔\n. . .. The axioms deﬁne\nDEFINITION\nthe Mother function and the Husband, Male, Parent, Grandparent, and Sibling predicates\nin terms of other predicates. Our deﬁnitions “bottom out” at a basic set of predicates (Child,\nSpouse, and Female) in terms of which the others are ultimately deﬁned. This is a natural\nway in which to build up the representation of a domain, and it is analogous to the way in\nwhich software packages are built up by successive deﬁnitions of subroutines from primitive",
  "way in which to build up the representation of a domain, and it is analogous to the way in\nwhich software packages are built up by successive deﬁnitions of subroutines from primitive\nlibrary functions. Notice that there is not necessarily a unique set of primitive predicates;\nwe could equally well have used Parent, Spouse, and Male. In some domains, as we show,\nthere is no clearly identiﬁable basic set.\nNot all logical sentences about a domain are axioms. Some are theorems—that is, they\nTHEOREM\nare entailed by the axioms. For example, consider the assertion that siblinghood is symmetric:\n∀x, y Sibling(x, y) ⇔Sibling(y, x) .\nIs this an axiom or a theorem? In fact, it is a theorem that follows logically from the axiom\nthat deﬁnes siblinghood. If we ASK the knowledge base this sentence, it should return true.\nFrom a purely logical point of view, a knowledge base need contain only axioms and\nno theorems, because the theorems do not increase the set of conclusions that follow from\nthe knowledge base. From a practical point of view, theorems are essential to reduce the\ncomputational cost of deriving new sentences. Without them, a reasoning system has to start\nfrom ﬁrst principles every time, rather like a physicist having to rederive the rules of calculus\nfor every new problem. Section 8.3.\nUsing First-Order Logic\n303\nNot all axioms are deﬁnitions. Some provide more general information about certain\npredicates without constituting a deﬁnition. Indeed, some predicates have no complete deﬁ-\nnition because we do not know enough to characterize them fully. For example, there is no\nobvious deﬁnitive way to complete the sentence\n∀x Person(x) ⇔. . .\nFortunately, ﬁrst-order logic allows us to make use of the Person predicate without com-\npletely deﬁning it. Instead, we can write partial speciﬁcations of properties that every person\nhas and properties that make something a person:\n∀x Person(x) ⇒. . .\n∀x . . . ⇒Person(x) .\nAxioms can also be “just plain facts,” such as Male(Jim) and Spouse(Jim, Laura).\nSuch facts form the descriptions of speciﬁc problem instances, enabling speciﬁc questions\nto be answered. The answers to these questions will then be theorems that follow from\nthe axioms. Often, one ﬁnds that the expected answers are not forthcoming—for example,\nfrom Spouse(Jim, Laura) one expects (under the laws of many countries) to be able to infer\n¬Spouse(George, Laura); but this does not follow from the axioms given earlier—even after",
  "from Spouse(Jim, Laura) one expects (under the laws of many countries) to be able to infer\n¬Spouse(George, Laura); but this does not follow from the axioms given earlier—even after\nwe add Jim ̸= George as suggested in Section 8.2.8. This is a sign that an axiom is missing.\nExercise 8.8 asks the reader to supply it.\n8.3.3\nNumbers, sets, and lists\nNumbers are perhaps the most vivid example of how a large theory can be built up from\na tiny kernel of axioms. We describe here the theory of natural numbers or non-negative\nNATURAL NUMBERS\nintegers. We need a predicate NatNum that will be true of natural numbers; we need one\nconstant symbol, 0; and we need one function symbol, S (successor). The Peano axioms\nPEANO AXIOMS\ndeﬁne natural numbers and addition.9 Natural numbers are deﬁned recursively:\nNatNum(0) .\n∀n NatNum(n) ⇒NatNum(S(n)) .\nThat is, 0 is a natural number, and for every object n, if n is a natural number, then S(n) is\na natural number. So the natural numbers are 0, S(0), S(S(0)), and so on. (After reading\nSection 8.2.8, you will notice that these axioms allow for other natural numbers besides the\nusual ones; see Exercise 8.12.) We also need axioms to constrain the successor function:\n∀n 0 ̸= S(n) .\n∀m, n m ̸= n ⇒S(m) ̸= S(n) .\nNow we can deﬁne addition in terms of the successor function:\n∀m NatNum(m) ⇒+ (0, m) = m .\n∀m, n NatNum(m) ∧NatNum(n) ⇒+ (S(m), n) = S(+(m, n)) .\nThe ﬁrst of these axioms says that adding 0 to any natural number m gives m itself. Notice\nthe use of the binary function symbol “+” in the term +(m, 0); in ordinary mathematics, the\nterm would be written m + 0 using inﬁx notation. (The notation we have used for ﬁrst-order\nINFIX\n9 The Peano axioms also include the principle of induction, which is a sentence of second-order logic rather\nthan of ﬁrst-order logic. The importance of this distinction is explained in Chapter 9. 304\nChapter\n8.\nFirst-Order Logic\nlogic is called preﬁx.) To make our sentences about numbers easier to read, we allow the use\nPREFIX\nof inﬁx notation. We can also write S(n) as n + 1, so the second axiom becomes\n∀m, n NatNum(m) ∧NatNum(n) ⇒(m + 1) + n = (m + n) + 1 .\nThis axiom reduces addition to repeated application of the successor function.\nThe use of inﬁx notation is an example of syntactic sugar, that is, an extension to or\nSYNTACTIC SUGAR\nabbreviation of the standard syntax that does not change the semantics. Any sentence that",
  "The use of inﬁx notation is an example of syntactic sugar, that is, an extension to or\nSYNTACTIC SUGAR\nabbreviation of the standard syntax that does not change the semantics. Any sentence that\nuses sugar can be “desugared” to produce an equivalent sentence in ordinary ﬁrst-order logic.\nOnce we have addition, it is straightforward to deﬁne multiplication as repeated addi-\ntion, exponentiation as repeated multiplication, integer division and remainders, prime num-\nbers, and so on. Thus, the whole of number theory (including cryptography) can be built up\nfrom one constant, one function, one predicate and four axioms.\nThe domain of sets is also fundamental to mathematics as well as to commonsense\nSET\nreasoning. (In fact, it is possible to deﬁne number theory in terms of set theory.) We want to\nbe able to represent individual sets, including the empty set. We need a way to build up sets\nby adding an element to a set or taking the union or intersection of two sets. We will want\nto know whether an element is a member of a set and we will want to distinguish sets from\nobjects that are not sets.\nWe will use the normal vocabulary of set theory as syntactic sugar. The empty set is a\nconstant written as { }. There is one unary predicate, Set, which is true of sets. The binary\npredicates are x ∈s (x is a member of set s) and s1 ⊆s2 (set s1 is a subset, not necessarily\nproper, of set s2). The binary functions are s1 ∩s2 (the intersection of two sets), s1 ∪s2\n(the union of two sets), and {x|s} (the set resulting from adjoining element x to set s). One\npossible set of axioms is as follows:\n1. The only sets are the empty set and those made by adjoining something to a set:\n∀s Set(s) ⇔(s = { }) ∨(∃x, s2 Set(s2) ∧s = {x|s2}) .\n2. The empty set has no elements adjoined into it. In other words, there is no way to\ndecompose { } into a smaller set and an element:\n¬∃x, s {x|s} = { } .\n3. Adjoining an element already in the set has no effect:\n∀x, s x ∈s ⇔s = {x|s} .\n4. The only members of a set are the elements that were adjoined into it. We express\nthis recursively, saying that x is a member of s if and only if s is equal to some set s2\nadjoined with some element y, where either y is the same as x or x is a member of s2:\n∀x, s x ∈s ⇔∃y, s2 (s = {y|s2} ∧(x = y ∨x ∈s2)) .\n5. A set is a subset of another set if and only if all of the ﬁrst set’s members are members\nof the second set:\n∀s1, s2 s1 ⊆s2 ⇔(∀x x ∈s1 ⇒x ∈s2) .",
  "∀x, s x ∈s ⇔∃y, s2 (s = {y|s2} ∧(x = y ∨x ∈s2)) .\n5. A set is a subset of another set if and only if all of the ﬁrst set’s members are members\nof the second set:\n∀s1, s2 s1 ⊆s2 ⇔(∀x x ∈s1 ⇒x ∈s2) .\n6. Two sets are equal if and only if each is a subset of the other:\n∀s1, s2 (s1 = s2) ⇔(s1 ⊆s2 ∧s2 ⊆s1) . Section 8.3.\nUsing First-Order Logic\n305\n7. An object is in the intersection of two sets if and only if it is a member of both sets:\n∀x, s1, s2 x ∈(s1 ∩s2) ⇔(x ∈s1 ∧x ∈s2) .\n8. An object is in the union of two sets if and only if it is a member of either set:\n∀x, s1, s2 x ∈(s1 ∪s2) ⇔(x ∈s1 ∨x ∈s2) .\nLists are similar to sets. The differences are that lists are ordered and the same element can\nLIST\nappear more than once in a list. We can use the vocabulary of Lisp for lists: Nil is the constant\nlist with no elements; Cons, Append, First, and Rest are functions; and Find is the pred-\nicate that does for lists what Member does for sets. List? is a predicate that is true only of\nlists. As with sets, it is common to use syntactic sugar in logical sentences involving lists. The\nempty list is [ ]. The term Cons(x, y), where y is a nonempty list, is written [x|y]. The term\nCons(x, Nil) (i.e., the list containing the element x) is written as [x]. A list of several ele-\nments, such as [A, B, C], corresponds to the nested term Cons(A, Cons(B, Cons(C, Nil))).\nExercise 8.16 asks you to write out the axioms for lists.\n8.3.4\nThe wumpus world\nSome propositional logic axioms for the wumpus world were given in Chapter 7. The ﬁrst-\norder axioms in this section are much more concise, capturing in a natural way exactly what\nwe want to say.\nRecall that the wumpus agent receives a percept vector with ﬁve elements. The corre-\nsponding ﬁrst-order sentence stored in the knowledge base must include both the percept and\nthe time at which it occurred; otherwise, the agent will get confused about when it saw what.\nWe use integers for time steps. A typical percept sentence would be\nPercept([Stench, Breeze, Glitter, None, None], 5) .\nHere, Percept is a binary predicate, and Stench and so on are constants placed in a list. The\nactions in the wumpus world can be represented by logical terms:\nTurn(Right), Turn(Left), Forward, Shoot, Grab, Climb .\nTo determine which is best, the agent program executes the query\nASKVARS(∃a BestAction(a, 5)) ,\nwhich returns a binding list such as {a/Grab}. The agent program can then return Grab as",
  "Turn(Right), Turn(Left), Forward, Shoot, Grab, Climb .\nTo determine which is best, the agent program executes the query\nASKVARS(∃a BestAction(a, 5)) ,\nwhich returns a binding list such as {a/Grab}. The agent program can then return Grab as\nthe action to take. The raw percept data implies certain facts about the current state. For\nexample:\n∀t, s, g, m, c Percept([s, Breeze, g, m, c], t) ⇒Breeze(t) ,\n∀t, s, b, m, c Percept([s, b, Glitter, m, c], t) ⇒Glitter(t) ,\nand so on. These rules exhibit a trivial form of the reasoning process called perception, which\nwe study in depth in Chapter 24. Notice the quantiﬁcation over time t. In propositional logic,\nwe would need copies of each sentence for each time step.\nSimple “reﬂex” behavior can also be implemented by quantiﬁed implication sentences.\nFor example, we have\n∀t Glitter(t) ⇒BestAction(Grab, t) . 306\nChapter\n8.\nFirst-Order Logic\nGiven the percept and rules from the preceding paragraphs, this would yield the desired con-\nclusion BestAction(Grab, 5)—that is, Grab is the right thing to do.\nWe have represented the agent’s inputs and outputs; now it is time to represent the\nenvironment itself. Let us begin with objects. Obvious candidates are squares, pits, and the\nwumpus. We could name each square—Square1,2 and so on—but then the fact that Square1,2\nand Square1,3 are adjacent would have to be an “extra” fact, and we would need one such\nfact for each pair of squares. It is better to use a complex term in which the row and column\nappear as integers; for example, we can simply use the list term [1, 2]. Adjacency of any two\nsquares can be deﬁned as\n∀x, y, a, b Adjacent([x, y], [a, b]) ⇔\n(x = a ∧(y = b −1 ∨y = b + 1)) ∨(y = b ∧(x = a −1 ∨x = a + 1)) .\nWe could name each pit, but this would be inappropriate for a different reason: there is no\nreason to distinguish among pits.10 It is simpler to use a unary predicate Pit that is true of\nsquares containing pits. Finally, since there is exactly one wumpus, a constant Wumpus is\njust as good as a unary predicate (and perhaps more digniﬁed from the wumpus’s viewpoint).\nThe agent’s location changes over time, so we write At(Agent, s, t) to mean that the\nagent is at square s at time t. We can ﬁx the wumpus’s location with ∀t At(Wumpus, [2, 2], t).\nWe can then say that objects can only be at one location at a time:\n∀x, s1, s2, t At(x, s1, t) ∧At(x, s2, t) ⇒s1 = s2 .\nGiven its current location, the agent can infer properties of the square from properties of its",
  "We can then say that objects can only be at one location at a time:\n∀x, s1, s2, t At(x, s1, t) ∧At(x, s2, t) ⇒s1 = s2 .\nGiven its current location, the agent can infer properties of the square from properties of its\ncurrent percept. For example, if the agent is at a square and perceives a breeze, then that\nsquare is breezy:\n∀s, t At(Agent, s, t) ∧Breeze(t) ⇒Breezy(s) .\nIt is useful to know that a square is breezy because we know that the pits cannot move about.\nNotice that Breezy has no time argument.\nHaving discovered which places are breezy (or smelly) and, very important, not breezy\n(or not smelly), the agent can deduce where the pits are (and where the wumpus is). Whereas\npropositional logic necessitates a separate axiom for each square (see R2 and R3 on page 247)\nand would need a different set of axioms for each geographical layout of the world, ﬁrst-order\nlogic just needs one axiom:\n∀s Breezy(s) ⇔∃r Adjacent(r, s) ∧Pit(r) .\n(8.4)\nSimilarly, in ﬁrst-order logic we can quantify over time, so we need just one successor-state\naxiom for each predicate, rather than a different copy for each time step. For example, the\naxiom for the arrow (Equation (7.2) on page 267) becomes\n∀t HaveArrow(t + 1) ⇔(HaveArrow(t) ∧¬Action(Shoot, t)) .\nFrom these two example sentences, we can see that the ﬁrst-order logic formulation is no\nless concise than the original English-language description given in Chapter 7. The reader\n10 Similarly, most of us do not name each bird that ﬂies overhead as it migrates to warmer regions in winter. An\nornithologist wishing to study migration patterns, survival rates, and so on does name each bird, by means of a\nring on its leg, because individual birds must be tracked. Section 8.4.\nKnowledge Engineering in First-Order Logic\n307\nis invited to construct analogous axioms for the agent’s location and orientation; in these\ncases, the axioms quantify over both space and time. As in the case of propositional state\nestimation, an agent can use logical inference with axioms of this kind to keep track of aspects\nof the world that are not directly observed. Chapter 10 goes into more depth on the subject of\nﬁrst-order successor-state axioms and their uses for constructing plans.\n8.4\nKNOWLEDGE ENGINEERING IN FIRST-ORDER LOGIC\nThe preceding section illustrated the use of ﬁrst-order logic to represent knowledge in three\nsimple domains. This section describes the general process of knowledge-base construction—",
  "8.4\nKNOWLEDGE ENGINEERING IN FIRST-ORDER LOGIC\nThe preceding section illustrated the use of ﬁrst-order logic to represent knowledge in three\nsimple domains. This section describes the general process of knowledge-base construction—\na process called knowledge engineering. A knowledge engineer is someone who investigates\nKNOWLEDGE\nENGINEERING\na particular domain, learns what concepts are important in that domain, and creates a formal\nrepresentation of the objects and relations in the domain. We illustrate the knowledge engi-\nneering process in an electronic circuit domain that should already be fairly familiar, so that\nwe can concentrate on the representational issues involved. The approach we take is suitable\nfor developing special-purpose knowledge bases whose domain is carefully circumscribed\nand whose range of queries is known in advance. General-purpose knowledge bases, which\ncover a broad range of human knowledge and are intended to support tasks such as natural\nlanguage understanding, are discussed in Chapter 12.\n8.4.1\nThe knowledge-engineering process\nKnowledge engineering projects vary widely in content, scope, and difﬁculty, but all such\nprojects include the following steps:\n1. Identify the task. The knowledge engineer must delineate the range of questions that\nthe knowledge base will support and the kinds of facts that will be available for each\nspeciﬁc problem instance. For example, does the wumpus knowledge base need to be\nable to choose actions or is it required to answer questions only about the contents\nof the environment? Will the sensor facts include the current location? The task will\ndetermine what knowledge must be represented in order to connect problem instances to\nanswers. This step is analogous to the PEAS process for designing agents in Chapter 2.\n2. Assemble the relevant knowledge. The knowledge engineer might already be an expert\nin the domain, or might need to work with real experts to extract what they know—a\nprocess called knowledge acquisition. At this stage, the knowledge is not represented\nKNOWLEDGE\nACQUISITION\nformally. The idea is to understand the scope of the knowledge base, as determined by\nthe task, and to understand how the domain actually works.\nFor the wumpus world, which is deﬁned by an artiﬁcial set of rules, the relevant\nknowledge is easy to identify. (Notice, however, that the deﬁnition of adjacency was\nnot supplied explicitly in the wumpus-world rules.) For real domains, the issue of",
  "For the wumpus world, which is deﬁned by an artiﬁcial set of rules, the relevant\nknowledge is easy to identify. (Notice, however, that the deﬁnition of adjacency was\nnot supplied explicitly in the wumpus-world rules.) For real domains, the issue of\nrelevance can be quite difﬁcult—for example, a system for simulating VLSI designs\nmight or might not need to take into account stray capacitances and skin effects. 308\nChapter\n8.\nFirst-Order Logic\n3. Decide on a vocabulary of predicates, functions, and constants. That is, translate the\nimportant domain-level concepts into logic-level names. This involves many questions\nof knowledge-engineering style. Like programming style, this can have a signiﬁcant\nimpact on the eventual success of the project. For example, should pits be represented\nby objects or by a unary predicate on squares? Should the agent’s orientation be a\nfunction or a predicate? Should the wumpus’s location depend on time? Once the\nchoices have been made, the result is a vocabulary that is known as the ontology of\nONTOLOGY\nthe domain. The word ontology means a particular theory of the nature of being or\nexistence. The ontology determines what kinds of things exist, but does not determine\ntheir speciﬁc properties and interrelationships.\n4. Encode general knowledge about the domain. The knowledge engineer writes down\nthe axioms for all the vocabulary terms. This pins down (to the extent possible) the\nmeaning of the terms, enabling the expert to check the content. Often, this step reveals\nmisconceptions or gaps in the vocabulary that must be ﬁxed by returning to step 3 and\niterating through the process.\n5. Encode a description of the speciﬁc problem instance. If the ontology is well thought\nout, this step will be easy. It will involve writing simple atomic sentences about in-\nstances of concepts that are already part of the ontology. For a logical agent, problem\ninstances are supplied by the sensors, whereas a “disembodied” knowledge base is sup-\nplied with additional sentences in the same way that traditional programs are supplied\nwith input data.\n6. Pose queries to the inference procedure and get answers. This is where the reward is:\nwe can let the inference procedure operate on the axioms and problem-speciﬁc facts to\nderive the facts we are interested in knowing. Thus, we avoid the need for writing an\napplication-speciﬁc solution algorithm.\n7. Debug the knowledge base. Alas, the answers to queries will seldom be correct on",
  "derive the facts we are interested in knowing. Thus, we avoid the need for writing an\napplication-speciﬁc solution algorithm.\n7. Debug the knowledge base. Alas, the answers to queries will seldom be correct on\nthe ﬁrst try. More precisely, the answers will be correct for the knowledge base as\nwritten, assuming that the inference procedure is sound, but they will not be the ones\nthat the user is expecting. For example, if an axiom is missing, some queries will not be\nanswerable from the knowledge base. A considerable debugging process could ensue.\nMissing axioms or axioms that are too weak can be easily identiﬁed by noticing places\nwhere the chain of reasoning stops unexpectedly. For example, if the knowledge base\nincludes a diagnostic rule (see Exercise 8.13) for ﬁnding the wumpus,\n∀s Smelly(s) ⇒Adjacent(Home(Wumpus), s) ,\ninstead of the biconditional, then the agent will never be able to prove the absence of\nwumpuses. Incorrect axioms can be identiﬁed because they are false statements about\nthe world. For example, the sentence\n∀x NumOfLegs(x, 4) ⇒Mammal(x)\nis false for reptiles, amphibians, and, more importantly, tables. The falsehood of this\nsentence can be determined independently of the rest of the knowledge base. In contrast, Section 8.4.\nKnowledge Engineering in First-Order Logic\n309\na typical error in a program looks like this:\noffset = position + 1 .\nIt is impossible to tell whether this statement is correct without looking at the rest of the\nprogram to see whether, for example, offset is used to refer to the current position,\nor to one beyond the current position, or whether the value of position is changed\nby another statement and so offset should also be changed again.\nTo understand this seven-step process better, we now apply it to an extended example—the\ndomain of electronic circuits.\n8.4.2\nThe electronic circuits domain\nWe will develop an ontology and knowledge base that allow us to reason about digital circuits\nof the kind shown in Figure 8.6. We follow the seven-step process for knowledge engineering.\nIdentify the task\nThere are many reasoning tasks associated with digital circuits. At the highest level, one\nanalyzes the circuit’s functionality. For example, does the circuit in Figure 8.6 actually add\nproperly? If all the inputs are high, what is the output of gate A2? Questions about the\ncircuit’s structure are also interesting. For example, what are all the gates connected to the",
  "properly? If all the inputs are high, what is the output of gate A2? Questions about the\ncircuit’s structure are also interesting. For example, what are all the gates connected to the\nﬁrst input terminal? Does the circuit contain feedback loops? These will be our tasks in this\nsection. There are more detailed levels of analysis, including those related to timing delays,\ncircuit area, power consumption, production cost, and so on. Each of these levels would\nrequire additional knowledge.\nAssemble the relevant knowledge\nWhat do we know about digital circuits? For our purposes, they are composed of wires and\ngates. Signals ﬂow along wires to the input terminals of gates, and each gate produces a\n1\n2\n3\n1\n2\nX1\nX2\nA1\nA2\nO1\nC1\nFigure 8.6\nA digital circuit C1, purporting to be a one-bit full adder. The ﬁrst two inputs\nare the two bits to be added, and the third input is a carry bit. The ﬁrst output is the sum, and\nthe second output is a carry bit for the next adder. The circuit contains two XOR gates, two\nAND gates, and one OR gate. 310\nChapter\n8.\nFirst-Order Logic\nsignal on the output terminal that ﬂows along another wire. To determine what these signals\nwill be, we need to know how the gates transform their input signals. There are four types\nof gates: AND, OR, and XOR gates have two input terminals, and NOT gates have one. All\ngates have one output terminal. Circuits, like gates, have input and output terminals.\nTo reason about functionality and connectivity, we do not need to talk about the wires\nthemselves, the paths they take, or the junctions where they come together. All that matters\nis the connections between terminals—we can say that one output terminal is connected to\nanother input terminal without having to say what actually connects them. Other factors such\nas the size, shape, color, or cost of the various components are irrelevant to our analysis.\nIf our purpose were something other than verifying designs at the gate level, the ontol-\nogy would be different. For example, if we were interested in debugging faulty circuits, then\nit would probably be a good idea to include the wires in the ontology, because a faulty wire\ncan corrupt the signal ﬂowing along it. For resolving timing faults, we would need to include\ngate delays. If we were interested in designing a product that would be proﬁtable, then the\ncost of the circuit and its speed relative to other products on the market would be important.\nDecide on a vocabulary",
  "gate delays. If we were interested in designing a product that would be proﬁtable, then the\ncost of the circuit and its speed relative to other products on the market would be important.\nDecide on a vocabulary\nWe now know that we want to talk about circuits, terminals, signals, and gates. The next step\nis to choose functions, predicates, and constants to represent them. First, we need to be able\nto distinguish gates from each other and from other objects. Each gate is represented as an\nobject named by a constant, about which we assert that it is a gate with, say, Gate(X1). The\nbehavior of each gate is determined by its type: one of the constants AND, OR, XOR, or\nNOT. Because a gate has exactly one type, a function is appropriate: Type(X1) = XOR.\nCircuits, like gates, are identiﬁed by a predicate: Circuit(C1).\nNext we consider terminals, which are identiﬁed by the predicate Terminal(x). A gate\nor circuit can have one or more input terminals and one or more output terminals. We use the\nfunction In(1, X1) to denote the ﬁrst input terminal for gate X1. A similar function Out is\nused for output terminals. The function Arity(c, i, j) says that circuit c has i input and j out-\nput terminals. The connectivity between gates can be represented by a predicate, Connected,\nwhich takes two terminals as arguments, as in Connected(Out(1, X1), In(1, X2)).\nFinally, we need to know whether a signal is on or off. One possibility is to use a unary\npredicate, On(t), which is true when the signal at a terminal is on. This makes it a little\ndifﬁcult, however, to pose questions such as “What are all the possible values of the signals\nat the output terminals of circuit C1 ?” We therefore introduce as objects two signal values, 1\nand 0, and a function Signal(t) that denotes the signal value for the terminal t.\nEncode general knowledge of the domain\nOne sign that we have a good ontology is that we require only a few general rules, which can\nbe stated clearly and concisely. These are all the axioms we will need:\n1. If two terminals are connected, then they have the same signal:\n∀t1, t2 Terminal(t1) ∧Terminal(t2) ∧Connected(t1, t2) ⇒\nSignal(t1) = Signal(t2) . Section 8.4.\nKnowledge Engineering in First-Order Logic\n311\n2. The signal at every terminal is either 1 or 0:\n∀t Terminal(t) ⇒Signal(t) = 1 ∨Signal(t) = 0 .\n3. Connected is commutative:\n∀t1, t2 Connected(t1, t2) ⇔Connected(t2, t1) .\n4. There are four types of gates:\n∀g Gate(g) ∧k = Type(g) ⇒k = AND ∨k = OR ∨k = XOR ∨k = NOT .",
  "∀t Terminal(t) ⇒Signal(t) = 1 ∨Signal(t) = 0 .\n3. Connected is commutative:\n∀t1, t2 Connected(t1, t2) ⇔Connected(t2, t1) .\n4. There are four types of gates:\n∀g Gate(g) ∧k = Type(g) ⇒k = AND ∨k = OR ∨k = XOR ∨k = NOT .\n5. An AND gate’s output is 0 if and only if any of its inputs is 0:\n∀g Gate(g) ∧Type(g) = AND ⇒\nSignal(Out(1, g)) = 0 ⇔∃n Signal(In(n, g)) = 0 .\n6. An OR gate’s output is 1 if and only if any of its inputs is 1:\n∀g Gate(g) ∧Type(g) = OR ⇒\nSignal(Out(1, g)) = 1 ⇔∃n Signal(In(n, g)) = 1 .\n7. An XOR gate’s output is 1 if and only if its inputs are different:\n∀g Gate(g) ∧Type(g) = XOR ⇒\nSignal(Out(1, g)) = 1 ⇔Signal(In(1, g)) ̸= Signal(In(2, g)) .\n8. A NOT gate’s output is different from its input:\n∀g Gate(g) ∧(Type(g) = NOT) ⇒\nSignal(Out(1, g)) ̸= Signal(In(1, g)) .\n9. The gates (except for NOT) have two inputs and one output.\n∀g Gate(g) ∧Type(g) = NOT ⇒Arity(g, 1, 1) .\n∀g Gate(g) ∧k = Type(g) ∧(k = AND ∨k = OR ∨k = XOR) ⇒\nArity(g, 2, 1)\n10. A circuit has terminals, up to its input and output arity, and nothing beyond its arity:\n∀c, i, j Circuit(c) ∧Arity(c, i, j) ⇒\n∀n (n ≤i ⇒Terminal(In(c, n))) ∧(n > i ⇒In(c, n) = Nothing) ∧\n∀n (n ≤j ⇒Terminal(Out(c, n))) ∧(n > j ⇒Out(c, n) = Nothing)\n11. Gates, terminals, signals, gate types, and Nothing are all distinct.\n∀g, t Gate(g) ∧Terminal(t) ⇒\ng ̸= t ̸= 1 ̸= 0 ̸= OR ̸= AND ̸= XOR ̸= NOT ̸= Nothing .\n12. Gates are circuits.\n∀g Gate(g) ⇒Circuit(g)\nEncode the speciﬁc problem instance\nThe circuit shown in Figure 8.6 is encoded as circuit C1 with the following description. First,\nwe categorize the circuit and its component gates:\nCircuit(C1) ∧Arity(C1, 3, 2)\nGate(X1) ∧Type(X1) = XOR\nGate(X2) ∧Type(X2) = XOR\nGate(A1) ∧Type(A1) = AND\nGate(A2) ∧Type(A2) = AND\nGate(O1) ∧Type(O1) = OR . 312\nChapter\n8.\nFirst-Order Logic\nThen, we show the connections between them:\nConnected(Out(1, X1), In(1, X2))\nConnected(In(1, C1), In(1, X1))\nConnected(Out(1, X1), In(2, A2))\nConnected(In(1, C1), In(1, A1))\nConnected(Out(1, A2), In(1, O1))\nConnected(In(2, C1), In(2, X1))\nConnected(Out(1, A1), In(2, O1))\nConnected(In(2, C1), In(2, A1))\nConnected(Out(1, X2), Out(1, C1)) Connected(In(3, C1), In(2, X2))\nConnected(Out(1, O1), Out(2, C1)) Connected(In(3, C1), In(1, A2)) .\nPose queries to the inference procedure\nWhat combinations of inputs would cause the ﬁrst output of C1 (the sum bit) to be 0 and the\nsecond output of C1 (the carry bit) to be 1?\n∃i1, i2, i3 Signal(In(1, C1)) = i1 ∧Signal(In(2, C1)) = i2 ∧Signal(In(3, C1)) = i3",
  "What combinations of inputs would cause the ﬁrst output of C1 (the sum bit) to be 0 and the\nsecond output of C1 (the carry bit) to be 1?\n∃i1, i2, i3 Signal(In(1, C1)) = i1 ∧Signal(In(2, C1)) = i2 ∧Signal(In(3, C1)) = i3\n∧Signal(Out(1, C1)) = 0 ∧Signal(Out(2, C1)) = 1 .\nThe answers are substitutions for the variables i1, i2, and i3 such that the resulting sentence\nis entailed by the knowledge base. ASKVARS will give us three such substitutions:\n{i1/1, i2/1, i3/0}\n{i1/1, i2/0, i3/1}\n{i1/0, i2/1, i3/1} .\nWhat are the possible sets of values of all the terminals for the adder circuit?\n∃i1, i2, i3, o1, o2 Signal(In(1, C1)) = i1 ∧Signal(In(2, C1)) = i2\n∧Signal(In(3, C1)) = i3 ∧Signal(Out(1, C1)) = o1 ∧Signal(Out(2, C1)) = o2 .\nThis ﬁnal query will return a complete input–output table for the device, which can be used\nto check that it does in fact add its inputs correctly. This is a simple example of circuit\nveriﬁcation. We can also use the deﬁnition of the circuit to build larger digital systems, for\nCIRCUIT\nVERIFICATION\nwhich the same kind of veriﬁcation procedure can be carried out. (See Exercise 8.26.) Many\ndomains are amenable to the same kind of structured knowledge-base development, in which\nmore complex concepts are deﬁned on top of simpler concepts.\nDebug the knowledge base\nWe can perturb the knowledge base in various ways to see what kinds of erroneous behaviors\nemerge. For example, suppose we fail to read Section 8.2.8 and hence forget to assert that\n1 ̸= 0. Suddenly, the system will be unable to prove any outputs for the circuit, except for\nthe input cases 000 and 110. We can pinpoint the problem by asking for the outputs of each\ngate. For example, we can ask\n∃i1, i2, o Signal(In(1, C1)) = i1 ∧Signal(In(2, C1)) = i2 ∧Signal(Out(1, X1)) ,\nwhich reveals that no outputs are known at X1 for the input cases 10 and 01. Then, we look\nat the axiom for XOR gates, as applied to X1:\nSignal(Out(1, X1)) = 1 ⇔Signal(In(1, X1)) ̸= Signal(In(2, X1)) .\nIf the inputs are known to be, say, 1 and 0, then this reduces to\nSignal(Out(1, X1)) = 1 ⇔1 ̸= 0 .\nNow the problem is apparent: the system is unable to infer that Signal(Out(1, X1)) = 1, so\nwe need to tell it that 1 ̸= 0. Section 8.5.\nSummary\n313\n8.5\nSUMMARY\nThis chapter has introduced ﬁrst-order logic, a representation language that is far more pow-\nerful than propositional logic. The important points are as follows:\n• Knowledge representation languages should be declarative, compositional, expressive,",
  "This chapter has introduced ﬁrst-order logic, a representation language that is far more pow-\nerful than propositional logic. The important points are as follows:\n• Knowledge representation languages should be declarative, compositional, expressive,\ncontext independent, and unambiguous.\n• Logics differ in their ontological commitments and epistemological commitments.\nWhile propositional logic commits only to the existence of facts, ﬁrst-order logic com-\nmits to the existence of objects and relations and thereby gains expressive power.\n• The syntax of ﬁrst-order logic builds on that of propositional logic. It adds terms to\nrepresent objects, and has universal and existential quantiﬁers to construct assertions\nabout all or some of the possible values of the quantiﬁed variables.\n• A possible world, or model, for ﬁrst-order logic includes a set of objects and an inter-\npretation that maps constant symbols to objects, predicate symbols to relations among\nobjects, and function symbols to functions on objects.\n• An atomic sentence is true just when the relation named by the predicate holds between\nthe objects named by the terms. Extended interpretations, which map quantiﬁer vari-\nables to objects in the model, deﬁne the truth of quantiﬁed sentences.\n• Developing a knowledge base in ﬁrst-order logic requires a careful process of analyzing\nthe domain, choosing a vocabulary, and encoding the axioms required to support the\ndesired inferences.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nAlthough Aristotle’s logic deals with generalizations over objects, it fell far short of the ex-\npressive power of ﬁrst-order logic. A major barrier to its further development was its concen-\ntration on one-place predicates to the exclusion of many-place relational predicates. The ﬁrst\nsystematic treatment of relations was given by Augustus De Morgan (1864), who cited the\nfollowing example to show the sorts of inferences that Aristotle’s logic could not handle: “All\nhorses are animals; therefore, the head of a horse is the head of an animal.” This inference\nis inaccessible to Aristotle because any valid rule that can support this inference must ﬁrst\nanalyze the sentence using the two-place predicate “x is the head of y.” The logic of relations\nwas studied in depth by Charles Sanders Peirce (1870, 2004).\nTrue ﬁrst-order logic dates from the introduction of quantiﬁers in Gottlob Frege’s (1879)\nBegriffschrift (“Concept Writing” or “Conceptual Notation”). Peirce (1883) also developed",
  "was studied in depth by Charles Sanders Peirce (1870, 2004).\nTrue ﬁrst-order logic dates from the introduction of quantiﬁers in Gottlob Frege’s (1879)\nBegriffschrift (“Concept Writing” or “Conceptual Notation”). Peirce (1883) also developed\nﬁrst-order logic independently of Frege, although slightly later. Frege’s ability to nest quan-\ntiﬁers was a big step forward, but he used an awkward notation. The present notation for\nﬁrst-order logic is due substantially to Giuseppe Peano (1889), but the semantics is virtually\nidentical to Frege’s. Oddly enough, Peano’s axioms were due in large measure to Grassmann\n(1861) and Dedekind (1888). 314\nChapter\n8.\nFirst-Order Logic\nLeopold L¨owenheim (1915) gave a systematic treatment of model theory for ﬁrst-order\nlogic, including the ﬁrst proper treatment of the equality symbol. L¨owenheim’s results were\nfurther extended by Thoralf Skolem (1920). Alfred Tarski (1935, 1956) gave an explicit\ndeﬁnition of truth and model-theoretic satisfaction in ﬁrst-order logic, using set theory.\nMcCarthy (1958) was primarily responsible for the introduction of ﬁrst-order logic as a\ntool for building AI systems. The prospects for logic-based AI were advanced signiﬁcantly by\nRobinson’s (1965) development of resolution, a complete procedure for ﬁrst-order inference\ndescribed in Chapter 9. The logicist approach took root at Stanford University. Cordell Green\n(1969a, 1969b) developed a ﬁrst-order reasoning system, QA3, leading to the ﬁrst attempts to\nbuild a logical robot at SRI (Fikes and Nilsson, 1971). First-order logic was applied by Zohar\nManna and Richard Waldinger (1971) for reasoning about programs and later by Michael\nGenesereth (1984) for reasoning about circuits. In Europe, logic programming (a restricted\nform of ﬁrst-order reasoning) was developed for linguistic analysis (Colmerauer et al., 1973)\nand for general declarative systems (Kowalski, 1974). Computational logic was also well\nentrenched at Edinburgh through the LCF (Logic for Computable Functions) project (Gordon\net al., 1979). These developments are chronicled further in Chapters 9 and 12.\nPractical applications built with ﬁrst-order logic include a system for evaluating the\nmanufacturing requirements for electronic products (Mannion, 2002), a system for reasoning\nabout policies for ﬁle access and digital rights management (Halpern and Weissman, 2008),\nand a system for the automated composition of Web services (McIlraith and Zeng, 2001).",
  "about policies for ﬁle access and digital rights management (Halpern and Weissman, 2008),\nand a system for the automated composition of Web services (McIlraith and Zeng, 2001).\nReactions to the Whorf hypothesis (Whorf, 1956) and the problem of language and\nthought in general, appear in several recent books (Gumperz and Levinson, 1996; Bowerman\nand Levinson, 2001; Pinker, 2003; Gentner and Goldin-Meadow, 2003). The “theory” theory\n(Gopnik and Glymour, 2002; Tenenbaum et al., 2007) views children’s learning about the\nworld as analogous to the construction of scientiﬁc theories. Just as the predictions of a\nmachine learning algorithm depend strongly on the vocabulary supplied to it, so will the\nchild’s formulation of theories depend on the linguistic environment in which learning occurs.\nThere are a number of good introductory texts on ﬁrst-order logic, including some by\nleading ﬁgures in the history of logic: Alfred Tarski (1941), Alonzo Church (1956), and\nW.V. Quine (1982) (which is one of the most readable). Enderton (1972) gives a more math-\nematically oriented perspective. A highly formal treatment of ﬁrst-order logic, along with\nmany more advanced topics in logic, is provided by Bell and Machover (1977). Manna and\nWaldinger (1985) give a readable introduction to logic from a computer science perspec-\ntive, as do Huth and Ryan (2004), who concentrate on program veriﬁcation. Barwise and\nEtchemendy (2002) take an approach similar to the one used here. Smullyan (1995) presents\nresults concisely, using the tableau format. Gallier (1986) provides an extremely rigorous\nmathematical exposition of ﬁrst-order logic, along with a great deal of material on its use in\nautomated reasoning. Logical Foundations of Artiﬁcial Intelligence (Genesereth and Nilsson,\n1987) is both a solid introduction to logic and the ﬁrst systematic treatment of logical agents\nwith percepts and actions, and there are two good handbooks: van Bentham and ter Meulen\n(1997) and Robinson and Voronkov (2001). The journal of record for the ﬁeld of pure math-\nematical logic is the Journal of Symbolic Logic, whereas the Journal of Applied Logic deals\nwith concerns closer to those of artiﬁcial intelligence. Exercises\n315\nEXERCISES\n8.1\nA logical knowledge base represents the world using a set of sentences with no explicit\nstructure. An analogical representation, on the other hand, has physical structure that corre-",
  "315\nEXERCISES\n8.1\nA logical knowledge base represents the world using a set of sentences with no explicit\nstructure. An analogical representation, on the other hand, has physical structure that corre-\nsponds directly to the structure of the thing represented. Consider a road map of your country\nas an analogical representation of facts about the country—it represents facts with a map lan-\nguage. The two-dimensional structure of the map corresponds to the two-dimensional surface\nof the area.\na. Give ﬁve examples of symbols in the map language.\nb. An explicit sentence is a sentence that the creator of the representation actually writes\ndown. An implicit sentence is a sentence that results from explicit sentences because\nof properties of the analogical representation. Give three examples each of implicit and\nexplicit sentences in the map language.\nc. Give three examples of facts about the physical structure of your country that cannot be\nrepresented in the map language.\nd. Give two examples of facts that are much easier to express in the map language than in\nﬁrst-order logic.\ne. Give two other examples of useful analogical representations. What are the advantages\nand disadvantages of each of these languages?\n8.2\nConsider a knowledge base containing just two sentences: P(a) and P(b). Does this\nknowledge base entail ∀x P(x)? Explain your answer in terms of models.\n8.3\nIs the sentence ∃x, y x = y valid? Explain.\n8.4\nWrite down a logical sentence such that every world in which it is true contains exactly\none object.\n8.5\nConsider a symbol vocabulary that contains c constant symbols, pk predicate symbols\nof each arity k, and fk function symbols of each arity k, where 1 ≤k ≤A. Let the domain\nsize be ﬁxed at D. For any given model, each predicate or function symbol is mapped onto a\nrelation or function, respectively, of the same arity. You may assume that the functions in the\nmodel allow some input tuples to have no value for the function (i.e., the value is the invisible\nobject). Derive a formula for the number of possible models for a domain with D elements.\nDon’t worry about eliminating redundant combinations.\n8.6\nWhich of the following are valid (necessarily true) sentences?\na. (∃x x = x) ⇒(∀y ∃z y = z).\nb. ∀x P(x) ∨¬P(x).\nc. ∀x Smart(x) ∨(x = x).\n8.7\nConsider a version of the semantics for ﬁrst-order logic in which models with empty\ndomains are allowed. Give at least two examples of sentences that are valid according to the 316\nChapter\n8.\nFirst-Order Logic",
  "c. ∀x Smart(x) ∨(x = x).\n8.7\nConsider a version of the semantics for ﬁrst-order logic in which models with empty\ndomains are allowed. Give at least two examples of sentences that are valid according to the 316\nChapter\n8.\nFirst-Order Logic\nstandard semantics but not according to the new semantics. Discuss which outcome makes\nmore intuitive sense for your examples.\n8.8\nDoes the fact ¬Spouse(George, Laura) follow from the facts Jim ̸= George and\nSpouse(Jim, Laura)? If so, give a proof; if not, supply additional axioms as needed. What\nhappens if we use Spouse as a unary function symbol instead of a binary predicate?\n8.9\nThis exercise uses the function MapColor and predicates In(x, y), Borders(x, y), and\nCountry(x), whose arguments are geographical regions, along with constant symbols for\nvarious regions. In each of the following we give an English sentence and a number of can-\ndidate logical expressions. For each of the logical expressions, state whether it (1) correctly\nexpresses the English sentence; (2) is syntactically invalid and therefore meaningless; or (3)\nis syntactically valid but does not express the meaning of the English sentence.\na. Paris and Marseilles are both in France.\n(i) In(Paris ∧Marseilles, France).\n(ii) In(Paris, France) ∧In(Marseilles, France).\n(iii) In(Paris, France) ∨In(Marseilles, France).\nb. There is a country that borders both Iraq and Pakistan.\n(i) ∃c Country(c) ∧Border(c, Iraq) ∧Border(c, Pakistan).\n(ii) ∃c Country(c) ⇒[Border(c, Iraq) ∧Border(c, Pakistan)].\n(iii) [∃c\nCountry(c)] ⇒[Border(c, Iraq) ∧Border(c, Pakistan)].\n(iv) ∃c Border(Country(c), Iraq ∧Pakistan).\nc. All countries that border Ecuador are in South America.\n(i) ∀c Country(c) ∧Border(c, Ecuador) ⇒In(c, SouthAmerica).\n(ii) ∀c Country(c) ⇒[Border(c, Ecuador) ⇒In(c, SouthAmerica)].\n(iii) ∀c [Country(c) ⇒Border(c, Ecuador)] ⇒In(c, SouthAmerica).\n(iv) ∀c Country(c) ∧Border(c, Ecuador) ∧In(c, SouthAmerica).\nd. No region in South America borders any region in Europe.\n(i) ¬[∃c, d In(c, SouthAmerica) ∧In(d, Europe) ∧Borders(c, d)].\n(ii) ∀c, d [In(c, SouthAmerica) ∧In(d, Europe)] ⇒¬Borders(c, d)].\n(iii) ¬∀c In(c, SouthAmerica) ⇒∃d In(d, Europe) ∧¬Borders(c, d).\n(iv) ∀c In(c, SouthAmerica) ⇒∀d In(d, Europe) ⇒¬Borders(c, d).\ne. No two adjacent countries have the same map color.\n(i) ∀x, y ¬Country(x) ∨¬Country(y) ∨¬Borders(x, y) ∨\n¬(MapColor(x) = MapColor(y)).\n(ii) ∀x, y (Country(x) ∧Country(y) ∧Borders(x, y) ∧¬(x = y)) ⇒\n¬(MapColor(x) = MapColor(y)).",
  "e. No two adjacent countries have the same map color.\n(i) ∀x, y ¬Country(x) ∨¬Country(y) ∨¬Borders(x, y) ∨\n¬(MapColor(x) = MapColor(y)).\n(ii) ∀x, y (Country(x) ∧Country(y) ∧Borders(x, y) ∧¬(x = y)) ⇒\n¬(MapColor(x) = MapColor(y)).\n(iii) ∀x, y Country(x) ∧Country(y) ∧Borders(x, y) ∧\n¬(MapColor(x) = MapColor(y)).\n(iv) ∀x, y (Country(x) ∧Country(y) ∧Borders(x, y)) ⇒MapColor(x ̸= y). Exercises\n317\n8.10\nConsider a vocabulary with the following symbols:\nOccupation(p, o): Predicate. Person p has occupation o.\nCustomer(p1, p2): Predicate. Person p1 is a customer of person p2.\nBoss(p1, p2): Predicate. Person p1 is a boss of person p2.\nDoctor, Surgeon, Lawyer, Actor: Constants denoting occupations.\nEmily, Joe: Constants denoting people.\nUse these symbols to write the following assertions in ﬁrst-order logic:\na. Emily is either a surgeon or a lawyer.\nb. Joe is an actor, but he also holds another job.\nc. All surgeons are doctors.\nd. Joe does not have a lawyer (i.e., is not a customer of any lawyer).\ne. Emily has a boss who is a lawyer.\nf. There exists a lawyer all of whose customers are doctors.\ng. Every surgeon has a lawyer.\n8.11\nComplete the following exercises about logical senntences:\na. Translate into good, natural English (no xs or ys!):\n∀x, y, l SpeaksLanguage(x, l) ∧SpeaksLanguage(y, l)\n⇒Understands(x, y) ∧Understands(y, x).\nb. Explain why this sentence is entailed by the sentence\n∀x, y, l SpeaksLanguage(x, l) ∧SpeaksLanguage(y, l)\n⇒Understands(x, y).\nc. Translate into ﬁrst-order logic the following sentences:\n(i) Understanding leads to friendship.\n(ii) Friendship is transitive.\nRemember to deﬁne all predicates, functions, and constants you use.\n8.12\nRewrite the ﬁrst two Peano axioms in Section 8.3.3 as a single axiom that deﬁnes\nNatNum(x) so as to exclude the possibility of natural numbers except for those generated by\nthe successor function.\n8.13\nEquation (8.4) on page 306 deﬁnes the conditions under which a square is breezy. Here\nwe consider two other ways to describe this aspect of the wumpus world.\na. We can write diagnostic rules leading from observed effects to hidden causes. For ﬁnd-\nDIAGNOSTIC RULE\ning pits, the obvious diagnostic rules say that if a square is breezy, some adjacent square\nmust contain a pit; and if a square is not breezy, then no adjacent square contains a pit.\nWrite these two rules in ﬁrst-order logic and show that their conjunction is logically\nequivalent to Equation (8.4).",
  "must contain a pit; and if a square is not breezy, then no adjacent square contains a pit.\nWrite these two rules in ﬁrst-order logic and show that their conjunction is logically\nequivalent to Equation (8.4).\nb. We can write causal rules leading from cause to effect. One obvious causal rule is that\nCAUSAL RULE\na pit causes all adjacent squares to be breezy. Write this rule in ﬁrst-order logic, explain\nwhy it is incomplete compared to Equation (8.4), and supply the missing axiom. 318\nChapter\n8.\nFirst-Order Logic\nBeatrice\nAndrew\nEugenie\nWilliam Harry\nCharles\nDiana\nMum\nGeorge\nPhilip\nElizabeth\nMargaret\nKydd\nSpencer\nPeter\nMark\nZara\nAnne\nSarah\nEdward\nSophie\nLouise\nJames\nFigure 8.7\nA typical family tree. The symbol “▷◁” connects spouses and arrows point to\nchildren.\n8.14\nWrite axioms describing the predicates Grandchild, Greatgrandparent , Ancestor,\nBrother, Sister, Daughter, Son, FirstCousin, BrotherInLaw, SisterInLaw, Aunt, and\nUncle. Find out the proper deﬁnition of mth cousin n times removed, and write the def-\ninition in ﬁrst-order logic. Now write down the basic facts depicted in the family tree in\nFigure 8.7. Using a suitable logical reasoning system, TELL it all the sentences you have\nwritten down, and ASK it who are Elizabeth’s grandchildren, Diana’s brothers-in-law, Zara’s\ngreat-grandparents, and Eugenie’s ancestors.\n8.15\nExplain what is wrong with the following proposed deﬁnition of the set membership\npredicate ∈:\n∀x, s x ∈{x|s}\n∀x, s x ∈s ⇒∀y x ∈{y|s} .\n8.16\nUsing the set axioms as examples, write axioms for the list domain, including all the\nconstants, functions, and predicates mentioned in the chapter.\n8.17\nExplain what is wrong with the following proposed deﬁnition of adjacent squares in\nthe wumpus world:\n∀x, y Adjacent([x, y], [x + 1, y]) ∧Adjacent([x, y], [x, y + 1]) .\n8.18\nWrite out the axioms required for reasoning about the wumpus’s location, using a\nconstant symbol Wumpus and a binary predicate At(Wumpus, Location). Remember that\nthere is only one wumpus.\n8.19\nAssuming predicates Parent(p, q) and Female(p) and constants Joan and Kevin,\nwith the obvious meanings, express each of the following sentences in ﬁrst-order logic. (You\nmay use the abbreviation ∃1 to mean “there exists exactly one.”)\na. Joan has a daughter (possibly more than one, and possibly sons as well).\nb. Joan has exactly one daughter (but may have sons as well).\nc. Joan has exactly one child, a daughter.\nd. Joan and Kevin have exactly one child together.",
  "a. Joan has a daughter (possibly more than one, and possibly sons as well).\nb. Joan has exactly one daughter (but may have sons as well).\nc. Joan has exactly one child, a daughter.\nd. Joan and Kevin have exactly one child together.\ne. Joan has at least one child with Kevin, and no children with anyone else. Exercises\n319\n8.20\nArithmetic assertions can be written in ﬁrst-order logic with the predicate symbol <,\nthe function symbols + and ×, and the constant symbols 0 and 1. Additional predicates can\nalso be deﬁned with biconditionals.\na. Represent the property “x is an even number.”\nb. Represent the property “x is prime.”\nc. Goldbach’s conjecture is the conjecture (unproven as yet) that every even number is\nequal to the sum of two primes. Represent this conjecture as a logical sentence.\n8.21\nIn Chapter 6, we used equality to indicate the relation between a variable and its value.\nFor instance, we wrote WA = red to mean that Western Australia is colored red. Repre-\nsenting this in ﬁrst-order logic, we must write more verbosely ColorOf (WA) = red. What\nincorrect inference could be drawn if we wrote sentences such as WA = red directly as logical\nassertions?\n8.22\nWrite in ﬁrst-order logic the assertion that every key and at least one of every pair of\nsocks will eventually be lost forever, using only the following vocabulary: Key(x), x is a key;\nSock(x), x is a sock; Pair(x, y), x and y are a pair; Now, the current time; Before(t1, t2),\ntime t1 comes before time t2; Lost(x, t), object x is lost at time t.\n8.23\nFor each of the following sentences in English, decide if the accompanying ﬁrst-order\nlogic sentence is a good translation. If not, explain why not and correct it. (Some sentences\nmay have more than one error!)\na. No two people have the same social security number.\n¬∃x, y, n Person(x) ∧Person(y) ⇒[HasSS#(x, n) ∧HasSS#(y, n)].\nb. John’s social security number is the same as Mary’s.\n∃n HasSS#(John, n) ∧HasSS#(Mary, n).\nc. Everyone’s social security number has nine digits.\n∀x, n Person(x) ⇒[HasSS#(x, n) ∧Digits(n, 9)].\nd. Rewrite each of the above (uncorrected) sentences using a function symbol SS# instead\nof the predicate HasSS#.\n8.24\nRepresent the following sentences in ﬁrst-order logic, using a consistent vocabulary\n(which you must deﬁne):\na. Some students took French in spring 2001.\nb. Every student who takes French passes it.\nc. Only one student took Greek in spring 2001.\nd. The best score in Greek is always higher than the best score in French.",
  "(which you must deﬁne):\na. Some students took French in spring 2001.\nb. Every student who takes French passes it.\nc. Only one student took Greek in spring 2001.\nd. The best score in Greek is always higher than the best score in French.\ne. Every person who buys a policy is smart.\nf. No person buys an expensive policy.\ng. There is an agent who sells policies only to people who are not insured. 320\nChapter\n8.\nFirst-Order Logic\nZ0\nZ1\nZ2\nZ3\nZ4\nX0\nY0\nX1\nY1\nX2\nY2\nX3\nY3\nAd0\nAd1\nAd2\nAd3\nX0\nX1\nX2\nX3\nZ0\nZ1\nZ2\nZ3\nZ4\nY0\nY1\nY2\nY3\n+\nFigure 8.8\nA four-bit adder. Each Adi is a one-bit adder, as in Figure 8.6 on page 309.\nh. There is a barber who shaves all men in town who do not shave themselves.\ni. A person born in the UK, each of whose parents is a UK citizen or a UK resident, is a\nUK citizen by birth.\nj. A person born outside the UK, one of whose parents is a UK citizen by birth, is a UK\ncitizen by descent.\nk. Politicians can fool some of the people all of the time, and they can fool all of the people\nsome of the time, but they can’t fool all of the people all of the time.\nl. All Greeks speak the same language. (Use Speaks(x, l) to mean that person x speaks\nlanguage l.)\n8.25\nWrite a general set of facts and axioms to represent the assertion “Wellington heard\nabout Napoleon’s death” and to correctly answer the question “Did Napoleon hear about\nWellington’s death?”\n8.26\nExtend the vocabulary from Section 8.4 to deﬁne addition for n-bit binary numbers.\nThen encode the description of the four-bit adder in Figure 8.8, and pose the queries needed\nto verify that it is in fact correct.\n8.27\nObtain a passport application for your country, identify the rules determining eligi-\nbility for a passport, and translate them into ﬁrst-order logic, following the steps outlined in\nSection 8.4.\n8.28\nConsider a ﬁrst-order logical knowledge base that describes worlds containing people,\nsongs, albums (e.g., “Meet the Beatles”) and disks (i.e., particular physical instances of CDs).\nThe vocabulary contains the following symbols:\nCopyOf (d, a): Predicate. Disk d is a copy of album a.\nOwns(p, d): Predicate. Person p owns disk d.\nSings(p, s, a): Album a includes a recording of song s sung by person p.\nWrote(p, s): Person p wrote song s.\nMcCartney, Gershwin, BHoliday, Joe, EleanorRigby, TheManILove, Revolver:\nConstants with the obvious meanings. Exercises\n321\nExpress the following statements in ﬁrst-order logic:\na. Gershwin wrote “The Man I Love.”\nb. Gershwin did not write “Eleanor Rigby.”",
  "Constants with the obvious meanings. Exercises\n321\nExpress the following statements in ﬁrst-order logic:\na. Gershwin wrote “The Man I Love.”\nb. Gershwin did not write “Eleanor Rigby.”\nc. Either Gershwin or McCartney wrote “The Man I Love.”\nd. Joe has written at least one song.\ne. Joe owns a copy of Revolver.\nf. Every song that McCartney sings on Revolver was written by McCartney.\ng. Gershwin did not write any of the songs on Revolver.\nh. Every song that Gershwin wrote has been recorded on some album. (Possibly different\nsongs are recorded on different albums.)\ni. There is a single album that contains every song that Joe has written.\nj. Joe owns a copy of an album that has Billie Holiday singing “The Man I Love.”\nk. Joe owns a copy of every album that has a song sung by McCartney. (Of course, each\ndifferent album is instantiated in a different physical CD.)\nl. Joe owns a copy of every album on which all the songs are sung by Billie Holiday. 9\nINFERENCE IN\nFIRST-ORDER LOGIC\nIn which we deﬁne effective procedures for answering questions posed in ﬁrst-\norder logic.\nChapter 7 showed how sound and complete inference can be achieved for propositional logic.\nIn this chapter, we extend those results to obtain algorithms that can answer any answer-\nable question stated in ﬁrst-order logic. Section 9.1 introduces inference rules for quantiﬁers\nand shows how to reduce ﬁrst-order inference to propositional inference, albeit at potentially\ngreat expense. Section 9.2 describes the idea of uniﬁcation, showing how it can be used\nto construct inference rules that work directly with ﬁrst-order sentences. We then discuss\nthree major families of ﬁrst-order inference algorithms. Forward chaining and its applica-\ntions to deductive databases and production systems are covered in Section 9.3; backward\nchaining and logic programming systems are developed in Section 9.4. Forward and back-\nward chaining can be very efﬁcient, but are applicable only to knowledge bases that can\nbe expressed as sets of Horn clauses. General ﬁrst-order sentences require resolution-based\ntheorem proving, which is described in Section 9.5.\n9.1\nPROPOSITIONAL VS. FIRST-ORDER INFERENCE\nThis section and the next introduce the ideas underlying modern logical inference systems.\nWe begin with some simple inference rules that can be applied to sentences with quantiﬁers\nto obtain sentences without quantiﬁers. These rules lead naturally to the idea that ﬁrst-order",
  "We begin with some simple inference rules that can be applied to sentences with quantiﬁers\nto obtain sentences without quantiﬁers. These rules lead naturally to the idea that ﬁrst-order\ninference can be done by converting the knowledge base to propositional logic and using\npropositional inference, which we already know how to do. The next section points out an\nobvious shortcut, leading to inference methods that manipulate ﬁrst-order sentences directly.\n9.1.1\nInference rules for quantiﬁers\nLet us begin with universal quantiﬁers. Suppose our knowledge base contains the standard\nfolkloric axiom stating that all greedy kings are evil:\n∀x King(x) ∧Greedy(x) ⇒Evil(x) .\n322 Section 9.1.\nPropositional vs. First-Order Inference\n323\nThen it seems quite permissible to infer any of the following sentences:\nKing(John) ∧Greedy(John) ⇒Evil(John)\nKing(Richard) ∧Greedy(Richard) ⇒Evil(Richard)\nKing(Father(John)) ∧Greedy(Father(John)) ⇒Evil(Father(John)) .\n...\nThe rule of Universal Instantiation (UI for short) says that we can infer any sentence ob-\nUNIVERSAL\nINSTANTIATION\ntained by substituting a ground term (a term without variables) for the variable.1 To write\nGROUND TERM\nout the inference rule formally, we use the notion of substitutions introduced in Section 8.3.\nLet SUBST(θ, α) denote the result of applying the substitution θ to the sentence α. Then the\nrule is written\n∀v α\nSUBST({v/g}, α)\nfor any variable v and ground term g. For example, the three sentences given earlier are\nobtained with the substitutions {x/John}, {x/Richard}, and {x/Father(John)}.\nIn the rule for Existential Instantiation, the variable is replaced by a single new con-\nEXISTENTIAL\nINSTANTIATION\nstant symbol. The formal statement is as follows: for any sentence α, variable v, and constant\nsymbol k that does not appear elsewhere in the knowledge base,\n∃v α\nSUBST({v/k}, α) .\nFor example, from the sentence\n∃x Crown(x) ∧OnHead(x, John)\nwe can infer the sentence\nCrown(C1) ∧OnHead(C1, John)\nas long as C1 does not appear elsewhere in the knowledge base. Basically, the existential\nsentence says there is some object satisfying a condition, and applying the existential instan-\ntiation rule just gives a name to that object. Of course, that name must not already belong\nto another object. Mathematics provides a nice example: suppose we discover that there is a\nnumber that is a little bigger than 2.71828 and that satisﬁes the equation d(xy)/dy = xy for x.",
  "to another object. Mathematics provides a nice example: suppose we discover that there is a\nnumber that is a little bigger than 2.71828 and that satisﬁes the equation d(xy)/dy = xy for x.\nWe can give this number a name, such as e, but it would be a mistake to give it the name of\nan existing object, such as π. In logic, the new name is called a Skolem constant. Existen-\nSKOLEM CONSTANT\ntial Instantiation is a special case of a more general process called skolemization, which we\ncover in Section 9.5.\nWhereas Universal Instantiation can be applied many times to produce many different\nconsequences, Existential Instantiation can be applied once, and then the existentially quan-\ntiﬁed sentence can be discarded. For example, we no longer need ∃x Kill(x, Victim) once\nwe have added the sentence Kill(Murderer, Victim). Strictly speaking, the new knowledge\nbase is not logically equivalent to the old, but it can be shown to be inferentially equivalent\nINFERENTIAL\nEQUIVALENCE\nin the sense that it is satisﬁable exactly when the original knowledge base is satisﬁable.\n1 Do not confuse these substitutions with the extended interpretations used to deﬁne the semantics of quantiﬁers.\nThe substitution replaces a variable with a term (a piece of syntax) to produce a new sentence, whereas an\ninterpretation maps a variable to an object in the domain. 324\nChapter\n9.\nInference in First-Order Logic\n9.1.2\nReduction to propositional inference\nOnce we have rules for inferring nonquantiﬁed sentences from quantiﬁed sentences, it be-\ncomes possible to reduce ﬁrst-order inference to propositional inference. In this section we\ngive the main ideas; the details are given in Section 9.5.\nThe ﬁrst idea is that, just as an existentially quantiﬁed sentence can be replaced by\none instantiation, a universally quantiﬁed sentence can be replaced by the set of all possible\ninstantiations. For example, suppose our knowledge base contains just the sentences\n∀x King(x) ∧Greedy(x) ⇒Evil(x)\nKing(John)\nGreedy(John)\nBrother(Richard, John) .\n(9.1)\nThen we apply UI to the ﬁrst sentence using all possible ground-term substitutions from the\nvocabulary of the knowledge base—in this case, {x/John} and {x/Richard}. We obtain\nKing(John) ∧Greedy(John) ⇒Evil(John)\nKing(Richard) ∧Greedy(Richard) ⇒Evil(Richard) ,\nand we discard the universally quantiﬁed sentence. Now, the knowledge base is essentially\npropositional if we view the ground atomic sentences—King(John), Greedy(John), and",
  "King(Richard) ∧Greedy(Richard) ⇒Evil(Richard) ,\nand we discard the universally quantiﬁed sentence. Now, the knowledge base is essentially\npropositional if we view the ground atomic sentences—King(John), Greedy(John), and\nso on—as proposition symbols. Therefore, we can apply any of the complete propositional\nalgorithms in Chapter 7 to obtain conclusions such as Evil(John).\nThis technique of propositionalization can be made completely general, as we show\nin Section 9.5; that is, every ﬁrst-order knowledge base and query can be propositionalized\nin such a way that entailment is preserved. Thus, we have a complete decision procedure\nfor entailment . . . or perhaps not. There is a problem: when the knowledge base includes\na function symbol, the set of possible ground-term substitutions is inﬁnite! For example, if\nthe knowledge base mentions the Father symbol, then inﬁnitely many nested terms such as\nFather(Father(Father(John))) can be constructed. Our propositional algorithms will have\ndifﬁculty with an inﬁnitely large set of sentences.\nFortunately, there is a famous theorem due to Jacques Herbrand (1930) to the effect\nthat if a sentence is entailed by the original, ﬁrst-order knowledge base, then there is a proof\ninvolving just a ﬁnite subset of the propositionalized knowledge base. Since any such subset\nhas a maximum depth of nesting among its ground terms, we can ﬁnd the subset by ﬁrst\ngenerating all the instantiations with constant symbols (Richard and John), then all terms of\ndepth 1 (Father(Richard) and Father(John)), then all terms of depth 2, and so on, until we\nare able to construct a propositional proof of the entailed sentence.\nWe have sketched an approach to ﬁrst-order inference via propositionalization that is\ncomplete—that is, any entailed sentence can be proved. This is a major achievement, given\nthat the space of possible models is inﬁnite. On the other hand, we do not know until the\nproof is done that the sentence is entailed! What happens when the sentence is not entailed?\nCan we tell? Well, for ﬁrst-order logic, it turns out that we cannot. Our proof procedure can\ngo on and on, generating more and more deeply nested terms, but we will not know whether\nit is stuck in a hopeless loop or whether the proof is just about to pop out. This is very much Section 9.2.\nUniﬁcation and Lifting\n325\nlike the halting problem for Turing machines. Alan Turing (1936) and Alonzo Church (1936)",
  "it is stuck in a hopeless loop or whether the proof is just about to pop out. This is very much Section 9.2.\nUniﬁcation and Lifting\n325\nlike the halting problem for Turing machines. Alan Turing (1936) and Alonzo Church (1936)\nboth proved, in rather different ways, the inevitability of this state of affairs. The question of\nentailment for ﬁrst-order logic is semidecidable—that is, algorithms exist that say yes to every\nentailed sentence, but no algorithm exists that also says no to every nonentailed sentence.\n9.2\nUNIFICATION AND LIFTING\nThe preceding section described the understanding of ﬁrst-order inference that existed up\nto the early 1960s. The sharp-eyed reader (and certainly the computational logicians of the\nearly 1960s) will have noticed that the propositionalization approach is rather inefﬁcient. For\nexample, given the query Evil(x) and the knowledge base in Equation (9.1), it seems per-\nverse to generate sentences such as King(Richard) ∧Greedy(Richard) ⇒Evil(Richard).\nIndeed, the inference of Evil(John) from the sentences\n∀x King(x) ∧Greedy(x) ⇒Evil(x)\nKing(John)\nGreedy(John)\nseems completely obvious to a human being. We now show how to make it completely\nobvious to a computer.\n9.2.1\nA ﬁrst-order inference rule\nThe inference that John is evil—that is, that {x/John} solves the query Evil(x)—works like\nthis: to use the rule that greedy kings are evil, ﬁnd some x such that x is a king and x is\ngreedy, and then infer that this x is evil. More generally, if there is some substitution θ that\nmakes each of the conjuncts of the premise of the implication identical to sentences already\nin the knowledge base, then we can assert the conclusion of the implication, after applying θ.\nIn this case, the substitution θ = {x/John} achieves that aim.\nWe can actually make the inference step do even more work. Suppose that instead of\nknowing Greedy(John), we know that everyone is greedy:\n∀y Greedy(y) .\n(9.2)\nThen we would still like to be able to conclude that Evil(John), because we know that\nJohn is a king (given) and John is greedy (because everyone is greedy). What we need for\nthis to work is to ﬁnd a substitution both for the variables in the implication sentence and\nfor the variables in the sentences that are in the knowledge base. In this case, applying the\nsubstitution {x/John, y/John} to the implication premises King(x) and Greedy(x) and the\nknowledge-base sentences King(John) and Greedy(y) will make them identical. Thus, we",
  "substitution {x/John, y/John} to the implication premises King(x) and Greedy(x) and the\nknowledge-base sentences King(John) and Greedy(y) will make them identical. Thus, we\ncan infer the conclusion of the implication.\nThis inference process can be captured as a single inference rule that we call Gener-\nalized Modus Ponens:2 For atomic sentences pi, pi′, and q, where there is a substitution θ\nGENERALIZED\nMODUS PONENS 326\nChapter\n9.\nInference in First-Order Logic\nsuch that SUBST(θ, pi′) = SUBST(θ, pi), for all i,\np1′, p2′, . . . , pn′, (p1 ∧p2 ∧. . . ∧pn ⇒q)\nSUBST(θ, q)\n.\nThere are n+1 premises to this rule: the n atomic sentences pi′ and the one implication. The\nconclusion is the result of applying the substitution θ to the consequent q. For our example:\np1′ is King(John)\np1 is King(x)\np2′ is Greedy(y)\np2 is Greedy(x)\nθ is {x/John, y/John}\nq is Evil(x)\nSUBST(θ, q) is Evil(John) .\nIt is easy to show that Generalized Modus Ponens is a sound inference rule. First, we observe\nthat, for any sentence p (whose variables are assumed to be universally quantiﬁed) and for\nany substitution θ,\np |= SUBST(θ, p)\nholds by Universal Instantiation. It holds in particular for a θ that satisﬁes the conditions of\nthe Generalized Modus Ponens rule. Thus, from p1′, . . . , pn′ we can infer\nSUBST(θ, p1′) ∧. . . ∧SUBST(θ, pn′)\nand from the implication p1 ∧. . . ∧pn ⇒q we can infer\nSUBST(θ, p1) ∧. . . ∧SUBST(θ, pn) ⇒SUBST(θ, q) .\nNow, θ in Generalized Modus Ponens is deﬁned so that SUBST(θ, pi′) = SUBST(θ, pi), for\nall i; therefore the ﬁrst of these two sentences matches the premise of the second exactly.\nHence, SUBST(θ, q) follows by Modus Ponens.\nGeneralized Modus Ponens is a lifted version of Modus Ponens—it raises Modus Po-\nLIFTING\nnens from ground (variable-free) propositional logic to ﬁrst-order logic. We will see in the\nrest of this chapter that we can develop lifted versions of the forward chaining, backward\nchaining, and resolution algorithms introduced in Chapter 7. The key advantage of lifted\ninference rules over propositionalization is that they make only those substitutions that are\nrequired to allow particular inferences to proceed.\n9.2.2\nUniﬁcation\nLifted inference rules require ﬁnding substitutions that make different logical expressions\nlook identical. This process is called uniﬁcation and is a key component of all ﬁrst-order\nUNIFICATION\ninference algorithms. The UNIFY algorithm takes two sentences and returns a uniﬁer for\nUNIFIER\nthem if one exists:",
  "look identical. This process is called uniﬁcation and is a key component of all ﬁrst-order\nUNIFICATION\ninference algorithms. The UNIFY algorithm takes two sentences and returns a uniﬁer for\nUNIFIER\nthem if one exists:\nUNIFY(p, q) = θ where SUBST(θ, p) = SUBST(θ, q) .\nLet us look at some examples of how UNIFY should behave. Suppose we have a query\nAskVars(Knows(John, x)): whom does John know? Answers to this query can be found\n2 Generalized Modus Ponens is more general than Modus Ponens (page 249) in the sense that the known facts\nand the premise of the implication need match only up to a substitution, rather than exactly. On the other hand,\nModus Ponens allows any sentence α as the premise, rather than just a conjunction of atomic sentences. Section 9.2.\nUniﬁcation and Lifting\n327\nby ﬁnding all sentences in the knowledge base that unify with Knows(John, x). Here are the\nresults of uniﬁcation with four different sentences that might be in the knowledge base:\nUNIFY(Knows(John, x), Knows(John, Jane)) = {x/Jane}\nUNIFY(Knows(John, x), Knows(y, Bill)) = {x/Bill, y/John}\nUNIFY(Knows(John, x), Knows(y, Mother(y))) = {y/John, x/Mother(John)}\nUNIFY(Knows(John, x), Knows(x, Elizabeth)) = fail .\nThe last uniﬁcation fails because x cannot take on the values John and Elizabeth at the\nsame time. Now, remember that Knows(x, Elizabeth) means “Everyone knows Elizabeth,”\nso we should be able to infer that John knows Elizabeth. The problem arises only because\nthe two sentences happen to use the same variable name, x. The problem can be avoided\nby standardizing apart one of the two sentences being uniﬁed, which means renaming its\nSTANDARDIZING\nAPART\nvariables to avoid name clashes. For example, we can rename x in Knows(x, Elizabeth) to\nx17 (a new variable name) without changing its meaning. Now the uniﬁcation will work:\nUNIFY(Knows(John, x), Knows(x17, Elizabeth)) = {x/Elizabeth, x17/John} .\nExercise 9.12 delves further into the need for standardizing apart.\nThere is one more complication: we said that UNIFY should return a substitution\nthat makes the two arguments look the same. But there could be more than one such uni-\nﬁer. For example, UNIFY(Knows(John, x), Knows(y, z)) could return {y/John, x/z} or\n{y/John, x/John, z/John}. The ﬁrst uniﬁer gives Knows(John, z) as the result of uniﬁ-\ncation, whereas the second gives Knows(John, John). The second result could be obtained\nfrom the ﬁrst by an additional substitution {z/John}; we say that the ﬁrst uniﬁer is more",
  "cation, whereas the second gives Knows(John, John). The second result could be obtained\nfrom the ﬁrst by an additional substitution {z/John}; we say that the ﬁrst uniﬁer is more\ngeneral than the second, because it places fewer restrictions on the values of the variables. It\nturns out that, for every uniﬁable pair of expressions, there is a single most general uniﬁer (or\nMOST GENERAL\nUNIFIER\nMGU) that is unique up to renaming and substitution of variables. (For example, {x/John}\nand {y/John} are considered equivalent, as are {x/John, y/John} and {x/John, y/x}.) In\nthis case it is {y/John, x/z}.\nAn algorithm for computing most general uniﬁers is shown in Figure 9.1. The process\nis simple: recursively explore the two expressions simultaneously “side by side,” building up\na uniﬁer along the way, but failing if two corresponding points in the structures do not match.\nThere is one expensive step: when matching a variable against a complex term, one must\ncheck whether the variable itself occurs inside the term; if it does, the match fails because no\nconsistent uniﬁer can be constructed. For example, S(x) can’t unify with S(S(x)). This so-\ncalled occur check makes the complexity of the entire algorithm quadratic in the size of the\nOCCUR CHECK\nexpressions being uniﬁed. Some systems, including all logic programming systems, simply\nomit the occur check and sometimes make unsound inferences as a result; other systems use\nmore complex algorithms with linear-time complexity.\n9.2.3\nStorage and retrieval\nUnderlying the TELL and ASK functions used to inform and interrogate a knowledge base\nare the more primitive STORE and FETCH functions. STORE(s) stores a sentence s into the\nknowledge base and FETCH(q) returns all uniﬁers such that the query q uniﬁes with some 328\nChapter\n9.\nInference in First-Order Logic\nfunction UNIFY(x,y,θ) returns a substitution to make x and y identical\ninputs: x, a variable, constant, list, or compound expression\ny, a variable, constant, list, or compound expression\nθ, the substitution built up so far (optional, defaults to empty)\nif θ = failure then return failure\nelse if x = y then return θ\nelse if VARIABLE?(x) then return UNIFY-VAR(x,y,θ)\nelse if VARIABLE?(y) then return UNIFY-VAR(y,x,θ)\nelse if COMPOUND?(x) and COMPOUND?(y) then\nreturn UNIFY(x.ARGS,y.ARGS, UNIFY(x.OP,y.OP,θ))\nelse if LIST?(x) and LIST?(y) then\nreturn UNIFY(x.REST,y.REST, UNIFY(x.FIRST,y.FIRST,θ))\nelse return failure\nfunction UNIFY-VAR(var,x,θ) returns a substitution",
  "return UNIFY(x.ARGS,y.ARGS, UNIFY(x.OP,y.OP,θ))\nelse if LIST?(x) and LIST?(y) then\nreturn UNIFY(x.REST,y.REST, UNIFY(x.FIRST,y.FIRST,θ))\nelse return failure\nfunction UNIFY-VAR(var,x,θ) returns a substitution\nif {var/val} ∈θ then return UNIFY(val,x,θ)\nelse if {x/val} ∈θ then return UNIFY(var,val,θ)\nelse if OCCUR-CHECK?(var,x) then return failure\nelse return add {var/x} to θ\nFigure 9.1\nThe uniﬁcation algorithm. The algorithm works by comparing the structures\nof the inputs, element by element. The substitution θ that is the argument to UNIFY is built\nup along the way and is used to make sure that later comparisons are consistent with bindings\nthat were established earlier. In a compound expression such as F(A, B), the OP ﬁeld picks\nout the function symbol F and the ARGS ﬁeld picks out the argument list (A, B).\nsentence in the knowledge base. The problem we used to illustrate uniﬁcation—ﬁnding all\nfacts that unify with Knows(John, x)—is an instance of FETCHing.\nThe simplest way to implement STORE and FETCH is to keep all the facts in one long\nlist and unify each query against every element of the list. Such a process is inefﬁcient, but\nit works, and it’s all you need to understand the rest of the chapter. The remainder of this\nsection outlines ways to make retrieval more efﬁcient; it can be skipped on ﬁrst reading.\nWe can make FETCH more efﬁcient by ensuring that uniﬁcations are attempted only\nwith sentences that have some chance of unifying. For example, there is no point in trying\nto unify Knows(John, x) with Brother(Richard, John). We can avoid such uniﬁcations by\nindexing the facts in the knowledge base. A simple scheme called predicate indexing puts\nINDEXING\nPREDICATE\nINDEXING\nall the Knows facts in one bucket and all the Brother facts in another. The buckets can be\nstored in a hash table for efﬁcient access.\nPredicate indexing is useful when there are many predicate symbols but only a few\nclauses for each symbol. Sometimes, however, a predicate has many clauses. For example,\nsuppose that the tax authorities want to keep track of who employs whom, using a predi-\ncate Employs(x, y). This would be a very large bucket with perhaps millions of employers Section 9.2.\nUniﬁcation and Lifting\n329\nEmploys(x,y)\nEmploys(x,Richard)\nEmploys(IBM,y)\nEmploys(IBM,Richard)\nEmploys(x,y)\nEmploys(John,John)\nEmploys(x,x)\nEmploys(x,John)\nEmploys(John,y)\n(a)\n(b)\nFigure 9.2\n(a) The subsumption lattice whose lowest node is Employs(IBM , Richard).",
  "329\nEmploys(x,y)\nEmploys(x,Richard)\nEmploys(IBM,y)\nEmploys(IBM,Richard)\nEmploys(x,y)\nEmploys(John,John)\nEmploys(x,x)\nEmploys(x,John)\nEmploys(John,y)\n(a)\n(b)\nFigure 9.2\n(a) The subsumption lattice whose lowest node is Employs(IBM , Richard).\n(b) The subsumption lattice for the sentence Employs(John, John).\nand tens of millions of employees. Answering a query such as Employs(x, Richard) with\npredicate indexing would require scanning the entire bucket.\nFor this particular query, it would help if facts were indexed both by predicate and by\nsecond argument, perhaps using a combined hash table key. Then we could simply construct\nthe key from the query and retrieve exactly those facts that unify with the query. For other\nqueries, such as Employs(IBM , y), we would need to have indexed the facts by combining\nthe predicate with the ﬁrst argument. Therefore, facts can be stored under multiple index\nkeys, rendering them instantly accessible to various queries that they might unify with.\nGiven a sentence to be stored, it is possible to construct indices for all possible queries\nthat unify with it. For the fact Employs(IBM , Richard), the queries are\nEmploys(IBM , Richard)\nDoes IBM employ Richard?\nEmploys(x, Richard)\nWho employs Richard?\nEmploys(IBM , y)\nWhom does IBM employ?\nEmploys(x, y)\nWho employs whom?\nThese queries form a subsumption lattice, as shown in Figure 9.2(a). The lattice has some\nSUBSUMPTION\nLATTICE\ninteresting properties. For example, the child of any node in the lattice is obtained from its\nparent by a single substitution; and the “highest” common descendant of any two nodes is\nthe result of applying their most general uniﬁer. The portion of the lattice above any ground\nfact can be constructed systematically (Exercise 9.5). A sentence with repeated constants has\na slightly different lattice, as shown in Figure 9.2(b). Function symbols and variables in the\nsentences to be stored introduce still more interesting lattice structures.\nThe scheme we have described works very well whenever the lattice contains a small\nnumber of nodes. For a predicate with n arguments, however, the lattice contains O(2n)\nnodes. If function symbols are allowed, the number of nodes is also exponential in the size\nof the terms in the sentence to be stored. This can lead to a huge number of indices. At some\npoint, the beneﬁts of indexing are outweighed by the costs of storing and maintaining all",
  "of the terms in the sentence to be stored. This can lead to a huge number of indices. At some\npoint, the beneﬁts of indexing are outweighed by the costs of storing and maintaining all\nthe indices. We can respond by adopting a ﬁxed policy, such as maintaining indices only on\nkeys composed of a predicate plus each argument, or by using an adaptive policy that creates\nindices to meet the demands of the kinds of queries being asked. For most AI systems, the\nnumber of facts to be stored is small enough that efﬁcient indexing is considered a solved\nproblem. For commercial databases, where facts number in the billions, the problem has\nbeen the subject of intensive study and technology development.. 330\nChapter\n9.\nInference in First-Order Logic\n9.3\nFORWARD CHAINING\nA forward-chaining algorithm for propositional deﬁnite clauses was given in Section 7.5.\nThe idea is simple: start with the atomic sentences in the knowledge base and apply Modus\nPonens in the forward direction, adding new atomic sentences, until no further inferences\ncan be made. Here, we explain how the algorithm is applied to ﬁrst-order deﬁnite clauses.\nDeﬁnite clauses such as Situation ⇒Response are especially useful for systems that make\ninferences in response to newly arrived information. Many systems can be deﬁned this way,\nand forward chaining can be implemented very efﬁciently.\n9.3.1\nFirst-order deﬁnite clauses\nFirst-order deﬁnite clauses closely resemble propositional deﬁnite clauses (page 256): they\nare disjunctions of literals of which exactly one is positive. A deﬁnite clause either is atomic\nor is an implication whose antecedent is a conjunction of positive literals and whose conse-\nquent is a single positive literal. The following are ﬁrst-order deﬁnite clauses:\nKing(x) ∧Greedy(x) ⇒Evil(x) .\nKing(John) .\nGreedy(y) .\nUnlike propositional literals, ﬁrst-order literals can include variables, in which case those\nvariables are assumed to be universally quantiﬁed. (Typically, we omit universal quantiﬁers\nwhen writing deﬁnite clauses.) Not every knowledge base can be converted into a set of\ndeﬁnite clauses because of the single-positive-literal restriction, but many can. Consider the\nfollowing problem:\nThe law says that it is a crime for an American to sell weapons to hostile nations. The\ncountry Nono, an enemy of America, has some missiles, and all of its missiles were sold\nto it by Colonel West, who is American.",
  "following problem:\nThe law says that it is a crime for an American to sell weapons to hostile nations. The\ncountry Nono, an enemy of America, has some missiles, and all of its missiles were sold\nto it by Colonel West, who is American.\nWe will prove that West is a criminal. First, we will represent these facts as ﬁrst-order deﬁnite\nclauses. The next section shows how the forward-chaining algorithm solves the problem.\n“. . . it is a crime for an American to sell weapons to hostile nations”:\nAmerican(x) ∧Weapon(y) ∧Sells(x, y, z) ∧Hostile(z) ⇒Criminal(x) .\n(9.3)\n“Nono . . . has some missiles.” The sentence ∃x Owns(Nono, x)∧Missile(x) is transformed\ninto two deﬁnite clauses by Existential Instantiation, introducing a new constant M1:\nOwns(Nono, M1)\n(9.4)\nMissile(M1)\n.\n(9.5)\n“All of its missiles were sold to it by Colonel West”:\nMissile(x) ∧Owns(Nono, x) ⇒Sells(West, x, Nono) .\n(9.6)\nWe will also need to know that missiles are weapons:\nMissile(x) ⇒Weapon(x)\n(9.7) Section 9.3.\nForward Chaining\n331\nand we must know that an enemy of America counts as “hostile”:\nEnemy(x, America) ⇒Hostile(x) .\n(9.8)\n“West, who is American . . .”:\nAmerican(West) .\n(9.9)\n“The country Nono, an enemy of America . . .”:\nEnemy(Nono, America) .\n(9.10)\nThis knowledge base contains no function symbols and is therefore an instance of the class\nof Datalog knowledge bases. Datalog is a language that is restricted to ﬁrst-order deﬁnite\nDATALOG\nclauses with no function symbols. Datalog gets its name because it can represent the type of\nstatements typically made in relational databases. We will see that the absence of function\nsymbols makes inference much easier.\n9.3.2\nA simple forward-chaining algorithm\nThe ﬁrst forward-chaining algorithm we consider is a simple one, shown in Figure 9.3. Start-\ning from the known facts, it triggers all the rules whose premises are satisﬁed, adding their\nconclusions to the known facts. The process repeats until the query is answered (assuming\nthat just one answer is required) or no new facts are added. Notice that a fact is not “new”\nif it is just a renaming of a known fact. One sentence is a renaming of another if they\nRENAMING\nare identical except for the names of the variables. For example, Likes(x, IceCream) and\nLikes(y, IceCream) are renamings of each other because they differ only in the choice of x\nor y; their meanings are identical: everyone likes ice cream.\nWe use our crime problem to illustrate how FOL-FC-ASK works. The implication",
  "Likes(y, IceCream) are renamings of each other because they differ only in the choice of x\nor y; their meanings are identical: everyone likes ice cream.\nWe use our crime problem to illustrate how FOL-FC-ASK works. The implication\nsentences are (9.3), (9.6), (9.7), and (9.8). Two iterations are required:\n• On the ﬁrst iteration, rule (9.3) has unsatisﬁed premises.\nRule (9.6) is satisﬁed with {x/M1}, and Sells(West, M1, Nono) is added.\nRule (9.7) is satisﬁed with {x/M1}, and Weapon(M1) is added.\nRule (9.8) is satisﬁed with {x/Nono}, and Hostile(Nono) is added.\n• On the second iteration, rule (9.3) is satisﬁed with {x/West, y/M1, z/Nono}, and\nCriminal(West) is added.\nFigure 9.4 shows the proof tree that is generated. Notice that no new inferences are possible\nat this point because every sentence that could be concluded by forward chaining is already\ncontained explicitly in the KB. Such a knowledge base is called a ﬁxed point of the inference\nprocess. Fixed points reached by forward chaining with ﬁrst-order deﬁnite clauses are similar\nto those for propositional forward chaining (page 258); the principal difference is that a ﬁrst-\norder ﬁxed point can include universally quantiﬁed atomic sentences.\nFOL-FC-ASK is easy to analyze. First, it is sound, because every inference is just an\napplication of Generalized Modus Ponens, which is sound. Second, it is complete for deﬁnite\nclause knowledge bases; that is, it answers every query whose answers are entailed by any\nknowledge base of deﬁnite clauses. For Datalog knowledge bases, which contain no function\nsymbols, the proof of completeness is fairly easy. We begin by counting the number of 332\nChapter\n9.\nInference in First-Order Logic\nfunction FOL-FC-ASK(KB,α) returns a substitution or false\ninputs: KB, the knowledge base, a set of ﬁrst-order deﬁnite clauses\nα, the query, an atomic sentence\nlocal variables: new, the new sentences inferred on each iteration\nrepeat until new is empty\nnew ←{ }\nfor each rule in KB do\n(p1 ∧. . . ∧pn ⇒q) ←STANDARDIZE-VARIABLES(rule)\nfor each θ such that SUBST(θ,p1 ∧. . . ∧pn) = SUBST(θ,p′\n1 ∧. . . ∧p′\nn)\nfor some p′\n1, . . . , p′\nn in KB\nq′ ←SUBST(θ,q)\nif q′ does not unify with some sentence already in KB or new then\nadd q′ to new\nφ ←UNIFY(q′,α)\nif φ is not fail then return φ\nadd new to KB\nreturn false\nFigure 9.3\nA conceptually straightforward, but very inefﬁcient, forward-chaining algo-\nrithm. On each iteration, it adds to KB all the atomic sentences that can be inferred in one",
  "if φ is not fail then return φ\nadd new to KB\nreturn false\nFigure 9.3\nA conceptually straightforward, but very inefﬁcient, forward-chaining algo-\nrithm. On each iteration, it adds to KB all the atomic sentences that can be inferred in one\nstep from the implication sentences and the atomic sentences already in KB. The function\nSTANDARDIZE-VARIABLES replaces all variables in its arguments with new ones that have\nnot been used before.\nHostile(Nono)\nEnemy(Nono,America)\nOwns(Nono,M1)\nMissile(M1)\nAmerican(West)\nWeapon(M1)\nCriminal(West)\nSells(West,M1,Nono)\nFigure 9.4\nThe proof tree generated by forward chaining on the crime example. The initial\nfacts appear at the bottom level, facts inferred on the ﬁrst iteration in the middle level, and\nfacts inferred on the second iteration at the top level.\npossible facts that can be added, which determines the maximum number of iterations. Let k\nbe the maximum arity (number of arguments) of any predicate, p be the number of predicates,\nand n be the number of constant symbols. Clearly, there can be no more than pnk distinct\nground facts, so after this many iterations the algorithm must have reached a ﬁxed point. Then\nwe can make an argument very similar to the proof of completeness for propositional forward Section 9.3.\nForward Chaining\n333\nchaining. (See page 258.) The details of how to make the transition from propositional to\nﬁrst-order completeness are given for the resolution algorithm in Section 9.5.\nFor general deﬁnite clauses with function symbols, FOL-FC-ASK can generate in-\nﬁnitely many new facts, so we need to be more careful. For the case in which an answer to\nthe query sentence q is entailed, we must appeal to Herbrand’s theorem to establish that the\nalgorithm will ﬁnd a proof. (See Section 9.5 for the resolution case.) If the query has no\nanswer, the algorithm could fail to terminate in some cases. For example, if the knowledge\nbase includes the Peano axioms\nNatNum(0)\n∀n NatNum(n) ⇒NatNum(S(n)) ,\nthen forward chaining adds NatNum(S(0)), NatNum(S(S(0))), NatNum(S(S(S(0)))),\nand so on. This problem is unavoidable in general. As with general ﬁrst-order logic, entail-\nment with deﬁnite clauses is semidecidable.\n9.3.3\nEfﬁcient forward chaining\nThe forward-chaining algorithm in Figure 9.3 is designed for ease of understanding rather\nthan for efﬁciency of operation. There are three possible sources of inefﬁciency. First, the\n“inner loop” of the algorithm involves ﬁnding all possible uniﬁers such that the premise of",
  "than for efﬁciency of operation. There are three possible sources of inefﬁciency. First, the\n“inner loop” of the algorithm involves ﬁnding all possible uniﬁers such that the premise of\na rule uniﬁes with a suitable set of facts in the knowledge base. This is often called pattern\nmatching and can be very expensive. Second, the algorithm rechecks every rule on every\nPATTERN MATCHING\niteration to see whether its premises are satisﬁed, even if very few additions are made to the\nknowledge base on each iteration. Finally, the algorithm might generate many facts that are\nirrelevant to the goal. We address each of these issues in turn.\nMatching rules against known facts\nThe problem of matching the premise of a rule against the facts in the knowledge base might\nseem simple enough. For example, suppose we want to apply the rule\nMissile(x) ⇒Weapon(x) .\nThen we need to ﬁnd all the facts that unify with Missile(x); in a suitably indexed knowledge\nbase, this can be done in constant time per fact. Now consider a rule such as\nMissile(x) ∧Owns(Nono, x) ⇒Sells(West, x, Nono) .\nAgain, we can ﬁnd all the objects owned by Nono in constant time per object; then, for each\nobject, we could check whether it is a missile. If the knowledge base contains many objects\nowned by Nono and very few missiles, however, it would be better to ﬁnd all the missiles ﬁrst\nand then check whether they are owned by Nono. This is the conjunct ordering problem:\nCONJUNCT\nORDERING\nﬁnd an ordering to solve the conjuncts of the rule premise so that the total cost is minimized.\nIt turns out that ﬁnding the optimal ordering is NP-hard, but good heuristics are available.\nFor example, the minimum-remaining-values (MRV) heuristic used for CSPs in Chapter 6\nwould suggest ordering the conjuncts to look for missiles ﬁrst if fewer missiles than objects\nare owned by Nono. 334\nChapter\n9.\nInference in First-Order Logic\nWA\nNT\nSA\nQ\nNSW\nV\nT\nDiﬀ(wa, nt) ∧Diﬀ(wa, sa) ∧\nDiﬀ(nt, q) ∧Diﬀ(nt, sa) ∧\nDiﬀ(q, nsw) ∧Diﬀ(q, sa) ∧\nDiﬀ(nsw, v) ∧Diﬀ(nsw, sa) ∧\nDiﬀ(v, sa) ⇒Colorable()\nDiﬀ(Red, Blue)\nDiﬀ(Red, Green)\nDiﬀ(Green, Red) Diﬀ(Green, Blue)\nDiﬀ(Blue, Red)\nDiﬀ(Blue, Green)\n(a)\n(b)\nFigure 9.5\n(a) Constraint graph for coloring the map of Australia. (b) The map-coloring\nCSP expressed as a single deﬁnite clause. Each map region is represented as a variable whose\nvalue can be one of the constants Red, Green or Blue.\nThe connection between pattern matching and constraint satisfaction is actually very",
  "CSP expressed as a single deﬁnite clause. Each map region is represented as a variable whose\nvalue can be one of the constants Red, Green or Blue.\nThe connection between pattern matching and constraint satisfaction is actually very\nclose. We can view each conjunct as a constraint on the variables that it contains—for ex-\nample, Missile(x) is a unary constraint on x. Extending this idea, we can express every\nﬁnite-domain CSP as a single deﬁnite clause together with some associated ground facts.\nConsider the map-coloring problem from Figure 6.1, shown again in Figure 9.5(a). An equiv-\nalent formulation as a single deﬁnite clause is given in Figure 9.5(b). Clearly, the conclusion\nColorable() can be inferred only if the CSP has a solution. Because CSPs in general include\n3-SAT problems as special cases, we can conclude that matching a deﬁnite clause against a\nset of facts is NP-hard.\nIt might seem rather depressing that forward chaining has an NP-hard matching problem\nin its inner loop. There are three ways to cheer ourselves up:\n• We can remind ourselves that most rules in real-world knowledge bases are small and\nsimple (like the rules in our crime example) rather than large and complex (like the\nCSP formulation in Figure 9.5). It is common in the database world to assume that\nboth the sizes of rules and the arities of predicates are bounded by a constant and to\nworry only about data complexity—that is, the complexity of inference as a function\nDATA COMPLEXITY\nof the number of ground facts in the knowledge base. It is easy to show that the data\ncomplexity of forward chaining is polynomial.\n• We can consider subclasses of rules for which matching is efﬁcient. Essentially every\nDatalog clause can be viewed as deﬁning a CSP, so matching will be tractable just\nwhen the corresponding CSP is tractable. Chapter 6 describes several tractable families\nof CSPs. For example, if the constraint graph (the graph whose nodes are variables\nand whose links are constraints) forms a tree, then the CSP can be solved in linear\ntime. Exactly the same result holds for rule matching. For instance, if we remove South Section 9.3.\nForward Chaining\n335\nAustralia from the map in Figure 9.5, the resulting clause is\nDiﬀ(wa, nt) ∧Diﬀ(nt, q) ∧Diﬀ(q, nsw) ∧Diﬀ(nsw, v) ⇒Colorable()\nwhich corresponds to the reduced CSP shown in Figure 6.12 on page 224. Algorithms\nfor solving tree-structured CSPs can be applied directly to the problem of rule matching.",
  "Diﬀ(wa, nt) ∧Diﬀ(nt, q) ∧Diﬀ(q, nsw) ∧Diﬀ(nsw, v) ⇒Colorable()\nwhich corresponds to the reduced CSP shown in Figure 6.12 on page 224. Algorithms\nfor solving tree-structured CSPs can be applied directly to the problem of rule matching.\n• We can try to to eliminate redundant rule-matching attempts in the forward-chaining\nalgorithm, as described next.\nIncremental forward chaining\nWhen we showed how forward chaining works on the crime example, we cheated; in partic-\nular, we omitted some of the rule matching done by the algorithm shown in Figure 9.3. For\nexample, on the second iteration, the rule\nMissile(x) ⇒Weapon(x)\nmatches against Missile(M1) (again), and of course the conclusion Weapon(M1) is already\nknown so nothing happens. Such redundant rule matching can be avoided if we make the\nfollowing observation: Every new fact inferred on iteration t must be derived from at least\none new fact inferred on iteration t −1.\nThis is true because any inference that does not\nrequire a new fact from iteration t −1 could have been done at iteration t −1 already.\nThis observation leads naturally to an incremental forward-chaining algorithm where,\nat iteration t, we check a rule only if its premise includes a conjunct pi that uniﬁes with a fact\np′\ni newly inferred at iteration t −1. The rule-matching step then ﬁxes pi to match with p′\ni, but\nallows the other conjuncts of the rule to match with facts from any previous iteration. This\nalgorithm generates exactly the same facts at each iteration as the algorithm in Figure 9.3, but\nis much more efﬁcient.\nWith suitable indexing, it is easy to identify all the rules that can be triggered by any\ngiven fact, and indeed many real systems operate in an “update” mode wherein forward chain-\ning occurs in response to each new fact that is TELLed to the system. Inferences cascade\nthrough the set of rules until the ﬁxed point is reached, and then the process begins again for\nthe next new fact.\nTypically, only a small fraction of the rules in the knowledge base are actually triggered\nby the addition of a given fact. This means that a great deal of redundant work is done in\nrepeatedly constructing partial matches that have some unsatisﬁed premises. Our crime ex-\nample is rather too small to show this effectively, but notice that a partial match is constructed\non the ﬁrst iteration between the rule\nAmerican(x) ∧Weapon(y) ∧Sells(x, y, z) ∧Hostile(z) ⇒Criminal(x)",
  "ample is rather too small to show this effectively, but notice that a partial match is constructed\non the ﬁrst iteration between the rule\nAmerican(x) ∧Weapon(y) ∧Sells(x, y, z) ∧Hostile(z) ⇒Criminal(x)\nand the fact American(West). This partial match is then discarded and rebuilt on the second\niteration (when the rule succeeds). It would be better to retain and gradually complete the\npartial matches as new facts arrive, rather than discarding them.\nThe rete algorithm3 was the ﬁrst to address this problem. The algorithm preprocesses\nRETE\nthe set of rules in the knowledge base to construct a sort of dataﬂow network in which each\n3 Rete is Latin for net. The English pronunciation rhymes with treaty. 336\nChapter\n9.\nInference in First-Order Logic\nnode is a literal from a rule premise. Variable bindings ﬂow through the network and are\nﬁltered out when they fail to match a literal. If two literals in a rule share a variable—for\nexample, Sells(x, y, z) ∧Hostile(z) in the crime example—then the bindings from each\nliteral are ﬁltered through an equality node. A variable binding reaching a node for an n-\nary literal such as Sells(x, y, z) might have to wait for bindings for the other variables to be\nestablished before the process can continue. At any given point, the state of a rete network\ncaptures all the partial matches of the rules, avoiding a great deal of recomputation.\nRete networks, and various improvements thereon, have been a key component of so-\ncalled production systems, which were among the earliest forward-chaining systems in\nPRODUCTION\nSYSTEM\nwidespread use.4 The XCON system (originally called R1; McDermott, 1982) was built\nwith a production-system architecture. XCON contained several thousand rules for designing\nconﬁgurations of computer components for customers of the Digital Equipment Corporation.\nIt was one of the ﬁrst clear commercial successes in the emerging ﬁeld of expert systems.\nMany other similar systems have been built with the same underlying technology, which has\nbeen implemented in the general-purpose language OPS-5.\nProduction systems are also popular in cognitive architectures—that is, models of hu-\nCOGNITIVE\nARCHITECTURES\nman reasoning—such as ACT (Anderson, 1983) and SOAR (Laird et al., 1987). In such sys-\ntems, the “working memory” of the system models human short-term memory, and the pro-\nductions are part of long-term memory. On each cycle of operation, productions are matched",
  "tems, the “working memory” of the system models human short-term memory, and the pro-\nductions are part of long-term memory. On each cycle of operation, productions are matched\nagainst the working memory of facts. A production whose conditions are satisﬁed can add or\ndelete facts in working memory. In contrast to the typical situation in databases, production\nsystems often have many rules and relatively few facts. With suitably optimized matching\ntechnology, some modern systems can operate in real time with tens of millions of rules.\nIrrelevant facts\nThe ﬁnal source of inefﬁciency in forward chaining appears to be intrinsic to the approach\nand also arises in the propositional context. Forward chaining makes all allowable inferences\nbased on the known facts, even if they are irrelevant to the goal at hand. In our crime example,\nthere were no rules capable of drawing irrelevant conclusions, so the lack of directedness was\nnot a problem. In other cases (e.g., if many rules describe the eating habits of Americans and\nthe prices of missiles), FOL-FC-ASK will generate many irrelevant conclusions.\nOne way to avoid drawing irrelevant conclusions is to use backward chaining, as de-\nscribed in Section 9.4. Another solution is to restrict forward chaining to a selected subset of\nrules, as in PL-FC-ENTAILS? (page 258). A third approach has emerged in the ﬁeld of de-\nductive databases, which are large-scale databases, like relational databases, but which use\nDEDUCTIVE\nDATABASES\nforward chaining as the standard inference tool rather than SQL queries. The idea is to rewrite\nthe rule set, using information from the goal, so that only relevant variable bindings—those\nbelonging to a so-called magic set—are considered during forward inference. For example, if\nMAGIC SET\nthe goal is Criminal(West), the rule that concludes Criminal(x) will be rewritten to include\nan extra conjunct that constrains the value of x:\nMagic(x) ∧American(x) ∧Weapon(y) ∧Sells(x, y, z) ∧Hostile(z) ⇒Criminal(x) .\n4 The word production in production systems denotes a condition–action rule. Section 9.4.\nBackward Chaining\n337\nThe fact Magic(West) is also added to the KB. In this way, even if the knowledge base\ncontains facts about millions of Americans, only Colonel West will be considered during the\nforward inference process. The complete process for deﬁning magic sets and rewriting the\nknowledge base is too complex to go into here, but the basic idea is to perform a sort of",
  "forward inference process. The complete process for deﬁning magic sets and rewriting the\nknowledge base is too complex to go into here, but the basic idea is to perform a sort of\n“generic” backward inference from the goal in order to work out which variable bindings\nneed to be constrained. The magic sets approach can therefore be thought of as a kind of\nhybrid between forward inference and backward preprocessing.\n9.4\nBACKWARD CHAINING\nThe second major family of logical inference algorithms uses the backward chaining ap-\nproach introduced in Section 7.5 for deﬁnite clauses. These algorithms work backward from\nthe goal, chaining through rules to ﬁnd known facts that support the proof. We describe\nthe basic algorithm, and then we describe how it is used in logic programming, which is the\nmost widely used form of automated reasoning. We also see that backward chaining has some\ndisadvantages compared with forward chaining, and we look at ways to overcome them. Fi-\nnally, we look at the close connection between logic programming and constraint satisfaction\nproblems.\n9.4.1\nA backward-chaining algorithm\nFigure 9.6 shows a backward-chaining algorithm for deﬁnite clauses. FOL-BC-ASK(KB,\ngoal) will be proved if the knowledge base contains a clause of the form lhs ⇒goal, where\nlhs (left-hand side) is a list of conjuncts. An atomic fact like American(West) is considered\nas a clause whose lhs is the empty list. Now a query that contains variables might be proved\nin multiple ways. For example, the query Person(x) could be proved with the substitution\n{x/John} as well as with {x/Richard}. So we implement FOL-BC-ASK as a generator—\nGENERATOR\na function that returns multiple times, each time giving one possible result.\nBackward chaining is a kind of AND/OR search—the OR part because the goal query\ncan be proved by any rule in the knowledge base, and the AND part because all the conjuncts\nin the lhs of a clause must be proved. FOL-BC-OR works by fetching all clauses that might\nunify with the goal, standardizing the variables in the clause to be brand-new variables, and\nthen, if the rhs of the clause does indeed unify with the goal, proving every conjunct in the\nlhs, using FOL-BC-AND. That function in turn works by proving each of the conjuncts in\nturn, keeping track of the accumulated substitution as we go. Figure 9.7 is the proof tree for\nderiving Criminal(West) from sentences (9.3) through (9.10).",
  "lhs, using FOL-BC-AND. That function in turn works by proving each of the conjuncts in\nturn, keeping track of the accumulated substitution as we go. Figure 9.7 is the proof tree for\nderiving Criminal(West) from sentences (9.3) through (9.10).\nBackward chaining, as we have written it, is clearly a depth-ﬁrst search algorithm.\nThis means that its space requirements are linear in the size of the proof (neglecting, for\nnow, the space required to accumulate the solutions). It also means that backward chaining\n(unlike forward chaining) suffers from problems with repeated states and incompleteness. We\nwill discuss these problems and some potential solutions, but ﬁrst we show how backward\nchaining is used in logic programming systems. 338\nChapter\n9.\nInference in First-Order Logic\nfunction FOL-BC-ASK(KB,query) returns a generator of substitutions\nreturn FOL-BC-OR(KB,query,{ })\ngenerator FOL-BC-OR(KB,goal,θ) yields a substitution\nfor each rule (lhs ⇒rhs) in FETCH-RULES-FOR-GOAL(KB, goal) do\n(lhs, rhs) ←STANDARDIZE-VARIABLES((lhs, rhs))\nfor each θ′ in FOL-BC-AND(KB,lhs, UNIFY(rhs, goal, θ)) do\nyield θ′\ngenerator FOL-BC-AND(KB,goals,θ) yields a substitution\nif θ = failure then return\nelse if LENGTH(goals) = 0 then yield θ\nelse do\nﬁrst,rest ←FIRST(goals), REST(goals)\nfor each θ′ in FOL-BC-OR(KB, SUBST(θ, ﬁrst), θ) do\nfor each θ′′ in FOL-BC-AND(KB,rest,θ′) do\nyield θ′′\nFigure 9.6\nA simple backward-chaining algorithm for ﬁrst-order knowledge bases.\nHostile(Nono)\nEnemy(Nono,America)\nOwns(Nono,M1)\nMissile(M1)\nCriminal(West)\nMissile(y)\nWeapon(y)\nSells(West,M1,z)\nAmerican(West)\n{y/M1}\n{ }\n{ }\n{ }\n {z/Nono}\n{ }\nFigure 9.7\nProof tree constructed by backward chaining to prove that West is a criminal.\nThe tree should be read depth ﬁrst, left to right. To prove Criminal(West), we have to prove\nthe four conjuncts below it. Some of these are in the knowledge base, and others require\nfurther backward chaining. Bindings for each successful uniﬁcation are shown next to the\ncorresponding subgoal. Note that once one subgoal in a conjunction succeeds, its substitution\nis applied to subsequent subgoals. Thus, by the time FOL-BC-ASK gets to the last conjunct,\noriginally Hostile(z), z is already bound to Nono. Section 9.4.\nBackward Chaining\n339\n9.4.2\nLogic programming\nLogic programming is a technology that comes fairly close to embodying the declarative\nideal described in Chapter 7: that systems should be constructed by expressing knowledge in",
  "Backward Chaining\n339\n9.4.2\nLogic programming\nLogic programming is a technology that comes fairly close to embodying the declarative\nideal described in Chapter 7: that systems should be constructed by expressing knowledge in\na formal language and that problems should be solved by running inference processes on that\nknowledge. The ideal is summed up in Robert Kowalski’s equation,\nAlgorithm = Logic + Control .\nProlog is the most widely used logic programming language. It is used primarily as a rapid-\nPROLOG\nprototyping language and for symbol-manipulation tasks such as writing compilers (Van Roy,\n1990) and parsing natural language (Pereira and Warren, 1980). Many expert systems have\nbeen written in Prolog for legal, medical, ﬁnancial, and other domains.\nProlog programs are sets of deﬁnite clauses written in a notation somewhat different\nfrom standard ﬁrst-order logic. Prolog uses uppercase letters for variables and lowercase for\nconstants—the opposite of our convention for logic. Commas separate conjuncts in a clause,\nand the clause is written “backwards” from what we are used to; instead of A ∧B ⇒C in\nProlog we have C :- A, B. Here is a typical example:\ncriminal(X) :- american(X), weapon(Y), sells(X,Y,Z), hostile(Z).\nThe notation [E|L] denotes a list whose ﬁrst element is E and whose rest is L. Here is a\nProlog program for append(X,Y,Z), which succeeds if list Z is the result of appending\nlists X and Y:\nappend([],Y,Y).\nappend([A|X],Y,[A|Z]) :- append(X,Y,Z).\nIn English, we can read these clauses as (1) appending an empty list with a list Y produces\nthe same list Y and (2) [A|Z] is the result of appending [A|X] onto Y, provided that Z is\nthe result of appending X onto Y. In most high-level languages we can write a similar recur-\nsive function that describes how to append two lists. The Prolog deﬁnition is actually much\nmore powerful, however, because it describes a relation that holds among three arguments,\nrather than a function computed from two arguments. For example, we can ask the query\nappend(X,Y,[1,2]): what two lists can be appended to give [1,2]? We get back the\nsolutions\nX=[]\nY=[1,2];\nX=[1]\nY=[2];\nX=[1,2] Y=[]\nThe execution of Prolog programs is done through depth-ﬁrst backward chaining, where\nclauses are tried in the order in which they are written in the knowledge base. Some aspects\nof Prolog fall outside standard logical inference:\n• Prolog uses the database semantics of Section 8.2.8 rather than ﬁrst-order semantics,",
  "clauses are tried in the order in which they are written in the knowledge base. Some aspects\nof Prolog fall outside standard logical inference:\n• Prolog uses the database semantics of Section 8.2.8 rather than ﬁrst-order semantics,\nand this is apparent in its treatment of equality and negation (see Section 9.4.5).\n• There is a set of built-in functions for arithmetic. Literals using these function symbols\nare “proved” by executing code rather than doing further inference. For example, the 340\nChapter\n9.\nInference in First-Order Logic\ngoal “X is 4+3” succeeds with X bound to 7. On the other hand, the goal “5 is X+Y”\nfails, because the built-in functions do not do arbitrary equation solving.5\n• There are built-in predicates that have side effects when executed. These include input–\noutput predicates and the assert/retract predicates for modifying the knowledge\nbase. Such predicates have no counterpart in logic and can produce confusing results—\nfor example, if facts are asserted in a branch of the proof tree that eventually fails.\n• The occur check is omitted from Prolog’s uniﬁcation algorithm. This means that some\nunsound inferences can be made; these are almost never a problem in practice.\n• Prolog uses depth-ﬁrst backward-chaining search with no checks for inﬁnite recursion.\nThis makes it very fast when given the right set of axioms, but incomplete when given\nthe wrong ones.\nProlog’s design represents a compromise between declarativeness and execution efﬁciency—\ninasmuch as efﬁciency was understood at the time Prolog was designed.\n9.4.3\nEfﬁcient implementation of logic programs\nThe execution of a Prolog program can happen in two modes: interpreted and compiled.\nInterpretation essentially amounts to running the FOL-BC-ASK algorithm from Figure 9.6,\nwith the program as the knowledge base. We say “essentially” because Prolog interpreters\ncontain a variety of improvements designed to maximize speed. Here we consider only two.\nFirst, our implementation had to explicitly manage the iteration over possible results\ngenerated by each of the subfunctions.\nProlog interpreters have a global data structure,\na stack of choice points, to keep track of the multiple possibilities that we considered in\nCHOICE POINT\nFOL-BC-OR. This global stack is more efﬁcient, and it makes debugging easier, because\nthe debugger can move up and down the stack.\nSecond, our simple implementation of FOL-BC-ASK spends a good deal of time gener-",
  "CHOICE POINT\nFOL-BC-OR. This global stack is more efﬁcient, and it makes debugging easier, because\nthe debugger can move up and down the stack.\nSecond, our simple implementation of FOL-BC-ASK spends a good deal of time gener-\nating substitutions. Instead of explicitly constructing substitutions, Prolog has logic variables\nthat remember their current binding. At any point in time, every variable in the program ei-\nther is unbound or is bound to some value. Together, these variables and values implicitly\ndeﬁne the substitution for the current branch of the proof. Extending the path can only add\nnew variable bindings, because an attempt to add a different binding for an already bound\nvariable results in a failure of uniﬁcation. When a path in the search fails, Prolog will back\nup to a previous choice point, and then it might have to unbind some variables. This is done\nby keeping track of all the variables that have been bound in a stack called the trail. As each\nTRAIL\nnew variable is bound by UNIFY-VAR, the variable is pushed onto the trail. When a goal fails\nand it is time to back up to a previous choice point, each of the variables is unbound as it is\nremoved from the trail.\nEven the most efﬁcient Prolog interpreters require several thousand machine instruc-\ntions per inference step because of the cost of index lookup, uniﬁcation, and building the\nrecursive call stack. In effect, the interpreter always behaves as if it has never seen the pro-\ngram before; for example, it has to ﬁnd clauses that match the goal. A compiled Prolog\n5 Note that if the Peano axioms are provided, such goals can be solved by inference within a Prolog program. Section 9.4.\nBackward Chaining\n341\nprocedure APPEND(ax,y,az,continuation)\ntrail ←GLOBAL-TRAIL-POINTER()\nif ax = [ ] and UNIFY(y,az) then CALL(continuation)\nRESET-TRAIL(trail)\na, x, z ←NEW-VARIABLE(), NEW-VARIABLE(), NEW-VARIABLE()\nif UNIFY(ax,[a | x]) and UNIFY(az,[a | z]) then APPEND(x,y,z,continuation)\nFigure 9.8\nPseudocode representing the result of compiling the Append predicate. The\nfunction NEW-VARIABLE returns a new variable, distinct from all other variables used so far.\nThe procedure CALL(continuation) continues execution with the speciﬁed continuation.\nprogram, on the other hand, is an inference procedure for a speciﬁc set of clauses, so it knows\nwhat clauses match the goal. Prolog basically generates a miniature theorem prover for each",
  "program, on the other hand, is an inference procedure for a speciﬁc set of clauses, so it knows\nwhat clauses match the goal. Prolog basically generates a miniature theorem prover for each\ndifferent predicate, thereby eliminating much of the overhead of interpretation. It is also pos-\nsible to open-code the uniﬁcation routine for each different call, thereby avoiding explicit\nOPEN-CODE\nanalysis of term structure. (For details of open-coded uniﬁcation, see Warren et al. (1977).)\nThe instruction sets of today’s computers give a poor match with Prolog’s semantics,\nso most Prolog compilers compile into an intermediate language rather than directly into ma-\nchine language. The most popular intermediate language is the Warren Abstract Machine,\nor WAM, named after David H. D. Warren, one of the implementers of the ﬁrst Prolog com-\npiler. The WAM is an abstract instruction set that is suitable for Prolog and can be either\ninterpreted or translated into machine language. Other compilers translate Prolog into a high-\nlevel language such as Lisp or C and then use that language’s compiler to translate to machine\nlanguage. For example, the deﬁnition of the Append predicate can be compiled into the code\nshown in Figure 9.8. Several points are worth mentioning:\n• Rather than having to search the knowledge base for Append clauses, the clauses be-\ncome a procedure and the inferences are carried out simply by calling the procedure.\n• As described earlier, the current variable bindings are kept on a trail. The ﬁrst step of the\nprocedure saves the current state of the trail, so that it can be restored by RESET-TRAIL\nif the ﬁrst clause fails. This will undo any bindings generated by the ﬁrst call to UNIFY.\n• The trickiest part is the use of continuations to implement choice points. You can think\nCONTINUATION\nof a continuation as packaging up a procedure and a list of arguments that together\ndeﬁne what should be done next whenever the current goal succeeds. It would not\ndo just to return from a procedure like APPEND when the goal succeeds, because it\ncould succeed in several ways, and each of them has to be explored. The continuation\nargument solves this problem because it can be called each time the goal succeeds. In\nthe APPEND code, if the ﬁrst argument is empty and the second argument uniﬁes with\nthe third, then the APPEND predicate has succeeded. We then CALL the continuation,\nwith the appropriate bindings on the trail, to do whatever should be done next. For",
  "the APPEND code, if the ﬁrst argument is empty and the second argument uniﬁes with\nthe third, then the APPEND predicate has succeeded. We then CALL the continuation,\nwith the appropriate bindings on the trail, to do whatever should be done next. For\nexample, if the call to APPEND were at the top level, the continuation would print the\nbindings of the variables. 342\nChapter\n9.\nInference in First-Order Logic\nBefore Warren’s work on the compilation of inference in Prolog, logic programming was\ntoo slow for general use. Compilers by Warren and others allowed Prolog code to achieve\nspeeds that are competitive with C on a variety of standard benchmarks (Van Roy, 1990).\nOf course, the fact that one can write a planner or natural language parser in a few dozen\nlines of Prolog makes it somewhat more desirable than C for prototyping most small-scale AI\nresearch projects.\nParallelization can also provide substantial speedup. There are two principal sources of\nparallelism. The ﬁrst, called OR-parallelism, comes from the possibility of a goal unifying\nOR-PARALLELISM\nwith many different clauses in the knowledge base. Each gives rise to an independent branch\nin the search space that can lead to a potential solution, and all such branches can be solved\nin parallel. The second, called AND-parallelism, comes from the possibility of solving\nAND-PARALLELISM\neach conjunct in the body of an implication in parallel. AND-parallelism is more difﬁcult to\nachieve, because solutions for the whole conjunction require consistent bindings for all the\nvariables. Each conjunctive branch must communicate with the other branches to ensure a\nglobal solution.\n9.4.4\nRedundant inference and inﬁnite loops\nWe now turn to the Achilles heel of Prolog: the mismatch between depth-ﬁrst search and\nsearch trees that include repeated states and inﬁnite paths. Consider the following logic pro-\ngram that decides if a path exists between two points on a directed graph:\npath(X,Z) :- link(X,Z).\npath(X,Z) :- path(X,Y), link(Y,Z).\nA simple three-node graph, described by the facts link(a,b) and link(b,c), is shown\nin Figure 9.9(a). With this program, the query path(a,c) generates the proof tree shown\nin Figure 9.10(a). On the other hand, if we put the two clauses in the order\npath(X,Z) :- path(X,Y), link(Y,Z).\npath(X,Z) :- link(X,Z).\nthen Prolog follows the inﬁnite path shown in Figure 9.10(b). Prolog is therefore incomplete",
  "in Figure 9.10(a). On the other hand, if we put the two clauses in the order\npath(X,Z) :- path(X,Y), link(Y,Z).\npath(X,Z) :- link(X,Z).\nthen Prolog follows the inﬁnite path shown in Figure 9.10(b). Prolog is therefore incomplete\nas a theorem prover for deﬁnite clauses—even for Datalog programs, as this example shows—\nbecause, for some knowledge bases, it fails to prove sentences that are entailed. Notice that\nforward chaining does not suffer from this problem: once path(a,b), path(b,c), and\npath(a,c) are inferred, forward chaining halts.\nDepth-ﬁrst backward chaining also has problems with redundant computations. For\nexample, when ﬁnding a path from A1 to J4 in Figure 9.9(b), Prolog performs 877 inferences,\nmost of which involve ﬁnding all possible paths to nodes from which the goal is unreachable.\nThis is similar to the repeated-state problem discussed in Chapter 3. The total amount of\ninference can be exponential in the number of ground facts that are generated. If we apply\nforward chaining instead, at most n2 path(X,Y) facts can be generated linking n nodes.\nFor the problem in Figure 9.9(b), only 62 inferences are needed.\nForward chaining on graph search problems is an example of dynamic programming,\nDYNAMIC\nPROGRAMMING\nin which the solutions to subproblems are constructed incrementally from those of smaller Section 9.4.\nBackward Chaining\n343\n(a)\n(b)\nA\nB\nC\nA1\nJ4\nFigure 9.9\n(a) Finding a path from A to C can lead Prolog into an inﬁnite loop. (b) A\ngraph in which each node is connected to two random successors in the next layer. Finding a\npath from A1 to J4 requires 877 inferences.\npath(a,c)\nfail\n{\n}\n/\nY b\n{ }\nlink(a,c)\npath(a,Y)\nlink(a,Y)\nlink(b,c)\npath(a,c)\npath(a,Y)\nlink(Y,c)\npath(a,Y’)\nlink(Y’,Y)\n(a)\n(b)\nFigure 9.10\n(a) Proof that a path exists from A to C. (b) Inﬁnite proof tree generated\nwhen the clauses are in the “wrong” order.\nsubproblems and are cached to avoid recomputation. We can obtain a similar effect in a\nbackward chaining system using memoization—that is, caching solutions to subgoals as\nthey are found and then reusing those solutions when the subgoal recurs, rather than repeat-\ning the previous computation. This is the approach taken by tabled logic programming sys-\nTABLED LOGIC\nPROGRAMMING\ntems, which use efﬁcient storage and retrieval mechanisms to perform memoization. Tabled\nlogic programming combines the goal-directedness of backward chaining with the dynamic-",
  "TABLED LOGIC\nPROGRAMMING\ntems, which use efﬁcient storage and retrieval mechanisms to perform memoization. Tabled\nlogic programming combines the goal-directedness of backward chaining with the dynamic-\nprogramming efﬁciency of forward chaining. It is also complete for Datalog knowledge\nbases, which means that the programmer need worry less about inﬁnite loops. (It is still pos-\nsible to get an inﬁnite loop with predicates like father(X,Y) that refer to a potentially\nunbounded number of objects.)\n9.4.5\nDatabase semantics of Prolog\nProlog uses database semantics, as discussed in Section 8.2.8. The unique names assumption\nsays that every Prolog constant and every ground term refers to a distinct object, and the\nclosed world assumption says that the only sentences that are true are those that are entailed 344\nChapter\n9.\nInference in First-Order Logic\nby the knowledge base. There is no way to assert that a sentence is false in Prolog. This makes\nProlog less expressive than ﬁrst-order logic, but it is part of what makes Prolog more efﬁcient\nand more concise. Consider the following Prolog assertions about some course offerings:\nCourse(CS, 101), Course(CS, 102), Course(CS, 106), Course(EE, 101). (9.11)\nUnder the unique names assumption, CS and EE are different (as are 101, 102, and 106),\nso this means that there are four distinct courses. Under the closed-world assumption there\nare no other courses, so there are exactly four courses. But if these were assertions in FOL\nrather than in Prolog, then all we could say is that there are somewhere between one and\ninﬁnity courses. That’s because the assertions (in FOL) do not deny the possibility that other\nunmentioned courses are also offered, nor do they say that the courses mentioned are different\nfrom each other. If we wanted to translate Equation (9.11) into FOL, we would get this:\nCourse(d, n)\n⇔\n(d = CS ∧n = 101) ∨(d = CS ∧n = 102)\n∨(d = CS ∧n = 106) ∨(d = EE ∧n = 101) .\n(9.12)\nThis is called the completion of Equation (9.11). It expresses in FOL the idea that there are\nCOMPLETION\nat most four courses. To express in FOL the idea that there are at least four courses, we need\nto write the completion of the equality predicate:\nx = y\n⇔\n(x = CS ∧y = CS) ∨(x = EE ∧y = EE) ∨(x = 101 ∧y = 101)\n∨(x = 102 ∧y = 102) ∨(x = 106 ∧y = 106) .\nThe completion is useful for understanding database semantics, but for practical purposes, if\nyour problem can be described with database semantics, it is more efﬁcient to reason with",
  "∨(x = 102 ∧y = 102) ∨(x = 106 ∧y = 106) .\nThe completion is useful for understanding database semantics, but for practical purposes, if\nyour problem can be described with database semantics, it is more efﬁcient to reason with\nProlog or some other database semantics system, rather than translating into FOL and rea-\nsoning with a full FOL theorem prover.\n9.4.6\nConstraint logic programming\nIn our discussion of forward chaining (Section 9.3), we showed how constraint satisfaction\nproblems (CSPs) can be encoded as deﬁnite clauses. Standard Prolog solves such problems\nin exactly the same way as the backtracking algorithm given in Figure 6.5.\nBecause backtracking enumerates the domains of the variables, it works only for ﬁnite-\ndomain CSPs. In Prolog terms, there must be a ﬁnite number of solutions for any goal\nwith unbound variables. (For example, the goal diff(Q,SA), which says that Queensland\nand South Australia must be different colors, has six solutions if three colors are allowed.)\nInﬁnite-domain CSPs—for example, with integer or real-valued variables—require quite dif-\nferent algorithms, such as bounds propagation or linear programming.\nConsider the following example. We deﬁne triangle(X,Y,Z) as a predicate that\nholds if the three arguments are numbers that satisfy the triangle inequality:\ntriangle(X,Y,Z) :-\nX>0, Y>0, Z>0, X+Y>=Z, Y+Z>=X, X+Z>=Y.\nIf we ask Prolog the query triangle(3,4,5), it succeeds. On the other hand, if we\nask triangle(3,4,Z), no solution will be found, because the subgoal Z>=0 cannot be\nhandled by Prolog; we can’t compare an unbound value to 0. Section 9.5.\nResolution\n345\nConstraint logic programming (CLP) allows variables to be constrained rather than\nCONSTRAINT LOGIC\nPROGRAMMING\nbound. A CLP solution is the most speciﬁc set of constraints on the query variables that can\nbe derived from the knowledge base. For example, the solution to the triangle(3,4,Z)\nquery is the constraint 7 >= Z >= 1. Standard logic programs are just a special case of\nCLP in which the solution constraints must be equality constraints—that is, bindings.\nCLP systems incorporate various constraint-solving algorithms for the constraints al-\nlowed in the language. For example, a system that allows linear inequalities on real-valued\nvariables might include a linear programming algorithm for solving those constraints. CLP\nsystems also adopt a much more ﬂexible approach to solving standard logic programming",
  "variables might include a linear programming algorithm for solving those constraints. CLP\nsystems also adopt a much more ﬂexible approach to solving standard logic programming\nqueries. For example, instead of depth-ﬁrst, left-to-right backtracking, they might use any of\nthe more efﬁcient algorithms discussed in Chapter 6, including heuristic conjunct ordering,\nbackjumping, cutset conditioning, and so on. CLP systems therefore combine elements of\nconstraint satisfaction algorithms, logic programming, and deductive databases.\nSeveral systems that allow the programmer more control over the search order for in-\nference have been deﬁned. The MRS language (Genesereth and Smith, 1981; Russell, 1985)\nallows the programmer to write metarules to determine which conjuncts are tried ﬁrst. The\nMETARULE\nuser could write a rule saying that the goal with the fewest variables should be tried ﬁrst or\ncould write domain-speciﬁc rules for particular predicates.\n9.5\nRESOLUTION\nThe last of our three families of logical systems is based on resolution. We saw on page 250\nthat propositional resolution using refutation is a complete inference procedure for proposi-\ntional logic. In this section, we describe how to extend resolution to ﬁrst-order logic.\n9.5.1\nConjunctive normal form for ﬁrst-order logic\nAs in the propositional case, ﬁrst-order resolution requires that sentences be in conjunctive\nnormal form (CNF)—that is, a conjunction of clauses, where each clause is a disjunction of\nliterals.6 Literals can contain variables, which are assumed to be universally quantiﬁed. For\nexample, the sentence\n∀x American(x) ∧Weapon(y) ∧Sells(x, y, z) ∧Hostile(z) ⇒Criminal(x)\nbecomes, in CNF,\n¬American(x) ∨¬Weapon(y) ∨¬Sells(x, y, z) ∨¬Hostile(z) ∨Criminal(x) .\nEvery sentence of ﬁrst-order logic can be converted into an inferentially equivalent CNF\nsentence. In particular, the CNF sentence will be unsatisﬁable just when the original sentence\nis unsatisﬁable, so we have a basis for doing proofs by contradiction on the CNF sentences.\n6 A clause can also be represented as an implication with a conjunction of atoms in the premise and a disjunction\nof atoms in the conclusion (Exercise 7.13). This is called implicative normal form or Kowalski form (especially\nwhen written with a right-to-left implication symbol (Kowalski, 1979)) and is often much easier to read. 346\nChapter\n9.\nInference in First-Order Logic\nThe procedure for conversion to CNF is similar to the propositional case, which we saw",
  "when written with a right-to-left implication symbol (Kowalski, 1979)) and is often much easier to read. 346\nChapter\n9.\nInference in First-Order Logic\nThe procedure for conversion to CNF is similar to the propositional case, which we saw\non page 253. The principal difference arises from the need to eliminate existential quantiﬁers.\nWe illustrate the procedure by translating the sentence “Everyone who loves all animals is\nloved by someone,” or\n∀x [∀y Animal(y) ⇒Loves(x, y)] ⇒[∃y Loves(y, x)] .\nThe steps are as follows:\n• Eliminate implications:\n∀x [¬∀y ¬Animal(y) ∨Loves(x, y)] ∨[∃y Loves(y, x)] .\n• Move ¬ inwards: In addition to the usual rules for negated connectives, we need rules\nfor negated quantiﬁers. Thus, we have\n¬∀x p\nbecomes\n∃x ¬p\n¬∃x p\nbecomes\n∀x ¬p .\nOur sentence goes through the following transformations:\n∀x [∃y ¬(¬Animal(y) ∨Loves(x, y))] ∨[∃y Loves(y, x)] .\n∀x [∃y ¬¬Animal(y) ∧¬Loves(x, y)] ∨[∃y Loves(y, x)] .\n∀x [∃y Animal(y) ∧¬Loves(x, y)] ∨[∃y Loves(y, x)] .\nNotice how a universal quantiﬁer (∀y) in the premise of the implication has become\nan existential quantiﬁer. The sentence now reads “Either there is some animal that x\ndoesn’t love, or (if this is not the case) someone loves x.” Clearly, the meaning of the\noriginal sentence has been preserved.\n• Standardize variables: For sentences like (∃x P(x))∨(∃x Q(x)) which use the same\nvariable name twice, change the name of one of the variables. This avoids confusion\nlater when we drop the quantiﬁers. Thus, we have\n∀x [∃y Animal(y) ∧¬Loves(x, y)] ∨[∃z Loves(z, x)] .\n• Skolemize: Skolemization is the process of removing existential quantiﬁers by elimi-\nSKOLEMIZATION\nnation. In the simple case, it is just like the Existential Instantiation rule of Section 9.1:\ntranslate ∃x P(x) into P(A), where A is a new constant. However, we can’t apply Ex-\nistential Instantiation to our sentence above because it doesn’t match the pattern ∃v α;\nonly parts of the sentence match the pattern. If we blindly apply the rule to the two\nmatching parts we get\n∀x [Animal(A) ∧¬Loves(x, A)] ∨Loves(B, x) ,\nwhich has the wrong meaning entirely: it says that everyone either fails to love a par-\nticular animal A or is loved by some particular entity B. In fact, our original sentence\nallows each person to fail to love a different animal or to be loved by a different person.\nThus, we want the Skolem entities to depend on x and z:\n∀x [Animal(F(x)) ∧¬Loves(x, F(x))] ∨Loves(G(z), x) .",
  "allows each person to fail to love a different animal or to be loved by a different person.\nThus, we want the Skolem entities to depend on x and z:\n∀x [Animal(F(x)) ∧¬Loves(x, F(x))] ∨Loves(G(z), x) .\nHere F and G are Skolem functions. The general rule is that the arguments of the\nSKOLEM FUNCTION\nSkolem function are all the universally quantiﬁed variables in whose scope the exis-\ntential quantiﬁer appears. As with Existential Instantiation, the Skolemized sentence is\nsatisﬁable exactly when the original sentence is satisﬁable. Section 9.5.\nResolution\n347\n• Drop universal quantiﬁers: At this point, all remaining variables must be universally\nquantiﬁed. Moreover, the sentence is equivalent to one in which all the universal quan-\ntiﬁers have been moved to the left. We can therefore drop the universal quantiﬁers:\n[Animal(F(x)) ∧¬Loves(x, F(x))] ∨Loves(G(z), x) .\n• Distribute ∨over ∧:\n[Animal(F(x)) ∨Loves(G(z), x)] ∧[¬Loves(x, F(x)) ∨Loves(G(z), x)] .\nThis step may also require ﬂattening out nested conjunctions and disjunctions.\nThe sentence is now in CNF and consists of two clauses. It is quite unreadable. (It may\nhelp to explain that the Skolem function F(x) refers to the animal potentially unloved by x,\nwhereas G(z) refers to someone who might love x.) Fortunately, humans seldom need look\nat CNF sentences—the translation process is easily automated.\n9.5.2\nThe resolution inference rule\nThe resolution rule for ﬁrst-order clauses is simply a lifted version of the propositional reso-\nlution rule given on page 253. Two clauses, which are assumed to be standardized apart so\nthat they share no variables, can be resolved if they contain complementary literals. Propo-\nsitional literals are complementary if one is the negation of the other; ﬁrst-order literals are\ncomplementary if one uniﬁes with the negation of the other. Thus, we have\nℓ1 ∨· · · ∨ℓk,\nm1 ∨· · · ∨mn\nSUBST(θ, ℓ1 ∨· · · ∨ℓi−1 ∨ℓi+1 ∨· · · ∨ℓk ∨m1 ∨· · · ∨mj−1 ∨mj+1 ∨· · · ∨mn)\nwhere UNIFY(ℓi, ¬mj) = θ. For example, we can resolve the two clauses\n[Animal(F(x)) ∨Loves(G(x), x)]\nand\n[¬Loves(u, v) ∨¬Kills(u, v)]\nby eliminating the complementary literals Loves(G(x), x) and ¬Loves(u, v), with uniﬁer\nθ = {u/G(x), v/x}, to produce the resolvent clause\n[Animal(F(x)) ∨¬Kills(G(x), x)] .\nThis rule is called the binary resolution rule because it resolves exactly two literals. The\nBINARY RESOLUTION\nbinary resolution rule by itself does not yield a complete inference procedure. The full reso-",
  "[Animal(F(x)) ∨¬Kills(G(x), x)] .\nThis rule is called the binary resolution rule because it resolves exactly two literals. The\nBINARY RESOLUTION\nbinary resolution rule by itself does not yield a complete inference procedure. The full reso-\nlution rule resolves subsets of literals in each clause that are uniﬁable. An alternative approach\nis to extend factoring—the removal of redundant literals—to the ﬁrst-order case. Proposi-\ntional factoring reduces two literals to one if they are identical; ﬁrst-order factoring reduces\ntwo literals to one if they are uniﬁable. The uniﬁer must be applied to the entire clause. The\ncombination of binary resolution and factoring is complete.\n9.5.3\nExample proofs\nResolution proves that KB |= α by proving KB ∧¬α unsatisﬁable, that is, by deriving the\nempty clause. The algorithmic approach is identical to the propositional case, described in 348\nChapter\n9.\nInference in First-Order Logic\n¬American(x)    ¬Weapon(y)    ¬Sells(x,y,z)   ¬Hostile(z)   Criminal(x)\n¬Criminal(West)\n¬Enemy(Nono,America)\nEnemy(Nono,America)\n¬Missile(x)   Weapon(x)\n¬Weapon(y)   ¬Sells(West,y,z)   ¬Hostile(z)\nMissile(M1)\n¬Missile(y)   ¬Sells(West,y,z)   ¬Hostile(z)\n¬Missile(x)   ¬Owns(Nono,x)    Sells(West,x,Nono)\n¬Sells(West,M1,z)   ¬Hostile(z)\n¬American(West)   ¬Weapon(y)   ¬Sells(West,y,z)   ¬Hostile(z)\nAmerican(West)\n¬Missile(M1)   ¬Owns(Nono,M1)   ¬Hostile(Nono)\nMissile(M1)\n¬Owns(Nono,M1)   ¬Hostile(Nono)\nOwns(Nono,M1)\n¬Enemy(x,America)   Hostile(x)\n¬Hostile(Nono)\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\n^\nFigure 9.11\nA resolution proof that West is a criminal. At each step, the literals that unify\nare in bold.\nFigure 7.12, so we need not repeat it here. Instead, we give two example proofs. The ﬁrst is\nthe crime example from Section 9.3. The sentences in CNF are\n¬American(x) ∨¬Weapon(y) ∨¬Sells(x, y, z) ∨¬Hostile(z) ∨Criminal(x)\n¬Missile(x) ∨¬Owns(Nono, x) ∨Sells(West, x, Nono)\n¬Enemy(x, America) ∨Hostile(x)\n¬Missile(x) ∨Weapon(x)\nOwns(Nono, M1)\nMissile(M1)\nAmerican(West)\nEnemy(Nono, America) .\nWe also include the negated goal ¬Criminal(West). The resolution proof is shown in Fig-\nure 9.11. Notice the structure: single “spine” beginning with the goal clause, resolving against\nclauses from the knowledge base until the empty clause is generated. This is characteristic\nof resolution on Horn clause knowledge bases. In fact, the clauses along the main spine\ncorrespond exactly to the consecutive values of the goals variable in the backward-chaining",
  "of resolution on Horn clause knowledge bases. In fact, the clauses along the main spine\ncorrespond exactly to the consecutive values of the goals variable in the backward-chaining\nalgorithm of Figure 9.6. This is because we always choose to resolve with a clause whose\npositive literal uniﬁed with the leftmost literal of the “current” clause on the spine; this is\nexactly what happens in backward chaining. Thus, backward chaining is just a special case\nof resolution with a particular control strategy to decide which resolution to perform next.\nOur second example makes use of Skolemization and involves clauses that are not def-\ninite clauses. This results in a somewhat more complex proof structure. In English, the\nproblem is as follows:\nEveryone who loves all animals is loved by someone.\nAnyone who kills an animal is loved by no one.\nJack loves all animals.\nEither Jack or Curiosity killed the cat, who is named Tuna.\nDid Curiosity kill the cat? Section 9.5.\nResolution\n349\nFirst, we express the original sentences, some background knowledge, and the negated goal\nG in ﬁrst-order logic:\nA.\n∀x [∀y Animal(y) ⇒Loves(x, y)] ⇒[∃y Loves(y, x)]\nB.\n∀x [∃z Animal(z) ∧Kills(x, z)] ⇒[∀y ¬Loves(y, x)]\nC.\n∀x Animal(x) ⇒Loves(Jack, x)\nD.\nKills(Jack, Tuna) ∨Kills(Curiosity, Tuna)\nE.\nCat(Tuna)\nF.\n∀x Cat(x) ⇒Animal(x)\n¬G.\n¬Kills(Curiosity, Tuna)\nNow we apply the conversion procedure to convert each sentence to CNF:\nA1.\nAnimal(F(x)) ∨Loves(G(x), x)\nA2.\n¬Loves(x, F(x)) ∨Loves(G(x), x)\nB.\n¬Loves(y, x) ∨¬Animal(z) ∨¬Kills(x, z)\nC.\n¬Animal(x) ∨Loves(Jack, x)\nD.\nKills(Jack, Tuna) ∨Kills(Curiosity, Tuna)\nE.\nCat(Tuna)\nF.\n¬Cat(x) ∨Animal(x)\n¬G.\n¬Kills(Curiosity, Tuna)\nThe resolution proof that Curiosity killed the cat is given in Figure 9.12. In English, the proof\ncould be paraphrased as follows:\nSuppose Curiosity did not kill Tuna. We know that either Jack or Curiosity did; thus\nJack must have. Now, Tuna is a cat and cats are animals, so Tuna is an animal. Because\nanyone who kills an animal is loved by no one, we know that no one loves Jack. On the\nother hand, Jack loves all animals, so someone loves him; so we have a contradiction.\nTherefore, Curiosity killed the cat.\n¬Loves(y, Jack)\nLoves(G(Jack), Jack)\n¬Kills(Curiosity, Tuna)\nKills(Jack, Tuna)   Kills(Curiosity, Tuna)\n¬Cat(x)   Animal(x)\nCat(Tuna)\n¬Animal(F(Jack))   Loves(G(Jack), Jack)\nAnimal(F(x))   Loves(G(x), x) \n¬Loves(y, x)   ¬Kills(x, Tuna)\nKills(Jack, Tuna)\n¬Loves(y, x)   ¬Animal(z)   ¬Kills(x, z)\nAnimal(Tuna)",
  "Kills(Jack, Tuna)   Kills(Curiosity, Tuna)\n¬Cat(x)   Animal(x)\nCat(Tuna)\n¬Animal(F(Jack))   Loves(G(Jack), Jack)\nAnimal(F(x))   Loves(G(x), x) \n¬Loves(y, x)   ¬Kills(x, Tuna)\nKills(Jack, Tuna)\n¬Loves(y, x)   ¬Animal(z)   ¬Kills(x, z)\nAnimal(Tuna)\n¬Loves(x,F(x))   Loves(G(x), x)\n¬Animal(x)   Loves(Jack, x)\n^\n^\n^\n^\n^\n^\n^\n^\n^\nFigure 9.12\nA resolution proof that Curiosity killed the cat. Notice the use of factoring\nin the derivation of the clause Loves(G(Jack), Jack). Notice also in the upper right, the\nuniﬁcation of Loves(x, F(x)) and Loves(Jack, x) can only succeed after the variables have\nbeen standardized apart. 350\nChapter\n9.\nInference in First-Order Logic\nThe proof answers the question “Did Curiosity kill the cat?” but often we want to pose more\ngeneral questions, such as “Who killed the cat?” Resolution can do this, but it takes a little\nmore work to obtain the answer. The goal is ∃w Kills(w, Tuna), which, when negated,\nbecomes ¬Kills(w, Tuna) in CNF. Repeating the proof in Figure 9.12 with the new negated\ngoal, we obtain a similar proof tree, but with the substitution {w/Curiosity} in one of the\nsteps. So, in this case, ﬁnding out who killed the cat is just a matter of keeping track of the\nbindings for the query variables in the proof.\nUnfortunately, resolution can produce nonconstructive proofs for existential goals.\nNONCONSTRUCTIVE\nPROOF\nFor example, ¬Kills(w, Tuna) resolves with Kills(Jack, Tuna) ∨Kills(Curiosity, Tuna)\nto give Kills(Jack, Tuna), which resolves again with ¬Kills(w, Tuna) to yield the empty\nclause. Notice that w has two different bindings in this proof; resolution is telling us that,\nyes, someone killed Tuna—either Jack or Curiosity. This is no great surprise! One so-\nlution is to restrict the allowed resolution steps so that the query variables can be bound\nonly once in a given proof; then we need to be able to backtrack over the possible bind-\nings. Another solution is to add a special answer literal to the negated goal, which be-\nANSWER LITERAL\ncomes ¬Kills(w, Tuna) ∨Answer(w). Now, the resolution process generates an answer\nwhenever a clause is generated containing just a single answer literal. For the proof in Fig-\nure 9.12, this is Answer(Curiosity). The nonconstructive proof would generate the clause\nAnswer(Curiosity) ∨Answer(Jack), which does not constitute an answer.\n9.5.4\nCompleteness of resolution\nThis section gives a completeness proof of resolution. It can be safely skipped by those who\nare willing to take it on faith.",
  "Answer(Curiosity) ∨Answer(Jack), which does not constitute an answer.\n9.5.4\nCompleteness of resolution\nThis section gives a completeness proof of resolution. It can be safely skipped by those who\nare willing to take it on faith.\nWe show that resolution is refutation-complete, which means that if a set of sentences\nREFUTATION\nCOMPLETENESS\nis unsatisﬁable, then resolution will always be able to derive a contradiction. Resolution\ncannot be used to generate all logical consequences of a set of sentences, but it can be used\nto establish that a given sentence is entailed by the set of sentences. Hence, it can be used to\nﬁnd all answers to a given question, Q(x), by proving that KB ∧¬Q(x) is unsatisﬁable.\nWe take it as given that any sentence in ﬁrst-order logic (without equality) can be rewrit-\nten as a set of clauses in CNF. This can be proved by induction on the form of the sentence,\nusing atomic sentences as the base case (Davis and Putnam, 1960). Our goal therefore is to\nprove the following: if S is an unsatisﬁable set of clauses, then the application of a ﬁnite\nnumber of resolution steps to S will yield a contradiction.\nOur proof sketch follows Robinson’s original proof with some simpliﬁcations from\nGenesereth and Nilsson (1987). The basic structure of the proof (Figure 9.13) is as follows:\n1. First, we observe that if S is unsatisﬁable, then there exists a particular set of ground\ninstances of the clauses of S such that this set is also unsatisﬁable (Herbrand’s theorem).\n2. We then appeal to the ground resolution theorem given in Chapter 7, which states that\npropositional resolution is complete for ground sentences.\n3. We then use a lifting lemma to show that, for any propositional resolution proof using\nthe set of ground sentences, there is a corresponding ﬁrst-order resolution proof using\nthe ﬁrst-order sentences from which the ground sentences were obtained. Section 9.5.\nResolution\n351\nResolution can find a contradiction in S'\nThere is a resolution proof for the contradiction in S'\nHerbrand’s theorem\nSome set S' of ground instances is unsatisfiable\nAny set of sentences S is representable in clausal form\nAssume S is unsatisfiable, and in clausal form\nLifting lemma\nGround resolution\ntheorem\nFigure 9.13\nStructure of a completeness proof for resolution.\nTo carry out the ﬁrst step, we need three new concepts:\n• Herbrand universe: If S is a set of clauses, then HS, the Herbrand universe of S, is\nHERBRAND\nUNIVERSE",
  "Ground resolution\ntheorem\nFigure 9.13\nStructure of a completeness proof for resolution.\nTo carry out the ﬁrst step, we need three new concepts:\n• Herbrand universe: If S is a set of clauses, then HS, the Herbrand universe of S, is\nHERBRAND\nUNIVERSE\nthe set of all ground terms constructable from the following:\na. The function symbols in S, if any.\nb. The constant symbols in S, if any; if none, then the constant symbol A.\nFor example, if S contains just the clause ¬P(x, F(x, A))∨¬Q(x, A)∨R(x, B), then\nHS is the following inﬁnite set of ground terms:\n{A, B, F(A, A), F(A, B), F(B, A), F(B, B), F(A, F(A, A)), . . .} .\n• Saturation: If S is a set of clauses and P is a set of ground terms, then P(S), the\nSATURATION\nsaturation of S with respect to P, is the set of all ground clauses obtained by applying\nall possible consistent substitutions of ground terms in P with variables in S.\n• Herbrand base: The saturation of a set S of clauses with respect to its Herbrand uni-\nHERBRAND BASE\nverse is called the Herbrand base of S, written as HS(S). For example, if S contains\nsolely the clause just given, then HS(S) is the inﬁnite set of clauses\n{¬P(A, F(A, A)) ∨¬Q(A, A) ∨R(A, B),\n¬P(B, F(B, A)) ∨¬Q(B, A) ∨R(B, B),\n¬P(F(A, A), F(F(A, A), A)) ∨¬Q(F(A, A), A) ∨R(F(A, A), B),\n¬P(F(A, B), F(F(A, B), A)) ∨¬Q(F(A, B), A) ∨R(F(A, B), B), . . . }\nThese deﬁnitions allow us to state a form of Herbrand’s theorem (Herbrand, 1930):\nHERBRAND’S\nTHEOREM\nIf a set S of clauses is unsatisﬁable, then there exists a ﬁnite subset of HS(S) that\nis also unsatisﬁable.\nLet S′ be this ﬁnite subset of ground sentences. Now, we can appeal to the ground resolution\ntheorem (page 255) to show that the resolution closure RC(S′) contains the empty clause.\nThat is, running propositional resolution to completion on S′ will derive a contradiction.\nNow that we have established that there is always a resolution proof involving some\nﬁnite subset of the Herbrand base of S, the next step is to show that there is a resolution 352\nChapter\n9.\nInference in First-Order Logic\nG ¨ODEL’S INCOMPLETENESS THEOREM\nBy slightly extending the language of ﬁrst-order logic to allow for the mathemat-\nical induction schema in arithmetic, Kurt G¨odel was able to show, in his incom-\npleteness theorem, that there are true arithmetic sentences that cannot be proved.\nThe proof of the incompleteness theorem is somewhat beyond the scope of\nthis book, occupying, as it does, at least 30 pages, but we can give a hint here. We",
  "pleteness theorem, that there are true arithmetic sentences that cannot be proved.\nThe proof of the incompleteness theorem is somewhat beyond the scope of\nthis book, occupying, as it does, at least 30 pages, but we can give a hint here. We\nbegin with the logical theory of numbers. In this theory, there is a single constant,\n0, and a single function, S (the successor function). In the intended model, S(0)\ndenotes 1, S(S(0)) denotes 2, and so on; the language therefore has names for all\nthe natural numbers. The vocabulary also includes the function symbols +, ×, and\nExpt (exponentiation) and the usual set of logical connectives and quantiﬁers. The\nﬁrst step is to notice that the set of sentences that we can write in this language can\nbe enumerated. (Imagine deﬁning an alphabetical order on the symbols and then\narranging, in alphabetical order, each of the sets of sentences of length 1, 2, and\nso on.) We can then number each sentence α with a unique natural number #α\n(the G¨odel number). This is crucial: number theory contains a name for each of\nits own sentences. Similarly, we can number each possible proof P with a G¨odel\nnumber G(P), because a proof is simply a ﬁnite sequence of sentences.\nNow suppose we have a recursively enumerable set A of sentences that are\ntrue statements about the natural numbers. Recalling that A can be named by a\ngiven set of integers, we can imagine writing in our language a sentence α(j, A) of\nthe following sort:\n∀i i is not the G¨odel number of a proof of the sentence whose G¨odel\nnumber is j, where the proof uses only premises in A.\nThen let σ be the sentence α(#σ, A), that is, a sentence that states its own unprov-\nability from A. (That this sentence always exists is true but not entirely obvious.)\nNow we make the following ingenious argument: Suppose that σ is provable\nfrom A; then σ is false (because σ says it cannot be proved). But then we have a\nfalse sentence that is provable from A, so A cannot consist of only true sentences—\na violation of our premise. Therefore, σ is not provable from A. But this is exactly\nwhat σ itself claims; hence σ is a true sentence.\nSo, we have shown (barring 29 1\n2 pages) that for any set of true sentences of\nnumber theory, and in particular any set of basic axioms, there are other true sen-\ntences that cannot be proved from those axioms. This establishes, among other\nthings, that we can never prove all the theorems of mathematics within any given",
  "number theory, and in particular any set of basic axioms, there are other true sen-\ntences that cannot be proved from those axioms. This establishes, among other\nthings, that we can never prove all the theorems of mathematics within any given\nsystem of axioms. Clearly, this was an important discovery for mathematics. Its\nsigniﬁcance for AI has been widely debated, beginning with speculations by G¨odel\nhimself. We take up the debate in Chapter 26. Section 9.5.\nResolution\n353\nproof using the clauses of S itself, which are not necessarily ground clauses. We start by\nconsidering a single application of the resolution rule. Robinson stated this lemma:\nLet C1 and C2 be two clauses with no shared variables, and let C′\n1 and C′\n2 be\nground instances of C1 and C2. If C′ is a resolvent of C′\n1 and C′\n2, then there exists\na clause C such that (1) C is a resolvent of C1 and C2 and (2) C′ is a ground\ninstance of C.\nThis is called a lifting lemma, because it lifts a proof step from ground clauses up to general\nLIFTING LEMMA\nﬁrst-order clauses. In order to prove his basic lifting lemma, Robinson had to invent uniﬁ-\ncation and derive all of the properties of most general uniﬁers. Rather than repeat the proof\nhere, we simply illustrate the lemma:\nC1 = ¬P(x, F(x, A)) ∨¬Q(x, A) ∨R(x, B)\nC2 = ¬N(G(y), z) ∨P(H(y), z)\nC′\n1 = ¬P(H(B), F(H(B), A)) ∨¬Q(H(B), A) ∨R(H(B), B)\nC′\n2 = ¬N(G(B), F(H(B), A)) ∨P(H(B), F(H(B), A))\nC′ = ¬N(G(B), F(H(B), A)) ∨¬Q(H(B), A) ∨R(H(B), B)\nC = ¬N(G(y), F(H(y), A)) ∨¬Q(H(y), A) ∨R(H(y), B) .\nWe see that indeed C′ is a ground instance of C. In general, for C′\n1 and C′\n2 to have any\nresolvents, they must be constructed by ﬁrst applying to C1 and C2 the most general uniﬁer\nof a pair of complementary literals in C1 and C2. From the lifting lemma, it is easy to derive\na similar statement about any sequence of applications of the resolution rule:\nFor any clause C′ in the resolution closure of S′ there is a clause C in the resolu-\ntion closure of S such that C′ is a ground instance of C and the derivation of C is\nthe same length as the derivation of C′.\nFrom this fact, it follows that if the empty clause appears in the resolution closure of S′, it\nmust also appear in the resolution closure of S. This is because the empty clause cannot be a\nground instance of any other clause. To recap: we have shown that if S is unsatisﬁable, then\nthere is a ﬁnite derivation of the empty clause using the resolution rule.",
  "ground instance of any other clause. To recap: we have shown that if S is unsatisﬁable, then\nthere is a ﬁnite derivation of the empty clause using the resolution rule.\nThe lifting of theorem proving from ground clauses to ﬁrst-order clauses provides a vast\nincrease in power. This increase comes from the fact that the ﬁrst-order proof need instantiate\nvariables only as far as necessary for the proof, whereas the ground-clause methods were\nrequired to examine a huge number of arbitrary instantiations.\n9.5.5\nEquality\nNone of the inference methods described so far in this chapter handle an assertion of the form\nx = y. Three distinct approaches can be taken. The ﬁrst approach is to axiomatize equality—\nto write down sentences about the equality relation in the knowledge base. We need to say that\nequality is reﬂexive, symmetric, and transitive, and we also have to say that we can substitute\nequals for equals in any predicate or function. So we need three basic axioms, and then one 354\nChapter\n9.\nInference in First-Order Logic\nfor each predicate and function:\n∀x x = x\n∀x, y x = y ⇒y = x\n∀x, y, z x = y ∧y = z ⇒x = z\n∀x, y x = y ⇒(P1(x) ⇔P1(y))\n∀x, y x = y ⇒(P2(x) ⇔P2(y))\n...\n∀w, x, y, z w = y ∧x = z ⇒(F1(w, x) = F1(y, z))\n∀w, x, y, z w = y ∧x = z ⇒(F2(w, x) = F2(y, z))\n...\nGiven these sentences, a standard inference procedure such as resolution can perform tasks\nrequiring equality reasoning, such as solving mathematical equations. However, these axioms\nwill generate a lot of conclusions, most of them not helpful to a proof. So there has been a\nsearch for more efﬁcient ways of handling equality. One alternative is to add inference rules\nrather than axioms. The simplest rule, demodulation, takes a unit clause x = y and some\nclause α that contains the term x, and yields a new clause formed by substituting y for x\nwithin α. It works if the term within α uniﬁes with x; it need not be exactly equal to x.\nNote that demodulation is directional; given x = y, the x always gets replaced with y, never\nvice versa. That means that demodulation can be used for simplifying expressions using\ndemodulators such as x + 0 = x or x1 = x. As another example, given\nFather(Father(x)) = PaternalGrandfather (x)\nBirthdate(Father(Father(Bella)), 1926)\nwe can conclude by demodulation\nBirthdate(PaternalGrandfather (Bella), 1926) .\nMore formally, we have\n• Demodulation: For any terms x, y, and z, where z appears somewhere in literal mi\nDEMODULATION\nand where UNIFY(x, z) = θ,\nx = y,",
  "we can conclude by demodulation\nBirthdate(PaternalGrandfather (Bella), 1926) .\nMore formally, we have\n• Demodulation: For any terms x, y, and z, where z appears somewhere in literal mi\nDEMODULATION\nand where UNIFY(x, z) = θ,\nx = y,\nm1 ∨· · · ∨mn\nSUB(SUBST(θ, x), SUBST(θ, y), m1 ∨· · · ∨mn) .\nwhere SUBST is the usual substitution of a binding list, and SUB(x, y, m) means to\nreplace x with y everywhere that x occurs within m.\nThe rule can also be extended to handle non-unit clauses in which an equality literal appears:\n• Paramodulation: For any terms x, y, and z, where z appears somewhere in literal mi,\nPARAMODULATION\nand where UNIFY(x, z) = θ,\nℓ1 ∨· · · ∨ℓk ∨x = y,\nm1 ∨· · · ∨mn\nSUB(SUBST(θ, x), SUBST(θ, y), SUBST(θ, ℓ1 ∨· · · ∨ℓk ∨m1 ∨· · · ∨mn) .\nFor example, from\nP(F(x, B), x) ∨Q(x)\nand\nF(A, y) = y ∨R(y) Section 9.5.\nResolution\n355\nwe have θ = UNIFY(F(A, y), F(x, B)) = {x/A, y/B}, and we can conclude by paramodu-\nlation the sentence\nP(B, A) ∨Q(A) ∨R(B) .\nParamodulation yields a complete inference procedure for ﬁrst-order logic with equality.\nA third approach handles equality reasoning entirely within an extended uniﬁcation\nalgorithm. That is, terms are uniﬁable if they are provably equal under some substitution,\nwhere “provably” allows for equality reasoning. For example, the terms 1 + 2 and 2 + 1\nnormally are not uniﬁable, but a uniﬁcation algorithm that knows that x + y = y + x could\nunify them with the empty substitution. Equational uniﬁcation of this kind can be done with\nEQUATIONAL\nUNIFICATION\nefﬁcient algorithms designed for the particular axioms used (commutativity, associativity, and\nso on) rather than through explicit inference with those axioms. Theorem provers using this\ntechnique are closely related to the CLP systems described in Section 9.4.\n9.5.6\nResolution strategies\nWe know that repeated applications of the resolution inference rule will eventually ﬁnd a\nproof if one exists. In this subsection, we examine strategies that help ﬁnd proofs efﬁciently.\nUnit preference: This strategy prefers to do resolutions where one of the sentences is a single\nUNIT PREFERENCE\nliteral (also known as a unit clause). The idea behind the strategy is that we are trying to\nproduce an empty clause, so it might be a good idea to prefer inferences that produce shorter\nclauses. Resolving a unit sentence (such as P) with any other sentence (such as ¬P ∨¬Q∨R)\nalways yields a clause (in this case, ¬Q ∨R) that is shorter than the other clause. When",
  "clauses. Resolving a unit sentence (such as P) with any other sentence (such as ¬P ∨¬Q∨R)\nalways yields a clause (in this case, ¬Q ∨R) that is shorter than the other clause. When\nthe unit preference strategy was ﬁrst tried for propositional inference in 1964, it led to a\ndramatic speedup, making it feasible to prove theorems that could not be handled without the\npreference. Unit resolution is a restricted form of resolution in which every resolution step\nmust involve a unit clause. Unit resolution is incomplete in general, but complete for Horn\nclauses. Unit resolution proofs on Horn clauses resemble forward chaining.\nThe OTTER theorem prover (Organized Techniques for Theorem-proving and Effective\nResearch, McCune, 1992), uses a form of best-ﬁrst search. Its heuristic function measures\nthe “weight” of each clause, where lighter clauses are preferred. The exact choice of heuristic\nis up to the user, but generally, the weight of a clause should be correlated with its size or\ndifﬁculty. Unit clauses are treated as light; the search can thus be seen as a generalization of\nthe unit preference strategy.\nSet of support: Preferences that try certain resolutions ﬁrst are helpful, but in general it is\nSET OF SUPPORT\nmore effective to try to eliminate some potential resolutions altogether. For example, we can\ninsist that every resolution step involve at least one element of a special set of clauses—the\nset of support. The resolvent is then added into the set of support. If the set of support is\nsmall relative to the whole knowledge base, the search space will be reduced dramatically.\nWe have to be careful with this approach because a bad choice for the set of support\nwill make the algorithm incomplete. However, if we choose the set of support S so that the\nremainder of the sentences are jointly satisﬁable, then set-of-support resolution is complete.\nFor example, one can use the negated query as the set of support, on the assumption that the 356\nChapter\n9.\nInference in First-Order Logic\noriginal knowledge base is consistent. (After all, if it is not consistent, then the fact that the\nquery follows from it is vacuous.) The set-of-support strategy has the additional advantage of\ngenerating goal-directed proof trees that are often easy for humans to understand.\nInput resolution: In this strategy, every resolution combines one of the input sentences (from\nINPUT RESOLUTION\nthe KB or the query) with some other sentence. The proof in Figure 9.11 on page 348 uses",
  "Input resolution: In this strategy, every resolution combines one of the input sentences (from\nINPUT RESOLUTION\nthe KB or the query) with some other sentence. The proof in Figure 9.11 on page 348 uses\nonly input resolutions and has the characteristic shape of a single “spine” with single sen-\ntences combining onto the spine. Clearly, the space of proof trees of this shape is smaller\nthan the space of all proof graphs. In Horn knowledge bases, Modus Ponens is a kind of\ninput resolution strategy, because it combines an implication from the original KB with some\nother sentences. Thus, it is no surprise that input resolution is complete for knowledge bases\nthat are in Horn form, but incomplete in the general case. The linear resolution strategy is a\nLINEAR RESOLUTION\nslight generalization that allows P and Q to be resolved together either if P is in the original\nKB or if P is an ancestor of Q in the proof tree. Linear resolution is complete.\nSubsumption: The subsumption method eliminates all sentences that are subsumed by (that\nSUBSUMPTION\nis, more speciﬁc than) an existing sentence in the KB. For example, if P(x) is in the KB, then\nthere is no sense in adding P(A) and even less sense in adding P(A) ∨Q(B). Subsumption\nhelps keep the KB small and thus helps keep the search space small.\nPractical uses of resolution theorem provers\nTheorem provers can be applied to the problems involved in the synthesis and veriﬁcation\nSYNTHESIS\nVERIFICATION\nof both hardware and software. Thus, theorem-proving research is carried out in the ﬁelds of\nhardware design, programming languages, and software engineering—not just in AI.\nIn the case of hardware, the axioms describe the interactions between signals and cir-\ncuit elements. (See Section 8.4.2 on page 309 for an example.) Logical reasoners designed\nspecially for veriﬁcation have been able to verify entire CPUs, including their timing prop-\nerties (Srivas and Bickford, 1990). The AURA theorem prover has been applied to design\ncircuits that are more compact than any previous design (Wojciechowski and Wojcik, 1983).\nIn the case of software, reasoning about programs is quite similar to reasoning about\nactions, as in Chapter 7: axioms describe the preconditions and effects of each statement.\nThe formal synthesis of algorithms was one of the ﬁrst uses of theorem provers, as outlined\nby Cordell Green (1969a), who built on earlier ideas by Herbert Simon (1963). The idea",
  "The formal synthesis of algorithms was one of the ﬁrst uses of theorem provers, as outlined\nby Cordell Green (1969a), who built on earlier ideas by Herbert Simon (1963). The idea\nis to constructively prove a theorem to the effect that “there exists a program p satisfying a\ncertain speciﬁcation.” Although fully automated deductive synthesis, as it is called, has not\nDEDUCTIVE\nSYNTHESIS\nyet become feasible for general-purpose programming, hand-guided deductive synthesis has\nbeen successful in designing several novel and sophisticated algorithms. Synthesis of special-\npurpose programs, such as scientiﬁc computing code, is also an active area of research.\nSimilar techniques are now being applied to software veriﬁcation by systems such as the\nSPIN model checker (Holzmann, 1997). For example, the Remote Agent spacecraft control\nprogram was veriﬁed before and after ﬂight (Havelund et al., 2000). The RSA public key\nencryption algorithm and the Boyer–Moore string-matching algorithm have been veriﬁed this\nway (Boyer and Moore, 1984). Section 9.6.\nSummary\n357\n9.6\nSUMMARY\nWe have presented an analysis of logical inference in ﬁrst-order logic and a number of algo-\nrithms for doing it.\n• A ﬁrst approach uses inference rules (universal instantiation and existential instan-\ntiation) to propositionalize the inference problem. Typically, this approach is slow,\nunless the domain is small.\n• The use of uniﬁcation to identify appropriate substitutions for variables eliminates the\ninstantiation step in ﬁrst-order proofs, making the process more efﬁcient in many cases.\n• A lifted version of Modus Ponens uses uniﬁcation to provide a natural and powerful\ninference rule, generalized Modus Ponens. The forward-chaining and backward-\nchaining algorithms apply this rule to sets of deﬁnite clauses.\n• Generalized Modus Ponens is complete for deﬁnite clauses, although the entailment\nproblem is semidecidable. For Datalog knowledge bases consisting of function-free\ndeﬁnite clauses, entailment is decidable.\n• Forward chaining is used in deductive databases, where it can be combined with re-\nlational database operations. It is also used in production systems, which perform\nefﬁcient updates with very large rule sets. Forward chaining is complete for Datalog\nand runs in polynomial time.\n• Backward chaining is used in logic programming systems, which employ sophisti-\ncated compiler technology to provide very fast inference. Backward chaining suffers",
  "and runs in polynomial time.\n• Backward chaining is used in logic programming systems, which employ sophisti-\ncated compiler technology to provide very fast inference. Backward chaining suffers\nfrom redundant inferences and inﬁnite loops; these can be alleviated by memoization.\n• Prolog, unlike ﬁrst-order logic, uses a closed world with the unique names assumption\nand negation as failure. These make Prolog a more practical programming language,\nbut bring it further from pure logic.\n• The generalized resolution inference rule provides a complete proof system for ﬁrst-\norder logic, using knowledge bases in conjunctive normal form.\n• Several strategies exist for reducing the search space of a resolution system without\ncompromising completeness. One of the most important issues is dealing with equality;\nwe showed how demodulation and paramodulation can be used.\n• Efﬁcient resolution-based theorem provers have been used to prove interesting mathe-\nmatical theorems and to verify and synthesize software and hardware.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nGottlob Frege, who developed full ﬁrst-order logic in 1879, based his system of inference\non a collection of valid schemas plus a single inference rule, Modus Ponens. Whitehead\nand Russell (1910) expounded the so-called rules of passage (the actual term is from Her-\nbrand (1930)) that are used to move quantiﬁers to the front of formulas. Skolem constants 358\nChapter\n9.\nInference in First-Order Logic\nand Skolem functions were introduced, appropriately enough, by Thoralf Skolem (1920).\nOddly enough, it was Skolem who introduced the Herbrand universe (Skolem, 1928).\nHerbrand’s theorem (Herbrand, 1930) has played a vital role in the development of\nautomated reasoning. Herbrand is also the inventor of uniﬁcation. G¨odel (1930) built on\nthe ideas of Skolem and Herbrand to show that ﬁrst-order logic has a complete proof pro-\ncedure. Alan Turing (1936) and Alonzo Church (1936) simultaneously showed, using very\ndifferent proofs, that validity in ﬁrst-order logic was not decidable. The excellent text by\nEnderton (1972) explains all of these results in a rigorous yet understandable fashion.\nAbraham Robinson proposed that an automated reasoner could be built using proposi-\ntionalization and Herbrand’s theorem, and Paul Gilmore (1960) wrote the ﬁrst program. Davis\nand Putnam (1960) introduced the propositionalization method of Section 9.1. Prawitz (1960)",
  "tionalization and Herbrand’s theorem, and Paul Gilmore (1960) wrote the ﬁrst program. Davis\nand Putnam (1960) introduced the propositionalization method of Section 9.1. Prawitz (1960)\ndeveloped the key idea of letting the quest for propositional inconsistency drive the search,\nand generating terms from the Herbrand universe only when they were necessary to estab-\nlish propositional inconsistency. After further development by other researchers, this idea led\nJ. A. Robinson (no relation) to develop resolution (Robinson, 1965).\nIn AI, resolution was adopted for question-answering systems by Cordell Green and\nBertram Raphael (1968). Early AI implementations put a good deal of effort into data struc-\ntures that would allow efﬁcient retrieval of facts; this work is covered in AI programming\ntexts (Charniak et al., 1987; Norvig, 1992; Forbus and de Kleer, 1993). By the early 1970s,\nforward chaining was well established in AI as an easily understandable alternative to res-\nolution. AI applications typically involved large numbers of rules, so it was important to\ndevelop efﬁcient rule-matching technology, particularly for incremental updates. The tech-\nnology for production systems was developed to support such applications. The production\nsystem language OPS-5 (Forgy, 1981; Brownston et al., 1985), incorporating the efﬁcient\nrete match process (Forgy, 1982), was used for applications such as the R1 expert system for\nRETE\nminicomputer conﬁguration (McDermott, 1982).\nThe SOAR cognitive architecture (Laird et al., 1987; Laird, 2008) was designed to han-\ndle very large rule sets—up to a million rules (Doorenbos, 1994). Example applications of\nSOAR include controlling simulated ﬁghter aircraft (Jones et al., 1998), airspace manage-\nment (Taylor et al., 2007), AI characters for computer games (Wintermute et al., 2007), and\ntraining tools for soldiers (Wray and Jones, 2005).\nThe ﬁeld of deductive databases began with a workshop in Toulouse in 1977 that\nbrought together experts in logical inference and database systems (Gallaire and Minker,\n1978). Inﬂuential work by Chandra and Harel (1980) and Ullman (1985) led to the adoption\nof Datalog as a standard language for deductive databases. The development of the magic sets\ntechnique for rule rewriting by Bancilhon et al. (1986) allowed forward chaining to borrow\nthe advantage of goal-directedness from backward chaining. Current work includes the idea",
  "technique for rule rewriting by Bancilhon et al. (1986) allowed forward chaining to borrow\nthe advantage of goal-directedness from backward chaining. Current work includes the idea\nof integrating multiple databases into a consistent dataspace (Halevy, 2007).\nBackward chaining for logical inference appeared ﬁrst in Hewitt’s PLANNER lan-\nguage (1969). Meanwhile, in 1972, Alain Colmerauer had developed and implemented Pro-\nlog for the purpose of parsing natural language—Prolog’s clauses were intended initially\nas context-free grammar rules (Roussel, 1975; Colmerauer et al., 1973). Much of the the-\noretical background for logic programming was developed by Robert Kowalski, working Bibliographical and Historical Notes\n359\nwith Colmerauer; see Kowalski (1988) and Colmerauer and Roussel (1993) for a historical\noverview. Efﬁcient Prolog compilers are generally based on the Warren Abstract Machine\n(WAM) model of computation developed by David H. D. Warren (1983). Van Roy (1990)\nshowed that Prolog programs can be competitive with C programs in terms of speed.\nMethods for avoiding unnecessary looping in recursive logic programs were developed\nindependently by Smith et al. (1986) and Tamaki and Sato (1986). The latter paper also\nincluded memoization for logic programs, a method developed extensively as tabled logic\nprogramming by David S. Warren. Swift and Warren (1994) show how to extend the WAM\nto handle tabling, enabling Datalog programs to execute an order of magnitude faster than\nforward-chaining deductive database systems.\nEarly work on constraint logic programming was done by Jaffar and Lassez (1987).\nJaffar et al. (1992) developed the CLP(R) system for handling real-valued constraints. There\nare now commercial products for solving large-scale conﬁguration and optimization problems\nwith constraint programming; one of the best known is ILOG (Junker, 2003). Answer set\nprogramming (Gelfond, 2008) extends Prolog, allowing disjunction and negation.\nTexts on logic programming and Prolog, including Shoham (1994), Bratko (2001),\nClocksin (2003), and Clocksin and Mellish (2003). Prior to 2000, the Journal of Logic Pro-\ngramming was the journal of record; it has now been replaced by Theory and Practice of\nLogic Programming. Logic programming conferences include the International Conference\non Logic Programming (ICLP) and the International Logic Programming Symposium (ILPS).\nResearch into mathematical theorem proving began even before the ﬁrst complete",
  "on Logic Programming (ICLP) and the International Logic Programming Symposium (ILPS).\nResearch into mathematical theorem proving began even before the ﬁrst complete\nﬁrst-order systems were developed. Herbert Gelernter’s Geometry Theorem Prover (Gelern-\nter, 1959) used heuristic search methods combined with diagrams for pruning false subgoals\nand was able to prove some quite intricate results in Euclidean geometry. The demodula-\ntion and paramodulation rules for equality reasoning were introduced by Wos et al. (1967)\nand Wos and Robinson (1968), respectively. These rules were also developed independently\nin the context of term-rewriting systems (Knuth and Bendix, 1970). The incorporation of\nequality reasoning into the uniﬁcation algorithm is due to Gordon Plotkin (1972). Jouannaud\nand Kirchner (1991) survey equational uniﬁcation from a term-rewriting perspective. An\noverview of uniﬁcation is given by Baader and Snyder (2001).\nA number of control strategies have been proposed for resolution, beginning with the\nunit preference strategy (Wos et al., 1964). The set-of-support strategy was proposed by Wos\net al. (1965) to provide a degree of goal-directedness in resolution. Linear resolution ﬁrst\nappeared in Loveland (1970). Genesereth and Nilsson (1987, Chapter 5) provide a short but\nthorough analysis of a wide variety of control strategies.\nA Computational Logic (Boyer and Moore, 1979) is the basic reference on the Boyer-\nMoore theorem prover. Stickel (1992) covers the Prolog Technology Theorem Prover (PTTP),\nwhich combines the advantages of Prolog compilation with the completeness of model elimi-\nnation. SETHEO (Letz et al., 1992) is another widely used theorem prover based on this ap-\nproach. LEANTAP (Beckert and Posegga, 1995) is an efﬁcient theorem prover implemented\nin only 25 lines of Prolog. Weidenbach (2001) describes SPASS, one of the strongest current\ntheorem provers. The most successful theorem prover in recent annual competitions has been\nVAMPIRE (Riazanov and Voronkov, 2002). The COQ system (Bertot et al., 2004) and the E 360\nChapter\n9.\nInference in First-Order Logic\nequational solver (Schulz, 2004) have also proven to be valuable tools for proving correct-\nness. Theorem provers have been used to automatically synthesize and verify software for\ncontrolling spacecraft (Denney et al., 2006), including NASA’s new Orion capsule (Lowry,\n2008). The design of the FM9001 32-bit microprocessor was proved correct by the NQTHM",
  "controlling spacecraft (Denney et al., 2006), including NASA’s new Orion capsule (Lowry,\n2008). The design of the FM9001 32-bit microprocessor was proved correct by the NQTHM\nsystem (Hunt and Brock, 1992). The Conference on Automated Deduction (CADE) runs an\nannual contest for automated theorem provers. From 2002 through 2008, the most successful\nsystem has been VAMPIRE (Riazanov and Voronkov, 2002). Wiedijk (2003) compares the\nstrength of 15 mathematical provers. TPTP (Thousands of Problems for Theorem Provers)\nis a library of theorem-proving problems, useful for comparing the performance of systems\n(Sutcliffe and Suttner, 1998; Sutcliffe et al., 2006).\nTheorem provers have come up with novel mathematical results that eluded human\nmathematicians for decades, as detailed in the book Automated Reasoning and the Discov-\nery of Missing Elegant Proofs (Wos and Pieper, 2003). The SAM (Semi-Automated Math-\nematics) program was the ﬁrst, proving a lemma in lattice theory (Guard et al., 1969). The\nAURA program has also answered open questions in several areas of mathematics (Wos and\nWinker, 1983). The Boyer–Moore theorem prover (Boyer and Moore, 1979) was used by\nNatarajan Shankar to give the ﬁrst fully rigorous formal proof of G¨odel’s Incompleteness\nTheorem (Shankar, 1986). The NUPRL system proved Girard’s paradox (Howe, 1987) and\nHigman’s Lemma (Murthy and Russell, 1990). In 1933, Herbert Robbins proposed a simple\nset of axioms—the Robbins algebra—that appeared to deﬁne Boolean algebra, but no proof\nROBBINS ALGEBRA\ncould be found (despite serious work by Alfred Tarski and others). On October 10, 1996,\nafter eight days of computation, EQP (a version of OTTER) found a proof (McCune, 1997).\nMany early papers in mathematical logic are to be found in From Frege to G¨odel:\nA Source Book in Mathematical Logic (van Heijenoort, 1967). Textbooks geared toward\nautomated deduction include the classic Symbolic Logic and Mechanical Theorem Prov-\ning (Chang and Lee, 1973), as well as more recent works by Duffy (1991), Wos et al. (1992),\nBibel (1993), and Kaufmann et al. (2000). The principal journal for theorem proving is the\nJournal of Automated Reasoning; the main conferences are the annual Conference on Auto-\nmated Deduction (CADE) and the International Joint Conference on Automated Reasoning\n(IJCAR). The Handbook of Automated Reasoning (Robinson and Voronkov, 2001) collects\npapers in the ﬁeld. MacKenzie’s Mechanizing Proof (2004) covers the history and technology",
  "(IJCAR). The Handbook of Automated Reasoning (Robinson and Voronkov, 2001) collects\npapers in the ﬁeld. MacKenzie’s Mechanizing Proof (2004) covers the history and technology\nof theorem proving for the popular audience.\nEXERCISES\n9.1\nProve that Universal Instantiation is sound and that Existential Instantiation produces\nan inferentially equivalent knowledge base.\n9.2\nFrom Likes(Jerry, IceCream) it seems reasonable to infer ∃x Likes(x, IceCream).\nWrite down a general inference rule, Existential Introduction, that sanctions this inference.\nEXISTENTIAL\nINTRODUCTION\nState carefully the conditions that must be satisﬁed by the variables and terms involved. Exercises\n361\n9.3\nSuppose a knowledge base contains just one sentence, ∃x AsHighAs(x, Everest).\nWhich of the following are legitimate results of applying Existential Instantiation?\na. AsHighAs(Everest, Everest).\nb. AsHighAs(Kilimanjaro, Everest).\nc. AsHighAs(Kilimanjaro, Everest) ∧AsHighAs(BenNevis, Everest)\n(after two applications).\n9.4\nFor each pair of atomic sentences, give the most general uniﬁer if it exists:\na. P(A, B, B), P(x, y, z).\nb. Q(y, G(A, B)), Q(G(x, x), y).\nc. Older(Father(y), y), Older(Father(x), John).\nd. Knows(Father(y), y), Knows(x, x).\n9.5\nConsider the subsumption lattices shown in Figure 9.2 (page 329).\na. Construct the lattice for the sentence Employs(Mother(John), Father(Richard)).\nb. Construct the lattice for the sentence Employs(IBM , y) (“Everyone works for IBM”).\nRemember to include every kind of query that uniﬁes with the sentence.\nc. Assume that STORE indexes each sentence under every node in its subsumption lattice.\nExplain how FETCH should work when some of these sentences contain variables; use\nas examples the sentences in (a) and (b) and the query Employs(x, Father(x)).\n9.6\nWrite down logical representations for the following sentences, suitable for use with\nGeneralized Modus Ponens:\na. Horses, cows, and pigs are mammals.\nb. An offspring of a horse is a horse.\nc. Bluebeard is a horse.\nd. Bluebeard is Charlie’s parent.\ne. Offspring and parent are inverse relations.\nf. Every mammal has a parent.\n9.7\nThese questions concern concern issues with substitution and Skolemization.\na. Given the premise ∀x ∃y P(x, y), it is not valid to conclude that ∃q P(q, q). Give\nan example of a predicate P where the ﬁrst is true but the second is false.\nb. Suppose that an inference engine is incorrectly written with the occurs check omitted,",
  "an example of a predicate P where the ﬁrst is true but the second is false.\nb. Suppose that an inference engine is incorrectly written with the occurs check omitted,\nso that it allows a literal like P(x, F(x)) to be uniﬁed with P(q, q). (As mentioned,\nmost standard implementations of Prolog actually do allow this.) Show that such an\ninference engine will allow the conclusion ∃y P(q, q) to be inferred from the premise\n∀x ∃y P(x, y). 362\nChapter\n9.\nInference in First-Order Logic\nc. Suppose that a procedure that converts ﬁrst-order logic to clausal form incorrectly\nSkolemizes ∀x\n∃y\nP(x, y) to P(x, Sk0)—that is, it replaces y by a Skolem con-\nstant rather than by a Skolem function of x. Show that an inference engine that uses\nsuch a procedure will likewise allow ∃q\nP(q, q) to be inferred from the premise\n∀x ∃y P(x, y).\nd. A common error among students is to suppose that, in uniﬁcation, one is allowed to\nsubstitute a term for a Skolem constant instead of for a variable. For instance, they will\nsay that the formulas P(Sk1) and P(A) can be uniﬁed under the substitution {Sk1/A}.\nGive an example where this leads to an invalid inference.\n9.8\nExplain how to write any given 3-SAT problem of arbitrary size using a single ﬁrst-order\ndeﬁnite clause and no more than 30 ground facts.\n9.9\nSuppose you are given the following axioms:\n1. 0 ≤3.\n2. 7 ≤9.\n3. ∀x\nx ≤x.\n4. ∀x\nx ≤x + 0.\n5. ∀x\nx + 0 ≤x.\n6. ∀x, y\nx + y ≤y + x.\n7. ∀w, x, y, z\nw ≤y ∧x ≤z ⇒w + x ≤y + z.\n8. ∀x, y, z\nx ≤y ∧y ≤z ⇒x ≤z\na. Give a backward-chaining proof of the sentence 7 ≤3 + 9. (Be sure, of course, to use\nonly the axioms given here, not anything else you may know about arithmetic.) Show\nonly the steps that leads to success, not the irrelevant steps.\nb. Give a forward-chaining proof of the sentence 7 ≤3 + 9. Again, show only the steps\nthat lead to success.\n9.10\nA popular children’s riddle is “Brothers and sisters have I none, but that man’s father\nis my father’s son.” Use the rules of the family domain (Section 8.3.2 on page 301) to show\nwho that man is. You may apply any of the inference methods described in this chapter. Why\ndo you think that this riddle is difﬁcult?\n9.11\nSuppose we put into a logical knowledge base a segment of the U.S. census data list-\ning the age, city of residence, date of birth, and mother of every person, using social se-\ncurity numbers as identifying constants for each person. Thus, George’s age is given by",
  "ing the age, city of residence, date of birth, and mother of every person, using social se-\ncurity numbers as identifying constants for each person. Thus, George’s age is given by\nAge(443-65-1282, 56). Which of the following indexing schemes S1–S5 enable an efﬁcient\nsolution for which of the queries Q1–Q4 (assuming normal backward chaining)?\n• S1: an index for each atom in each position.\n• S2: an index for each ﬁrst argument.\n• S3: an index for each predicate atom.\n• S4: an index for each combination of predicate and ﬁrst argument. Exercises\n363\n• S5: an index for each combination of predicate and second argument and an index for\neach ﬁrst argument.\n• Q1: Age(443-44-4321, x)\n• Q2: ResidesIn(x, Houston)\n• Q3: Mother(x, y)\n• Q4: Age(x, 34) ∧ResidesIn(x, TinyTownUSA)\n9.12\nOne might suppose that we can avoid the problem of variable conﬂict in uniﬁcation\nduring backward chaining by standardizing apart all of the sentences in the knowledge base\nonce and for all. Show that, for some sentences, this approach cannot work. (Hint: Consider\na sentence in which one part uniﬁes with another.)\n9.13\nIn this exercise, use the sentences you wrote in Exercise 9.6 to answer a question by\nusing a backward-chaining algorithm.\na. Draw the proof tree generated by an exhaustive backward-chaining algorithm for the\nquery ∃h Horse(h), where clauses are matched in the order given.\nb. What do you notice about this domain?\nc. How many solutions for h actually follow from your sentences?\nd. Can you think of a way to ﬁnd all of them? (Hint: See Smith et al. (1986).)\n9.14\nTrace the execution of the backward-chaining algorithm in Figure 9.6 (page 338) when\nit is applied to solve the crime problem (page 330). Show the sequence of values taken on by\nthe goals variable, and arrange them into a tree.\n9.15\nThe following Prolog code deﬁnes a predicate P. (Remember that uppercase terms are\nvariables, not constants, in Prolog.)\nP(X,[X|Y]).\nP(X,[Y|Z]) :- P(X,Z).\na. Show proof trees and solutions for the queries P(A,[2,1,3])and P(2,[1,A,3]).\nb. What standard list operation does P represent?\n9.16\nThis exercise looks at sorting in Prolog.\na. Write Prolog clauses that deﬁne the predicate sorted(L), which is true if and only if\nlist L is sorted in ascending order.\nb. Write a Prolog deﬁnition for the predicate perm(L,M), which is true if and only if L\nis a permutation of M.\nc. Deﬁne sort(L,M) (M is a sorted version of L) using perm and sorted.",
  "list L is sorted in ascending order.\nb. Write a Prolog deﬁnition for the predicate perm(L,M), which is true if and only if L\nis a permutation of M.\nc. Deﬁne sort(L,M) (M is a sorted version of L) using perm and sorted.\nd. Run sort on longer and longer lists until you lose patience. What is the time complex-\nity of your program?\ne. Write a faster sorting algorithm, such as insertion sort or quicksort, in Prolog. 364\nChapter\n9.\nInference in First-Order Logic\n9.17\nThis exercise looks at the recursive application of rewrite rules, using logic program-\nming. A rewrite rule (or demodulator in OTTER terminology) is an equation with a speciﬁed\ndirection. For example, the rewrite rule x + 0 →x suggests replacing any expression that\nmatches x+0 with the expression x. Rewrite rules are a key component of equational reason-\ning systems. Use the predicate rewrite(X,Y) to represent rewrite rules. For example, the\nearlier rewrite rule is written as rewrite(X+0,X). Some terms are primitive and cannot\nbe further simpliﬁed; thus, we write primitive(0) to say that 0 is a primitive term.\na. Write a deﬁnition of a predicate simplify(X,Y), that is true when Y is a simpliﬁed\nversion of X—that is, when no further rewrite rules apply to any subexpression of Y.\nb. Write a collection of rules for the simpliﬁcation of expressions involving arithmetic\noperators, and apply your simpliﬁcation algorithm to some sample expressions.\nc. Write a collection of rewrite rules for symbolic differentiation, and use them along with\nyour simpliﬁcation rules to differentiate and simplify expressions involving arithmetic\nexpressions, including exponentiation.\n9.18\nThis exercise considers the implementation of search algorithms in Prolog. Suppose\nthat successor(X,Y) is true when state Y is a successor of state X; and that goal(X)\nis true when X is a goal state. Write a deﬁnition for solve(X,P), which means that P is a\npath (list of states) beginning with X, ending in a goal state, and consisting of a sequence of\nlegal steps as deﬁned by successor. You will ﬁnd that depth-ﬁrst search is the easiest way\nto do this. How easy would it be to add heuristic search control?\n9.19\nSuppose a knowledge base contains just the following ﬁrst-order Horn clauses:\nAncestor(Mother(x), x)\nAncestor(x, y) ∧Ancestor(y, z) ⇒Ancestor(x, z)\nConsider a forward chaining algorithm that, on the jth iteration, terminates if the KB contains",
  "9.19\nSuppose a knowledge base contains just the following ﬁrst-order Horn clauses:\nAncestor(Mother(x), x)\nAncestor(x, y) ∧Ancestor(y, z) ⇒Ancestor(x, z)\nConsider a forward chaining algorithm that, on the jth iteration, terminates if the KB contains\na sentence that uniﬁes with the query, else adds to the KB every atomic sentence that can be\ninferred from the sentences already in the KB after iteration j −1.\na. For each of the following queries, say whether the algorithm will (1) give an answer (if\nso, write down that answer); or (2) terminate with no answer; or (3) never terminate.\n(i) Ancestor(Mother(y), John)\n(ii) Ancestor(Mother(Mother(y)), John)\n(iii) Ancestor(Mother(Mother(Mother(y))), Mother(y))\n(iv) Ancestor(Mother(John), Mother(Mother(John)))\nb. Can a resolution algorithm prove the sentence ¬Ancestor(John, John) from the orig-\ninal knowledge base? Explain how, or why not.\nc. Suppose we add the assertion that ¬(Mother(x) = x) and augment the resolution al-\ngorithm with inference rules for equality. Now what is the answer to (b)?\n9.20\nLet L be the ﬁrst-order language with a single predicate S(p, q), meaning “p shaves q.”\nAssume a domain of people. Exercises\n365\na. Consider the sentence “There exists a person P who shaves every one who does not\nshave themselves, and only people that do not shave themselves.” Express this in L.\nb. Convert the sentence in (a) to clausal form.\nc. Construct a resolution proof to show that the clauses in (b) are inherently inconsistent.\n(Note: you do not need any additional axioms.)\n9.21\nHow can resolution be used to show that a sentence is valid? Unsatisﬁable?\n9.22\nConstruct an example of two clauses that can be resolved together in two different\nways giving two different outcomes.\n9.23\nFrom “Horses are animals,” it follows that “The head of a horse is the head of an\nanimal.” Demonstrate that this inference is valid by carrying out the following steps:\na. Translate the premise and the conclusion into the language of ﬁrst-order logic. Use three\npredicates: HeadOf (h, x) (meaning “h is the head of x”), Horse(x), and Animal(x).\nb. Negate the conclusion, and convert the premise and the negated conclusion into con-\njunctive normal form.\nc. Use resolution to show that the conclusion follows from the premise.\n9.24\nHere are two sentences in the language of ﬁrst-order logic:\n(A) ∀x ∃y (x ≥y)\n(B) ∃y ∀x (x ≥y)\na. Assume that the variables range over all the natural numbers 0, 1, 2, . . . , ∞and that the",
  "9.24\nHere are two sentences in the language of ﬁrst-order logic:\n(A) ∀x ∃y (x ≥y)\n(B) ∃y ∀x (x ≥y)\na. Assume that the variables range over all the natural numbers 0, 1, 2, . . . , ∞and that the\n“≥” predicate means “is greater than or equal to.” Under this interpretation, translate\n(A) and (B) into English.\nb. Is (A) true under this interpretation?\nc. Is (B) true under this interpretation?\nd. Does (A) logically entail (B)?\ne. Does (B) logically entail (A)?\nf. Using resolution, try to prove that (A) follows from (B). Do this even if you think that\n(B) does not logically entail (A); continue until the proof breaks down and you cannot\nproceed (if it does break down). Show the unifying substitution for each resolution step.\nIf the proof fails, explain exactly where, how, and why it breaks down.\ng. Now try to prove that (B) follows from (A).\n9.25\nResolution can produce nonconstructive proofs for queries with variables, so we had\nto introduce special mechanisms to extract deﬁnite answers. Explain why this issue does not\narise with knowledge bases containing only deﬁnite clauses.\n9.26\nWe said in this chapter that resolution cannot be used to generate all logical conse-\nquences of a set of sentences. Can any algorithm do this? 10\nCLASSICAL PLANNING\nIn which we see how an agent can take advantage of the structure of a problem to\nconstruct complex plans of action.\nWe have deﬁned AI as the study of rational action, which means that planning—devising a\nplan of action to achieve one’s goals—is a critical part of AI. We have seen two examples\nof planning agents so far: the search-based problem-solving agent of Chapter 3 and the hy-\nbrid logical agent of Chapter 7. In this chapter we introduce a representation for planning\nproblems that scales up to problems that could not be handled by those earlier approaches.\nSection 10.1 develops an expressive yet carefully constrained language for representing\nplanning problems. Section 10.2 shows how forward and backward search algorithms can\ntake advantage of this representation, primarily through accurate heuristics that can be derived\nautomatically from the structure of the representation. (This is analogous to the way in which\neffective domain-independent heuristics were constructed for constraint satisfaction problems\nin Chapter 6.) Section 10.3 shows how a data structure called the planning graph can make the\nsearch for a plan more efﬁcient. We then describe a few of the other approaches to planning,",
  "in Chapter 6.) Section 10.3 shows how a data structure called the planning graph can make the\nsearch for a plan more efﬁcient. We then describe a few of the other approaches to planning,\nand conclude by comparing the various approaches.\nThis chapter covers fully observable, deterministic, static environments with single\nagents. Chapters 11 and 17 cover partially observable, stochastic, dynamic environments\nwith multiple agents.\n10.1\nDEFINITION OF CLASSICAL PLANNING\nThe problem-solving agent of Chapter 3 can ﬁnd sequences of actions that result in a goal\nstate. But it deals with atomic representations of states and thus needs good domain-speciﬁc\nheuristics to perform well. The hybrid propositional logical agent of Chapter 7 can ﬁnd plans\nwithout domain-speciﬁc heuristics because it uses domain-independent heuristics based on\nthe logical structure of the problem. But it relies on ground (variable-free) propositional\ninference, which means that it may be swamped when there are many actions and states. For\nexample, in the wumpus world, the simple action of moving a step forward had to be repeated\nfor all four agent orientations, T time steps, and n2 current locations.\n366 Section 10.1.\nDeﬁnition of Classical Planning\n367\nIn response to this, planning researchers have settled on a factored representation—\none in which a state of the world is represented by a collection of variables. We use a language\ncalled PDDL, the Planning Domain Deﬁnition Language, that allows us to express all 4Tn2\nPDDL\nactions with one action schema. There have been several versions of PDDL; we select a\nsimple version and alter its syntax to be consistent with the rest of the book.1 We now show\nhow PDDL describes the four things we need to deﬁne a search problem: the initial state, the\nactions that are available in a state, the result of applying an action, and the goal test.\nEach state is represented as a conjunction of ﬂuents that are ground, functionless atoms.\nFor example, Poor ∧Unknown might represent the state of a hapless agent, and a state\nin a package delivery problem might be At(Truck 1, Melbourne) ∧At(Truck 2, Sydney).\nDatabase semantics is used: the closed-world assumption means that any ﬂuents that are not\nmentioned are false, and the unique names assumption means that Truck 1 and Truck 2 are\ndistinct. The following ﬂuents are not allowed in a state: At(x, y) (because it is non-ground),",
  "mentioned are false, and the unique names assumption means that Truck 1 and Truck 2 are\ndistinct. The following ﬂuents are not allowed in a state: At(x, y) (because it is non-ground),\n¬Poor (because it is a negation), and At(Father(Fred), Sydney) (because it uses a function\nsymbol). The representation of states is carefully designed so that a state can be treated\neither as a conjunction of ﬂuents, which can be manipulated by logical inference, or as a set\nof ﬂuents, which can be manipulated with set operations. The set semantics is sometimes\nSET SEMANTICS\neasier to deal with.\nActions are described by a set of action schemas that implicitly deﬁne the ACTIONS(s)\nand RESULT(s, a) functions needed to do a problem-solving search. We saw in Chapter 7 that\nany system for action description needs to solve the frame problem—to say what changes and\nwhat stays the same as the result of the action. Classical planning concentrates on problems\nwhere most actions leave most things unchanged. Think of a world consisting of a bunch of\nobjects on a ﬂat surface. The action of nudging an object causes that object to change its lo-\ncation by a vector Δ. A concise description of the action should mention only Δ; it shouldn’t\nhave to mention all the objects that stay in place. PDDL does that by specifying the result of\nan action in terms of what changes; everything that stays the same is left unmentioned.\nA set of ground (variable-free) actions can be represented by a single action schema.\nACTION SCHEMA\nThe schema is a lifted representation—it lifts the level of reasoning from propositional logic\nto a restricted subset of ﬁrst-order logic. For example, here is an action schema for ﬂying a\nplane from one location to another:\nAction(Fly(p, from, to),\nPRECOND:At(p, from) ∧Plane(p) ∧Airport(from) ∧Airport(to)\nEFFECT:¬At(p, from) ∧At(p, to))\nThe schema consists of the action name, a list of all the variables used in the schema, a\nprecondition and an effect. Although we haven’t said yet how the action schema converts\nPRECONDITION\nEFFECT\ninto logical sentences, think of the variables as being universally quantiﬁed. We are free to\nchoose whatever values we want to instantiate the variables. For example, here is one ground\n1 PDDL was derived from the original STRIPS planning language(Fikes and Nilsson, 1971). which is slightly\nmore restricted than PDDL: STRIPS preconditions and goals cannot contain negative literals. 368\nChapter\n10.\nClassical Planning",
  "1 PDDL was derived from the original STRIPS planning language(Fikes and Nilsson, 1971). which is slightly\nmore restricted than PDDL: STRIPS preconditions and goals cannot contain negative literals. 368\nChapter\n10.\nClassical Planning\naction that results from substituting values for all the variables:\nAction(Fly(P1, SFO, JFK),\nPRECOND:At(P1, SFO) ∧Plane(P1) ∧Airport(SFO) ∧Airport(JFK)\nEFFECT:¬At(P1, SFO) ∧At(P1, JFK))\nThe precondition and effect of an action are each conjunctions of literals (positive or negated\natomic sentences). The precondition deﬁnes the states in which the action can be executed,\nand the effect deﬁnes the result of executing the action. An action a can be executed in state\ns if s entails the precondition of a. Entailment can also be expressed with the set semantics:\ns |= q iff every positive literal in q is in s and every negated literal in q is not. In formal\nnotation we say\n(a ∈ACTIONS(s)) ⇔s |= PRECOND(a) ,\nwhere any variables in a are universally quantiﬁed. For example,\n∀p, from, to (Fly(p, from, to) ∈ACTIONS(s)) ⇔\ns |= (At(p, from) ∧Plane(p) ∧Airport(from) ∧Airport(to))\nWe say that action a is applicable in state s if the preconditions are satisﬁed by s. When\nAPPLICABLE\nan action schema a contains variables, it may have multiple applicable instantiations. For\nexample, with the initial state deﬁned in Figure 10.1, the Fly action can be instantiated as\nFly(P1, SFO, JFK) or as Fly(P2, JFK , SFO), both of which are applicable in the initial\nstate. If an action a has v variables, then, in a domain with k unique names of objects, it takes\nO(vk) time in the worst case to ﬁnd the applicable ground actions.\nSometimes we want to propositionalize a PDDL problem—replace each action schema\nPROPOSITIONALIZE\nwith a set of ground actions and then use a propositional solver such as SATPLAN to ﬁnd a\nsolution. However, this is impractical when v and k are large.\nThe result of executing action a in state s is deﬁned as a state s′ which is represented\nby the set of ﬂuents formed by starting with s, removing the ﬂuents that appear as negative\nliterals in the action’s effects (what we call the delete list or DEL(a)), and adding the ﬂuents\nDELETE LIST\nthat are positive literals in the action’s effects (what we call the add list or ADD(a)):\nADD LIST\nRESULT(s, a) = (s −DEL(a)) ∪ADD(a) .\n(10.1)\nFor example, with the action Fly(P1, SFO, JFK), we would remove At(P1, SFO) and add",
  "DELETE LIST\nthat are positive literals in the action’s effects (what we call the add list or ADD(a)):\nADD LIST\nRESULT(s, a) = (s −DEL(a)) ∪ADD(a) .\n(10.1)\nFor example, with the action Fly(P1, SFO, JFK), we would remove At(P1, SFO) and add\nAt(P1, JFK). It is a requirement of action schemas that any variable in the effect must also\nappear in the precondition. That way, when the precondition is matched against the state s,\nall the variables will be bound, and RESULT(s, a) will therefore have only ground atoms. In\nother words, ground states are closed under the RESULT operation.\nAlso note that the ﬂuents do not explicitly refer to time, as they did in Chapter 7. There\nwe needed superscripts for time, and successor-state axioms of the form\nF t+1 ⇔ActionCausesF t ∨(F t ∧¬ActionCausesNotF t) .\nIn PDDL the times and states are implicit in the action schemas: the precondition always\nrefers to time t and the effect to time t + 1.\nA set of action schemas serves as a deﬁnition of a planning domain. A speciﬁc problem\nwithin the domain is deﬁned with the addition of an initial state and a goal. The initial Section 10.1.\nDeﬁnition of Classical Planning\n369\nInit(At(C1, SFO) ∧At(C2, JFK) ∧At(P1, SFO) ∧At(P2, JFK)\n∧Cargo(C1) ∧Cargo(C2) ∧Plane(P1) ∧Plane(P2)\n∧Airport(JFK) ∧Airport(SFO))\nGoal(At(C1, JFK) ∧At(C2, SFO))\nAction(Load(c, p, a),\nPRECOND: At(c, a) ∧At(p, a) ∧Cargo(c) ∧Plane(p) ∧Airport(a)\nEFFECT: ¬ At(c, a) ∧In(c, p))\nAction(Unload(c, p, a),\nPRECOND: In(c, p) ∧At(p, a) ∧Cargo(c) ∧Plane(p) ∧Airport(a)\nEFFECT: At(c, a) ∧¬ In(c, p))\nAction(Fly(p, from, to),\nPRECOND: At(p, from) ∧Plane(p) ∧Airport(from) ∧Airport(to)\nEFFECT: ¬ At(p, from) ∧At(p, to))\nFigure 10.1\nA PDDL description of an air cargo transportation planning problem.\nstate is a conjunction of ground atoms. (As with all states, the closed-world assumption is\nINITIAL STATE\nused, which means that any atoms that are not mentioned are false.) The goal is just like a\nGOAL\nprecondition: a conjunction of literals (positive or negative) that may contain variables, such\nas At(p, SFO) ∧Plane(p). Any variables are treated as existentially quantiﬁed, so this goal\nis to have any plane at SFO. The problem is solved when we can ﬁnd a sequence of actions\nthat end in a state s that entails the goal. For example, the state Rich ∧Famous ∧Miserable\nentails the goal Rich ∧Famous, and the state Plane(Plane1) ∧At(Plane1, SFO) entails\nthe goal At(p, SFO) ∧Plane(p).",
  "that end in a state s that entails the goal. For example, the state Rich ∧Famous ∧Miserable\nentails the goal Rich ∧Famous, and the state Plane(Plane1) ∧At(Plane1, SFO) entails\nthe goal At(p, SFO) ∧Plane(p).\nNow we have deﬁned planning as a search problem: we have an initial state, an ACTIONS\nfunction, a RESULT function, and a goal test. We’ll look at some example problems before\ninvestigating efﬁcient search algorithms.\n10.1.1\nExample: Air cargo transport\nFigure 10.1 shows an air cargo transport problem involving loading and unloading cargo and\nﬂying it from place to place. The problem can be deﬁned with three actions: Load, Unload,\nand Fly. The actions affect two predicates: In(c, p) means that cargo c is inside plane p, and\nAt(x, a) means that object x (either plane or cargo) is at airport a. Note that some care must\nbe taken to make sure the At predicates are maintained properly. When a plane ﬂies from\none airport to another, all the cargo inside the plane goes with it. In ﬁrst-order logic it would\nbe easy to quantify over all objects that are inside the plane. But basic PDDL does not have\na universal quantiﬁer, so we need a different solution. The approach we use is to say that a\npiece of cargo ceases to be At anywhere when it is In a plane; the cargo only becomes At the\nnew airport when it is unloaded. So At really means “available for use at a given location.”\nThe following plan is a solution to the problem:\n[Load(C1, P1, SFO), Fly(P1, SFO, JFK ), Unload(C1, P1, JFK),\nLoad(C2, P2, JFK ), Fly(P2, JFK , SFO), Unload(C2, P2, SFO)] . 370\nChapter\n10.\nClassical Planning\nFinally, there is the problem of spurious actions such as Fly(P1, JFK, JFK ), which should\nbe a no-op, but which has contradictory effects (according to the deﬁnition, the effect would\ninclude At(P1, JFK) ∧¬At(P1, JFK)). It is common to ignore such problems, because\nthey seldom cause incorrect plans to be produced. The correct approach is to add inequality\npreconditions saying that the from and to airports must be different; see another example of\nthis in Figure 10.3.\n10.1.2\nExample: The spare tire problem\nConsider the problem of changing a ﬂat tire (Figure 10.2). The goal is to have a good spare\ntire properly mounted onto the car’s axle, where the initial state has a ﬂat tire on the axle and\na good spare tire in the trunk. To keep it simple, our version of the problem is an abstract\none, with no sticky lug nuts or other complications. There are just four actions: removing the",
  "a good spare tire in the trunk. To keep it simple, our version of the problem is an abstract\none, with no sticky lug nuts or other complications. There are just four actions: removing the\nspare from the trunk, removing the ﬂat tire from the axle, putting the spare on the axle, and\nleaving the car unattended overnight. We assume that the car is parked in a particularly bad\nneighborhood, so that the effect of leaving it overnight is that the tires disappear. A solution\nto the problem is [Remove(Flat, Axle), Remove(Spare, Trunk), PutOn(Spare, Axle)].\nInit(Tire(Flat) ∧Tire(Spare) ∧At(Flat, Axle) ∧At(Spare, Trunk))\nGoal(At(Spare, Axle))\nAction(Remove(obj , loc),\nPRECOND: At(obj , loc)\nEFFECT: ¬ At(obj , loc) ∧At(obj , Ground))\nAction(PutOn(t, Axle),\nPRECOND: Tire(t) ∧At(t, Ground) ∧¬ At(Flat, Axle)\nEFFECT: ¬ At(t, Ground) ∧At(t, Axle))\nAction(LeaveOvernight,\nPRECOND:\nEFFECT: ¬ At(Spare, Ground) ∧¬ At(Spare, Axle) ∧¬ At(Spare, Trunk)\n∧¬ At(Flat, Ground) ∧¬ At(Flat, Axle) ∧¬ At(Flat, Trunk))\nFigure 10.2\nThe simple spare tire problem.\n10.1.3\nExample: The blocks world\nOne of the most famous planning domains is known as the blocks world. This domain\nBLOCKS WORLD\nconsists of a set of cube-shaped blocks sitting on a table.2 The blocks can be stacked, but\nonly one block can ﬁt directly on top of another. A robot arm can pick up a block and move\nit to another position, either on the table or on top of another block. The arm can pick up\nonly one block at a time, so it cannot pick up a block that has another one on it. The goal will\nalways be to build one or more stacks of blocks, speciﬁed in terms of what blocks are on top\n2 The blocks world used in planning research is much simpler than SHRDLU’s version, shown on page 20. Section 10.1.\nDeﬁnition of Classical Planning\n371\nInit(On(A, Table) ∧On(B, Table) ∧On(C, A)\n∧Block(A) ∧Block(B) ∧Block(C) ∧Clear(B) ∧Clear(C))\nGoal(On(A, B) ∧On(B, C))\nAction(Move(b, x, y),\nPRECOND: On(b, x) ∧Clear(b) ∧Clear(y) ∧Block(b) ∧Block(y) ∧\n(b̸=x) ∧(b̸=y) ∧(x̸=y),\nEFFECT: On(b, y) ∧Clear(x) ∧¬On(b, x) ∧¬Clear(y))\nAction(MoveToTable(b, x),\nPRECOND: On(b, x) ∧Clear(b) ∧Block(b) ∧(b̸=x),\nEFFECT: On(b, Table) ∧Clear(x) ∧¬On(b, x))\nFigure 10.3\nA planning problem in the blocks world: building a three-block tower. One\nsolution is the sequence [MoveToTable(C, A), Move(B, Table, C), Move(A, Table, B)].\nStart State\nGoal State\nB\nA\nC\nA\nB\nC\nFigure 10.4\nDiagram of the blocks-world problem in Figure 10.3.",
  "solution is the sequence [MoveToTable(C, A), Move(B, Table, C), Move(A, Table, B)].\nStart State\nGoal State\nB\nA\nC\nA\nB\nC\nFigure 10.4\nDiagram of the blocks-world problem in Figure 10.3.\nof what other blocks. For example, a goal might be to get block A on B and block B on C\n(see Figure 10.4).\nWe use On(b, x) to indicate that block b is on x, where x is either another block or the\ntable. The action for moving block b from the top of x to the top of y will be Move(b, x, y).\nNow, one of the preconditions on moving b is that no other block be on it. In ﬁrst-order logic,\nthis would be ¬∃x On(x, b) or, alternatively, ∀x ¬On(x, b). Basic PDDL does not allow\nquantiﬁers, so instead we introduce a predicate Clear(x) that is true when nothing is on x.\n(The complete problem description is in Figure 10.3.)\nThe action Move moves a block b from x to y if both b and y are clear. After the move\nis made, b is still clear but y is not. A ﬁrst attempt at the Move schema is\nAction(Move(b, x, y),\nPRECOND:On(b, x) ∧Clear(b) ∧Clear(y),\nEFFECT:On(b, y) ∧Clear(x) ∧¬On(b, x) ∧¬Clear(y)) .\nUnfortunately, this does not maintain Clear properly when x or y is the table. When x is the\nTable, this action has the effect Clear(Table), but the table should not become clear; and\nwhen y = Table, it has the precondition Clear(Table), but the table does not have to be clear 372\nChapter\n10.\nClassical Planning\nfor us to move a block onto it. To ﬁx this, we do two things. First, we introduce another\naction to move a block b from x to the table:\nAction(MoveToTable(b, x),\nPRECOND:On(b, x) ∧Clear(b),\nEFFECT:On(b, Table) ∧Clear(x) ∧¬On(b, x)) .\nSecond, we take the interpretation of Clear(x) to be “there is a clear space on x to hold a\nblock.” Under this interpretation, Clear(Table) will always be true. The only problem is that\nnothing prevents the planner from using Move(b, x, Table) instead of MoveToTable(b, x).\nWe could live with this problem—it will lead to a larger-than-necessary search space, but will\nnot lead to incorrect answers—or we could introduce the predicate Block and add Block(b)∧\nBlock(y) to the precondition of Move.\n10.1.4\nThe complexity of classical planning\nIn this subsection we consider the theoretical complexity of planning and distinguish two\ndecision problems. PlanSAT is the question of whether there exists any plan that solves a\nPLANSAT\nplanning problem. Bounded PlanSAT asks whether there is a solution of length k or less;\nBOUNDED PLANSAT\nthis can be used to ﬁnd an optimal plan.",
  "decision problems. PlanSAT is the question of whether there exists any plan that solves a\nPLANSAT\nplanning problem. Bounded PlanSAT asks whether there is a solution of length k or less;\nBOUNDED PLANSAT\nthis can be used to ﬁnd an optimal plan.\nThe ﬁrst result is that both decision problems are decidable for classical planning. The\nproof follows from the fact that the number of states is ﬁnite. But if we add function symbols\nto the language, then the number of states becomes inﬁnite, and PlanSAT becomes only\nsemidecidable: an algorithm exists that will terminate with the correct answer for any solvable\nproblem, but may not terminate on unsolvable problems. The Bounded PlanSAT problem\nremains decidable even in the presence of function symbols. For proofs of the assertions in\nthis section, see Ghallab et al. (2004).\nBoth PlanSAT and Bounded PlanSAT are in the complexity class PSPACE, a class that\nis larger (and hence more difﬁcult) than NP and refers to problems that can be solved by a\ndeterministic Turing machine with a polynomial amount of space. Even if we make some\nrather severe restrictions, the problems remain quite difﬁcult. For example, if we disallow\nnegative effects, both problems are still NP-hard. However, if we also disallow negative\npreconditions, PlanSAT reduces to the class P.\nThese worst-case results may seem discouraging. We can take solace in the fact that\nagents are usually not asked to ﬁnd plans for arbitrary worst-case problem instances, but\nrather are asked for plans in speciﬁc domains (such as blocks-world problems with n blocks),\nwhich can be much easier than the theoretical worst case. For many domains (including the\nblocks world and the air cargo world), Bounded PlanSAT is NP-complete while PlanSAT is\nin P; in other words, optimal planning is usually hard, but sub-optimal planning is sometimes\neasy. To do well on easier-than-worst-case problems, we will need good search heuristics.\nThat’s the true advantage of the classical planning formalism: it has facilitated the develop-\nment of very accurate domain-independent heuristics, whereas systems based on successor-\nstate axioms in ﬁrst-order logic have had less success in coming up with good heuristics. Section 10.2.\nAlgorithms for Planning as State-Space Search\n373\n10.2\nALGORITHMS FOR PLANNING AS STATE-SPACE SEARCH\nNow we turn our attention to planning algorithms. We saw how the description of a planning",
  "Algorithms for Planning as State-Space Search\n373\n10.2\nALGORITHMS FOR PLANNING AS STATE-SPACE SEARCH\nNow we turn our attention to planning algorithms. We saw how the description of a planning\nproblem deﬁnes a search problem: we can search from the initial state through the space\nof states, looking for a goal. One of the nice advantages of the declarative representation of\naction schemas is that we can also search backward from the goal, looking for the initial state.\nFigure 10.5 compares forward and backward searches.\n10.2.1\nForward (progression) state-space search\nNow that we have shown how a planning problem maps into a search problem, we can solve\nplanning problems with any of the heuristic search algorithms from Chapter 3 or a local\nsearch algorithm from Chapter 4 (provided we keep track of the actions used to reach the\ngoal). From the earliest days of planning research (around 1961) until around 1998 it was\nassumed that forward state-space search was too inefﬁcient to be practical. It is not hard to\ncome up with reasons why.\nFirst, forward search is prone to exploring irrelevant actions. Consider the noble task\nof buying a copy of AI: A Modern Approach from an online bookseller. Suppose there is an\n(a)\n(b)\nAt(P1, A)\nFly(P1, A, B)\nFly(P2, A, B)\nFly(P1, A, B)\nFly(P2, A, B)\nAt(P2, A)\nAt(P1, B)\nAt(P2, A)\nAt(P1, A)\nAt(P2, B)\nAt(P1, B)\nAt(P2, B)\nAt(P1, B)\nAt(P2, A)\nAt(P1, A)\nAt(P2, B)\nFigure 10.5\nTwo approaches to searching for a plan. (a) Forward (progression) search\nthrough the space of states, starting in the initial state and using the problem’s actions to\nsearch forward for a member of the set of goal states. (b) Backward (regression) search\nthrough sets of relevant states, starting at the set of states representing the goal and using the\ninverse of the actions to search backward for the initial state. 374\nChapter\n10.\nClassical Planning\naction schema Buy(isbn) with effect Own(isbn). ISBNs are 10 digits, so this action schema\nrepresents 10 billion ground actions. An uninformed forward-search algorithm would have\nto start enumerating these 10 billion actions to ﬁnd one that leads to the goal.\nSecond, planning problems often have large state spaces. Consider an air cargo problem\nwith 10 airports, where each airport has 5 planes and 20 pieces of cargo. The goal is to move\nall the cargo at airport A to airport B. There is a simple solution to the problem: load the 20",
  "with 10 airports, where each airport has 5 planes and 20 pieces of cargo. The goal is to move\nall the cargo at airport A to airport B. There is a simple solution to the problem: load the 20\npieces of cargo into one of the planes at A, ﬂy the plane to B, and unload the cargo. Finding\nthe solution can be difﬁcult because the average branching factor is huge: each of the 50\nplanes can ﬂy to 9 other airports, and each of the 200 packages can be either unloaded (if\nit is loaded) or loaded into any plane at its airport (if it is unloaded). So in any state there\nis a minimum of 450 actions (when all the packages are at airports with no planes) and a\nmaximum of 10,450 (when all packages and planes are at the same airport). On average, let’s\nsay there are about 2000 possible actions per state, so the search graph up to the depth of the\nobvious solution has about 200041 nodes.\nClearly, even this relatively small problem instance is hopeless without an accurate\nheuristic. Although many real-world applications of planning have relied on domain-speciﬁc\nheuristics, it turns out (as we see in Section 10.2.3) that strong domain-independent heuristics\ncan be derived automatically; that is what makes forward search feasible.\n10.2.2\nBackward (regression) relevant-states search\nIn regression search we start at the goal and apply the actions backward until we ﬁnd a\nsequence of steps that reaches the initial state. It is called relevant-states search because we\nRELEVANT-STATES\nonly consider actions that are relevant to the goal (or current state). As in belief-state search\n(Section 4.4), there is a set of relevant states to consider at each step, not just a single state.\nWe start with the goal, which is a conjunction of literals forming a description of a set of\nstates—for example, the goal ¬Poor ∧Famous describes those states in which Poor is false,\nFamous is true, and any other ﬂuent can have any value. If there are n ground ﬂuents in a\ndomain, then there are 2n ground states (each ﬂuent can be true or false), but 3n descriptions\nof sets of goal states (each ﬂuent can be positive, negative, or not mentioned).\nIn general, backward search works only when we know how to regress from a state\ndescription to the predecessor state description. For example, it is hard to search backwards\nfor a solution to the n-queens problem because there is no easy way to describe the states that\nare one move away from the goal. Happily, the PDDL representation was designed to make",
  "for a solution to the n-queens problem because there is no easy way to describe the states that\nare one move away from the goal. Happily, the PDDL representation was designed to make\nit easy to regress actions—if a domain can be expressed in PDDL, then we can do regression\nsearch on it. Given a ground goal description g and a ground action a, the regression from g\nover a gives us a state description g′ deﬁned by\ng′ = (g −ADD(a)) ∪Precond(a) .\nThat is, the effects that were added by the action need not have been true before, and also\nthe preconditions must have held before, or else the action could not have been executed.\nNote that DEL(a) does not appear in the formula; that’s because while we know the ﬂuents\nin DEL(a) are no longer true after the action, we don’t know whether or not they were true\nbefore, so there’s nothing to be said about them. Section 10.2.\nAlgorithms for Planning as State-Space Search\n375\nTo get the full advantage of backward search, we need to deal with partially uninstanti-\nated actions and states, not just ground ones. For example, suppose the goal is to deliver a spe-\nciﬁc piece of cargo to SFO: At(C2, SFO). That suggests the action Unload(C2, p′, SFO):\nAction(Unload(C2, p′, SFO),\nPRECOND:In(C2, p′) ∧At(p′, SFO) ∧Cargo(C2) ∧Plane(p′) ∧Airport(SFO)\nEFFECT:At(C2, SFO) ∧¬In(C2, p′) .\n(Note that we have standardized variable names (changing p to p′ in this case) so that there\nwill be no confusion between variable names if we happen to use the same action schema\ntwice in a plan. The same approach was used in Chapter 9 for ﬁrst-order logical inference.)\nThis represents unloading the package from an unspeciﬁed plane at SFO; any plane will do,\nbut we need not say which one now. We can take advantage of the power of ﬁrst-order\nrepresentations: a single description summarizes the possibility of using any of the planes by\nimplicitly quantifying over p′. The regressed state description is\ng′ = In(C2, p′) ∧At(p′, SFO) ∧Cargo(C2) ∧Plane(p′) ∧Airport(SFO) .\nThe ﬁnal issue is deciding which actions are candidates to regress over. In the forward direc-\ntion we chose actions that were applicable—those actions that could be the next step in the\nplan. In backward search we want actions that are relevant—those actions that could be the\nRELEVANCE\nlast step in a plan leading up to the current goal state.\nFor an action to be relevant to a goal it obviously must contribute to the goal: at least",
  "plan. In backward search we want actions that are relevant—those actions that could be the\nRELEVANCE\nlast step in a plan leading up to the current goal state.\nFor an action to be relevant to a goal it obviously must contribute to the goal: at least\none of the action’s effects (either positive or negative) must unify with an element of the goal.\nWhat is less obvious is that the action must not have any effect (positive or negative) that\nnegates an element of the goal. Now, if the goal is A ∧B ∧C and an action has the effect\nA∧B∧¬C then there is a colloquial sense in which that action is very relevant to the goal—it\ngets us two-thirds of the way there. But it is not relevant in the technical sense deﬁned here,\nbecause this action could not be the ﬁnal step of a solution—we would always need at least\none more step to achieve C.\nGiven the goal At(C2, SFO), several instantiations of Unload are relevant: we could\nchose any speciﬁc plane to unload from, or we could leave the plane unspeciﬁed by using\nthe action Unload(C2, p′, SFO). We can reduce the branching factor without ruling out any\nsolutions by always using the action formed by substituting the most general uniﬁer into the\n(standardized) action schema.\nAs another example, consider the goal Own(0136042597), given an initial state with\n10 billion ISBNs, and the single action schema\nA = Action(Buy(i), PRECOND:ISBN (i), EFFECT:Own(i)) .\nAs we mentioned before, forward search without a heuristic would have to start enumer-\nating the 10 billion ground Buy actions. But with backward search, we would unify the\ngoal Own(0136042597) with the (standardized) effect Own(i′), yielding the substitution\nθ = {i′/0136042597}. Then we would regress over the action Subst(θ, A′) to yield the\npredecessor state description ISBN (0136042597). This is part of, and thus entailed by, the\ninitial state, so we are done. 376\nChapter\n10.\nClassical Planning\nWe can make this more formal. Assume a goal description g which contains a goal\nliteral gi and an action schema A that is standardized to produce A′. If A′ has an effect literal\ne′\nj where Unify(gi, e′\nj) = θ and where we deﬁne a′ = SUBST(θ, A′) and if there is no effect\nin a′ that is the negation of a literal in g, then a′ is a relevant action towards g.\nBackward search keeps the branching factor lower than forward search, for most prob-\nlem domains. However, the fact that backward search uses state sets rather than individual",
  "Backward search keeps the branching factor lower than forward search, for most prob-\nlem domains. However, the fact that backward search uses state sets rather than individual\nstates makes it harder to come up with good heuristics. That is the main reason why the\nmajority of current systems favor forward search.\n10.2.3\nHeuristics for planning\nNeither forward nor backward search is efﬁcient without a good heuristic function. Recall\nfrom Chapter 3 that a heuristic function h(s) estimates the distance from a state s to the\ngoal and that if we can derive an admissible heuristic for this distance—one that does not\noverestimate—then we can use A∗search to ﬁnd optimal solutions. An admissible heuristic\ncan be derived by deﬁning a relaxed problem that is easier to solve. The exact cost of a\nsolution to this easier problem then becomes the heuristic for the original problem.\nBy deﬁnition, there is no way to analyze an atomic state, and thus it it requires some\ningenuity by a human analyst to deﬁne good domain-speciﬁc heuristics for search problems\nwith atomic states. Planning uses a factored representation for states and action schemas.\nThat makes it possible to deﬁne good domain-independent heuristics and for programs to\nautomatically apply a good domain-independent heuristic for a given problem.\nThink of a search problem as a graph where the nodes are states and the edges are\nactions. The problem is to ﬁnd a path connecting the initial state to a goal state. There are\ntwo ways we can relax this problem to make it easier: by adding more edges to the graph,\nmaking it strictly easier to ﬁnd a path, or by grouping multiple nodes together, forming an\nabstraction of the state space that has fewer states, and thus is easier to search.\nWe look ﬁrst at heuristics that add edges to the graph. For example, the ignore pre-\nconditions heuristic drops all preconditions from actions. Every action becomes applicable\nIGNORE\nPRECONDITIONS\nHEURISTIC\nin every state, and any single goal ﬂuent can be achieved in one step (if there is an applica-\nble action—if not, the problem is impossible). This almost implies that the number of steps\nrequired to solve the relaxed problem is the number of unsatisﬁed goals—almost but not\nquite, because (1) some action may achieve multiple goals and (2) some actions may undo\nthe effects of others. For many problems an accurate heuristic is obtained by considering (1)",
  "quite, because (1) some action may achieve multiple goals and (2) some actions may undo\nthe effects of others. For many problems an accurate heuristic is obtained by considering (1)\nand ignoring (2). First, we relax the actions by removing all preconditions and all effects\nexcept those that are literals in the goal. Then, we count the minimum number of actions\nrequired such that the union of those actions’ effects satisﬁes the goal. This is an instance\nof the set-cover problem. There is one minor irritation: the set-cover problem is NP-hard.\nSET-COVER\nPROBLEM\nFortunately a simple greedy algorithm is guaranteed to return a set covering whose size is\nwithin a factor of log n of the true minimum covering, where n is the number of literals in\nthe goal. Unfortunately, the greedy algorithm loses the guarantee of admissibility.\nIt is also possible to ignore only selected preconditions of actions. Consider the sliding-\nblock puzzle (8-puzzle or 15-puzzle) from Section 3.2. We could encode this as a planning Section 10.2.\nAlgorithms for Planning as State-Space Search\n377\nproblem involving tiles with a single schema Slide:\nAction(Slide(t, s1, s2),\nPRECOND:On(t, s1) ∧Tile(t) ∧Blank(s2) ∧Adjacent(s1, s2)\nEFFECT:On(t, s2) ∧Blank(s1) ∧¬On(t, s1) ∧¬Blank(s2))\nAs we saw in Section 3.6, if we remove the preconditions Blank(s2) ∧Adjacent(s1, s2)\nthen any tile can move in one action to any space and we get the number-of-misplaced-tiles\nheuristic. If we remove Blank(s2) then we get the Manhattan-distance heuristic. It is easy to\nsee how these heuristics could be derived automatically from the action schema description.\nThe ease of manipulating the schemas is the great advantage of the factored representation of\nplanning problems, as compared with the atomic representation of search problems.\nAnother possibility is the ignore delete lists heuristic. Assume for a moment that all\nIGNORE DELETE\nLISTS\ngoals and preconditions contain only positive literals3 We want to create a relaxed version of\nthe original problem that will be easier to solve, and where the length of the solution will serve\nas a good heuristic. We can do that by removing the delete lists from all actions (i.e., removing\nall negative literals from effects). That makes it possible to make monotonic progress towards\nthe goal—no action will ever undo progress made by another action. It turns out it is still NP-\nhard to ﬁnd the optimal solution to this relaxed problem, but an approximate solution can be",
  "the goal—no action will ever undo progress made by another action. It turns out it is still NP-\nhard to ﬁnd the optimal solution to this relaxed problem, but an approximate solution can be\nfound in polynomial time by hill-climbing. Figure 10.6 diagrams part of the state space for\ntwo planning problems using the ignore-delete-lists heuristic. The dots represent states and\nthe edges actions, and the height of each dot above the bottom plane represents the heuristic\nvalue. States on the bottom plane are solutions. In both these problems, there is a wide path\nto the goal. There are no dead ends, so no need for backtracking; a simple hillclimbing search\nwill easily ﬁnd a solution to these problems (although it may not be an optimal solution).\nThe relaxed problems leave us with a simpliﬁed—but still expensive—planning prob-\nlem just to calculate the value of the heuristic function. Many planning problems have 10100\nstates or more, and relaxing the actions does nothing to reduce the number of states. There-\nfore, we now look at relaxations that decrease the number of states by forming a state ab-\nstraction—a many-to-one mapping from states in the ground representation of the problem\nSTATE ABSTRACTION\nto the abstract representation.\nThe easiest form of state abstraction is to ignore some ﬂuents. For example, consider\nan air cargo problem with 10 airports, 50 planes, and 200 pieces of cargo. Each plane can\nbe at one of 10 airports and each package can be either in one of the planes or unloaded at\none of the airports. So there are 5010 × 20050+10 ≈10155 states. Now consider a particular\nproblem in that domain in which it happens that all the packages are at just 5 of the airports,\nand all packages at a given airport have the same destination. Then a useful abstraction of the\nproblem is to drop all the At ﬂuents except for the ones involving one plane and one package\nat each of the 5 airports. Now there are only 510 × 55+10 ≈1017 states. A solution in this\nabstract state space will be shorter than a solution in the original space (and thus will be an\nadmissible heuristic), and the abstract solution is easy to extend to a solution to the original\nproblem (by adding additional Load and Unload actions).\n3 Many problems are written with this convention. For problems that aren’t, replace every negative literal ¬P\nin a goal or precondition with a new positive literal, P ′. 378\nChapter\n10.\nClassical Planning\nFigure 10.6",
  "3 Many problems are written with this convention. For problems that aren’t, replace every negative literal ¬P\nin a goal or precondition with a new positive literal, P ′. 378\nChapter\n10.\nClassical Planning\nFigure 10.6\nTwo state spaces from planning problems with the ignore-delete-lists heuris-\ntic. The height above the bottom plane is the heuristic score of a state; states on the bottom\nplane are goals. There are no local minima, so search for the goal is straightforward. From\nHoffmann (2005).\nA key idea in deﬁning heuristics is decomposition: dividing a problem into parts, solv-\nDECOMPOSITION\ning each part independently, and then combining the parts. The subgoal independence as-\nSUBGOAL\nINDEPENDENCE\nsumption is that the cost of solving a conjunction of subgoals is approximated by the sum\nof the costs of solving each subgoal independently. The subgoal independence assumption\ncan be optimistic or pessimistic. It is optimistic when there are negative interactions between\nthe subplans for each subgoal—for example, when an action in one subplan deletes a goal\nachieved by another subplan. It is pessimistic, and therefore inadmissible, when subplans\ncontain redundant actions—for instance, two actions that could be replaced by a single action\nin the merged plan.\nSuppose the goal is a set of ﬂuents G, which we divide into disjoint subsets G1, . . . , Gn.\nWe then ﬁnd plans P1, . . . , Pn that solve the respective subgoals. What is an estimate of the\ncost of the plan for achieving all of G? We can think of each Cost(Pi) as a heuristic estimate,\nand we know that if we combine estimates by taking their maximum value, we always get an\nadmissible heuristic. So maxi COST(Pi) is admissible, and sometimes it is exactly correct:\nit could be that P1 serendipitously achieves all the Gi. But in most cases, in practice the\nestimate is too low. Could we sum the costs instead? For many problems that is a reasonable\nestimate, but it is not admissible. The best case is when we can determine that Gi and Gj are\nindependent. If the effects of Pi leave all the preconditions and goals of Pj unchanged, then\nthe estimate COST(Pi) + COST(Pj) is admissible, and more accurate than the max estimate.\nWe show in Section 10.3.1 that planning graphs can help provide better heuristic estimates.\nIt is clear that there is great potential for cutting down the search space by forming ab-\nstractions. The trick is choosing the right abstractions and using them in a way that makes",
  "It is clear that there is great potential for cutting down the search space by forming ab-\nstractions. The trick is choosing the right abstractions and using them in a way that makes\nthe total cost—deﬁning an abstraction, doing an abstract search, and mapping the abstraction\nback to the original problem—less than the cost of solving the original problem. The tech- Section 10.3.\nPlanning Graphs\n379\nniques of pattern databases from Section 3.6.3 can be useful, because the cost of creating\nthe pattern database can be amortized over multiple problem instances.\nAn example of a system that makes use of effective heuristics is FF, or FASTFORWARD\n(Hoffmann, 2005), a forward state-space searcher that uses the ignore-delete-lists heuristic,\nestimating the heuristic with the help of a planning graph (see Section 10.3). FF then uses\nhill-climbing search (modiﬁed to keep track of the plan) with the heuristic to ﬁnd a solution.\nWhen it hits a plateau or local maximum—when no action leads to a state with better heuristic\nscore—then FF uses iterative deepening search until it ﬁnds a state that is better, or it gives\nup and restarts hill-climbing.\n10.3\nPLANNING GRAPHS\nAll of the heuristics we have suggested can suffer from inaccuracies. This section shows\nhow a special data structure called a planning graph can be used to give better heuristic\nPLANNING GRAPH\nestimates. These heuristics can be applied to any of the search techniques we have seen so\nfar. Alternatively, we can search for a solution over the space formed by the planning graph,\nusing an algorithm called GRAPHPLAN.\nA planning problem asks if we can reach a goal state from the initial state. Suppose we\nare given a tree of all possible actions from the initial state to successor states, and their suc-\ncessors, and so on. If we indexed this tree appropriately, we could answer the planning ques-\ntion “can we reach state G from state S0” immediately, just by looking it up. Of course, the\ntree is of exponential size, so this approach is impractical. A planning graph is polynomial-\nsize approximation to this tree that can be constructed quickly. The planning graph can’t\nanswer deﬁnitively whether G is reachable from S0, but it can estimate how many steps it\ntakes to reach G. The estimate is always correct when it reports the goal is not reachable, and\nit never overestimates the number of steps, so it is an admissible heuristic.\nA planning graph is a directed graph organized into levels: ﬁrst a level S0 for the initial",
  "it never overestimates the number of steps, so it is an admissible heuristic.\nA planning graph is a directed graph organized into levels: ﬁrst a level S0 for the initial\nLEVEL\nstate, consisting of nodes representing each ﬂuent that holds in S0; then a level A0 consisting\nof nodes for each ground action that might be applicable in S0; then alternating levels Si\nfollowed by Ai; until we reach a termination condition (to be discussed later).\nRoughly speaking, Si contains all the literals that could hold at time i, depending on\nthe actions executed at preceding time steps. If it is possible that either P or ¬P could hold,\nthen both will be represented in Si. Also roughly speaking, Ai contains all the actions that\ncould have their preconditions satisﬁed at time i. We say “roughly speaking” because the\nplanning graph records only a restricted subset of the possible negative interactions among\nactions; therefore, a literal might show up at level Sj when actually it could not be true until\na later level, if at all. (A literal will never show up too late.) Despite the possible error, the\nlevel j at which a literal ﬁrst appears is a good estimate of how difﬁcult it is to achieve the\nliteral from the initial state.\nPlanning graphs work only for propositional planning problems—ones with no vari-\nables. As we mentioned on page 368, it is straightforward to propositionalize a set of ac- 380\nChapter\n10.\nClassical Planning\nInit(Have(Cake))\nGoal(Have(Cake) ∧Eaten(Cake))\nAction(Eat(Cake)\nPRECOND: Have(Cake)\nEFFECT: ¬ Have(Cake) ∧Eaten(Cake))\nAction(Bake(Cake)\nPRECOND: ¬ Have(Cake)\nEFFECT: Have(Cake))\nFigure 10.7\nThe “have cake and eat cake too” problem.\nBake(Cake)\nEat(Cake)\nHave(Cake)\nS0\nA0\nS1\nA1\nS2\nHave(Cake)\nHave(Cake)\nHave(Cake)\nHave(Cake)\nEaten(Cake)\nEaten(Cake)\nEaten(Cake)\nEaten(Cake)\nEaten(Cake)\nEat(Cake)\n¬\n¬\n¬\n¬\n¬\nFigure 10.8\nThe planning graph for the “have cake and eat cake too” problem up to level\nS2. Rectangles indicate actions (small squares indicate persistence actions), and straight\nlines indicate preconditions and effects. Mutex links are shown as curved gray lines. Not all\nmutex links are shown, because the graph would be too cluttered. In general, if two literals\nare mutex at Si, then the persistence actions for those literals will be mutex at Ai and we\nneed not draw that mutex link.\ntion schemas. Despite the resulting increase in the size of the problem description, planning\ngraphs have proved to be effective tools for solving hard planning problems.",
  "need not draw that mutex link.\ntion schemas. Despite the resulting increase in the size of the problem description, planning\ngraphs have proved to be effective tools for solving hard planning problems.\nFigure 10.7 shows a simple planning problem, and Figure 10.8 shows its planning\ngraph. Each action at level Ai is connected to its preconditions at Si and its effects at Si+1.\nSo a literal appears because an action caused it, but we also want to say that a literal can\npersist if no action negates it. This is represented by a persistence action (sometimes called\nPERSISTENCE\nACTION\na no-op). For every literal C, we add to the problem a persistence action with precondition C\nand effect C. Level A0 in Figure 10.8 shows one “real” action, Eat(Cake), along with two\npersistence actions drawn as small square boxes.\nLevel A0 contains all the actions that could occur in state S0, but just as important it\nrecords conﬂicts between actions that would prevent them from occurring together. The gray\nlines in Figure 10.8 indicate mutual exclusion (or mutex) links. For example, Eat(Cake) is\nMUTUAL EXCLUSION\nMUTEX\nmutually exclusive with the persistence of either Have(Cake) or ¬Eaten(Cake). We shall\nsee shortly how mutex links are computed.\nLevel S1 contains all the literals that could result from picking any subset of the actions\nin A0, as well as mutex links (gray lines) indicating literals that could not appear together,\nregardless of the choice of actions. For example, Have(Cake) and Eaten(Cake) are mutex: Section 10.3.\nPlanning Graphs\n381\ndepending on the choice of actions in A0, either, but not both, could be the result. In other\nwords, S1 represents a belief state: a set of possible states. The members of this set are all\nsubsets of the literals such that there is no mutex link between any members of the subset.\nWe continue in this way, alternating between state level Si and action level Ai until we\nreach a point where two consecutive levels are identical. At this point, we say that the graph\nhas leveled off. The graph in Figure 10.8 levels off at S2.\nLEVELED OFF\nWhat we end up with is a structure where every Ai level contains all the actions that are\napplicable in Si, along with constraints saying that two actions cannot both be executed at the\nsame level. Every Si level contains all the literals that could result from any possible choice\nof actions in Ai−1, along with constraints saying which pairs of literals are not possible.",
  "same level. Every Si level contains all the literals that could result from any possible choice\nof actions in Ai−1, along with constraints saying which pairs of literals are not possible.\nIt is important to note that the process of constructing the planning graph does not require\nchoosing among actions, which would entail combinatorial search. Instead, it just records the\nimpossibility of certain choices using mutex links.\nWe now deﬁne mutex links for both actions and literals. A mutex relation holds between\ntwo actions at a given level if any of the following three conditions holds:\n• Inconsistent effects: one action negates an effect of the other. For example, Eat(Cake)\nand the persistence of Have(Cake) have inconsistent effects because they disagree on\nthe effect Have(Cake).\n• Interference: one of the effects of one action is the negation of a precondition of the\nother. For example Eat(Cake) interferes with the persistence of Have(Cake) by negat-\ning its precondition.\n• Competing needs: one of the preconditions of one action is mutually exclusive with a\nprecondition of the other. For example, Bake(Cake) and Eat(Cake) are mutex because\nthey compete on the value of the Have(Cake) precondition.\nA mutex relation holds between two literals at the same level if one is the negation of the other\nor if each possible pair of actions that could achieve the two literals is mutually exclusive.\nThis condition is called inconsistent support. For example, Have(Cake) and Eaten(Cake)\nare mutex in S1 because the only way of achieving Have(Cake), the persistence action, is\nmutex with the only way of achieving Eaten(Cake), namely Eat(Cake). In S2 the two\nliterals are not mutex, because there are new ways of achieving them, such as Bake(Cake)\nand the persistence of Eaten(Cake), that are not mutex.\nA planning graph is polynomial in the size of the planning problem. For a planning\nproblem with l literals and a actions, each Si has no more than l nodes and l2 mutex links,\nand each Ai has no more than a + l nodes (including the no-ops), (a + l)2 mutex links, and\n2(al + l) precondition and effect links. Thus, an entire graph with n levels has a size of\nO(n(a + l)2). The time to build the graph has the same complexity.\n10.3.1\nPlanning graphs for heuristic estimation\nA planning graph, once constructed, is a rich source of information about the problem. First,\nif any goal literal fails to appear in the ﬁnal level of the graph, then the problem is unsolvable.",
  "10.3.1\nPlanning graphs for heuristic estimation\nA planning graph, once constructed, is a rich source of information about the problem. First,\nif any goal literal fails to appear in the ﬁnal level of the graph, then the problem is unsolvable.\nSecond, we can estimate the cost of achieving any goal literal gi from state s as the level at\nwhich gi ﬁrst appears in the planning graph constructed from initial state s. We call this the 382\nChapter\n10.\nClassical Planning\nlevel cost of gi. In Figure 10.8, Have(Cake) has level cost 0 and Eaten(Cake) has level cost\nLEVEL COST\n1. It is easy to show (Exercise 10.10) that these estimates are admissible for the individual\ngoals. The estimate might not always be accurate, however, because planning graphs allow\nseveral actions at each level, whereas the heuristic counts just the level and not the number\nof actions. For this reason, it is common to use a serial planning graph for computing\nSERIAL PLANNING\nGRAPH\nheuristics. A serial graph insists that only one action can actually occur at any given time\nstep; this is done by adding mutex links between every pair of nonpersistence actions. Level\ncosts extracted from serial graphs are often quite reasonable estimates of actual costs.\nTo estimate the cost of a conjunction of goals, there are three simple approaches. The\nmax-level heuristic simply takes the maximum level cost of any of the goals; this is admissi-\nMAX-LEVEL\nble, but not necessarily accurate.\nThe level sum heuristic, following the subgoal independence assumption, returns the\nLEVEL SUM\nsum of the level costs of the goals; this can be inadmissible but works well in practice\nfor problems that are largely decomposable. It is much more accurate than the number-\nof-unsatisﬁed-goals heuristic from Section 10.2. For our problem, the level-sum heuristic\nestimate for the conjunctive goal Have(Cake) ∧Eaten(Cake) will be 0 + 1 = 1, whereas\nthe correct answer is 2, achieved by the plan [Eat(Cake), Bake(Cake)]. That doesn’t seem\nso bad. A more serious error is that if Bake(Cake) were not in the set of actions, then the\nestimate would still be 1, when in fact the conjunctive goal would be impossible.\nFinally, the set-level heuristic ﬁnds the level at which all the literals in the conjunctive\nSET-LEVEL\ngoal appear in the planning graph without any pair of them being mutually exclusive. This\nheuristic gives the correct values of 2 for our original problem and inﬁnity for the problem",
  "SET-LEVEL\ngoal appear in the planning graph without any pair of them being mutually exclusive. This\nheuristic gives the correct values of 2 for our original problem and inﬁnity for the problem\nwithout Bake(Cake). It is admissible, it dominates the max-level heuristic, and it works\nextremely well on tasks in which there is a good deal of interaction among subplans. It is not\nperfect, of course; for example, it ignores interactions among three or more literals.\nAs a tool for generating accurate heuristics, we can view the planning graph as a relaxed\nproblem that is efﬁciently solvable. To understand the nature of the relaxed problem, we\nneed to understand exactly what it means for a literal g to appear at level Si in the planning\ngraph. Ideally, we would like it to be a guarantee that there exists a plan with i action levels\nthat achieves g, and also that if g does not appear, there is no such plan. Unfortunately,\nmaking that guarantee is as difﬁcult as solving the original planning problem. So the planning\ngraph makes the second half of the guarantee (if g does not appear, there is no plan), but\nif g does appear, then all the planning graph promises is that there is a plan that possibly\nachieves g and has no “obvious” ﬂaws. An obvious ﬂaw is deﬁned as a ﬂaw that can be\ndetected by considering two actions or two literals at a time—in other words, by looking at\nthe mutex relations. There could be more subtle ﬂaws involving three, four, or more actions,\nbut experience has shown that it is not worth the computational effort to keep track of these\npossible ﬂaws. This is similar to a lesson learned from constraint satisfaction problems—that\nit is often worthwhile to compute 2-consistency before searching for a solution, but less often\nworthwhile to compute 3-consistency or higher. (See page 211.)\nOne example of an unsolvable problem that cannot be recognized as such by a planning\ngraph is the blocks-world problem where the goal is to get block A on B, B on C, and C on\nA. This is an impossible goal; a tower with the bottom on top of the top. But a planning graph Section 10.3.\nPlanning Graphs\n383\ncannot detect the impossibility, because any two of the three subgoals are achievable. There\nare no mutexes between any pair of literals, only between the three as a whole. To detect that\nthis problem is impossible, we would have to search over the planning graph.\n10.3.2\nThe GRAPHPLAN algorithm",
  "are no mutexes between any pair of literals, only between the three as a whole. To detect that\nthis problem is impossible, we would have to search over the planning graph.\n10.3.2\nThe GRAPHPLAN algorithm\nThis subsection shows how to extract a plan directly from the planning graph, rather than just\nusing the graph to provide a heuristic. The GRAPHPLAN algorithm (Figure 10.9) repeatedly\nadds a level to a planning graph with EXPAND-GRAPH. Once all the goals show up as non-\nmutex in the graph, GRAPHPLAN calls EXTRACT-SOLUTION to search for a plan that solves\nthe problem. If that fails, it expands another level and tries again, terminating with failure\nwhen there is no reason to go on.\nfunction GRAPHPLAN(problem) returns solution or failure\ngraph ←INITIAL-PLANNING-GRAPH(problem)\ngoals ←CONJUNCTS(problem.GOAL)\nnogoods ←an empty hash table\nfor tl = 0 to ∞do\nif goals all non-mutex in St of graph then\nsolution ←EXTRACT-SOLUTION(graph, goals, NUMLEVELS(graph), nogoods)\nif solution ̸= failure then return solution\nif graph and nogoods have both leveled off then return failure\ngraph ←EXPAND-GRAPH(graph, problem)\nFigure 10.9\nThe GRAPHPLAN algorithm. GRAPHPLAN calls EXPAND-GRAPH to add a\nlevel until either a solution is found by EXTRACT-SOLUTION, or no solution is possible.\nLet us now trace the operation of GRAPHPLAN on the spare tire problem from page 370.\nThe graph is shown in Figure 10.10. The ﬁrst line of GRAPHPLAN initializes the planning\ngraph to a one-level (S0) graph representing the initial state. The positive ﬂuents from the\nproblem description’s initial state are shown, as are the relevant negative ﬂuents. Not shown\nare the unchanging positive literals (such as Tire(Spare)) and the irrelevant negative literals.\nThe goal At(Spare, Axle) is not present in S0, so we need not call EXTRACT-SOLUTION—\nwe are certain that there is no solution yet. Instead, EXPAND-GRAPH adds into A0 the three\nactions whose preconditions exist at level S0 (i.e., all the actions except PutOn(Spare, Axle)),\nalong with persistence actions for all the literals in S0. The effects of the actions are added at\nlevel S1. EXPAND-GRAPH then looks for mutex relations and adds them to the graph.\nAt(Spare, Axle) is still not present in S1, so again we do not call EXTRACT-SOLUTION.\nWe call EXPAND-GRAPH again, adding A1 and S1 and giving us the planning graph shown\nin Figure 10.10. Now that we have the full complement of actions, it is worthwhile to look at",
  "We call EXPAND-GRAPH again, adding A1 and S1 and giving us the planning graph shown\nin Figure 10.10. Now that we have the full complement of actions, it is worthwhile to look at\nsome of the examples of mutex relations and their causes:\n• Inconsistent effects: Remove(Spare, Trunk) is mutex with LeaveOvernight because\none has the effect At(Spare, Ground) and the other has its negation. 384\nChapter\n10.\nClassical Planning\nS0\nA1\nS2\nAt(Spare,Trunk)\nAt(Spare,Trunk)\nAt(Flat,Axle)\nAt(Flat,Axle)\nAt(Spare,Axle)\nAt(Flat,Ground)\nAt(Flat,Ground)\nAt(Spare,Ground)\nAt(Spare,Ground)\nAt(Spare,Trunk)\nAt(Spare,Trunk)\nAt(Flat,Axle)\nAt(Flat,Axle)\nAt(Spare,Axle)\nAt(Flat,Ground)\nAt(Flat,Ground)\nAt(Spare,Ground)\nAt(Spare,Ground)\nAt(Spare,Axle)\nAt(Spare,Trunk)\nAt(Flat,Axle)\nAt(Spare,Axle)\nAt(Flat,Ground)\nAt(Spare,Ground)\nPutOn(Spare,Axle)\nLeaveOvernight\nRemove(Flat,Axle)\nRemove(Spare,Trunk)\nRemove(Spare,Trunk)\nRemove(Flat,Axle)\nLeaveOvernight\n¬\n¬\n¬\n¬\n¬\n¬\n¬\n¬\n¬\n¬\n¬\n¬\n¬\nA0\nS1\nFigure 10.10\nThe planning graph for the spare tire problem after expansion to level S2.\nMutex links are shown as gray lines. Not all links are shown, because the graph would be too\ncluttered if we showed them all. The solution is indicated by bold lines and outlines.\n• Interference: Remove(Flat, Axle) is mutex with LeaveOvernight because one has the\nprecondition At(Flat, Axle) and the other has its negation as an effect.\n• Competing needs: PutOn(Spare, Axle) is mutex with Remove(Flat, Axle) because\none has At(Flat, Axle) as a precondition and the other has its negation.\n• Inconsistent support: At(Spare, Axle) is mutex with At(Flat, Axle) in S2 because the\nonly way of achieving At(Spare, Axle) is by PutOn(Spare, Axle), and that is mutex\nwith the persistence action that is the only way of achieving At(Flat, Axle). Thus, the\nmutex relations detect the immediate conﬂict that arises from trying to put two objects\nin the same place at the same time.\nThis time, when we go back to the start of the loop, all the literals from the goal are present\nin S2, and none of them is mutex with any other. That means that a solution might exist,\nand EXTRACT-SOLUTION will try to ﬁnd it. We can formulate EXTRACT-SOLUTION as a\nBoolean constraint satisfaction problem (CSP) where the variables are the actions at each\nlevel, the values for each variable are in or out of the plan, and the constraints are the mutexes\nand the need to satisfy each goal and precondition.",
  "Boolean constraint satisfaction problem (CSP) where the variables are the actions at each\nlevel, the values for each variable are in or out of the plan, and the constraints are the mutexes\nand the need to satisfy each goal and precondition.\nAlternatively, we can deﬁne EXTRACT-SOLUTION as a backward search problem, where\neach state in the search contains a pointer to a level in the planning graph and a set of unsat-\nisﬁed goals. We deﬁne this search problem as follows:\n• The initial state is the last level of the planning graph, Sn, along with the set of goals\nfrom the planning problem.\n• The actions available in a state at level Si are to select any conﬂict-free subset of the\nactions in Ai−1 whose effects cover the goals in the state. The resulting state has level\nSi−1 and has as its set of goals the preconditions for the selected set of actions. By\n“conﬂict free,” we mean a set of actions such that no two of them are mutex and no two\nof their preconditions are mutex. Section 10.3.\nPlanning Graphs\n385\n• The goal is to reach a state at level S0 such that all the goals are satisﬁed.\n• The cost of each action is 1.\nFor this particular problem, we start at S2 with the goal At(Spare, Axle). The only choice we\nhave for achieving the goal set is PutOn(Spare, Axle). That brings us to a search state at S1\nwith goals At(Spare, Ground) and ¬At(Flat, Axle). The former can be achieved only by\nRemove(Spare, Trunk), and the latter by either Remove(Flat, Axle) or LeaveOvernight.\nBut LeaveOvernight is mutex with Remove(Spare, Trunk), so the only solution is to choose\nRemove(Spare, Trunk) and Remove(Flat, Axle). That brings us to a search state at S0 with\nthe goals At(Spare, Trunk) and At(Flat, Axle). Both of these are present in the state, so\nwe have a solution: the actions Remove(Spare, Trunk) and Remove(Flat, Axle) in level\nA0, followed by PutOn(Spare, Axle) in A1.\nIn the case where EXTRACT-SOLUTION fails to ﬁnd a solution for a set of goals at\na given level, we record the (level, goals) pair as a no-good, just as we did in constraint\nlearning for CSPs (page 220). Whenever EXTRACT-SOLUTION is called again with the same\nlevel and goals, we can ﬁnd the recorded no-good and immediately return failure rather than\nsearching again. We see shortly that no-goods are also used in the termination test.\nWe know that planning is PSPACE-complete and that constructing the planning graph\ntakes polynomial time, so it must be the case that solution extraction is intractable in the worst",
  "We know that planning is PSPACE-complete and that constructing the planning graph\ntakes polynomial time, so it must be the case that solution extraction is intractable in the worst\ncase. Therefore, we will need some heuristic guidance for choosing among actions during the\nbackward search. One approach that works well in practice is a greedy algorithm based on\nthe level cost of the literals. For any set of goals, we proceed in the following order:\n1. Pick ﬁrst the literal with the highest level cost.\n2. To achieve that literal, prefer actions with easier preconditions. That is, choose an action\nsuch that the sum (or maximum) of the level costs of its preconditions is smallest.\n10.3.3\nTermination of GRAPHPLAN\nSo far, we have skated over the question of termination. Here we show that GRAPHPLAN will\nin fact terminate and return failure when there is no solution.\nThe ﬁrst thing to understand is why we can’t stop expanding the graph as soon as it has\nleveled off. Consider an air cargo domain with one plane and n pieces of cargo at airport\nA, all of which have airport B as their destination. In this version of the problem, only one\npiece of cargo can ﬁt in the plane at a time. The graph will level off at level 4, reﬂecting the\nfact that for any single piece of cargo, we can load it, ﬂy it, and unload it at the destination in\nthree steps. But that does not mean that a solution can be extracted from the graph at level 4;\nin fact a solution will require 4n −1 steps: for each piece of cargo we load, ﬂy, and unload,\nand for all but the last piece we need to ﬂy back to airport A to get the next piece.\nHow long do we have to keep expanding after the graph has leveled off? If the function\nEXTRACT-SOLUTION fails to ﬁnd a solution, then there must have been at least one set of\ngoals that were not achievable and were marked as a no-good. So if it is possible that there\nmight be fewer no-goods in the next level, then we should continue. As soon as the graph\nitself and the no-goods have both leveled off, with no solution found, we can terminate with\nfailure because there is no possibility of a subsequent change that could add a solution. 386\nChapter\n10.\nClassical Planning\nNow all we have to do is prove that the graph and the no-goods will always level off. The\nkey to this proof is that certain properties of planning graphs are monotonically increasing or\ndecreasing. “X increases monotonically” means that the set of Xs at level i + 1 is a superset",
  "key to this proof is that certain properties of planning graphs are monotonically increasing or\ndecreasing. “X increases monotonically” means that the set of Xs at level i + 1 is a superset\n(not necessarily proper) of the set at level i. The properties are as follows:\n• Literals increase monotonically: Once a literal appears at a given level, it will appear\nat all subsequent levels. This is because of the persistence actions; once a literal shows\nup, persistence actions cause it to stay forever.\n• Actions increase monotonically: Once an action appears at a given level, it will appear\nat all subsequent levels. This is a consequence of the monotonic increase of literals; if\nthe preconditions of an action appear at one level, they will appear at subsequent levels,\nand thus so will the action.\n• Mutexes decrease monotonically: If two actions are mutex at a given level Ai, then they\nwill also be mutex for all previous levels at which they both appear. The same holds for\nmutexes between literals. It might not always appear that way in the ﬁgures, because\nthe ﬁgures have a simpliﬁcation: they display neither literals that cannot hold at level\nSi nor actions that cannot be executed at level Ai. We can see that “mutexes decrease\nmonotonically” is true if you consider that these invisible literals and actions are mutex\nwith everything.\nThe proof can be handled by cases: if actions A and B are mutex at level Ai, it\nmust be because of one of the three types of mutex. The ﬁrst two, inconsistent effects\nand interference, are properties of the actions themselves, so if the actions are mutex\nat Ai, they will be mutex at every level. The third case, competing needs, depends on\nconditions at level Si: that level must contain a precondition of A that is mutex with\na precondition of B. Now, these two preconditions can be mutex if they are negations\nof each other (in which case they would be mutex in every level) or if all actions for\nachieving one are mutex with all actions for achieving the other. But we already know\nthat the available actions are increasing monotonically, so, by induction, the mutexes\nmust be decreasing.\n• No-goods decrease monotonically: If a set of goals is not achievable at a given level,\nthen they are not achievable in any previous level. The proof is by contradiction: if they\nwere achievable at some previous level, then we could just add persistence actions to\nmake them achievable at a subsequent level.",
  "then they are not achievable in any previous level. The proof is by contradiction: if they\nwere achievable at some previous level, then we could just add persistence actions to\nmake them achievable at a subsequent level.\nBecause the actions and literals increase monotonically and because there are only a ﬁnite\nnumber of actions and literals, there must come a level that has the same number of actions\nand literals as the previous level. Because mutexes and no-goods decrease, and because there\ncan never be fewer than zero mutexes or no-goods, there must come a level that has the\nsame number of mutexes and no-goods as the previous level. Once a graph has reached this\nstate, then if one of the goals is missing or is mutex with another goal, then we can stop the\nGRAPHPLAN algorithm and return failure. That concludes a sketch of the proof; for more\ndetails see Ghallab et al. (2004). Section 10.4.\nOther Classical Planning Approaches\n387\nYear\nTrack\nWinning Systems (approaches)\n2008\nOptimal\nGAMER (model checking, bidirectional search)\n2008\nSatisﬁcing\nLAMA (fast downward search with FF heuristic)\n2006\nOptimal\nSATPLAN, MAXPLAN (Boolean satisﬁability)\n2006\nSatisﬁcing\nSGPLAN (forward search; partitions into independent subproblems)\n2004\nOptimal\nSATPLAN (Boolean satisﬁability)\n2004\nSatisﬁcing\nFAST DIAGONALLY DOWNWARD (forward search with causal graph)\n2002\nAutomated\nLPG (local search, planning graphs converted to CSPs)\n2002\nHand-coded\nTLPLAN (temporal action logic with control rules for forward search)\n2000\nAutomated\nFF (forward search)\n2000\nHand-coded\nTALPLANNER (temporal action logic with control rules for forward search)\n1998\nAutomated\nIPP (planning graphs); HSP (forward search)\nFigure 10.11\nSome of the top-performing systems in the International Planning Compe-\ntition. Each year there are various tracks: “Optimal” means the planners must produce the\nshortest possible plan, while “Satisﬁcing” means nonoptimal solutions are accepted. “Hand-\ncoded” means domain-speciﬁc heuristics are allowed; “Automated” means they are not.\n10.4\nOTHER CLASSICAL PLANNING APPROACHES\nCurrently the most popular and effective approaches to fully automated planning are:\n• Translating to a Boolean satisﬁability (SAT) problem\n• Forward state-space search with carefully crafted heuristics (Section 10.2)\n• Search using a planning graph (Section 10.3)\nThese three approaches are not the only ones tried in the 40-year history of automated plan-",
  "• Forward state-space search with carefully crafted heuristics (Section 10.2)\n• Search using a planning graph (Section 10.3)\nThese three approaches are not the only ones tried in the 40-year history of automated plan-\nning. Figure 10.11 shows some of the top systems in the International Planning Competitions,\nwhich have been held every even year since 1998. In this section we ﬁrst describe the transla-\ntion to a satisﬁability problem and then describe three other inﬂuential approaches: planning\nas ﬁrst-order logical deduction; as constraint satisfaction; and as plan reﬁnement.\n10.4.1\nClassical planning as Boolean satisﬁability\nIn Section 7.7.4 we saw how SATPLAN solves planning problems that are expressed in propo-\nsitional logic. Here we show how to translate a PDDL description into a form that can be\nprocessed by SATPLAN. The translation is a series of straightforward steps:\n• Propositionalize the actions: replace each action schema with a set of ground actions\nformed by substituting constants for each of the variables. These ground actions are not\npart of the translation, but will be used in subsequent steps.\n• Deﬁne the initial state: assert F 0 for every ﬂuent F in the problem’s initial state, and\n¬F for every ﬂuent not mentioned in the initial state.\n• Propositionalize the goal: for every variable in the goal, replace the literals that contain\nthe variable with a disjunction over constants. For example, the goal of having block A 388\nChapter\n10.\nClassical Planning\non another block, On(A, x) ∧Block(x) in a world with objects A, B and C, would be\nreplaced by the goal\n(On(A, A) ∧Block(A)) ∨(On(A, B) ∧Block(B)) ∨(On(A, C) ∧Block(C)) .\n• Add successor-state axioms: For each ﬂuent F, add an axiom of the form\nF t+1 ⇔ActionCausesF t ∨(F t ∧¬ActionCausesNotF t) ,\nwhere ActionCausesF is a disjunction of all the ground actions that have F in their\nadd list, and ActionCausesNotF is a disjunction of all the ground actions that have F\nin their delete list.\n• Add precondition axioms: For each ground action A, add the axiom At ⇒PRE(A)t,\nthat is, if an action is taken at time t, then the preconditions must have been true.\n• Add action exclusion axioms: say that every action is distinct from every other action.\nThe resulting translation is in the form that we can hand to SATPLAN to ﬁnd a solution.\n10.4.2\nPlanning as ﬁrst-order logical deduction: Situation calculus\nPDDL is a language that carefully balances the expressiveness of the language with the com-",
  "The resulting translation is in the form that we can hand to SATPLAN to ﬁnd a solution.\n10.4.2\nPlanning as ﬁrst-order logical deduction: Situation calculus\nPDDL is a language that carefully balances the expressiveness of the language with the com-\nplexity of the algorithms that operate on it. But some problems remain difﬁcult to express in\nPDDL. For example, we can’t express the goal “move all the cargo from A to B regardless\nof how many pieces of cargo there are” in PDDL, but we can do it in ﬁrst-order logic, using a\nuniversal quantiﬁer. Likewise, ﬁrst-order logic can concisely express global constraints such\nas “no more than four robots can be in the same place at the same time.” PDDL can only say\nthis with repetitious preconditions on every possible action that involves a move.\nThe propositional logic representation of planning problems also has limitations, such\nas the fact that the notion of time is tied directly to ﬂuents. For example, South2 means\n“the agent is facing south at time 2.” With that representation, there is no way to say “the\nagent would be facing south at time 2 if it executed a right turn at time 1; otherwise it would\nbe facing east.” First-order logic lets us get around this limitation by replacing the notion\nof linear time with a notion of branching situations, using a representation called situation\ncalculus that works like this:\nSITUATION\nCALCULUS\n• The initial state is called a situation.\nIf s is a situation and a is an action, then\nSITUATION\nRESULT(s, a) is also a situation. There are no other situations. Thus, a situation cor-\nresponds to a sequence, or history, of actions. You can also think of a situation as the\nresult of applying the actions, but note that two situations are the same only if their start\nand actions are the same: (RESULT(s, a) = RESULT(s′, a′)) ⇔(s = s′ ∧a = a′).\nSome examples of actions and situations are shown in Figure 10.12.\n• A function or relation that can vary from one situation to the next is a ﬂuent. By conven-\ntion, the situation s is always the last argument to the ﬂuent, for example At(x, l, s) is a\nrelational ﬂuent that is true when object x is at location l in situation s, and Location is a\nfunctional ﬂuent such that Location(x, s) = l holds in the same situations as At(x, l, s).\n• Each action’s preconditions are described with a possibility axiom that says when the\nPOSSIBILITY AXIOM\naction can be taken. It has the form Φ(s) ⇒Poss(a, s) where Φ(s) is some formula Section 10.4.",
  "• Each action’s preconditions are described with a possibility axiom that says when the\nPOSSIBILITY AXIOM\naction can be taken. It has the form Φ(s) ⇒Poss(a, s) where Φ(s) is some formula Section 10.4.\nOther Classical Planning Approaches\n389\nPIT\nPIT\nPIT\nGold\nPIT\nPIT\nPIT\nGold\nPIT\nPIT\nPIT\nGold\nS0\nForward\nResult(S0, Forward)\nResult(Result(S0, Forward),\nTurn(Right))\nTurn(Right)\nFigure 10.12\nSituations as the results of actions in the wumpus world.\ninvolving s that describes the preconditions. An example from the wumpus world says\nthat it is possible to shoot if the agent is alive and has an arrow:\nAlive(Agent, s) ∧Have(Agent, Arrow, s) ⇒Poss(Shoot, s)\n• Each ﬂuent is described with a successor-state axiom that says what happens to the\nﬂuent, depending on what action is taken. This is similar to the approach we took for\npropositional logic. The axiom has the form\nAction is possible ⇒\n(Fluent is true in result state ⇔Action’s effect made it true\n∨It was true before and action left it alone) .\nFor example, the axiom for the relational ﬂuent Holding says that the agent is holding\nsome gold g after executing a possible action if and only if the action was a Grab of g\nor if the agent was already holding g and the action was not releasing it:\nPoss(a, s) ⇒\n(Holding(Agent, g, Result(a, s)) ⇔\na = Grab(g) ∨(Holding(Agent, g, s) ∧a ̸= Release(g))) .\n• We need unique action axioms so that the agent can deduce that, for example, a ̸=\nUNIQUE ACTION\nAXIOMS\nRelease(g). For each distinct pair of action names Ai and Aj we have an axiom that\nsays the actions are different:\nAi(x, . . .) ̸= Aj(y, . . .) 390\nChapter\n10.\nClassical Planning\nand for each action name Ai we have an axiom that says two uses of that action name\nare equal if and only if all their arguments are equal:\nAi(x1, . . . , xn) = Ai(y1, . . . , yn) ⇔x1 = y1 ∧. . . ∧xn = yn .\n• A solution is a situation (and hence a sequence of actions) that satisﬁes the goal.\nWork in situation calculus has done a lot to deﬁne the formal semantics of planning and to\nopen up new areas of investigation. But so far there have not been any practical large-scale\nplanning programs based on logical deduction over the situation calculus. This is in part\nbecause of the difﬁculty of doing efﬁcient inference in FOL, but is mainly because the ﬁeld\nhas not yet developed effective heuristics for planning with situation calculus.\n10.4.3\nPlanning as constraint satisfaction",
  "because of the difﬁculty of doing efﬁcient inference in FOL, but is mainly because the ﬁeld\nhas not yet developed effective heuristics for planning with situation calculus.\n10.4.3\nPlanning as constraint satisfaction\nWe have seen that constraint satisfaction has a lot in common with Boolean satisﬁability, and\nwe have seen that CSP techniques are effective for scheduling problems, so it is not surprising\nthat it is possible to encode a bounded planning problem (i.e., the problem of ﬁnding a plan of\nlength k) as a constraint satisfaction problem (CSP). The encoding is similar to the encoding\nto a SAT problem (Section 10.4.1), with one important simpliﬁcation: at each time step we\nneed only a single variable, Actiont, whose domain is the set of possible actions. We no\nlonger need one variable for every action, and we don’t need the action exclusion axioms. It\nis also possible to encode a planning graph into a CSP. This is the approach taken by GP-CSP\n(Do and Kambhampati, 2003).\n10.4.4\nPlanning as reﬁnement of partially ordered plans\nAll the approaches we have seen so far construct totally ordered plans consisting of a strictly\nlinear sequences of actions. This representation ignores the fact that many subproblems are\nindependent. A solution to an air cargo problem consists of a totally ordered sequence of\nactions, yet if 30 packages are being loaded onto one plane in one airport and 50 packages are\nbeing loaded onto another at another airport, it seems pointless to come up with a strict linear\nordering of 80 load actions; the two subsets of actions should be thought of independently.\nAn alternative is to represent plans as partially ordered structures: a plan is a set of\nactions and a set of constraints of the form Before(ai, aj) saying that one action occurs\nbefore another. In the bottom of Figure 10.13, we see a partially ordered plan that is a solution\nto the spare tire problem. Actions are boxes and ordering constraints are arrows. Note that\nRemove(Spare, Trunk) and Remove(Flat, Axle) can be done in either order as long as they\nare both completed before the PutOn(Spare, Axle) action.\nPartially ordered plans are created by a search through the space of plans rather than\nthrough the state space. We start with the empty plan consisting of just the initial state and\nthe goal, with no actions in between, as in the top of Figure 10.13. The search procedure then\nlooks for a ﬂaw in the plan, and makes an addition to the plan to correct the ﬂaw (or if no\nFLAW",
  "the goal, with no actions in between, as in the top of Figure 10.13. The search procedure then\nlooks for a ﬂaw in the plan, and makes an addition to the plan to correct the ﬂaw (or if no\nFLAW\ncorrection can be made, the search backtracks and tries something else). A ﬂaw is anything\nthat keeps the partial plan from being a solution. For example, one ﬂaw in the empty plan is\nthat no action achieves At(Spare, Axle). One way to correct the ﬂaw is to insert into the plan Section 10.4.\nOther Classical Planning Approaches\n391\nFinish\nAt(Spare,Axle)\nStart\nAt(Flat,Axle)\nAt(Spare,Trunk)\n(a)\nRemove(Spare,Trunk)\nAt(Spare,Trunk)\nPutOn(Spare,Axle)\nAt(Spare,Ground)\nAt(Flat,Axle)\nFinish\nAt(Spare,Axle)\nStart\nAt(Flat,Axle)\nAt(Spare,Trunk)\n¬\n(b)\nStart\nRemove(Spare,Trunk)\nAt(Spare,Trunk)\nRemove(Flat,Axle)\nAt(Flat,Axle)\nPutOn(Spare,Axle)\nAt(Spare,Ground)\nAt(Flat,Axle)\nFinish\nAt(Spare,Axle)\nAt(Flat,Axle)\nAt(Spare,Trunk)\n¬\n(c)\nFigure 10.13\n(a) the tire problem expressed as an empty plan. (b) an incomplete partially\nordered plan for the tire problem. Boxes represent actions and arrows indicate that one action\nmust occur before another. (c) a complete partially-ordered solution.\nthe action PutOn(Spare, Axle). Of course that introduces some new ﬂaws: the preconditions\nof the new action are not achieved. The search keeps adding to the plan (backtracking if\nnecessary) until all ﬂaws are resolved, as in the bottom of Figure 10.13. At every step, we\nmake the least commitment possible to ﬁx the ﬂaw. For example, in adding the action\nLEAST COMMITMENT\nRemove(Spare, Trunk) we need to commit to having it occur before PutOn(Spare, Axle),\nbut we make no other commitment that places it before or after other actions. If there were a\nvariable in the action schema that could be left unbound, we would do so.\nIn the 1980s and 90s, partial-order planning was seen as the best way to handle plan-\nning problems with independent subproblems—after all, it was the only approach that ex-\nplicitly represents independent branches of a plan. On the other hand, it has the disadvantage\nof not having an explicit representation of states in the state-transition model. That makes\nsome computations cumbersome. By 2000, forward-search planners had developed excellent\nheuristics that allowed them to efﬁciently discover the independent subproblems that partial-\norder planning was designed for. As a result, partial-order planners are not competitive on\nfully automated classical planning problems.",
  "heuristics that allowed them to efﬁciently discover the independent subproblems that partial-\norder planning was designed for. As a result, partial-order planners are not competitive on\nfully automated classical planning problems.\nHowever, partial-order planning remains an important part of the ﬁeld. For some spe-\nciﬁc tasks, such as operations scheduling, partial-order planning with domain speciﬁc heuris-\ntics is the technology of choice. Many of these systems use libraries of high-level plans, as\ndescribed in Section 11.2. Partial-order planning is also often used in domains where it is im-\nportant for humans to understand the plans. Operational plans for spacecraft and Mars rovers\nare generated by partial-order planners and are then checked by human operators before being\nuploaded to the vehicles for execution. The plan reﬁnement approach makes it easier for the\nhumans to understand what the planning algorithms are doing and verify that they are correct. 392\nChapter\n10.\nClassical Planning\n10.5\nANALYSIS OF PLANNING APPROACHES\nPlanning combines the two major areas of AI we have covered so far: search and logic. A\nplanner can be seen either as a program that searches for a solution or as one that (construc-\ntively) proves the existence of a solution. The cross-fertilization of ideas from the two areas\nhas led both to improvements in performance amounting to several orders of magnitude in\nthe last decade and to an increased use of planners in industrial applications. Unfortunately,\nwe do not yet have a clear understanding of which techniques work best on which kinds of\nproblems. Quite possibly, new techniques will emerge that dominate existing methods.\nPlanning is foremost an exercise in controlling combinatorial explosion. If there are n\npropositions in a domain, then there are 2n states. As we have seen, planning is PSPACE-\nhard. Against such pessimism, the identiﬁcation of independent subproblems can be a pow-\nerful weapon. In the best case—full decomposability of the problem—we get an exponential\nspeedup. Decomposability is destroyed, however, by negative interactions between actions.\nGRAPHPLAN records mutexes to point out where the difﬁcult interactions are. SATPLAN rep-\nresents a similar range of mutex relations, but does so by using the general CNF form rather\nthan a speciﬁc data structure. Forward search addresses the problem heuristically by trying\nto ﬁnd patterns (subsets of propositions) that cover the independent subproblems. Since this",
  "than a speciﬁc data structure. Forward search addresses the problem heuristically by trying\nto ﬁnd patterns (subsets of propositions) that cover the independent subproblems. Since this\napproach is heuristic, it can work even when the subproblems are not completely independent.\nSometimes it is possible to solve a problem efﬁciently by recognizing that negative\ninteractions can be ruled out. We say that a problem has serializable subgoals if there exists\nSERIALIZABLE\nSUBGOAL\nan order of subgoals such that the planner can achieve them in that order without having to\nundo any of the previously achieved subgoals. For example, in the blocks world, if the goal\nis to build a tower (e.g., A on B, which in turn is on C, which in turn is on the Table, as in\nFigure 10.4 on page 371), then the subgoals are serializable bottom to top: if we ﬁrst achieve\nC on Table, we will never have to undo it while we are achieving the other subgoals. A\nplanner that uses the bottom-to-top trick can solve any problem in the blocks world without\nbacktracking (although it might not always ﬁnd the shortest plan).\nAs a more complex example, for the Remote Agent planner that commanded NASA’s\nDeep Space One spacecraft, it was determined that the propositions involved in command-\ning a spacecraft are serializable. This is perhaps not too surprising, because a spacecraft is\ndesigned by its engineers to be as easy as possible to control (subject to other constraints).\nTaking advantage of the serialized ordering of goals, the Remote Agent planner was able to\neliminate most of the search. This meant that it was fast enough to control the spacecraft in\nreal time, something previously considered impossible.\nPlanners such as GRAPHPLAN, SATPLAN, and FF have moved the ﬁeld of planning\nforward, by raising the level of performance of planning systems, by clarifying the repre-\nsentational and combinatorial issues involved, and by the development of useful heuristics.\nHowever, there is a question of how far these techniques will scale. It seems likely that further\nprogress on larger problems cannot rely only on factored and propositional representations,\nand will require some kind of synthesis of ﬁrst-order and hierarchical representations with\nthe efﬁcient heuristics currently in use. Section 10.6.\nSummary\n393\n10.6\nSUMMARY\nIn this chapter, we deﬁned the problem of planning in deterministic, fully observable, static\nenvironments. We described the PDDL representation for planning problems and several",
  "Summary\n393\n10.6\nSUMMARY\nIn this chapter, we deﬁned the problem of planning in deterministic, fully observable, static\nenvironments. We described the PDDL representation for planning problems and several\nalgorithmic approaches for solving them. The points to remember:\n• Planning systems are problem-solving algorithms that operate on explicit propositional\nor relational representations of states and actions. These representations make possi-\nble the derivation of effective heuristics and the development of powerful and ﬂexible\nalgorithms for solving problems.\n• PDDL, the Planning Domain Deﬁnition Language, describes the initial and goal states\nas conjunctions of literals, and actions in terms of their preconditions and effects.\n• State-space search can operate in the forward direction (progression) or the backward\ndirection (regression). Effective heuristics can be derived by subgoal independence\nassumptions and by various relaxations of the planning problem.\n• A planning graph can be constructed incrementally, starting from the initial state. Each\nlayer contains a superset of all the literals or actions that could occur at that time step\nand encodes mutual exclusion (mutex) relations among literals or actions that cannot co-\noccur. Planning graphs yield useful heuristics for state-space and partial-order planners\nand can be used directly in the GRAPHPLAN algorithm.\n• Other approaches include ﬁrst-order deduction over situation calculus axioms; encoding\na planning problem as a Boolean satisﬁability problem or as a constraint satisfaction\nproblem; and explicitly searching through the space of partially ordered plans.\n• Each of the major approaches to planning has its adherents, and there is as yet no con-\nsensus on which is best. Competition and cross-fertilization among the approaches have\nresulted in signiﬁcant gains in efﬁciency for planning systems.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nAI planning arose from investigations into state-space search, theorem proving, and control\ntheory and from the practical needs of robotics, scheduling, and other domains. STRIPS (Fikes\nand Nilsson, 1971), the ﬁrst major planning system, illustrates the interaction of these inﬂu-\nences. STRIPS was designed as the planning component of the software for the Shakey robot\nproject at SRI. Its overall control structure was modeled on that of GPS, the General Problem\nSolver (Newell and Simon, 1961), a state-space search system that used means–ends anal-",
  "project at SRI. Its overall control structure was modeled on that of GPS, the General Problem\nSolver (Newell and Simon, 1961), a state-space search system that used means–ends anal-\nysis. Bylander (1992) shows simple STRIPS planning to be PSPACE-complete. Fikes and\nNilsson (1993) give a historical retrospective on the STRIPS project and its relationship to\nmore recent planning efforts.\nThe representation language used by STRIPS has been far more inﬂuential than its al-\ngorithmic approach; what we call the “classical” language is close to what STRIPS used. 394\nChapter\n10.\nClassical Planning\nThe Action Description Language, or ADL (Pednault, 1986), relaxed some of the STRIPS\nrestrictions and made it possible to encode more realistic problems. Nebel (2000) explores\nschemes for compiling ADL into STRIPS. The Problem Domain Description Language, or\nPDDL (Ghallab et al., 1998), was introduced as a computer-parsable, standardized syntax for\nrepresenting planning problems and has been used as the standard language for the Interna-\ntional Planning Competition since 1998. There have been several extensions; the most recent\nversion, PDDL 3.0, includes plan constraints and preferences (Gerevini and Long, 2005).\nPlanners in the early 1970s generally considered totally ordered action sequences. Prob-\nlem decomposition was achieved by computing a subplan for each subgoal and then stringing\nthe subplans together in some order. This approach, called linear planning by Sacerdoti\nLINEAR PLANNING\n(1975), was soon discovered to be incomplete. It cannot solve some very simple problems,\nsuch as the Sussman anomaly (see Exercise 10.7), found by Allen Brown during experimen-\ntation with the HACKER system (Sussman, 1975). A complete planner must allow for inter-\nleaving of actions from different subplans within a single sequence. The notion of serializable\nINTERLEAVING\nsubgoals (Korf, 1987) corresponds exactly to the set of problems for which noninterleaved\nplanners are complete.\nOne solution to the interleaving problem was goal-regression planning, a technique in\nwhich steps in a totally ordered plan are reordered so as to avoid conﬂict between subgoals.\nThis was introduced by Waldinger (1975) and also used by Warren’s (1974) WARPLAN.\nWARPLAN is also notable in that it was the ﬁrst planner to be written in a logic program-\nming language (Prolog) and is one of the best examples of the remarkable economy that can",
  "WARPLAN is also notable in that it was the ﬁrst planner to be written in a logic program-\nming language (Prolog) and is one of the best examples of the remarkable economy that can\nsometimes be gained with logic programming: WARPLAN is only 100 lines of code, a small\nfraction of the size of comparable planners of the time.\nThe ideas underlying partial-order planning include the detection of conﬂicts (Tate,\n1975a) and the protection of achieved conditions from interference (Sussman, 1975). The\nconstruction of partially ordered plans (then called task networks) was pioneered by the\nNOAH planner (Sacerdoti, 1975, 1977) and by Tate’s (1975b, 1977) NONLIN system.\nPartial-order planning dominated the next 20 years of research, yet the ﬁrst clear for-\nmal exposition was TWEAK (Chapman, 1987), a planner that was simple enough to allow\nproofs of completeness and intractability (NP-hardness and undecidability) of various plan-\nning problems. Chapman’s work led to a straightforward description of a complete partial-\norder planner (McAllester and Rosenblitt, 1991), then to the widely distributed implementa-\ntions SNLP (Soderland and Weld, 1991) and UCPOP (Penberthy and Weld, 1992). Partial-\norder planning fell out of favor in the late 1990s as faster methods emerged. Nguyen and\nKambhampati (2001) suggest that a reconsideration is merited: with accurate heuristics de-\nrived from a planning graph, their REPOP planner scales up much better than GRAPHPLAN\nin parallelizable domains and is competitive with the fastest state-space planners.\nThe resurgence of interest in state-space planning was pioneered by Drew McDer-\nmott’s UNPOP program (1996), which was the ﬁrst to suggest the ignore-delete-list heuristic,\nThe name UNPOP was a reaction to the overwhelming concentration on partial-order plan-\nning at the time; McDermott suspected that other approaches were not getting the attention\nthey deserved. Bonet and Geffner’s Heuristic Search Planner (HSP) and its later deriva-\ntives (Bonet and Geffner, 1999; Haslum et al., 2005; Haslum, 2006) were the ﬁrst to make Bibliographical and Historical Notes\n395\nstate-space search practical for large planning problems. HSP searches in the forward di-\nrection while HSPR (Bonet and Geffner, 1999) searches backward. The most successful\nstate-space searcher to date is FF (Hoffmann, 2001; Hoffmann and Nebel, 2001; Hoffmann,\n2005), winner of the AIPS 2000 planning competition. FASTDOWNWARD (Helmert, 2006)",
  "state-space searcher to date is FF (Hoffmann, 2001; Hoffmann and Nebel, 2001; Hoffmann,\n2005), winner of the AIPS 2000 planning competition. FASTDOWNWARD (Helmert, 2006)\nis a forward state-space search planner that preprocesses the action schemas into an alter-\nnative representation which makes some of the constraints more explicit. FASTDOWNWARD\n(Helmert and Richter, 2004; Helmert, 2006) won the 2004 planning competition, and LAMA\n(Richter and Westphal, 2008), a planner based on FASTDOWNWARD with improved heuris-\ntics, won the 2008 competition.\nBylander (1994) and Ghallab et al. (2004) discuss the computational complexity of\nseveral variants of the planning problem. Helmert (2003) proves complexity bounds for many\nof the standard benchmark problems, and Hoffmann (2005) analyzes the search space of the\nignore-delete-list heuristic. Heuristics for the set-covering problem are discussed by Caprara\net al. (1995) for scheduling operations of the Italian railway. Edelkamp (2009) and Haslum\net al. (2007) describe how to construct pattern databases for planning heuristics. As we\nmentioned in Chapter 3, Felner et al. (2004) show encouraging results using pattern databases\nfor sliding blocks puzzles, which can be thought of as a planning domain, but Hoffmann et al.\n(2006) show some limitations of abstraction for classical planning problems.\nAvrim Blum and Merrick Furst (1995, 1997) revitalized the ﬁeld of planning with their\nGRAPHPLAN system, which was orders of magnitude faster than the partial-order planners of\nthe time. Other graph-planning systems, such as IPP (Koehler et al., 1997), STAN (Fox and\nLong, 1998), and SGP (Weld et al., 1998), soon followed. A data structure closely resembling\nthe planning graph had been developed slightly earlier by Ghallab and Laruelle (1994), whose\nIXTET partial-order planner used it to derive accurate heuristics to guide searches. Nguyen\net al. (2001) thoroughly analyze heuristics derived from planning graphs. Our discussion of\nplanning graphs is based partly on this work and on lecture notes and articles by Subbarao\nKambhampati (Bryce and Kambhampati, 2007). As mentioned in the chapter, a planning\ngraph can be used in many different ways to guide the search for a solution. The winner\nof the 2002 AIPS planning competition, LPG (Gerevini and Serina, 2002, 2003), searched\nplanning graphs using a local search technique inspired by WALKSAT.\nThe situation calculus approach to planning was introduced by John McCarthy (1963).",
  "of the 2002 AIPS planning competition, LPG (Gerevini and Serina, 2002, 2003), searched\nplanning graphs using a local search technique inspired by WALKSAT.\nThe situation calculus approach to planning was introduced by John McCarthy (1963).\nThe version we show here was proposed by Ray Reiter (1991, 2001).\nKautz et al. (1996) investigated various ways to propositionalize action schemas, ﬁnd-\ning that the most compact forms did not necessarily lead to the fastest solution times. A\nsystematic analysis was carried out by Ernst et al. (1997), who also developed an auto-\nmatic “compiler” for generating propositional representations from PDDL problems. The\nBLACKBOX planner, which combines ideas from GRAPHPLAN and SATPLAN, was devel-\noped by Kautz and Selman (1998). CPLAN, a planner based on constraint satisfaction, was\ndescribed by van Beek and Chen (1999).\nMost recently, there has been interest in the representation of plans as binary decision\ndiagrams, compact data structures for Boolean expressions widely studied in the hardware\nBINARY DECISION\nDIAGRAM\nveriﬁcation community (Clarke and Grumberg, 1987; McMillan, 1993). There are techniques\nfor proving properties of binary decision diagrams, including the property of being a solution 396\nChapter\n10.\nClassical Planning\nto a planning problem. Cimatti et al. (1998) present a planner based on this approach. Other\nrepresentations have also been used; for example, Vossen et al. (2001) survey the use of\ninteger programming for planning.\nThe jury is still out, but there are now some interesting comparisons of the various\napproaches to planning. Helmert (2001) analyzes several classes of planning problems, and\nshows that constraint-based approaches such as GRAPHPLAN and SATPLAN are best for NP-\nhard domains, while search-based approaches do better in domains where feasible solutions\ncan be found without backtracking. GRAPHPLAN and SATPLAN have trouble in domains\nwith many objects because that means they must create many actions. In some cases the\nproblem can be delayed or avoided by generating the propositionalized actions dynamically,\nonly as needed, rather than instantiating them all before the search begins.\nReadings in Planning (Allen et al., 1990) is a comprehensive anthology of early work\nin the ﬁeld. Weld (1994, 1999) provides two excellent surveys of planning algorithms of\nthe 1990s. It is interesting to see the change in the ﬁve years between the two surveys:",
  "in the ﬁeld. Weld (1994, 1999) provides two excellent surveys of planning algorithms of\nthe 1990s. It is interesting to see the change in the ﬁve years between the two surveys:\nthe ﬁrst concentrates on partial-order planning, and the second introduces GRAPHPLAN and\nSATPLAN. Automated Planning (Ghallab et al., 2004) is an excellent textbook on all aspects\nof planning. LaValle’s text Planning Algorithms (2006) covers both classical and stochastic\nplanning, with extensive coverage of robot motion planning.\nPlanning research has been central to AI since its inception, and papers on planning are\na staple of mainstream AI journals and conferences. There are also specialized conferences\nsuch as the International Conference on AI Planning Systems, the International Workshop on\nPlanning and Scheduling for Space, and the European Conference on Planning.\nEXERCISES\n10.1\nDescribe the differences and similarities between problem solving and planning.\n10.2\nGiven the action schemas and initial state from Figure 10.1, what are all the applicable\nconcrete instances of Fly(p, from, to) in the state described by\nAt(P1, JFK) ∧At(P2, SFO) ∧Plane(P1) ∧Plane(P2)\n∧Airport(JFK ) ∧Airport(SFO) ?\n10.3\nThe monkey-and-bananas problem is faced by a monkey in a laboratory with some\nbananas hanging out of reach from the ceiling. A box is available that will enable the monkey\nto reach the bananas if he climbs on it. Initially, the monkey is at A, the bananas at B, and the\nbox at C. The monkey and box have height Low, but if the monkey climbs onto the box he\nwill have height High, the same as the bananas. The actions available to the monkey include\nGo from one place to another, Push an object from one place to another, ClimbUp onto or\nClimbDown from an object, and Grasp or Ungrasp an object. The result of a Grasp is that\nthe monkey holds the object if the monkey and object are in the same place at the same height.\na. Write down the initial state description. Exercises\n397\nRoom 4\nRoom 3\nRoom 2\nRoom 1\nDoor 1\nDoor 2\nDoor 3\nDoor 4\nBox 1\nBox 2\nBox 3\nShakey\nSwitch 1\nSwitch 2\nSwitch 3\nSwitch 4\nBox 4\nCorridor\nFigure 10.14\nShakey’s world. Shakey can move between landmarks within a room, can\npass through the door between rooms, can climb climbable objects and push pushable objects,\nand can ﬂip light switches.\nb. Write the six action schemas.\nc. Suppose the monkey wants to fool the scientists, who are off to tea, by grabbing the",
  "pass through the door between rooms, can climb climbable objects and push pushable objects,\nand can ﬂip light switches.\nb. Write the six action schemas.\nc. Suppose the monkey wants to fool the scientists, who are off to tea, by grabbing the\nbananas, but leaving the box in its original place. Write this as a general goal (i.e., not\nassuming that the box is necessarily at C) in the language of situation calculus. Can this\ngoal be solved by a classical planning system?\nd. Your schema for pushing is probably incorrect, because if the object is too heavy, its\nposition will remain the same when the Push schema is applied. Fix your action schema\nto account for heavy objects.\n10.4\nThe original STRIPS planner was designed to control Shakey the robot. Figure 10.14\nshows a version of Shakey’s world consisting of four rooms lined up along a corridor, where\neach room has a door and a light switch. The actions in Shakey’s world include moving from\nplace to place, pushing movable objects (such as boxes), climbing onto and down from rigid 398\nChapter\n10.\nClassical Planning\nobjects (such as boxes), and turning light switches on and off. The robot itself could not climb\non a box or toggle a switch, but the planner was capable of ﬁnding and printing out plans that\nwere beyond the robot’s abilities. Shakey’s six actions are the following:\n• Go(x, y, r), which requires that Shakey be At x and that x and y are locations In the\nsame room r. By convention a door between two rooms is in both of them.\n• Push a box b from location x to location y within the same room: Push(b, x, y, r). You\nwill need the predicate Box and constants for the boxes.\n• Climb onto a box from position x: ClimbUp(x, b); climb down from a box to position\nx: ClimbDown(b, x). We will need the predicate On and the constant Floor.\n• Turn a light switch on or off: TurnOn(s, b); TurnOﬀ(s, b). To turn a light on or off,\nShakey must be on top of a box at the light switch’s location.\nWrite PDDL sentences for Shakey’s six actions and the initial state from Figure 10.14. Con-\nstruct a plan for Shakey to get Box 2 into Room2.\n10.5\nA ﬁnite Turing machine has a ﬁnite one-dimensional tape of cells, each cell containing\none of a ﬁnite number of symbols. One cell has a read and write head above it. There is a\nﬁnite set of states the machine can be in, one of which is the accept state. At each time step,\ndepending on the symbol on the cell under the head and the machine’s current state, there are",
  "ﬁnite set of states the machine can be in, one of which is the accept state. At each time step,\ndepending on the symbol on the cell under the head and the machine’s current state, there are\na set of actions we can choose from. Each action involves writing a symbol to the cell under\nthe head, transitioning the machine to a state, and optionally moving the head left or right.\nThe mapping that determines which actions are allowed is the Turing machine’s program.\nYour goal is to control the machine into the accept state.\nRepresent the Turing machine acceptance problem as a planning problem. If you can\ndo this, it demonstrates that determining whether a planning problem has a solution is at least\nas hard as the Turing acceptance problem, which is PSPACE-hard.\n10.6\nExplain why dropping negative effects from every action schema in a planning prob-\nlem results in a relaxed problem.\n10.7\nFigure 10.4 (page 371) shows a blocks-world problem that is known as the Sussman\nanomaly. The problem was considered anomalous because the noninterleaved planners of\nSUSSMAN ANOMALY\nthe early 1970s could not solve it. Write a deﬁnition of the problem and solve it, either by\nhand or with a planning program. A noninterleaved planner is a planner that, when given two\nsubgoals G1 and G2, produces either a plan for G1 concatenated with a plan for G2, or vice\nversa. Explain why a noninterleaved planner cannot solve this problem.\n10.8\nProve that backward search with PDDL problems is complete.\n10.9\nConstruct levels 0, 1, and 2 of the planning graph for the problem in Figure 10.1.\n10.10\nProve the following assertions about planning graphs:\na. A literal that does not appear in the ﬁnal level of the graph cannot be achieved. Exercises\n399\nb. The level cost of a literal in a serial graph is no greater than the actual cost of an optimal\nplan for achieving it.\n10.11\nThe set-level heuristic (see page 382) uses a planning graph to estimate the cost of\nachieving a conjunctive goal from the current state. What relaxed problem is the set-level\nheuristic the solution to?\n10.12\nExamine the deﬁnition of bidirectional search in Chapter 3.\na. Would bidirectional state-space search be a good idea for planning?\nb. What about bidirectional search in the space of partial-order plans?\nc. Devise a version of partial-order planning in which an action can be added to a plan if its\npreconditions can be achieved by the effects of actions already in the plan. Explain how",
  "c. Devise a version of partial-order planning in which an action can be added to a plan if its\npreconditions can be achieved by the effects of actions already in the plan. Explain how\nto deal with conﬂicts and ordering constraints. Is the algorithm essentially identical to\nforward state-space search?\n10.13\nWe contrasted forward and backward state-space searchers with partial-order plan-\nners, saying that the latter is a plan-space searcher. Explain how forward and backward state-\nspace search can also be considered plan-space searchers, and say what the plan reﬁnement\noperators are.\n10.14\nUp to now we have assumed that the plans we create always make sure that an action’s\npreconditions are satisﬁed. Let us now investigate what propositional successor-state axioms\nsuch as HaveArrow t+1\n⇔\n(HaveArrow t ∧¬Shoott) have to say about actions whose\npreconditions are not satisﬁed.\na. Show that the axioms predict that nothing will happen when an action is executed in a\nstate where its preconditions are not satisﬁed.\nb. Consider a plan p that contains the actions required to achieve a goal but also includes\nillegal actions. Is it the case that\ninitial state ∧successor-state axioms ∧p |= goal ?\nc. With ﬁrst-order successor-state axioms in situation calculus, is it possible to prove that\na plan containing illegal actions will achieve the goal?\n10.15\nConsider how to translate a set of action schemas into the successor-state axioms of\nsituation calculus.\na. Consider the schema for Fly(p, from, to). Write a logical deﬁnition for the predicate\nPoss(Fly(p, from, to), s), which is true if the preconditions for Fly(p, from, to) are\nsatisﬁed in situation s.\nb. Next, assuming that Fly(p, from, to) is the only action schema available to the agent,\nwrite down a successor-state axiom for At(p, x, s) that captures the same information\nas the action schema. 400\nChapter\n10.\nClassical Planning\nc. Now suppose there is an additional method of travel: Teleport(p, from, to). It has\nthe additional precondition ¬Warped(p) and the additional effect Warped(p). Explain\nhow the situation calculus knowledge base must be modiﬁed.\nd. Finally, develop a general and precisely speciﬁed procedure for carrying out the trans-\nlation from a set of action schemas to a set of successor-state axioms.\n10.16\nIn the SATPLAN algorithm in Figure 7.22 (page 272), each call to the satisﬁabil-\nity algorithm asserts a goal gT , where T ranges from 0 to Tmax. Suppose instead that the",
  "lation from a set of action schemas to a set of successor-state axioms.\n10.16\nIn the SATPLAN algorithm in Figure 7.22 (page 272), each call to the satisﬁabil-\nity algorithm asserts a goal gT , where T ranges from 0 to Tmax. Suppose instead that the\nsatisﬁability algorithm is called only once, with the goal g0 ∨g1 ∨· · · ∨gTmax.\na. Will this always return a plan if one exists with length less than or equal to Tmax?\nb. Does this approach introduce any new spurious “solutions”?\nc. Discuss how one might modify a satisﬁability algorithm such as WALKSAT so that it\nﬁnds short solutions (if they exist) when given a disjunctive goal of this form. 11\nPLANNING AND ACTING\nIN THE REAL WORLD\nIn which we see how more expressive representations and more interactive agent\narchitectures lead to planners that are useful in the real world.\nThe previous chapter introduced the most basic concepts, representations, and algorithms for\nplanning. Planners that are are used in the real world for planning and scheduling the oper-\nations of spacecraft, factories, and military campaigns are more complex; they extend both\nthe representation language and the way the planner interacts with the environment. This\nchapter shows how. Section 11.1 extends the classical language for planning to talk about\nactions with durations and resource constraints. Section 11.2 describes methods for con-\nstructing plans that are organized hierarchically. This allows human experts to communicate\nto the planner what they know about how to solve the problem. Hierarchy also lends itself to\nefﬁcient plan construction because the planner can solve a problem at an abstract level before\ndelving into details. Section 11.3 presents agent architectures that can handle uncertain envi-\nronments and interleave deliberation with execution, and gives some examples of real-world\nsystems. Section 11.4 shows how to plan when the environment contains other agents.\n11.1\nTIME, SCHEDULES, AND RESOURCES\nThe classical planning representation talks about what to do, and in what order, but the repre-\nsentation cannot talk about time: how long an action takes and when it occurs. For example,\nthe planners of Chapter 10 could produce a schedule for an airline that says which planes are\nassigned to which ﬂights, but we really need to know departure and arrival times as well. This\nis the subject matter of scheduling. The real world also imposes many resource constraints;",
  "assigned to which ﬂights, but we really need to know departure and arrival times as well. This\nis the subject matter of scheduling. The real world also imposes many resource constraints;\nfor example, an airline has a limited number of staff—and staff who are on one ﬂight cannot\nbe on another at the same time. This section covers methods for representing and solving\nplanning problems that include temporal and resource constraints.\nThe approach we take in this section is “plan ﬁrst, schedule later”: that is, we divide\nthe overall problem into a planning phase in which actions are selected, with some ordering\nconstraints, to meet the goals of the problem, and a later scheduling phase, in which tempo-\nral information is added to the plan to ensure that it meets resource and deadline constraints.\n401 402\nChapter\n11.\nPlanning and Acting in the Real World\nJobs({AddEngine1 ≺AddWheels1 ≺Inspect1 },\n{AddEngine2 ≺AddWheels2 ≺Inspect2})\nResources(EngineHoists(1), WheelStations(1), Inspectors(2), LugNuts(500))\nAction(AddEngine1, DURATION:30,\nUSE:EngineHoists(1))\nAction(AddEngine2, DURATION:60,\nUSE:EngineHoists(1))\nAction(AddWheels1 , DURATION:30,\nCONSUME:LugNuts(20), USE:WheelStations(1))\nAction(AddWheels2 , DURATION:15,\nCONSUME:LugNuts(20), USE:WheelStations(1))\nAction(Inspecti, DURATION:10,\nUSE:Inspectors(1))\nFigure 11.1\nA job-shop scheduling problem for assembling two cars, with resource con-\nstraints. The notation A ≺B means that action A must precede action B.\nThis approach is common in real-world manufacturing and logistical settings, where the plan-\nning phase is often performed by human experts. The automated methods of Chapter 10 can\nalso be used for the planning phase, provided that they produce plans with just the minimal\nordering constraints required for correctness. G RAPHPLAN (Section 10.3), SATPLAN (Sec-\ntion 10.4.1), and partial-order planners (Section 10.4.4) can do this; search-based methods\n(Section 10.2) produce totally ordered plans, but these can easily be converted to plans with\nminimal ordering constraints.\n11.1.1\nRepresenting temporal and resource constraints\nA typical job-shop scheduling problem, as ﬁrst introduced in Section 6.1.2, consists of a\nset of jobs, each of which consists a collection of actions with ordering constraints among\nJOB\nthem. Each action has a duration and a set of resource constraints required by the action.\nDURATION\nEach constraint speciﬁes a type of resource (e.g., bolts, wrenches, or pilots), the number",
  "JOB\nthem. Each action has a duration and a set of resource constraints required by the action.\nDURATION\nEach constraint speciﬁes a type of resource (e.g., bolts, wrenches, or pilots), the number\nof that resource required, and whether that resource is consumable (e.g., the bolts are no\nCONSUMABLE\nlonger available for use) or reusable (e.g., a pilot is occupied during a ﬂight but is available\nREUSABLE\nagain when the ﬂight is over). Resources can also be produced by actions with negative con-\nsumption, including manufacturing, growing, and resupply actions. A solution to a job-shop\nscheduling problem must specify the start times for each action and must satisfy all the tem-\nporal ordering constraints and resource constraints. As with search and planning problems,\nsolutions can be evaluated according to a cost function; this can be quite complicated, with\nnonlinear resource costs, time-dependent delay costs, and so on. For simplicity, we assume\nthat the cost function is just the total duration of the plan, which is called the makespan.\nMAKESPAN\nFigure 11.1 shows a simple example: a problem involving the assembly of two cars. The\nproblem consists of two jobs, each of the form [AddEngine, AddWheels, Inspect]. Then the Section 11.1.\nTime, Schedules, and Resources\n403\nResources statement declares that there are four types of resources, and gives the number\nof each type available at the start: 1 engine hoist, 1 wheel station, 2 inspectors, and 500 lug\nnuts. The action schemas give the duration and resource needs of each action. The lug nuts\nare consumed as wheels are added to the car, whereas the other resources are “borrowed” at\nthe start of an action and released at the action’s end.\nThe representation of resources as numerical quantities, such as Inspectors(2), rather\nthan as named entities, such as Inspector(I1) and Inspector(I2), is an example of a very\ngeneral technique called aggregation. The central idea of aggregation is to group individual\nAGGREGATION\nobjects into quantities when the objects are all indistinguishable with respect to the purpose\nat hand. In our assembly problem, it does not matter which inspector inspects the car, so there\nis no need to make the distinction. (The same idea works in the missionaries-and-cannibals\nproblem in Exercise 3.9.) Aggregation is essential for reducing complexity. Consider what\nhappens when a proposed schedule has 10 concurrent Inspect actions but only 9 inspectors",
  "problem in Exercise 3.9.) Aggregation is essential for reducing complexity. Consider what\nhappens when a proposed schedule has 10 concurrent Inspect actions but only 9 inspectors\nare available. With inspectors represented as quantities, a failure is detected immediately and\nthe algorithm backtracks to try another schedule. With inspectors represented as individuals,\nthe algorithm backtracks to try all 10! ways of assigning inspectors to actions.\n11.1.2\nSolving scheduling problems\nWe begin by considering just the temporal scheduling problem, ignoring resource constraints.\nTo minimize makespan (plan duration), we must ﬁnd the earliest start times for all the actions\nconsistent with the ordering constraints supplied with the problem. It is helpful to view these\nordering constraints as a directed graph relating the actions, as shown in Figure 11.2. We can\napply the critical path method (CPM) to this graph to determine the possible start and end\nCRITICAL PATH\nMETHOD\ntimes of each action. A path through a graph representing a partial-order plan is a linearly\nordered sequence of actions beginning with Start and ending with Finish. (For example,\nthere are two paths in the partial-order plan in Figure 11.2.)\nThe critical path is that path whose total duration is longest; the path is “critical”\nCRITICAL PATH\nbecause it determines the duration of the entire plan—shortening other paths doesn’t shorten\nthe plan as a whole, but delaying the start of any action on the critical path slows down the\nwhole plan. Actions that are off the critical path have a window of time in which they can be\nexecuted. The window is speciﬁed in terms of an earliest possible start time, ES, and a latest\npossible start time, LS. The quantity LS – ES is known as the slack of an action. We can\nSLACK\nsee in Figure 11.2 that the whole plan will take 85 minutes, that each action in the top job\nhas 15 minutes of slack, and that each action on the critical path has no slack (by deﬁnition).\nTogether the ES and LS times for all the actions constitute a schedule for the problem.\nSCHEDULE\nThe following formulas serve as a deﬁnition for ES and LS and also as the outline of a\ndynamic-programming algorithm to compute them. A and B are actions, and A ≺B means\nthat A comes before B:\nES(Start) = 0\nES(B) = maxA ≺B ES(A) + Duration(A)\nLS(Finish) = ES(Finish)\nLS(A) = minB ≻A LS(B) −Duration(A) . 404\nChapter\n11.\nPlanning and Acting in the Real World\nStart\n  [0,0]\nAddEngine1\n30\n  [0,15]\nAddWheels1\n30\n  [30,45]\n10",
  "ES(Start) = 0\nES(B) = maxA ≺B ES(A) + Duration(A)\nLS(Finish) = ES(Finish)\nLS(A) = minB ≻A LS(B) −Duration(A) . 404\nChapter\n11.\nPlanning and Acting in the Real World\nStart\n  [0,0]\nAddEngine1\n30\n  [0,15]\nAddWheels1\n30\n  [30,45]\n10\nInspect1\n  [60,75]\nFinish\n  [85,85]\n10\nInspect2\n  [75,75]\n15\nAddWheels2\n  [60,60]\n60\nAddEngine2\n  [0,0]\nAddEngine1\nAddWheels1\nInspect1\nAddWheels2\nInspect2\nAddEngine2\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nFigure 11.2\nTop: a representation of the temporal constraints for the job-shop scheduling\nproblem of Figure 11.1. The duration of each action is given at the bottom of each rectangle.\nIn solving the problem, we compute the earliest and latest start times as the pair [ES, LS],\ndisplayed in the upper left. The difference between these two numbers is the slack of an\naction; actions with zero slack are on the critical path, shown with bold arrows. Bottom: the\nsame solution shown as a timeline. Grey rectangles represent time intervals during which an\naction may be executed, provided that the ordering constraints are respected. The unoccupied\nportion of a gray rectangle indicates the slack.\nThe idea is that we start by assigning ES(Start) to be 0. Then, as soon as we get an action\nB such that all the actions that come immediately before B have ES values assigned, we\nset ES(B) to be the maximum of the earliest ﬁnish times of those immediately preceding\nactions, where the earliest ﬁnish time of an action is deﬁned as the earliest start time plus the\nduration. This process repeats until every action has been assigned an ES value. The LS\nvalues are computed in a similar manner, working backward from the Finish action.\nThe complexity of the critical path algorithm is just O(Nb), where N is the number of\nactions and b is the maximum branching factor into or out of an action. (To see this, note that\nthe LS and ES computations are done once for each action, and each computation iterates\nover at most b other actions.) Therefore, ﬁnding a minimum-duration schedule, given a partial\nordering on the actions and no resource constraints, is quite easy.\nMathematically speaking, critical-path problems are easy to solve because they are de-\nﬁned as a conjunction of linear inequalities on the start and end times. When we introduce\nresource constraints, the resulting constraints on start and end times become more compli-\ncated. For example, the AddEngine actions, which begin at the same time in Figure 11.2, Section 11.1.\nTime, Schedules, and Resources\n405",
  "resource constraints, the resulting constraints on start and end times become more compli-\ncated. For example, the AddEngine actions, which begin at the same time in Figure 11.2, Section 11.1.\nTime, Schedules, and Resources\n405\nAddEngine1\nAddWheels1\nInspect1\nAddWheels2\nInspect2\nAddEngine2\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\n120\nEngineHoists(1)\nWheelStations(1)\nInspectors(2)\nFigure 11.3\nA solution to the job-shop scheduling problem from Figure 11.1, taking into\naccount resource constraints. The left-hand margin lists the three reusable resources, and\nactions are shown aligned horizontally with the resources they use. There are two possi-\nble schedules, depending on which assembly uses the engine hoist ﬁrst; we’ve shown the\nshortest-duration solution, which takes 115 minutes.\nrequire the same EngineHoist and so cannot overlap. The “cannot overlap” constraint is a\ndisjunction of two linear inequalities, one for each possible ordering. The introduction of\ndisjunctions turns out to make scheduling with resource constraints NP-hard.\nFigure 11.3 shows the solution with the fastest completion time, 115 minutes. This is\n30 minutes longer than the 85 minutes required for a schedule without resource constraints.\nNotice that there is no time at which both inspectors are required, so we can immediately\nmove one of our two inspectors to a more productive position.\nThe complexity of scheduling with resource constraints is often seen in practice as\nwell as in theory. A challenge problem posed in 1963—to ﬁnd the optimal schedule for a\nproblem involving just 10 machines and 10 jobs of 100 actions each—went unsolved for\n23 years (Lawler et al., 1993). Many approaches have been tried, including branch-and-\nbound, simulated annealing, tabu search, constraint satisfaction, and other techniques from\nChapters 3 and 4. One simple but popular heuristic is the minimum slack algorithm: on\nMINIMUM SLACK\neach iteration, schedule for the earliest possible start whichever unscheduled action has all\nits predecessors scheduled and has the least slack; then update the ES and LS times for each\naffected action and repeat. The heuristic resembles the minimum-remaining-values (MRV)\nheuristic in constraint satisfaction. It often works well in practice, but for our assembly\nproblem it yields a 130–minute solution, not the 115–minute solution of Figure 11.3.\nUp to this point, we have assumed that the set of actions and ordering constraints is",
  "problem it yields a 130–minute solution, not the 115–minute solution of Figure 11.3.\nUp to this point, we have assumed that the set of actions and ordering constraints is\nﬁxed. Under these assumptions, every scheduling problem can be solved by a nonoverlapping\nsequence that avoids all resource conﬂicts, provided that each action is feasible by itself. If\na scheduling problem is proving very difﬁcult, however, it may not be a good idea to solve\nit this way—it may be better to reconsider the actions and constraints, in case that leads to a\nmuch easier scheduling problem. Thus, it makes sense to integrate planning and scheduling\nby taking into account durations and overlaps during the construction of a partial-order plan.\nSeveral of the planning algorithms in Chapter 10 can be augmented to handle this information.\nFor example, partial-order planners can detect resource constraint violations in much the\nsame way they detect conﬂicts with causal links. Heuristics can be devised to estimate the\ntotal completion time of a plan. This is currently an active area of research. 406\nChapter\n11.\nPlanning and Acting in the Real World\n11.2\nHIERARCHICAL PLANNING\nThe problem-solving and planning methods of the preceding chapters all operate with a ﬁxed\nset of atomic actions. Actions can be strung together into sequences or branching networks;\nstate-of-the-art algorithms can generate solutions containing thousands of actions.\nFor plans executed by the human brain, atomic actions are muscle activations. In very\nround numbers, we have about 103 muscles to activate (639, by some counts, but many of\nthem have multiple subunits); we can modulate their activation perhaps 10 times per second;\nand we are alive and awake for about 109 seconds in all. Thus, a human life contains about\n1013 actions, give or take one or two orders of magnitude. Even if we restrict ourselves to\nplanning over much shorter time horizons—for example, a two-week vacation in Hawaii—a\ndetailed motor plan would contain around 1010 actions. This is a lot more than 1000.\nTo bridge this gap, AI systems will probably have to do what humans appear to do: plan\nat higher levels of abstraction. A reasonable plan for the Hawaii vacation might be “Go to\nSan Francisco airport; take Hawaiian Airlines ﬂight 11 to Honolulu; do vacation stuff for two\nweeks; take Hawaiian Airlines ﬂight 12 back to San Francisco; go home.” Given such a plan,",
  "San Francisco airport; take Hawaiian Airlines ﬂight 11 to Honolulu; do vacation stuff for two\nweeks; take Hawaiian Airlines ﬂight 12 back to San Francisco; go home.” Given such a plan,\nthe action “Go to San Francisco airport” can be viewed as a planning task in itself, with a\nsolution such as “Drive to the long-term parking lot; park; take the shuttle to the terminal.”\nEach of these actions, in turn, can be decomposed further, until we reach the level of actions\nthat can be executed without deliberation to generate the required motor control sequences.\nIn this example, we see that planning can occur both before and during the execution\nof the plan; for example, one would probably defer the problem of planning a route from a\nparking spot in long-term parking to the shuttle bus stop until a particular parking spot has\nbeen found during execution. Thus, that particular action will remain at an abstract level\nprior to the execution phase. We defer discussion of this topic until Section 11.3. Here, we\nconcentrate on the aspect of hierarchical decomposition, an idea that pervades almost all\nHIERARCHICAL\nDECOMPOSITION\nattempts to manage complexity. For example, complex software is created from a hierarchy\nof subroutines or object classes; armies operate as a hierarchy of units; governments and cor-\nporations have hierarchies of departments, subsidiaries, and branch ofﬁces. The key beneﬁt\nof hierarchical structure is that, at each level of the hierarchy, a computational task, military\nmission, or administrative function is reduced to a small number of activities at the next lower\nlevel, so the computational cost of ﬁnding the correct way to arrange those activities for the\ncurrent problem is small. Nonhierarchical methods, on the other hand, reduce a task to a\nlarge number of individual actions; for large-scale problems, this is completely impractical.\n11.2.1\nHigh-level actions\nThe basic formalism we adopt to understand hierarchical decomposition comes from the area\nof hierarchical task networks or HTN planning. As in classical planning (Chapter 10), we\nHIERARCHICAL TASK\nNETWORK\nassume full observability and determinism and the availability of a set of actions, now called\nprimitive actions, with standard precondition–effect schemas. The key additional concept is\nPRIMITIVE ACTION\nthe high-level action or HLA—for example, the action “Go to San Francisco airport” in the\nHIGH-LEVEL ACTION Section 11.2.\nHierarchical Planning\n407\nReﬁnement(Go(Home, SFO),",
  "PRIMITIVE ACTION\nthe high-level action or HLA—for example, the action “Go to San Francisco airport” in the\nHIGH-LEVEL ACTION Section 11.2.\nHierarchical Planning\n407\nReﬁnement(Go(Home, SFO),\nSTEPS: [Drive(Home, SFOLongTermParking),\nShuttle(SFOLongTermParking, SFO)] )\nReﬁnement(Go(Home, SFO),\nSTEPS: [Taxi(Home, SFO)] )\nReﬁnement(Navigate([a, b], [x, y]),\nPRECOND: a = x ∧b = y\nSTEPS: [ ] )\nReﬁnement(Navigate([a, b], [x, y]),\nPRECOND:Connected([a, b], [a −1, b])\nSTEPS: [Left, Navigate([a −1, b], [x, y])] )\nReﬁnement(Navigate([a, b], [x, y]),\nPRECOND:Connected([a, b], [a + 1, b])\nSTEPS: [Right, Navigate([a + 1, b], [x, y])] )\n. . .\nFigure 11.4\nDeﬁnitions of possible reﬁnements for two high-level actions: going to San\nFrancisco airport and navigating in the vacuum world. In the latter case, note the recursive\nnature of the reﬁnements and the use of preconditions.\nexample given earlier. Each HLA has one or more possible reﬁnements, into a sequence1\nREFINEMENT\nof actions, each of which may be an HLA or a primitive action (which has no reﬁnements\nby deﬁnition). For example, the action “Go to San Francisco airport,” represented formally\nas Go(Home, SFO), might have two possible reﬁnements, as shown in Figure 11.4. The\nsame ﬁgure shows a recursive reﬁnement for navigation in the vacuum world: to get to a\ndestination, take a step, and then go to the destination.\nThese examples show that high-level actions and their reﬁnements embody knowledge\nabout how to do things. For instance, the reﬁnements for Go(Home, SFO) say that to get to\nthe airport you can drive or take a taxi; buying milk, sitting down, and moving the knight to\ne4 are not to be considered.\nAn HLA reﬁnement that contains only primitive actions is called an implementation\nIMPLEMENTATION\nof the HLA. For example, in the vacuum world, the sequences [Right, Right, Down] and\n[Down, Right, Right] both implement the HLA Navigate([1, 3], [3, 2]). An implementation\nof a high-level plan (a sequence of HLAs) is the concatenation of implementations of each\nHLA in the sequence. Given the precondition–effect deﬁnitions of each primitive action, it is\nstraightforward to determine whether any given implementation of a high-level plan achieves\nthe goal. We can say, then, that a high-level plan achieves the goal from a given state if at\nleast one of its implementations achieves the goal from that state. The “at least one” in this\ndeﬁnition is crucial—not all implementations need to achieve the goal, because the agent gets",
  "least one of its implementations achieves the goal from that state. The “at least one” in this\ndeﬁnition is crucial—not all implementations need to achieve the goal, because the agent gets\n1 HTN planners often allow reﬁnement into partially ordered plans, and they allow the reﬁnements of two\ndifferent HLAs in a plan to share actions. We omit these important complications in the interest of understanding\nthe basic concepts of hierarchical planning. 408\nChapter\n11.\nPlanning and Acting in the Real World\nto decide which implementation it will execute. Thus, the set of possible implementations in\nHTN planning—each of which may have a different outcome—is not the same as the set of\npossible outcomes in nondeterministic planning. There, we required that a plan work for all\noutcomes because the agent doesn’t get to choose the outcome; nature does.\nThe simplest case is an HLA that has exactly one implementation. In that case, we\ncan compute the preconditions and effects of the HLA from those of the implementation\n(see Exercise 11.3) and then treat the HLA exactly as if it were a primitive action itself. It\ncan be shown that the right collection of HLAs can result in the time complexity of blind\nsearch dropping from exponential in the solution depth to linear in the solution depth, al-\nthough devising such a collection of HLAs may be a nontrivial task in itself. When HLAs\nhave multiple possible implementations, there are two options: one is to search among the\nimplementations for one that works, as in Section 11.2.2; the other is to reason directly about\nthe HLAs—despite the multiplicity of implementations—as explained in Section 11.2.3. The\nlatter method enables the derivation of provably correct abstract plans, without the need to\nconsider their implementations.\n11.2.2\nSearching for primitive solutions\nHTN planning is often formulated with a single “top level” action called Act, where the aim is\nto ﬁnd an implementation of Act that achieves the goal. This approach is entirely general. For\nexample, classical planning problems can be deﬁned as follows: for each primitive action ai,\nprovide one reﬁnement of Act with steps [ai, Act]. That creates a recursive deﬁnition of Act\nthat lets us add actions. But we need some way to stop the recursion; we do that by providing\none more reﬁnement for Act, one with an empty list of steps and with a precondition equal\nto the goal of the problem. This says that if the goal is already achieved, then the right",
  "one more reﬁnement for Act, one with an empty list of steps and with a precondition equal\nto the goal of the problem. This says that if the goal is already achieved, then the right\nimplementation is to do nothing.\nThe approach leads to a simple algorithm: repeatedly choose an HLA in the current\nplan and replace it with one of its reﬁnements, until the plan achieves the goal. One possible\nimplementation based on breadth-ﬁrst tree search is shown in Figure 11.5. Plans are consid-\nered in order of depth of nesting of the reﬁnements, rather than number of primitive steps. It\nis straightforward to design a graph-search version of the algorithm as well as depth-ﬁrst and\niterative deepening versions.\nIn essence, this form of hierarchical search explores the space of sequences that conform\nto the knowledge contained in the HLA library about how things are to be done. A great deal\nof knowledge can be encoded, not just in the action sequences speciﬁed in each reﬁnement but\nalso in the preconditions for the reﬁnements. For some domains, HTN planners have been\nable to generate huge plans with very little search. For example, O-PLAN (Bell and Tate,\n1985), which combines HTN planning with scheduling, has been used to develop production\nplans for Hitachi. A typical problem involves a product line of 350 different products, 35\nassembly machines, and over 2000 different operations. The planner generates a 30-day\nschedule with three 8-hour shifts a day, involving tens of millions of steps. Another important\naspect of HTN plans is that they are, by deﬁnition, hierarchically structured; usually this\nmakes them easy for humans to understand. Section 11.2.\nHierarchical Planning\n409\nfunction HIERARCHICAL-SEARCH(problem,hierarchy) returns a solution, or failure\nfrontier ←a FIFO queue with [Act] as the only element\nloop do\nif EMPTY?(frontier) then return failure\nplan ←POP(frontier) /* chooses the shallowest plan in frontier */\nhla ←the ﬁrst HLA in plan, or null if none\npreﬁx,suﬃx ←the action subsequences before and after hla in plan\noutcome ←RESULT(problem.INITIAL-STATE, preﬁx)\nif hla is null then /* so plan is primitive and outcome is its result */\nif outcome satisﬁes problem.GOAL then return plan\nelse for each sequence in REFINEMENTS(hla,outcome,hierarchy) do\nfrontier ←INSERT(APPEND(preﬁx,sequence,suﬃx),frontier)\nFigure 11.5\nA breadth-ﬁrst implementation of hierarchical forward planning search. The",
  "else for each sequence in REFINEMENTS(hla,outcome,hierarchy) do\nfrontier ←INSERT(APPEND(preﬁx,sequence,suﬃx),frontier)\nFigure 11.5\nA breadth-ﬁrst implementation of hierarchical forward planning search. The\ninitial plan supplied to the algorithm is [Act]. The REFINEMENTS function returns a set of\naction sequences, one for each reﬁnement of the HLA whose preconditions are satisﬁed by\nthe speciﬁed state, outcome.\nThe computational beneﬁts of hierarchical search can be seen by examining an ide-\nalized case. Suppose that a planning problem has a solution with d primitive actions. For\na nonhierarchical, forward state-space planner with b allowable actions at each state, the\ncost is O(bd), as explained in Chapter 3. For an HTN planner, let us suppose a very reg-\nular reﬁnement structure: each nonprimitive action has r possible reﬁnements, each into\nk actions at the next lower level. We want to know how many different reﬁnement trees\nthere are with this structure. Now, if there are d actions at the primitive level, then the\nnumber of levels below the root is logk d, so the number of internal reﬁnement nodes is\n1 + k + k2 + · · · + klogk d−1 = (d −1)/(k −1). Each internal node has r possible reﬁne-\nments, so r(d−1)/(k−1) possible regular decomposition trees could be constructed. Examining\nthis formula, we see that keeping r small and k large can result in huge savings: essentially\nwe are taking the kth root of the nonhierarchical cost, if b and r are comparable. Small r and\nlarge k means a library of HLAs with a small number of reﬁnements each yielding a long\naction sequence (that nonetheless allows us to solve any problem). This is not always pos-\nsible: long action sequences that are usable across a wide range of problems are extremely\nprecious.\nThe key to HTN planning, then, is the construction of a plan library containing known\nmethods for implementing complex, high-level actions. One method of constructing the li-\nbrary is to learn the methods from problem-solving experience. After the excruciating ex-\nperience of constructing a plan from scratch, the agent can save the plan in the library as a\nmethod for implementing the high-level action deﬁned by the task. In this way, the agent can\nbecome more and more competent over time as new methods are built on top of old methods.\nOne important aspect of this learning process is the ability to generalize the methods that",
  "become more and more competent over time as new methods are built on top of old methods.\nOne important aspect of this learning process is the ability to generalize the methods that\nare constructed, eliminating detail that is speciﬁc to the problem instance (e.g., the name of 410\nChapter\n11.\nPlanning and Acting in the Real World\nthe builder or the address of the plot of land) and keeping just the key elements of the plan.\nMethods for achieving this kind of generalization are described in Chapter 19. It seems to us\ninconceivable that humans could be as competent as they are without some such mechanism.\n11.2.3\nSearching for abstract solutions\nThe hierarchical search algorithm in the preceding section reﬁnes HLAs all the way to primi-\ntive action sequences to determine if a plan is workable. This contradicts common sense: one\nshould be able to determine that the two-HLA high-level plan\n[Drive(Home, SFOLongTermParking), Shuttle(SFOLongTermParking, SFO)]\ngets one to the airport without having to determine a precise route, choice of parking spot,\nand so on. The solution seems obvious: write precondition–effect descriptions of the HLAs,\njust as we write down what the primitive actions do. From the descriptions, it ought to be\neasy to prove that the high-level plan achieves the goal. This is the holy grail, so to speak, of\nhierarchical planning because if we derive a high-level plan that provably achieves the goal,\nworking in a small search space of high-level actions, then we can commit to that plan and\nwork on the problem of reﬁning each step of the plan. This gives us the exponential reduction\nwe seek. For this to work, it has to be the case that every high-level plan that “claims” to\nachieve the goal (by virtue of the descriptions of its steps) does in fact achieve the goal in\nthe sense deﬁned earlier: it must have at least one implementation that does achieve the goal.\nThis property has been called the downward reﬁnement property for HLA descriptions.\nDOWNWARD\nREFINEMENT\nPROPERTY\nWriting HLA descriptions that satisfy the downward reﬁnement property is, in princi-\nple, easy: as long as the descriptions are true, then any high-level plan that claims to achieve\nthe goal must in fact do so—otherwise, the descriptions are making some false claim about\nwhat the HLAs do. We have already seen how to write true descriptions for HLAs that have\nexactly one implementation (Exercise 11.3); a problem arises when the HLA has multiple",
  "what the HLAs do. We have already seen how to write true descriptions for HLAs that have\nexactly one implementation (Exercise 11.3); a problem arises when the HLA has multiple\nimplementations. How can we describe the effects of an action that can be implemented in\nmany different ways?\nOne safe answer (at least for problems where all preconditions and goals are positive) is\nto include only the positive effects that are achieved by every implementation of the HLA and\nthe negative effects of any implementation. Then the downward reﬁnement property would\nbe satisﬁed. Unfortunately, this semantics for HLAs is much too conservative. Consider again\nthe HLA Go(Home, SFO), which has two reﬁnements, and suppose, for the sake of argu-\nment, a simple world in which one can always drive to the airport and park, but taking a taxi\nrequires Cash as a precondition. In that case, Go(Home, SFO) doesn’t always get you to\nthe airport. In particular, it fails if Cash is false, and so we cannot assert At(Agent, SFO) as\nan effect of the HLA. This makes no sense, however; if the agent didn’t have Cash, it would\ndrive itself. Requiring that an effect hold for every implementation is equivalent to assuming\nthat someone else—an adversary—will choose the implementation. It treats the HLA’s mul-\ntiple outcomes exactly as if the HLA were a nondeterministic action, as in Section 4.3. For\nour case, the agent itself will choose the implementation.\nThe programming languages community has coined the term demonic nondetermin-\nism for the case where an adversary makes the choices, contrasting this with angelic nonde-\nDEMONIC\nNONDETERMINISM Section 11.2.\nHierarchical Planning\n411\n(a)\n(b)\nFigure 11.6\nSchematic examples of reachable sets. The set of goal states is shaded. Black\nand gray arrows indicate possible implementations of h1 and h2, respectively. (a) The reach-\nable set of an HLA h1 in a state s. (b) The reachable set for the sequence [h1, h2]. Because\nthis intersects the goal set, the sequence achieves the goal.\nterminism, where the agent itself makes the choices. We borrow this term to deﬁne angelic\nANGELIC\nNONDETERMINISM\nsemantics for HLA descriptions. The basic concept required for understanding angelic se-\nANGELIC SEMANTICS\nmantics is the reachable set of an HLA: given a state s, the reachable set for an HLA h,\nREACHABLE SET\nwritten as REACH(s, h), is the set of states reachable by any of the HLA’s implementations.",
  "ANGELIC SEMANTICS\nmantics is the reachable set of an HLA: given a state s, the reachable set for an HLA h,\nREACHABLE SET\nwritten as REACH(s, h), is the set of states reachable by any of the HLA’s implementations.\nThe key idea is that the agent can choose which element of the reachable set it ends up in\nwhen it executes the HLA; thus, an HLA with multiple reﬁnements is more “powerful” than\nthe same HLA with fewer reﬁnements. We can also deﬁne the reachable set of a sequences of\nHLAs. For example, the reachable set of a sequence [h1, h2] is the union of all the reachable\nsets obtained by applying h2 in each state in the reachable set of h1:\nREACH(s, [h1, h2]) =\n\u000f\ns′∈REACH(s, h1)\nREACH(s′, h2) .\nGiven these deﬁnitions, a high-level plan—a sequence of HLAs—achieves the goal if its\nreachable set intersects the set of goal states. (Compare this to the much stronger condition\nfor demonic semantics, where every member of the reachable set has to be a goal state.)\nConversely, if the reachable set doesn’t intersect the goal, then the plan deﬁnitely doesn’t\nwork. Figure 11.6 illustrates these ideas.\nThe notion of reachable sets yields a straightforward algorithm: search among high-\nlevel plans, looking for one whose reachable set intersects the goal; once that happens, the\nalgorithm can commit to that abstract plan, knowing that it works, and focus on reﬁning\nthe plan further. We will come back to the algorithmic issues later; ﬁrst, we consider the\nquestion of how the effects of an HLA—the reachable set for each possible initial state—are\nrepresented. As with the classical action schemas of Chapter 10, we represent the changes 412\nChapter\n11.\nPlanning and Acting in the Real World\nmade to each ﬂuent. Think of a ﬂuent as a state variable. A primitive action can add or delete\na variable or leave it unchanged. (With conditional effects (see Section 11.3.1) there is a\nfourth possibility: ﬂipping a variable to its opposite.)\nAn HLA under angelic semantics can do more: it can control the value of a variable,\nsetting it to true or false depending on which implementation is chosen. In fact, an HLA can\nhave nine different effects on a variable: if the variable starts out true, it can always keep\nit true, always make it false, or have a choice; if the variable starts out false, it can always\nkeep it false, always make it true, or have a choice; and the three choices for each case can\nbe combined arbitrarily, making nine. Notationally, this is a bit challenging. We’ll use the \u0017",
  "keep it false, always make it true, or have a choice; and the three choices for each case can\nbe combined arbitrarily, making nine. Notationally, this is a bit challenging. We’ll use the \u0017\nsymbol to mean “possibly, if the agent so chooses.” Thus, an effect \u0017+A means “possibly add\nA,” that is, either leave A unchanged or make it true. Similarly, \u0017−A means “possibly delete\nA” and \u0017±A means “possibly add or delete A.” For example, the HLA Go(Home, SFO),\nwith the two reﬁnements shown in Figure 11.4, possibly deletes Cash (if the agent decides to\ntake a taxi), so it should have the effect \u0017−Cash. Thus, we see that the descriptions of HLAs\nare derivable, in principle, from the descriptions of their reﬁnements—in fact, this is required\nif we want true HLA descriptions, such that the downward reﬁnement property holds. Now,\nsuppose we have the following schemas for the HLAs h1 and h2:\nAction(h1, PRECOND:¬A, EFFECT:A ∧\u0017−B) ,\nAction(h2, PRECOND:¬B, EFFECT: \u0017+A ∧\u0017±C) .\nThat is, h1 adds A and possible deletes B, while h2 possibly adds A and has full control over\nC. Now, if only B is true in the initial state and the goal is A ∧C then the sequence [h1, h2]\nachieves the goal: we choose an implementation of h1 that makes B false, then choose an\nimplementation of h2 that leaves A true and makes C true.\nThe preceding discussion assumes that the effects of an HLA—the reachable set for\nany given initial state—can be described exactly by describing the effect on each variable. It\nwould be nice if this were always true, but in many cases we can only approximate the ef-\nfects because an HLA may have inﬁnitely many implementations and may produce arbitrarily\nwiggly reachable sets—rather like the wiggly-belief-state problem illustrated in Figure 7.21\non page 271. For example, we said that Go(Home, SFO) possibly deletes Cash; it also\npossibly adds At(Car, SFOLongTermParking); but it cannot do both—in fact, it must do\nexactly one. As with belief states, we may need to write approximate descriptions. We will\nuse two kinds of approximation: an optimistic description REACH+(s,h) of an HLA h may\nOPTIMISTIC\nDESCRIPTION\noverstate the reachable set, while a pessimistic description REACH−(s,h) may understate\nPESSIMISTIC\nDESCRIPTION\nthe reachable set. Thus, we have\nREACH−(s, h) ⊆REACH(s, h) ⊆REACH+(s, h) .\nFor example, an optimistic description of Go(Home, SFO) says that it possible deletes Cash\nand possibly adds At(Car, SFOLongTermParking). Another good example arises in the",
  "the reachable set. Thus, we have\nREACH−(s, h) ⊆REACH(s, h) ⊆REACH+(s, h) .\nFor example, an optimistic description of Go(Home, SFO) says that it possible deletes Cash\nand possibly adds At(Car, SFOLongTermParking). Another good example arises in the\n8-puzzle, half of whose states are unreachable from any given state (see Exercise 3.4 on\npage 113): the optimistic description of Act might well include the whole state space, since\nthe exact reachable set is quite wiggly.\nWith approximate descriptions, the test for whether a plan achieves the goal needs to\nbe modiﬁed slightly. If the optimistic reachable set for the plan doesn’t intersect the goal, Section 11.2.\nHierarchical Planning\n413\n(a)\n(b)\nFigure 11.7\nGoal achievement for high-level plans with approximate descriptions. The\nset of goal states is shaded. For each plan, the pessimistic (solid lines) and optimistic (dashed\nlines) reachable sets are shown. (a) The plan indicated by the black arrow deﬁnitely achieves\nthe goal, while the plan indicated by the gray arrow deﬁnitely doesn’t. (b) A plan that would\nneed to be reﬁned further to determine if it really does achieve the goal.\nthen the plan doesn’t work; if the pessimistic reachable set intersects the goal, then the plan\ndoes work (Figure 11.7(a)). With exact descriptions, a plan either works or it doesn’t, but\nwith approximate descriptions, there is a middle ground: if the optimistic set intersects the\ngoal but the pessimistic set doesn’t, then we cannot tell if the plan works (Figure 11.7(b)).\nWhen this circumstance arises, the uncertainty can be resolved by reﬁning the plan. This is\na very common situation in human reasoning. For example, in planning the aforementioned\ntwo-week Hawaii vacation, one might propose to spend two days on each of seven islands.\nPrudence would indicate that this ambitious plan needs to be reﬁned by adding details of\ninter-island transportation.\nAn algorithm for hierarchical planning with approximate angelic descriptions is shown\nin Figure 11.8. For simplicity, we have kept to the same overall scheme used previously in\nFigure 11.5, that is, a breadth-ﬁrst search in the space of reﬁnements. As just explained, the\nalgorithm can detect plans that will and won’t work by checking the intersections of the opti-\nmistic and pessimistic reachable sets with the goal. (The details of how to compute the reach-\nable sets of a plan, given approximate descriptions of each step, are covered in Exercise 11.5.)",
  "mistic and pessimistic reachable sets with the goal. (The details of how to compute the reach-\nable sets of a plan, given approximate descriptions of each step, are covered in Exercise 11.5.)\nWhen a workable abstract plan is found, the algorithm decomposes the original problem into\nsubproblems, one for each step of the plan. The initial state and goal for each subproblem\nare obtained by regressing a guaranteed-reachable goal state through the action schemas for\neach step of the plan. (See Section 10.2.2 for a discussion of how regression works.) Fig-\nure 11.6(b) illustrates the basic idea: the right-hand circled state is the guaranteed-reachable\ngoal state, and the left-hand circled state is the intermediate goal obtained by regressing the 414\nChapter\n11.\nPlanning and Acting in the Real World\nfunction ANGELIC-SEARCH(problem,hierarchy,initialPlan) returns solution or fail\nfrontier ←a FIFO queue with initialPlan as the only element\nloop do\nif EMPTY?(frontier) then return fail\nplan ←POP(frontier) /* chooses the shallowest node in frontier */\nif REACH+(problem.INITIAL-STATE, plan) intersects problem.GOAL then\nif plan is primitive then return plan\n/* REACH+ is exact for primitive plans */\nguaranteed ←REACH−(problem.INITIAL-STATE,plan) ∩problem.GOAL\nif guaranteed̸={ } and MAKING-PROGRESS(plan, initialPlan) then\nﬁnalState ←any element of guaranteed\nreturn DECOMPOSE(hierarchy,problem.INITIAL-STATE,plan,ﬁnalState)\nhla ←some HLA in plan\npreﬁx,suﬃx ←the action subsequences before and after hla in plan\nfor each sequence in REFINEMENTS(hla,outcome,hierarchy) do\nfrontier ←INSERT(APPEND(preﬁx,sequence,suﬃx),frontier)\nfunction DECOMPOSE(hierarchy,s0,plan,sf ) returns a solution\nsolution ←an empty plan\nwhile plan is not empty do\naction ←REMOVE-LAST(plan)\nsi ←a state in REACH−(s0, plan) such that sf ∈REACH−(si,action)\nproblem ←a problem with INITIAL-STATE = si and GOAL = sf\nsolution ←APPEND(ANGELIC-SEARCH(problem,hierarchy,action),solution)\nsf ←si\nreturn solution\nFigure 11.8\nA hierarchical planning algorithm that uses angelic semantics to identify and\ncommit to high-level plans that work while avoiding high-level plans that don’t. The predi-\ncate MAKING-PROGRESS checks to make sure that we aren’t stuck in an inﬁnite regression\nof reﬁnements. At top level, call ANGELIC-SEARCH with [Act] as the initialPlan.\ngoal through the ﬁnal action.\nThe ability to commit to or reject high-level plans can give ANGELIC-SEARCH a sig-",
  "of reﬁnements. At top level, call ANGELIC-SEARCH with [Act] as the initialPlan.\ngoal through the ﬁnal action.\nThe ability to commit to or reject high-level plans can give ANGELIC-SEARCH a sig-\nniﬁcant computational advantage over HIERARCHICAL-SEARCH, which in turn may have\na large advantage over plain old BREADTH-FIRST-SEARCH. Consider, for example, clean-\ning up a large vacuum world consisting of rectangular rooms connected by narrow corri-\ndors. It makes sense to have an HLA for Navigate (as shown in Figure 11.4) and one for\nCleanWholeRoom. (Cleaning the room could be implemented with the repeated application\nof another HLA to clean each row.) Since there are ﬁve actions in this domain, the cost\nfor BREADTH-FIRST-SEARCH grows as 5d, where d is the length of the shortest solution\n(roughly twice the total number of squares); the algorithm cannot manage even two 2 × 2\nrooms. HIERARCHICAL-SEARCH is more efﬁcient, but still suffers from exponential growth\nbecause it tries all ways of cleaning that are consistent with the hierarchy. ANGELIC-SEARCH\nscales approximately linearly in the number of squares—it commits to a good high-level se- Section 11.3.\nPlanning and Acting in Nondeterministic Domains\n415\nquence and prunes away the other options. Notice that cleaning a set of rooms by cleaning\neach room in turn is hardly rocket science: it is easy for humans precisely because of the\nhierarchical structure of the task. When we consider how difﬁcult humans ﬁnd it to solve\nsmall puzzles such as the 8-puzzle, it seems likely that the human capacity for solving com-\nplex problems derives to a great extent from their skill in abstracting and decomposing the\nproblem to eliminate combinatorics.\nThe angelic approach can be extended to ﬁnd least-cost solutions by generalizing the\nnotion of reachable set. Instead of a state being reachable or not, it has a cost for the most\nefﬁcient way to get there. (The cost is ∞for unreachable states.) The optimistic and pes-\nsimistic descriptions bound these costs. In this way, angelic search can ﬁnd provably optimal\nabstract plans without considering their implementations. The same approach can be used to\nobtain effective hierarchical lookahead algorithms for online search, in the style of LRTA∗\nHIERARCHICAL\nLOOKAHEAD\n(page 152). In some ways, such algorithms mirror aspects of human deliberation in tasks such\nas planning a vacation to Hawaii—consideration of alternatives is done initially at an abstract",
  "HIERARCHICAL\nLOOKAHEAD\n(page 152). In some ways, such algorithms mirror aspects of human deliberation in tasks such\nas planning a vacation to Hawaii—consideration of alternatives is done initially at an abstract\nlevel over long time scales; some parts of the plan are left quite abstract until execution time,\nsuch as how to spend two lazy days on Molokai, while others parts are planned in detail, such\nas the ﬂights to be taken and lodging to be reserved—without these reﬁnements, there is no\nguarantee that the plan would be feasible.\n11.3\nPLANNING AND ACTING IN NONDETERMINISTIC DOMAINS\nIn this section we extend planning to handle partially observable, nondeterministic, and un-\nknown environments. Chapter 4 extended search in similar ways, and the methods here are\nalso similar: sensorless planning (also known as conformant planning) for environments\nwith no observations; contingency planning for partially observable and nondeterministic\nenvironments; and online planning and replanning for unknown environments.\nWhile the basic concepts are the same as in Chapter 4, there are also signiﬁcant dif-\nferences. These arise because planners deal with factored representations rather than atomic\nrepresentations. This affects the way we represent the agent’s capability for action and obser-\nvation and the way we represent belief states—the sets of possible physical states the agent\nmight be in—for unobservable and partially observable environments. We can also take ad-\nvantage of many of the domain-independent methods given in Chapter 10 for calculating\nsearch heuristics.\nConsider this problem: given a chair and a table, the goal is to have them match—have\nthe same color. In the initial state we have two cans of paint, but the colors of the paint and\nthe furniture are unknown. Only the table is initially in the agent’s ﬁeld of view:\nInit(Object(Table) ∧Object(Chair) ∧Can(C1) ∧Can(C2) ∧InView(Table))\nGoal(Color(Chair, c) ∧Color(Table, c))\nThere are two actions: removing the lid from a paint can and painting an object using the\npaint from an open can. The action schemas are straightforward, with one exception: we now\nallow preconditions and effects to contain variables that are not part of the action’s variable 416\nChapter\n11.\nPlanning and Acting in the Real World\nlist. That is, Paint(x, can) does not mention the variable c, representing the color of the\npaint in the can. In the fully observable case, this is not allowed—we would have to name",
  "Chapter\n11.\nPlanning and Acting in the Real World\nlist. That is, Paint(x, can) does not mention the variable c, representing the color of the\npaint in the can. In the fully observable case, this is not allowed—we would have to name\nthe action Paint(x, can, c). But in the partially observable case, we might or might not\nknow what color is in the can. (The variable c is universally quantiﬁed, just like all the other\nvariables in an action schema.)\nAction(RemoveLid(can),\nPRECOND:Can(can)\nEFFECT:Open(can))\nAction(Paint(x, can),\nPRECOND:Object(x) ∧Can(can) ∧Color(can, c) ∧Open(can)\nEFFECT:Color(x, c))\nTo solve a partially observable problem, the agent will have to reason about the percepts it will\nobtain when it is executing the plan. The percept will be supplied by the agent’s sensors when\nit is actually acting, but when it is planning it will need a model of its sensors. In Chapter 4,\nthis model was given by a function, PERCEPT(s). For planning, we augment PDDL with a\nnew type of schema, the percept schema:\nPERCEPT SCHEMA\nPercept(Color(x, c),\nPRECOND:Object(x) ∧InView(x)\nPercept(Color(can, c),\nPRECOND:Can(can) ∧InView(can) ∧Open(can)\nThe ﬁrst schema says that whenever an object is in view, the agent will perceive the color\nof the object (that is, for the object x, the agent will learn the truth value of Color(x, c) for\nall c). The second schema says that if an open can is in view, then the agent perceives the\ncolor of the paint in the can. Because there are no exogenous events in this world, the color\nof an object will remain the same, even if it is not being perceived, until the agent performs\nan action to change the object’s color. Of course, the agent will need an action that causes\nobjects (one at a time) to come into view:\nAction(LookAt(x),\nPRECOND:InView(y) ∧(x ̸= y)\nEFFECT:InView(x) ∧¬InView(y))\nFor a fully observable environment, we would have a Percept axiom with no preconditions\nfor each ﬂuent. A sensorless agent, on the other hand, has no Percept axioms at all. Note\nthat even a sensorless agent can solve the painting problem. One solution is to open any can\nof paint and apply it to both chair and table, thus coercing them to be the same color (even\nthough the agent doesn’t know what the color is).\nA contingent planning agent with sensors can generate a better plan. First, look at the\ntable and chair to obtain their colors; if they are already the same then the plan is done. If",
  "though the agent doesn’t know what the color is).\nA contingent planning agent with sensors can generate a better plan. First, look at the\ntable and chair to obtain their colors; if they are already the same then the plan is done. If\nnot, look at the paint cans; if the paint in a can is the same color as one piece of furniture,\nthen apply that paint to the other piece. Otherwise, paint both pieces with any color.\nFinally, an online planning agent might generate a contingent plan with fewer branches\nat ﬁrst—perhaps ignoring the possibility that no cans match any of the furniture—and deal Section 11.3.\nPlanning and Acting in Nondeterministic Domains\n417\nwith problems when they arise by replanning. It could also deal with incorrectness of its\naction schemas. Whereas a contingent planner simply assumes that the effects of an action\nalways succeed—that painting the chair does the job—a replanning agent would check the\nresult and make an additional plan to ﬁx any unexpected failure, such as an unpainted area or\nthe original color showing through.\nIn the real world, agents use a combination of approaches. Car manufacturers sell spare\ntires and air bags, which are physical embodiments of contingent plan branches designed\nto handle punctures or crashes. On the other hand, most car drivers never consider these\npossibilities; when a problem arises they respond as replanning agents. In general, agents\nplan only for contingencies that have important consequences and a nonnegligible chance\nof happening. Thus, a car driver contemplating a trip across the Sahara desert should make\nexplicit contingency plans for breakdowns, whereas a trip to the supermarket requires less\nadvance planning. We next look at each of the three approaches in more detail.\n11.3.1\nSensorless planning\nSection 4.4.1 (page 138) introduced the basic idea of searching in belief-state space to ﬁnd\na solution for sensorless problems. Conversion of a sensorless planning problem to a belief-\nstate planning problem works much the same way as it did in Section 4.4.1; the main differ-\nences are that the underlying physical transition model is represented by a collection of action\nschemas and the belief state can be represented by a logical formula instead of an explicitly\nenumerated set of states. For simplicity, we assume that the underlying planning problem is\ndeterministic.\nThe initial belief state for the sensorless painting problem can ignore InView ﬂuents\nbecause the agent has no sensors.",
  "enumerated set of states. For simplicity, we assume that the underlying planning problem is\ndeterministic.\nThe initial belief state for the sensorless painting problem can ignore InView ﬂuents\nbecause the agent has no sensors.\nFurthermore, we take as given the unchanging facts\nObject(Table) ∧Object(Chair) ∧Can(C1) ∧Can(C2) because these hold in every be-\nlief state. The agent doesn’t know the colors of the cans or the objects, or whether the cans\nare open or closed, but it does know that objects and cans have colors: ∀x ∃c Color(x, c).\nAfter Skolemizing, (see Section 9.5), we obtain the initial belief state:\nb0 = Color(x, C(x)) .\nIn classical planning, where the closed-world assumption is made, we would assume that\nany ﬂuent not mentioned in a state is false, but in sensorless (and partially observable) plan-\nning we have to switch to an open-world assumption in which states contain both positive\nand negative ﬂuents, and if a ﬂuent does not appear, its value is unknown. Thus, the belief\nstate corresponds exactly to the set of possible worlds that satisfy the formula. Given this\ninitial belief state, the following action sequence is a solution:\n[RemoveLid(Can1), Paint(Chair, Can1), Paint(Table, Can1)] .\nWe now show how to progress the belief state through the action sequence to show that the\nﬁnal belief state satisﬁes the goal.\nFirst, note that in a given belief state b, the agent can consider any action whose pre-\nconditions are satisﬁed by b. (The other actions cannot be used because the transition model\ndoesn’t deﬁne the effects of actions whose preconditions might be unsatisﬁed.) According 418\nChapter\n11.\nPlanning and Acting in the Real World\nto Equation (4.4) (page 139), the general formula for updating the belief state b given an\napplicable action a in a deterministic world is as follows:\nb′ = RESULT(b, a) = {s′ : s′ = RESULTP (s, a) and s ∈b}\nwhere RESULTP deﬁnes the physical transition model. For the time being, we assume that the\ninitial belief state is always a conjunction of literals, that is, a 1-CNF formula. To construct\nthe new belief state b′, we must consider what happens to each literal ℓin each physical state\ns in b when action a is applied. For literals whose truth value is already known in b, the truth\nvalue in b′ is computed from the current value and the add list and delete list of the action.\n(For example, if ℓis in the delete list of the action, then ¬ℓis added to b′.) What about a",
  "value in b′ is computed from the current value and the add list and delete list of the action.\n(For example, if ℓis in the delete list of the action, then ¬ℓis added to b′.) What about a\nliteral whose truth value is unknown in b? There are three cases:\n1. If the action adds ℓ, then ℓwill be true in b′ regardless of its initial value.\n2. If the action deletes ℓ, then ℓwill be false in b′ regardless of its initial value.\n3. If the action does not affect ℓ, then ℓwill retain its initial value (which is unknown) and\nwill not appear in b′.\nHence, we see that the calculation of b′ is almost identical to the observable case, which was\nspeciﬁed by Equation (10.1) on page 368:\nb′ = RESULT(b, a) = (b −DEL(a)) ∪ADD(a) .\nWe cannot quite use the set semantics because (1) we must make sure that b′ does not con-\ntain both ℓand ¬ℓ, and (2) atoms may contain unbound variables. But it is still the case\nthat RESULT(b, a) is computed by starting with b, setting any atom that appears in DEL(a)\nto false, and setting any atom that appears in ADD(a) to true. For example, if we apply\nRemoveLid(Can1) to the initial belief state b0, we get\nb1 = Color(x, C(x)) ∧Open(Can1) .\nWhen we apply the action Paint(Chair, Can1), the precondition Color(Can1, c) is satisﬁed\nby the known literal Color(x, C(x)) with binding {x/Can1, c/C(Can1)} and the new belief\nstate is\nb2 = Color(x, C(x)) ∧Open(Can1) ∧Color(Chair, C(Can1)) .\nFinally, we apply the action Paint(Table, Can1) to obtain\nb3 = Color(x, C(x)) ∧Open(Can1) ∧Color(Chair, C(Can1))\n∧Color(Table, C(Can1)) .\nThe ﬁnal belief state satisﬁes the goal, Color(Table, c) ∧Color(Chair, c), with the variable\nc bound to C(Can1).\nThe preceding analysis of the update rule has shown a very important fact: the family\nof belief states deﬁned as conjunctions of literals is closed under updates deﬁned by PDDL\naction schemas. That is, if the belief state starts as a conjunction of literals, then any update\nwill yield a conjunction of literals. That means that in a world with n ﬂuents, any belief\nstate can be represented by a conjunction of size O(n). This is a very comforting result,\nconsidering that there are 2n states in the world. It says we can compactly represent all the\nsubsets of those 2n states that we will ever need. Moreover, the process of checking for belief Section 11.3.\nPlanning and Acting in Nondeterministic Domains\n419\nstates that are subsets or supersets of previously visited belief states is also easy, at least in\nthe propositional case.",
  "Planning and Acting in Nondeterministic Domains\n419\nstates that are subsets or supersets of previously visited belief states is also easy, at least in\nthe propositional case.\nThe ﬂy in the ointment of this pleasant picture is that it only works for action schemas\nthat have the same effects for all states in which their preconditions are satisﬁed. It is this\nproperty that enables the preservation of the 1-CNF belief-state representation. As soon as the\neffect can depend on the state, dependencies are introduced between ﬂuents and the 1-CNF\nproperty is lost. Consider, for example, the simple vacuum world deﬁned in Section 3.2.1.\nLet the ﬂuents be AtL and AtR for the location of the robot and CleanL and CleanR for\nthe state of the squares. According to the deﬁnition of the problem, the Suck action has no\nprecondition—it can always be done. The difﬁculty is that its effect depends on the robot’s lo-\ncation: when the robot is AtL, the result is CleanL, but when it is AtR, the result is CleanR.\nFor such actions, our action schemas will need something new: a conditional effect. These\nCONDITIONAL\nEFFECT\nhave the syntax “when condition: eﬀect,” where condition is a logical formula to be com-\npared against the current state, and effect is a formula describing the resulting state. For the\nvacuum world, we have\nAction(Suck,\nEFFECT:when AtL: CleanL ∧when AtR: CleanR) .\nWhen applied to the initial belief state True, the resulting belief state is (AtL ∧CleanL) ∨\n(AtR ∧CleanR), which is no longer in 1-CNF. (This transition can be seen in Figure 4.14\non page 141.) In general, conditional effects can induce arbitrary dependencies among the\nﬂuents in a belief state, leading to belief states of exponential size in the worst case.\nIt is important to understand the difference between preconditions and conditional ef-\nfects. All conditional effects whose conditions are satisﬁed have their effects applied to gener-\nate the resulting state; if none are satisﬁed, then the resulting state is unchanged. On the other\nhand, if a precondition is unsatisﬁed, then the action is inapplicable and the resulting state\nis undeﬁned. From the point of view of sensorless planning, it is better to have conditional\neffects than an inapplicable action. For example, we could split Suck into two actions with\nunconditional effects as follows:\nAction(SuckL,\nPRECOND:AtL; EFFECT:CleanL)\nAction(SuckR,\nPRECOND:AtR; EFFECT:CleanR) .",
  "effects than an inapplicable action. For example, we could split Suck into two actions with\nunconditional effects as follows:\nAction(SuckL,\nPRECOND:AtL; EFFECT:CleanL)\nAction(SuckR,\nPRECOND:AtR; EFFECT:CleanR) .\nNow we have only unconditional schemas, so the belief states all remain in 1-CNF; unfortu-\nnately, we cannot determine the applicability of SuckL and SuckR in the initial belief state.\nIt seems inevitable, then, that nontrivial problems will involve wiggly belief states, just\nlike those encountered when we considered the problem of state estimation for the wumpus\nworld (see Figure 7.21 on page 271). The solution suggested then was to use a conservative\napproximation to the exact belief state; for example, the belief state can remain in 1-CNF\nif it contains all literals whose truth values can be determined and treats all other literals as\nunknown. While this approach is sound, in that it never generates an incorrect plan, it is\nincomplete because it may be unable to ﬁnd solutions to problems that necessarily involve\ninteractions among literals. To give a trivial example, if the goal is for the robot to be on 420\nChapter\n11.\nPlanning and Acting in the Real World\na clean square, then [Suck] is a solution but a sensorless agent that insists on 1-CNF belief\nstates will not ﬁnd it.\nPerhaps a better solution is to look for action sequences that keep the belief state\nas simple as possible. For example, in the sensorless vacuum world, the action sequence\n[Right, Suck, Left, Suck] generates the following sequence of belief states:\nb0 = True\nb1 = AtR\nb2 = AtR ∧CleanR\nb3 = AtL ∧CleanR\nb4 = AtL ∧CleanR ∧CleanL\nThat is, the agent can solve the problem while retaining a 1-CNF belief state, even though\nsome sequences (e.g., those beginning with Suck) go outside 1-CNF. The general lesson is\nnot lost on humans: we are always performing little actions (checking the time, patting our\npockets to make sure we have the car keys, reading street signs as we navigate through a city)\nto eliminate uncertainty and keep our belief state manageable.\nThere is another, quite different approach to the problem of unmanageably wiggly be-\nlief states: don’t bother computing them at all. Suppose the initial belief state is b0 and we\nwould like to know the belief state resulting from the action sequence [a1, . . . , am]. Instead\nof computing it explicitly, just represent it as “b0 then [a1, . . . , am].” This is a lazy but un-",
  "would like to know the belief state resulting from the action sequence [a1, . . . , am]. Instead\nof computing it explicitly, just represent it as “b0 then [a1, . . . , am].” This is a lazy but un-\nambiguous representation of the belief state, and it’s quite concise—O(n + m) where n is\nthe size of the initial belief state (assumed to be in 1-CNF) and m is the maximum length\nof an action sequence. As a belief-state representation, it suffers from one drawback, how-\never: determining whether the goal is satisﬁed, or an action is applicable, may require a lot\nof computation.\nThe computation can be implemented as an entailment test: if Am represents the collec-\ntion of successor-state axioms required to deﬁne occurrences of the actions a1, . . . , am—as\nexplained for SATPLAN in Section 10.4.1—and Gm asserts that the goal is true after m steps,\nthen the plan achieves the goal if b0 ∧Am |= Gm, that is, if b0 ∧Am ∧¬Gm is unsatisﬁable.\nGiven a modern SAT solver, it may be possible to do this much more quickly than computing\nthe full belief state. For example, if none of the actions in the sequence has a particular goal\nﬂuent in its add list, the solver will detect this immediately. It also helps if partial results\nabout the belief state—for example, ﬂuents known to be true or false—are cached to simplify\nsubsequent computations.\nThe ﬁnal piece of the sensorless planning puzzle is a heuristic function to guide the\nsearch. The meaning of the heuristic function is the same as for classical planning: an esti-\nmate (perhaps admissible) of the cost of achieving the goal from the given belief state. With\nbelief states, we have one additional fact: solving any subset of a belief state is necessarily\neasier than solving the belief state:\nif b1 ⊆b2 then h∗(b1) ≤h∗(b2) .\nHence, any admissible heuristic computed for a subset is admissible for the belief state itself.\nThe most obvious candidates are the singleton subsets, that is, individual physical states. We Section 11.3.\nPlanning and Acting in Nondeterministic Domains\n421\ncan take any random collection of states s1, . . . , sN that are in the belief state b, apply any\nadmissible heuristic h from Chapter 10, and return\nH(b) = max{h(s1), . . . , h(sN)}\nas the heuristic estimate for solving b. We could also use a planning graph directly on b itself:\nif it is a conjunction of literals (1-CNF), simply set those literals to be the initial state layer",
  "H(b) = max{h(s1), . . . , h(sN)}\nas the heuristic estimate for solving b. We could also use a planning graph directly on b itself:\nif it is a conjunction of literals (1-CNF), simply set those literals to be the initial state layer\nof the graph. If b is not in 1-CNF, it may be possible to ﬁnd sets of literals that together entail\nb. For example, if b is in disjunctive normal form (DNF), each term of the DNF formula is\na conjunction of literals that entails b and can form the initial layer of a planning graph. As\nbefore, we can take the maximum of the heuristics obtained from each set of literals. We can\nalso use inadmissible heuristics such as the ignore-delete-lists heuristic (page 377), which\nseems to work quite well in practice.\n11.3.2\nContingent planning\nWe saw in Chapter 4 that contingent planning—the generation of plans with conditional\nbranching based on percepts—is appropriate for environments with partial observability, non-\ndeterminism, or both. For the partially observable painting problem with the percept axioms\ngiven earlier, one possible contingent solution is as follows:\n[LookAt(Table), LookAt(Chair),\nif Color(Table, c) ∧Color(Chair, c) then NoOp\nelse [RemoveLid(Can1), LookAt(Can1), RemoveLid(Can2), LookAt(Can2),\nif Color(Table, c) ∧Color(can, c) then Paint(Chair, can)\nelse if Color(Chair, c) ∧Color(can, c) then Paint(Table, can)\nelse [Paint(Chair, Can1), Paint(Table, Can1)]]]\nVariables in this plan should be considered existentially quantiﬁed; the second line says\nthat if there exists some color c that is the color of the table and the chair, then the agent\nneed not do anything to achieve the goal. When executing this plan, a contingent-planning\nagent can maintain its belief state as a logical formula and evaluate each branch condition\nby determining if the belief state entails the condition formula or its negation. (It is up to\nthe contingent-planning algorithm to make sure that the agent will never end up in a be-\nlief state where the condition formula’s truth value is unknown.) Note that with ﬁrst-order\nconditions, the formula may be satisﬁed in more than one way; for example, the condition\nColor(Table, c) ∧Color(can, c) might be satisﬁed by {can/Can1} and by {can/Can2} if\nboth cans are the same color as the table. In that case, the agent can choose any satisfying\nsubstitution to apply to the rest of the plan.\nAs shown in Section 4.4.2, calculating the new belief state after an action and subse-",
  "both cans are the same color as the table. In that case, the agent can choose any satisfying\nsubstitution to apply to the rest of the plan.\nAs shown in Section 4.4.2, calculating the new belief state after an action and subse-\nquent percept is done in two stages. The ﬁrst stage calculates the belief state after the action,\njust as for the sensorless agent:\nˆb = (b −DEL(a)) ∪ADD(a)\nwhere, as before, we have assumed a belief state represented as a conjunction of literals. The\nsecond stage is a little trickier. Suppose that percept literals p1, . . . , pk are received. One\nmight think that we simply need to add these into the belief state; in fact, we can also infer 422\nChapter\n11.\nPlanning and Acting in the Real World\nthat the preconditions for sensing are satisﬁed. Now, if a percept p has exactly one percept\naxiom, Percept(p, PRECOND:c), where c is a conjunction of literals, then those literals can\nbe thrown into the belief state along with p. On the other hand, if p has more than one percept\naxiom whose preconditions might hold according to the predicted belief state ˆb, then we have\nto add in the disjunction of the preconditions. Obviously, this takes the belief state outside\n1-CNF and brings up the same complications as conditional effects, with much the same\nclasses of solutions.\nGiven a mechanism for computing exact or approximate belief states, we can generate\ncontingent plans with an extension of the AND–OR forward search over belief states used\nin Section 4.4. Actions with nondeterministic effects—which are deﬁned simply by using a\ndisjunction in the EFFECT of the action schema—can be accommodated with minor changes\nto the belief-state update calculation and no change to the search algorithm.2 For the heuristic\nfunction, many of the methods suggested for sensorless planning are also applicable in the\npartially observable, nondeterministic case.\n11.3.3\nOnline replanning\nImagine watching a spot-welding robot in a car plant. The robot’s fast, accurate motions are\nrepeated over and over again as each car passes down the line. Although technically im-\npressive, the robot probably does not seem at all intelligent because the motion is a ﬁxed,\npreprogrammed sequence; the robot obviously doesn’t “know what it’s doing” in any mean-\ningful sense. Now suppose that a poorly attached door falls off the car just as the robot is\nabout to apply a spot-weld. The robot quickly replaces its welding actuator with a gripper,",
  "ingful sense. Now suppose that a poorly attached door falls off the car just as the robot is\nabout to apply a spot-weld. The robot quickly replaces its welding actuator with a gripper,\npicks up the door, checks it for scratches, reattaches it to the car, sends an email to the ﬂoor\nsupervisor, switches back to the welding actuator, and resumes its work. All of a sudden,\nthe robot’s behavior seems purposive rather than rote; we assume it results not from a vast,\nprecomputed contingent plan but from an online replanning process—which means that the\nrobot does need to know what it’s trying to do.\nReplanning presupposes some form of execution monitoring to determine the need for\nEXECUTION\nMONITORING\na new plan. One such need arises when a contingent planning agent gets tired of planning\nfor every little contingency, such as whether the sky might fall on its head.3 Some branches\nof a partially constructed contingent plan can simply say Replan; if such a branch is reached\nduring execution, the agent reverts to planning mode. As we mentioned earlier, the decision\nas to how much of the problem to solve in advance and how much to leave to replanning\nis one that involves tradeoffs among possible events with different costs and probabilities of\noccurring. Nobody wants to have their car break down in the middle of the Sahara desert and\nonly then think about having enough water.\n2 If cyclic solutions are required for a nondeterministic problem, AND–OR search must be generalized to a loopy\nversion such as LAO∗(Hansen and Zilberstein, 2001).\n3 In 1954, a Mrs. Hodges of Alabama was hit by meteorite that crashed through her roof. In 1992, a piece of\nthe Mbale meteorite hit a small boy on the head; fortunately, its descent was slowed by banana leaves (Jenniskens\net al., 1994). And in 2009, a German boy claimed to have been hit in the hand by a pea-sized meteorite. No serious\ninjuries resulted from any of these incidents, suggesting that the need for preplanning against such contingencies\nis sometimes overstated. Section 11.3.\nPlanning and Acting in Nondeterministic Domains\n423\nwhole plan\nplan\nrepair\nS\nP\nO\nE\nG\ncontinuation\nFigure 11.9\nBefore execution, the planner comes up with a plan, here called whole plan,\nto get from S to G. The agent executes steps of the plan until it expects to be in state E, but\nobserves it is actually in O. The agent then replans for the minimal repair plus continuation\nto reach G.",
  "to get from S to G. The agent executes steps of the plan until it expects to be in state E, but\nobserves it is actually in O. The agent then replans for the minimal repair plus continuation\nto reach G.\nReplanning may also be needed if the agent’s model of the world is incorrect. The model\nfor an action may have a missing precondition—for example, the agent may not know that\nMISSING\nPRECONDITION\nremoving the lid of a paint can often requires a screwdriver; the model may have a missing\neffect—for example, painting an object may get paint on the ﬂoor as well; or the model may\nMISSING EFFECT\nhave a missing state variable—for example, the model given earlier has no notion of the\nMISSING STATE\nVARIABLE\namount of paint in a can, of how its actions affect this amount, or of the need for the amount\nto be nonzero. The model may also lack provision for exogenous events such as someone\nEXOGENOUS EVENT\nknocking over the paint can. Exogenous events can also include changes in the goal, such\nas the addition of the requirement that the table and chair not be painted black. Without the\nability to monitor and replan, an agent’s behavior is likely to be extremely fragile if it relies\non absolute correctness of its model.\nThe online agent has a choice of how carefully to monitor the environment. We distin-\nguish three levels:\n• Action monitoring: before executing an action, the agent veriﬁes that all the precondi-\nACTION MONITORING\ntions still hold.\n• Plan monitoring: before executing an action, the agent veriﬁes that the remaining plan\nPLAN MONITORING\nwill still succeed.\n• Goal monitoring: before executing an action, the agent checks to see if there is a better\nGOAL MONITORING\nset of goals it could be trying to achieve.\nIn Figure 11.9 we see a schematic of action monitoring. The agent keeps track of both its\noriginal plan, wholeplan, and the part of the plan that has not been executed yet, which is\ndenoted by plan. After executing the ﬁrst few steps of the plan, the agent expects to be in\nstate E. But the agent observes it is actually in state O. It then needs to repair the plan by\nﬁnding some point P on the original plan that it can get back to. (It may be that P is the goal\nstate, G.) The agent tries to minimize the total cost of the plan: the repair part (from O to P)\nplus the continuation (from P to G). 424\nChapter\n11.\nPlanning and Acting in the Real World\nNow let’s return to the example problem of achieving a chair and table of matching",
  "plus the continuation (from P to G). 424\nChapter\n11.\nPlanning and Acting in the Real World\nNow let’s return to the example problem of achieving a chair and table of matching\ncolor. Suppose the agent comes up with this plan:\n[LookAt(Table), LookAt(Chair),\nif Color(Table, c) ∧Color(Chair, c) then NoOp\nelse [RemoveLid(Can1), LookAt(Can1),\nif Color(Table, c) ∧Color(Can1, c) then Paint(Chair, Can1)\nelse REPLAN]] .\nNow the agent is ready to execute the plan. Suppose the agent observes that the table and\ncan of paint are white and the chair is black. It then executes Paint(Chair, Can1). At this\npoint a classical planner would declare victory; the plan has been executed. But an online\nexecution monitoring agent needs to check the preconditions of the remaining empty plan—\nthat the table and chair are the same color. Suppose the agent perceives that they do not\nhave the same color—in fact, the chair is now a mottled gray because the black paint is\nshowing through. The agent then needs to ﬁgure out a position in whole plan to aim for\nand a repair action sequence to get there. The agent notices that the current state is identical\nto the precondition before the Paint(Chair, Can1) action, so the agent chooses the empty\nsequence for repair and makes its plan be the same [Paint] sequence that it just attempted.\nWith this new plan in place, execution monitoring resumes, and the Paint action is retried.\nThis behavior will loop until the chair is perceived to be completely painted. But notice that\nthe loop is created by a process of plan–execute–replan, rather than by an explicit loop in a\nplan. Note also that the original plan need not cover every contingency. If the agent reaches\nthe step marked REPLAN, it can then generate a new plan (perhaps involving Can2).\nAction monitoring is a simple method of execution monitoring, but it can sometimes\nlead to less than intelligent behavior. For example, suppose there is no black or white paint,\nand the agent constructs a plan to solve the painting problem by painting both the chair and\ntable red. Suppose that there is only enough red paint for the chair. With action monitoring,\nthe agent would go ahead and paint the chair red, then notice that it is out of paint and cannot\npaint the table, at which point it would replan a repair—perhaps painting both chair and table\ngreen. A plan-monitoring agent can detect failure whenever the current state is such that the",
  "paint the table, at which point it would replan a repair—perhaps painting both chair and table\ngreen. A plan-monitoring agent can detect failure whenever the current state is such that the\nremaining plan no longer works. Thus, it would not waste time painting the chair red. Plan\nmonitoring achieves this by checking the preconditions for success of the entire remaining\nplan—that is, the preconditions of each step in the plan, except those preconditions that are\nachieved by another step in the remaining plan. Plan monitoring cuts off execution of a\ndoomed plan as soon as possible, rather than continuing until the failure actually occurs.4\nPlan monitoring also allows for serendipity—accidental success. If someone comes along\nand paints the table red at the same time that the agent is painting the chair red, then the ﬁnal\nplan preconditions are satisﬁed (the goal has been achieved), and the agent can go home early.\nIt is straightforward to modify a planning algorithm so that each action in the plan\nis annotated with the action’s preconditions, thus enabling action monitoring. It is slightly\n4 Plan monitoring means that ﬁnally, after 424 pages, we have an agent that is smarter than a dung beetle (see\npage 39). A plan-monitoring agent would notice that the dung ball was missing from its grasp and would replan\nto get another ball and plug its hole. Section 11.4.\nMultiagent Planning\n425\nmore complex to enable plan monitoring. Partial-order and planning-graph planners have\nthe advantage that they have already built up structures that contain the relations necessary\nfor plan monitoring. Augmenting state-space planners with the necessary annotations can be\ndone by careful bookkeeping as the goal ﬂuents are regressed through the plan.\nNow that we have described a method for monitoring and replanning, we need to ask,\n“Does it work?” This is a surprisingly tricky question. If we mean, “Can we guarantee that\nthe agent will always achieve the goal?” then the answer is no, because the agent could\ninadvertently arrive at a dead end from which there is no repair. For example, the vacuum\nagent might have a faulty model of itself and not know that its batteries can run out. Once\nthey do, it cannot repair any plans. If we rule out dead ends—assume that there exists a plan\nto reach the goal from any state in the environment—and assume that the environment is\nreally nondeterministic, in the sense that such a plan always has some chance of success on",
  "to reach the goal from any state in the environment—and assume that the environment is\nreally nondeterministic, in the sense that such a plan always has some chance of success on\nany given execution attempt, then the agent will eventually reach the goal.\nTrouble occurs when an action is actually not nondeterministic, but rather depends on\nsome precondition that the agent does not know about. For example, sometimes a paint\ncan may be empty, so painting from that can has no effect. No amount of retrying is going to\nchange this.5 One solution is to choose randomly from among the set of possible repair plans,\nrather than to try the same one each time. In this case, the repair plan of opening another can\nmight work. A better approach is to learn a better model. Every prediction failure is an\nopportunity for learning; an agent should be able to modify its model of the world to accord\nwith its percepts. From then on, the replanner will be able to come up with a repair that gets\nat the root problem, rather than relying on luck to choose a good repair. This kind of learning\nis described in Chapters 18 and 19.\n11.4\nMULTIAGENT PLANNING\nSo far, we have assumed that only one agent is doing the sensing, planning, and acting.\nWhen there are multiple agents in the environment, each agent faces a multiagent planning\nproblem in which it tries to achieve its own goals with the help or hindrance of others.\nMULTIAGENT\nPLANNING PROBLEM\nBetween the purely single-agent and truly multiagent cases is a wide spectrum of prob-\nlems that exhibit various degrees of decomposition of the monolithic agent. An agent with\nmultiple effectors that can operate concurrently—for example, a human who can type and\nspeak at the same time—needs to do multieffector planning to manage each effector while\nMULTIEFFECTOR\nPLANNING\nhandling positive and negative interactions among the effectors.\nWhen the effectors are\nphysically decoupled into detached units—as in a ﬂeet of delivery robots in a factory—\nmultieffector planning becomes multibody planning. A multibody problem is still a “stan-\nMULTIBODY\nPLANNING\ndard” single-agent problem as long as the relevant sensor information collected by each body\ncan be pooled—either centrally or within each body—to form a common estimate of the\nworld state that then informs the execution of the overall plan; in this case, the multiple bod-\nies act as a single body. When communication constraints make this impossible, we have",
  "world state that then informs the execution of the overall plan; in this case, the multiple bod-\nies act as a single body. When communication constraints make this impossible, we have\n5 Futile repetition of a plan repair is exactly the behavior exhibited by the sphex wasp (page 39). 426\nChapter\n11.\nPlanning and Acting in the Real World\nwhat is sometimes called a decentralized planning problem; this is perhaps a misnomer, be-\nDECENTRALIZED\nPLANNING\ncause the planning phase is centralized but the execution phase is at least partially decoupled.\nIn this case, the subplan constructed for each body may need to include explicit communica-\ntive actions with other bodies. For example, multiple reconnaissance robots covering a wide\narea may often be out of radio contact with each other and should share their ﬁndings during\ntimes when communication is feasible.\nWhen a single entity is doing the planning, there is really only one goal, which all the\nbodies necessarily share. When the bodies are distinct agents that do their own planning, they\nmay still share identical goals; for example, two human tennis players who form a doubles\nteam share the goal of winning the match. Even with shared goals, however, the multibody\nand multiagent cases are quite different. In a multibody robotic doubles team, a single plan\ndictates which body will go where on the court and which body will hit the ball. In a multi-\nagent doubles team, on the other hand, each agent decides what to do; without some method\nfor coordination, both agents may decide to cover the same part of the court and each may\nCOORDINATION\nleave the ball for the other to hit.\nThe clearest case of a multiagent problem, of course, is when the agents have different\ngoals. In tennis, the goals of two opposing teams are in direct conﬂict, leading to the zero-\nsum situation of Chapter 5. Spectators could be viewed as agents if their support or disdain\nis a signiﬁcant factor and can be inﬂuenced by the players’ conduct; otherwise, they can be\ntreated as an aspect of nature—just like the weather—that is assumed to be indifferent to the\nplayers’ intentions.6\nFinally, some systems are a mixture of centralized and multiagent planning. For ex-\nample, a delivery company may do centralized, ofﬂine planning for the routes of its trucks\nand planes each day, but leave some aspects open for autonomous decisions by drivers and\npilots who can respond individually to trafﬁc and weather situations. Also, the goals of the",
  "and planes each day, but leave some aspects open for autonomous decisions by drivers and\npilots who can respond individually to trafﬁc and weather situations. Also, the goals of the\ncompany and its employees are brought into alignment, to some extent, by the payment of\nincentives (salaries and bonuses)—a sure sign that this is a true multiagent system.\nINCENTIVE\nThe issues involved in multiagent planning can be divided roughly into two sets. The\nﬁrst, covered in Section 11.4.1, involves issues of representing and planning for multiple\nsimultaneous actions; these issues occur in all settings from multieffector to multiagent plan-\nning. The second, covered in Section 11.4.2, involves issues of cooperation, coordination,\nand competition arising in true multiagent settings.\n11.4.1\nPlanning with multiple simultaneous actions\nFor the time being, we will treat the multieffector, multibody, and multiagent settings in the\nsame way, labeling them generically as multiactor settings, using the generic term actor to\nMULTIACTOR\nACTOR\ncover effectors, bodies, and agents. The goal of this section is to work out how to deﬁne\ntransition models, correct plans, and efﬁcient planning algorithms for the multiactor setting.\nA correct plan is one that, if executed by the actors, achieves the goal. (In the true multiagent\nsetting, of course, the agents may not agree to execute any particular plan, but at least they\n6 We apologize to residents of the United Kingdom, where the mere act of contemplating a game of tennis\nguarantees rain. Section 11.4.\nMultiagent Planning\n427\nActors(A, B)\nInit(At(A, LeftBaseline) ∧At(B, RightNet) ∧\nApproaching(Ball, RightBaseline)) ∧Partner(A, B) ∧Partner(B, A)\nGoal(Returned(Ball) ∧(At(a, RightNet) ∨At(a, LeftNet))\nAction(Hit(actor, Ball),\nPRECOND:Approaching(Ball, loc) ∧At(actor, loc)\nEFFECT:Returned(Ball))\nAction(Go(actor, to),\nPRECOND:At(actor,loc) ∧to ̸= loc,\nEFFECT:At(actor, to) ∧¬ At(actor, loc))\nFigure 11.10\nThe doubles tennis problem. Two actors A and B are playing together and\ncan be in one of four locations: LeftBaseline, RightBaseline, LeftNet, and RightNet. The\nball can be returned only if a player is in the right place. Note that each action must include\nthe actor as an argument.\nwill know what plans would work if they did agree to execute them.) For simplicity, we\nassume perfect synchronization: each action takes the same amount of time and actions at\nSYNCHRONIZATION\neach point in the joint plan are simultaneous.",
  "will know what plans would work if they did agree to execute them.) For simplicity, we\nassume perfect synchronization: each action takes the same amount of time and actions at\nSYNCHRONIZATION\neach point in the joint plan are simultaneous.\nWe begin with the transition model; for the deterministic case, this is the function\nRESULT(s, a). In the single-agent setting, there might be b different choices for the action;\nb can be quite large, especially for ﬁrst-order representations with many objects to act on,\nbut action schemas provide a concise representation nonetheless. In the multiactor setting\nwith n actors, the single action a is replaced by a joint action ⟨a1, . . . , an⟩, where ai is the\nJOINT ACTION\naction taken by the ith actor. Immediately, we see two problems: ﬁrst, we have to describe\nthe transition model for bn different joint actions; second, we have a joint planning problem\nwith a branching factor of bn.\nHaving put the actors together into a multiactor system with a huge branching factor,\nthe principal focus of research on multiactor planning has been to decouple the actors to\nthe extent possible, so that the complexity of the problem grows linearly with n rather than\nexponentially. If the actors have no interaction with one another—for example, n actors each\nplaying a game of solitaire—then we can simply solve n separate problems. If the actors are\nloosely coupled, can we attain something close to this exponential improvement? This is, of\nLOOSELY COUPLED\ncourse, a central question in many areas of AI. We have seen it explicitly in the context of\nCSPs, where “tree like” constraint graphs yielded efﬁcient solution methods (see page 225),\nas well as in the context of disjoint pattern databases (page 106) and additive heuristics for\nplanning (page 378).\nThe standard approach to loosely coupled problems is to pretend the problems are com-\npletely decoupled and then ﬁx up the interactions. For the transition model, this means writing\naction schemas as if the actors acted independently. Let’s see how this works for the doubles\ntennis problem. Let’s suppose that at one point in the game, the team has the goal of returning\nthe ball that has been hit to them and ensuring that at least one of them is covering the net. 428\nChapter\n11.\nPlanning and Acting in the Real World\nA ﬁrst pass at a multiactor deﬁnition might look like Figure 11.10. With this deﬁnition, it is\neasy to see that the following joint plan plan works:\nJOINT PLAN\nPLAN 1:",
  "Chapter\n11.\nPlanning and Acting in the Real World\nA ﬁrst pass at a multiactor deﬁnition might look like Figure 11.10. With this deﬁnition, it is\neasy to see that the following joint plan plan works:\nJOINT PLAN\nPLAN 1:\nA : [Go(A, RightBaseline), Hit(A, Ball)]\nB : [NoOp(B), NoOp(B)] .\nProblems arise, however, when a plan has both agents hitting the ball at the same time. In the\nreal world, this won’t work, but the action schema for Hit says that the ball will be returned\nsuccessfully. Technically, the difﬁculty is that preconditions constrain the state in which an\naction can be executed successfully, but do not constrain other actions that might mess it up.\nWe solve this by augmenting action schemas with one new feature: a concurrent action list\nCONCURRENT\nACTION LIST\nstating which actions must or must not be executed concurrently. For example, the Hit action\ncould be described as follows:\nAction(Hit(a, Ball),\nCONCURRENT:b ̸= a ⇒¬Hit(b, Ball)\nPRECOND:Approaching(Ball, loc) ∧At(a, loc)\nEFFECT:Returned(Ball)) .\nIn other words, the Hit action has its stated effect only if no other Hit action by another\nagent occurs at the same time. (In the SATPLAN approach, this would be handled by a\npartial action exclusion axiom.) For some actions, the desired effect is achieved only when\nanother action occurs concurrently. For example, two agents are needed to carry a cooler full\nof beverages to the tennis court:\nAction(Carry(a, cooler, here, there),\nCONCURRENT:b ̸= a ∧Carry(b, cooler, here, there)\nPRECOND:At(a, here) ∧At(cooler, here) ∧Cooler(cooler)\nEFFECT:At(a, there) ∧At(cooler, there) ∧¬At(a, here) ∧¬At(cooler, here)).\nWith these kinds of action schemas, any of the planning algorithms described in Chapter 10\ncan be adapted with only minor modiﬁcations to generate multiactor plans. To the extent that\nthe coupling among subplans is loose—meaning that concurrency constraints come into play\nonly rarely during plan search—one would expect the various heuristics derived for single-\nagent planning to also be effective in the multiactor context. We could extend this approach\nwith the reﬁnements of the last two chapters—HTNs, partial observability, conditionals, exe-\ncution monitoring, and replanning—but that is beyond the scope of this book.\n11.4.2\nPlanning with multiple agents: Cooperation and coordination\nNow let us consider the true multiagent setting in which each agent makes its own plan. To",
  "cution monitoring, and replanning—but that is beyond the scope of this book.\n11.4.2\nPlanning with multiple agents: Cooperation and coordination\nNow let us consider the true multiagent setting in which each agent makes its own plan. To\nstart with, let us assume that the goals and knowledge base are shared. One might think\nthat this reduces to the multibody case—each agent simply computes the joint solution and\nexecutes its own part of that solution. Alas, the “the” in “the joint solution” is misleading.\nFor our doubles team, more than one joint solution exists:\nPLAN 2:\nA : [Go(A, LeftNet), NoOp(A)]\nB : [Go(B, RightBaseline), Hit(B, Ball)] . Section 11.4.\nMultiagent Planning\n429\nIf both agents can agree on either plan 1 or plan 2, the goal will be achieved. But if A chooses\nplan 2 and B chooses plan 1, then nobody will return the ball. Conversely, if A chooses 1 and\nB chooses 2, then they will both try to hit the ball. The agents may realize this, but how can\nthey coordinate to make sure they agree on the plan?\nOne option is to adopt a convention before engaging in joint activity. A convention is\nCONVENTION\nany constraint on the selection of joint plans. For example, the convention “stick to your side\nof the court” would rule out plan 1, causing the doubles partners to select plan 2. Drivers on\na road face the problem of not colliding with each other; this is (partially) solved by adopting\nthe convention “stay on the right side of the road” in most countries; the alternative, “stay\non the left side,” works equally well as long as all agents in an environment agree. Similar\nconsiderations apply to the development of human language, where the important thing is not\nwhich language each individual should speak, but the fact that a community all speaks the\nsame language. When conventions are widespread, they are called social laws.\nSOCIAL LAWS\nIn the absence of a convention, agents can use communication to achieve common\nknowledge of a feasible joint plan. For example, a tennis player could shout “Mine!” or\n“Yours!” to indicate a preferred joint plan. We cover mechanisms for communication in more\ndepth in Chapter 22, where we observe that communication does not necessarily involve a\nverbal exchange. For example, one player can communicate a preferred joint plan to the other\nsimply by executing the ﬁrst part of it. If agent A heads for the net, then agent B is obliged",
  "verbal exchange. For example, one player can communicate a preferred joint plan to the other\nsimply by executing the ﬁrst part of it. If agent A heads for the net, then agent B is obliged\nto go back to the baseline to hit the ball, because plan 2 is the only joint plan that begins with\nA’s heading for the net. This approach to coordination, sometimes called plan recognition,\nPLAN RECOGNITION\nworks when a single action (or short sequence of actions) is enough to determine a joint plan\nunambiguously. Note that communication can work as well with competitive agents as with\ncooperative ones.\nConventions can also arise through evolutionary processes. For example, seed-eating\nharvester ants are social creatures that evolved from the less social wasps. Colonies of ants\nexecute very elaborate joint plans without any centralized control—the queen’s job is to re-\nproduce, not to do centralized planning—and with very limited computation, communica-\ntion, and memory capabilities in each ant (Gordon, 2000, 2007). The colony has many roles,\nincluding interior workers, patrollers, and foragers. Each ant chooses to perform a role ac-\ncording to the local conditions it observes. For example, foragers travel away from the nest,\nsearch for a seed, and when they ﬁnd one, bring it back immediately. Thus, the rate at which\nforagers return to the nest is an approximation of the availability of food today. When the\nrate is high, other ants abandon their current role and take on the role of scavenger. The ants\nappear to have a convention on the importance of roles—foraging is the most important—and\nants will easily switch into the more important roles, but not into the less important. There is\nsome learning mechanism: a colony learns to make more successful and prudent actions over\nthe course of its decades-long life, even though individual ants live only about a year.\nOne ﬁnal example of cooperative multiagent behavior appears in the ﬂocking behavior\nof birds. We can obtain a reasonable simulation of a ﬂock if each bird agent (sometimes\ncalled a boid) observes the positions of its nearest neighbors and then chooses the heading\nBOID\nand acceleration that maximizes the weighted sum of these three components: 430\nChapter\n11.\nPlanning and Acting in the Real World\n(a)\n(b)\n(c)\nFigure 11.11\n(a) A simulated ﬂock of birds, using Reynold’s boids model. Image courtesy\nGiuseppe Randazzo, novastructura.net. (b) An actual ﬂock of starlings. Image by Eduardo",
  "Chapter\n11.\nPlanning and Acting in the Real World\n(a)\n(b)\n(c)\nFigure 11.11\n(a) A simulated ﬂock of birds, using Reynold’s boids model. Image courtesy\nGiuseppe Randazzo, novastructura.net. (b) An actual ﬂock of starlings. Image by Eduardo\n(pastaboy sleeps on ﬂickr). (c) Two competitive teams of agents attempting to capture the\ntowers in the NERO game. Image courtesy Risto Miikkulainen.\n1. Cohesion: a positive score for getting closer to the average position of the neighbors\n2. Separation: a negative score for getting too close to any one neighbor\n3. Alignment: a positive score for getting closer to the average heading of the neighbors\nIf all the boids execute this policy, the ﬂock exhibits the emergent behavior of ﬂying as a\nEMERGENT\nBEHAVIOR\npseudorigid body with roughly constant density that does not disperse over time, and that\noccasionally makes sudden swooping motions. You can see a still images in Figure 11.11(a)\nand compare it to an actual ﬂock in (b). As with ants, there is no need for each agent to\npossess a joint plan that models the actions of other agents.\nThe most difﬁcult multiagent problems involve both cooperation with members of one’s\nown team and competition against members of opposing teams, all without centralized con-\ntrol. We see this in games such as robotic soccer or the NERO game shown in Figure 11.11(c),\nin which two teams of software agents compete to capture the control towers. As yet, meth-\nods for efﬁcient planning in these kinds of environments—for example, taking advantage of\nloose coupling—are in their infancy.\n11.5\nSUMMARY\nThis chapter has addressed some of the complications of planning and acting in the real world.\nThe main points:\n• Many actions consume resources, such as money, gas, or raw materials. It is convenient\nto treat these resources as numeric measures in a pool rather than try to reason about,\nsay, each individual coin and bill in the world. Actions can generate and consume\nresources, and it is usually cheap and effective to check partial plans for satisfaction of\nresource constraints before attempting further reﬁnements.\n• Time is one of the most important resources. It can be handled by specialized schedul-\ning algorithms, or scheduling can be integrated with planning. Bibliographical and Historical Notes\n431\n• Hierarchical task network (HTN) planning allows the agent to take advice from the\ndomain designer in the form of high-level actions (HLAs) that can be implemented in",
  "431\n• Hierarchical task network (HTN) planning allows the agent to take advice from the\ndomain designer in the form of high-level actions (HLAs) that can be implemented in\nvarious ways by lower-level action sequences. The effects of HLAs can be deﬁned with\nangelic semantics, allowing provably correct high-level plans to be derived without\nconsideration of lower-level implementations. HTN methods can create the very large\nplans required by many real-world applications.\n• Standard planning algorithms assume complete and correct information and determin-\nistic, fully observable environments. Many domains violate this assumption.\n• Contingent plans allow the agent to sense the world during execution to decide what\nbranch of the plan to follow. In some cases, sensorless or conformant planning can be\nused to construct a plan that works without the need for perception. Both conformant\nand contingent plans can be constructed by search in the space of belief states. Efﬁcient\nrepresentation or computation of belief states is a key problem.\n• An online planning agent uses execution monitoring and splices in repairs as needed\nto recover from unexpected situations, which can be due to nondeterministic actions,\nexogenous events, or incorrect models of the environment.\n• Multiagent planning is necessary when there are other agents in the environment with\nwhich to cooperate or compete. Joint plans can be constructed, but must be augmented\nwith some form of coordination if two agents are to agree on which joint plan to execute.\n• This chapter extends classic planning to cover nondeterministic environments (where\noutcomes of actions are uncertain), but it is not the last word on planning. Chapter 17\ndescribes techniques for stochastic environments (in which outcomes of actions have\nprobabilities associated with them): Markov decision processes, partially observable\nMarkov decision processes, and game theory. In Chapter 21 we show that reinforcement\nlearning allows an agent to learn how to behave from past successes and failures.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nPlanning with time constraints was ﬁrst dealt with by DEVISER (Vere, 1983). The repre-\nsentation of time in plans was addressed by Allen (1984) and by Dean et al. (1990) in the\nFORBIN system. NONLIN+ (Tate and Whiter, 1984) and SIPE (Wilkins, 1988, 1990) could\nreason about the allocation of limited resources to various plan steps. O-PLAN (Bell and",
  "FORBIN system. NONLIN+ (Tate and Whiter, 1984) and SIPE (Wilkins, 1988, 1990) could\nreason about the allocation of limited resources to various plan steps. O-PLAN (Bell and\nTate, 1985), an HTN planner, had a uniform, general representation for constraints on time\nand resources. In addition to the Hitachi application mentioned in the text, O-PLAN has\nbeen applied to software procurement planning at Price Waterhouse and back-axle assembly\nplanning at Jaguar Cars.\nThe two planners SAPA (Do and Kambhampati, 2001) and T4 (Haslum and Geffner,\n2001) both used forward state-space search with sophisticated heuristics to handle actions\nwith durations and resources. An alternative is to use very expressive action languages, but\nguide them by human-written domain-speciﬁc heuristics, as is done by ASPEN (Fukunaga\net al., 1997), HSTS (Jonsson et al., 2000), and IxTeT (Ghallab and Laruelle, 1994). 432\nChapter\n11.\nPlanning and Acting in the Real World\nA number of hybrid planning-and-scheduling systems have been deployed: ISIS (Fox\net al., 1982; Fox, 1990) has been used for job shop scheduling at Westinghouse, GARI (De-\nscotte and Latombe, 1985) planned the machining and construction of mechanical parts,\nFORBIN was used for factory control, and NONLIN+ was used for naval logistics planning.\nWe chose to present planning and scheduling as two separate problems; (Cushing et al., 2007)\nshow that this can lead to incompleteness on certain problems. There is a long history of\nscheduling in aerospace. T-SCHED (Drabble, 1990) was used to schedule mission-command\nsequences for the UOSAT-II satellite. OPTIMUM-AIV (Aarup et al., 1994) and PLAN-ERS1\n(Fuchs et al., 1990), both based on O-PLAN, were used for spacecraft assembly and obser-\nvation planning, respectively, at the European Space Agency. SPIKE (Johnston and Adorf,\n1992) was used for observation planning at NASA for the Hubble Space Telescope, while\nthe Space Shuttle Ground Processing Scheduling System (Deale et al., 1994) does job-shop\nscheduling of up to 16,000 worker-shifts. Remote Agent (Muscettola et al., 1998) became\nthe ﬁrst autonomous planner–scheduler to control a spacecraft when it ﬂew onboard the Deep\nSpace One probe in 1999. Space applications have driven the development of algorithms for\nresource allocations; see Laborie (2003) and Muscettola (2002). The literature on scheduling\nis presented in a classic survey article (Lawler et al., 1993), a recent book (Pinedo, 2008),",
  "resource allocations; see Laborie (2003) and Muscettola (2002). The literature on scheduling\nis presented in a classic survey article (Lawler et al., 1993), a recent book (Pinedo, 2008),\nand an edited handbook (Blazewicz et al., 2007).\nThe facility in the STRIPS program for learning macrops—“macro-operators” consist-\nMACROPS\ning of a sequence of primitive steps—could be considered the ﬁrst mechanism for hierarchi-\ncal planning (Fikes et al., 1972). Hierarchy was also used in the LAWALY system (Siklossy\nand Dreussi, 1973). The ABSTRIPS system (Sacerdoti, 1974) introduced the idea of an ab-\nstraction hierarchy, whereby planning at higher levels was permitted to ignore lower-level\nABSTRACTION\nHIERARCHY\npreconditions of actions in order to derive the general structure of a working plan. Austin\nTate’s Ph.D. thesis (1975b) and work by Earl Sacerdoti (1977) developed the basic ideas of\nHTN planning in its modern form. Many practical planners, including O-PLAN and SIPE,\nare HTN planners. Yang (1990) discusses properties of actions that make HTN planning ef-\nﬁcient. Erol, Hendler, and Nau (1994, 1996) present a complete hierarchical decomposition\nplanner as well as a range of complexity results for pure HTN planners. Our presentation of\nHLAs and angelic semantics is due to Marthi et al. (2007, 2008). Kambhampati et al. (1998)\nhave proposed an approach in which decompositions are just another form of plan reﬁnement,\nsimilar to the reﬁnements for non-hierarchical partial-order planning.\nBeginning with the work on macro-operators in STRIPS, one of the goals of hierarchical\nplanning has been the reuse of previous planning experience in the form of generalized plans.\nThe technique of explanation-based learning, described in depth in Chapter 19, has been\napplied in several systems as a means of generalizing previously computed plans, including\nSOAR (Laird et al., 1986) and PRODIGY (Carbonell et al., 1989). An alternative approach is\nto store previously computed plans in their original form and then reuse them to solve new,\nsimilar problems by analogy to the original problem. This is the approach taken by the ﬁeld\ncalled case-based planning (Carbonell, 1983; Alterman, 1988; Hammond, 1989). Kamb-\nCASE-BASED\nPLANNING\nhampati (1994) argues that case-based planning should be analyzed as a form of reﬁnement\nplanning and provides a formal foundation for case-based partial-order planning. Bibliographical and Historical Notes\n433",
  "CASE-BASED\nPLANNING\nhampati (1994) argues that case-based planning should be analyzed as a form of reﬁnement\nplanning and provides a formal foundation for case-based partial-order planning. Bibliographical and Historical Notes\n433\nEarly planners lacked conditionals and loops, but some could use coercion to form\nconformant plans. Sacerdoti’s NOAH solved the “keys and boxes” problem, a planning chal-\nlenge problem in which the planner knows little about the initial state, using coercion. Ma-\nson (1993) argued that sensing often can and should be dispensed with in robotic planning,\nand described a sensorless plan that can move a tool into a speciﬁc position on a table by a\nsequence of tilting actions, regardless of the initial position.\nGoldman and Boddy (1996) introduced the term conformant planning, noting that sen-\nsorless plans are often effective even if the agent has sensors. The ﬁrst moderately efﬁcient\nconformant planner was Smith and Weld’s (1998) Conformant Graphplan or CGP. Ferraris\nand Giunchiglia (2000) and Rintanen (1999) independently developed SATPLAN-based con-\nformant planners. Bonet and Geffner (2000) describe a conformant planner based on heuristic\nsearch in the space of belief states, drawing on ideas ﬁrst developed in the 1960s for partially\nobservable Markov decision processes, or POMDPs (see Chapter 17).\nCurrently, there are three main approaches to conformant planning. The ﬁrst two use\nheuristic search in belief-state space: HSCP (Bertoli et al., 2001a) uses binary decision\ndiagrams (BDDs) to represent belief states, whereas Hoffmann and Brafman (2006) adopt\nthe lazy approach of computing precondition and goal tests on demand using a SAT solver.\nThe third approach, championed primarily by Jussi Rintanen (2007), formulates the entire\nsensorless planning problem as a quantiﬁed Boolean formula (QBF) and solves it using a\ngeneral-purpose QBF solver. Current conformant planners are ﬁve orders of magnitude faster\nthan CGP. The winner of the 2006 conformant-planning track at the International Planning\nCompetition was T0 (Palacios and Geffner, 2007), which uses heuristic search in belief-state\nspace while keeping the belief-state representation simple by deﬁning derived literals that\ncover conditional effects. Bryce and Kambhampati (2007) discuss how a planning graph can\nbe generalized to generate good heuristics for conformant and contingent planning.\nThere has been some confusion in the literature between the terms “conditional” and",
  "be generalized to generate good heuristics for conformant and contingent planning.\nThere has been some confusion in the literature between the terms “conditional” and\n“contingent” planning. Following Majercik and Littman (2003), we use “conditional” to\nmean a plan (or action) that has different effects depending on the actual state of the world,\nand “contingent” to mean a plan in which the agent can choose different actions depending\non the results of sensing. The problem of contingent planning received more attention after\nthe publication of Drew McDermott’s (1978a) inﬂuential article, Planning and Acting.\nThe contingent-planning approach described in the chapter is based on Hoffmann and\nBrafman (2005), and was inﬂuenced by the efﬁcient search algorithms for cyclic AND–OR\ngraphs developed by Jimenez and Torras (2000) and Hansen and Zilberstein (2001). Bertoli\net al. (2001b) describe MBP (Model-Based Planner), which uses binary decision diagrams\nto do conformant and contingent planning.\nIn retrospect, it is now possible to see how the major classical planning algorithms led\nto extended versions for uncertain domains. Fast-forward heuristic search through state space\nled to forward search in belief space (Bonet and Geffner, 2000; Hoffmann and Brafman,\n2005); SATPLAN led to stochastic SATPLAN (Majercik and Littman, 2003) and to planning\nwith quantiﬁed Boolean logic (Rintanen, 2007); partial order planning led to UWL (Etzioni\net al., 1992) and CNLP (Peot and Smith, 1992); GRAPHPLAN led to Sensory Graphplan or\nSGP (Weld et al., 1998). 434\nChapter\n11.\nPlanning and Acting in the Real World\nThe ﬁrst online planner with execution monitoring was PLANEX (Fikes et al., 1972),\nwhich worked with the STRIPS planner to control the robot Shakey. The NASL planner\n(McDermott, 1978a) treated a planning problem simply as a speciﬁcation for carrying out a\ncomplex action, so that execution and planning were completely uniﬁed. SIPE (System for\nInteractive Planning and Execution monitoring) (Wilkins, 1988, 1990) was the ﬁrst planner\nto deal systematically with the problem of replanning. It has been used in demonstration\nprojects in several domains, including planning operations on the ﬂight deck of an aircraft\ncarrier, job-shop scheduling for an Australian beer factory, and planning the construction of\nmultistory buildings (Kartam and Levitt, 1990).\nIn the mid-1980s, pessimism about the slow run times of planning systems led to the",
  "carrier, job-shop scheduling for an Australian beer factory, and planning the construction of\nmultistory buildings (Kartam and Levitt, 1990).\nIn the mid-1980s, pessimism about the slow run times of planning systems led to the\nproposal of reﬂex agents called reactive planning systems (Brooks, 1986; Agre and Chap-\nREACTIVE PLANNING\nman, 1987). PENGI (Agre and Chapman, 1987) could play a (fully observable) video game\nby using Boolean circuits combined with a “visual” representation of current goals and the\nagent’s internal state. “Universal plans” (Schoppers, 1987, 1989) were developed as a lookup-\ntable method for reactive planning, but turned out to be a rediscovery of the idea of policies\nPOLICY\nthat had long been used in Markov decision processes (see Chapter 17). A universal plan (or\na policy) contains a mapping from any state to the action that should be taken in that state.\nKoenig (2001) surveys online planning techniques, under the name Agent-Centered Search.\nMultiagent planning has leaped in popularity in recent years, although it does have\na long history. Konolige (1982) formalizes multiagent planning in ﬁrst-order logic, while\nPednault (1986) gives a STRIPS-style description. The notion of joint intention, which is es-\nsential if agents are to execute a joint plan, comes from work on communicative acts (Cohen\nand Levesque, 1990; Cohen et al., 1990). Boutilier and Brafman (2001) show how to adapt\npartial-order planning to a multiactor setting. Brafman and Domshlak (2008) devise a mul-\ntiactor planning algorithm whose complexity grows only linearly with the number of actors,\nprovided that the degree of coupling (measured partly by the tree width of the graph of inter-\nactions among agents) is bounded. Petrik and Zilberstein (2009) show that an approach based\non bilinear programming outperforms the cover-set approach we outlined in the chapter.\nWe have barely skimmed the surface of work on negotiation in multiagent planning.\nDurfee and Lesser (1989) discuss how tasks can be shared out among agents by negotiation.\nKraus et al. (1991) describe a system for playing Diplomacy, a board game requiring negoti-\nation, coalition formation, and dishonesty. Stone (2000) shows how agents can cooperate as\nteammates in the competitive, dynamic, partially observable environment of robotic soccer. In\na later article, Stone (2003) analyzes two competitive multiagent environments—RoboCup,",
  "teammates in the competitive, dynamic, partially observable environment of robotic soccer. In\na later article, Stone (2003) analyzes two competitive multiagent environments—RoboCup,\na robotic soccer competition, and TAC, the auction-based Trading Agents Competition—\nand ﬁnds that the computational intractability of our current theoretically well-founded ap-\nproaches has led to many multiagent systems being designed by ad hoc methods.\nIn his highly inﬂuential Society of Mind theory, Marvin Minsky (1986, 2007) proposes\nthat human minds are constructed from an ensemble of agents. Livnat and Pippenger (2006)\nprove that, for the problem of optimal path-ﬁnding, and given a limitation on the total amount\nof computing resources, the best architecture for an agent is an ensemble of subagents, each\nof which tries to optimize its own objective, and all of which are in conﬂict with one another. Exercises\n435\nThe boid model on page 429 is due to Reynolds (1987), who won an Academy Award\nfor its application to swarms of penguins in Batman Returns. The NERO game and the meth-\nods for learning strategies are described by Bryant and Miikkulainen (2007).\nRecent book on multiagent systems include those by Weiss (2000a), Young (2004),\nVlassis (2008), and Shoham and Leyton-Brown (2009). There is an annual conference on\nautonomous agents and multiagent systems (AAMAS).\nEXERCISES\n11.1\nThe goals we have considered so far all ask the planner to make the world satisfy the\ngoal at just one time step. Not all goals can be expressed this way: you do not achieve the\ngoal of suspending a chandelier above the ground by throwing it in the air. More seriously,\nyou wouldn’t want your spacecraft life-support system to supply oxygen one day but not\nthe next. A maintenance goal is achieved when the agent’s plan causes a condition to hold\ncontinuously from a given state onward. Describe how to extend the formalism of this chapter\nto support maintenance goals.\n11.2\nYou have a number of trucks with which to deliver a set of packages. Each package\nstarts at some location on a grid map, and has a destination somewhere else. Each truck is di-\nrectly controlled by moving forward and turning. Construct a hierarchy of high-level actions\nfor this problem. What knowledge about the solution does your hierarchy encode?\n11.3\nSuppose that a high-level action has exactly one implementation as a sequence of\nprimitive actions. Give an algorithm for computing its preconditions and effects, given the",
  "11.3\nSuppose that a high-level action has exactly one implementation as a sequence of\nprimitive actions. Give an algorithm for computing its preconditions and effects, given the\ncomplete reﬁnement hierarchy and schemas for the primitive actions.\n11.4\nSuppose that the optimistic reachable set of a high-level plan is a superset of the goal\nset; can anything be concluded about whether the plan achieves the goal? What if the pes-\nsimistic reachable set doesn’t intersect the goal set? Explain.\n11.5\nWrite an algorithm that takes an initial state (speciﬁed by a set of propositional literals)\nand a sequence of HLAs (each deﬁned by preconditions and angelic speciﬁcations of opti-\nmistic and pessimistic reachable sets) and computes optimistic and pessimistic descriptions\nof the reachable set of the sequence.\n11.6\nIn Figure 11.2 we showed how to describe actions in a scheduling problem by using\nseparate ﬁelds for DURATION, USE, and CONSUME. Now suppose we wanted to combine\nscheduling with nondeterministic planning, which requires nondeterministic and conditional\neffects. Consider each of the three ﬁelds and explain if they should remain separate ﬁelds, or\nif they should become effects of the action. Give an example for each of the three.\n11.7\nSome of the operations in standard programming languages can be modeled as actions\nthat change the state of the world. For example, the assignment operation changes the con-\ntents of a memory location, and the print operation changes the state of the output stream. A\nprogram consisting of these operations can also be considered as a plan, whose goal is given 436\nChapter\n11.\nPlanning and Acting in the Real World\nby the speciﬁcation of the program. Therefore, planning algorithms can be used to construct\nprograms that achieve a given speciﬁcation.\na. Write an action schema for the assignment operator (assigning the value of one variable\nto another). Remember that the original value will be overwritten!\nb. Show how object creation can be used by a planner to produce a plan for exchanging\nthe values of two variables by using a temporary variable.\n11.8\nSuppose the Flip action always changes the truth value of variable L. Show how\nto deﬁne its effects by using an action schema with conditional effects. Show that, despite\nthe use of conditional effects, a 1-CNF belief state representation remains in 1-CNF after a\nFlip.\n11.9\nIn the blocks world we were forced to introduce two action schemas, Move and",
  "the use of conditional effects, a 1-CNF belief state representation remains in 1-CNF after a\nFlip.\n11.9\nIn the blocks world we were forced to introduce two action schemas, Move and\nMoveToTable, in order to maintain the Clear predicate properly. Show how conditional\neffects can be used to represent both of these cases with a single action.\n11.10\nConditional effects were illustrated for the Suck action in the vacuum world—which\nsquare becomes clean depends on which square the robot is in. Can you think of a new set of\npropositional variables to deﬁne states of the vacuum world, such that Suck has an uncondi-\ntional description? Write out the descriptions of Suck, Left, and Right, using your proposi-\ntions, and demonstrate that they sufﬁce to describe all possible states of the world.\n11.11\nFind a suitably dirty carpet, free of obstacles, and vacuum it. Draw the path taken\nby the vacuum cleaner as accurately as you can. Explain it, with reference to the forms of\nplanning discussed in this chapter.\n11.12\nTo the medication problem in the previous exercise, add a Test action that has the\nconditional effect CultureGrowth when Disease is true and in any case has the perceptual\neffect Known(CultureGrowth). Diagram a conditional plan that solves the problem and\nminimizes the use of the Medicate action. 12\nKNOWLEDGE\nREPRESENTATION\nIn which we show how to use ﬁrst-order logic to represent the most important\naspects of the real world, such as action, space, time, thoughts, and shopping.\nThe previous chapters described the technology for knowledge-based agents: the syntax,\nsemantics, and proof theory of propositional and ﬁrst-order logic, and the implementation of\nagents that use these logics. In this chapter we address the question of what content to put\ninto such an agent’s knowledge base—how to represent facts about the world.\nSection 12.1 introduces the idea of a general ontology, which organizes everything in\nthe world into a hierarchy of categories. Section 12.2 covers the basic categories of objects,\nsubstances, and measures; Section 12.3 covers events, and Section 12.4 discusses knowledge\nabout beliefs. We then return to consider the technology for reasoning with this content:\nSection 12.5 discusses reasoning systems designed for efﬁcient inference with categories,\nand Section 12.6 discusses reasoning with default information. Section 12.7 brings all the\nknowledge together in the context of an Internet shopping environment.\n12.1\nONTOLOGICAL ENGINEERING",
  "and Section 12.6 discusses reasoning with default information. Section 12.7 brings all the\nknowledge together in the context of an Internet shopping environment.\n12.1\nONTOLOGICAL ENGINEERING\nIn “toy” domains, the choice of representation is not that important; many choices will work.\nComplex domains such as shopping on the Internet or driving a car in trafﬁc require more\ngeneral and ﬂexible representations. This chapter shows how to create these representations,\nconcentrating on general concepts—such as Events, Time, Physical Objects, and Beliefs—\nthat occur in many different domains. Representing these abstract concepts is sometimes\ncalled ontological engineering.\nONTOLOGICAL\nENGINEERING\nThe prospect of representing everything in the world is daunting. Of course, we won’t\nactually write a complete description of everything—that would be far too much for even a\n1000-page textbook—but we will leave placeholders where new knowledge for any domain\ncan ﬁt in. For example, we will deﬁne what it means to be a physical object, and the details of\ndifferent types of objects—robots, televisions, books, or whatever—can be ﬁlled in later. This\nis analogous to the way that designers of an object-oriented programming framework (such as\nthe Java Swing graphical framework) deﬁne general concepts like Window, expecting users to\n437 438\nChapter\n12.\nKnowledge Representation\nAnything\nAbstractObjects\nSets\nNumbers RepresentationalObjects\nInterval\nPlaces\nProcesses\nPhysicalObjects\nHumans\nCategories\nSentences Measurements\nMoments\nThings\nStuff\nTimes\nWeights\nAnimals Agents\nSolid Liquid Gas\nGeneralizedEvents\nFigure 12.1\nThe upper ontology of the world, showing the topics to be covered later in\nthe chapter. Each link indicates that the lower concept is a specialization of the upper one.\nSpecializations are not necessarily disjoint; a human is both an animal and an agent, for\nexample. We will see in Section 12.3.3 why physical objects come under generalized events.\nuse these to deﬁne more speciﬁc concepts like SpreadsheetWindow. The general framework\nof concepts is called an upper ontology because of the convention of drawing graphs with\nUPPER ONTOLOGY\nthe general concepts at the top and the more speciﬁc concepts below them, as in Figure 12.1.\nBefore considering the ontology further, we should state one important caveat. We\nhave elected to use ﬁrst-order logic to discuss the content and organization of knowledge,",
  "Before considering the ontology further, we should state one important caveat. We\nhave elected to use ﬁrst-order logic to discuss the content and organization of knowledge,\nalthough certain aspects of the real world are hard to capture in FOL. The principal difﬁculty\nis that most generalizations have exceptions or hold only to a degree. For example, although\n“tomatoes are red” is a useful rule, some tomatoes are green, yellow, or orange. Similar\nexceptions can be found to almost all the rules in this chapter. The ability to handle exceptions\nand uncertainty is extremely important, but is orthogonal to the task of understanding the\ngeneral ontology. For this reason, we delay the discussion of exceptions until Section 12.5 of\nthis chapter, and the more general topic of reasoning with uncertainty until Chapter 13.\nOf what use is an upper ontology? Consider the ontology for circuits in Section 8.4.2.\nIt makes many simplifying assumptions: time is omitted completely; signals are ﬁxed and do\nnot propagate; the structure of the circuit remains constant. A more general ontology would\nconsider signals at particular times, and would include the wire lengths and propagation de-\nlays. This would allow us to simulate the timing properties of the circuit, and indeed such\nsimulations are often carried out by circuit designers. We could also introduce more inter-\nesting classes of gates, for example, by describing the technology (TTL, CMOS, and so on)\nas well as the input–output speciﬁcation. If we wanted to discuss reliability or diagnosis, we\nwould include the possibility that the structure of the circuit or the properties of the gates\nmight change spontaneously. To account for stray capacitances, we would need to represent\nwhere the wires are on the board. Section 12.1.\nOntological Engineering\n439\nIf we look at the wumpus world, similar considerations apply. Although we do represent\ntime, it has a simple structure: Nothing happens except when the agent acts, and all changes\nare instantaneous. A more general ontology, better suited for the real world, would allow for\nsimultaneous changes extended over time. We also used a Pit predicate to say which squares\nhave pits. We could have allowed for different kinds of pits by having several individuals\nbelonging to the class of pits, each having different properties. Similarly, we might want to\nallow for other animals besides wumpuses. It might not be possible to pin down the exact",
  "belonging to the class of pits, each having different properties. Similarly, we might want to\nallow for other animals besides wumpuses. It might not be possible to pin down the exact\nspecies from the available percepts, so we would need to build up a biological taxonomy to\nhelp the agent predict the behavior of cave-dwellers from scanty clues.\nFor any special-purpose ontology, it is possible to make changes like these to move\ntoward greater generality. An obvious question then arises: do all these ontologies converge\non a general-purpose ontology? After centuries of philosophical and computational inves-\ntigation, the answer is “Maybe.” In this section, we present one general-purpose ontology\nthat synthesizes ideas from those centuries. Two major characteristics of general-purpose\nontologies distinguish them from collections of special-purpose ontologies:\n• A general-purpose ontology should be applicable in more or less any special-purpose\ndomain (with the addition of domain-speciﬁc axioms). This means that no representa-\ntional issue can be ﬁnessed or brushed under the carpet.\n• In any sufﬁciently demanding domain, different areas of knowledge must be uniﬁed,\nbecause reasoning and problem solving could involve several areas simultaneously. A\nrobot circuit-repair system, for instance, needs to reason about circuits in terms of elec-\ntrical connectivity and physical layout, and about time, both for circuit timing analysis\nand estimating labor costs. The sentences describing time therefore must be capable\nof being combined with those describing spatial layout and must work equally well for\nnanoseconds and minutes and for angstroms and meters.\nWe should say up front that the enterprise of general ontological engineering has so far had\nonly limited success. None of the top AI applications (as listed in Chapter 1) make use\nof a shared ontology—they all use special-purpose knowledge engineering. Social/political\nconsiderations can make it difﬁcult for competing parties to agree on an ontology. As Tom\nGruber (2004) says, “Every ontology is a treaty—a social agreement—among people with\nsome common motive in sharing.” When competing concerns outweigh the motivation for\nsharing, there can be no common ontology. Those ontologies that do exist have been created\nalong four routes:\n1. By a team of trained ontologist/logicians, who architect the ontology and write axioms.\nThe CYC system was mostly built this way (Lenat and Guha, 1990).",
  "along four routes:\n1. By a team of trained ontologist/logicians, who architect the ontology and write axioms.\nThe CYC system was mostly built this way (Lenat and Guha, 1990).\n2. By importing categories, attributes, and values from an existing database or databases.\nDBPEDIA was built by importing structured facts from Wikipedia (Bizer et al., 2007).\n3. By parsing text documents and extracting information from them. TEXTRUNNER was\nbuilt by reading a large corpus of Web pages (Banko and Etzioni, 2008).\n4. By enticing unskilled amateurs to enter commonsense knowledge. The OPENMIND\nsystem was built by volunteers who proposed facts in English (Singh et al., 2002;\nChklovski and Gil, 2005). 440\nChapter\n12.\nKnowledge Representation\n12.2\nCATEGORIES AND OBJECTS\nThe organization of objects into categories is a vital part of knowledge representation. Al-\nCATEGORY\nthough interaction with the world takes place at the level of individual objects, much reason-\ning takes place at the level of categories. For example, a shopper would normally have the\ngoal of buying a basketball, rather than a particular basketball such as BB9. Categories also\nserve to make predictions about objects once they are classiﬁed. One infers the presence of\ncertain objects from perceptual input, infers category membership from the perceived proper-\nties of the objects, and then uses category information to make predictions about the objects.\nFor example, from its green and yellow mottled skin, one-foot diameter, ovoid shape, red\nﬂesh, black seeds, and presence in the fruit aisle, one can infer that an object is a watermelon;\nfrom this, one infers that it would be useful for fruit salad.\nThere are two choices for representing categories in ﬁrst-order logic: predicates and\nobjects. That is, we can use the predicate Basketball(b), or we can reify1 the category as\nREIFICATION\nan object, Basketballs. We could then say Member(b, Basketballs), which we will abbre-\nviate as b ∈Basketballs, to say that b is a member of the category of basketballs. We say\nSubset(Basketballs, Balls), abbreviated as Basketballs ⊂Balls, to say that Basketballs is\na subcategory of Balls. We will use subcategory, subclass, and subset interchangeably.\nSUBCATEGORY\nCategories serve to organize and simplify the knowledge base through inheritance. If\nINHERITANCE\nwe say that all instances of the category Food are edible, and if we assert that Fruit is a",
  "SUBCATEGORY\nCategories serve to organize and simplify the knowledge base through inheritance. If\nINHERITANCE\nwe say that all instances of the category Food are edible, and if we assert that Fruit is a\nsubclass of Food and Apples is a subclass of Fruit, then we can infer that every apple is\nedible. We say that the individual apples inherit the property of edibility, in this case from\ntheir membership in the Food category.\nSubclass relations organize categories into a taxonomy, or taxonomic hierarchy. Tax-\nTAXONOMY\nonomies have been used explicitly for centuries in technical ﬁelds. The largest such taxonomy\norganizes about 10 million living and extinct species, many of them beetles,2 into a single hi-\nerarchy; library science has developed a taxonomy of all ﬁelds of knowledge, encoded as the\nDewey Decimal system; and tax authorities and other government departments have devel-\noped extensive taxonomies of occupations and commercial products. Taxonomies are also an\nimportant aspect of general commonsense knowledge.\nFirst-order logic makes it easy to state facts about categories, either by relating ob-\njects to categories or by quantifying over their members. Here are some types of facts, with\nexamples of each:\n• An object is a member of a category.\nBB9 ∈Basketballs\n• A category is a subclass of another category.\nBasketballs ⊂Balls\n• All members of a category have some properties.\n(x ∈Basketballs) ⇒Spherical(x)\n1 Turning a proposition into an object is called reiﬁcation, from the Latin word res, or thing. John McCarthy\nproposed the term “thingiﬁcation,” but it never caught on.\n2 The famous biologist J. B. S. Haldane deduced “An inordinate fondness for beetles” on the part of the Creator. Section 12.2.\nCategories and Objects\n441\n• Members of a category can be recognized by some properties.\nOrange(x) ∧Round(x) ∧Diameter(x) = 9.5′′ ∧x ∈Balls ⇒x ∈Basketballs\n• A category as a whole has some properties.\nDogs ∈DomesticatedSpecies\nNotice that because Dogs is a category and is a member of DomesticatedSpecies, the latter\nmust be a category of categories. Of course there are exceptions to many of the above rules\n(punctured basketballs are not spherical); we deal with these exceptions later.\nAlthough subclass and member relations are the most important ones for categories,\nwe also want to be able to state relations between categories that are not subclasses of each\nother. For example, if we just say that Males and Females are subclasses of Animals, then",
  "we also want to be able to state relations between categories that are not subclasses of each\nother. For example, if we just say that Males and Females are subclasses of Animals, then\nwe have not said that a male cannot be a female. We say that two or more categories are\ndisjoint if they have no members in common. And even if we know that males and females\nDISJOINT\nare disjoint, we will not know that an animal that is not a male must be a female, unless\nwe say that males and females constitute an exhaustive decomposition of the animals. A\nEXHAUSTIVE\nDECOMPOSITION\ndisjoint exhaustive decomposition is known as a partition. The following examples illustrate\nPARTITION\nthese three concepts:\nDisjoint({Animals, Vegetables})\nExhaustiveDecomposition({Americans, Canadians, Mexicans},\nNorthAmericans)\nPartition({Males, Females}, Animals) .\n(Note that the ExhaustiveDecomposition of NorthAmericans is not a Partition, because\nsome people have dual citizenship.) The three predicates are deﬁned as follows:\nDisjoint(s) ⇔(∀c1, c2 c1 ∈s ∧c2 ∈s ∧c1 ̸= c2 ⇒Intersection(c1, c2) = { })\nExhaustiveDecomposition(s, c) ⇔(∀i i ∈c ⇔∃c2 c2 ∈s ∧i ∈c2)\nPartition(s, c) ⇔Disjoint(s) ∧ExhaustiveDecomposition(s, c) .\nCategories can also be deﬁned by providing necessary and sufﬁcient conditions for\nmembership. For example, a bachelor is an unmarried adult male:\nx ∈Bachelors ⇔Unmarried(x) ∧x ∈Adults ∧x ∈Males .\nAs we discuss in the sidebar on natural kinds on page 443, strict logical deﬁnitions for cate-\ngories are neither always possible nor always necessary.\n12.2.1\nPhysical composition\nThe idea that one object can be part of another is a familiar one. One’s nose is part of one’s\nhead, Romania is part of Europe, and this chapter is part of this book. We use the general\nPartOf relation to say that one thing is part of another. Objects can be grouped into PartOf\nhierarchies, reminiscent of the Subset hierarchy:\nPartOf (Bucharest, Romania)\nPartOf (Romania, EasternEurope)\nPartOf (EasternEurope, Europe)\nPartOf (Europe, Earth) . 442\nChapter\n12.\nKnowledge Representation\nThe PartOf relation is transitive and reﬂexive; that is,\nPartOf (x, y) ∧PartOf (y, z) ⇒PartOf (x, z) .\nPartOf (x, x) .\nTherefore, we can conclude PartOf (Bucharest, Earth).\nCategories of composite objects are often characterized by structural relations among\nCOMPOSITE OBJECT\nparts. For example, a biped has two legs attached to a body:\nBiped(a)\n⇒\n∃l1, l2, b Leg(l1) ∧Leg(l2) ∧Body(b) ∧\nPartOf (l1, a) ∧PartOf (l2, a) ∧PartOf (b, a) ∧",
  "COMPOSITE OBJECT\nparts. For example, a biped has two legs attached to a body:\nBiped(a)\n⇒\n∃l1, l2, b Leg(l1) ∧Leg(l2) ∧Body(b) ∧\nPartOf (l1, a) ∧PartOf (l2, a) ∧PartOf (b, a) ∧\nAttached(l1, b) ∧Attached(l2, b) ∧\nl1 ̸= l2 ∧[∀l3 Leg(l3) ∧PartOf (l3, a) ⇒(l3 = l1 ∨l3 = l2)] .\nThe notation for “exactly two” is a little awkward; we are forced to say that there are two\nlegs, that they are not the same, and that if anyone proposes a third leg, it must be the same\nas one of the other two. In Section 12.5.2, we describe a formalism called description logic\nmakes it easier to represent constraints like “exactly two.”\nWe can deﬁne a PartPartition relation analogous to the Partition relation for cate-\ngories. (See Exercise 12.8.) An object is composed of the parts in its PartPartition and can\nbe viewed as deriving some properties from those parts. For example, the mass of a compos-\nite object is the sum of the masses of the parts. Notice that this is not the case with categories,\nwhich have no mass, even though their elements might.\nIt is also useful to deﬁne composite objects with deﬁnite parts but no particular struc-\nture. For example, we might want to say “The apples in this bag weigh two pounds.” The\ntemptation would be to ascribe this weight to the set of apples in the bag, but this would be\na mistake because the set is an abstract mathematical concept that has elements but does not\nhave weight. Instead, we need a new concept, which we will call a bunch. For example, if\nBUNCH\nthe apples are Apple1, Apple2, and Apple3, then\nBunchOf ({Apple1, Apple2, Apple3})\ndenotes the composite object with the three apples as parts (not elements). We can then use the\nbunch as a normal, albeit unstructured, object. Notice that BunchOf ({x}) = x. Furthermore,\nBunchOf (Apples) is the composite object consisting of all apples—not to be confused with\nApples, the category or set of all apples.\nWe can deﬁne BunchOf in terms of the PartOf relation. Obviously, each element of\ns is part of BunchOf (s):\n∀x x ∈s ⇒PartOf (x, BunchOf (s)) .\nFurthermore, BunchOf (s) is the smallest object satisfying this condition. In other words,\nBunchOf (s) must be part of any object that has all the elements of s as parts:\n∀y [∀x x ∈s ⇒PartOf (x, y)] ⇒PartOf (BunchOf (s), y) .\nThese axioms are an example of a general technique called logical minimization, which\nLOGICAL\nMINIMIZATION\nmeans deﬁning an object as the smallest one satisfying certain conditions. Section 12.2.\nCategories and Objects\n443",
  "These axioms are an example of a general technique called logical minimization, which\nLOGICAL\nMINIMIZATION\nmeans deﬁning an object as the smallest one satisfying certain conditions. Section 12.2.\nCategories and Objects\n443\nNATURAL KINDS\nSome categories have strict deﬁnitions: an object is a triangle if and only if it is\na polygon with three sides. On the other hand, most categories in the real world\nhave no clear-cut deﬁnition; these are called natural kind categories. For example,\ntomatoes tend to be a dull scarlet; roughly spherical; with an indentation at the top\nwhere the stem was; about two to four inches in diameter; with a thin but tough\nskin; and with ﬂesh, seeds, and juice inside. There is, however, variation: some\ntomatoes are yellow or orange, unripe tomatoes are green, some are smaller or\nlarger than average, and cherry tomatoes are uniformly small. Rather than having\na complete deﬁnition of tomatoes, we have a set of features that serves to identify\nobjects that are clearly typical tomatoes, but might not be able to decide for other\nobjects. (Could there be a tomato that is fuzzy like a peach?)\nThis poses a problem for a logical agent. The agent cannot be sure that an\nobject it has perceived is a tomato, and even if it were sure, it could not be cer-\ntain which of the properties of typical tomatoes this one has. This problem is an\ninevitable consequence of operating in partially observable environments.\nOne useful approach is to separate what is true of all instances of a cate-\ngory from what is true only of typical instances. So in addition to the category\nTomatoes, we will also have the category Typical(Tomatoes). Here, the Typical\nfunction maps a category to the subclass that contains only typical instances:\nTypical(c) ⊆c .\nMost knowledge about natural kinds will actually be about their typical instances:\nx ∈Typical(Tomatoes) ⇒Red(x) ∧Round(x) .\nThus, we can write down useful facts about categories without exact deﬁni-\ntions. The difﬁculty of providing exact deﬁnitions for most natural categories was\nexplained in depth by Wittgenstein (1953). He used the example of games to show\nthat members of a category shared “family resemblances” rather than necessary\nand sufﬁcient characteristics: what strict deﬁnition encompasses chess, tag, soli-\ntaire, and dodgeball?\nThe utility of the notion of strict deﬁnition was also challenged by\nQuine (1953). He pointed out that even the deﬁnition of “bachelor” as an un-",
  "taire, and dodgeball?\nThe utility of the notion of strict deﬁnition was also challenged by\nQuine (1953). He pointed out that even the deﬁnition of “bachelor” as an un-\nmarried adult male is suspect; one might, for example, question a statement such\nas “the Pope is a bachelor.” While not strictly false, this usage is certainly infe-\nlicitous because it induces unintended inferences on the part of the listener. The\ntension could perhaps be resolved by distinguishing between logical deﬁnitions\nsuitable for internal knowledge representation and the more nuanced criteria for\nfelicitous linguistic usage. The latter may be achieved by “ﬁltering” the assertions\nderived from the former. It is also possible that failures of linguistic usage serve as\nfeedback for modifying internal deﬁnitions, so that ﬁltering becomes unnecessary. 444\nChapter\n12.\nKnowledge Representation\n12.2.2\nMeasurements\nIn both scientiﬁc and commonsense theories of the world, objects have height, mass, cost,\nand so on.\nThe values that we assign for these properties are called measures.\nOrdi-\nMEASURE\nnary quantitative measures are quite easy to represent. We imagine that the universe in-\ncludes abstract “measure objects,” such as the length that is the length of this line seg-\nment:\n. We can call this length 1.5 inches or 3.81 centimeters. Thus,\nthe same length has different names in our language.We represent the length with a units\nfunction that takes a number as argument.\n(An alternative scheme is explored in Exer-\nUNITS FUNCTION\ncise 12.9.) If the line segment is called L1, we can write\nLength(L1) = Inches(1.5) = Centimeters(3.81) .\nConversion between units is done by equating multiples of one unit to another:\nCentimeters(2.54 × d) = Inches(d) .\nSimilar axioms can be written for pounds and kilograms, seconds and days, and dollars and\ncents. Measures can be used to describe objects as follows:\nDiameter(Basketball 12) = Inches(9.5) .\nListPrice(Basketball 12) = $(19) .\nd ∈Days ⇒Duration(d) = Hours(24) .\nNote that $(1) is not a dollar bill! One can have two dollar bills, but there is only one object\nnamed $(1). Note also that, while Inches(0) and Centimeters(0) refer to the same zero\nlength, they are not identical to other zero measures, such as Seconds(0).\nSimple, quantitative measures are easy to represent. Other measures present more of a\nproblem, because they have no agreed scale of values. Exercises have difﬁculty, desserts have",
  "Simple, quantitative measures are easy to represent. Other measures present more of a\nproblem, because they have no agreed scale of values. Exercises have difﬁculty, desserts have\ndeliciousness, and poems have beauty, yet numbers cannot be assigned to these qualities. One\nmight, in a moment of pure accountancy, dismiss such properties as useless for the purpose of\nlogical reasoning; or, still worse, attempt to impose a numerical scale on beauty. This would\nbe a grave mistake, because it is unnecessary. The most important aspect of measures is not\nthe particular numerical values, but the fact that measures can be ordered.\nAlthough measures are not numbers, we can still compare them, using an ordering\nsymbol such as >. For example, we might well believe that Norvig’s exercises are tougher\nthan Russell’s, and that one scores less on tougher exercises:\ne1 ∈Exercises ∧e2 ∈Exercises ∧Wrote(Norvig, e1) ∧Wrote(Russell, e2) ⇒\nDiﬃculty(e1) > Diﬃculty(e2) .\ne1 ∈Exercises ∧e2 ∈Exercises ∧Diﬃculty(e1) > Diﬃculty(e2) ⇒\nExpectedScore(e1) < ExpectedScore(e2) .\nThis is enough to allow one to decide which exercises to do, even though no numerical values\nfor difﬁculty were ever used. (One does, however, have to discover who wrote which exer-\ncises.) These sorts of monotonic relationships among measures form the basis for the ﬁeld of\nqualitative physics, a subﬁeld of AI that investigates how to reason about physical systems\nwithout plunging into detailed equations and numerical simulations. Qualitative physics is\ndiscussed in the historical notes section. Section 12.2.\nCategories and Objects\n445\n12.2.3\nObjects: Things and stuff\nThe real world can be seen as consisting of primitive objects (e.g., atomic particles) and\ncomposite objects built from them. By reasoning at the level of large objects such as apples\nand cars, we can overcome the complexity involved in dealing with vast numbers of primitive\nobjects individually. There is, however, a signiﬁcant portion of reality that seems to defy any\nobvious individuation—division into distinct objects. We give this portion the generic name\nINDIVIDUATION\nstuff. For example, suppose I have some butter and an aardvark in front of me. I can say\nSTUFF\nthere is one aardvark, but there is no obvious number of “butter-objects,” because any part of\na butter-object is also a butter-object, at least until we get to very small parts indeed. This is\nthe major distinction between stuff and things. If we cut an aardvark in half, we do not get",
  "a butter-object is also a butter-object, at least until we get to very small parts indeed. This is\nthe major distinction between stuff and things. If we cut an aardvark in half, we do not get\ntwo aardvarks (unfortunately).\nThe English language distinguishes clearly between stuff and things. We say “an aard-\nvark,” but, except in pretentious California restaurants, one cannot say “a butter.” Linguists\ndistinguish between count nouns, such as aardvarks, holes, and theorems, and mass nouns,\nCOUNT NOUNS\nMASS NOUN\nsuch as butter, water, and energy. Several competing ontologies claim to handle this distinc-\ntion. Here we describe just one; the others are covered in the historical notes section.\nTo represent stuff properly, we begin with the obvious. We need to have as objects in\nour ontology at least the gross “lumps” of stuff we interact with. For example, we might\nrecognize a lump of butter as the one left on the table the night before; we might pick it up,\nweigh it, sell it, or whatever. In these senses, it is an object just like the aardvark. Let us\ncall it Butter3. We also deﬁne the category Butter. Informally, its elements will be all those\nthings of which one might say “It’s butter,” including Butter3. With some caveats about very\nsmall parts that we w omit for now, any part of a butter-object is also a butter-object:\nb ∈Butter ∧PartOf (p, b) ⇒p ∈Butter .\nWe can now say that butter melts at around 30 degrees centigrade:\nb ∈Butter ⇒MeltingPoint(b, Centigrade(30)) .\nWe could go on to say that butter is yellow, is less dense than water, is soft at room tempera-\nture, has a high fat content, and so on. On the other hand, butter has no particular size, shape,\nor weight. We can deﬁne more specialized categories of butter such as UnsaltedButter,\nwhich is also a kind of stuff. Note that the category PoundOfButter, which includes as\nmembers all butter-objects weighing one pound, is not a kind of stuff. If we cut a pound of\nbutter in half, we do not, alas, get two pounds of butter.\nWhat is actually going on is this: some properties are intrinsic: they belong to the very\nINTRINSIC\nsubstance of the object, rather than to the object as a whole. When you cut an instance of\nstuff in half, the two pieces retain the intrinsic properties—things like density, boiling point,\nﬂavor, color, ownership, and so on. On the other hand, their extrinsic properties—weight,\nEXTRINSIC\nlength, shape, and so on—are not retained under subdivision. A category of objects that",
  "ﬂavor, color, ownership, and so on. On the other hand, their extrinsic properties—weight,\nEXTRINSIC\nlength, shape, and so on—are not retained under subdivision. A category of objects that\nincludes in its deﬁnition only intrinsic properties is then a substance, or mass noun; a class\nthat includes any extrinsic properties in its deﬁnition is a count noun. The category Stuﬀis\nthe most general substance category, specifying no intrinsic properties. The category Thing\nis the most general discrete object category, specifying no extrinsic properties. 446\nChapter\n12.\nKnowledge Representation\n12.3\nEVENTS\nIn Section 10.4.2, we showed how situation calculus represents actions and their effects.\nSituation calculus is limited in its applicability: it was designed to describe a world in which\nactions are discrete, instantaneous, and happen one at a time. Consider a continuous action,\nsuch as ﬁlling a bathtub. Situation calculus can say that the tub is empty before the action and\nfull when the action is done, but it can’t talk about what happens during the action. It also\ncan’t describe two actions happening at the same time—such as brushing one’s teeth while\nwaiting for the tub to ﬁll. To handle such cases we introduce an alternative formalism known\nas event calculus, which is based on points of time rather than on situations.3\nEVENT CALCULUS\nEvent calculus reiﬁes ﬂuents and events. The ﬂuent At(Shankar, Berkeley) is an ob-\nject that refers to the fact of Shankar being in Berkeley, but does not by itself say anything\nabout whether it is true. To assert that a ﬂuent is actually true at some point in time we use\nthe predicate T, as in T(At(Shankar, Berkeley), t).\nEvents are described as instances of event categories.4 The event E1 of Shankar ﬂying\nfrom San Francisco to Washington, D.C. is described as\nE1 ∈Flyings ∧Flyer(E1, Shankar) ∧Origin(E1, SF) ∧Destination(E1, DC ) .\nIf this is too verbose, we can deﬁne an alternative three-argument version of the category of\nﬂying events and say\nE1 ∈Flyings(Shankar, SF, DC) .\nWe then use Happens(E1, i) to say that the event E1 took place over the time interval i, and\nwe say the same thing in functional form with Extent(E1) = i. We represent time intervals\nby a (start, end) pair of times; that is, i = (t1, t2) is the time interval that starts at t1 and ends\nat t2. The complete set of predicates for one version of the event calculus is\nT(f, t)\nFluent f is true at time t\nHappens(e, i)\nEvent e happens over the time interval i",
  "at t2. The complete set of predicates for one version of the event calculus is\nT(f, t)\nFluent f is true at time t\nHappens(e, i)\nEvent e happens over the time interval i\nInitiates(e, f, t)\nEvent e causes ﬂuent f to start to hold at time t\nTerminates(e, f, t)\nEvent e causes ﬂuent f to cease to hold at time t\nClipped(f, i)\nFluent f ceases to be true at some point during time interval i\nRestored(f, i)\nFluent f becomes true sometime during time interval i\nWe assume a distinguished event, Start, that describes the initial state by saying which ﬂuents\nare initiated or terminated at the start time. We deﬁne T by saying that a ﬂuent holds at a point\nin time if the ﬂuent was initiated by an event at some time in the past and was not made false\n(clipped) by an intervening event. A ﬂuent does not hold if it was terminated by an event and\n3 The terms “event” and “action” may be used interchangeably. Informally, “action” connotes an agent while\n“event” connotes the possibility of agentless actions.\n4 Some versions of event calculus do not distinguish event categories from instances of the categories. Section 12.3.\nEvents\n447\nnot made true (restored) by another event. Formally, the axioms are:\nHappens(e, (t1, t2)) ∧Initiates(e, f, t1) ∧¬Clipped(f, (t1, t)) ∧t1 < t ⇒\nT(f, t)\nHappens(e, (t1, t2)) ∧Terminates(e, f, t1) ∧¬Restored(f, (t1, t)) ∧t1 < t ⇒\n¬T(f, t)\nwhere Clipped and Restored are deﬁned by\nClipped(f, (t1, t2)) ⇔\n∃e, t, t3 Happens(e, (t, t3)) ∧t1 ≤t < t2 ∧Terminates(e, f, t)\nRestored(f, (t1, t2)) ⇔\n∃e, t, t3 Happens(e, (t, t3)) ∧t1 ≤t < t2 ∧Initiates(e, f, t)\nIt is convenient to extend T to work over intervals as well as time points; a ﬂuent holds over\nan interval if it holds on every point within the interval:\nT(f, (t1, t2)) ⇔[∀t (t1 ≤t < t2) ⇒T(f, t)]\nFluents and actions are deﬁned with domain-speciﬁc axioms that are similar to successor-\nstate axioms. For example, we can say that the only way a wumpus-world agent gets an\narrow is at the start, and the only way to use up an arrow is to shoot it:\nInitiates(e, HaveArrow(a), t) ⇔e = Start\nTerminates(e, HaveArrow(a), t) ⇔e ∈Shootings(a)\nBy reifying events we make it possible to add any amount of arbitrary information about\nthem. For example, we can say that Shankar’s ﬂight was bumpy with Bumpy(E1). In an\nontology where events are n-ary predicates, there would be no way to add extra information\nlike this; moving to an n + 1-ary predicate isn’t a scalable solution.",
  "them. For example, we can say that Shankar’s ﬂight was bumpy with Bumpy(E1). In an\nontology where events are n-ary predicates, there would be no way to add extra information\nlike this; moving to an n + 1-ary predicate isn’t a scalable solution.\nWe can extend event calculus to make it possible to represent simultaneous events (such\nas two people being necessary to ride a seesaw), exogenous events (such as the wind blowing\nand changing the location of an object), continuous events (such as the level of water in the\nbathtub continuously rising) and other complications.\n12.3.1\nProcesses\nThe events we have seen so far are what we call discrete events—they have a deﬁnite struc-\nDISCRETE EVENTS\nture. Shankar’s trip has a beginning, middle, and end. If interrupted halfway, the event would\nbe something different—it would not be a trip from San Francisco to Washington, but instead\na trip from San Francisco to somewhere over Kansas. On the other hand, the category of\nevents denoted by Flyings has a different quality. If we take a small interval of Shankar’s\nﬂight, say, the third 20-minute segment (while he waits anxiously for a bag of peanuts), that\nevent is still a member of Flyings. In fact, this is true for any subinterval.\nCategories of events with this property are called process categories or liquid event\nPROCESS\nLIQUID EVENT\ncategories. Any process e that happens over an interval also happens over any subinterval:\n(e ∈Processes) ∧Happens(e, (t1, t4)) ∧(t1 < t2 < t3 < t4) ⇒Happens(e, (t2, t3)) .\nThe distinction between liquid and nonliquid events is exactly analogous to the difference\nbetween substances, or stuff, and individual objects, or things. In fact, some have called\nliquid events temporal substances, whereas substances like butter are spatial substances.\nTEMPORAL\nSUBSTANCE\nSPATIAL SUBSTANCE 448\nChapter\n12.\nKnowledge Representation\n12.3.2\nTime intervals\nEvent calculus opens us up to the possibility of talking about time, and time intervals. We\nwill consider two kinds of time intervals: moments and extended intervals. The distinction is\nthat only moments have zero duration:\nPartition({Moments, ExtendedIntervals}, Intervals)\ni ∈Moments ⇔Duration(i) = Seconds(0) .\nNext we invent a time scale and associate points on that scale with moments, giving us ab-\nsolute times. The time scale is arbitrary; we measure it in seconds and say that the moment\nat midnight (GMT) on January 1, 1900, has time 0. The functions Begin and End pick out",
  "solute times. The time scale is arbitrary; we measure it in seconds and say that the moment\nat midnight (GMT) on January 1, 1900, has time 0. The functions Begin and End pick out\nthe earliest and latest moments in an interval, and the function Time delivers the point on the\ntime scale for a moment. The function Duration gives the difference between the end time\nand the start time.\nInterval(i) ⇒Duration(i) = (Time(End(i)) −Time(Begin(i))) .\nTime(Begin(AD1900)) = Seconds(0) .\nTime(Begin(AD2001)) = Seconds(3187324800) .\nTime(End(AD2001)) = Seconds(3218860800) .\nDuration(AD2001) = Seconds(31536000) .\nTo make these numbers easier to read, we also introduce a function Date, which takes six\narguments (hours, minutes, seconds, day, month, and year) and returns a time point:\nTime(Begin(AD2001)) = Date(0, 0, 0, 1, Jan, 2001)\nDate(0, 20, 21, 24, 1, 1995) = Seconds(3000000000) .\nTwo intervals Meet if the end time of the ﬁrst equals the start time of the second. The com-\nplete set of interval relations, as proposed by Allen (1983), is shown graphically in Figure 12.2\nand logically below:\nMeet(i, j)\n⇔\nEnd(i) = Begin(j)\nBefore(i, j)\n⇔\nEnd(i) < Begin(j)\nAfter(j, i)\n⇔\nBefore(i, j)\nDuring(i, j)\n⇔\nBegin(j) < Begin(i) < End(i) < End(j)\nOverlap(i, j)\n⇔\nBegin(i) < Begin(j) < End(i) < End(j)\nBegins(i, j)\n⇔\nBegin(i) = Begin(j)\nFinishes(i, j)\n⇔\nEnd(i) = End(j)\nEquals(i, j)\n⇔\nBegin(i) = Begin(j) ∧End(i) = End(j)\nThese all have their intuitive meaning, with the exception of Overlap: we tend to think of\noverlap as symmetric (if i overlaps j then j overlaps i), but in this deﬁnition, Overlap(i, j)\nonly holds if i begins before j. To say that the reign of Elizabeth II immediately followed that\nof George VI, and the reign of Elvis overlapped with the 1950s, we can write the following:\nMeets(ReignOf (GeorgeVI ), ReignOf (ElizabethII )) .\nOverlap(Fifties, ReignOf (Elvis)) .\nBegin(Fifties) = Begin(AD1950) .\nEnd(Fifties) = End(AD1959) . Section 12.3.\nEvents\n449\nFigure 12.2\nPredicates on time intervals.\ntime\n1801\n1797\n1789\nWashington\nAdams\nJefferson\nFigure 12.3\nA schematic view of the object President(USA) for the ﬁrst 15 years of its\nexistence.\n12.3.3\nFluents and objects\nPhysical objects can be viewed as generalized events, in the sense that a physical object is\na chunk of space–time. For example, USA can be thought of as an event that began in,\nsay, 1776 as a union of 13 states and is still in progress today as a union of 50. We can",
  "a chunk of space–time. For example, USA can be thought of as an event that began in,\nsay, 1776 as a union of 13 states and is still in progress today as a union of 50. We can\ndescribe the changing properties of USA using state ﬂuents, such as Population(USA). A\nproperty of the USA that changes every four or eight years, barring mishaps, is its president.\nOne might propose that President(USA) is a logical term that denotes a different object\nat different times. Unfortunately, this is not possible, because a term denotes exactly one\nobject in a given model structure. (The term President(USA, t) can denote different objects,\ndepending on the value of t, but our ontology keeps time indices separate from ﬂuents.) The 450\nChapter\n12.\nKnowledge Representation\nonly possibility is that President(USA) denotes a single object that consists of different\npeople at different times. It is the object that is George Washington from 1789 to 1797, John\nAdams from 1797 to 1801, and so on, as in Figure 12.3. To say that George Washington was\npresident throughout 1790, we can write\nT(Equals(President(USA), GeorgeWashington), AD1790) .\nWe use the function symbol Equals rather than the standard logical predicate =, because\nwe cannot have a predicate as an argument to T, and because the interpretation is not that\nGeorgeWashington and President(USA) are logically identical in 1790; logical identity is\nnot something that can change over time. The identity is between the subevents of each object\nthat are deﬁned by the period 1790.\n12.4\nMENTAL EVENTS AND MENTAL OBJECTS\nThe agents we have constructed so far have beliefs and can deduce new beliefs. Yet none\nof them has any knowledge about beliefs or about deduction. Knowledge about one’s own\nknowledge and reasoning processes is useful for controlling inference. For example, suppose\nAlice asks “what is the square root of 1764” and Bob replies “I don’t know.” If Alice insists\n“think harder,” Bob should realize that with some more thought, this question can in fact\nbe answered. On the other hand, if the question were “Is your mother sitting down right\nnow?” then Bob should realize that thinking harder is unlikely to help. Knowledge about\nthe knowledge of other agents is also important; Bob should realize that his mother knows\nwhether she is sitting or not, and that asking her would be a way to ﬁnd out.\nWhat we need is a model of the mental objects that are in someone’s head (or some-",
  "whether she is sitting or not, and that asking her would be a way to ﬁnd out.\nWhat we need is a model of the mental objects that are in someone’s head (or some-\nthing’s knowledge base) and of the mental processes that manipulate those mental objects.\nThe model does not have to be detailed. We do not have to be able to predict how many\nmilliseconds it will take for a particular agent to make a deduction. We will be happy just to\nbe able to conclude that mother knows whether or not she is sitting.\nWe begin with the propositional attitudes that an agent can have toward mental ob-\nPROPOSITIONAL\nATTITUDE\njects: attitudes such as Believes, Knows, Wants, Intends, and Informs. The difﬁculty is\nthat these attitudes do not behave like “normal” predicates. For example, suppose we try to\nassert that Lois knows that Superman can ﬂy:\nKnows(Lois, CanFly(Superman)) .\nOne minor issue with this is that we normally think of CanFly(Superman) as a sentence, but\nhere it appears as a term. That issue can be patched up just be reifying CanFly(Superman);\nmaking it a ﬂuent. A more serious problem is that, if it is true that Superman is Clark Kent,\nthen we must conclude that Lois knows that Clark can ﬂy:\n(Superman = Clark) ∧Knows(Lois, CanFly(Superman))\n|= Knows(Lois, CanFly(Clark)) .\nThis is a consequence of the fact that equality reasoning is built into logic. Normally that is\na good thing; if our agent knows that 2 + 2 = 4 and 4 < 5, then we want our agent to know Section 12.4.\nMental Events and Mental Objects\n451\nthat 2 + 2 < 5. This property is called referential transparency—it doesn’t matter what\nREFERENTIAL\nTRANSPARENCY\nterm a logic uses to refer to an object, what matters is the object that the term names. But for\npropositional attitudes like believes and knows, we would like to have referential opacity—the\nterms used do matter, because not all agents know which terms are co-referential.\nModal logic is designed to address this problem. Regular logic is concerned with a sin-\nMODAL LOGIC\ngle modality, the modality of truth, allowing us to express “P is true.” Modal logic includes\nspecial modal operators that take sentences (rather than terms) as arguments. For example,\n“A knows P” is represented with the notation KAP, where K is the modal operator for knowl-\nedge. It takes two arguments, an agent (written as the subscript) and a sentence. The syntax\nof modal logic is the same as ﬁrst-order logic, except that sentences can also be formed with\nmodal operators.",
  "edge. It takes two arguments, an agent (written as the subscript) and a sentence. The syntax\nof modal logic is the same as ﬁrst-order logic, except that sentences can also be formed with\nmodal operators.\nThe semantics of modal logic is more complicated. In ﬁrst-order logic a model con-\ntains a set of objects and an interpretation that maps each name to the appropriate object,\nrelation, or function. In modal logic we want to be able to consider both the possibility that\nSuperman’s secret identity is Clark and that it isn’t. Therefore, we will need a more com-\nplicated model, one that consists of a collection of possible worlds rather than just one true\nPOSSIBLE WORLD\nworld. The worlds are connected in a graph by accessibility relations, one relation for each\nACCESSIBILITY\nRELATIONS\nmodal operator. We say that world w1 is accessible from world w0 with respect to the modal\noperator KA if everything in w1 is consistent with what A knows in w0, and we write this\nas Acc(KA, w0, w1). In diagrams such as Figure 12.4 we show accessibility as an arrow be-\ntween possible worlds. As an example, in the real world, Bucharest is the capital of Romania,\nbut for an agent that did not know that, other possible worlds are accessible, including ones\nwhere the capital of Romania is Sibiu or Soﬁa. Presumably a world where 2 + 2 = 5 would\nnot be accessible to any agent.\nIn general, a knowledge atom KAP is true in world w if and only if P is true in every\nworld accessible from w. The truth of more complex sentences is derived by recursive appli-\ncation of this rule and the normal rules of ﬁrst-order logic. That means that modal logic can\nbe used to reason about nested knowledge sentences: what one agent knows about another\nagent’s knowledge. For example, we can say that, even though Lois doesn’t know whether\nSuperman’s secret identity is Clark Kent, she does know that Clark knows:\nKLois[KClarkIdentity(Superman, Clark) ∨KClark¬Identity(Superman, Clark)]\nFigure 12.4 shows some possible worlds for this domain, with accessibility relations for Lois\nand Superman.\nIn the TOP-LEFT diagram, it is common knowledge that Superman knows his own iden-\ntity, and neither he nor Lois has seen the weather report. So in w0 the worlds w0 and w2 are\naccessible to Superman; maybe rain is predicted, maybe not. For Lois all four worlds are ac-\ncessible from each other; she doesn’t know anything about the report or if Clark is Superman.",
  "accessible to Superman; maybe rain is predicted, maybe not. For Lois all four worlds are ac-\ncessible from each other; she doesn’t know anything about the report or if Clark is Superman.\nBut she does know that Superman knows whether he is Clark, because in every world that is\naccessible to Lois, either Superman knows I, or he knows ¬I. Lois does not know which is\nthe case, but either way she knows Superman knows.\nIn the TOP-RIGHT diagram it is common knowledge that Lois has seen the weather\nreport. So in w4 she knows rain is predicted and in w6 she knows rain is not predicted. 452\nChapter\n12.\nKnowledge Representation\n(a)\n(b)\n(c)\nw0: I,R\nw2: I,¬R\nw3: ¬I,¬R\nw1: ¬I,R\nw4: I,R\nw6: I,¬R\nw7: ¬I,¬R\nw5: ¬I,R\nw0: I,R\nw2: I,¬R\nw3: ¬I,¬R\nw1: ¬I,R\nw4: I,R\nw5: ¬I,R\nw6: I,¬R\nw7: ¬I,¬R\nFigure 12.4\nPossible worlds with accessibility relations KSuperman (solid arrows) and\nKLois (dotted arrows). The proposition R means “the weather report for tomorrow is rain”\nand I means “Superman’s secret identity is Clark Kent.” All worlds are accessible to them-\nselves; the arrows from a world to itself are not shown.\nSuperman does not know the report, but he knows that Lois knows, because in every world\nthat is accessible to him, either she knows R or she knows ¬R.\nIn the BOTTOM diagram we represent the scenario where it is common knowledge that\nSuperman knows his identity, and Lois might or might not have seen the weather report. We\nrepresent this by combining the two top scenarios, and adding arrows to show that Superman\ndoes not know which scenario actually holds. Lois does know, so we don’t need to add any\narrows for her. In w0 Superman still knows I but not R, and now he does not know whether\nLois knows R. From what Superman knows, he might be in w0 or w2, in which case Lois\ndoes not know whether R is true, or he could be in w4, in which case she knows R, or w6, in\nwhich case she knows ¬R.\nThere are an inﬁnite number of possible worlds, so the trick is to introduce just the ones\nyou need to represent what you are trying to model. A new possible world is needed to talk\nabout different possible facts (e.g., rain is predicted or not), or to talk about different states\nof knowledge (e.g., does Lois know that rain is predicted). That means two possible worlds,\nsuch as w4 and w0 in Figure 12.4, might have the same base facts about the world, but differ\nin their accessibility relations, and therefore in facts about knowledge.",
  "such as w4 and w0 in Figure 12.4, might have the same base facts about the world, but differ\nin their accessibility relations, and therefore in facts about knowledge.\nModal logic solves some tricky issues with the interplay of quantiﬁers and knowledge.\nThe English sentence “Bond knows that someone is a spy” is ambiguous. The ﬁrst reading is Section 12.5.\nReasoning Systems for Categories\n453\nthat there is a particular someone who Bond knows is a spy; we can write this as\n∃x KBondSpy(x) ,\nwhich in modal logic means that there is an x that, in all accessible worlds, Bond knows to\nbe a spy. The second reading is that Bond just knows that there is at least one spy:\nKBond∃x Spy(x) .\nThe modal logic interpretation is that in each accessible world there is an x that is a spy, but\nit need not be the same x in each world.\nNow that we have a modal operator for knowledge, we can write axioms for it. First,\nwe can say that agents are able to draw deductions; if an agent knows P and knows that P\nimplies Q, then the agent knows Q:\n(KaP ∧Ka(P ⇒Q)) ⇒KaQ .\nFrom this (and a few other rules about logical identities) we can establish that KA(P ∨¬P)\nis a tautology; every agent knows every proposition P is either true or false. On the other\nhand, (KAP) ∨(KA¬P) is not a tautology; in general, there will be lots of propositions that\nan agent does not know to be true and does not know to be false.\nIt is said (going back to Plato) that knowledge is justiﬁed true belief. That is, if it is\ntrue, if you believe it, and if you have an unassailably good reason, then you know it. That\nmeans that if you know something, it must be true, and we have the axiom:\nKaP ⇒P .\nFurthermore, logical agents should be able to introspect on their own knowledge. If they\nknow something, then they know that they know it:\nKaP ⇒Ka(KaP) .\nWe can deﬁne similar axioms for belief (often denoted by B) and other modalities. However,\none problem with the modal logic approach is that it assumes logical omniscience on the\nLOGICAL\nOMNISCIENCE\npart of agents. That is, if an agent knows a set of axioms, then it knows all consequences of\nthose axioms. This is on shaky ground even for the somewhat abstract notion of knowledge,\nbut it seems even worse for belief, because belief has more connotation of referring to things\nthat are physically represented in the agent, not just potentially derivable. There have been\nattempts to deﬁne a form of limited rationality for agents; to say that agents believe those",
  "that are physically represented in the agent, not just potentially derivable. There have been\nattempts to deﬁne a form of limited rationality for agents; to say that agents believe those\nassertions that can be derived with the application of no more than k reasoning steps, or no\nmore than s seconds of computation. These attempts have been generally unsatisfactory.\n12.5\nREASONING SYSTEMS FOR CATEGORIES\nCategories are the primary building blocks of large-scale knowledge representation schemes.\nThis section describes systems specially designed for organizing and reasoning with cate-\ngories. There are two closely related families of systems: semantic networks provide graph-\nical aids for visualizing a knowledge base and efﬁcient algorithms for inferring properties 454\nChapter\n12.\nKnowledge Representation\nof an object on the basis of its category membership; and description logics provide a for-\nmal language for constructing and combining category deﬁnitions and efﬁcient algorithms\nfor deciding subset and superset relationships between categories.\n12.5.1\nSemantic networks\nIn 1909, Charles S. Peirce proposed a graphical notation of nodes and edges called existential\ngraphs that he called “the logic of the future.” Thus began a long-running debate between\nEXISTENTIAL\nGRAPHS\nadvocates of “logic” and advocates of “semantic networks.” Unfortunately, the debate ob-\nscured the fact that semantics networks—at least those with well-deﬁned semantics—are a\nform of logic. The notation that semantic networks provide for certain kinds of sentences\nis often more convenient, but if we strip away the “human interface” issues, the underlying\nconcepts—objects, relations, quantiﬁcation, and so on—are the same.\nThere are many variants of semantic networks, but all are capable of representing in-\ndividual objects, categories of objects, and relations among objects. A typical graphical no-\ntation displays object or category names in ovals or boxes, and connects them with labeled\nlinks. For example, Figure 12.5 has a MemberOf link between Mary and FemalePersons,\ncorresponding to the logical assertion Mary ∈FemalePersons; similarly, the SisterOf link\nbetween Mary and John corresponds to the assertion SisterOf (Mary, John). We can con-\nnect categories using SubsetOf links, and so on. It is such fun drawing bubbles and arrows\nthat one can get carried away. For example, we know that persons have female persons as",
  "nect categories using SubsetOf links, and so on. It is such fun drawing bubbles and arrows\nthat one can get carried away. For example, we know that persons have female persons as\nmothers, so can we draw a HasMother link from Persons to FemalePersons? The answer\nis no, because HasMother is a relation between a person and his or her mother, and categories\ndo not have mothers.5\nFor this reason, we have used a special notation—the double-boxed link—in Figure 12.5.\nThis link asserts that\n∀x x ∈Persons ⇒[∀y HasMother(x, y) ⇒y ∈FemalePersons] .\nWe might also want to assert that persons have two legs—that is,\n∀x x ∈Persons ⇒Legs(x, 2) .\nAs before, we need to be careful not to assert that a category has legs; the single-boxed link\nin Figure 12.5 is used to assert properties of every member of a category.\nThe semantic network notation makes it convenient to perform inheritance reasoning\nof the kind introduced in Section 12.2. For example, by virtue of being a person, Mary inherits\nthe property of having two legs. Thus, to ﬁnd out how many legs Mary has, the inheritance\nalgorithm follows the MemberOf link from Mary to the category she belongs to, and then\nfollows SubsetOf links up the hierarchy until it ﬁnds a category for which there is a boxed\nLegs link—in this case, the Persons category. The simplicity and efﬁciency of this inference\n5 Several early systems failed to distinguish between properties of members of a category and properties of the\ncategory as a whole. This can lead directly to inconsistencies, as pointed out by Drew McDermott (1976) in his\narticle “Artiﬁcial Intelligence Meets Natural Stupidity.” Another common problem was the use of IsA links for\nboth subset and membership relations, in correspondence with English usage: “a cat is a mammal” and “Fiﬁis a\ncat.” See Exercise 12.22 for more on these issues. Section 12.5.\nReasoning Systems for Categories\n455\nMammals\nJohn\nMary\nPersons\nMale\nPersons\nFemale\nPersons\n1\n2\nSubsetOf\nSubsetOf\nSubsetOf\nMemberOf\nMemberOf\nSisterOf\nLegs\nLegs\nHasMother\nFigure 12.5\nA semantic network with four objects (John, Mary, 1, and 2) and four cate-\ngories. Relations are denoted by labeled links.\nMemberOf\nFlyEvents\nFly17\nShankar\nNewYork\nNewDelhi\nYesterday\nAgent\nOrigin\nDestination\nDuring\nFigure 12.6\nA fragment of a semantic network showing the representation of the logical\nassertion Fly(Shankar, NewYork, NewDelhi, Yesterday).\nmechanism, compared with logical theorem proving, has been one of the main attractions of",
  "Destination\nDuring\nFigure 12.6\nA fragment of a semantic network showing the representation of the logical\nassertion Fly(Shankar, NewYork, NewDelhi, Yesterday).\nmechanism, compared with logical theorem proving, has been one of the main attractions of\nsemantic networks.\nInheritance becomes complicated when an object can belong to more than one category\nor when a category can be a subset of more than one other category; this is called multiple in-\nheritance. In such cases, the inheritance algorithm might ﬁnd two or more conﬂicting values\nMULTIPLE\nINHERITANCE\nanswering the query. For this reason, multiple inheritance is banned in some object-oriented\nprogramming (OOP) languages, such as Java, that use inheritance in a class hierarchy. It is\nusually allowed in semantic networks, but we defer discussion of that until Section 12.6.\nThe reader might have noticed an obvious drawback of semantic network notation, com-\npared to ﬁrst-order logic: the fact that links between bubbles represent only binary relations.\nFor example, the sentence Fly(Shankar, NewYork, NewDelhi, Yesterday) cannot be as-\nserted directly in a semantic network. Nonetheless, we can obtain the effect of n-ary asser-\ntions by reifying the proposition itself as an event belonging to an appropriate event category.\nFigure 12.6 shows the semantic network structure for this particular event. Notice that the\nrestriction to binary relations forces the creation of a rich ontology of reiﬁed concepts.\nReiﬁcation of propositions makes it possible to represent every ground, function-free\natomic sentence of ﬁrst-order logic in the semantic network notation. Certain kinds of univer- 456\nChapter\n12.\nKnowledge Representation\nsally quantiﬁed sentences can be asserted using inverse links and the singly boxed and doubly\nboxed arrows applied to categories, but that still leaves us a long way short of full ﬁrst-order\nlogic. Negation, disjunction, nested function symbols, and existential quantiﬁcation are all\nmissing. Now it is possible to extend the notation to make it equivalent to ﬁrst-order logic—as\nin Peirce’s existential graphs—but doing so negates one of the main advantages of semantic\nnetworks, which is the simplicity and transparency of the inference processes. Designers can\nbuild a large network and still have a good idea about what queries will be efﬁcient, because\n(a) it is easy to visualize the steps that the inference procedure will go through and (b) in some",
  "build a large network and still have a good idea about what queries will be efﬁcient, because\n(a) it is easy to visualize the steps that the inference procedure will go through and (b) in some\ncases the query language is so simple that difﬁcult queries cannot be posed. In cases where\nthe expressive power proves to be too limiting, many semantic network systems provide for\nprocedural attachment to ﬁll in the gaps. Procedural attachment is a technique whereby\na query about (or sometimes an assertion of) a certain relation results in a call to a special\nprocedure designed for that relation rather than a general inference algorithm.\nOne of the most important aspects of semantic networks is their ability to represent\ndefault values for categories. Examining Figure 12.5 carefully, one notices that John has one\nDEFAULT VALUE\nleg, despite the fact that he is a person and all persons have two legs. In a strictly logical KB,\nthis would be a contradiction, but in a semantic network, the assertion that all persons have\ntwo legs has only default status; that is, a person is assumed to have two legs unless this is\ncontradicted by more speciﬁc information. The default semantics is enforced naturally by the\ninheritance algorithm, because it follows links upwards from the object itself (John in this\ncase) and stops as soon as it ﬁnds a value. We say that the default is overridden by the more\nOVERRIDING\nspeciﬁc value. Notice that we could also override the default number of legs by creating a\ncategory of OneLeggedPersons, a subset of Persons of which John is a member.\nWe can retain a strictly logical semantics for the network if we say that the Legs asser-\ntion for Persons includes an exception for John:\n∀x x ∈Persons ∧x ̸= John ⇒Legs(x, 2) .\nFor a ﬁxed network, this is semantically adequate but will be much less concise than the\nnetwork notation itself if there are lots of exceptions. For a network that will be updated with\nmore assertions, however, such an approach fails—we really want to say that any persons as\nyet unknown with one leg are exceptions too. Section 12.6 goes into more depth on this issue\nand on default reasoning in general.\n12.5.2\nDescription logics\nThe syntax of ﬁrst-order logic is designed to make it easy to say things about objects. De-\nscription logics are notations that are designed to make it easier to describe deﬁnitions and\nDESCRIPTION LOGIC\nproperties of categories. Description logic systems evolved from semantic networks in re-",
  "scription logics are notations that are designed to make it easier to describe deﬁnitions and\nDESCRIPTION LOGIC\nproperties of categories. Description logic systems evolved from semantic networks in re-\nsponse to pressure to formalize what the networks mean while retaining the emphasis on\ntaxonomic structure as an organizing principle.\nThe principal inference tasks for description logics are subsumption (checking if one\nSUBSUMPTION\ncategory is a subset of another by comparing their deﬁnitions) and classiﬁcation (checking\nCLASSIFICATION\nwhether an object belongs to a category).. Some systems also include consistency of a cate-\ngory deﬁnition—whether the membership criteria are logically satisﬁable. Section 12.5.\nReasoning Systems for Categories\n457\nConcept\n→\nThing | ConceptName\n|\nAnd(Concept, . . .)\n|\nAll(RoleName, Concept)\n|\nAtLeast(Integer, RoleName)\n|\nAtMost(Integer, RoleName)\n|\nFills(RoleName, IndividualName, . . .)\n|\nSameAs(Path, Path)\n|\nOneOf(IndividualName, . . .)\nPath\n→\n[RoleName, . . .]\nFigure 12.7\nThe syntax of descriptions in a subset of the CLASSIC language.\nThe CLASSIC language (Borgida et al., 1989) is a typical description logic. The syntax\nof CLASSIC descriptions is shown in Figure 12.7.6 For example, to say that bachelors are\nunmarried adult males we would write\nBachelor = And(Unmarried, Adult, Male) .\nThe equivalent in ﬁrst-order logic would be\nBachelor(x) ⇔Unmarried(x) ∧Adult(x) ∧Male(x) .\nNotice that the description logic has an an algebra of operations on predicates, which of\ncourse we can’t do in ﬁrst-order logic. Any description in CLASSIC can be translated into an\nequivalent ﬁrst-order sentence, but some descriptions are more straightforward in CLASSIC.\nFor example, to describe the set of men with at least three sons who are all unemployed\nand married to doctors, and at most two daughters who are all professors in physics or math\ndepartments, we would use\nAnd(Man, AtLeast(3, Son), AtMost(2, Daughter),\nAll(Son, And(Unemployed, Married, All(Spouse, Doctor))),\nAll(Daughter, And(Professor, Fills(Department, Physics, Math)))) .\nWe leave it as an exercise to translate this into ﬁrst-order logic.\nPerhaps the most important aspect of description logics is their emphasis on tractability\nof inference. A problem instance is solved by describing it and then asking if it is subsumed\nby one of several possible solution categories. In standard ﬁrst-order logic systems, predicting",
  "of inference. A problem instance is solved by describing it and then asking if it is subsumed\nby one of several possible solution categories. In standard ﬁrst-order logic systems, predicting\nthe solution time is often impossible. It is frequently left to the user to engineer the represen-\ntation to detour around sets of sentences that seem to be causing the system to take several\nweeks to solve a problem. The thrust in description logics, on the other hand, is to ensure that\nsubsumption-testing can be solved in time polynomial in the size of the descriptions.7\n6 Notice that the language does not allow one to simply state that one concept, or category, is a subset of\nanother. This is a deliberate policy: subsumption between categories must be derivable from some aspects of the\ndescriptions of the categories. If not, then something is missing from the descriptions.\n7\nCLASSIC provides efﬁcient subsumption testing in practice, but the worst-case run time is exponential. 458\nChapter\n12.\nKnowledge Representation\nThis sounds wonderful in principle, until one realizes that it can only have one of two\nconsequences: either hard problems cannot be stated at all, or they require exponentially\nlarge descriptions! However, the tractability results do shed light on what sorts of constructs\ncause problems and thus help the user to understand how different representations behave.\nFor example, description logics usually lack negation and disjunction. Each forces ﬁrst-\norder logical systems to go through a potentially exponential case analysis in order to ensure\ncompleteness. CLASSIC allows only a limited form of disjunction in the Fills and OneOf\nconstructs, which permit disjunction over explicitly enumerated individuals but not over de-\nscriptions. With disjunctive descriptions, nested deﬁnitions can lead easily to an exponential\nnumber of alternative routes by which one category can subsume another.\n12.6\nREASONING WITH DEFAULT INFORMATION\nIn the preceding section, we saw a simple example of an assertion with default status: people\nhave two legs. This default can be overridden by more speciﬁc information, such as that\nLong John Silver has one leg. We saw that the inheritance mechanism in semantic networks\nimplements the overriding of defaults in a simple and natural way. In this section, we study\ndefaults more generally, with a view toward understanding the semantics of defaults rather\nthan just providing a procedural mechanism.\n12.6.1\nCircumscription and default logic",
  "defaults more generally, with a view toward understanding the semantics of defaults rather\nthan just providing a procedural mechanism.\n12.6.1\nCircumscription and default logic\nWe have seen two examples of reasoning processes that violate the monotonicity property of\nlogic that was proved in Chapter 7.8 In this chapter we saw that a property inherited by all\nmembers of a category in a semantic network could be overridden by more speciﬁc informa-\ntion for a subcategory. In Section 9.4.5, we saw that under the closed-world assumption, if a\nproposition α is not mentioned in KB then KB |= ¬α, but KB ∧α |= α.\nSimple introspection suggests that these failures of monotonicity are widespread in\ncommonsense reasoning. It seems that humans often “jump to conclusions.” For example,\nwhen one sees a car parked on the street, one is normally willing to believe that it has four\nwheels even though only three are visible. Now, probability theory can certainly provide a\nconclusion that the fourth wheel exists with high probability, yet, for most people, the possi-\nbility of the car’s not having four wheels does not arise unless some new evidence presents\nitself. Thus, it seems that the four-wheel conclusion is reached by default, in the absence of\nany reason to doubt it. If new evidence arrives—for example, if one sees the owner carrying\na wheel and notices that the car is jacked up—then the conclusion can be retracted. This kind\nof reasoning is said to exhibit nonmonotonicity, because the set of beliefs does not grow\nNONMONOTONICITY\nmonotonically over time as new evidence arrives. Nonmonotonic logics have been devised\nNONMONOTONIC\nLOGIC\nwith modiﬁed notions of truth and entailment in order to capture such behavior. We will look\nat two such logics that have been studied extensively: circumscription and default logic.\n8 Recall that monotonicity requires all entailed sentences to remain entailed after new sentences are added to the\nKB. That is, if KB |= α then KB ∧β |= α. Section 12.6.\nReasoning with Default Information\n459\nCircumscription can be seen as a more powerful and precise version of the closed-\nCIRCUMSCRIPTION\nworld assumption. The idea is to specify particular predicates that are assumed to be “as false\nas possible”—that is, false for every object except those for which they are known to be true.\nFor example, suppose we want to assert the default rule that birds ﬂy. We would introduce a\npredicate, say Abnormal1(x), and write\nBird(x) ∧¬Abnormal1(x) ⇒Flies(x) .",
  "For example, suppose we want to assert the default rule that birds ﬂy. We would introduce a\npredicate, say Abnormal1(x), and write\nBird(x) ∧¬Abnormal1(x) ⇒Flies(x) .\nIf we say that Abnormal 1 is to be circumscribed, a circumscriptive reasoner is entitled to\nassume ¬Abnormal1(x) unless Abnormal1(x) is known to be true. This allows the con-\nclusion Flies(Tweety) to be drawn from the premise Bird(Tweety), but the conclusion no\nlonger holds if Abnormal1(Tweety) is asserted.\nCircumscription can be viewed as an example of a model preference logic. In such\nMODEL\nPREFERENCE\nlogics, a sentence is entailed (with default status) if it is true in all preferred models of the KB,\nas opposed to the requirement of truth in all models in classical logic. For circumscription,\none model is preferred to another if it has fewer abnormal objects.9 Let us see how this idea\nworks in the context of multiple inheritance in semantic networks. The standard example for\nwhich multiple inheritance is problematic is called the “Nixon diamond.” It arises from the\nobservation that Richard Nixon was both a Quaker (and hence by default a paciﬁst) and a\nRepublican (and hence by default not a paciﬁst). We can write this as follows:\nRepublican(Nixon) ∧Quaker(Nixon) .\nRepublican(x) ∧¬Abnormal2(x) ⇒¬Paciﬁst(x) .\nQuaker(x) ∧¬Abnormal3(x) ⇒Paciﬁst(x) .\nIf we circumscribe Abnormal2 and Abnormal3, there are two preferred models: one in\nwhich Abnormal 2(Nixon) and Paciﬁst(Nixon) hold and one in which Abnormal 3(Nixon)\nand ¬Paciﬁst(Nixon) hold. Thus, the circumscriptive reasoner remains properly agnostic as\nto whether Nixon was a paciﬁst. If we wish, in addition, to assert that religious beliefs take\nprecedence over political beliefs, we can use a formalism called prioritized circumscription\nPRIORITIZED\nCIRCUMSCRIPTION\nto give preference to models where Abnormal3 is minimized.\nDefault logic is a formalism in which default rules can be written to generate contin-\nDEFAULT LOGIC\nDEFAULT RULES\ngent, nonmonotonic conclusions. A default rule looks like this:\nBird(x) : Flies(x)/Flies(x) .\nThis rule means that if Bird(x) is true, and if Flies(x) is consistent with the knowledge base,\nthen Flies(x) may be concluded by default. In general, a default rule has the form\nP : J1, . . . , Jn/C\nwhere P is called the prerequisite, C is the conclusion, and Ji are the justiﬁcations—if any\none of them can be proven false, then the conclusion cannot be drawn. Any variable that",
  "P : J1, . . . , Jn/C\nwhere P is called the prerequisite, C is the conclusion, and Ji are the justiﬁcations—if any\none of them can be proven false, then the conclusion cannot be drawn. Any variable that\n9 For the closed-world assumption, one model is preferred to another if it has fewer true atoms—that is, preferred\nmodels are minimal models. There is a natural connection between the closed-world assumption and deﬁnite-\nclause KBs, because the ﬁxed point reached by forward chaining on deﬁnite-clause KBs is the unique minimal\nmodel. See page 258 for more on this point. 460\nChapter\n12.\nKnowledge Representation\nappears in Ji or C must also appear in P. The Nixon-diamond example can be represented\nin default logic with one fact and two default rules:\nRepublican(Nixon) ∧Quaker(Nixon) .\nRepublican(x) : ¬Paciﬁst(x)/¬Paciﬁst(x) .\nQuaker(x) : Paciﬁst(x)/Paciﬁst(x) .\nTo interpret what the default rules mean, we deﬁne the notion of an extension of a default\nEXTENSION\ntheory to be a maximal set of consequences of the theory. That is, an extension S consists\nof the original known facts and a set of conclusions from the default rules, such that no\nadditional conclusions can be drawn from S and the justiﬁcations of every default conclusion\nin S are consistent with S. As in the case of the preferred models in circumscription, we have\ntwo possible extensions for the Nixon diamond: one wherein he is a paciﬁst and one wherein\nhe is not. Prioritized schemes exist in which some default rules can be given precedence over\nothers, allowing some ambiguities to be resolved.\nSince 1980, when nonmonotonic logics were ﬁrst proposed, a great deal of progress\nhas been made in understanding their mathematical properties. There are still unresolved\nquestions, however. For example, if “Cars have four wheels” is false, what does it mean\nto have it in one’s knowledge base? What is a good set of default rules to have? If we\ncannot decide, for each rule separately, whether it belongs in our knowledge base, then we\nhave a serious problem of nonmodularity. Finally, how can beliefs that have default status be\nused to make decisions? This is probably the hardest issue for default reasoning. Decisions\noften involve tradeoffs, and one therefore needs to compare the strengths of belief in the\noutcomes of different actions, and the costs of making a wrong decision. In cases where the\nsame kinds of decisions are being made repeatedly, it is possible to interpret default rules",
  "outcomes of different actions, and the costs of making a wrong decision. In cases where the\nsame kinds of decisions are being made repeatedly, it is possible to interpret default rules\nas “threshold probability” statements. For example, the default rule “My brakes are always\nOK” really means “The probability that my brakes are OK, given no other information, is\nsufﬁciently high that the optimal decision is for me to drive without checking them.” When\nthe decision context changes—for example, when one is driving a heavily laden truck down a\nsteep mountain road—the default rule suddenly becomes inappropriate, even though there is\nno new evidence of faulty brakes. These considerations have led some researchers to consider\nhow to embed default reasoning within probability theory or utility theory.\n12.6.2\nTruth maintenance systems\nWe have seen that many of the inferences drawn by a knowledge representation system will\nhave only default status, rather than being absolutely certain. Inevitably, some of these in-\nferred facts will turn out to be wrong and will have to be retracted in the face of new informa-\ntion. This process is called belief revision.10 Suppose that a knowledge base KB contains\nBELIEF REVISION\na sentence P—perhaps a default conclusion recorded by a forward-chaining algorithm, or\nperhaps just an incorrect assertion—and we want to execute TELL(KB, ¬P). To avoid cre-\nating a contradiction, we must ﬁrst execute RETRACT(KB, P). This sounds easy enough.\n10 Belief revision is often contrasted with belief update, which occurs when a knowledge base is revised to reﬂect\na change in the world rather than new information about a ﬁxed world. Belief update combines belief revision\nwith reasoning about time and change; it is also related to the process of ﬁltering described in Chapter 15. Section 12.6.\nReasoning with Default Information\n461\nProblems arise, however, if any additional sentences were inferred from P and asserted in\nthe KB. For example, the implication P\n⇒Q might have been used to add Q. The obvious\n“solution”—retracting all sentences inferred from P—fails because such sentences may have\nother justiﬁcations besides P. For example, if R and R ⇒Q are also in the KB, then Q\ndoes not have to be removed after all. Truth maintenance systems, or TMSs, are designed\nTRUTH\nMAINTENANCE\nSYSTEM\nto handle exactly these kinds of complications.\nOne simple approach to truth maintenance is to keep track of the order in which sen-",
  "does not have to be removed after all. Truth maintenance systems, or TMSs, are designed\nTRUTH\nMAINTENANCE\nSYSTEM\nto handle exactly these kinds of complications.\nOne simple approach to truth maintenance is to keep track of the order in which sen-\ntences are told to the knowledge base by numbering them from P1 to Pn. When the call\nRETRACT(KB, Pi) is made, the system reverts to the state just before Pi was added, thereby\nremoving both Pi and any inferences that were derived from Pi. The sentences Pi+1 through\nPn can then be added again. This is simple, and it guarantees that the knowledge base will\nbe consistent, but retracting Pi requires retracting and reasserting n −i sentences as well as\nundoing and redoing all the inferences drawn from those sentences. For systems to which\nmany facts are being added—such as large commercial databases—this is impractical.\nA more efﬁcient approach is the justiﬁcation-based truth maintenance system, or JTMS.\nJTMS\nIn a JTMS, each sentence in the knowledge base is annotated with a justiﬁcation consisting\nJUSTIFICATION\nof the set of sentences from which it was inferred. For example, if the knowledge base\nalready contains P\n⇒Q, then TELL(P) will cause Q to be added with the justiﬁcation\n{P, P\n⇒Q}. In general, a sentence can have any number of justiﬁcations. Justiﬁca-\ntions make retraction efﬁcient. Given the call RETRACT(P), the JTMS will delete exactly\nthose sentences for which P is a member of every justiﬁcation. So, if a sentence Q had\nthe single justiﬁcation {P, P ⇒Q}, it would be removed; if it had the additional justi-\nﬁcation {P, P ∨R ⇒Q}, it would still be removed; but if it also had the justiﬁcation\n{R, P ∨R ⇒Q}, then it would be spared. In this way, the time required for retraction of P\ndepends only on the number of sentences derived from P rather than on the number of other\nsentences added since P entered the knowledge base.\nThe JTMS assumes that sentences that are considered once will probably be considered\nagain, so rather than deleting a sentence from the knowledge base entirely when it loses\nall justiﬁcations, we merely mark the sentence as being out of the knowledge base. If a\nsubsequent assertion restores one of the justiﬁcations, then we mark the sentence as being\nback in. In this way, the JTMS retains all the inference chains that it uses and need not\nrederive sentences when a justiﬁcation becomes valid again.\nIn addition to handling the retraction of incorrect information, TMSs can be used to",
  "back in. In this way, the JTMS retains all the inference chains that it uses and need not\nrederive sentences when a justiﬁcation becomes valid again.\nIn addition to handling the retraction of incorrect information, TMSs can be used to\nspeed up the analysis of multiple hypothetical situations. Suppose, for example, that the\nRomanian Olympic Committee is choosing sites for the swimming, athletics, and eques-\ntrian events at the 2048 Games to be held in Romania. For example, let the ﬁrst hypothe-\nsis be Site(Swimming, Pitesti), Site(Athletics, Bucharest), and Site(Equestrian, Arad).\nA great deal of reasoning must then be done to work out the logistical consequences and\nhence the desirability of this selection. If we want to consider Site(Athletics, Sibiu) in-\nstead, the TMS avoids the need to start again from scratch.\nInstead, we simply retract\nSite(Athletics, Bucharest) and assert Site(Athletics, Sibiu) and the TMS takes care of the\nnecessary revisions. Inference chains generated from the choice of Bucharest can be reused\nwith Sibiu, provided that the conclusions are the same. 462\nChapter\n12.\nKnowledge Representation\nAn assumption-based truth maintenance system, or ATMS, makes this type of context-\nATMS\nswitching between hypothetical worlds particularly efﬁcient. In a JTMS, the maintenance of\njustiﬁcations allows you to move quickly from one state to another by making a few retrac-\ntions and assertions, but at any time only one state is represented. An ATMS represents all the\nstates that have ever been considered at the same time. Whereas a JTMS simply labels each\nsentence as being in or out, an ATMS keeps track, for each sentence, of which assumptions\nwould cause the sentence to be true. In other words, each sentence has a label that consists of\na set of assumption sets. The sentence holds just in those cases in which all the assumptions\nin one of the assumption sets hold.\nTruth maintenance systems also provide a mechanism for generating explanations.\nEXPLANATION\nTechnically, an explanation of a sentence P is a set of sentences E such that E entails P.\nIf the sentences in E are already known to be true, then E simply provides a sufﬁcient ba-\nsis for proving that P must be the case. But explanations can also include assumptions—\nASSUMPTION\nsentences that are not known to be true, but would sufﬁce to prove P if they were true. For\nexample, one might not have enough information to prove that one’s car won’t start, but a",
  "ASSUMPTION\nsentences that are not known to be true, but would sufﬁce to prove P if they were true. For\nexample, one might not have enough information to prove that one’s car won’t start, but a\nreasonable explanation might include the assumption that the battery is dead. This, combined\nwith knowledge of how cars operate, explains the observed nonbehavior. In most cases, we\nwill prefer an explanation E that is minimal, meaning that there is no proper subset of E that\nis also an explanation. An ATMS can generate explanations for the “car won’t start” problem\nby making assumptions (such as “gas in car” or “battery dead”) in any order we like, even if\nsome assumptions are contradictory. Then we look at the label for the sentence “car won’t\nstart” to read off the sets of assumptions that would justify the sentence.\nThe exact algorithms used to implement truth maintenance systems are a little compli-\ncated, and we do not cover them here. The computational complexity of the truth maintenance\nproblem is at least as great as that of propositional inference—that is, NP-hard. Therefore,\nyou should not expect truth maintenance to be a panacea. When used carefully, however, a\nTMS can provide a substantial increase in the ability of a logical system to handle complex\nenvironments and hypotheses.\n12.7\nTHE INTERNET SHOPPING WORLD\nIn this ﬁnal section we put together all we have learned to encode knowledge for a shopping\nresearch agent that helps a buyer ﬁnd product offers on the Internet. The shopping agent is\ngiven a product description by the buyer and has the task of producing a list of Web pages\nthat offer such a product for sale, and ranking which offers are best. In some cases the\nbuyer’s product description will be precise, as in Canon Rebel XTi digital camera, and the\ntask is then to ﬁnd the store(s) with the best offer. In other cases the description will be only\npartially speciﬁed, as in digital camera for under $300, and the agent will have to compare\ndifferent products.\nThe shopping agent’s environment is the entire World Wide Web in its full complexity—\nnot a toy simulated environment. The agent’s percepts are Web pages, but whereas a human Section 12.7.\nThe Internet Shopping World\n463\nExample Online Store\nSelect from our ﬁne line of products:\n• Computers\n• Cameras\n• Books\n• Videos\n• Music\n<h1>Example Online Store</h1>\n<i>Select</i> from our fine line of products:\n<ul>\n<li> <a href=\"http://example.com/compu\">Computers</a>",
  "463\nExample Online Store\nSelect from our ﬁne line of products:\n• Computers\n• Cameras\n• Books\n• Videos\n• Music\n<h1>Example Online Store</h1>\n<i>Select</i> from our fine line of products:\n<ul>\n<li> <a href=\"http://example.com/compu\">Computers</a>\n<li> <a href=\"http://example.com/camer\">Cameras</a>\n<li> <a href=\"http://example.com/books\">Books</a>\n<li> <a href=\"http://example.com/video\">Videos</a>\n<li> <a href=\"http://example.com/music\">Music</a>\n</ul>\nFigure 12.8\nA Web page from a generic online store in the form perceived by the human\nuser of a browser (top), and the corresponding HTML string as perceived by the browser or\nthe shopping agent (bottom). In HTML, characters between < and > are markup directives\nthat specify how the page is displayed. For example, the string <i>Select</i> means\nto switch to italic font, display the word Select, and then end the use of italic font. A page\nidentiﬁer such as http://example.com/books is called a uniform resource locator\n(URL). The markup <a href=\"url\">Books</a> means to create a hypertext link to url\nwith the anchor text Books.\nWeb user would see pages displayed as an array of pixels on a screen, the shopping agent\nwill perceive a page as a character string consisting of ordinary words interspersed with for-\nmatting commands in the HTML markup language. Figure 12.8 shows a Web page and a\ncorresponding HTML character string. The perception problem for the shopping agent in-\nvolves extracting useful information from percepts of this kind.\nClearly, perception on Web pages is easier than, say, perception while driving a taxi in\nCairo. Nonetheless, there are complications to the Internet perception task. The Web page in\nFigure 12.8 is simple compared to real shopping sites, which may include CSS, cookies, Java,\nJavascript, Flash, robot exclusion protocols, malformed HTML, sound ﬁles, movies, and text\nthat appears only as part of a JPEG image. An agent that can deal with all of the Internet is\nalmost as complex as a robot that can move in the real world. We concentrate on a simple\nagent that ignores most of these complications.\nThe agent’s ﬁrst task is to collect product offers that are relevant to a query. If the query\nis “laptops,” then a Web page with a review of the latest high-end laptop would be relevant,\nbut if it doesn’t provide a way to buy, it isn’t an offer. For now, we can say a page is an offer\nif it contains the words “buy” or “price” or “add to cart” within an HTML link or form on the 464\nChapter\n12.",
  "but if it doesn’t provide a way to buy, it isn’t an offer. For now, we can say a page is an offer\nif it contains the words “buy” or “price” or “add to cart” within an HTML link or form on the 464\nChapter\n12.\nKnowledge Representation\npage. For example, if the page contains a string of the form “<a . . . add to cart . . . </a”\nthen it is an offer. This could be represented in ﬁrst-order logic, but it is more straightforward\nto encode it into program code. We show how to do more sophisticated information extraction\nin Section 22.4.\n12.7.1\nFollowing links\nThe strategy is to start at the home page of an online store and consider all pages that can be\nreached by following relevant links.11 The agent will have knowledge of a number of stores,\nfor example:\nAmazon ∈OnlineStores ∧Homepage(Amazon, “amazon.com”) .\nEbay ∈OnlineStores ∧Homepage(Ebay, “ebay.com”) .\nExampleStore ∈OnlineStores ∧Homepage(ExampleStore, “example.com”) .\nThese stores classify their goods into product categories, and provide links to the major cat-\negories from their home page. Minor categories can be reached through a chain of relevant\nlinks, and eventually we will reach offers. In other words, a page is relevant to the query if it\ncan be reached by a chain of zero or more relevant category links from a store’s home page,\nand then from one more link to the product offer. We can deﬁne relevance:\nRelevant(page, query) ⇔\n∃store, home store ∈OnlineStores ∧Homepage(store, home)\n∧∃url, url2 RelevantChain(home, url2, query) ∧Link(url2, url)\n∧page = Contents(url) .\nHere the predicate Link(from, to) means that there is a hyperlink from the from URL to\nthe to URL. To deﬁne what counts as a RelevantChain, we need to follow not just any old\nhyperlinks, but only those links whose associated anchor text indicates that the link is relevant\nto the product query. For this, we use LinkText(from, to, text) to mean that there is a link\nbetween from and to with text as the anchor text. A chain of links between two URLs, start\nand end, is relevant to a description d if the anchor text of each link is a relevant category\nname for d. The existence of the chain itself is determined by a recursive deﬁnition, with the\nempty chain (start = end) as the base case:\nRelevantChain(start, end, query) ⇔(start = end)\n∨(∃u, text LinkText(start, u, text) ∧RelevantCategoryName(query, text)\n∧RelevantChain(u, end, query)) .\nNow we must deﬁne what it means for text to be a RelevantCategoryName for query.",
  "RelevantChain(start, end, query) ⇔(start = end)\n∨(∃u, text LinkText(start, u, text) ∧RelevantCategoryName(query, text)\n∧RelevantChain(u, end, query)) .\nNow we must deﬁne what it means for text to be a RelevantCategoryName for query.\nFirst, we need to relate strings to the categories they name. This is done using the predicate\nName(s, c), which says that string s is a name for category c—for example, we might assert\nthat Name(“laptops”, LaptopComputers). Some more examples of the Name predicate\nappear in Figure 12.9(b). Next, we deﬁne relevance. Suppose that query is “laptops.” Then\nRelevantCategoryName(query, text) is true when one of the following holds:\n• The text and query name the same category—e.g., “notebooks” and “laptops.”\n11 An alternative to the link-following strategy is to use an Internet search engine; the technology behind Internet\nsearch, information retrieval, will be covered in Section 22.3. Section 12.7.\nThe Internet Shopping World\n465\nBooks ⊂Products\nMusicRecordings ⊂Products\nMusicCDs ⊂MusicRecordings\nElectronics ⊂Products\nDigitalCameras ⊂Electronics\nStereoEquipment ⊂Electronics\nComputers ⊂Electronics\nDesktopComputers ⊂Computers\nLaptopComputers ⊂Computers\n. . .\nName(“books”, Books)\nName(“music”, MusicRecordings)\nName(“CDs”, MusicCDs)\nName(“electronics”, Electronics)\nName(“digital cameras”, DigitalCameras)\nName(“stereos”, StereoEquipment)\nName(“computers”, Computers)\nName(“desktops”, DesktopComputers)\nName(“laptops”, LaptopComputers)\nName(“notebooks”, LaptopComputers)\n. . .\n(a)\n(b)\nFigure 12.9\n(a) Taxonomy of product categories. (b) Names for those categories.\n• The text names a supercategory such as “computers.”\n• The text names a subcategory such as “ultralight notebooks.”\nThe logical deﬁnition of RelevantCategoryName is as follows:\nRelevantCategoryName(query, text) ⇔\n∃c1, c2 Name(query, c1) ∧Name(text, c2) ∧(c1 ⊆c2 ∨c2 ⊆c1) .\n(12.1)\nOtherwise, the anchor text is irrelevant because it names a category outside this line, such as\n“clothes” or “lawn & garden.”\nTo follow relevant links, then, it is essential to have a rich hierarchy of product cate-\ngories. The top part of this hierarchy might look like Figure 12.9(a). It will not be feasible to\nlist all possible shopping categories, because a buyer could always come up with some new\ndesire and manufacturers will always come out with new products to satisfy them (electric\nkneecap warmers?). Nonetheless, an ontology of about a thousand categories will serve as a",
  "desire and manufacturers will always come out with new products to satisfy them (electric\nkneecap warmers?). Nonetheless, an ontology of about a thousand categories will serve as a\nvery useful tool for most buyers.\nIn addition to the product hierarchy itself, we also need to have a rich vocabulary of\nnames for categories.\nLife would be much easier if there were a one-to-one correspon-\ndence between categories and the character strings that name them. We have already seen\nthe problem of synonymy—two names for the same category, such as “laptop computers”\nand “laptops.” There is also the problem of ambiguity—one name for two or more different\ncategories. For example, if we add the sentence\nName(“CDs”, CertiﬁcatesOfDeposit)\nto the knowledge base in Figure 12.9(b), then “CDs” will name two different categories.\nSynonymy and ambiguity can cause a signiﬁcant increase in the number of paths that\nthe agent has to follow, and can sometimes make it difﬁcult to determine whether a given\npage is indeed relevant. A much more serious problem is the very broad range of descriptions\nthat a user can type and category names that a store can use. For example, the link might say\n“laptop” when the knowledge base has only “laptops” or the user might ask for “a computer 466\nChapter\n12.\nKnowledge Representation\nI can ﬁt on the tray table of an economy-class airline seat.” It is impossible to enumerate in\nadvance all the ways a category can be named, so the agent will have to be able to do addi-\ntional reasoning in some cases to determine if the Name relation holds. In the worst case, this\nrequires full natural language understanding, a topic that we will defer to Chapter 22. In prac-\ntice, a few simple rules—such as allowing “laptop” to match a category named “laptops”—go\na long way. Exercise 12.10 asks you to develop a set of such rules after doing some research\ninto online stores.\nGiven the logical deﬁnitions from the preceding paragraphs and suitable knowledge\nbases of product categories and naming conventions, are we ready to apply an inference\nalgorithm to obtain a set of relevant offers for our query? Not quite! The missing element\nis the Contents(url) function, which refers to the HTML page at a given URL. The agent\ndoesn’t have the page contents of every URL in its knowledge base; nor does it have explicit\nrules for deducing what those contents might be. Instead, we can arrange for the right HTTP",
  "doesn’t have the page contents of every URL in its knowledge base; nor does it have explicit\nrules for deducing what those contents might be. Instead, we can arrange for the right HTTP\nprocedure to be executed whenever a subgoal involves the Contents function. In this way, it\nappears to the inference engine as if the entire Web is inside the knowledge base. This is an\nexample of a general technique called procedural attachment, whereby particular predicates\nPROCEDURAL\nATTACHMENT\nand functions can be handled by special-purpose methods.\n12.7.2\nComparing offers\nLet us assume that the reasoning processes of the preceding section have produced a set of\noffer pages for our “laptops” query. To compare those offers, the agent must extract the rele-\nvant information—price, speed, disk size, weight, and so on—from the offer pages. This can\nbe a difﬁcult task with real Web pages, for all the reasons mentioned previously. A common\nway of dealing with this problem is to use programs called wrappers to extract information\nWRAPPER\nfrom a page. The technology of information extraction is discussed in Section 22.4. For\nnow we assume that wrappers exist, and when given a page and a knowledge base, they add\nassertions to the knowledge base. Typically, a hierarchy of wrappers would be applied to a\npage: a very general one to extract dates and prices, a more speciﬁc one to extract attributes\nfor computer-related products, and if necessary a site-speciﬁc one that knows the format of a\nparticular store. Given a page on the example.com site with the text\nIBM ThinkBook 970.\nOur price:\n$399.00\nfollowed by various technical speciﬁcations, we would like a wrapper to extract information\nsuch as the following:\n∃c, oﬀer c ∈LaptopComputers ∧oﬀer ∈ProductOﬀers ∧\nManufacturer(c, IBM ) ∧Model(c, ThinkBook970 ) ∧\nScreenSize(c, Inches(14)) ∧ScreenType(c, ColorLCD) ∧\nMemorySize(c, Gigabytes(2)) ∧CPUSpeed(c, GHz(1.2)) ∧\nOﬀeredProduct(oﬀer, c) ∧Store(oﬀer, GenStore) ∧\nURL(oﬀer, “example.com/computers/34356.html”) ∧\nPrice(oﬀer, $(399)) ∧Date(oﬀer, Today) .\nThis example illustrates several issues that arise when we take seriously the task of knowledge\nengineering for commercial transactions. For example, notice that the price is an attribute of Section 12.8.\nSummary\n467\nthe offer, not the product itself. This is important because the offer at a given store may\nchange from day to day even for the same individual laptop; for some categories—such as",
  "Summary\n467\nthe offer, not the product itself. This is important because the offer at a given store may\nchange from day to day even for the same individual laptop; for some categories—such as\nhouses and paintings—the same individual object may even be offered simultaneously by\ndifferent intermediaries at different prices. There are still more complications that we have\nnot handled, such as the possibility that the price depends on the method of payment and on\nthe buyer’s qualiﬁcations for certain discounts. The ﬁnal task is to compare the offers that\nhave been extracted. For example, consider these three offers:\nA : 1.4 GHz CPU, 2GB RAM, 250 GB disk, $299 .\nB : 1.2 GHz CPU, 4GB RAM, 350 GB disk, $500 .\nC : 1.2 GHz CPU, 2GB RAM, 250 GB disk, $399 .\nC is dominated by A; that is, A is cheaper and faster, and they are otherwise the same. In\ngeneral, X dominates Y if X has a better value on at least one attribute, and is not worse on\nany attribute. But neither A nor B dominates the other. To decide which is better we need\nto know how the buyer weighs CPU speed and price against memory and disk space. The\ngeneral topic of preferences among multiple attributes is addressed in Section 16.4; for now,\nour shopping agent will simply return a list of all undominated offers that meet the buyer’s\ndescription. In this example, both A and B are undominated. Notice that this outcome relies\non the assumption that everyone prefers cheaper prices, faster processors, and more storage.\nSome attributes, such as screen size on a notebook, depend on the user’s particular preference\n(portability versus visibility); for these, the shopping agent will just have to ask the user.\nThe shopping agent we have described here is a simple one; many reﬁnements are\npossible. Still, it has enough capability that with the right domain-speciﬁc knowledge it can\nactually be of use to a shopper. Because of its declarative construction, it extends easily to\nmore complex applications. The main point of this section is to show that some knowledge\nrepresentation—in particular, the product hierarchy—is necessary for such an agent, and that\nonce we have some knowledge in this form, the rest follows naturally.\n12.8\nSUMMARY\nBy delving into the details of how one represents a variety of knowledge, we hope we have\ngiven the reader a sense of how real knowledge bases are constructed and a feeling for the\ninteresting philosophical issues that arise. The major points are as follows:",
  "given the reader a sense of how real knowledge bases are constructed and a feeling for the\ninteresting philosophical issues that arise. The major points are as follows:\n• Large-scale knowledge representation requires a general-purpose ontology to organize\nand tie together the various speciﬁc domains of knowledge.\n• A general-purpose ontology needs to cover a wide variety of knowledge and should be\ncapable, in principle, of handling any domain.\n• Building a large, general-purpose ontology is a signiﬁcant challenge that has yet to be\nfully realized, although current frameworks seem to be quite robust.\n• We presented an upper ontology based on categories and the event calculus.\nWe\ncovered categories, subcategories, parts, structured objects, measurements, substances,\nevents, time and space, change, and beliefs. 468\nChapter\n12.\nKnowledge Representation\n• Natural kinds cannot be deﬁned completely in logic, but properties of natural kinds can\nbe represented.\n• Actions, events, and time can be represented either in situation calculus or in more\nexpressive representations such as event calculus. Such representations enable an agent\nto construct plans by logical inference.\n• We presented a detailed analysis of the Internet shopping domain, exercising the general\nontology and showing how the domain knowledge can be used by a shopping agent.\n• Special-purpose representation systems, such as semantic networks and description\nlogics, have been devised to help in organizing a hierarchy of categories. Inheritance\nis an important form of inference, allowing the properties of objects to be deduced from\ntheir membership in categories.\n• The closed-world assumption, as implemented in logic programs, provides a simple\nway to avoid having to specify lots of negative information. It is best interpreted as a\ndefault that can be overridden by additional information.\n• Nonmonotonic logics, such as circumscription and default logic, are intended to cap-\nture default reasoning in general.\n• Truth maintenance systems handle knowledge updates and revisions efﬁciently.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nBriggs (1985) claims that formal knowledge representation research began with classical In-\ndian theorizing about the grammar of Shastric Sanskrit, which dates back to the ﬁrst millen-\nnium B.C. In the West, the use of deﬁnitions of terms in ancient Greek mathematics can be\nregarded as the earliest instance: Aristotle’s Metaphysics (literally, what comes after the book",
  "nium B.C. In the West, the use of deﬁnitions of terms in ancient Greek mathematics can be\nregarded as the earliest instance: Aristotle’s Metaphysics (literally, what comes after the book\non physics) is a near-synonym for Ontology. Indeed, the development of technical terminol-\nogy in any ﬁeld can be regarded as a form of knowledge representation.\nEarly discussions of representation in AI tended to focus on “problem representation”\nrather than “knowledge representation.” (See, for example, Amarel’s (1968) discussion of the\nMissionaries and Cannibals problem.) In the 1970s, AI emphasized the development of “ex-\npert systems” (also called “knowledge-based systems”) that could, if given the appropriate\ndomain knowledge, match or exceed the performance of human experts on narrowly deﬁned\ntasks. For example, the ﬁrst expert system, DENDRAL (Feigenbaum et al., 1971; Lindsay\net al., 1980), interpreted the output of a mass spectrometer (a type of instrument used to ana-\nlyze the structure of organic chemical compounds) as accurately as expert chemists. Although\nthe success of DENDRAL was instrumental in convincing the AI research community of the\nimportance of knowledge representation, the representational formalisms used in DENDRAL\nare highly speciﬁc to the domain of chemistry. Over time, researchers became interested in\nstandardized knowledge representation formalisms and ontologies that could streamline the\nprocess of creating new expert systems. In so doing, they ventured into territory previously\nexplored by philosophers of science and of language. The discipline imposed in AI by the\nneed for one’s theories to “work” has led to more rapid and deeper progress than was the case Bibliographical and Historical Notes\n469\nwhen these problems were the exclusive domain of philosophy (although it has at times also\nled to the repeated reinvention of the wheel).\nThe creation of comprehensive taxonomies or classiﬁcations dates back to ancient times.\nAristotle (384–322 B.C.) strongly emphasized classiﬁcation and categorization schemes. His\nOrganon, a collection of works on logic assembled by his students after his death, included a\ntreatise called Categories in which he attempted to construct what we would now call an upper\nontology. He also introduced the notions of genus and species for lower-level classiﬁcation.\nOur present system of biological classiﬁcation, including the use of “binomial nomenclature”",
  "ontology. He also introduced the notions of genus and species for lower-level classiﬁcation.\nOur present system of biological classiﬁcation, including the use of “binomial nomenclature”\n(classiﬁcation via genus and species in the technical sense), was invented by the Swedish\nbiologist Carolus Linnaeus, or Carl von Linne (1707–1778). The problems associated with\nnatural kinds and inexact category boundaries have been addressed by Wittgenstein (1953),\nQuine (1953), Lakoff (1987), and Schwartz (1977), among others.\nInterest in larger-scale ontologies is increasing, as documented by the Handbook on\nOntologies (Staab, 2004). The OPENCYC project (Lenat and Guha, 1990; Matuszek et al.,\n2006) has released a 150,000-concept ontology, with an upper ontology similar to the one in\nFigure 12.1 as well as speciﬁc concepts like “OLED Display” and “iPhone,” which is a type\nof “cellular phone,” which in turn is a type of “consumer electronics,” “phone,” “wireless\ncommunication device,” and other concepts. The DBPEDIA project extracts structured data\nfrom Wikipedia; speciﬁcally from Infoboxes: the boxes of attribute/value pairs that accom-\npany many Wikipedia articles (Wu and Weld, 2008; Bizer et al., 2007). As of mid-2009,\nDBPEDIA contains 2.6 million concepts, with about 100 facts per concept. The IEEE work-\ning group P1600.1 created the Suggested Upper Merged Ontology (SUMO) (Niles and Pease,\n2001; Pease and Niles, 2002), which contains about 1000 terms in the upper ontology and\nlinks to over 20,000 domain-speciﬁc terms. Stoffel et al. (1997) describe algorithms for ef-\nﬁciently managing a very large ontology. A survey of techniques for extracting knowledge\nfrom Web pages is given by Etzioni et al. (2008).\nOn the Web, representation languages are emerging. RDF (Brickley and Guha, 2004)\nallows for assertions to be made in the form of relational triples, and provides some means\nfor evolving the meaning of names over time. OWL (Smith et al., 2004) is a description logic\nthat supports inferences over these triples. So far, usage seems to be inversely proportional to\nrepresentational complexity: the traditional HTML and CSS formats account for over 99% of\nWeb content, followed by the simplest representation schemes, such as microformats (Khare,\n2006) and RDFa (Adida and Birbeck, 2008), which use HTML and XHTML markup to\nadd attributes to literal text. Usage of sophisticated RDF and OWL ontologies is not yet",
  "2006) and RDFa (Adida and Birbeck, 2008), which use HTML and XHTML markup to\nadd attributes to literal text. Usage of sophisticated RDF and OWL ontologies is not yet\nwidespread, and the full vision of the Semantic Web (Berners-Lee et al., 2001) has not yet\nbeen realized. The conferences on Formal Ontology in Information Systems (FOIS) contain\nmany interesting papers on both general and domain-speciﬁc ontologies.\nThe taxonomy used in this chapter was developed by the authors and is based in part\non their experience in the CYC project and in part on work by Hwang and Schubert (1993)\nand Davis (1990, 2005). An inspirational discussion of the general project of commonsense\nknowledge representation appears in Hayes’s (1978, 1985b) “Naive Physics Manifesto.”\nSuccessful deep ontologies within a speciﬁc ﬁeld include the Gene Ontology project\n(Consortium, 2008) and CML, the Chemical Markup Language (Murray-Rust et al., 2003). 470\nChapter\n12.\nKnowledge Representation\nDoubts about the feasibility of a single ontology for all knowledge are expressed by\nDoctorow (2001), Gruber (2004), Halevy et al. (2009), and Smith (2004), who states, “the\ninitial project of building one single ontology . . . has . . . largely been abandoned.”\nThe event calculus was introduced by Kowalski and Sergot (1986) to handle continuous\ntime, and there have been several variations (Sadri and Kowalski, 1995; Shanahan, 1997) and\noverviews (Shanahan, 1999; Mueller, 2006). van Lambalgen and Hamm (2005) show how\nthe logic of events maps onto the language we use to talk about events. An alternative to the\nevent and situation calculi is the ﬂuent calculus (Thielscher, 1999). James Allen introduced\ntime intervals for the same reason (Allen, 1984), arguing that intervals were much more natu-\nral than situations for reasoning about extended and concurrent events. Peter Ladkin (1986a,\n1986b) introduced “concave” time intervals (intervals with gaps; essentially, unions of ordi-\nnary “convex” time intervals) and applied the techniques of mathematical abstract algebra to\ntime representation. Allen (1991) systematically investigates the wide variety of techniques\navailable for time representation; van Beek and Manchak (1996) analyze algorithms for tem-\nporal reasoning. There are signiﬁcant commonalities between the event-based ontology given\nin this chapter and an analysis of events due to the philosopher Donald Davidson (1980).",
  "poral reasoning. There are signiﬁcant commonalities between the event-based ontology given\nin this chapter and an analysis of events due to the philosopher Donald Davidson (1980).\nThe histories in Pat Hayes’s (1985a) ontology of liquids and the chronicles in McDermott’s\n(1985) theory of plans were also important inﬂuences on the ﬁeld and this chapter.\nThe question of the ontological status of substances has a long history. Plato proposed\nthat substances were abstract entities entirely distinct from physical objects; he would say\nMadeOf (Butter 3, Butter) rather than Butter3 ∈Butter. This leads to a substance hierar-\nchy in which, for example, UnsaltedButter is a more speciﬁc substance than Butter. The po-\nsition adopted in this chapter, in which substances are categories of objects, was championed\nby Richard Montague (1973). It has also been adopted in the CYC project. Copeland (1993)\nmounts a serious, but not invincible, attack. The alternative approach mentioned in the chap-\nter, in which butter is one object consisting of all buttery objects in the universe, was proposed\noriginally by the Polish logician Le´sniewski (1916). His mereology (the name is derived from\nMEREOLOGY\nthe Greek word for “part”) used the part–whole relation as a substitute for mathematical set\ntheory, with the aim of eliminating abstract entities such as sets. A more readable exposition\nof these ideas is given by Leonard and Goodman (1940), and Goodman’s The Structure of\nAppearance (1977) applies the ideas to various problems in knowledge representation. While\nsome aspects of the mereological approach are awkward—for example, the need for a sepa-\nrate inheritance mechanism based on part–whole relations—the approach gained the support\nof Quine (1960). Harry Bunt (1985) has provided an extensive analysis of its use in knowl-\nedge representation. Casati and Varzi (1999) cover parts, wholes, and the spatial locations.\nMental objects have been the subject of intensive study in philosophy and AI. There\nare three main approaches. The one taken in this chapter, based on modal logic and possible\nworlds, is the classical approach from philosophy (Hintikka, 1962; Kripke, 1963; Hughes\nand Cresswell, 1996). The book Reasoning about Knowledge (Fagin et al., 1995) provides a\nthorough introduction. The second approach is a ﬁrst-order theory in which mental objects\nare ﬂuents. Davis (2005) and Davis and Morgenstern (2005) describe this approach. It relies",
  "thorough introduction. The second approach is a ﬁrst-order theory in which mental objects\nare ﬂuents. Davis (2005) and Davis and Morgenstern (2005) describe this approach. It relies\non the possible-worlds formalism, and builds on work by Robert Moore (1980, 1985). The\nthird approach is a syntactic theory, in which mental objects are represented by character\nSYNTACTIC THEORY Bibliographical and Historical Notes\n471\nstrings. A string is just a complex term denoting a list of symbols, so CanFly(Clark) can\nbe represented by the list of symbols [C, a, n, F, l, y, (, C, l, a, r, k, )]. The syntactic theory\nof mental objects was ﬁrst studied in depth by Kaplan and Montague (1960), who showed\nthat it led to paradoxes if not handled carefully. Ernie Davis (1990) provides an excellent\ncomparison of the syntactic and modal theories of knowledge.\nThe Greek philosopher Porphyry (c. 234–305 A.D.), commenting on Aristotle’s Cat-\negories, drew what might qualify as the ﬁrst semantic network. Charles S. Peirce (1909)\ndeveloped existential graphs as the ﬁrst semantic network formalism using modern logic.\nRoss Quillian (1961), driven by an interest in human memory and language processing, ini-\ntiated work on semantic networks within AI. An inﬂuential paper by Marvin Minsky (1975)\npresented a version of semantic networks called frames; a frame was a representation of\nan object or category, with attributes and relations to other objects or categories. The ques-\ntion of semantics arose quite acutely with respect to Quillian’s semantic networks (and those\nof others who followed his approach), with their ubiquitous and very vague “IS-A links”\nWoods’s (1975) famous article “What’s In a Link?” drew the attention of AI researchers to the\nneed for precise semantics in knowledge representation formalisms. Brachman (1979) elab-\norated on this point and proposed solutions. Patrick Hayes’s (1979) “The Logic of Frames”\ncut even deeper, claiming that “Most of ‘frames’ is just a new syntax for parts of ﬁrst-order\nlogic.” Drew McDermott’s (1978b) “Tarskian Semantics, or, No Notation without Denota-\ntion!” argued that the model-theoretic approach to semantics used in ﬁrst-order logic should\nbe applied to all knowledge representation formalisms. This remains a controversial idea;\nnotably, McDermott himself has reversed his position in “A Critique of Pure Reason” (Mc-\nDermott, 1987). Selman and Levesque (1993) discuss the complexity of inheritance with",
  "notably, McDermott himself has reversed his position in “A Critique of Pure Reason” (Mc-\nDermott, 1987). Selman and Levesque (1993) discuss the complexity of inheritance with\nexceptions, showing that in most formulations it is NP-complete.\nThe development of description logics is the most recent stage in a long line of re-\nsearch aimed at ﬁnding useful subsets of ﬁrst-order logic for which inference is computa-\ntionally tractable. Hector Levesque and Ron Brachman (1987) showed that certain logical\nconstructs—notably, certain uses of disjunction and negation—were primarily responsible\nfor the intractability of logical inference. Building on the KL-ONE system (Schmolze and\nLipkis, 1983), several researchers developed systems that incorporate theoretical complex-\nity analysis, most notably KRYPTON (Brachman et al., 1983) and Classic (Borgida et al.,\n1989). The result has been a marked increase in the speed of inference and a much better\nunderstanding of the interaction between complexity and expressiveness in reasoning sys-\ntems. Calvanese et al. (1999) summarize the state of the art, and Baader et al. (2007) present\na comprehensive handbook of description logic. Against this trend, Doyle and Patil (1991)\nhave argued that restricting the expressiveness of a language either makes it impossible to\nsolve certain problems or encourages the user to circumvent the language restrictions through\nnonlogical means.\nThe three main formalisms for dealing with nonmonotonic inference—circumscription\n(McCarthy, 1980), default logic (Reiter, 1980), and modal nonmonotonic logic (McDermott\nand Doyle, 1980)—were all introduced in one special issue of the AI Journal. Delgrande and\nSchaub (2003) discuss the merits of the variants, given 25 years of hindsight. Answer set\nprogramming can be seen as an extension of negation as failure or as a reﬁnement of circum- 472\nChapter\n12.\nKnowledge Representation\nscription; the underlying theory of stable model semantics was introduced by Gelfond and\nLifschitz (1988), and the leading answer set programming systems are DLV (Eiter et al., 1998)\nand SMODELS (Niemel¨a et al., 2000). The disk drive example comes from the SMODELS user\nmanual (Syrj¨anen, 2000). Lifschitz (2001) discusses the use of answer set programming for\nplanning. Brewka et al. (1997) give a good overview of the various approaches to nonmono-\ntonic logic. Clark (1978) covers the negation-as-failure approach to logic programming and",
  "planning. Brewka et al. (1997) give a good overview of the various approaches to nonmono-\ntonic logic. Clark (1978) covers the negation-as-failure approach to logic programming and\nClark completion. Van Emden and Kowalski (1976) show that every Prolog program without\nnegation has a unique minimal model. Recent years have seen renewed interest in applica-\ntions of nonmonotonic logics to large-scale knowledge representation systems. The BENINQ\nsystems for handling insurance-beneﬁt inquiries was perhaps the ﬁrst commercially success-\nful application of a nonmonotonic inheritance system (Morgenstern, 1998). Lifschitz (2001)\ndiscusses the application of answer set programming to planning. A variety of nonmonotonic\nreasoning systems based on logic programming are documented in the proceedings of the\nconferences on Logic Programming and Nonmonotonic Reasoning (LPNMR).\nThe study of truth maintenance systems began with the TMS (Doyle, 1979) and RUP\n(McAllester, 1980) systems, both of which were essentially JTMSs. Forbus and de Kleer\n(1993) explain in depth how TMSs can be used in AI applications. Nayak and Williams\n(1997) show how an efﬁcient incremental TMS called an ITMS makes it feasible to plan the\noperations of a NASA spacecraft in real time.\nThis chapter could not cover every area of knowledge representation in depth. The three\nprincipal topics omitted are the following:\nQualitative physics: Qualitative physics is a subﬁeld of knowledge representation concerned\nQUALITATIVE\nPHYSICS\nspeciﬁcally with constructing a logical, nonnumeric theory of physical objects and processes.\nThe term was coined by Johan de Kleer (1975), although the enterprise could be said to\nhave started in Fahlman’s (1974) BUILD, a sophisticated planner for constructing complex\ntowers of blocks. Fahlman discovered in the process of designing it that most of the effort\n(80%, by his estimate) went into modeling the physics of the blocks world to calculate the\nstability of various subassemblies of blocks, rather than into planning per se. He sketches a\nhypothetical naive-physics-like process to explain why young children can solve BUILD-like\nproblems without access to the high-speed ﬂoating-point arithmetic used in BUILD’s physical\nmodeling. Hayes (1985a) uses “histories”—four-dimensional slices of space-time similar to\nDavidson’s events—to construct a fairly complex naive physics of liquids. Hayes was the",
  "modeling. Hayes (1985a) uses “histories”—four-dimensional slices of space-time similar to\nDavidson’s events—to construct a fairly complex naive physics of liquids. Hayes was the\nﬁrst to prove that a bath with the plug in will eventually overﬂow if the tap keeps running and\nthat a person who falls into a lake will get wet all over. Davis (2008) gives an update to the\nontology of liquids that describes the pouring of liquids into containers.\nDe Kleer and Brown (1985), Ken Forbus (1985), and Benjamin Kuipers (1985) inde-\npendently and almost simultaneously developed systems that can reason about a physical\nsystem based on qualitative abstractions of the underlying equations. Qualitative physics\nsoon developed to the point where it became possible to analyze an impressive variety of\ncomplex physical systems (Yip, 1991). Qualitative techniques have been used to construct\nnovel designs for clocks, windshield wipers, and six-legged walkers (Subramanian and Wang,\n1994). The collection Readings in Qualitative Reasoning about Physical Systems (Weld and Exercises\n473\nde Kleer, 1990) an encyclopedia article by Kuipers (2001), and a handbook article by Davis\n(2007) introduce to the ﬁeld.\nSpatial reasoning: The reasoning necessary to navigate in the wumpus world and shopping\nSPATIAL REASONING\nworld is trivial in comparison to the rich spatial structure of the real world. The earliest\nserious attempt to capture commonsense reasoning about space appears in the work of Ernest\nDavis (1986, 1990). The region connection calculus of Cohn et al. (1997) supports a form of\nqualitative spatial reasoning and has led to new kinds of geographical information systems;\nsee also (Davis, 2006). As with qualitative physics, an agent can go a long way, so to speak,\nwithout resorting to a full metric representation. When such a representation is necessary,\ntechniques developed in robotics (Chapter 25) can be used.\nPsychological reasoning: Psychological reasoning involves the development of a working\nPSYCHOLOGICAL\nREASONING\npsychology for artiﬁcial agents to use in reasoning about themselves and other agents. This\nis often based on so-called folk psychology, the theory that humans in general are believed\nto use in reasoning about themselves and other humans. When AI researchers provide their\nartiﬁcial agents with psychological theories for reasoning about other agents, the theories are\nfrequently based on the researchers’ description of the logical agents’ own design. Psycholog-",
  "artiﬁcial agents with psychological theories for reasoning about other agents, the theories are\nfrequently based on the researchers’ description of the logical agents’ own design. Psycholog-\nical reasoning is currently most useful within the context of natural language understanding,\nwhere divining the speaker’s intentions is of paramount importance.\nMinker (2001) collects papers by leading researchers in knowledge representation, sum-\nmarizing 40 years of work in the ﬁeld. The proceedings of the international conferences on\nPrinciples of Knowledge Representation and Reasoning provide the most up-to-date sources\nfor work in this area.\nReadings in Knowledge Representation (Brachman and Levesque,\n1985) and Formal Theories of the Commonsense World (Hobbs and Moore, 1985) are ex-\ncellent anthologies on knowledge representation; the former focuses more on historically\nimportant papers in representation languages and formalisms, the latter on the accumulation\nof the knowledge itself. Davis (1990), Steﬁk (1995), and Sowa (1999) provide textbook in-\ntroductions to knowledge representation, van Harmelen et al. (2007) contributes a handbook,\nand a special issue of AI Journal covers recent progress (Davis and Morgenstern, 2004). The\nbiennial conference on Theoretical Aspects of Reasoning About Knowledge (TARK) covers\napplications of the theory of knowledge in AI, economics, and distributed systems.\nEXERCISES\n12.1\nDeﬁne an ontology in ﬁrst-order logic for tic-tac-toe. The ontology should contain\nsituations, actions, squares, players, marks (X, O, or blank), and the notion of winning, losing,\nor drawing a game. Also deﬁne the notion of a forced win (or draw): a position from which\na player can force a win (or draw) with the right sequence of actions. Write axioms for the\ndomain. (Note: The axioms that enumerate the different squares and that characterize the\nwinning positions are rather long. You need not write these out in full, but indicate clearly\nwhat they look like.) 474\nChapter\n12.\nKnowledge Representation\n12.2\nFigure 12.1 shows the top levels of a hierarchy for everything. Extend it to include\nas many real categories as possible. A good way to do this is to cover all the things in your\neveryday life. This includes objects and events. Start with waking up, and proceed in an\norderly fashion noting everything that you see, touch, do, and think about. For example,\na random sampling produces music, news, milk, walking, driving, gas, Soda Hall, carpet,",
  "orderly fashion noting everything that you see, touch, do, and think about. For example,\na random sampling produces music, news, milk, walking, driving, gas, Soda Hall, carpet,\ntalking, Professor Fateman, chicken curry, tongue, $7, sun, the daily newspaper, and so on.\nYou should produce both a single hierarchy chart (on a large sheet of paper) and a\nlisting of objects and categories with the relations satisﬁed by members of each category.\nEvery object should be in a category, and every category should be in the hierarchy.\n12.3\nDevelop a representational system for reasoning about windows in a window-based\ncomputer interface. In particular, your representation should be able to describe:\n• The state of a window: minimized, displayed, or nonexistent.\n• Which window (if any) is the active window.\n• The position of every window at a given time.\n• The order (front to back) of overlapping windows.\n• The actions of creating, destroying, resizing, and moving windows; changing the state\nof a window; and bringing a window to the front. Treat these actions as atomic; that is,\ndo not deal with the issue of relating them to mouse actions. Give axioms describing\nthe effects of actions on ﬂuents. You may use either event or situation calculus.\nAssume an ontology containing situations, actions, integers (for x and y coordinates) and\nwindows. Deﬁne a language over this ontology; that is, a list of constants, function symbols,\nand predicates with an English description of each. If you need to add more categories to the\nontology (e.g., pixels), you may do so, but be sure to specify these in your write-up. You may\n(and should) use symbols deﬁned in the text, but be sure to list these explicitly.\n12.4\nState the following in the language you developed for the previous exercise:\na. In situation S0, window W1 is behind W2 but sticks out on the left and right. Do not\nstate exact coordinates for these; describe the general situation.\nb. If a window is displayed, then its top edge is higher than its bottom edge.\nc. After you create a window w, it is displayed.\nd. A window can be minimized if it is displayed.\n12.5\n(Adapted from an example by Doug Lenat.) Your mission is to capture, in logical\nform, enough knowledge to answer a series of questions about the following simple scenario:\nYesterday John went to the North Berkeley Safeway supermarket and bought two\npounds of tomatoes and a pound of ground beef.",
  "form, enough knowledge to answer a series of questions about the following simple scenario:\nYesterday John went to the North Berkeley Safeway supermarket and bought two\npounds of tomatoes and a pound of ground beef.\nStart by trying to represent the content of the sentence as a series of assertions. You should\nwrite sentences that have straightforward logical structure (e.g., statements that objects have\ncertain properties, that objects are related in certain ways, that all objects satisfying one prop-\nerty satisfy another). The following might help you get started: Exercises\n475\n• Which classes, objects, and relations would you need? What are their parents, siblings\nand so on? (You will need events and temporal ordering, among other things.)\n• Where would they ﬁt in a more general hierarchy?\n• What are the constraints and interrelationships among them?\n• How detailed must you be about each of the various concepts?\nTo answer the questions below, your knowledge base must include background knowledge.\nYou’ll have to deal with what kind of things are at a supermarket, what is involved with\npurchasing the things one selects, what the purchases will be used for, and so on. Try to make\nyour representation as general as possible. To give a trivial example: don’t say “People buy\nfood from Safeway,” because that won’t help you with those who shop at another supermarket.\nAlso, don’t turn the questions into answers; for example, question (c) asks “Did John buy any\nmeat?”—not “Did John buy a pound of ground beef?”\nSketch the chains of reasoning that would answer the questions. If possible, use a\nlogical reasoning system to demonstrate the sufﬁciency of your knowledge base. Many of the\nthings you write might be only approximately correct in reality, but don’t worry too much;\nthe idea is to extract the common sense that lets you answer these questions at all. A truly\ncomplete answer to this question is extremely difﬁcult, probably beyond the state of the art of\ncurrent knowledge representation. But you should be able to put together a consistent set of\naxioms for the limited questions posed here.\na. Is John a child or an adult? [Adult]\nb. Does John now have at least two tomatoes? [Yes]\nc. Did John buy any meat? [Yes]\nd. If Mary was buying tomatoes at the same time as John, did he see her? [Yes]\ne. Are the tomatoes made in the supermarket? [No]\nf. What is John going to do with the tomatoes? [Eat them]\ng. Does Safeway sell deodorant? [Yes]",
  "d. If Mary was buying tomatoes at the same time as John, did he see her? [Yes]\ne. Are the tomatoes made in the supermarket? [No]\nf. What is John going to do with the tomatoes? [Eat them]\ng. Does Safeway sell deodorant? [Yes]\nh. Did John bring some money or a credit card to the supermarket? [Yes]\ni. Does John have less money after going to the supermarket? [Yes]\n12.6\nMake the necessary additions or changes to your knowledge base from the previous\nexercise so that the questions that follow can be answered. Include in your report a discussion\nof your changes, explaining why they were needed, whether they were minor or major, and\nwhat kinds of questions would necessitate further changes.\na. Are there other people in Safeway while John is there? [Yes—staff!]\nb. Is John a vegetarian? [No]\nc. Who owns the deodorant in Safeway? [Safeway Corporation]\nd. Did John have an ounce of ground beef? [Yes]\ne. Does the Shell station next door have any gas? [Yes]\nf. Do the tomatoes ﬁt in John’s car trunk? [Yes] 476\nChapter\n12.\nKnowledge Representation\n12.7\nRepresent the following seven sentences using and extending the representations de-\nveloped in the chapter:\na. Water is a liquid between 0 and 100 degrees.\nb. Water boils at 100 degrees.\nc. The water in John’s water bottle is frozen.\nd. Perrier is a kind of water.\ne. John has Perrier in his water bottle.\nf. All liquids have a freezing point.\ng. A liter of water weighs more than a liter of alcohol.\n12.8\nWrite deﬁnitions for the following:\na. ExhaustivePartDecomposition\nb. PartPartition\nc. PartwiseDisjoint\nThese should be analogous to the deﬁnitions for ExhaustiveDecomposition, Partition, and\nDisjoint. Is it the case that PartPartition(s, BunchOf (s))? If so, prove it; if not, give a\ncounterexample and deﬁne sufﬁcient conditions under which it does hold.\n12.9\nAn alternative scheme for representing measures involves applying the units function\nto an abstract length object. In such a scheme, one would write Inches(Length(L1)) =\n1.5. How does this scheme compare with the one in the chapter? Issues include conversion\naxioms, names for abstract quantities (such as “50 dollars”), and comparisons of abstract\nmeasures in different units (50 inches is more than 50 centimeters).\n12.10\nAdd sentences to extend the deﬁnition of the predicate Name(s, c) so that a string\nsuch as “laptop computer” matches the appropriate category names from a variety of stores.",
  "measures in different units (50 inches is more than 50 centimeters).\n12.10\nAdd sentences to extend the deﬁnition of the predicate Name(s, c) so that a string\nsuch as “laptop computer” matches the appropriate category names from a variety of stores.\nTry to make your deﬁnition general. Test it by looking at ten online stores, and at the category\nnames they give for three different categories. For example, for the category of laptops, we\nfound the names “Notebooks,” “Laptops,” “Notebook Computers,” “Notebook,” “Laptops\nand Notebooks,” and “Notebook PCs.” Some of these can be covered by explicit Name facts,\nwhile others could be covered by sentences for handling plurals, conjunctions, etc.\n12.11\nWrite event calculus axioms to describe the actions in the wumpus world.\n12.12\nState the interval-algebra relation that holds between every pair of the following real-\nworld events:\nLK: The life of President Kennedy.\nIK: The infancy of President Kennedy.\nPK: The presidency of President Kennedy.\nLJ: The life of President Johnson.\nPJ: The presidency of President Johnson.\nLO: The life of President Obama. Exercises\n477\n12.13\nInvestigate ways to extend the event calculus to handle simultaneous events. Is it\npossible to avoid a combinatorial explosion of axioms?\n12.14\nConstruct a representation for exchange rates between currencies that allows for daily\nﬂuctuations.\n12.15\nDeﬁne the predicate Fixed, where Fixed(Location(x)) means that the location of\nobject x is ﬁxed over time.\n12.16\nDescribe the event of trading something for something else. Describe buying as a\nkind of trading in which one of the objects traded is a sum of money.\n12.17\nThe two preceding exercises assume a fairly primitive notion of ownership. For ex-\nample, the buyer starts by owning the dollar bills. This picture begins to break down when,\nfor example, one’s money is in the bank, because there is no longer any speciﬁc collection\nof dollar bills that one owns. The picture is complicated still further by borrowing, leasing,\nrenting, and bailment. Investigate the various commonsense and legal concepts of ownership,\nand propose a scheme by which they can be represented formally.\n12.18\n(Adapted from Fagin et al. (1995).) Consider a game played with a deck of just 8\ncards, 4 aces and 4 kings. The three players, Alice, Bob, and Carlos, are dealt two cards each.\nWithout looking at them, they place the cards on their foreheads so that the other players can",
  "cards, 4 aces and 4 kings. The three players, Alice, Bob, and Carlos, are dealt two cards each.\nWithout looking at them, they place the cards on their foreheads so that the other players can\nsee them. Then the players take turns either announcing that they know what cards are on\ntheir own forehead, thereby winning the game, or saying “I don’t know.” Everyone knows\nthe players are truthful and are perfect at reasoning about beliefs.\na. Game 1. Alice and Bob have both said “I don’t know.” Carlos sees that Alice has two\naces (A-A) and Bob has two kings (K-K). What should Carlos say? (Hint: consider all\nthree possible cases for Carlos: A-A, K-K, A-K.)\nb. Describe each step of Game 1 using the notation of modal logic.\nc. Game 2. Carlos, Alice, and Bob all said “I don’t know” on their ﬁrst turn. Alice holds\nK-K and Bob holds A-K. What should Carlos say on his second turn?\nd. Game 3. Alice, Carlos, and Bob all say “I don’t know” on their ﬁrst turn, as does Alice\non her second turn. Alice and Bob both hold A-K. What should Carlos say?\ne. Prove that there will always be a winner to this game.\n12.19\nThe assumption of logical omniscience, discussed on page 453, is of course not true\nof any actual reasoners. Rather, it is an idealization of the reasoning process that may be\nmore or less acceptable depending on the applications. Discuss the reasonableness of the\nassumption for each of the following applications of reasoning about knowledge:\na. Partial knowledge adversary games, such as card games. Here one player wants to\nreason about what his opponent knows about the state of the game.\nb. Chess with a clock. Here the player may wish to reason about the limits of his oppo-\nnent’s or his own ability to ﬁnd the best move in the time available. For instance, if\nplayer A has much more time left than player B, then A will sometimes make a move\nthat greatly complicates the situation, in the hopes of gaining an advantage because he\nhas more time to work out the proper strategy. 478\nChapter\n12.\nKnowledge Representation\nc. A shopping agent in an environment in which there are costs of gathering information.\nd. Reasoning about public key cryptography, which rests on the intractability of certain\ncomputational problems.\n12.20\nTranslate the following description logic expression (from page 457) into ﬁrst-order\nlogic, and comment on the result:\nAnd(Man, AtLeast(3, Son), AtMost(2, Daughter),\nAll(Son, And(Unemployed, Married, All(Spouse, Doctor))),",
  "12.20\nTranslate the following description logic expression (from page 457) into ﬁrst-order\nlogic, and comment on the result:\nAnd(Man, AtLeast(3, Son), AtMost(2, Daughter),\nAll(Son, And(Unemployed, Married, All(Spouse, Doctor))),\nAll(Daughter, And(Professor, Fills(Department, Physics, Math)))) .\n12.21\nRecall that inheritance information in semantic networks can be captured logically\nby suitable implication sentences. This exercise investigates the efﬁciency of using such\nsentences for inheritance.\na. Consider the information in a used-car catalog such as Kelly’s Blue Book—for exam-\nple, that 1973 Dodge vans are (or perhaps were once) worth $575. Suppose all this\ninformation (for 11,000 models) is encoded as logical sentences, as suggested in the\nchapter. Write down three such sentences, including that for 1973 Dodge vans. How\nwould you use the sentences to ﬁnd the value of a particular car, given a backward-\nchaining theorem prover such as Prolog?\nb. Compare the time efﬁciency of the backward-chaining method for solving this problem\nwith the inheritance method used in semantic nets.\nc. Explain how forward chaining allows a logic-based system to solve the same problem\nefﬁciently, assuming that the KB contains only the 11,000 sentences about prices.\nd. Describe a situation in which neither forward nor backward chaining on the sentences\nwill allow the price query for an individual car to be handled efﬁciently.\ne. Can you suggest a solution enabling this type of query to be solved efﬁciently in all\ncases in logic systems? (Hint: Remember that two cars of the same year and model\nhave the same price.)\n12.22\nOne might suppose that the syntactic distinction between unboxed links and singly\nboxed links in semantic networks is unnecessary, because singly boxed links are always at-\ntached to categories; an inheritance algorithm could simply assume that an unboxed link\nattached to a category is intended to apply to all members of that category. Show that this\nargument is fallacious, giving examples of errors that would arise.\n12.23\nOne part of the shopping process that was not covered in this chapter is checking\nfor compatibility between items. For example, if a digital camera is ordered, what accessory\nbatteries, memory cards, and cases are compatible with the camera? Write a knowledge base\nthat can determine the compatibility of a set of items and suggest replacements or additional",
  "batteries, memory cards, and cases are compatible with the camera? Write a knowledge base\nthat can determine the compatibility of a set of items and suggest replacements or additional\nitems if the shopper makes a choice that is not compatible. The knowledge base should works\nwith at least one line of products and extend easily to other lines.\n12.24\nA complete solution to the problem of inexact matches to the buyer’s description\nin shopping is very difﬁcult and requires a full array of natural language processing and Exercises\n479\ninformation retrieval techniques. (See Chapters 22 and 23.) One small step is to allow the\nuser to specify minimum and maximum values for various attributes. The buyer must use the\nfollowing grammar for product descriptions:\nDescription\n→\nCategory [Connector Modiﬁer]∗\nConnector\n→\n“with” | “and” | “,”\nModiﬁer\n→\nAttribute | Attribute Op Value\nOp\n→\n“=” | “>” | “<”\nHere, Category names a product category, Attribute is some feature such as “CPU” or\n“price,” and Value is the target value for the attribute. So the query “computer with at least a\n2.5 GHz CPU for under $500” must be re-expressed as “computer with CPU > 2.5 GHz and\nprice < $500.” Implement a shopping agent that accepts descriptions in this language.\n12.25\nOur description of Internet shopping omitted the all-important step of actually buying\nthe product. Provide a formal logical description of buying, using event calculus. That is,\ndeﬁne the sequence of events that occurs when a buyer submits a credit-card purchase and\nthen eventually gets billed and receives the product. 13\nQUANTIFYING\nUNCERTAINTY\nIn which we see how an agent can tame uncertainty with degrees of belief.\n13.1\nACTING UNDER UNCERTAINTY\nAgents may need to handle uncertainty, whether due to partial observability, nondetermin-\nUNCERTAINTY\nism, or a combination of the two. An agent may never know for certain what state it’s in or\nwhere it will end up after a sequence of actions.\nWe have seen problem-solving agents (Chapter 4) and logical agents (Chapters 7 and 11)\ndesigned to handle uncertainty by keeping track of a belief state—a representation of the set\nof all possible world states that it might be in—and generating a contingency plan that han-\ndles every possible eventuality that its sensors may report during execution. Despite its many\nvirtues, however, this approach has signiﬁcant drawbacks when taken literally as a recipe for\ncreating agent programs:",
  "dles every possible eventuality that its sensors may report during execution. Despite its many\nvirtues, however, this approach has signiﬁcant drawbacks when taken literally as a recipe for\ncreating agent programs:\n• When interpreting partial sensor information, a logical agent must consider every log-\nically possible explanation for the observations, no matter how unlikely. This leads to\nimpossible large and complex belief-state representations.\n• A correct contingent plan that handles every eventuality can grow arbitrarily large and\nmust consider arbitrarily unlikely contingencies.\n• Sometimes there is no plan that is guaranteed to achieve the goal—yet the agent must\nact. It must have some way to compare the merits of plans that are not guaranteed.\nSuppose, for example, that an automated taxi!automated has the goal of delivering a pas-\nsenger to the airport on time. The agent forms a plan, A90, that involves leaving home 90\nminutes before the ﬂight departs and driving at a reasonable speed. Even though the airport\nis only about 5 miles away, a logical taxi agent will not be able to conclude with certainty\nthat “Plan A90 will get us to the airport in time.” Instead, it reaches the weaker conclusion\n“Plan A90 will get us to the airport in time, as long as the car doesn’t break down or run out\nof gas, and I don’t get into an accident, and there are no accidents on the bridge, and the plane\ndoesn’t leave early, and no meteorite hits the car, and . . . .” None of these conditions can be\n480 Section 13.1.\nActing under Uncertainty\n481\ndeduced for sure, so the plan’s success cannot be inferred. This is the qualiﬁcation problem\n(page 268), for which we so far have seen no real solution.\nNonetheless, in some sense A90 is in fact the right thing to do. What do we mean by\nthis? As we discussed in Chapter 2, we mean that out of all the plans that could be executed,\nA90 is expected to maximize the agent’s performance measure (where the expectation is rel-\native to the agent’s knowledge about the environment). The performance measure includes\ngetting to the airport in time for the ﬂight, avoiding a long, unproductive wait at the airport,\nand avoiding speeding tickets along the way. The agent’s knowledge cannot guarantee any of\nthese outcomes for A90, but it can provide some degree of belief that they will be achieved.\nOther plans, such as A180, might increase the agent’s belief that it will get to the airport on",
  "these outcomes for A90, but it can provide some degree of belief that they will be achieved.\nOther plans, such as A180, might increase the agent’s belief that it will get to the airport on\ntime, but also increase the likelihood of a long wait. The right thing to do—the rational\ndecision—therefore depends on both the relative importance of various goals and the likeli-\nhood that, and degree to which, they will be achieved. The remainder of this section hones\nthese ideas, in preparation for the development of the general theories of uncertain reasoning\nand rational decisions that we present in this and subsequent chapters.\n13.1.1\nSummarizing uncertainty\nLet’s consider an example of uncertain reasoning: diagnosing a dental patient’s toothache.\nDiagnosis—whether for medicine, automobile repair, or whatever—almost always involves\nuncertainty. Let us try to write rules for dental diagnosis using propositional logic, so that we\ncan see how the logical approach breaks down. Consider the following simple rule:\nToothache ⇒Cavity .\nThe problem is that this rule is wrong. Not all patients with toothaches have cavities; some\nof them have gum disease, an abscess, or one of several other problems:\nToothache ⇒Cavity ∨GumProblem ∨Abscess . . .\nUnfortunately, in order to make the rule true, we have to add an almost unlimited list of\npossible problems. We could try turning the rule into a causal rule:\nCavity ⇒Toothache .\nBut this rule is not right either; not all cavities cause pain. The only way to ﬁx the rule\nis to make it logically exhaustive: to augment the left-hand side with all the qualiﬁcations\nrequired for a cavity to cause a toothache. Trying to use logic to cope with a domain like\nmedical diagnosis thus fails for three main reasons:\n• Laziness: It is too much work to list the complete set of antecedents or consequents\nLAZINESS\nneeded to ensure an exceptionless rule and too hard to use such rules.\n• Theoretical ignorance: Medical science has no complete theory for the domain.\nTHEORETICAL\nIGNORANCE\n• Practical ignorance: Even if we know all the rules, we might be uncertain about a\nPRACTICAL\nIGNORANCE\nparticular patient because not all the necessary tests have been or can be run.\nThe connection between toothaches and cavities is just not a logical consequence in either\ndirection. This is typical of the medical domain, as well as most other judgmental domains:\nlaw, business, design, automobile repair, gardening, dating, and so on. The agent’s knowledge 482\nChapter",
  "direction. This is typical of the medical domain, as well as most other judgmental domains:\nlaw, business, design, automobile repair, gardening, dating, and so on. The agent’s knowledge 482\nChapter\n13.\nQuantifying Uncertainty\ncan at best provide only a degree of belief in the relevant sentences. Our main tool for\nDEGREE OF BELIEF\ndealing with degrees of belief is probability theory. In the terminology of Section 8.1, the\nPROBABILITY\nTHEORY\nontological commitments of logic and probability theory are the same—that the world is\ncomposed of facts that do or do not hold in any particular case—but the epistemological\ncommitments are different: a logical agent believes each sentence to be true or false or has\nno opinion, whereas a probabilistic agent may have a numerical degree of belief between 0\n(for sentences that are certainly false) and 1 (certainly true).\nProbability provides a way of summarizing the uncertainty that comes from our lazi-\nness and ignorance, thereby solving the qualiﬁcation problem. We might not know for sure\nwhat afﬂicts a particular patient, but we believe that there is, say, an 80% chance—that is,\na probability of 0.8—that the patient who has a toothache has a cavity. That is, we expect\nthat out of all the situations that are indistinguishable from the current situation as far as our\nknowledge goes, the patient will have a cavity in 80% of them. This belief could be derived\nfrom statistical data—80% of the toothache patients seen so far have had cavities—or from\nsome general dental knowledge, or from a combination of evidence sources.\nOne confusing point is that at the time of our diagnosis, there is no uncertainty in the\nactual world: the patient either has a cavity or doesn’t. So what does it mean to say the\nprobability of a cavity is 0.8? Shouldn’t it be either 0 or 1? The answer is that probability\nstatements are made with respect to a knowledge state, not with respect to the real world. We\nsay “The probability that the patient has a cavity, given that she has a toothache, is 0.8.” If we\nlater learn that the patient has a history of gum disease, we can make a different statement:\n“The probability that the patient has a cavity, given that she has a toothache and a history of\ngum disease, is 0.4.” If we gather further conclusive evidence against a cavity, we can say\n“The probability that the patient has a cavity, given all we now know, is almost 0.” Note that",
  "gum disease, is 0.4.” If we gather further conclusive evidence against a cavity, we can say\n“The probability that the patient has a cavity, given all we now know, is almost 0.” Note that\nthese statements do not contradict each other; each is a separate assertion about a different\nknowledge state.\n13.1.2\nUncertainty and rational decisions\nConsider again the A90 plan for getting to the airport. Suppose it gives us a 97% chance\nof catching our ﬂight. Does this mean it is a rational choice? Not necessarily: there might\nbe other plans, such as A180, with higher probabilities. If it is vital not to miss the ﬂight,\nthen it is worth risking the longer wait at the airport. What about A1440, a plan that involves\nleaving home 24 hours in advance? In most circumstances, this is not a good choice, because\nalthough it almost guarantees getting there on time, it involves an intolerable wait—not to\nmention a possibly unpleasant diet of airport food.\nTo make such choices, an agent must ﬁrst have preferences between the different pos-\nPREFERENCE\nsible outcomes of the various plans. An outcome is a completely speciﬁed state, including\nOUTCOME\nsuch factors as whether the agent arrives on time and the length of the wait at the airport. We\nuse utility theory to represent and reason with preferences. (The term utility is used here in\nUTILITY THEORY\nthe sense of “the quality of being useful,” not in the sense of the electric company or water\nworks.) Utility theory says that every state has a degree of usefulness, or utility, to an agent\nand that the agent will prefer states with higher utility. Section 13.2.\nBasic Probability Notation\n483\nThe utility of a state is relative to an agent. For example, the utility of a state in which\nWhite has checkmated Black in a game of chess is obviously high for the agent playing White,\nbut low for the agent playing Black. But we can’t go strictly by the scores of 1, 1/2, and 0 that\nare dictated by the rules of tournament chess—some players (including the authors) might be\nthrilled with a draw against the world champion, whereas other players (including the former\nworld champion) might not. There is no accounting for taste or preferences: you might think\nthat an agent who prefers jalape˜no bubble-gum ice cream to chocolate chocolate chip is odd\nor even misguided, but you could not say the agent is irrational. A utility function can account\nfor any set of preferences—quirky or typical, noble or perverse. Note that utilities can account",
  "or even misguided, but you could not say the agent is irrational. A utility function can account\nfor any set of preferences—quirky or typical, noble or perverse. Note that utilities can account\nfor altruism, simply by including the welfare of others as one of the factors.\nPreferences, as expressed by utilities, are combined with probabilities in the general\ntheory of rational decisions called decision theory:\nDECISION THEORY\nDecision theory = probability theory + utility theory .\nThe fundamental idea of decision theory is that an agent is rational if and only if it chooses\nthe action that yields the highest expected utility, averaged over all the possible outcomes\nof the action. This is called the principle of maximum expected utility (MEU). Note that\nMAXIMUM EXPECTED\nUTILITY\n“expected” might seem like a vague, hypothetical term, but as it is used here it has a precise\nmeaning: it means the “average,” or “statistical mean” of the outcomes, weighted by the\nprobability of the outcome. We saw this principle in action in Chapter 5 when we touched\nbrieﬂy on optimal decisions in backgammon; it is in fact a completely general principle.\nFigure 13.1 sketches the structure of an agent that uses decision theory to select actions.\nThe agent is identical, at an abstract level, to the agents described in Chapters 4 and 7 that\nmaintain a belief state reﬂecting the history of percepts to date. The primary difference is\nthat the decision-theoretic agent’s belief state represents not just the possibilities for world\nstates but also their probabilities. Given the belief state, the agent can make probabilistic\npredictions of action outcomes and hence select the action with highest expected utility. This\nchapter and the next concentrate on the task of representing and computing with probabilistic\ninformation in general. Chapter 15 deals with methods for the speciﬁc tasks of representing\nand updating the belief state over time and predicting the environment. Chapter 16 covers\nutility theory in more depth, and Chapter 17 develops algorithms for planning sequences of\nactions in uncertain environments.\n13.2\nBASIC PROBABILITY NOTATION\nFor our agent to represent and use probabilistic information, we need a formal language.\nThe language of probability theory has traditionally been informal, written by human math-\nematicians to other human mathematicians. Appendix A includes a standard introduction to",
  "The language of probability theory has traditionally been informal, written by human math-\nematicians to other human mathematicians. Appendix A includes a standard introduction to\nelementary probability theory; here, we take an approach more suited to the needs of AI and\nmore consistent with the concepts of formal logic. 484\nChapter\n13.\nQuantifying Uncertainty\nfunction DT-AGENT(percept) returns an action\npersistent: belief state, probabilistic beliefs about the current state of the world\naction, the agent’s action\nupdate belief state based on action and percept\ncalculate outcome probabilities for actions,\ngiven action descriptions and current belief state\nselect action with highest expected utility\ngiven probabilities of outcomes and utility information\nreturn action\nFigure 13.1\nA decision-theoretic agent that selects rational actions.\n13.2.1\nWhat probabilities are about\nLike logical assertions, probabilistic assertions are about possible worlds. Whereas logical\nassertions say which possible worlds are strictly ruled out (all those in which the assertion is\nfalse), probabilistic assertions talk about how probable the various worlds are. In probability\ntheory, the set of all possible worlds is called the sample space. The possible worlds are\nSAMPLE SPACE\nmutually exclusive and exhaustive—two possible worlds cannot both be the case, and one\npossible world must be the case. For example, if we are about to roll two (distinguishable)\ndice, there are 36 possible worlds to consider: (1,1), (1,2), . . ., (6,6). The Greek letter Ω\n(uppercase omega) is used to refer to the sample space, and ω (lowercase omega) refers to\nelements of the space, that is, particular possible worlds.\nA fully speciﬁed probability model associates a numerical probability P(ω) with each\nPROBABILITY MODEL\npossible world.1 The basic axioms of probability theory say that every possible world has a\nprobability between 0 and 1 and that the total probability of the set of possible worlds is 1:\n0 ≤P(ω) ≤1 for every ω and\n\f\nω∈Ω\nP(ω) = 1 .\n(13.1)\nFor example, if we assume that each die is fair and the rolls don’t interfere with each other,\nthen each of the possible worlds (1,1), (1,2), . . ., (6,6) has probability 1/36. On the other\nhand, if the dice conspire to produce the same number, then the worlds (1,1), (2,2), (3,3), etc.,\nmight have higher probabilities, leaving the others with lower probabilities.\nProbabilistic assertions and queries are not usually about particular possible worlds, but",
  "might have higher probabilities, leaving the others with lower probabilities.\nProbabilistic assertions and queries are not usually about particular possible worlds, but\nabout sets of them. For example, we might be interested in the cases where the two dice add\nup to 11, the cases where doubles are rolled, and so on. In probability theory, these sets are\ncalled events—a term already used extensively in Chapter 12 for a different concept. In AI,\nEVENT\nthe sets are always described by propositions in a formal language. (One such language is\ndescribed in Section 13.2.2.) For each proposition, the corresponding set contains just those\npossible worlds in which the proposition holds. The probability associated with a proposition\n1 For now, we assume a discrete, countable set of worlds. The proper treatment of the continuous case brings in\ncertain complications that are less relevant for most purposes in AI. Section 13.2.\nBasic Probability Notation\n485\nis deﬁned to be the sum of the probabilities of the worlds in which it holds:\nFor any proposition φ, P(φ) =\n\f\nω∈φ\nP(ω) .\n(13.2)\nFor example, when rolling fair dice, we have P(Total = 11) = P((5, 6)) + P((6, 5)) =\n1/36 + 1/36 = 1/18. Note that probability theory does not require complete knowledge\nof the probabilities of each possible world. For example, if we believe the dice conspire to\nproduce the same number, we might assert that P(doubles) = 1/4 without knowing whether\nthe dice prefer double 6 to double 2. Just as with logical assertions, this assertion constrains\nthe underlying probability model without fully determining it.\nProbabilities such as P(Total = 11) and P(doubles) are called unconditional or prior\nUNCONDITIONAL\nPROBABILITY\nprobabilities (and sometimes just “priors” for short); they refer to degrees of belief in propo-\nPRIOR PROBABILITY\nsitions in the absence of any other information. Most of the time, however, we have some\ninformation, usually called evidence, that has already been revealed. For example, the ﬁrst\nEVIDENCE\ndie may already be showing a 5 and we are waiting with bated breath for the other one to\nstop spinning. In that case, we are interested not in the unconditional probability of rolling\ndoubles, but the conditional or posterior probability (or just “posterior” for short) of rolling\nCONDITIONAL\nPROBABILITY\nPOSTERIOR\nPROBABILITY\ndoubles given that the ﬁrst die is a 5. This probability is written P(doubles | Die1 = 5), where",
  "doubles, but the conditional or posterior probability (or just “posterior” for short) of rolling\nCONDITIONAL\nPROBABILITY\nPOSTERIOR\nPROBABILITY\ndoubles given that the ﬁrst die is a 5. This probability is written P(doubles | Die1 = 5), where\nthe “ | ” is pronounced “given.” Similarly, if I am going to the dentist for a regular checkup,\nthe probability P(cavity) = 0.2 might be of interest; but if I go to the dentist because I have\na toothache, it’s P(cavity | toothache) = 0.6 that matters. Note that the precedence of “ | ” is\nsuch that any expression of the form P(. . . | . . .) always means P((. . .)|(. . .)).\nIt is important to understand that P(cavity) = 0.2 is still valid after toothache is ob-\nserved; it just isn’t especially useful. When making decisions, an agent needs to condition\non all the evidence it has observed. It is also important to understand the difference be-\ntween conditioning and logical implication. The assertion that P(cavity | toothache) = 0.6\ndoes not mean “Whenever toothache is true, conclude that cavity is true with probabil-\nity 0.6” rather it means “Whenever toothache is true and we have no further information,\nconclude that cavity is true with probability 0.6.” The extra condition is important; for ex-\nample, if we had the further information that the dentist found no cavities, we deﬁnitely\nwould not want to conclude that cavity is true with probability 0.6; instead we need to use\nP(cavity|toothache ∧¬cavity) = 0.\nMathematically speaking, conditional probabilities are deﬁned in terms of uncondi-\ntional probabilities as follows: for any propositions a and b, we have\nP(a | b) = P(a ∧b)\nP(b)\n,\n(13.3)\nwhich holds whenever P(b) > 0. For example,\nP(doubles | Die1 = 5) = P(doubles ∧Die1 = 5)\nP(Die1 = 5)\n.\nThe deﬁnition makes sense if you remember that observing b rules out all those possible\nworlds where b is false, leaving a set whose total probability is just P(b). Within that set, the\na-worlds satisfy a ∧b and constitute a fraction P(a ∧b)/P(b). 486\nChapter\n13.\nQuantifying Uncertainty\nThe deﬁnition of conditional probability, Equation (13.3), can be written in a different\nform called the product rule:\nPRODUCT RULE\nP(a ∧b) = P(a | b)P(b) ,\nThe product rule is perhaps easier to remember: it comes from the fact that, for a and b to be\ntrue, we need b to be true, and we also need a to be true given b.\n13.2.2\nThe language of propositions in probability assertions",
  "P(a ∧b) = P(a | b)P(b) ,\nThe product rule is perhaps easier to remember: it comes from the fact that, for a and b to be\ntrue, we need b to be true, and we also need a to be true given b.\n13.2.2\nThe language of propositions in probability assertions\nIn this chapter and the next, propositions describing sets of possible worlds are written in a\nnotation that combines elements of propositional logic and constraint satisfaction notation. In\nthe terminology of Section 2.4.7, it is a factored representation, in which a possible world\nis represented by a set of variable/value pairs.\nVariables in probability theory are called random variables and their names begin with\nRANDOM VARIABLE\nan uppercase letter. Thus, in the dice example, Total and Die1 are random variables. Every\nrandom variable has a domain—the set of possible values it can take on. The domain of\nDOMAIN\nTotal for two dice is the set {2, . . . , 12} and the domain of Die1 is {1, . . . , 6}. A Boolean\nrandom variable has the domain {true, false} (notice that values are always lowercase); for\nexample, the proposition that doubles are rolled can be written as Doubles = true. By con-\nvention, propositions of the form A = true are abbreviated simply as a, while A = false is\nabbreviated as ¬a. (The uses of doubles, cavity, and toothache in the preceding section are\nabbreviations of this kind.) As in CSPs, domains can be sets of arbitrary tokens; we might\nchoose the domain of Age to be {juvenile, teen, adult} and the domain of Weather might\nbe {sunny, rain, cloudy, snow}. When no ambiguity is possible, it is common to use a value\nby itself to stand for the proposition that a particular variable has that value; thus, sunny can\nstand for Weather = sunny.\nThe preceding examples all have ﬁnite domains. Variables can have inﬁnite domains,\ntoo—either discrete (like the integers) or continuous (like the reals). For any variable with an\nordered domain, inequalities are also allowed, such as NumberOfAtomsInUniverse ≥1070.\nFinally, we can combine these sorts of elementary propositions (including the abbre-\nviated forms for Boolean variables) by using the connectives of propositional logic. For\nexample, we can express “The probability that the patient has a cavity, given that she is a\nteenager with no toothache, is 0.1” as follows:\nP(cavity | ¬toothache ∧teen) = 0.1 .\nSometimes we will want to talk about the probabilities of all the possible values of a random\nvariable. We could write:\nP(Weather = sunny) = 0.6",
  "teenager with no toothache, is 0.1” as follows:\nP(cavity | ¬toothache ∧teen) = 0.1 .\nSometimes we will want to talk about the probabilities of all the possible values of a random\nvariable. We could write:\nP(Weather = sunny) = 0.6\nP(Weather = rain) = 0.1\nP(Weather = cloudy) = 0.29\nP(Weather = snow) = 0.01 ,\nbut as an abbreviation we will allow\nP(Weather) = ⟨0.6, 0.1, 0.29, 0.01⟩, Section 13.2.\nBasic Probability Notation\n487\nwhere the bold P indicates that the result is a vector of numbers, and where we assume a pre-\ndeﬁned ordering ⟨sunny, rain, cloudy, snow⟩on the domain of Weather. We say that the\nP statement deﬁnes a probability distribution for the random variable Weather. The P nota-\nPROBABILITY\nDISTRIBUTION\ntion is also used for conditional distributions: P(X | Y ) gives the values of P(X = xi | Y = yj)\nfor each possible i, j pair.\nFor continuous variables, it is not possible to write out the entire distribution as a vector,\nbecause there are inﬁnitely many values. Instead, we can deﬁne the probability that a random\nvariable takes on some value x as a parameterized function of x. For example, the sentence\nP(NoonTemp = x) = Uniform[18C,26C](x)\nexpresses the belief that the temperature at noon is distributed uniformly between 18 and 26\ndegrees Celsius. We call this a probability density function.\nPROBABILITY\nDENSITY FUNCTION\nProbability density functions (sometimes called pdfs) differ in meaning from discrete\ndistributions. Saying that the probability density is uniform from 18C to 26C means that\nthere is a 100% chance that the temperature will fall somewhere in that 8C-wide region\nand a 50% chance that it will fall in any 4C-wide region, and so on. We write the probability\ndensity for a continuous random variable X at value x as P(X = x) or just P(x); the intuitive\ndeﬁnition of P(x) is the probability that X falls within an arbitrarily small region beginning\nat x, divided by the width of the region:\nP(x) = lim\ndx→0 P(x ≤X ≤x + dx)/dx .\nFor NoonTemp we have\nP(NoonTemp = x) = Uniform[18C,26C](x) =\n\u0018\n1\n8C if 18C ≤x ≤26C\n0 otherwise\n,\nwhere C stands for centigrade (not for a constant). In P(NoonTemp = 20.18C) =\n1\n8C , note\nthat\n1\n8C is not a probability, it is a probability density. The probability that NoonTemp is\nexactly 20.18C is zero, because 20.18C is a region of width 0. Some authors use different\nsymbols for discrete distributions and density functions; we use P in both cases, since confu-",
  "exactly 20.18C is zero, because 20.18C is a region of width 0. Some authors use different\nsymbols for discrete distributions and density functions; we use P in both cases, since confu-\nsion seldom arises and the equations are usually identical. Note that probabilities are unitless\nnumbers, whereas density functions are measured with a unit, in this case reciprocal degrees.\nIn addition to distributions on single variables, we need notation for distributions on\nmultiple variables. Commas are used for this. For example, P(Weather, Cavity) denotes\nthe probabilities of all combinations of the values of Weather and Cavity. This is a 4 × 2\ntable of probabilities called the joint probability distribution of Weather and Cavity. We\nJOINT PROBABILITY\nDISTRIBUTION\ncan also mix variables with and without values; P(sunny, Cavity) would be a two-element\nvector giving the probabilities of a sunny day with a cavity and a sunny day with no cavity.\nThe P notation makes certain expressions much more concise than they might otherwise be.\nFor example, the product rules for all possible values of Weather and Cavity can be written\nas a single equation:\nP(Weather, Cavity) = P(Weather | Cavity)P(Cavity) , 488\nChapter\n13.\nQuantifying Uncertainty\ninstead of as these 4 × 2 = 8 equations (using abbreviations W and C):\nP(W = sunny ∧C = true) = P(W = sunny|C = true) P(C = true)\nP(W = rain ∧C = true) = P(W = rain|C = true) P(C = true)\nP(W = cloudy ∧C = true) = P(W = cloudy|C = true) P(C = true)\nP(W = snow ∧C = true) = P(W = snow|C = true) P(C = true)\nP(W = sunny ∧C = false) = P(W = sunny|C = false) P(C = false)\nP(W = rain ∧C = false) = P(W = rain|C = false) P(C = false)\nP(W = cloudy ∧C = false) = P(W = cloudy|C = false) P(C = false)\nP(W = snow ∧C = false) = P(W = snow|C = false) P(C = false) .\nAs a degenerate case, P(sunny, cavity) has no variables and thus is a one-element vec-\ntor that is the probability of a sunny day with a cavity, which could also be written as\nP(sunny, cavity) or P(sunny ∧cavity). We will sometimes use P notation to derive results\nabout individual P values, and when we say “P(sunny) = 0.6” it is really an abbreviation for\n“P(sunny) is the one-element vector ⟨0.6⟩, which means that P(sunny) = 0.6.”\nNow we have deﬁned a syntax for propositions and probability assertions and we have\ngiven part of the semantics: Equation (13.2) deﬁnes the probability of a proposition as the sum\nof the probabilities of worlds in which it holds. To complete the semantics, we need to say",
  "given part of the semantics: Equation (13.2) deﬁnes the probability of a proposition as the sum\nof the probabilities of worlds in which it holds. To complete the semantics, we need to say\nwhat the worlds are and how to determine whether a proposition holds in a world. We borrow\nthis part directly from the semantics of propositional logic, as follows. A possible world is\ndeﬁned to be an assignment of values to all of the random variables under consideration. It is\neasy to see that this deﬁnition satisﬁes the basic requirement that possible worlds be mutually\nexclusive and exhaustive (Exercise 13.5). For example, if the random variables are Cavity,\nToothache, and Weather, then there are 2 × 2 × 4 = 16 possible worlds. Furthermore, the\ntruth of any given proposition, no matter how complex, can be determined easily in such\nworlds using the same recursive deﬁnition of truth as for formulas in propositional logic.\nFrom the preceding deﬁnition of possible worlds, it follows that a probability model is\ncompletely determined by the joint distribution for all of the random variables—the so-called\nfull joint probability distribution. For example, if the variables are Cavity, Toothache,\nFULL JOINT\nPROBABILITY\nDISTRIBUTION\nand Weather, then the full joint distribution is given by P(Cavity, Toothache, Weather).\nThis joint distribution can be represented as a 2 × 2 × 4 table with 16 entries. Because every\nproposition’s probability is a sum over possible worlds, a full joint distribution sufﬁces, in\nprinciple, for calculating the probability of any proposition.\n13.2.3\nProbability axioms and their reasonableness\nThe basic axioms of probability (Equations (13.1) and (13.2)) imply certain relationships\namong the degrees of belief that can be accorded to logically related propositions. For exam-\nple, we can derive the familiar relationship between the probability of a proposition and the\nprobability of its negation:\nP(¬a) = \u0002\nω∈¬a P(ω)\nby Equation (13.2)\n= \u0002\nω∈¬a P(ω) + \u0002\nω∈a P(ω) −\u0002\nω∈a P(ω)\n= \u0002\nω∈Ω P(ω) −\u0002\nω∈a P(ω)\ngrouping the ﬁrst two terms\n= 1 −P(a)\nby (13.1) and (13.2). Section 13.2.\nBasic Probability Notation\n489\nWe can also derive the well-known formula for the probability of a disjunction, sometimes\ncalled the inclusion–exclusion principle:\nINCLUSION–\nEXCLUSION\nPRINCIPLE\nP(a ∨b) = P(a) + P(b) −P(a ∧b) .\n(13.4)\nThis rule is easily remembered by noting that the cases where a holds, together with the cases",
  "called the inclusion–exclusion principle:\nINCLUSION–\nEXCLUSION\nPRINCIPLE\nP(a ∨b) = P(a) + P(b) −P(a ∧b) .\n(13.4)\nThis rule is easily remembered by noting that the cases where a holds, together with the cases\nwhere b holds, certainly cover all the cases where a ∨b holds; but summing the two sets of\ncases counts their intersection twice, so we need to subtract P(a ∧b). The proof is left as an\nexercise (Exercise 13.6).\nEquations (13.1) and (13.4) are often called Kolmogorov’s axioms in honor of the Rus-\nKOLMOGOROV’S\nAXIOMS\nsian mathematician Andrei Kolmogorov, who showed how to build up the rest of probability\ntheory from this simple foundation and how to handle the difﬁculties caused by continuous\nvariables.2 While Equation (13.2) has a deﬁnitional ﬂavor, Equation (13.4) reveals that the\naxioms really do constrain the degrees of belief an agent can have concerning logically re-\nlated propositions. This is analogous to the fact that a logical agent cannot simultaneously\nbelieve A, B, and ¬(A ∧B), because there is no possible world in which all three are true.\nWith probabilities, however, statements refer not to the world directly, but to the agent’s own\nstate of knowledge. Why, then, can an agent not hold the following set of beliefs (even though\nthey violate Kolmogorov’s axioms)?\nP(a) = 0.4\nP(a ∧b) = 0.0\nP(b) = 0.3\nP(a ∨b) = 0.8 .\n(13.5)\nThis kind of question has been the subject of decades of intense debate between those who\nadvocate the use of probabilities as the only legitimate form for degrees of belief and those\nwho advocate alternative approaches.\nOne argument for the axioms of probability, ﬁrst stated in 1931 by Bruno de Finetti\n(and translated into English in de Finetti (1993)), is as follows: If an agent has some degree of\nbelief in a proposition a, then the agent should be able to state odds at which it is indifferent\nto a bet for or against a.3 Think of it as a game between two agents: Agent 1 states, “my\ndegree of belief in event a is 0.4.” Agent 2 is then free to choose whether to wager for or\nagainst a at stakes that are consistent with the stated degree of belief. That is, Agent 2 could\nchoose to accept Agent 1’s bet that a will occur, offering $6 against Agent 1’s $4. Or Agent\n2 could accept Agent 1’s bet that ¬a will occur, offering $4 against Agent 1’s $6. Then we\nobserve the outcome of a, and whoever is right collects the money. If an agent’s degrees of",
  "2 could accept Agent 1’s bet that ¬a will occur, offering $4 against Agent 1’s $6. Then we\nobserve the outcome of a, and whoever is right collects the money. If an agent’s degrees of\nbelief do not accurately reﬂect the world, then you would expect that it would tend to lose\nmoney over the long run to an opposing agent whose beliefs more accurately reﬂect the state\nof the world.\nBut de Finetti proved something much stronger: If Agent 1 expresses a set of degrees\nof belief that violate the axioms of probability theory then there is a combination of bets by\nAgent 2 that guarantees that Agent 1 will lose money every time. For example, suppose that\nAgent 1 has the set of degrees of belief from Equation (13.5). Figure 13.2 shows that if Agent\n2 The difﬁculties include the Vitali set, a well-deﬁned subset of the interval [0, 1] with no well-deﬁned size.\n3 One might argue that the agent’s preferences for different bank balances are such that the possibility of losing\n$1 is not counterbalanced by an equal possibility of winning $1. One possible response is to make the bet amounts\nsmall enough to avoid this problem. Savage’s analysis (1954) circumvents the issue altogether. 490\nChapter\n13.\nQuantifying Uncertainty\n2 chooses to bet $4 on a, $3 on b, and $2 on ¬(a ∨b), then Agent 1 always loses money,\nregardless of the outcomes for a and b. De Finetti’s theorem implies that no rational agent\ncan have beliefs that violate the axioms of probability.\nAgent 1\nAgent 2\nOutcomes and payoffs to Agent 1\nProposition\nBelief\nBet\nStakes\na, b\na, ¬b\n¬a, b\n¬a, ¬b\na\n0.4\na\n4 to 6\n–6\n–6\n4\n4\nb\n0.3\nb\n3 to 7\n–7\n3\n–7\n3\na ∨b\n0.8\n¬(a ∨b)\n2 to 8\n2\n2\n2\n–8\n–11\n–1\n–1\n–1\nFigure 13.2\nBecause Agent 1 has inconsistent beliefs, Agent 2 is able to devise a set of\nbets that guarantees a loss for Agent 1, no matter what the outcome of a and b.\nOne common objection to de Finetti’s theorem is that this betting game is rather con-\ntrived. For example, what if one refuses to bet? Does that end the argument? The answer is\nthat the betting game is an abstract model for the decision-making situation in which every\nagent is unavoidably involved at every moment. Every action (including inaction) is a kind\nof bet, and every outcome can be seen as a payoff of the bet. Refusing to bet is like refusing\nto allow time to pass.\nOther strong philosophical arguments have been put forward for the use of probabilities,\nmost notably those of Cox (1946), Carnap (1950), and Jaynes (2003). They each construct a",
  "to allow time to pass.\nOther strong philosophical arguments have been put forward for the use of probabilities,\nmost notably those of Cox (1946), Carnap (1950), and Jaynes (2003). They each construct a\nset of axioms for reasoning with degrees of beliefs: no contradictions, correspondence with\nordinary logic (for example, if belief in A goes up, then belief in ¬A must go down), and so\non. The only controversial axiom is that degrees of belief must be numbers, or at least act\nlike numbers in that they must be transitive (if belief in A is greater than belief in B, which is\ngreater than belief in C, then belief in A must be greater than C) and comparable (the belief\nin A must be one of equal to, greater than, or less than belief in B). It can then be proved that\nprobability is the only approach that satisﬁes these axioms.\nThe world being the way it is, however, practical demonstrations sometimes speak\nlouder than proofs. The success of reasoning systems based on probability theory has been\nmuch more effective in making converts. We now look at how the axioms can be deployed to\nmake inferences.\n13.3\nINFERENCE USING FULL JOINT DISTRIBUTIONS\nIn this section we describe a simple method for probabilistic inference—that is, the compu-\nPROBABILISTIC\nINFERENCE\ntation of posterior probabilities for query propositions given observed evidence. We use the\nfull joint distribution as the “knowledge base” from which answers to all questions may be de-\nrived. Along the way we also introduce several useful techniques for manipulating equations\ninvolving probabilities. Section 13.3.\nInference Using Full Joint Distributions\n491\nWHERE DO PROBABILITIES COME FROM?\nThere has been endless debate over the source and status of probability numbers.\nThe frequentist position is that the numbers can come only from experiments: if\nwe test 100 people and ﬁnd that 10 of them have a cavity, then we can say that\nthe probability of a cavity is approximately 0.1. In this view, the assertion “the\nprobability of a cavity is 0.1” means that 0.1 is the fraction that would be observed\nin the limit of inﬁnitely many samples. From any ﬁnite sample, we can estimate\nthe true fraction and also calculate how accurate our estimate is likely to be.\nThe objectivist view is that probabilities are real aspects of the universe—\npropensities of objects to behave in certain ways—rather than being just descrip-\ntions of an observer’s degree of belief. For example, the fact that a fair coin comes",
  "The objectivist view is that probabilities are real aspects of the universe—\npropensities of objects to behave in certain ways—rather than being just descrip-\ntions of an observer’s degree of belief. For example, the fact that a fair coin comes\nup heads with probability 0.5 is a propensity of the coin itself. In this view, fre-\nquentist measurements are attempts to observe these propensities. Most physicists\nagree that quantum phenomena are objectively probabilistic, but uncertainty at the\nmacroscopic scale—e.g., in coin tossing—usually arises from ignorance of initial\nconditions and does not seem consistent with the propensity view.\nThe subjectivist view describes probabilities as a way of characterizing an\nagent’s beliefs, rather than as having any external physical signiﬁcance. The sub-\njective Bayesian view allows any self-consistent ascription of prior probabilities to\npropositions, but then insists on proper Bayesian updating as evidence arrives.\nIn the end, even a strict frequentist position involves subjective analysis be-\ncause of the reference class problem: in trying to determine the outcome probabil-\nity of a particular experiment, the frequentist has to place it in a reference class of\n“similar” experiments with known outcome frequencies. I. J. Good (1983, p. 27)\nwrote, “every event in life is unique, and every real-life probability that we esti-\nmate in practice is that of an event that has never occurred before.” For example,\ngiven a particular patient, a frequentist who wants to estimate the probability of a\ncavity will consider a reference class of other patients who are similar in important\nways—age, symptoms, diet—and see what proportion of them had a cavity. If the\ndentist considers everything that is known about the patient—weight to the nearest\ngram, hair color, mother’s maiden name—then the reference class becomes empty.\nThis has been a vexing problem in the philosophy of science.\nThe principle of indifference attributed to Laplace (1816) states that propo-\nsitions that are syntactically “symmetric” with respect to the evidence should be\naccorded equal probability. Various reﬁnements have been proposed, culminating\nin the attempt by Carnap and others to develop a rigorous inductive logic, capa-\nble of computing the correct probability for any proposition from any collection of\nobservations. Currently, it is believed that no unique inductive logic exists; rather,",
  "ble of computing the correct probability for any proposition from any collection of\nobservations. Currently, it is believed that no unique inductive logic exists; rather,\nany such logic rests on a subjective prior probability distribution whose effect is\ndiminished as more observations are collected. 492\nChapter\n13.\nQuantifying Uncertainty\ntoothache\n¬toothache\ncatch\n¬catch\ncatch\n¬catch\ncavity\n0.108\n0.012\n0.072\n0.008\n¬cavity\n0.016\n0.064\n0.144\n0.576\nFigure 13.3\nA full joint distribution for the Toothache, Cavity, Catch world.\nWe begin with a simple example: a domain consisting of just the three Boolean variables\nToothache, Cavity, and Catch (the dentist’s nasty steel probe catches in my tooth). The full\njoint distribution is a 2 × 2 × 2 table as shown in Figure 13.3.\nNotice that the probabilities in the joint distribution sum to 1, as required by the axioms\nof probability. Notice also that Equation (13.2) gives us a direct way to calculate the probabil-\nity of any proposition, simple or complex: simply identify those possible worlds in which the\nproposition is true and add up their probabilities. For example, there are six possible worlds\nin which cavity ∨toothache holds:\nP(cavity ∨toothache) = 0.108 + 0.012 + 0.072 + 0.008 + 0.016 + 0.064 = 0.28 .\nOne particularly common task is to extract the distribution over some subset of variables or\na single variable. For example, adding the entries in the ﬁrst row gives the unconditional or\nmarginal probability4 of cavity:\nMARGINAL\nPROBABILITY\nP(cavity) = 0.108 + 0.012 + 0.072 + 0.008 = 0.2 .\nThis process is called marginalization, or summing out—because we sum up the probabil-\nMARGINALIZATION\nities for each possible value of the other variables, thereby taking them out of the equation.\nWe can write the following general marginalization rule for any sets of variables Y and Z:\nP(Y) =\n\f\nz∈Z\nP(Y, z) ,\n(13.6)\nwhere \u0002\nz∈Z means to sum over all the possible combinations of values of the set of variables\nZ. We sometimes abbreviate this as \u0002\nz, leaving Z implicit. We just used the rule as\nP(Cavity) =\n\f\nz∈{Catch,Toothache}\nP(Cavity, z) .\n(13.7)\nA variant of this rule involves conditional probabilities instead of joint probabilities, using\nthe product rule:\nP(Y) =\n\f\nz\nP(Y | z)P(z) .\n(13.8)\nThis rule is called conditioning. Marginalization and conditioning turn out to be useful rules\nCONDITIONING\nfor all kinds of derivations involving probability expressions.",
  "the product rule:\nP(Y) =\n\f\nz\nP(Y | z)P(z) .\n(13.8)\nThis rule is called conditioning. Marginalization and conditioning turn out to be useful rules\nCONDITIONING\nfor all kinds of derivations involving probability expressions.\nIn most cases, we are interested in computing conditional probabilities of some vari-\nables, given evidence about others. Conditional probabilities can be found by ﬁrst using\n4 So called because of a common practice among actuaries of writing the sums of observed frequencies in the\nmargins of insurance tables. Section 13.3.\nInference Using Full Joint Distributions\n493\nEquation (13.3) to obtain an expression in terms of unconditional probabilities and then eval-\nuating the expression from the full joint distribution.\nFor example, we can compute the\nprobability of a cavity, given evidence of a toothache, as follows:\nP(cavity | toothache) = P(cavity ∧toothache)\nP(toothache)\n=\n0.108 + 0.012\n0.108 + 0.012 + 0.016 + 0.064 = 0.6 .\nJust to check, we can also compute the probability that there is no cavity, given a toothache:\nP(¬cavity | toothache) = P(¬cavity ∧toothache)\nP(toothache)\n=\n0.016 + 0.064\n0.108 + 0.012 + 0.016 + 0.064 = 0.4 .\nThe two values sum to 1.0, as they should. Notice that in these two calculations the term\n1/P(toothache) remains constant, no matter which value of Cavity we calculate. In fact,\nit can be viewed as a normalization constant for the distribution P(Cavity | toothache),\nNORMALIZATION\nensuring that it adds up to 1. Throughout the chapters dealing with probability, we use α to\ndenote such constants. With this notation, we can write the two preceding equations in one:\nP(Cavity | toothache) = α P(Cavity, toothache)\n= α [P(Cavity, toothache, catch) + P(Cavity, toothache, ¬catch)]\n= α [⟨0.108, 0.016⟩+ ⟨0.012, 0.064⟩] = α ⟨0.12, 0.08⟩= ⟨0.6, 0.4⟩.\nIn other words, we can calculate P(Cavity | toothache) even if we don’t know the value of\nP(toothache)! We temporarily forget about the factor 1/P(toothache) and add up the values\nfor cavity and ¬cavity, getting 0.12 and 0.08. Those are the correct relative proportions, but\nthey don’t sum to 1, so we normalize them by dividing each one by 0.12 + 0.08, getting\nthe true probabilities of 0.6 and 0.4. Normalization turns out to be a useful shortcut in many\nprobability calculations, both to make the computation easier and to allow us to proceed when\nsome probability assessment (such as P(toothache)) is not available.",
  "probability calculations, both to make the computation easier and to allow us to proceed when\nsome probability assessment (such as P(toothache)) is not available.\nFrom the example, we can extract a general inference procedure. We begin with the\ncase in which the query involves a single variable, X (Cavity in the example). Let E be the\nlist of evidence variables (just Toothache in the example), let e be the list of observed values\nfor them, and let Y be the remaining unobserved variables (just Catch in the example). The\nquery is P(X | e) and can be evaluated as\nP(X | e) = α P(X, e) = α\n\f\ny\nP(X, e, y) ,\n(13.9)\nwhere the summation is over all possible ys (i.e., all possible combinations of values of the\nunobserved variables Y). Notice that together the variables X, E, and Y constitute the com-\nplete set of variables for the domain, so P(X, e, y) is simply a subset of probabilities from the\nfull joint distribution.\nGiven the full joint distribution to work with, Equation (13.9) can answer probabilistic\nqueries for discrete variables. It does not scale well, however: for a domain described by n\nBoolean variables, it requires an input table of size O(2n) and takes O(2n) time to process the 494\nChapter\n13.\nQuantifying Uncertainty\ntable. In a realistic problem we could easily have n > 100, making O(2n) impractical. The\nfull joint distribution in tabular form is just not a practical tool for building reasoning systems.\nInstead, it should be viewed as the theoretical foundation on which more effective approaches\nmay be built, just as truth tables formed a theoretical foundation for more practical algorithms\nlike DPLL. The remainder of this chapter introduces some of the basic ideas required in\npreparation for the development of realistic systems in Chapter 14.\n13.4\nINDEPENDENCE\nLet us expand the full joint distribution in Figure 13.3 by adding a fourth variable, Weather.\nThe full joint distribution then becomes P(Toothache, Catch, Cavity, Weather), which has\n2 × 2 × 2 × 4 = 32 entries. It contains four “editions” of the table shown in Figure 13.3,\none for each kind of weather. What relationship do these editions have to each other and to\nthe original three-variable table? For example, how are P(toothache, catch, cavity, cloudy)\nand P(toothache, catch, cavity) related? We can use the product rule:\nP(toothache, catch, cavity, cloudy)\n= P(cloudy | toothache, catch, cavity)P(toothache, catch, cavity) .",
  "and P(toothache, catch, cavity) related? We can use the product rule:\nP(toothache, catch, cavity, cloudy)\n= P(cloudy | toothache, catch, cavity)P(toothache, catch, cavity) .\nNow, unless one is in the deity business, one should not imagine that one’s dental problems\ninﬂuence the weather. And for indoor dentistry, at least, it seems safe to say that the weather\ndoes not inﬂuence the dental variables. Therefore, the following assertion seems reasonable:\nP(cloudy | toothache, catch, cavity) = P(cloudy) .\n(13.10)\nFrom this, we can deduce\nP(toothache, catch, cavity, cloudy) = P(cloudy)P(toothache, catch, cavity) .\nA similar equation exists for every entry in P(Toothache, Catch, Cavity, Weather). In fact,\nwe can write the general equation\nP(Toothache, Catch, Cavity, Weather) = P(Toothache, Catch, Cavity)P(Weather) .\nThus, the 32-element table for four variables can be constructed from one 8-element table\nand one 4-element table. This decomposition is illustrated schematically in Figure 13.4(a).\nThe property we used in Equation (13.10) is called independence (also marginal in-\nINDEPENDENCE\ndependence and absolute independence). In particular, the weather is independent of one’s\ndental problems. Independence between propositions a and b can be written as\nP(a | b) = P(a)\nor\nP(b | a) = P(b)\nor\nP(a ∧b) = P(a)P(b) .\n(13.11)\nAll these forms are equivalent (Exercise 13.12). Independence between variables X and Y\ncan be written as follows (again, these are all equivalent):\nP(X | Y ) = P(X)\nor\nP(Y | X) = P(Y )\nor\nP(X, Y ) = P(X)P(Y ) .\nIndependence assertions are usually based on knowledge of the domain. As the toothache–\nweather example illustrates, they can dramatically reduce the amount of information nec-\nessary to specify the full joint distribution. If the complete set of variables can be divided Section 13.5.\nBayes’ Rule and Its Use\n495\nWeather\nToothache\nCatch\nCavity\ndecomposes\n      into\nWeather\nToothache\nCatch\nCavity\ndecomposes\n       into\nCoin1\nCoinn\nCoin1\nCoinn\n(a)\n(b)\nFigure 13.4\nTwo examples of factoring a large joint distribution into smaller distributions,\nusing absolute independence. (a) Weather and dental problems are independent. (b) Coin\nﬂips are independent.\ninto independent subsets, then the full joint distribution can be factored into separate joint\ndistributions on those subsets. For example, the full joint distribution on the outcome of n\nindependent coin ﬂips, P(C1, . . . , Cn), has 2n entries, but it can be represented as the prod-",
  "distributions on those subsets. For example, the full joint distribution on the outcome of n\nindependent coin ﬂips, P(C1, . . . , Cn), has 2n entries, but it can be represented as the prod-\nuct of n single-variable distributions P(Ci). In a more practical vein, the independence of\ndentistry and meteorology is a good thing, because otherwise the practice of dentistry might\nrequire intimate knowledge of meteorology, and vice versa.\nWhen they are available, then, independence assertions can help in reducing the size of\nthe domain representation and the complexity of the inference problem. Unfortunately, clean\nseparation of entire sets of variables by independence is quite rare. Whenever a connection,\nhowever indirect, exists between two variables, independence will fail to hold. Moreover,\neven independent subsets can be quite large—for example, dentistry might involve dozens of\ndiseases and hundreds of symptoms, all of which are interrelated. To handle such problems,\nwe need more subtle methods than the straightforward concept of independence.\n13.5\nBAYES’ RULE AND ITS USE\nOn page 486, we deﬁned the product rule. It can actually be written in two forms:\nP(a ∧b) = P(a | b)P(b)\nand\nP(a ∧b) = P(b | a)P(a) .\nEquating the two right-hand sides and dividing by P(a), we get\nP(b | a) = P(a | b)P(b)\nP(a)\n.\n(13.12)\nThis equation is known as Bayes’ rule (also Bayes’ law or Bayes’ theorem). This simple\nBAYES’ RULE\nequation underlies most modern AI systems for probabilistic inference. 496\nChapter\n13.\nQuantifying Uncertainty\nThe more general case of Bayes’ rule for multivalued variables can be written in the P\nnotation as follows:\nP(Y | X) = P(X | Y )P(Y )\nP(X)\n,\nAs before, this is to be taken as representing a set of equations, each dealing with speciﬁc val-\nues of the variables. We will also have occasion to use a more general version conditionalized\non some background evidence e:\nP(Y | X, e) = P(X | Y, e)P(Y | e)\nP(X | e)\n.\n(13.13)\n13.5.1\nApplying Bayes’ rule: The simple case\nOn the surface, Bayes’ rule does not seem very useful. It allows us to compute the single\nterm P(b | a) in terms of three terms: P(a | b), P(b), and P(a). That seems like two steps\nbackwards, but Bayes’ rule is useful in practice because there are many cases where we do\nhave good probability estimates for these three numbers and need to compute the fourth.\nOften, we perceive as evidence the effect of some unknown cause and we would like to\ndetermine that cause. In that case, Bayes’ rule becomes",
  "have good probability estimates for these three numbers and need to compute the fourth.\nOften, we perceive as evidence the effect of some unknown cause and we would like to\ndetermine that cause. In that case, Bayes’ rule becomes\nP(cause | effect) = P(effect | cause)P(cause)\nP(effect)\n.\nThe conditional probability P(effect | cause) quantiﬁes the relationship in the causal direc-\nCAUSAL\ntion, whereas P(cause | effect) describes the diagnostic direction. In a task such as medical\nDIAGNOSTIC\ndiagnosis, we often have conditional probabilities on causal relationships (that is, the doctor\nknows P(symptoms | disease)) and want to derive a diagnosis, P(disease | symptoms). For\nexample, a doctor knows that the disease meningitis causes the patient to have a stiff neck,\nsay, 70% of the time. The doctor also knows some unconditional facts: the prior probabil-\nity that a patient has meningitis is 1/50,000, and the prior probability that any patient has a\nstiff neck is 1%. Letting s be the proposition that the patient has a stiff neck and m be the\nproposition that the patient has meningitis, we have\nP(s | m) = 0.7\nP(m) = 1/50000\nP(s) = 0.01\nP(m | s) = P(s | m)P(m)\nP(s)\n= 0.7 × 1/50000\n0.01\n= 0.0014 .\n(13.14)\nThat is, we expect less than 1 in 700 patients with a stiff neck to have meningitis. Notice that\neven though a stiff neck is quite strongly indicated by meningitis (with probability 0.7), the\nprobability of meningitis in the patient remains small. This is because the prior probability of\nstiff necks is much higher than that of meningitis.\nSection 13.3 illustrated a process by which one can avoid assessing the prior probability\nof the evidence (here, P(s)) by instead computing a posterior probability for each value of Section 13.5.\nBayes’ Rule and Its Use\n497\nthe query variable (here, m and ¬m) and then normalizing the results. The same process can\nbe applied when using Bayes’ rule. We have\nP(M | s) = α ⟨P(s | m)P(m), P(s | ¬m)P(¬m)⟩.\nThus, to use this approach we need to estimate P(s | ¬m) instead of P(s). There is no free\nlunch—sometimes this is easier, sometimes it is harder. The general form of Bayes’ rule with\nnormalization is\nP(Y | X) = α P(X | Y )P(Y ) ,\n(13.15)\nwhere α is the normalization constant needed to make the entries in P(Y | X) sum to 1.\nOne obvious question to ask about Bayes’ rule is why one might have available the\nconditional probability in one direction, but not the other. In the meningitis domain, perhaps",
  "One obvious question to ask about Bayes’ rule is why one might have available the\nconditional probability in one direction, but not the other. In the meningitis domain, perhaps\nthe doctor knows that a stiff neck implies meningitis in 1 out of 5000 cases; that is, the doctor\nhas quantitative information in the diagnostic direction from symptoms to causes. Such a\ndoctor has no need to use Bayes’ rule. Unfortunately, diagnostic knowledge is often more\nfragile than causal knowledge. If there is a sudden epidemic of meningitis, the unconditional\nprobability of meningitis, P(m), will go up. The doctor who derived the diagnostic proba-\nbility P(m | s) directly from statistical observation of patients before the epidemic will have\nno idea how to update the value, but the doctor who computes P(m | s) from the other three\nvalues will see that P(m | s) should go up proportionately with P(m). Most important, the\ncausal information P(s | m) is unaffected by the epidemic, because it simply reﬂects the way\nmeningitis works. The use of this kind of direct causal or model-based knowledge provides\nthe crucial robustness needed to make probabilistic systems feasible in the real world.\n13.5.2\nUsing Bayes’ rule: Combining evidence\nWe have seen that Bayes’ rule can be useful for answering probabilistic queries conditioned\non one piece of evidence—for example, the stiff neck. In particular, we have argued that\nprobabilistic information is often available in the form P(effect | cause). What happens when\nwe have two or more pieces of evidence? For example, what can a dentist conclude if her\nnasty steel probe catches in the aching tooth of a patient? If we know the full joint distribution\n(Figure 13.3), we can read off the answer:\nP(Cavity | toothache ∧catch) = α ⟨0.108, 0.016⟩≈⟨0.871, 0.129⟩.\nWe know, however, that such an approach does not scale up to larger numbers of variables.\nWe can try using Bayes’ rule to reformulate the problem:\nP(Cavity | toothache ∧catch)\n= α P(toothache ∧catch | Cavity) P(Cavity) .\n(13.16)\nFor this reformulation to work, we need to know the conditional probabilities of the conjunc-\ntion toothache ∧catch for each value of Cavity. That might be feasible for just two evidence\nvariables, but again it does not scale up. If there are n possible evidence variables (X rays,\ndiet, oral hygiene, etc.), then there are 2n possible combinations of observed values for which\nwe would need to know conditional probabilities. We might as well go back to using the",
  "diet, oral hygiene, etc.), then there are 2n possible combinations of observed values for which\nwe would need to know conditional probabilities. We might as well go back to using the\nfull joint distribution. This is what ﬁrst led researchers away from probability theory toward 498\nChapter\n13.\nQuantifying Uncertainty\napproximate methods for evidence combination that, while giving incorrect answers, require\nfewer numbers to give any answer at all.\nRather than taking this route, we need to ﬁnd some additional assertions about the\ndomain that will enable us to simplify the expressions. The notion of independence in Sec-\ntion 13.4 provides a clue, but needs reﬁning. It would be nice if Toothache and Catch were\nindependent, but they are not: if the probe catches in the tooth, then it is likely that the tooth\nhas a cavity and that the cavity causes a toothache. These variables are independent, how-\never, given the presence or the absence of a cavity. Each is directly caused by the cavity, but\nneither has a direct effect on the other: toothache depends on the state of the nerves in the\ntooth, whereas the probe’s accuracy depends on the dentist’s skill, to which the toothache is\nirrelevant.5 Mathematically, this property is written as\nP(toothache ∧catch | Cavity) = P(toothache | Cavity)P(catch | Cavity) .\n(13.17)\nThis equation expresses the conditional independence of toothache and catch given Cavity.\nCONDITIONAL\nINDEPENDENCE\nWe can plug it into Equation (13.16) to obtain the probability of a cavity:\nP(Cavity | toothache ∧catch)\n= α P(toothache | Cavity) P(catch | Cavity) P(Cavity) .\n(13.18)\nNow the information requirements are the same as for inference, using each piece of evi-\ndence separately: the prior probability P(Cavity) for the query variable and the conditional\nprobability of each effect, given its cause.\nThe general deﬁnition of conditional independence of two variables X and Y , given a\nthird variable Z, is\nP(X, Y | Z) = P(X | Z)P(Y | Z) .\nIn the dentist domain, for example, it seems reasonable to assert conditional independence of\nthe variables Toothache and Catch, given Cavity:\nP(Toothache, Catch | Cavity) = P(Toothache | Cavity)P(Catch | Cavity) . (13.19)\nNotice that this assertion is somewhat stronger than Equation (13.17), which asserts indepen-\ndence only for speciﬁc values of Toothache and Catch. As with absolute independence in\nEquation (13.11), the equivalent forms\nP(X | Y, Z) = P(X | Z)\nand\nP(Y | X, Z) = P(Y | Z)",
  "dence only for speciﬁc values of Toothache and Catch. As with absolute independence in\nEquation (13.11), the equivalent forms\nP(X | Y, Z) = P(X | Z)\nand\nP(Y | X, Z) = P(Y | Z)\ncan also be used (see Exercise 13.17). Section 13.4 showed that absolute independence as-\nsertions allow a decomposition of the full joint distribution into much smaller pieces. It turns\nout that the same is true for conditional independence assertions. For example, given the\nassertion in Equation (13.19), we can derive a decomposition as follows:\nP(Toothache, Catch, Cavity)\n= P(Toothache, Catch | Cavity)P(Cavity)\n(product rule)\n= P(Toothache | Cavity)P(Catch | Cavity)P(Cavity)\n(using 13.19).\n(The reader can easily check that this equation does in fact hold in Figure 13.3.) In this way,\nthe original large table is decomposed into three smaller tables. The original table has seven\n5 We assume that the patient and dentist are distinct individuals. Section 13.6.\nThe Wumpus World Revisited\n499\nindependent numbers (23 = 8 entries in the table, but they must sum to 1, so 7 are indepen-\ndent). The smaller tables contain ﬁve independent numbers (for a conditional probability\ndistributions such as P(T|C there are two rows of two numbers, and each row sums to 1, so\nthat’s two independent numbers; for a prior distribution like P(C) there is only one indepen-\ndent number). Going from seven to ﬁve might not seem like a major triumph, but the point\nis that, for n symptoms that are all conditionally independent given Cavity, the size of the\nrepresentation grows as O(n) instead of O(2n). That means that conditional independence\nassertions can allow probabilistic systems to scale up; moreover, they are much more com-\nmonly available than absolute independence assertions. Conceptually, Cavity separates\nSEPARATION\nToothache and Catch because it is a direct cause of both of them. The decomposition of\nlarge probabilistic domains into weakly connected subsets through conditional independence\nis one of the most important developments in the recent history of AI.\nThe dentistry example illustrates a commonly occurring pattern in which a single cause\ndirectly inﬂuences a number of effects, all of which are conditionally independent, given the\ncause. The full joint distribution can be written as\nP(Cause, Effect1, . . . , Effectn) = P(Cause)\n\u0019\ni\nP(Effecti | Cause) .\nSuch a probability distribution is called a naive Bayes model—“naive” because it is often\nNAIVE BAYES",
  "cause. The full joint distribution can be written as\nP(Cause, Effect1, . . . , Effectn) = P(Cause)\n\u0019\ni\nP(Effecti | Cause) .\nSuch a probability distribution is called a naive Bayes model—“naive” because it is often\nNAIVE BAYES\nused (as a simplifying assumption) in cases where the “effect” variables are not actually\nconditionally independent given the cause variable. (The naive Bayes model is sometimes\ncalled a Bayesian classiﬁer, a somewhat careless usage that has prompted true Bayesians\nto call it the idiot Bayes model.) In practice, naive Bayes systems can work surprisingly\nwell, even when the conditional independence assumption is not true. Chapter 20 describes\nmethods for learning naive Bayes distributions from observations.\n13.6\nTHE WUMPUS WORLD REVISITED\nWe can combine of the ideas in this chapter to solve probabilistic reasoning problems in the\nwumpus world. (See Chapter 7 for a complete description of the wumpus world.) Uncertainty\narises in the wumpus world because the agent’s sensors give only partial information about\nthe world. For example, Figure 13.5 shows a situation in which each of the three reachable\nsquares—[1,3], [2,2], and [3,1]—might contain a pit. Pure logical inference can conclude\nnothing about which square is most likely to be safe, so a logical agent might have to choose\nrandomly. We will see that a probabilistic agent can do much better than the logical agent.\nOur aim is to calculate the probability that each of the three squares contains a pit. (For\nthis example we ignore the wumpus and the gold.) The relevant properties of the wumpus\nworld are that (1) a pit causes breezes in all neighboring squares, and (2) each square other\nthan [1,1] contains a pit with probability 0.2. The ﬁrst step is to identify the set of random\nvariables we need:\n• As in the propositional logic case, we want one Boolean variable Pij for each square,\nwhich is true iff square [i, j] actually contains a pit. 500\nChapter\n13.\nQuantifying Uncertainty\nOK\n 1,1\n 2,1\n 3,1\n 4,1\n 1,2\n 2,2\n 3,2\n 4,2\n 1,3\n 2,3\n 3,3\n 4,3\n 1,4\n 2,4\nOK\nOK\n 3,4\n 4,4\nB\nB\n 1,1\n 2,1\n 3,1\n 4,1\n 1,2\n 2,2\n 3,2\n 4,2\n 1,3\n 2,3\n 3,3\n 4,3\n 1,4\n 2,4\n 3,4\n 4,4\nKNOWN\nFRONTIER\nQUERY\nOTHER\n(a)\n(b)\nFigure 13.5\n(a) After ﬁnding a breeze in both [1,2] and [2,1], the agent is stuck—there is\nno safe place to explore. (b) Division of the squares into Known, Frontier, and Other, for\na query about [1,3].\n• We also have Boolean variables Bij that are true iff square [i, j] is breezy; we include",
  "no safe place to explore. (b) Division of the squares into Known, Frontier, and Other, for\na query about [1,3].\n• We also have Boolean variables Bij that are true iff square [i, j] is breezy; we include\nthese variables only for the observed squares—in this case, [1,1], [1,2], and [2,1].\nThe next step is to specify the full joint distribution, P(P1,1, . . . , P4,4, B1,1, B1,2, B2,1). Ap-\nplying the product rule, we have\nP(P1,1, . . . , P4,4, B1,1, B1,2, B2,1) =\nP(B1,1, B1,2, B2,1 | P1,1, . . . , P4,4)P(P1,1, . . . , P4,4) .\nThis decomposition makes it easy to see what the joint probability values should be. The\nﬁrst term is the conditional probability distribution of a breeze conﬁguration, given a pit\nconﬁguration; its values are 1 if the breezes are adjacent to the pits and 0 otherwise. The\nsecond term is the prior probability of a pit conﬁguration. Each square contains a pit with\nprobability 0.2, independently of the other squares; hence,\nP(P1,1, . . . , P4,4) =\n4,4\n\u0019\ni,j = 1,1\nP(Pi,j) .\n(13.20)\nFor a particular conﬁguration with exactly n pits, P(P1,1, . . . , P4,4) = 0.2n × 0.816−n.\nIn the situation in Figure 13.5(a), the evidence consists of the observed breeze (or its\nabsence) in each square that is visited, combined with the fact that each such square contains\nno pit. We abbreviate these facts as b = ¬b1,1 ∧b1,2 ∧b2,1 and known = ¬p1,1 ∧¬p1,2 ∧¬p2,1.\nWe are interested in answering queries such as P(P1,3 | known, b): how likely is it that [1,3]\ncontains a pit, given the observations so far?\nTo answer this query, we can follow the standard approach of Equation (13.9), namely,\nsumming over entries from the full joint distribution. Let Unknown be the set of Pi,j vari- Section 13.6.\nThe Wumpus World Revisited\n501\nables for squares other than the Known squares and the query square [1,3]. Then, by Equa-\ntion (13.9), we have\nP(P1,3 | known, b) = α\n\f\nunknown\nP(P1,3, unknown, known, b) .\nThe full joint probabilities have already been speciﬁed, so we are done—that is, unless we\ncare about computation.\nThere are 12 unknown squares; hence the summation contains\n212 = 4096 terms. In general, the summation grows exponentially with the number of squares.\nSurely, one might ask, aren’t the other squares irrelevant?\nHow could [4,4] affect\nwhether [1,3] has a pit? Indeed, this intuition is correct. Let Frontier be the pit variables\n(other than the query variable) that are adjacent to visited squares, in this case just [2,2] and",
  "How could [4,4] affect\nwhether [1,3] has a pit? Indeed, this intuition is correct. Let Frontier be the pit variables\n(other than the query variable) that are adjacent to visited squares, in this case just [2,2] and\n[3,1]. Also, let Other be the pit variables for the other unknown squares; in this case, there are\n10 other squares, as shown in Figure 13.5(b). The key insight is that the observed breezes are\nconditionally independent of the other variables, given the known, frontier, and query vari-\nables. To use the insight, we manipulate the query formula into a form in which the breezes\nare conditioned on all the other variables, and then we apply conditional independence:\nP(P1,3 | known, b)\n= α\n\f\nunknown\nP(P1,3, known, b, unknown)\n(by Equation (13.9))\n= α\n\f\nunknown\nP(b | P1,3, known, unknown)P(P1,3, known, unknown)\n(by the product rule)\n= α\n\f\nfrontier\n\f\nother\nP(b | known, P1,3, frontier, other)P(P1,3, known, frontier, other)\n= α\n\f\nfrontier\n\f\nother\nP(b | known, P1,3, frontier)P(P1,3, known, frontier, other) ,\nwhere the ﬁnal step uses conditional independence: b is independent of other given known,\nP1,3, and frontier. Now, the ﬁrst term in this expression does not depend on the Other\nvariables, so we can move the summation inward:\nP(P1,3 | known, b)\n= α\n\f\nfrontier\nP(b | known, P1,3, frontier)\n\f\nother\nP(P1,3, known, frontier, other) .\nBy independence, as in Equation (13.20), the prior term can be factored, and then the terms\ncan be reordered:\nP(P1,3 | known, b)\n= α\n\f\nfrontier\nP(b | known, P1,3, frontier)\n\f\nother\nP(P1,3)P(known)P(frontier)P(other)\n= α P(known)P(P1,3)\n\f\nfrontier\nP(b | known, P1,3, frontier)P(frontier)\n\f\nother\nP(other)\n= α′ P(P1,3)\n\f\nfrontier\nP(b | known, P1,3, frontier)P(frontier) , 502\nChapter\n13.\nQuantifying Uncertainty\nOK\n 1,1\n 2,1\n 3,1\n 1,2\nOK\nOK\nB\nB\nOK\n 1,1\n 2,1\n 1,2\n 2,2\nOK\nOK\nB\nB\nOK\n 1,1\n 2,1\n 3,1\n 1,2\nOK\nOK\nB\nB\n0.2 x 0.2 = 0.04\n0.2 x 0.8 = 0.16\n0.8 x 0.2 = 0.16\nOK\n 1,1\n 2,1\n 1,2\n 1,3\nOK\nOK\nB\nB\nOK\n 1,1\n 2,1\n 3,1\n 1,2\n 1,3\nOK\nOK\nB\nB\n0.2 x 0.2 = 0.04\n0.2 x 0.8 = 0.16\n(a)\n(b)\n 2,2\n 1,3\n 3,1\n 1,3\n 2,2\n 1,3\n 3,1\n 2,2\n 2,2\nFigure 13.6\nConsistent models for the frontier variables P2,2 and P3,1, showing\nP(frontier) for each model: (a) three models with P1,3 = true showing two or three pits,\nand (b) two models with P1,3 = false showing one or two pits.\nwhere the last step folds P(known) into the normalizing constant and uses the fact that\n\u0002\nother P(other) equals 1.",
  "and (b) two models with P1,3 = false showing one or two pits.\nwhere the last step folds P(known) into the normalizing constant and uses the fact that\n\u0002\nother P(other) equals 1.\nNow, there are just four terms in the summation over the frontier variables P2,2 and\nP3,1. The use of independence and conditional independence has completely eliminated the\nother squares from consideration.\nNotice that the expression P(b | known, P1,3, frontier) is 1 when the frontier is consis-\ntent with the breeze observations, and 0 otherwise. Thus, for each value of P1,3, we sum over\nthe logical models for the frontier variables that are consistent with the known facts. (Com-\npare with the enumeration over models in Figure 7.5 on page 241.) The models and their\nassociated prior probabilities—P(frontier )—are shown in Figure 13.6. We have\nP(P1,3 | known, b) = α′ ⟨0.2(0.04 + 0.16 + 0.16), 0.8(0.04 + 0.16)⟩≈⟨0.31, 0.69⟩.\nThat is, [1,3] (and [3,1] by symmetry) contains a pit with roughly 31% probability. A similar\ncalculation, which the reader might wish to perform, shows that [2,2] contains a pit with\nroughly 86% probability. The wumpus agent should deﬁnitely avoid [2,2]! Note that our\nlogical agent from Chapter 7 did not know that [2,2] was worse than the other squares. Logic\ncan tell us that it is unknown whether there is a pit in [2, 2], but we need probability to tell us\nhow likely it is.\nWhat this section has shown is that even seemingly complicated problems can be for-\nmulated precisely in probability theory and solved with simple algorithms. To get efﬁcient\nsolutions, independence and conditional independence relationships can be used to simplify\nthe summations required. These relationships often correspond to our natural understanding\nof how the problem should be decomposed. In the next chapter, we develop formal represen-\ntations for such relationships as well as algorithms that operate on those representations to\nperform probabilistic inference efﬁciently. Section 13.7.\nSummary\n503\n13.7\nSUMMARY\nThis chapter has suggested probability theory as a suitable foundation for uncertain reasoning\nand provided a gentle introduction to its use.\n• Uncertainty arises because of both laziness and ignorance. It is inescapable in complex,\nnondeterministic, or partially observable environments.\n• Probabilities express the agent’s inability to reach a deﬁnite decision regarding the truth\nof a sentence. Probabilities summarize the agent’s beliefs relative to the evidence.",
  "nondeterministic, or partially observable environments.\n• Probabilities express the agent’s inability to reach a deﬁnite decision regarding the truth\nof a sentence. Probabilities summarize the agent’s beliefs relative to the evidence.\n• Decision theory combines the agent’s beliefs and desires, deﬁning the best action as the\none that maximizes expected utility.\n• Basic probability statements include prior probabilities and conditional probabilities\nover simple and complex propositions.\n• The axioms of probability constrain the possible assignments of probabilities to propo-\nsitions. An agent that violates the axioms must behave irrationally in some cases.\n• The full joint probability distribution speciﬁes the probability of each complete as-\nsignment of values to random variables. It is usually too large to create or use in its\nexplicit form, but when it is available it can be used to answer queries simply by adding\nup entries for the possible worlds corresponding to the query propositions.\n• Absolute independence between subsets of random variables allows the full joint dis-\ntribution to be factored into smaller joint distributions, greatly reducing its complexity.\nAbsolute independence seldom occurs in practice.\n• Bayes’ rule allows unknown probabilities to be computed from known conditional\nprobabilities, usually in the causal direction. Applying Bayes’ rule with many pieces of\nevidence runs into the same scaling problems as does the full joint distribution.\n• Conditional independence brought about by direct causal relationships in the domain\nmight allow the full joint distribution to be factored into smaller, conditional distri-\nbutions. The naive Bayes model assumes the conditional independence of all effect\nvariables, given a single cause variable, and grows linearly with the number of effects.\n• A wumpus-world agent can calculate probabilities for unobserved aspects of the world,\nthereby improving on the decisions of a purely logical agent. Conditional independence\nmakes these calculations tractable.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nProbability theory was invented as a way of analyzing games of chance. In about 850 A.D.\nthe Indian mathematician Mahaviracarya described how to arrange a set of bets that can’t lose\n(what we now call a Dutch book). In Europe, the ﬁrst signiﬁcant systematic analyses were\nproduced by Girolamo Cardano around 1565, although publication was posthumous (1663).",
  "(what we now call a Dutch book). In Europe, the ﬁrst signiﬁcant systematic analyses were\nproduced by Girolamo Cardano around 1565, although publication was posthumous (1663).\nBy that time, probability had been established as a mathematical discipline due to a series of 504\nChapter\n13.\nQuantifying Uncertainty\nresults established in a famous correspondence between Blaise Pascal and Pierre de Fermat\nin 1654. As with probability itself, the results were initially motivated by gambling problems\n(see Exercise 13.9). The ﬁrst published textbook on probability was De Ratiociniis in Ludo\nAleae (Huygens, 1657). The “laziness and ignorance” view of uncertainty was described\nby John Arbuthnot in the preface of his translation of Huygens (Arbuthnot, 1692): “It is\nimpossible for a Die, with such determin’d force and direction, not to fall on such determin’d\nside, only I don’t know the force and direction which makes it fall on such determin’d side,\nand therefore I call it Chance, which is nothing but the want of art...”\nLaplace (1816) gave an exceptionally accurate and modern overview of probability; he\nwas the ﬁrst to use the example “take two urns, A and B, the ﬁrst containing four white and\ntwo black balls, . . . ” The Rev. Thomas Bayes (1702–1761) introduced the rule for reasoning\nabout conditional probabilities that was named after him (Bayes, 1763). Bayes only con-\nsidered the case of uniform priors; it was Laplace who independently developed the general\ncase. Kolmogorov (1950, ﬁrst published in German in 1933) presented probability theory in\na rigorously axiomatic framework for the ﬁrst time. R´enyi (1970) later gave an axiomatic\npresentation that took conditional probability, rather than absolute probability, as primitive.\nPascal used probability in ways that required both the objective interpretation, as a prop-\nerty of the world based on symmetry or relative frequency, and the subjective interpretation,\nbased on degree of belief—the former in his analyses of probabilities in games of chance, the\nlatter in the famous “Pascal’s wager” argument about the possible existence of God. How-\never, Pascal did not clearly realize the distinction between these two interpretations. The\ndistinction was ﬁrst drawn clearly by James Bernoulli (1654–1705).\nLeibniz introduced the “classical” notion of probability as a proportion of enumerated,\nequally probable cases, which was also used by Bernoulli, although it was brought to promi-",
  "distinction was ﬁrst drawn clearly by James Bernoulli (1654–1705).\nLeibniz introduced the “classical” notion of probability as a proportion of enumerated,\nequally probable cases, which was also used by Bernoulli, although it was brought to promi-\nnence by Laplace (1749–1827). This notion is ambiguous between the frequency interpreta-\ntion and the subjective interpretation. The cases can be thought to be equally probable either\nbecause of a natural, physical symmetry between them, or simply because we do not have\nany knowledge that would lead us to consider one more probable than another. The use of\nthis latter, subjective consideration to justify assigning equal probabilities is known as the\nprinciple of indifference. The principle is often attributed to Laplace, but he never isolated\nPRINCIPLE OF\nINDIFFERENCE\nthe principle explicitly. George Boole and John Venn both referred to it as the principle of\ninsufﬁcient reason; the modern name is due to Keynes (1921).\nPRINCIPLE OF\nINSUFFICIENT\nREASON\nThe debate between objectivists and subjectivists became sharper in the 20th century.\nKolmogorov (1963), R. A. Fisher (1922), and Richard von Mises (1928) were advocates of\nthe relative frequency interpretation. Karl Popper’s (1959, ﬁrst published in German in 1934)\n“propensity” interpretation traces relative frequencies to an underlying physical symmetry.\nFrank Ramsey (1931), Bruno de Finetti (1937), R. T. Cox (1946), Leonard Savage (1954),\nRichard Jeffrey (1983), and E. T. Jaynes (2003) interpreted probabilities as the degrees of\nbelief of speciﬁc individuals. Their analyses of degree of belief were closely tied to utili-\nties and to behavior—speciﬁcally, to the willingness to place bets. Rudolf Carnap, following\nLeibniz and Laplace, offered a different kind of subjective interpretation of probability—\nnot as any actual individual’s degree of belief, but as the degree of belief that an idealized\nindividual should have in a particular proposition a, given a particular body of evidence e. Bibliographical and Historical Notes\n505\nCarnap attempted to go further than Leibniz or Laplace by making this notion of degree of\nconﬁrmation mathematically precise, as a logical relation between a and e. The study of this\nCONFIRMATION\nrelation was intended to constitute a mathematical discipline called inductive logic, analo-\nINDUCTIVE LOGIC\ngous to ordinary deductive logic (Carnap, 1948, 1950). Carnap was not able to extend his",
  "CONFIRMATION\nrelation was intended to constitute a mathematical discipline called inductive logic, analo-\nINDUCTIVE LOGIC\ngous to ordinary deductive logic (Carnap, 1948, 1950). Carnap was not able to extend his\ninductive logic much beyond the propositional case, and Putnam (1963) showed by adversar-\nial arguments that some fundamental difﬁculties would prevent a strict extension to languages\ncapable of expressing arithmetic.\nCox’s theorem (1946) shows that any system for uncertain reasoning that meets his set\nof assumptions is equivalent to probability theory. This gave renewed conﬁdence to those\nwho already favored probability, but others were not convinced, pointing to the assumptions\n(primarily that belief must be represented by a single number, and thus the belief in ¬p must\nbe a function of the belief in p). Halpern (1999) describes the assumptions and shows some\ngaps in Cox’s original formulation. Horn (2003) shows how to patch up the difﬁculties.\nJaynes (2003) has a similar argument that is easier to read.\nThe question of reference classes is closely tied to the attempt to ﬁnd an inductive logic.\nThe approach of choosing the “most speciﬁc” reference class of sufﬁcient size was formally\nproposed by Reichenbach (1949). Various attempts have been made, notably by Henry Ky-\nburg (1977, 1983), to formulate more sophisticated policies in order to avoid some obvious\nfallacies that arise with Reichenbach’s rule, but such approaches remain somewhat ad hoc.\nMore recent work by Bacchus, Grove, Halpern, and Koller (1992) extends Carnap’s methods\nto ﬁrst-order theories, thereby avoiding many of the difﬁculties associated with the straight-\nforward reference-class method. Kyburg and Teng (2006) contrast probabilistic inference\nwith nonmonotonic logic.\nBayesian probabilistic reasoning has been used in AI since the 1960s, especially in\nmedical diagnosis. It was used not only to make a diagnosis from available evidence, but also\nto select further questions and tests by using the theory of information value (Section 16.6)\nwhen available evidence was inconclusive (Gorry, 1968; Gorry et al., 1973). One system\noutperformed human experts in the diagnosis of acute abdominal illnesses (de Dombal et al.,\n1974). Lucas et al. (2004) gives an overview. These early Bayesian systems suffered from a\nnumber of problems, however. Because they lacked any theoretical model of the conditions\nthey were diagnosing, they were vulnerable to unrepresentative data occurring in situations",
  "number of problems, however. Because they lacked any theoretical model of the conditions\nthey were diagnosing, they were vulnerable to unrepresentative data occurring in situations\nfor which only a small sample was available (de Dombal et al., 1981). Even more fundamen-\ntally, because they lacked a concise formalism (such as the one to be described in Chapter 14)\nfor representing and using conditional independence information, they depended on the ac-\nquisition, storage, and processing of enormous tables of probabilistic data. Because of these\ndifﬁculties, probabilistic methods for coping with uncertainty fell out of favor in AI from the\n1970s to the mid-1980s. Developments since the late 1980s are described in the next chapter.\nThe naive Bayes model for joint distributions has been studied extensively in the pat-\ntern recognition literature since the 1950s (Duda and Hart, 1973). It has also been used, often\nunwittingly, in information retrieval, beginning with the work of Maron (1961). The proba-\nbilistic foundations of this technique, described further in Exercise 13.22, were elucidated by\nRobertson and Sparck Jones (1976). Domingos and Pazzani (1997) provide an explanation 506\nChapter\n13.\nQuantifying Uncertainty\nfor the surprising success of naive Bayesian reasoning even in domains where the indepen-\ndence assumptions are clearly violated.\nThere are many good introductory textbooks on probability theory, including those by\nBertsekas and Tsitsiklis (2008) and Grinstead and Snell (1997). DeGroot and Schervish\n(2001) offer a combined introduction to probability and statistics from a Bayesian stand-\npoint. Richard Hamming’s (1991) textbook gives a mathematically sophisticated introduc-\ntion to probability theory from the standpoint of a propensity interpretation based on physical\nsymmetry. Hacking (1975) and Hald (1990) cover the early history of the concept of proba-\nbility. Bernstein (1996) gives an entertaining popular account of the story of risk.\nEXERCISES\n13.1\nShow from ﬁrst principles that P(a | b ∧a) = 1.\n13.2\nUsing the axioms of probability, prove that any probability distribution on a discrete\nrandom variable must sum to 1.\n13.3\nFor each of the following statements, either prove it is true or give a counterexample.\na. If P(a | b, c) = P(b | a, c), then P(a | c) = P(b | c)\nb. If P(a | b, c) = P(a), then P(b | c) = P(b)\nc. If P(a | b) = P(a), then P(a | b, c) = P(a | c)\n13.4",
  "For each of the following statements, either prove it is true or give a counterexample.\na. If P(a | b, c) = P(b | a, c), then P(a | c) = P(b | c)\nb. If P(a | b, c) = P(a), then P(b | c) = P(b)\nc. If P(a | b) = P(a), then P(a | b, c) = P(a | c)\n13.4\nWould it be rational for an agent to hold the three beliefs P(A) = 0.4, P(B) = 0.3, and\nP(A ∨B) = 0.5? If so, what range of probabilities would be rational for the agent to hold for\nA ∧B? Make up a table like the one in Figure 13.2, and show how it supports your argument\nabout rationality. Then draw another version of the table where P(A ∨B) = 0.7. Explain\nwhy it is rational to have this probability, even though the table shows one case that is a loss\nand three that just break even. (Hint: what is Agent 1 committed to about the probability of\neach of the four cases, especially the case that is a loss?)\n13.5\nThis question deals with the properties of possible worlds, deﬁned on page 488 as\nassignments to all random variables. We will work with propositions that correspond to\nexactly one possible world because they pin down the assignments of all the variables. In\nprobability theory, such propositions are called atomic events. For example, with Boolean\nATOMIC EVENT\nvariables X1, X2, X3, the proposition x1 ∧¬x2 ∧¬x3 ﬁxes the assignment of the variables;\nin the language of propositional logic, we would say it has exactly one model.\na. Prove, for the case of n Boolean variables, that any two distinct atomic events are\nmutually exclusive; that is, their conjunction is equivalent to false.\nb. Prove that the disjunction of all possible atomic events is logically equivalent to true.\nc. Prove that any proposition is logically equivalent to the disjunction of the atomic events\nthat entail its truth. Exercises\n507\n13.6\nProve Equation (13.4) from Equations (13.1) and (13.2).\n13.7\nConsider the set of all possible ﬁve-card poker hands dealt fairly from a standard deck\nof ﬁfty-two cards.\na. How many atomic events are there in the joint probability distribution (i.e., how many\nﬁve-card hands are there)?\nb. What is the probability of each atomic event?\nc. What is the probability of being dealt a royal straight ﬂush? Four of a kind?\n13.8\nGiven the full joint distribution shown in Figure 13.3, calculate the following:\na. P(toothache) .\nb. P(Cavity) .\nc. P(Toothache | cavity) .\nd. P(Cavity | toothache ∨catch) .\n13.9\nIn his letter of August 24, 1654, Pascal was trying to show how a pot of money should",
  "a. P(toothache) .\nb. P(Cavity) .\nc. P(Toothache | cavity) .\nd. P(Cavity | toothache ∨catch) .\n13.9\nIn his letter of August 24, 1654, Pascal was trying to show how a pot of money should\nbe allocated when a gambling game must end prematurely. Imagine a game where each turn\nconsists of the roll of a die, player E gets a point when the die is even, and player O gets a\npoint when the die is odd. The ﬁrst player to get 7 points wins the pot. Suppose the game is\ninterrupted with E leading 4–2. How should the money be fairly split in this case? What is\nthe general formula? (Fermat and Pascal made several errors before solving the problem, but\nyou should be able to get it right the ﬁrst time.)\n13.10\nDeciding to put probability theory to good use, we encounter a slot machine with\nthree independent wheels, each producing one of the four symbols BAR, BELL, LEMON, or\nCHERRY with equal probability. The slot machine has the following payout scheme for a bet\nof 1 coin (where “?” denotes that we don’t care what comes up for that wheel):\nBAR/BAR/BAR pays 20 coins\nBELL/BELL/BELL pays 15 coins\nLEMON/LEMON/LEMON pays 5 coins\nCHERRY/CHERRY/CHERRY pays 3 coins\nCHERRY/CHERRY/? pays 2 coins\nCHERRY/?/? pays 1 coin\na. Compute the expected “payback” percentage of the machine. In other words, for each\ncoin played, what is the expected coin return?\nb. Compute the probability that playing the slot machine once will result in a win.\nc. Estimate the mean and median number of plays you can expect to make until you go\nbroke, if you start with 10 coins. You can run a simulation to estimate this, rather than\ntrying to compute an exact answer.\n13.11\nWe wish to transmit an n-bit message to a receiving agent. The bits in the message are\nindependently corrupted (ﬂipped) during transmission with ϵ probability each. With an extra\nparity bit sent along with the original information, a message can be corrected by the receiver 508\nChapter\n13.\nQuantifying Uncertainty\nif at most one bit in the entire message (including the parity bit) has been corrupted. Suppose\nwe want to ensure that the correct message is received with probability at least 1−δ. What is\nthe maximum feasible value of n? Calculate this value for the case ϵ = 0.001, δ = 0.01.\n13.12\nShow that the three forms of independence in Equation (13.11) are equivalent.\n13.13\nConsider two medical tests, A and B, for a virus. Test A is 95% effective at recog-",
  "13.12\nShow that the three forms of independence in Equation (13.11) are equivalent.\n13.13\nConsider two medical tests, A and B, for a virus. Test A is 95% effective at recog-\nnizing the virus when it is present, but has a 10% false positive rate (indicating that the virus\nis present, when it is not). Test B is 90% effective at recognizing the virus, but has a 5% false\npositive rate. The two tests use independent methods of identifying the virus. The virus is\ncarried by 1% of all people. Say that a person is tested for the virus using only one of the tests,\nand that test comes back positive for carrying the virus. Which test returning positive is more\nindicative of someone really carrying the virus? Justify your answer mathematically.\n13.14\nSuppose you are given a coin that lands heads with probability x and tails with\nprobability 1 −x. Are the outcomes of successive ﬂips of the coin independent of each\nother given that you know the value of x? Are the outcomes of successive ﬂips of the coin\nindependent of each other if you do not know the value of x? Justify your answer.\n13.15\nAfter your yearly checkup, the doctor has bad news and good news. The bad news\nis that you tested positive for a serious disease and that the test is 99% accurate (i.e., the\nprobability of testing positive when you do have the disease is 0.99, as is the probability of\ntesting negative when you don’t have the disease). The good news is that this is a rare disease,\nstriking only 1 in 10,000 people of your age. Why is it good news that the disease is rare?\nWhat are the chances that you actually have the disease?\n13.16\nIt is quite often useful to consider the effect of some speciﬁc propositions in the\ncontext of some general background evidence that remains ﬁxed, rather than in the complete\nabsence of information. The following questions ask you to prove more general versions of\nthe product rule and Bayes’ rule, with respect to some background evidence e:\na. Prove the conditionalized version of the general product rule:\nP(X, Y | e) = P(X | Y, e)P(Y | e) .\nb. Prove the conditionalized version of Bayes’ rule in Equation (13.13).\n13.17\nShow that the statement of conditional independence\nP(X, Y | Z) = P(X | Z)P(Y | Z)\nis equivalent to each of the statements\nP(X | Y, Z) = P(X | Z)\nand\nP(B | X, Z) = P(Y | Z) .\n13.18\nSuppose you are given a bag containing n unbiased coins. You are told that n −1 of\nthese coins are normal, with heads on one side and tails on the other, whereas one coin is a",
  "P(X | Y, Z) = P(X | Z)\nand\nP(B | X, Z) = P(Y | Z) .\n13.18\nSuppose you are given a bag containing n unbiased coins. You are told that n −1 of\nthese coins are normal, with heads on one side and tails on the other, whereas one coin is a\nfake, with heads on both sides.\na. Suppose you reach into the bag, pick out a coin at random, ﬂip it, and get a head. What\nis the (conditional) probability that the coin you chose is the fake coin? Exercises\n509\nb. Suppose you continue ﬂipping the coin for a total of k times after picking it and see k\nheads. Now what is the conditional probability that you picked the fake coin?\nc. Suppose you wanted to decide whether the chosen coin was fake by ﬂipping it k times.\nThe decision procedure returns fake if all k ﬂips come up heads; otherwise it returns\nnormal. What is the (unconditional) probability that this procedure makes an error?\n13.19\nIn this exercise, you will complete the normalization calculation for the meningitis\nexample. First, make up a suitable value for P(s | ¬m), and use it to calculate unnormalized\nvalues for P(m | s) and P(¬m | s) (i.e., ignoring the P(s) term in the Bayes’ rule expression,\nEquation (13.14)). Now normalize these values so that they add to 1.\n13.20\nLet X, Y , Z be Boolean random variables. Label the eight entries in the joint dis-\ntribution P(X, Y, Z) as a through h. Express the statement that X and Y are conditionally\nindependent given Z, as a set of equations relating a through h. How many nonredundant\nequations are there?\n13.21\n(Adapted from Pearl (1988).) Suppose you are a witness to a nighttime hit-and-run\naccident involving a taxi in Athens. All taxis in Athens are blue or green. You swear, under\noath, that the taxi was blue. Extensive testing shows that, under the dim lighting conditions,\ndiscrimination between blue and green is 75% reliable.\na. Is it possible to calculate the most likely color for the taxi? (Hint: distinguish carefully\nbetween the proposition that the taxi is blue and the proposition that it appears blue.)\nb. What if you know that 9 out of 10 Athenian taxis are green?\n13.22\nText categorization is the task of assigning a given document to one of a ﬁxed set of\ncategories on the basis of the text it contains. Naive Bayes models are often used for this\ntask. In these models, the query variable is the document category, and the “effect” variables\nare the presence or absence of each word in the language; the assumption is that words occur",
  "task. In these models, the query variable is the document category, and the “effect” variables\nare the presence or absence of each word in the language; the assumption is that words occur\nindependently in documents, with frequencies determined by the document category.\na. Explain precisely how such a model can be constructed, given as “training data” a set\nof documents that have been assigned to categories.\nb. Explain precisely how to categorize a new document.\nc. Is the conditional independence assumption reasonable? Discuss.\n13.23\nIn our analysis of the wumpus world, we used the fact that each square contains a\npit with probability 0.2, independently of the contents of the other squares. Suppose instead\nthat exactly N/5 pits are scattered at random among the N squares other than [1,1]. Are\nthe variables Pi,j and Pk,l still independent? What is the joint distribution P(P1,1, . . . , P4,4)\nnow? Redo the calculation for the probabilities of pits in [1,3] and [2,2].\n13.24\nRedo the probability calculation for pits in [1,3] and [2,2], assuming that each square\ncontains a pit with probability 0.01, independent of the other squares. What can you say\nabout the relative performance of a logical versus a probabilistic agent in this case?\n13.25\nImplement a hybrid probabilistic agent for the wumpus world, based on the hybrid\nagent in Figure 7.20 and the probabilistic inference procedure outlined in this chapter. 14\nPROBABILISTIC\nREASONING\nIn which we explain how to build network models to reason under uncertainty\naccording to the laws of probability theory.\nChapter 13 introduced the basic elements of probability theory and noted the importance of\nindependence and conditional independence relationships in simplifying probabilistic repre-\nsentations of the world. This chapter introduces a systematic way to represent such relation-\nships explicitly in the form of Bayesian networks. We deﬁne the syntax and semantics of\nthese networks and show how they can be used to capture uncertain knowledge in a natu-\nral and efﬁcient way. We then show how probabilistic inference, although computationally\nintractable in the worst case, can be done efﬁciently in many practical situations. We also\ndescribe a variety of approximate inference algorithms that are often applicable when exact\ninference is infeasible. We explore ways in which probability theory can be applied to worlds\nwith objects and relations—that is, to ﬁrst-order, as opposed to propositional, representations.",
  "inference is infeasible. We explore ways in which probability theory can be applied to worlds\nwith objects and relations—that is, to ﬁrst-order, as opposed to propositional, representations.\nFinally, we survey alternative approaches to uncertain reasoning.\n14.1\nREPRESENTING KNOWLEDGE IN AN UNCERTAIN DOMAIN\nIn Chapter 13, we saw that the full joint probability distribution can answer any question about\nthe domain, but can become intractably large as the number of variables grows. Furthermore,\nspecifying probabilities for possible worlds one by one is unnatural and tedious.\nWe also saw that independence and conditional independence relationships among vari-\nables can greatly reduce the number of probabilities that need to be speciﬁed in order to deﬁne\nthe full joint distribution. This section introduces a data structure called a Bayesian network1\nBAYESIAN NETWORK\nto represent the dependencies among variables. Bayesian networks can represent essentially\nany full joint probability distribution and in many cases can do so very concisely.\n1 This is the most common name, but there are many synonyms, including belief network, probabilistic net-\nwork, causal network, and knowledge map. In statistics, the term graphical model refers to a somewhat\nbroader class that includes Bayesian networks. An extension of Bayesian networks called a decision network or\ninﬂuence diagram is covered in Chapter 16.\n510 Section 14.1.\nRepresenting Knowledge in an Uncertain Domain\n511\nA Bayesian network is a directed graph in which each node is annotated with quantita-\ntive probability information. The full speciﬁcation is as follows:\n1. Each node corresponds to a random variable, which may be discrete or continuous.\n2. A set of directed links or arrows connects pairs of nodes. If there is an arrow from node\nX to node Y , X is said to be a parent of Y. The graph has no directed cycles (and hence\nis a directed acyclic graph, or DAG.\n3. Each node Xi has a conditional probability distribution P(Xi | Parents(Xi)) that quan-\ntiﬁes the effect of the parents on the node.\nThe topology of the network—the set of nodes and links—speciﬁes the conditional indepen-\ndence relationships that hold in the domain, in a way that will be made precise shortly. The\nintuitive meaning of an arrow is typically that X has a direct inﬂuence on Y, which suggests\nthat causes should be parents of effects. It is usually easy for a domain expert to decide what",
  "intuitive meaning of an arrow is typically that X has a direct inﬂuence on Y, which suggests\nthat causes should be parents of effects. It is usually easy for a domain expert to decide what\ndirect inﬂuences exist in the domain—much easier, in fact, than actually specifying the prob-\nabilities themselves. Once the topology of the Bayesian network is laid out, we need only\nspecify a conditional probability distribution for each variable, given its parents. We will\nsee that the combination of the topology and the conditional distributions sufﬁces to specify\n(implicitly) the full joint distribution for all the variables.\nRecall the simple world described in Chapter 13, consisting of the variables Toothache,\nCavity, Catch, and Weather. We argued that Weather is independent of the other vari-\nables; furthermore, we argued that Toothache and Catch are conditionally independent,\ngiven Cavity. These relationships are represented by the Bayesian network structure shown\nin Figure 14.1. Formally, the conditional independence of Toothache and Catch, given\nCavity, is indicated by the absence of a link between Toothache and Catch. Intuitively, the\nnetwork represents the fact that Cavity is a direct cause of Toothache and Catch, whereas\nno direct causal relationship exists between Toothache and Catch.\nNow consider the following example, which is just a little more complex. You have\na new burglar alarm installed at home. It is fairly reliable at detecting a burglary, but also\nresponds on occasion to minor earthquakes. (This example is due to Judea Pearl, a resident\nof Los Angeles—hence the acute interest in earthquakes.) You also have two neighbors, John\nand Mary, who have promised to call you at work when they hear the alarm. John nearly\nalways calls when he hears the alarm, but sometimes confuses the telephone ringing with\nWeather\nCavity\nToothache\nCatch\nFigure 14.1\nA simple Bayesian network in which Weather is independent of the other\nthree variables and Toothache and Catch are conditionally independent, given Cavity. 512\nChapter\n14.\nProbabilistic Reasoning\n.001\nP(B)\nAlarm\nEarthquake\nMaryCalls\nJohnCalls\nBurglary\nA\nP(J)\nt\nf\n.90\n.05\nB\nt\nt\nf\nf\nE\nt\nf\nt\nf\nP(A)\n.95\n.29\n.001\n.94\n.002\nP(E)\nA P(M)\nt\nf\n.70\n.01\nFigure 14.2\nA typical Bayesian network, showing both the topology and the conditional\nprobability tables (CPTs). In the CPTs, the letters B, E, A, J, and M stand for Burglary,\nEarthquake, Alarm, JohnCalls, and MaryCalls, respectively.",
  "t\nf\n.70\n.01\nFigure 14.2\nA typical Bayesian network, showing both the topology and the conditional\nprobability tables (CPTs). In the CPTs, the letters B, E, A, J, and M stand for Burglary,\nEarthquake, Alarm, JohnCalls, and MaryCalls, respectively.\nthe alarm and calls then, too. Mary, on the other hand, likes rather loud music and often\nmisses the alarm altogether. Given the evidence of who has or has not called, we would like\nto estimate the probability of a burglary.\nA Bayesian network for this domain appears in Figure 14.2. The network structure\nshows that burglary and earthquakes directly affect the probability of the alarm’s going off,\nbut whether John and Mary call depends only on the alarm. The network thus represents\nour assumptions that they do not perceive burglaries directly, they do not notice minor earth-\nquakes, and they do not confer before calling.\nThe conditional distributions in Figure 14.2 are shown as a conditional probability\ntable, or CPT. (This form of table can be used for discrete variables; other representations,\nCONDITIONAL\nPROBABILITY TABLE\nincluding those suitable for continuous variables, are described in Section 14.2.) Each row\nin a CPT contains the conditional probability of each node value for a conditioning case.\nCONDITIONING CASE\nA conditioning case is just a possible combination of values for the parent nodes—a minia-\nture possible world, if you like. Each row must sum to 1, because the entries represent an\nexhaustive set of cases for the variable. For Boolean variables, once you know that the prob-\nability of a true value is p, the probability of false must be 1 – p, so we often omit the second\nnumber, as in Figure 14.2. In general, a table for a Boolean variable with k Boolean parents\ncontains 2k independently speciﬁable probabilities. A node with no parents has only one row,\nrepresenting the prior probabilities of each possible value of the variable.\nNotice that the network does not have nodes corresponding to Mary’s currently listening\nto loud music or to the telephone ringing and confusing John. These factors are summarized\nin the uncertainty associated with the links from Alarm to JohnCalls and MaryCalls. This\nshows both laziness and ignorance in operation: it would be a lot of work to ﬁnd out why those\nfactors would be more or less likely in any particular case, and we have no reasonable way to\nobtain the relevant information anyway. The probabilities actually summarize a potentially Section 14.2.",
  "factors would be more or less likely in any particular case, and we have no reasonable way to\nobtain the relevant information anyway. The probabilities actually summarize a potentially Section 14.2.\nThe Semantics of Bayesian Networks\n513\ninﬁnite set of circumstances in which the alarm might fail to go off (high humidity, power\nfailure, dead battery, cut wires, a dead mouse stuck inside the bell, etc.) or John or Mary\nmight fail to call and report it (out to lunch, on vacation, temporarily deaf, passing helicopter,\netc.). In this way, a small agent can cope with a very large world, at least approximately. The\ndegree of approximation can be improved if we introduce additional relevant information.\n14.2\nTHE SEMANTICS OF BAYESIAN NETWORKS\nThe previous section described what a network is, but not what it means. There are two\nways in which one can understand the semantics of Bayesian networks. The ﬁrst is to see\nthe network as a representation of the joint probability distribution. The second is to view\nit as an encoding of a collection of conditional independence statements. The two views are\nequivalent, but the ﬁrst turns out to be helpful in understanding how to construct networks,\nwhereas the second is helpful in designing inference procedures.\n14.2.1\nRepresenting the full joint distribution\nViewed as a piece of “syntax,” a Bayesian network is a directed acyclic graph with some\nnumeric parameters attached to each node. One way to deﬁne what the network means—its\nsemantics—is to deﬁne the way in which it represents a speciﬁc joint distribution over all the\nvariables. To do this, we ﬁrst need to retract (temporarily) what we said earlier about the pa-\nrameters associated with each node. We said that those parameters correspond to conditional\nprobabilities P(Xi | Parents(Xi)); this is a true statement, but until we assign semantics to\nthe network as a whole, we should think of them just as numbers θ(Xi | Parents(Xi)).\nA generic entry in the joint distribution is the probability of a conjunction of particular\nassignments to each variable, such as P(X1 = x1 ∧. . . ∧Xn = xn). We use the notation\nP(x1, . . . , xn) as an abbreviation for this. The value of this entry is given by the formula\nP(x1, . . . , xn) =\nn\n\u0019\ni = 1\nθ(xi | parents(Xi)) ,\n(14.1)\nwhere parents(Xi) denotes the values of Parents(Xi) that appear in x1, . . . , xn. Thus,\neach entry in the joint distribution is represented by the product of the appropriate elements",
  "P(x1, . . . , xn) =\nn\n\u0019\ni = 1\nθ(xi | parents(Xi)) ,\n(14.1)\nwhere parents(Xi) denotes the values of Parents(Xi) that appear in x1, . . . , xn. Thus,\neach entry in the joint distribution is represented by the product of the appropriate elements\nof the conditional probability tables (CPTs) in the Bayesian network.\nFrom this deﬁnition, it is easy to prove that the parameters θ(Xi | Parents(Xi)) are\nexactly the conditional probabilities P(Xi | Parents(Xi)) implied by the joint distribution\n(see Exercise 14.2). Hence, we can rewrite Equation (14.1) as\nP(x1, . . . , xn) =\nn\n\u0019\ni = 1\nP(xi | parents(Xi)) .\n(14.2)\nIn other words, the tables we have been calling conditional probability tables really are con-\nditional probability tables according to the semantics deﬁned in Equation (14.1).\nTo illustrate this, we can calculate the probability that the alarm has sounded, but neither\na burglary nor an earthquake has occurred, and both John and Mary call. We multiply entries 514\nChapter\n14.\nProbabilistic Reasoning\nfrom the joint distribution (using single-letter names for the variables):\nP(j, m, a, ¬b, ¬e) = P(j | a)P(m | a)P(a | ¬b ∧¬e)P(¬b)P(¬e)\n= 0.90 × 0.70 × 0.001 × 0.999 × 0.998 = 0.000628 .\nSection 13.3 explained that the full joint distribution can be used to answer any query about\nthe domain. If a Bayesian network is a representation of the joint distribution, then it too can\nbe used to answer any query, by summing all the relevant joint entries. Section 14.4 explains\nhow to do this, but also describes methods that are much more efﬁcient.\nA method for constructing Bayesian networks\nEquation (14.2) deﬁnes what a given Bayesian network means. The next step is to explain\nhow to construct a Bayesian network in such a way that the resulting joint distribution is a\ngood representation of a given domain. We will now show that Equation (14.2) implies certain\nconditional independence relationships that can be used to guide the knowledge engineer in\nconstructing the topology of the network. First, we rewrite the entries in the joint distribution\nin terms of conditional probability, using the product rule (see page 486):\nP(x1, . . . , xn) = P(xn | xn−1, . . . , x1)P(xn−1, . . . , x1) .\nThen we repeat the process, reducing each conjunctive probability to a conditional probability\nand a smaller conjunction. We end up with one big product:\nP(x1, . . . , xn) = P(xn | xn−1, . . . , x1)P(xn−1 | xn−2, . . . , x1) · · · P(x2 | x1)P(x1)\n=\nn\n\u0019\ni = 1\nP(xi | xi−1, . . . , x1) .",
  "and a smaller conjunction. We end up with one big product:\nP(x1, . . . , xn) = P(xn | xn−1, . . . , x1)P(xn−1 | xn−2, . . . , x1) · · · P(x2 | x1)P(x1)\n=\nn\n\u0019\ni = 1\nP(xi | xi−1, . . . , x1) .\nThis identity is called the chain rule. It holds for any set of random variables. Comparing it\nCHAIN RULE\nwith Equation (14.2), we see that the speciﬁcation of the joint distribution is equivalent to the\ngeneral assertion that, for every variable Xi in the network,\nP(Xi | Xi−1, . . . , X1) = P(Xi | Parents(Xi)) ,\n(14.3)\nprovided that Parents(Xi) ⊆{Xi−1, . . . , X1}. This last condition is satisﬁed by numbering\nthe nodes in a way that is consistent with the partial order implicit in the graph structure.\nWhat Equation (14.3) says is that the Bayesian network is a correct representation of\nthe domain only if each node is conditionally independent of its other predecessors in the\nnode ordering, given its parents. We can satisfy this condition with this methodology:\n1. Nodes: First determine the set of variables that are required to model the domain. Now\norder them, {X1, . . . , Xn}. Any order will work, but the resulting network will be more\ncompact if the variables are ordered such that causes precede effects.\n2. Links: For i = 1 to n do:\n• Choose, from X1, . . . , Xi−1, a minimal set of parents for Xi, such that Equa-\ntion (14.3) is satisﬁed.\n• For each parent insert a link from the parent to Xi.\n• CPTs: Write down the conditional probability table, P(Xi|Parents(Xi)). Section 14.2.\nThe Semantics of Bayesian Networks\n515\nIntuitively, the parents of node Xi should contain all those nodes in X1, . . . , Xi−1 that\ndirectly inﬂuence Xi. For example, suppose we have completed the network in Figure 14.2\nexcept for the choice of parents for MaryCalls. MaryCalls is certainly inﬂuenced by whether\nthere is a Burglary or an Earthquake, but not directly inﬂuenced. Intuitively, our knowledge\nof the domain tells us that these events inﬂuence Mary’s calling behavior only through their\neffect on the alarm. Also, given the state of the alarm, whether John calls has no inﬂuence on\nMary’s calling. Formally speaking, we believe that the following conditional independence\nstatement holds:\nP(MaryCalls | JohnCalls, Alarm, Earthquake, Burglary) = P(MaryCalls | Alarm) .\nThus, Alarm will be the only parent node for MaryCalls.\nBecause each node is connected only to earlier nodes, this construction method guaran-",
  "statement holds:\nP(MaryCalls | JohnCalls, Alarm, Earthquake, Burglary) = P(MaryCalls | Alarm) .\nThus, Alarm will be the only parent node for MaryCalls.\nBecause each node is connected only to earlier nodes, this construction method guaran-\ntees that the network is acyclic. Another important property of Bayesian networks is that they\ncontain no redundant probability values. If there is no redundancy, then there is no chance\nfor inconsistency: it is impossible for the knowledge engineer or domain expert to create a\nBayesian network that violates the axioms of probability.\nCompactness and node ordering\nAs well as being a complete and nonredundant representation of the domain, a Bayesian net-\nwork can often be far more compact than the full joint distribution. This property is what\nmakes it feasible to handle domains with many variables. The compactness of Bayesian net-\nworks is an example of a general property of locally structured (also called sparse) systems.\nLOCALLY\nSTRUCTURED\nSPARSE\nIn a locally structured system, each subcomponent interacts directly with only a bounded\nnumber of other components, regardless of the total number of components. Local structure\nis usually associated with linear rather than exponential growth in complexity. In the case of\nBayesian networks, it is reasonable to suppose that in most domains each random variable\nis directly inﬂuenced by at most k others, for some constant k. If we assume n Boolean\nvariables for simplicity, then the amount of information needed to specify each conditional\nprobability table will be at most 2k numbers, and the complete network can be speciﬁed by\nn2k numbers. In contrast, the joint distribution contains 2n numbers. To make this concrete,\nsuppose we have n = 30 nodes, each with ﬁve parents (k = 5). Then the Bayesian network\nrequires 960 numbers, but the full joint distribution requires over a billion.\nThere are domains in which each variable can be inﬂuenced directly by all the others,\nso that the network is fully connected. Then specifying the conditional probability tables re-\nquires the same amount of information as specifying the joint distribution. In some domains,\nthere will be slight dependencies that should strictly be included by adding a new link. But\nif these dependencies are tenuous, then it may not be worth the additional complexity in the\nnetwork for the small gain in accuracy. For example, one might object to our burglary net-",
  "if these dependencies are tenuous, then it may not be worth the additional complexity in the\nnetwork for the small gain in accuracy. For example, one might object to our burglary net-\nwork on the grounds that if there is an earthquake, then John and Mary would not call even\nif they heard the alarm, because they assume that the earthquake is the cause. Whether to\nadd the link from Earthquake to JohnCalls and MaryCalls (and thus enlarge the tables)\ndepends on comparing the importance of getting more accurate probabilities with the cost of\nspecifying the extra information. 516\nChapter\n14.\nProbabilistic Reasoning\nJohnCalls\nMaryCalls\nAlarm\nBurglary\nEarthquake\nMaryCalls\nAlarm\nEarthquake\nBurglary\nJohnCalls\n(a)\n(b)\nFigure 14.3\nNetwork structure depends on order of introduction. In each network, we\nhave introduced nodes in top-to-bottom order.\nEven in a locally structured domain, we will get a compact Bayesian network only if\nwe choose the node ordering well. What happens if we happen to choose the wrong or-\nder? Consider the burglary example again. Suppose we decide to add the nodes in the order\nMaryCalls, JohnCalls, Alarm, Burglary, Earthquake. We then get the somewhat more\ncomplicated network shown in Figure 14.3(a). The process goes as follows:\n• Adding MaryCalls: No parents.\n• Adding JohnCalls: If Mary calls, that probably means the alarm has gone off, which\nof course would make it more likely that John calls.\nTherefore, JohnCalls needs\nMaryCalls as a parent.\n• Adding Alarm: Clearly, if both call, it is more likely that the alarm has gone off than if\njust one or neither calls, so we need both MaryCalls and JohnCalls as parents.\n• Adding Burglary: If we know the alarm state, then the call from John or Mary might\ngive us information about our phone ringing or Mary’s music, but not about burglary:\nP(Burglary | Alarm, JohnCalls, MaryCalls) = P(Burglary | Alarm) .\nHence we need just Alarm as parent.\n• Adding Earthquake: If the alarm is on, it is more likely that there has been an earth-\nquake. (The alarm is an earthquake detector of sorts.) But if we know that there has\nbeen a burglary, then that explains the alarm, and the probability of an earthquake would\nbe only slightly above normal. Hence, we need both Alarm and Burglary as parents.\nThe resulting network has two more links than the original network in Figure 14.2 and re-\nquires three more probabilities to be speciﬁed. What’s worse, some of the links represent",
  "The resulting network has two more links than the original network in Figure 14.2 and re-\nquires three more probabilities to be speciﬁed. What’s worse, some of the links represent\ntenuous relationships that require difﬁcult and unnatural probability judgments, such as as- Section 14.2.\nThe Semantics of Bayesian Networks\n517\nsessing the probability of Earthquake, given Burglary and Alarm. This phenomenon is\nquite general and is related to the distinction between causal and diagnostic models intro-\nduced in Section 13.5.1 (see also Exercise 8.13). If we try to build a diagnostic model with\nlinks from symptoms to causes (as from MaryCalls to Alarm or Alarm to Burglary), we\nend up having to specify additional dependencies between otherwise independent causes (and\noften between separately occurring symptoms as well). If we stick to a causal model, we end\nup having to specify fewer numbers, and the numbers will often be easier to come up with. In\nthe domain of medicine, for example, it has been shown by Tversky and Kahneman (1982)\nthat expert physicians prefer to give probability judgments for causal rules rather than for\ndiagnostic ones.\nFigure 14.3(b) shows a very bad node ordering: MaryCalls, JohnCalls, Earthquake,\nBurglary, Alarm. This network requires 31 distinct probabilities to be speciﬁed—exactly the\nsame number as the full joint distribution. It is important to realize, however, that any of the\nthree networks can represent exactly the same joint distribution. The last two versions simply\nfail to represent all the conditional independence relationships and hence end up specifying a\nlot of unnecessary numbers instead.\n14.2.2\nConditional independence relations in Bayesian networks\nWe have provided a “numerical” semantics for Bayesian networks in terms of the represen-\ntation of the full joint distribution, as in Equation (14.2). Using this semantics to derive a\nmethod for constructing Bayesian networks, we were led to the consequence that a node is\nconditionally independent of its other predecessors, given its parents. It turns out that we\ncan also go in the other direction. We can start from a “topological” semantics that speciﬁes\nthe conditional independence relationships encoded by the graph structure, and from this we\ncan derive the “numerical” semantics. The topological semantics2 speciﬁes that each vari-\nable is conditionally independent of its non-descendants, given its parents. For example, in\nDESCENDANT",
  "can derive the “numerical” semantics. The topological semantics2 speciﬁes that each vari-\nable is conditionally independent of its non-descendants, given its parents. For example, in\nDESCENDANT\nFigure 14.2, JohnCalls is independent of Burglary, Earthquake, and MaryCalls given the\nvalue of Alarm. The deﬁnition is illustrated in Figure 14.4(a). From these conditional inde-\npendence assertions and the interpretation of the network parameters θ(Xi | Parents(Xi))\nas speciﬁcations of conditional probabilities P(Xi | Parents(Xi)), the full joint distribution\ngiven in Equation (14.2) can be reconstructed. In this sense, the “numerical” semantics and\nthe “topological” semantics are equivalent.\nAnother important independence property is implied by the topological semantics: a\nnode is conditionally independent of all other nodes in the network, given its parents, children,\nand children’s parents—that is, given its Markov blanket. (Exercise 14.7 asks you to prove\nMARKOV BLANKET\nthis.) For example, Burglary is independent of JohnCalls and MaryCalls, given Alarm and\nEarthquake. This property is illustrated in Figure 14.4(b).\n2 There is also a general topological criterion called d-separation for deciding whether a set of nodes X is\nconditionally independent of another set Y, given a third set Z. The criterion is rather complicated and is not\nneeded for deriving the algorithms in this chapter, so we omit it. Details may be found in Pearl (1988) or Darwiche\n(2009). Shachter (1998) gives a more intuitive method of ascertaining d-separation. 518\nChapter\n14.\nProbabilistic Reasoning\n. . .\n. . .\nU1\nX\nUm\nYn\nZnj\nY1\nZ1j\n. . .\n. . .\nU1\nUm\nYn\nZnj\nY1\nZ1j\nX\n(a)\n(b)\nFigure 14.4\n(a) A node X is conditionally independent of its non-descendants (e.g., the\nZijs) given its parents (the Uis shown in the gray area). (b) A node X is conditionally\nindependent of all other nodes in the network given its Markov blanket (the gray area).\n14.3\nEFFICIENT REPRESENTATION OF CONDITIONAL DISTRIBUTIONS\nEven if the maximum number of parents k is smallish, ﬁlling in the CPT for a node requires\nup to O(2k) numbers and perhaps a great deal of experience with all the possible conditioning\ncases. In fact, this is a worst-case scenario in which the relationship between the parents and\nthe child is completely arbitrary. Usually, such relationships are describable by a canonical\ndistribution that ﬁts some standard pattern. In such cases, the complete table can be speciﬁed\nCANONICAL\nDISTRIBUTION",
  "the child is completely arbitrary. Usually, such relationships are describable by a canonical\ndistribution that ﬁts some standard pattern. In such cases, the complete table can be speciﬁed\nCANONICAL\nDISTRIBUTION\nby naming the pattern and perhaps supplying a few parameters—much easier than supplying\nan exponential number of parameters.\nThe simplest example is provided by deterministic nodes. A deterministic node has\nDETERMINISTIC\nNODES\nits value speciﬁed exactly by the values of its parents, with no uncertainty. The relationship\ncan be a logical one: for example, the relationship between the parent nodes Canadian, US,\nMexican and the child node NorthAmerican is simply that the child is the disjunction of\nthe parents. The relationship can also be numerical: for example, if the parent nodes are\nthe prices of a particular model of car at several dealers and the child node is the price that\na bargain hunter ends up paying, then the child node is the minimum of the parent values;\nor if the parent nodes are a lake’s inﬂows (rivers, runoff, precipitation) and outﬂows (rivers,\nevaporation, seepage) and the child is the change in the water level of the lake, then the value\nof the child is the sum of the inﬂow parents minus the sum of the outﬂow parents.\nUncertain relationships can often be characterized by so-called noisy logical relation-\nships. The standard example is the noisy-OR relation, which is a generalization of the log-\nNOISY-OR\nical OR. In propositional logic, we might say that Fever is true if and only if Cold, Flu, or\nMalaria is true. The noisy-OR model allows for uncertainty about the ability of each par-\nent to cause the child to be true—the causal relationship between parent and child may be Section 14.3.\nEfﬁcient Representation of Conditional Distributions\n519\ninhibited, and so a patient could have a cold, but not exhibit a fever. The model makes two\nassumptions. First, it assumes that all the possible causes are listed. (If some are missing,\nwe can always add a so-called leak node that covers “miscellaneous causes.”) Second, it\nLEAK NODE\nassumes that inhibition of each parent is independent of inhibition of any other parents: for\nexample, whatever inhibits Malaria from causing a fever is independent of whatever inhibits\nFlu from causing a fever. Given these assumptions, Fever is false if and only if all its true\nparents are inhibited, and the probability of this is the product of the inhibition probabilities",
  "Flu from causing a fever. Given these assumptions, Fever is false if and only if all its true\nparents are inhibited, and the probability of this is the product of the inhibition probabilities\nq for each parent. Let us suppose these individual inhibition probabilities are as follows:\nqcold = P(¬fever | cold, ¬ﬂu, ¬malaria) = 0.6 ,\nqﬂu = P(¬fever | ¬cold, ﬂu, ¬malaria) = 0.2 ,\nqmalaria = P(¬fever | ¬cold, ¬ﬂu, malaria) = 0.1 .\nThen, from this information and the noisy-OR assumptions, the entire CPT can be built. The\ngeneral rule is that\nP(xi | parents(Xi)) = 1 −\n\u0019\n{j:Xj = true}\nqj ,\nwhere the product is taken over the parents that are set to true for that row of the CPT. The\nfollowing table illustrates this calculation:\nCold\nFlu\nMalaria P(Fever) P(¬Fever)\nF\nF\nF\n0.0\n1.0\nF\nF\nT\n0.9\n0.1\nF\nT\nF\n0.8\n0.2\nF\nT\nT\n0.98\n0.02 = 0.2 × 0.1\nT\nF\nF\n0.4\n0.6\nT\nF\nT\n0.94\n0.06 = 0.6 × 0.1\nT\nT\nF\n0.88\n0.12 = 0.6 × 0.2\nT\nT\nT\n0.988\n0.012 = 0.6 × 0.2 × 0.1\nIn general, noisy logical relationships in which a variable depends on k parents can be de-\nscribed using O(k) parameters instead of O(2k) for the full conditional probability table.\nThis makes assessment and learning much easier. For example, the CPCS network (Prad-\nhan et al., 1994) uses noisy-OR and noisy-MAX distributions to model relationships among\ndiseases and symptoms in internal medicine. With 448 nodes and 906 links, it requires only\n8,254 values instead of 133,931,430 for a network with full CPTs.\nBayesian nets with continuous variables\nMany real-world problems involve continuous quantities, such as height, mass, temperature,\nand money; in fact, much of statistics deals with random variables whose domains are contin-\nuous. By deﬁnition, continuous variables have an inﬁnite number of possible values, so it is\nimpossible to specify conditional probabilities explicitly for each value. One possible way to\nhandle continuous variables is to avoid them by using discretization—that is, dividing up the\nDISCRETIZATION 520\nChapter\n14.\nProbabilistic Reasoning\nHarvest\nSubsidy\nBuys\nCost\nFigure 14.5\nA simple network with discrete variables (Subsidy and Buys) and continuous\nvariables (Harvest and Cost).\npossible values into a ﬁxed set of intervals. For example, temperatures could be divided into\n(<0oC), (0oC−100oC), and (>100oC). Discretization is sometimes an adequate solution,\nbut often results in a considerable loss of accuracy and very large CPTs. The most com-",
  "(<0oC), (0oC−100oC), and (>100oC). Discretization is sometimes an adequate solution,\nbut often results in a considerable loss of accuracy and very large CPTs. The most com-\nmon solution is to deﬁne standard families of probability density functions (see Appendix A)\nthat are speciﬁed by a ﬁnite number of parameters. For example, a Gaussian (or normal)\nPARAMETER\ndistribution N(μ, σ2)(x) has the mean μ and the variance σ2 as parameters. Yet another\nsolution—sometimes called a nonparametric representation—is to deﬁne the conditional\nNONPARAMETRIC\ndistribution implicitly with a collection of instances, each containing speciﬁc values of the\nparent and child variables. We explore this approach further in Chapter 18.\nA network with both discrete and continuous variables is called a hybrid Bayesian\nnetwork. To specify a hybrid network, we have to specify two new kinds of distributions:\nHYBRID BAYESIAN\nNETWORK\nthe conditional distribution for a continuous variable given discrete or continuous parents;\nand the conditional distribution for a discrete variable given continuous parents. Consider the\nsimple example in Figure 14.5, in which a customer buys some fruit depending on its cost,\nwhich depends in turn on the size of the harvest and whether the government’s subsidy scheme\nis operating. The variable Cost is continuous and has continuous and discrete parents; the\nvariable Buys is discrete and has a continuous parent.\nFor the Cost variable, we need to specify P(Cost | Harvest, Subsidy). The discrete\nparent is handled by enumeration—that is, by specifying both P(Cost | Harvest, subsidy)\nand P(Cost | Harvest, ¬subsidy). To handle Harvest, we specify how the distribution over\nthe cost c depends on the continuous value h of Harvest. In other words, we specify the\nparameters of the cost distribution as a function of h. The most common choice is the linear\nGaussian distribution, in which the child has a Gaussian distribution whose mean μ varies\nLINEAR GAUSSIAN\nlinearly with the value of the parent and whose standard deviation σ is ﬁxed. We need two\ndistributions, one for subsidy and one for ¬subsidy, with different parameters:\nP(c | h, subsidy) = N(ath + bt, σ2\nt )(c) =\n1\nσt\n√\n2π e−1\n2\n“ c−(ath+bt)\nσt\n”2\nP(c | h, ¬subsidy) = N(afh + bf, σ2\nf)(c) =\n1\nσf\n√\n2π e\n−1\n2\n„\nc−(af h+bf )\nσf\n«2\n.\nFor this example, then, the conditional distribution for Cost is speciﬁed by naming the linear",
  "t )(c) =\n1\nσt\n√\n2π e−1\n2\n“ c−(ath+bt)\nσt\n”2\nP(c | h, ¬subsidy) = N(afh + bf, σ2\nf)(c) =\n1\nσf\n√\n2π e\n−1\n2\n„\nc−(af h+bf )\nσf\n«2\n.\nFor this example, then, the conditional distribution for Cost is speciﬁed by naming the linear\nGaussian distribution and providing the parameters at, bt, σt, af, bf, and σf. Figures 14.6(a) Section 14.3.\nEfﬁcient Representation of Conditional Distributions\n521\n0 2 4 6 8 10\nCost c\n024681012\nHarvest h\n0\n0.1\n0.2\n0.3\n0.4\nP(c | h, subsidy)\n0 2 4 6 8 10\nCost c\n024681012\nHarvest h\n0\n0.1\n0.2\n0.3\n0.4\nP(c | h, ¬subsidy)\n0 2 4 6 8 10\nCost c\n024681012\nHarvest h\n0\n0.1\n0.2\n0.3\n0.4\nP(c | h)\n(a)\n(b)\n(c)\nFigure 14.6\nThe graphs in (a) and (b) show the probability distribution over Cost as a\nfunction of Harvest size, with Subsidy true and false, respectively. Graph (c) shows the\ndistribution P(Cost | Harvest), obtained by summing over the two subsidy cases.\nand (b) show these two relationships. Notice that in each case the slope is negative, because\ncost decreases as supply increases. (Of course, the assumption of linearity implies that the\ncost becomes negative at some point; the linear model is reasonable only if the harvest size is\nlimited to a narrow range.) Figure 14.6(c) shows the distribution P(c | h), averaging over the\ntwo possible values of Subsidy and assuming that each has prior probability 0.5. This shows\nthat even with very simple models, quite interesting distributions can be represented.\nThe linear Gaussian conditional distribution has some special properties. A network\ncontaining only continuous variables with linear Gaussian distributions has a joint distribu-\ntion that is a multivariate Gaussian distribution (see Appendix A) over all the variables (Exer-\ncise 14.9). Furthermore, the posterior distribution given any evidence also has this property.3\nWhen discrete variables are added as parents (not as children) of continuous variables, the\nnetwork deﬁnes a conditional Gaussian, or CG, distribution: given any assignment to the\nCONDITIONAL\nGAUSSIAN\ndiscrete variables, the distribution over the continuous variables is a multivariate Gaussian.\nNow we turn to the distributions for discrete variables with continuous parents. Con-\nsider, for example, the Buys node in Figure 14.5. It seems reasonable to assume that the\ncustomer will buy if the cost is low and will not buy if it is high and that the probability of\nbuying varies smoothly in some intermediate region. In other words, the conditional distribu-",
  "customer will buy if the cost is low and will not buy if it is high and that the probability of\nbuying varies smoothly in some intermediate region. In other words, the conditional distribu-\ntion is like a “soft” threshold function. One way to make soft thresholds is to use the integral\nof the standard normal distribution:\nΦ(x) =\n\u001a x\n−∞\nN(0, 1)(x)dx .\nThen the probability of Buys given Cost might be\nP(buys | Cost = c) = Φ((−c + μ)/σ) ,\nwhich means that the cost threshold occurs around μ, the width of the threshold region is pro-\nportional to σ, and the probability of buying decreases as cost increases. This probit distri-\n3 It follows that inference in linear Gaussian networks takes only O(n3) time in the worst case, regardless of the\nnetwork topology. In Section 14.4, we see that inference for networks of discrete variables is NP-hard. 522\nChapter\n14.\nProbabilistic Reasoning\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0\n 2\n 4\n 6\n 8\n 10\n 12\nP(c)\nCost c\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0\n 2\n 4\n 6\n 8\n 10\n 12\nP(buys | c)\nCost c\nLogit\nProbit\n(a)\n(b)\nFigure 14.7\n(a) A normal (Gaussian) distribution for the cost threshold, centered on\nμ = 6.0 with standard deviation σ = 1.0. (b) Logit and probit distributions for the probability\nof buys given cost, for the parameters μ = 6.0 and σ = 1.0.\nbution (pronounced “pro-bit” and short for “probability unit”) is illustrated in Figure 14.7(a).\nPROBIT\nDISTRIBUTION\nThe form can be justiﬁed by proposing that the underlying decision process has a hard thresh-\nold, but that the precise location of the threshold is subject to random Gaussian noise.\nAn alternative to the probit model is the logit distribution (pronounced “low-jit”). It\nLOGIT DISTRIBUTION\nuses the logistic function 1/(1 + e−x) to produce a soft threshold:\nLOGISTIC FUNCTION\nP(buys | Cost = c) =\n1\n1 + exp(−2−c+μ\nσ\n) .\nThis is illustrated in Figure 14.7(b). The two distributions look similar, but the logit actually\nhas much longer “tails.” The probit is often a better ﬁt to real situations, but the logit is some-\ntimes easier to deal with mathematically. It is used widely in neural networks (Chapter 20).\nBoth probit and logit can be generalized to handle multiple continuous parents by taking a\nlinear combination of the parent values.\n14.4\nEXACT INFERENCE IN BAYESIAN NETWORKS\nThe basic task for any probabilistic inference system is to compute the posterior probability\ndistribution for a set of query variables, given some observed event—that is, some assign-\nEVENT",
  "14.4\nEXACT INFERENCE IN BAYESIAN NETWORKS\nThe basic task for any probabilistic inference system is to compute the posterior probability\ndistribution for a set of query variables, given some observed event—that is, some assign-\nEVENT\nment of values to a set of evidence variables. To simplify the presentation, we will consider\nonly one query variable at a time; the algorithms can easily be extended to queries with mul-\ntiple variables. We will use the notation from Chapter 13: X denotes the query variable; E\ndenotes the set of evidence variables E1, . . . , Em, and e is a particular observed event; Y will\ndenotes the nonevidence, nonquery variables Y1, . . . , Yl (called the hidden variables). Thus,\nHIDDEN VARIABLE\nthe complete set of variables is X = {X} ∪E ∪Y. A typical query asks for the posterior\nprobability distribution P(X | e). Section 14.4.\nExact Inference in Bayesian Networks\n523\nIn the burglary network, we might observe the event in which JohnCalls = true and\nMaryCalls = true. We could then ask for, say, the probability that a burglary has occurred:\nP(Burglary | JohnCalls = true, MaryCalls = true) = ⟨0.284, 0.716⟩.\nIn this section we discuss exact algorithms for computing posterior probabilities and will\nconsider the complexity of this task. It turns out that the general case is intractable, so Sec-\ntion 14.5 covers methods for approximate inference.\n14.4.1\nInference by enumeration\nChapter 13 explained that any conditional probability can be computed by summing terms\nfrom the full joint distribution. More speciﬁcally, a query P(X | e) can be answered using\nEquation (13.9), which we repeat here for convenience:\nP(X | e) = α P(X, e) = α\n\f\ny\nP(X, e, y) .\nNow, a Bayesian network gives a complete representation of the full joint distribution. More\nspeciﬁcally, Equation (14.2) on page 513 shows that the terms P(x, e, y) in the joint distri-\nbution can be written as products of conditional probabilities from the network. Therefore, a\nquery can be answered using a Bayesian network by computing sums of products of condi-\ntional probabilities from the network.\nConsider the query P(Burglary | JohnCalls = true, MaryCalls = true). The hidden\nvariables for this query are Earthquake and Alarm. From Equation (13.9), using initial\nletters for the variables to shorten the expressions, we have4\nP(B | j, m) = α P(B, j, m) = α\n\f\ne\n\f\na\nP(B, j, m, e, a, ) .\nThe semantics of Bayesian networks (Equation (14.2)) then gives us an expression in terms",
  "letters for the variables to shorten the expressions, we have4\nP(B | j, m) = α P(B, j, m) = α\n\f\ne\n\f\na\nP(B, j, m, e, a, ) .\nThe semantics of Bayesian networks (Equation (14.2)) then gives us an expression in terms\nof CPT entries. For simplicity, we do this just for Burglary = true:\nP(b | j, m) = α\n\f\ne\n\f\na\nP(b)P(e)P(a | b, e)P(j | a)P(m | a) .\nTo compute this expression, we have to add four terms, each computed by multiplying ﬁve\nnumbers. In the worst case, where we have to sum out almost all the variables, the complexity\nof the algorithm for a network with n Boolean variables is O(n2n).\nAn improvement can be obtained from the following simple observations: the P(b)\nterm is a constant and can be moved outside the summations over a and e, and the P(e) term\ncan be moved outside the summation over a. Hence, we have\nP(b | j, m) = α P(b)\n\f\ne\nP(e)\n\f\na\nP(a | b, e)P(j | a)P(m | a) .\n(14.4)\nThis expression can be evaluated by looping through the variables in order, multiplying CPT\nentries as we go. For each summation, we also need to loop over the variable’s possible\n4 An expression such as P\ne P(a, e) means to sum P(A = a, E = e) for all possible values of e. When E is\nBoolean, there is an ambiguity in that P(e) is used to mean both P(E = true) and P(E = e), but it should be\nclear from context which is intended; in particular, in the context of a sum the latter is intended. 524\nChapter\n14.\nProbabilistic Reasoning\nvalues. The structure of this computation is shown in Figure 14.8. Using the numbers from\nFigure 14.2, we obtain P(b | j, m) = α × 0.00059224. The corresponding computation for\n¬b yields α × 0.0014919; hence,\nP(B | j, m) = α ⟨0.00059224, 0.0014919⟩≈⟨0.284, 0.716⟩.\nThat is, the chance of a burglary, given calls from both neighbors, is about 28%.\nThe evaluation process for the expression in Equation (14.4) is shown as an expression\ntree in Figure 14.8. The ENUMERATION-ASK algorithm in Figure 14.9 evaluates such trees\nusing depth-ﬁrst recursion. The algorithm is very similar in structure to the backtracking al-\ngorithm for solving CSPs (Figure 6.5) and the DPLL algorithm for satisﬁability (Figure 7.17).\nThe space complexity of ENUMERATION-ASK is only linear in the number of variables:\nthe algorithm sums over the full joint distribution without ever constructing it explicitly. Un-\nfortunately, its time complexity for a network with n Boolean variables is always O(2n)—\nbetter than the O(n 2n) for the simple approach described earlier, but still rather grim.",
  "fortunately, its time complexity for a network with n Boolean variables is always O(2n)—\nbetter than the O(n 2n) for the simple approach described earlier, but still rather grim.\nNote that the tree in Figure 14.8 makes explicit the repeated subexpressions evalu-\nated by the algorithm. The products P(j | a)P(m | a) and P(j | ¬a)P(m | ¬a) are computed\ntwice, once for each value of e. The next section describes a general method that avoids such\nwasted computations.\n14.4.2\nThe variable elimination algorithm\nThe enumeration algorithm can be improved substantially by eliminating repeated calcula-\ntions of the kind illustrated in Figure 14.8. The idea is simple: do the calculation once and\nsave the results for later use. This is a form of dynamic programming. There are several ver-\nsions of this approach; we present the variable elimination algorithm, which is the simplest.\nVARIABLE\nELIMINATION\nVariable elimination works by evaluating expressions such as Equation (14.4) in right-to-left\norder (that is, bottom up in Figure 14.8). Intermediate results are stored, and summations over\neach variable are done only for those portions of the expression that depend on the variable.\nLet us illustrate this process for the burglary network. We evaluate the expression\nP(B | j, m) = α P(B)\n\u001b \u001c\u001d \u001e\nf1(B)\n\f\ne\nP(e)\n\u001b\u001c\u001d\u001e\nf2(E)\n\f\na\nP(a | B, e)\n\u001b\n\u001c\u001d\n\u001e\nf3(A,B,E)\nP(j | a)\n\u001b \u001c\u001d \u001e\nf4(A)\nP(m | a)\n\u001b\n\u001c\u001d\n\u001e\nf5(A)\n.\nNotice that we have annotated each part of the expression with the name of the corresponding\nfactor; each factor is a matrix indexed by the values of its argument variables. For example,\nFACTOR\nthe factors f4(A) and f5(A) corresponding to P(j | a) and P(m | a) depend just on A because\nJ and M are ﬁxed by the query. They are therefore two-element vectors:\nf4(A) =\n\r P(j | a)\nP(j | ¬a)\n\u000e\n=\n\r 0.90\n0.05\n\u000e\nf5(A) =\n\r P(m | a)\nP(m | ¬a)\n\u000e\n=\n\r 0.70\n0.01\n\u000e\n.\nf3(A, B, E) will be a 2 × 2 × 2 matrix, which is hard to show on the printed page. (The “ﬁrst”\nelement is given by P(a | b, e) = 0.95 and the “last” by P(¬a | ¬b, ¬e) = 0.999.) In terms of\nfactors, the query expression is written as\nP(B | j, m) = α f1(B) ×\n\f\ne\nf2(E) ×\n\f\na\nf3(A, B, E) × f4(A) × f5(A) Section 14.4.\nExact Inference in Bayesian Networks\n525\nP(j|a)\n.90\nP(m|a)\n.70\n.01\nP(m|¬a)\n.05\nP( j|¬a)\nP( j|a)\n.90\nP(m|a)\n.70\n.01\nP(m|¬a)\n.05\nP( j|¬a)\nP(b)\n.001\nP(e)\n.002\nP(¬e)\n.998\nP(a|b,e)\n.95\n.06\nP(¬a|b,¬e)\n.05\nP(¬a|b,e)\n.94\nP(a|b,¬e)\nFigure 14.8\nThe structure of the expression shown in Equation (14.4). The evaluation",
  ".70\n.01\nP(m|¬a)\n.05\nP( j|¬a)\nP( j|a)\n.90\nP(m|a)\n.70\n.01\nP(m|¬a)\n.05\nP( j|¬a)\nP(b)\n.001\nP(e)\n.002\nP(¬e)\n.998\nP(a|b,e)\n.95\n.06\nP(¬a|b,¬e)\n.05\nP(¬a|b,e)\n.94\nP(a|b,¬e)\nFigure 14.8\nThe structure of the expression shown in Equation (14.4). The evaluation\nproceeds top down, multiplying values along each path and summing at the “+” nodes. Notice\nthe repetition of the paths for j and m.\nfunction ENUMERATION-ASK(X ,e,bn) returns a distribution over X\ninputs: X , the query variable\ne, observed values for variables E\nbn, a Bayes net with variables {X} ∪E ∪Y\n/* Y = hidden variables */\nQ(X ) ←a distribution over X , initially empty\nfor each value xi of X do\nQ(xi) ←ENUMERATE-ALL(bn.VARS,exi)\nwhere exi is e extended with X = xi\nreturn NORMALIZE(Q(X))\nfunction ENUMERATE-ALL(vars,e) returns a real number\nif EMPTY?(vars) then return 1.0\nY ←FIRST(vars)\nif Y has value y in e\nthen return P(y | parents(Y )) × ENUMERATE-ALL(REST(vars),e)\nelse return \u0002\ny P(y | parents(Y )) × ENUMERATE-ALL(REST(vars),ey)\nwhere ey is e extended with Y = y\nFigure 14.9\nThe enumeration algorithm for answering queries on Bayesian networks. 526\nChapter\n14.\nProbabilistic Reasoning\nwhere the “×” operator is not ordinary matrix multiplication but instead the pointwise prod-\nuct operation, to be described shortly.\nPOINTWISE\nPRODUCT\nThe process of evaluation is a process of summing out variables (right to left) from\npointwise products of factors to produce new factors, eventually yielding a factor that is the\nsolution, i.e., the posterior distribution over the query variable. The steps are as follows:\n• First, we sum out A from the product of f3, f4, and f5. This gives us a new 2 × 2 factor\nf6(B, E) whose indices range over just B and E:\nf6(B, E) =\n\f\na\nf3(A, B, E) × f4(A) × f5(A)\n= (f3(a, B, E) × f4(a) × f5(a)) + (f3(¬a, B, E) × f4(¬a) × f5(¬a)) .\nNow we are left with the expression\nP(B | j, m) = α f1(B) ×\n\f\ne\nf2(E) × f6(B, E) .\n• Next, we sum out E from the product of f2 and f6:\nf7(B) =\n\f\ne\nf2(E) × f6(B, E)\n= f2(e) × f6(B, e) + f2(¬e) × f6(B, ¬e) .\nThis leaves the expression\nP(B | j, m) = α f1(B) × f7(B)\nwhich can be evaluated by taking the pointwise product and normalizing the result.\nExamining this sequence, we see that two basic computational operations are required: point-\nwise product of a pair of factors, and summing out a variable from a product of factors. The\nnext section describes each of these operations.\nOperations on factors",
  "wise product of a pair of factors, and summing out a variable from a product of factors. The\nnext section describes each of these operations.\nOperations on factors\nThe pointwise product of two factors f1 and f2 yields a new factor f whose variables are\nthe union of the variables in f1 and f2 and whose elements are given by the product of the\ncorresponding elements in the two factors. Suppose the two factors have variables Y1, . . . , Yk\nin common. Then we have\nf(X1 . . . Xj, Y1 . . . Yk, Z1 . . . Zl) = f1(X1 . . . Xj, Y1 . . . Yk) f2(Y1 . . . Yk, Z, . . . Zl).\nIf all the variables are binary, then f1 and f2 have 2j+k and 2k+l entries, respectively, and\nthe pointwise product has 2j+k+l entries.\nFor example, given two factors f1(A, B) and\nf2(B, C), the pointwise product f1 × f2 = f3(A, B, C) has 21+1+1 = 8 entries, as illustrated\nin Figure 14.10. Notice that the factor resulting from a pointwise product can contain more\nvariables than any of the factors being multiplied and that the size of a factor is exponential in\nthe number of variables. This is where both space and time complexity arise in the variable\nelimination algorithm. Section 14.4.\nExact Inference in Bayesian Networks\n527\nA\nB\nf1(A, B)\nB\nC\nf2(B, C)\nA\nB\nC\nf3(A, B, C)\nT\nT\n.3\nT\nT\n.2\nT\nT\nT\n.3 × .2 = .06\nT\nF\n.7\nT\nF\n.8\nT\nT\nF\n.3 × .8 = .24\nF\nT\n.9\nF\nT\n.6\nT\nF\nT\n.7 × .6 = .42\nF\nF\n.1\nF\nF\n.4\nT\nF\nF\n.7 × .4 = .28\nF\nT\nT\n.9 × .2 = .18\nF\nT\nF\n.9 × .8 = .72\nF\nF\nT\n.1 × .6 = .06\nF\nF\nF\n.1 × .4 = .04\nFigure 14.10\nIllustrating pointwise multiplication: f1(A, B) × f2(B, C) = f3(A, B, C).\nSumming out a variable from a product of factors is done by adding up the submatrices\nformed by ﬁxing the variable to each of its values in turn. For example, to sum out A from\nf3(A, B, C), we write\nf(B, C) =\n\f\na\nf3(A, B, C) = f3(a, B, C) + f3(¬a, B, C)\n=\n\r .06 .24\n.42 .28\n\u000e\n+\n\r .18 .72\n.06 .04\n\u000e\n=\n\r .24 .96\n.48 .32\n\u000e\n.\nThe only trick is to notice that any factor that does not depend on the variable to be summed\nout can be moved outside the summation. For example, if we were to sum out E ﬁrst in the\nburglary network, the relevant part of the expression would be\n\f\ne\nf2(E) × f3(A, B, E) × f4(A) × f5(A) = f4(A) × f5(A) ×\n\f\ne\nf2(E) × f3(A, B, E) .\nNow the pointwise product inside the summation is computed, and the variable is summed\nout of the resulting matrix.\nNotice that matrices are not multiplied until we need to sum out a variable from the\naccumulated product. At that point, we multiply just those matrices that include the variable",
  "out of the resulting matrix.\nNotice that matrices are not multiplied until we need to sum out a variable from the\naccumulated product. At that point, we multiply just those matrices that include the variable\nto be summed out. Given functions for pointwise product and summing out, the variable\nelimination algorithm itself can be written quite simply, as shown in Figure 14.11.\nVariable ordering and variable relevance\nThe algorithm in Figure 14.11 includes an unspeciﬁed ORDER function to choose an ordering\nfor the variables. Every choice of ordering yields a valid algorithm, but different orderings\ncause different intermediate factors to be generated during the calculation. For example, in\nthe calculation shown previously, we eliminated A before E; if we do it the other way, the\ncalculation becomes\nP(B | j, m) = α f1(B) ×\n\f\na\nf4(A) × f5(A) ×\n\f\ne\nf2(E) × f3(A, B, E) ,\nduring which a new factor f6(A, B) will be generated.\nIn general, the time and space requirements of variable elimination are dominated by\nthe size of the largest factor constructed during the operation of the algorithm. This in turn 528\nChapter\n14.\nProbabilistic Reasoning\nfunction ELIMINATION-ASK(X ,e,bn) returns a distribution over X\ninputs: X , the query variable\ne, observed values for variables E\nbn, a Bayesian network specifying joint distribution P(X1, . . . , Xn)\nfactors ←[ ]\nfor each var in ORDER(bn.VARS) do\nfactors ←[MAKE-FACTOR(var, e)|factors]\nif var is a hidden variable then factors ←SUM-OUT(var,factors)\nreturn NORMALIZE(POINTWISE-PRODUCT(factors))\nFigure 14.11\nThe variable elimination algorithm for inference in Bayesian networks.\nis determined by the order of elimination of variables and by the structure of the network.\nIt turns out to be intractable to determine the optimal ordering, but several good heuristics\nare available. One fairly effective method is a greedy one: eliminate whichever variable\nminimizes the size of the next factor to be constructed.\nLet us consider one more query: P(JohnCalls | Burglary = true). As usual, the ﬁrst\nstep is to write out the nested summation:\nP(J | b) = α P(b)\n\f\ne\nP(e)\n\f\na\nP(a | b, e)P(J | a)\n\f\nm\nP(m | a) .\nEvaluating this expression from right to left, we notice something interesting: \u0002\nm P(m | a)\nis equal to 1 by deﬁnition! Hence, there was no need to include it in the ﬁrst place; the vari-\nable M is irrelevant to this query. Another way of saying this is that the result of the query",
  "m P(m | a)\nis equal to 1 by deﬁnition! Hence, there was no need to include it in the ﬁrst place; the vari-\nable M is irrelevant to this query. Another way of saying this is that the result of the query\nP(JohnCalls | Burglary = true) is unchanged if we remove MaryCalls from the network\naltogether. In general, we can remove any leaf node that is not a query variable or an evidence\nvariable. After its removal, there may be some more leaf nodes, and these too may be irrele-\nvant. Continuing this process, we eventually ﬁnd that every variable that is not an ancestor\nof a query variable or evidence variable is irrelevant to the query. A variable elimination\nalgorithm can therefore remove all these variables before evaluating the query.\n14.4.3\nThe complexity of exact inference\nThe complexity of exact inference in Bayesian networks depends strongly on the structure of\nthe network. The burglary network of Figure 14.2 belongs to the family of networks in which\nthere is at most one undirected path between any two nodes in the network. These are called\nsingly connected networks or polytrees, and they have a particularly nice property: The time\nSINGLY CONNECTED\nPOLYTREE\nand space complexity of exact inference in polytrees is linear in the size of the network. Here,\nthe size is deﬁned as the number of CPT entries; if the number of parents of each node is\nbounded by a constant, then the complexity will also be linear in the number of nodes.\nFor multiply connected networks, such as that of Figure 14.12(a), variable elimination\nMULTIPLY\nCONNECTED\ncan have exponential time and space complexity in the worst case, even when the number\nof parents per node is bounded. This is not surprising when one considers that because it Section 14.4.\nExact Inference in Bayesian Networks\n529\nP(C)=.5\nC\nP(R)\nt\nf\n.80\n.20\nC\nP(S)\nt\nf\n.10\n.50\nS\nR\nt\nt\nt\nf\nf\nt\nf\nf\n.90\n.90\n.00\n.99\nCloudy\nRain\nSprinkler\n Wet\nGrass\nP(W)\nP(C)=.5\nt\nf\n.08 .02 .72 .18\nP(S+R=x)\nS+R\nP(W)\nt t\nt f\nf t\nf f\n.90\n.90\n.00\n.99\nCloudy\nSpr+Rain\n Wet\nGrass\n.10 .40 .10 .40\nC\nt t\nt f\nf t\nf f\n(a)\n(b)\nFigure 14.12\n(a) A multiply connected network with conditional probability tables. (b) A\nclustered equivalent of the multiply connected network.\nincludes inference in propositional logic as a special case, inference in Bayesian networks is\nNP-hard. In fact, it can be shown (Exercise 14.16) that the problem is as hard as that of com-\nputing the number of satisfying assignments for a propositional logic formula. This means",
  "NP-hard. In fact, it can be shown (Exercise 14.16) that the problem is as hard as that of com-\nputing the number of satisfying assignments for a propositional logic formula. This means\nthat it is #P-hard (“number-P hard”)—that is, strictly harder than NP-complete problems.\nThere is a close connection between the complexity of Bayesian network inference and\nthe complexity of constraint satisfaction problems (CSPs). As we discussed in Chapter 6,\nthe difﬁculty of solving a discrete CSP is related to how “treelike” its constraint graph is.\nMeasures such as tree width, which bound the complexity of solving a CSP, can also be\napplied directly to Bayesian networks. Moreover, the variable elimination algorithm can be\ngeneralized to solve CSPs as well as Bayesian networks.\n14.4.4\nClustering algorithms\nThe variable elimination algorithm is simple and efﬁcient for answering individual queries. If\nwe want to compute posterior probabilities for all the variables in a network, however, it can\nbe less efﬁcient. For example, in a polytree network, one would need to issue O(n) queries\ncosting O(n) each, for a total of O(n2) time. Using clustering algorithms (also known as\nCLUSTERING\njoin tree algorithms), the time can be reduced to O(n). For this reason, these algorithms are\nJOIN TREE\nwidely used in commercial Bayesian network tools.\nThe basic idea of clustering is to join individual nodes of the network to form clus-\nter nodes in such a way that the resulting network is a polytree. For example, the multiply\nconnected network shown in Figure 14.12(a) can be converted into a polytree by combin-\ning the Sprinkler and Rain node into a cluster node called Sprinkler+Rain, as shown in\nFigure 14.12(b). The two Boolean nodes are replaced by a “meganode” that takes on four\npossible values: tt, tf, ft, and ff. The meganode has only one parent, the Boolean variable\nCloudy, so there are two conditioning cases. Although this example doesn’t show it, the\nprocess of clustering often produces meganodes that share some variables. 530\nChapter\n14.\nProbabilistic Reasoning\nOnce the network is in polytree form, a special-purpose inference algorithm is required,\nbecause ordinary inference methods cannot handle meganodes that share variables with each\nother. Essentially, the algorithm is a form of constraint propagation (see Chapter 6) where the\nconstraints ensure that neighboring meganodes agree on the posterior probability of any vari-",
  "other. Essentially, the algorithm is a form of constraint propagation (see Chapter 6) where the\nconstraints ensure that neighboring meganodes agree on the posterior probability of any vari-\nables that they have in common. With careful bookkeeping, this algorithm is able to compute\nposterior probabilities for all the nonevidence nodes in the network in time linear in the size\nof the clustered network. However, the NP-hardness of the problem has not disappeared: if a\nnetwork requires exponential time and space with variable elimination, then the CPTs in the\nclustered network will necessarily be exponentially large.\n14.5\nAPPROXIMATE INFERENCE IN BAYESIAN NETWORKS\nGiven the intractability of exact inference in large, multiply connected networks, it is essen-\ntial to consider approximate inference methods. This section describes randomized sampling\nalgorithms, also called Monte Carlo algorithms, that provide approximate answers whose\nMONTE CARLO\naccuracy depends on the number of samples generated. Monte Carlo algorithms, of which\nsimulated annealing (page 126) is an example, are used in many branches of science to es-\ntimate quantities that are difﬁcult to calculate exactly. In this section, we are interested in\nsampling applied to the computation of posterior probabilities. We describe two families of\nalgorithms: direct sampling and Markov chain sampling. Two other approaches—variational\nmethods and loopy propagation—are mentioned in the notes at the end of the chapter.\n14.5.1\nDirect sampling methods\nThe primitive element in any sampling algorithm is the generation of samples from a known\nprobability distribution. For example, an unbiased coin can be thought of as a random variable\nCoin with values ⟨heads, tails⟩and a prior distribution P(Coin) = ⟨0.5, 0.5⟩. Sampling\nfrom this distribution is exactly like ﬂipping the coin: with probability 0.5 it will return heads,\nand with probability 0.5 it will return tails. Given a source of random numbers uniformly\ndistributed in the range [0, 1], it is a simple matter to sample any distribution on a single\nvariable, whether discrete or continuous. (See Exercise 14.17.)\nThe simplest kind of random sampling process for Bayesian networks generates events\nfrom a network that has no evidence associated with it. The idea is to sample each variable\nin turn, in topological order. The probability distribution from which the value is sampled is",
  "from a network that has no evidence associated with it. The idea is to sample each variable\nin turn, in topological order. The probability distribution from which the value is sampled is\nconditioned on the values already assigned to the variable’s parents. This algorithm is shown\nin Figure 14.13. We can illustrate its operation on the network in Figure 14.12(a), assuming\nan ordering [Cloudy, Sprinkler, Rain, WetGrass]:\n1. Sample from P(Cloudy) = ⟨0.5, 0.5⟩, value is true.\n2. Sample from P(Sprinkler | Cloudy = true) = ⟨0.1, 0.9⟩, value is false.\n3. Sample from P(Rain | Cloudy = true) = ⟨0.8, 0.2⟩, value is true.\n4. Sample from P(WetGrass | Sprinkler = false, Rain = true) = ⟨0.9, 0.1⟩, value is true.\nIn this case, PRIOR-SAMPLE returns the event [true, false, true, true]. Section 14.5.\nApproximate Inference in Bayesian Networks\n531\nfunction PRIOR-SAMPLE(bn) returns an event sampled from the prior speciﬁed by bn\ninputs: bn, a Bayesian network specifying joint distribution P(X1, . . . , Xn)\nx ←an event with n elements\nforeach variable Xi in X1, . . . , Xn do\nx[i] ←a random sample from P(Xi | parents(Xi))\nreturn x\nFigure 14.13\nA sampling algorithm that generates events from a Bayesian network. Each\nvariable is sampled according to the conditional distribution given the values already sampled\nfor the variable’s parents.\nIt is easy to see that PRIOR-SAMPLE generates samples from the prior joint distribution\nspeciﬁed by the network. First, let SPS(x1, . . . , xn) be the probability that a speciﬁc event is\ngenerated by the PRIOR-SAMPLE algorithm. Just looking at the sampling process, we have\nSPS(x1 . . . xn) =\nn\n\u0019\ni = 1\nP(xi | parents(Xi))\nbecause each sampling step depends only on the parent values. This expression should look\nfamiliar, because it is also the probability of the event according to the Bayesian net’s repre-\nsentation of the joint distribution, as stated in Equation (14.2). That is, we have\nSPS(x1 . . . xn) = P(x1 . . . xn) .\nThis simple fact makes it easy to answer questions by using samples.\nIn any sampling algorithm, the answers are computed by counting the actual samples\ngenerated. Suppose there are N total samples, and let NPS(x1, . . . , xn) be the number of\ntimes the speciﬁc event x1, . . . , xn occurs in the set of samples. We expect this number, as a\nfraction of the total, to converge in the limit to its expected value according to the sampling\nprobability:\nlim\nN→∞\nNPS(x1, . . . , xn)\nN\n= SPS(x1, . . . , xn) = P(x1, . . . , xn) .\n(14.5)",
  "fraction of the total, to converge in the limit to its expected value according to the sampling\nprobability:\nlim\nN→∞\nNPS(x1, . . . , xn)\nN\n= SPS(x1, . . . , xn) = P(x1, . . . , xn) .\n(14.5)\nFor example, consider the event produced earlier: [true, false, true, true]. The sampling\nprobability for this event is\nSPS(true, false, true, true) = 0.5 × 0.9 × 0.8 × 0.9 = 0.324 .\nHence, in the limit of large N, we expect 32.4% of the samples to be of this event.\nWhenever we use an approximate equality (“≈”) in what follows, we mean it in exactly\nthis sense—that the estimated probability becomes exact in the large-sample limit. Such an\nestimate is called consistent. For example, one can produce a consistent estimate of the\nCONSISTENT\nprobability of any partially speciﬁed event x1, . . . , xm, where m ≤n, as follows:\nP(x1, . . . , xm) ≈NPS(x1, . . . , xm)/N .\n(14.6)\nThat is, the probability of the event can be estimated as the fraction of all complete events\ngenerated by the sampling process that match the partially speciﬁed event. For example, if 532\nChapter\n14.\nProbabilistic Reasoning\nwe generate 1000 samples from the sprinkler network, and 511 of them have Rain = true,\nthen the estimated probability of rain, written as ˆP(Rain = true), is 0.511.\nRejection sampling in Bayesian networks\nRejection sampling is a general method for producing samples from a hard-to-sample distri-\nREJECTION\nSAMPLING\nbution given an easy-to-sample distribution. In its simplest form, it can be used to compute\nconditional probabilities—that is, to determine P(X | e). The REJECTION-SAMPLING algo-\nrithm is shown in Figure 14.14. First, it generates samples from the prior distribution speciﬁed\nby the network. Then, it rejects all those that do not match the evidence. Finally, the estimate\nˆP(X = x | e) is obtained by counting how often X = x occurs in the remaining samples.\nLet ˆP(X | e) be the estimated distribution that the algorithm returns. From the deﬁnition\nof the algorithm, we have\nˆP(X | e) = α NPS(X, e) = NPS(X, e)\nNPS(e)\n.\nFrom Equation (14.6), this becomes\nˆP(X | e) ≈P(X, e)\nP(e)\n= P(X | e) .\nThat is, rejection sampling produces a consistent estimate of the true probability.\nContinuing with our example from Figure 14.12(a), let us assume that we wish to esti-\nmate P(Rain | Sprinkler = true), using 100 samples. Of the 100 that we generate, suppose\nthat 73 have Sprinkler = false and are rejected, while 27 have Sprinkler = true; of the 27,",
  "mate P(Rain | Sprinkler = true), using 100 samples. Of the 100 that we generate, suppose\nthat 73 have Sprinkler = false and are rejected, while 27 have Sprinkler = true; of the 27,\n8 have Rain = true and 19 have Rain = false. Hence,\nP(Rain | Sprinkler = true) ≈NORMALIZE(⟨8, 19⟩) = ⟨0.296, 0.704⟩.\nThe true answer is ⟨0.3, 0.7⟩. As more samples are collected, the estimate will converge to\nthe true answer. The standard deviation of the error in each probability will be proportional\nto 1/√n, where n is the number of samples used in the estimate.\nThe biggest problem with rejection sampling is that it rejects so many samples! The\nfraction of samples consistent with the evidence e drops exponentially as the number of evi-\ndence variables grows, so the procedure is simply unusable for complex problems.\nNotice that rejection sampling is very similar to the estimation of conditional probabili-\nties directly from the real world. For example, to estimate P(Rain | RedSkyAtNight = true),\none can simply count how often it rains after a red sky is observed the previous evening—\nignoring those evenings when the sky is not red. (Here, the world itself plays the role of\nthe sample-generation algorithm.) Obviously, this could take a long time if the sky is very\nseldom red, and that is the weakness of rejection sampling.\nLikelihood weighting\nLikelihood weighting avoids the inefﬁciency of rejection sampling by generating only events\nLIKELIHOOD\nWEIGHTING\nthat are consistent with the evidence e. It is a particular instance of the general statistical\ntechnique of importance sampling, tailored for inference in Bayesian networks. We begin by\nIMPORTANCE\nSAMPLING Section 14.5.\nApproximate Inference in Bayesian Networks\n533\nfunction REJECTION-SAMPLING(X ,e,bn,N ) returns an estimate of P(X|e)\ninputs: X , the query variable\ne, observed values for variables E\nbn, a Bayesian network\nN , the total number of samples to be generated\nlocal variables: N, a vector of counts for each value of X , initially zero\nfor j = 1 to N do\nx ←PRIOR-SAMPLE(bn)\nif x is consistent with e then\nN[x] ←N[x]+1 where x is the value of X in x\nreturn NORMALIZE(N)\nFigure 14.14\nThe rejection-sampling algorithm for answering queries given evidence in a\nBayesian network.\ndescribing how the algorithm works; then we show that it works correctly—that is, generates\nconsistent probability estimates.\nLIKELIHOOD-WEIGHTING (see Figure 14.15) ﬁxes the values for the evidence vari-",
  "Bayesian network.\ndescribing how the algorithm works; then we show that it works correctly—that is, generates\nconsistent probability estimates.\nLIKELIHOOD-WEIGHTING (see Figure 14.15) ﬁxes the values for the evidence vari-\nables E and samples only the nonevidence variables. This guarantees that each event gener-\nated is consistent with the evidence. Not all events are equal, however. Before tallying the\ncounts in the distribution for the query variable, each event is weighted by the likelihood that\nthe event accords to the evidence, as measured by the product of the conditional probabilities\nfor each evidence variable, given its parents. Intuitively, events in which the actual evidence\nappears unlikely should be given less weight.\nLet us apply the algorithm to the network shown in Figure 14.12(a), with the query\nP(Rain | Cloudy = true, WetGrass = true) and the ordering Cloudy, Sprinkler, Rain, Wet-\nGrass. (Any topological ordering will do.) The process goes as follows: First, the weight w\nis set to 1.0. Then an event is generated:\n1. Cloudy is an evidence variable with value true. Therefore, we set\nw ←w × P(Cloudy = true) = 0.5 .\n2. Sprinkler is not an evidence variable, so sample from P(Sprinkler | Cloudy = true) =\n⟨0.1, 0.9⟩; suppose this returns false.\n3. Similarly, sample from P(Rain | Cloudy = true) = ⟨0.8, 0.2⟩; suppose this returns\ntrue.\n4. WetGrass is an evidence variable with value true. Therefore, we set\nw ←w × P(WetGrass = true | Sprinkler = false, Rain = true) = 0.45 .\nHere WEIGHTED-SAMPLE returns the event [true, false, true, true] with weight 0.45, and\nthis is tallied under Rain = true.\nTo understand why likelihood weighting works, we start by examining the sampling\nprobability SWS for WEIGHTED-SAMPLE. Remember that the evidence variables E are ﬁxed 534\nChapter\n14.\nProbabilistic Reasoning\nfunction LIKELIHOOD-WEIGHTING(X ,e,bn,N ) returns an estimate of P(X|e)\ninputs: X , the query variable\ne, observed values for variables E\nbn, a Bayesian network specifying joint distribution P(X1, . . . , Xn)\nN , the total number of samples to be generated\nlocal variables: W, a vector of weighted counts for each value of X , initially zero\nfor j = 1 to N do\nx,w ←WEIGHTED-SAMPLE(bn,e)\nW[x] ←W[x] + w where x is the value of X in x\nreturn NORMALIZE(W)\nfunction WEIGHTED-SAMPLE(bn,e) returns an event and a weight\nw ←1; x ←an event with n elements initialized from e\nforeach variable Xi in X1, . . . , Xn do\nif Xi is an evidence variable with value xi in e",
  "return NORMALIZE(W)\nfunction WEIGHTED-SAMPLE(bn,e) returns an event and a weight\nw ←1; x ←an event with n elements initialized from e\nforeach variable Xi in X1, . . . , Xn do\nif Xi is an evidence variable with value xi in e\nthen w ←w × P(Xi = xi | parents(Xi))\nelse x[i] ←a random sample from P(Xi | parents(Xi))\nreturn x, w\nFigure 14.15\nThe likelihood-weighting algorithm for inference in Bayesian networks. In\nWEIGHTED-SAMPLE, each nonevidence variable is sampled according to the conditional\ndistribution given the values already sampled for the variable’s parents, while a weight is\naccumulated based on the likelihood for each evidence variable.\nwith values e. We call the nonevidence variables Z (including the query variable X). The\nalgorithm samples each variable in Z given its parent values:\nSWS(z, e) =\nl\u0019\ni = 1\nP(zi | parents(Zi)) .\n(14.7)\nNotice that Parents(Zi) can include both nonevidence variables and evidence variables. Un-\nlike the prior distribution P(z), the distribution SWS pays some attention to the evidence: the\nsampled values for each Zi will be inﬂuenced by evidence among Zi’s ancestors. For exam-\nple, when sampling Sprinkler the algorithm pays attention to the evidence Cloudy = true in\nits parent variable. On the other hand, SWS pays less attention to the evidence than does the\ntrue posterior distribution P(z | e), because the sampled values for each Zi ignore evidence\namong Zi’s non-ancestors.5 For example, when sampling Sprinkler and Rain the algorithm\nignores the evidence in the child variable WetGrass = true; this means it will generate many\nsamples with Sprinkler = false and Rain = false despite the fact that the evidence actually\nrules out this case.\n5 Ideally, we would like to use a sampling distribution equal to the true posterior P(z | e), to take all the evidence\ninto account. This cannot be done efﬁciently, however. If it could, then we could approximate the desired\nprobability to arbitrary accuracy with a polynomial number of samples. It can be shown that no such polynomial-\ntime approximation scheme can exist. Section 14.5.\nApproximate Inference in Bayesian Networks\n535\nThe likelihood weight w makes up for the difference between the actual and desired\nsampling distributions. The weight for a given sample x, composed from z and e, is the\nproduct of the likelihoods for each evidence variable given its parents (some or all of which\nmay be among the Zis):\nw(z, e) =\nm\n\u0019\ni = 1\nP(ei | parents(Ei)) .\n(14.8)",
  "product of the likelihoods for each evidence variable given its parents (some or all of which\nmay be among the Zis):\nw(z, e) =\nm\n\u0019\ni = 1\nP(ei | parents(Ei)) .\n(14.8)\nMultiplying Equations (14.7) and (14.8), we see that the weighted probability of a sample has\nthe particularly convenient form\nSWS(z, e)w(z, e) =\nl\u0019\ni = 1\nP(zi | parents(Zi))\nm\n\u0019\ni = 1\nP(ei | parents(Ei))\n= P(z, e)\n(14.9)\nbecause the two products cover all the variables in the network, allowing us to use Equa-\ntion (14.2) for the joint probability.\nNow it is easy to show that likelihood weighting estimates are consistent. For any\nparticular value x of X, the estimated posterior probability can be calculated as follows:\nˆP(x | e) = α\n\f\ny\nNWS(x, y, e)w(x, y, e)\nfrom LIKELIHOOD-WEIGHTING\n≈α′ \f\ny\nSWS(x, y, e)w(x, y, e)\nfor large N\n= α′ \f\ny\nP(x, y, e)\nby Equation (14.9)\n= α′P(x, e) = P(x | e) .\nHence, likelihood weighting returns consistent estimates.\nBecause likelihood weighting uses all the samples generated, it can be much more ef-\nﬁcient than rejection sampling. It will, however, suffer a degradation in performance as the\nnumber of evidence variables increases. This is because most samples will have very low\nweights and hence the weighted estimate will be dominated by the tiny fraction of samples\nthat accord more than an inﬁnitesimal likelihood to the evidence. The problem is exacerbated\nif the evidence variables occur late in the variable ordering, because then the nonevidence\nvariables will have no evidence in their parents and ancestors to guide the generation of sam-\nples. This means the samples will be simulations that bear little resemblance to the reality\nsuggested by the evidence.\n14.5.2\nInference by Markov chain simulation\nMarkov chain Monte Carlo (MCMC) algorithms work quite differently from rejection sam-\nMARKOV CHAIN\nMONTE CARLO\npling and likelihood weighting. Instead of generating each sample from scratch, MCMC al-\ngorithms generate each sample by making a random change to the preceding sample. It is\ntherefore helpful to think of an MCMC algorithm as being in a particular current state speci-\nfying a value for every variable and generating a next state by making random changes to the 536\nChapter\n14.\nProbabilistic Reasoning\ncurrent state. (If this reminds you of simulated annealing from Chapter 4 or WALKSAT from\nChapter 7, that is because both are members of the MCMC family.) Here we describe a par-",
  "Chapter\n14.\nProbabilistic Reasoning\ncurrent state. (If this reminds you of simulated annealing from Chapter 4 or WALKSAT from\nChapter 7, that is because both are members of the MCMC family.) Here we describe a par-\nticular form of MCMC called Gibbs sampling, which is especially well suited for Bayesian\nGIBBS SAMPLING\nnetworks. (Other forms, some of them signiﬁcantly more powerful, are discussed in the notes\nat the end of the chapter.) We will ﬁrst describe what the algorithm does, then we will explain\nwhy it works.\nGibbs sampling in Bayesian networks\nThe Gibbs sampling algorithm for Bayesian networks starts with an arbitrary state (with the\nevidence variables ﬁxed at their observed values) and generates a next state by randomly\nsampling a value for one of the nonevidence variables Xi. The sampling for Xi is done\nconditioned on the current values of the variables in the Markov blanket of Xi. (Recall from\npage 517 that the Markov blanket of a variable consists of its parents, children, and children’s\nparents.) The algorithm therefore wanders randomly around the state space—the space of\npossible complete assignments—ﬂipping one variable at a time, but keeping the evidence\nvariables ﬁxed.\nConsider the query P(Rain | Sprinkler = true, WetGrass = true) applied to the net-\nwork in Figure 14.12(a). The evidence variables Sprinkler and WetGrass are ﬁxed to their\nobserved values and the nonevidence variables Cloudy and Rain are initialized randomly—\nlet us say to true and false respectively. Thus, the initial state is [true, true, false, true].\nNow the nonevidence variables are sampled repeatedly in an arbitrary order. For example:\n1. Cloudy is sampled, given the current values of its Markov blanket variables: in this\ncase, we sample from P(Cloudy | Sprinkler = true, Rain = false). (Shortly, we will\nshow how to calculate this distribution.) Suppose the result is Cloudy = false. Then\nthe new current state is [false, true, false, true].\n2. Rain is sampled, given the current values of its Markov blanket variables: in this case,\nwe sample from P(Rain | Cloudy = false, Sprinkler = true, WetGrass = true). Sup-\npose this yields Rain = true. The new current state is [false, true, true, true].\nEach state visited during this process is a sample that contributes to the estimate for the query\nvariable Rain. If the process visits 20 states where Rain is true and 60 states where Rain is\nfalse, then the answer to the query is NORMALIZE(⟨20, 60⟩) = ⟨0.25, 0.75⟩. The complete",
  "variable Rain. If the process visits 20 states where Rain is true and 60 states where Rain is\nfalse, then the answer to the query is NORMALIZE(⟨20, 60⟩) = ⟨0.25, 0.75⟩. The complete\nalgorithm is shown in Figure 14.16.\nWhy Gibbs sampling works\nWe will now show that Gibbs sampling returns consistent estimates for posterior probabil-\nities. The material in this section is quite technical, but the basic claim is straightforward:\nthe sampling process settles into a “dynamic equilibrium” in which the long-run fraction of\ntime spent in each state is exactly proportional to its posterior probability. This remarkable\nproperty follows from the speciﬁc transition probability with which the process moves from\nTRANSITION\nPROBABILITY\none state to another, as deﬁned by the conditional distribution given the Markov blanket of\nthe variable being sampled. Section 14.5.\nApproximate Inference in Bayesian Networks\n537\nfunction GIBBS-ASK(X ,e,bn,N ) returns an estimate of P(X|e)\nlocal variables: N, a vector of counts for each value of X , initially zero\nZ, the nonevidence variables in bn\nx, the current state of the network, initially copied from e\ninitialize x with random values for the variables in Z\nfor j = 1 to N do\nfor each Zi in Z do\nset the value of Zi in x by sampling from P(Zi|mb(Zi))\nN[x] ←N[x] + 1 where x is the value of X in x\nreturn NORMALIZE(N)\nFigure 14.16\nThe Gibbs sampling algorithm for approximate inference in Bayesian net-\nworks; this version cycles through the variables, but choosing variables at random also works.\nLet q(x →x′) be the probability that the process makes a transition from state x to\nstate x′. This transition probability deﬁnes what is called a Markov chain on the state space.\nMARKOV CHAIN\n(Markov chains also ﬁgure prominently in Chapters 15 and 17.) Now suppose that we run\nthe Markov chain for t steps, and let πt(x) be the probability that the system is in state x at\ntime t. Similarly, let πt+1(x′) be the probability of being in state x′ at time t + 1. Given\nπt(x), we can calculate πt+1(x′) by summing, for all states the system could be in at time t,\nthe probability of being in that state times the probability of making the transition to x′:\nπt+1(x′) =\n\f\nx\nπt(x)q(x →x′) .\nWe say that the chain has reached its stationary distribution if πt = πt+1. Let us call this\nSTATIONARY\nDISTRIBUTION\nstationary distribution π; its deﬁning equation is therefore\nπ(x′) =\n\f\nx\nπ(x)q(x →x′)\nfor all x′ .\n(14.10)",
  "x\nπt(x)q(x →x′) .\nWe say that the chain has reached its stationary distribution if πt = πt+1. Let us call this\nSTATIONARY\nDISTRIBUTION\nstationary distribution π; its deﬁning equation is therefore\nπ(x′) =\n\f\nx\nπ(x)q(x →x′)\nfor all x′ .\n(14.10)\nProvided the transition probability distribution q is ergodic—that is, every state is reachable\nERGODIC\nfrom every other and there are no strictly periodic cycles—there is exactly one distribution π\nsatisfying this equation for any given q.\nEquation (14.10) can be read as saying that the expected “outﬂow” from each state (i.e.,\nits current “population”) is equal to the expected “inﬂow” from all the states. One obvious\nway to satisfy this relationship is if the expected ﬂow between any pair of states is the same\nin both directions; that is,\nπ(x)q(x →x′) = π(x′)q(x′ →x)\nfor all x, x′ .\n(14.11)\nWhen these equations hold, we say that q(x →x′) is in detailed balance with π(x).\nDETAILED BALANCE\nWe can show that detailed balance implies stationarity simply by summing over x in\nEquation (14.11). We have\n\f\nx\nπ(x)q(x →x′) =\n\f\nx\nπ(x′)q(x′ →x) = π(x′)\n\f\nx\nq(x′ →x) = π(x′) 538\nChapter\n14.\nProbabilistic Reasoning\nwhere the last step follows because a transition from x′ is guaranteed to occur.\nThe transition probability q(x →x′) deﬁned by the sampling step in GIBBS-ASK is\nactually a special case of the more general deﬁnition of Gibbs sampling, according to which\neach variable is sampled conditionally on the current values of all the other variables. We\nstart by showing that this general deﬁnition of Gibbs sampling satisﬁes the detailed balance\nequation with a stationary distribution equal to P(x | e), (the true posterior distribution on\nthe nonevidence variables). Then, we simply observe that, for Bayesian networks, sampling\nconditionally on all variables is equivalent to sampling conditionally on the variable’s Markov\nblanket (see page 517).\nTo analyze the general Gibbs sampler, which samples each Xi in turn with a transition\nprobability qi that conditions on all the other variables, we deﬁne Xi to be these other vari-\nables (except the evidence variables); their values in the current state are xi. If we sample a\nnew value x′\ni for Xi conditionally on all the other variables, including the evidence, we have\nqi(x →x′) = qi((xi, xi) →(x′\ni, xi)) = P(x′\ni | xi, e) .\nNow we show that the transition probability for each step of the Gibbs sampler is in detailed\nbalance with the true posterior:\nπ(x)qi(x →x′) = P(x | e)P(x′",
  "qi(x →x′) = qi((xi, xi) →(x′\ni, xi)) = P(x′\ni | xi, e) .\nNow we show that the transition probability for each step of the Gibbs sampler is in detailed\nbalance with the true posterior:\nπ(x)qi(x →x′) = P(x | e)P(x′\ni | xi, e) = P(xi, xi | e)P(x′\ni | xi, e)\n= P(xi | xi, e)P(xi | e)P(x′\ni | xi, e)\n(using the chain rule on the ﬁrst term)\n= P(xi | xi, e)P(x′\ni, xi | e)\n(using the chain rule backward)\n= π(x′)qi(x′ →x) .\nWe can think of the loop “for each Zi in Z do” in Figure 14.16 as deﬁning one large transition\nprobability q that is the sequential composition q1 ◦q2 ◦· · · ◦qn of the transition probabilities\nfor the individual variables. It is easy to show (Exercise 14.19) that if each of qi and qj has\nπ as its stationary distribution, then the sequential composition qi ◦qj does too; hence the\ntransition probability q for the whole loop has P(x | e) as its stationary distribution. Finally,\nunless the CPTs contain probabilities of 0 or 1—which can cause the state space to become\ndisconnected—it is easy to see that q is ergodic. Hence, the samples generated by Gibbs\nsampling will eventually be drawn from the true posterior distribution.\nThe ﬁnal step is to show how to perform the general Gibbs sampling step—sampling\nXi from P(Xi | xi, e)—in a Bayesian network. Recall from page 517 that a variable is inde-\npendent of all other variables given its Markov blanket; hence,\nP(x′\ni | xi, e) = P(x′\ni | mb(Xi)) ,\nwhere mb(Xi) denotes the values of the variables in Xi’s Markov blanket, MB(Xi). As\nshown in Exercise 14.7, the probability of a variable given its Markov blanket is proportional\nto the probability of the variable given its parents times the probability of each child given its\nrespective parents:\nP(x′\ni | mb(Xi)) = α P(x′\ni | parents(Xi)) ×\n\u0019\nYj∈Children(Xi)\nP(yj | parents(Yj)) . (14.12)\nHence, to ﬂip each variable Xi conditioned on its Markov blanket, the number of multiplica-\ntions required is equal to the number of Xi’s children. Section 14.6.\nRelational and First-Order Probability Models\n539\nRecommendation(C1, B1)\nHonesty(C1)\nKindness(C1)\nQuality(B1)\nRecommendation(C1, B1)\nHonesty(C1)\nKindness(C1)\nQuality(B1)\nRecommendation(C2, B1)\nHonesty(C2)\nKindness(C2)\nQuality(B2)\nRecommendation(C1, B2)\nRecommendation(C2, B2)\n(a)\n(b)\nFigure 14.17\n(a) Bayes net for a single customer C1 recommending a single book B1.\nHonest(C1) is Boolean, while the other variables have integer values from 1 to 5. (b) Bayes\nnet with two customers and two books.\n14.6",
  "Recommendation(C2, B2)\n(a)\n(b)\nFigure 14.17\n(a) Bayes net for a single customer C1 recommending a single book B1.\nHonest(C1) is Boolean, while the other variables have integer values from 1 to 5. (b) Bayes\nnet with two customers and two books.\n14.6\nRELATIONAL AND FIRST-ORDER PROBABILITY MODELS\nIn Chapter 8, we explained the representational advantages possessed by ﬁrst-order logic in\ncomparison to propositional logic. First-order logic commits to the existence of objects and\nrelations among them and can express facts about some or all of the objects in a domain. This\noften results in representations that are vastly more concise than the equivalent propositional\ndescriptions. Now, Bayesian networks are essentially propositional: the set of random vari-\nables is ﬁxed and ﬁnite, and each has a ﬁxed domain of possible values. This fact limits the\napplicability of Bayesian networks. If we can ﬁnd a way to combine probability theory with\nthe expressive power of ﬁrst-order representations, we expect to be able to increase dramati-\ncally the range of problems that can be handled.\nFor example, suppose that an online book retailer would like to provide overall evalu-\nations of products based on recommendations received from its customers. The evaluation\nwill take the form of a posterior distribution over the quality of the book, given the avail-\nable evidence. The simplest solution to base the evaluation on the average recommendation,\nperhaps with a variance determined by the number of recommendations, but this fails to take\ninto account the fact that some customers are kinder than others and some are less honest than\nothers. Kind customers tend to give high recommendations even to fairly mediocre books,\nwhile dishonest customers give very high or very low recommendations for reasons other\nthan quality—for example, they might work for a publisher.6\nFor a single customer C1, recommending a single book B1, the Bayes net might look\nlike the one shown in Figure 14.17(a). (Just as in Section 9.1, expressions with parentheses\nsuch as Honest(C1) are just fancy symbols—in this case, fancy names for random variables.)\n6 A game theorist would advise a dishonest customer to avoid detection by occasionally recommending a good\nbook from a competitor. See Chapter 17. 540\nChapter\n14.\nProbabilistic Reasoning\nWith two customers and two books, the Bayes net looks like the one in Figure 14.17(b). For",
  "book from a competitor. See Chapter 17. 540\nChapter\n14.\nProbabilistic Reasoning\nWith two customers and two books, the Bayes net looks like the one in Figure 14.17(b). For\nlarger numbers of books and customers, it becomes completely impractical to specify the\nnetwork by hand.\nFortunately, the network has a lot of repeated structure. Each Recommendation(c, b)\nvariable has as its parents the variables Honest(c), Kindness(c), and Quality(b). Moreover,\nthe CPTs for all the Recommendation(c, b) variables are identical, as are those for all the\nHonest(c) variables, and so on. The situation seems tailor-made for a ﬁrst-order language.\nWe would like to say something like\nRecommendation(c, b) ∼RecCPT(Honest(c), Kindness(c), Quality(b))\nwith the intended meaning that a customer’s recommendation for a book depends on the\ncustomer’s honesty and kindness and the book’s quality according to some ﬁxed CPT. This\nsection develops a language that lets us say exactly this, and a lot more besides.\n14.6.1\nPossible worlds\nRecall from Chapter 13 that a probability model deﬁnes a set Ω of possible worlds with\na probability P(ω) for each world ω. For Bayesian networks, the possible worlds are as-\nsignments of values to variables; for the Boolean case in particular, the possible worlds are\nidentical to those of propositional logic. For a ﬁrst-order probability model, then, it seems\nwe need the possible worlds to be those of ﬁrst-order logic—that is, a set of objects with\nrelations among them and an interpretation that maps constant symbols to objects, predicate\nsymbols to relations, and function symbols to functions on those objects. (See Section 8.2.)\nThe model also needs to deﬁne a probability for each such possible world, just as a Bayesian\nnetwork deﬁnes a probability for each assignment of values to variables.\nLet us suppose, for a moment, that we have ﬁgured out how to do this. Then, as usual\n(see page 485), we can obtain the probability of any ﬁrst-order logical sentence φ as a sum\nover the possible worlds where it is true:\nP(φ) =\n\f\nω:φ is true in ω\nP(ω) .\n(14.13)\nConditional probabilities P(φ | e) can be obtained similarly, so we can, in principle, ask any\nquestion we want of our model—e.g., “Which books are most likely to be recommended\nhighly by dishonest customers?”—and get an answer. So far, so good.\nThere is, however, a problem: the set of ﬁrst-order models is inﬁnite. We saw this",
  "question we want of our model—e.g., “Which books are most likely to be recommended\nhighly by dishonest customers?”—and get an answer. So far, so good.\nThere is, however, a problem: the set of ﬁrst-order models is inﬁnite. We saw this\nexplicitly in Figure 8.4 on page 293, which we show again in Figure 14.18 (top). This means\nthat (1) the summation in Equation (14.13) could be infeasible, and (2) specifying a complete,\nconsistent distribution over an inﬁnite set of worlds could be very difﬁcult.\nSection 14.6.2 explores one approach to dealing with this problem. The idea is to\nborrow not from the standard semantics of ﬁrst-order logic but from the database seman-\ntics deﬁned in Section 8.2.8 (page 299). The database semantics makes the unique names\nassumption—here, we adopt it for the constant symbols. It also assumes domain closure—\nthere are no more objects than those that are named. We can then guarantee a ﬁnite set of\npossible worlds by making the set of objects in each world be exactly the set of constant Section 14.6.\nRelational and First-Order Probability Models\n541\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\n. . .\n. . .\n. . .\n. . .\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nR\nJ\nFigure 14.18\nTop: Some members of the set of all possible worlds for a language with two\nconstant symbols, R and J, and one binary relation symbol, under the standard semantics for\nﬁrst-order logic. Bottom: the possible worlds under database semantics. The interpretation\nof the constant symbols is ﬁxed, and there is a distinct object for each constant symbol.\nsymbols that are used; as shown in Figure 14.18 (bottom), there is no uncertainty about the\nmapping from symbols to objects or about the objects that exist. We will call models deﬁned\nin this way relational probability models, or RPMs.7 The most signiﬁcant difference be-\nRELATIONAL\nPROBABILITY MODEL\ntween the semantics of RPMs and the database semantics introduced in Section 8.2.8 is that\nRPMs do not make the closed-world assumption—obviously, assuming that every unknown\nfact is false doesn’t make sense in a probabilistic reasoning system!\nWhen the underlying assumptions of database semantics fail to hold, RPMs won’t work\nwell. For example, a book retailer might use an ISBN (International Standard Book Number)\nas a constant symbol to name each book, even though a given “logical” book (e.g., “Gone\nWith the Wind”) may have several ISBNs. It would make sense to aggregate recommenda-",
  "as a constant symbol to name each book, even though a given “logical” book (e.g., “Gone\nWith the Wind”) may have several ISBNs. It would make sense to aggregate recommenda-\ntions across multiple ISBNs, but the retailer may not know for sure which ISBNs are really\nthe same book. (Note that we are not reifying the individual copies of the book, which might\nbe necessary for used-book sales, car sales, and so on.) Worse still, each customer is iden-\ntiﬁed by a login ID, but a dishonest customer may have thousands of IDs! In the computer\nsecurity ﬁeld, these multiple IDs are called sibyls and their use to confound a reputation sys-\nSIBYL\ntem is called a sibyl attack. Thus, even a simple application in a relatively well-deﬁned,\nSIBYL ATTACK\nonline domain involves both existence uncertainty (what are the real books and customers\nEXISTENCE\nUNCERTAINTY\nunderlying the observed data) and identity uncertainty (which symbol really refer to the\nIDENTITY\nUNCERTAINTY\nsame object). We need to bite the bullet and deﬁne probability models based on the standard\nsemantics of ﬁrst-order logic, for which the possible worlds vary in the objects they contain\nand in the mappings from symbols to objects. Section 14.6.3 shows how to do this.\n7 The name relational probability model was given by Pfeffer (2000) to a slightly different representation, but\nthe underlying ideas are the same. 542\nChapter\n14.\nProbabilistic Reasoning\n14.6.2\nRelational probability models\nLike ﬁrst-order logic, RPMs have constant, function, and predicate symbols. (It turns out to\nbe easier to view predicates as functions that return true or false.) We will also assume a\ntype signature for each function, that is, a speciﬁcation of the type of each argument and the\nTYPE SIGNATURE\nfunction’s value. If the type of each object is known, many spurious possible worlds are elim-\ninated by this mechanism. For the book-recommendation domain, the types are Customer\nand Book, and the type signatures for the functions and predicates are as follows:\nHonest : Customer →{true, false}Kindness : Customer →{1, 2, 3, 4, 5}\nQuality : Book →{1, 2, 3, 4, 5}\nRecommendation : Customer × Book →{1, 2, 3, 4, 5}\nThe constant symbols will be whatever customer and book names appear in the retailer’s data\nset. In the example given earlier (Figure 14.17(b)), these were C1, C2 and B1, B2.\nGiven the constants and their types, together with the functions and their type signa-",
  "set. In the example given earlier (Figure 14.17(b)), these were C1, C2 and B1, B2.\nGiven the constants and their types, together with the functions and their type signa-\ntures, the random variables of the RPM are obtained by instantiating each function with each\npossible combination of objects: Honest(C1), Quality(B2), Recommendation(C1, B2),\nand so on. These are exactly the variables appearing in Figure 14.17(b). Because each type\nhas only ﬁnitely many instances, the number of basic random variables is also ﬁnite.\nTo complete the RPM, we have to write the dependencies that govern these random\nvariables. There is one dependency statement for each function, where each argument of the\nfunction is a logical variable (i.e., a variable that ranges over objects, as in ﬁrst-order logic):\nHonest(c) ∼⟨0.99, 0.01⟩\nKindness(c) ∼⟨0.1, 0.1, 0.2, 0.3, 0.3⟩\nQuality(b) ∼⟨0.05, 0.2, 0.4, 0.2, 0.15⟩\nRecommendation(c, b) ∼RecCPT(Honest(c), Kindness(c), Quality(b))\nwhere RecCPT is a separately deﬁned conditional distribution with 2 × 5 × 5 = 50 rows,\neach with 5 entries. The semantics of the RPM can be obtained by instantiating these de-\npendencies for all known constants, giving a Bayesian network (as in Figure 14.17(b)) that\ndeﬁnes a joint distribution over the RPM’s random variables.8\nWe can reﬁne the model by introducing a context-speciﬁc independence to reﬂect the\nCONTEXT-SPECIFIC\nINDEPENDENCE\nfact that dishonest customers ignore quality when giving a recommendation; moreover, kind-\nness plays no role in their decisions. A context-speciﬁc independence allows a variable to be\nindependent of some of its parents given certain values of others; thus, Recommendation(c, b)\nis independent of Kindness(c) and Quality(b) when Honest(c) = false:\nRecommendation(c, b) ∼\nif Honest(c) then\nHonestRecCPT(Kindness(c), Quality(b))\nelse ⟨0.4, 0.1, 0.0, 0.1, 0.4⟩.\n8 Some technical conditions must be observed to guarantee that the RPM deﬁnes a proper distribution. First,\nthe dependencies must be acyclic, otherwise the resulting Bayesian network will have cycles and will not deﬁne\na proper distribution. Second, the dependencies must be well-founded, that is, there can be no inﬁnite ancestor\nchains, such as might arise from recursive dependencies. Under some circumstances (see Exercise 14.6), a ﬁxed-\npoint calculation yields a well-deﬁned probability model for a recursive RPM. Section 14.6.\nRelational and First-Order Probability Models\n543\nRecommendation(C1, B1)\nHonesty(C1)\nKindness(C1)",
  "point calculation yields a well-deﬁned probability model for a recursive RPM. Section 14.6.\nRelational and First-Order Probability Models\n543\nRecommendation(C1, B1)\nHonesty(C1)\nKindness(C1)\nQuality(B1)\nRecommendation(C2, B1)\nQuality(B2)\nFan(C1, A1)\nFan(C1, A2)\nAuthor(B2)\nFigure 14.19\nFragment of the equivalent Bayes net when Author(B2) is unknown.\nThis kind of dependency may look like an ordinary if–then–else statement on a programming\nlanguage, but there is a key difference: the inference engine doesn’t necessarily know the\nvalue of the conditional test!\nWe can elaborate this model in endless ways to make it more realistic. For example,\nsuppose that an honest customer who is a fan of a book’s author always gives the book a 5,\nregardless of quality:\nRecommendation(c, b) ∼\nif Honest(c) then\nif Fan(c, Author(b)) then Exactly(5)\nelse HonestRecCPT(Kindness(c), Quality(b))\nelse ⟨0.4, 0.1, 0.0, 0.1, 0.4⟩\nAgain, the conditional test Fan(c, Author(b)) is unknown, but if a customer gives only 5s to\na particular author’s books and is not otherwise especially kind, then the posterior probability\nthat the customer is a fan of that author will be high. Furthermore, the posterior distribution\nwill tend to discount the customer’s 5s in evaluating the quality of that author’s books.\nIn the preceding example, we implicitly assumed that the value of Author(b) is known\nfor every b, but this may not be the case. How can the system reason about whether, say, C1\nis a fan of Author(B2) when Author(B2) is unknown? The answer is that the system may\nhave to reason about all possible authors. Suppose (to keep things simple) that there are just\ntwo authors, A1 and A2. Then Author(B2) is a random variable with two possible values,\nA1 and A2, and it is a parent of Recommendation(C1, B2). The variables Fan(C1, A1) and\nFan(C1, A2) are parents too. The conditional distribution for Recommendation(C1, B2) is\nthen essentially a multiplexer in which the Author(B2) parent acts as a selector to choose\nMULTIPLEXER\nwhich of Fan(C1, A1) and Fan(C1, A2) actually gets to inﬂuence the recommendation. A\nfragment of the equivalent Bayes net is shown in Figure 14.19. Uncertainty in the value\nof Author(B2), which affects the dependency structure of the network, is an instance of\nrelational uncertainty.\nRELATIONAL\nUNCERTAINTY\nIn case you are wondering how the system can possibly work out who the author of\nB2 is: consider the possibility that three other customers are fans of A1 (and have no other",
  "relational uncertainty.\nRELATIONAL\nUNCERTAINTY\nIn case you are wondering how the system can possibly work out who the author of\nB2 is: consider the possibility that three other customers are fans of A1 (and have no other\nfavorite authors in common) and all three have given B2 a 5, even though most other cus-\ntomers ﬁnd it quite dismal. In that case, it is extremely likely that A1 is the author of B2. 544\nChapter\n14.\nProbabilistic Reasoning\nThe emergence of sophisticated reasoning like this from an RPM model of just a few lines\nis an intriguing example of how probabilistic inﬂuences spread through the web of intercon-\nnections among objects in the model. As more dependencies and more objects are added, the\npicture conveyed by the posterior distribution often becomes clearer and clearer.\nThe next question is how to do inference in RPMs. One approach is to collect the\nevidence and query and the constant symbols therein, construct the equivalent Bayes net,\nand apply any of the inference methods discussed in this chapter. This technique is called\nunrolling. The obvious drawback is that the resulting Bayes net may be very large. Further-\nUNROLLING\nmore, if there are many candidate objects for an unknown relation or function—for example,\nthe unknown author of B2—then some variables in the network may have many parents.\nFortunately, much can be done to improve on generic inference algorithms. First, the\npresence of repeated substructure in the unrolled Bayes net means that many of the factors\nconstructed during variable elimination (and similar kinds of tables constructed by cluster-\ning algorithms) will be identical; effective caching schemes have yielded speedups of three\norders of magnitude for large networks. Second, inference methods developed to take advan-\ntage of context-speciﬁc independence in Bayes nets ﬁnd many applications in RPMs. Third,\nMCMC inference algorithms have some interesting properties when applied to RPMs with\nrelational uncertainty. MCMC works by sampling complete possible worlds, so in each state\nthe relational structure is completely known. In the example given earlier, each MCMC state\nwould specify the value of Author(B2), and so the other potential authors are no longer par-\nents of the recommendation nodes for B2. For MCMC, then, relational uncertainty causes no\nincrease in network complexity; instead, the MCMC process includes transitions that change",
  "ents of the recommendation nodes for B2. For MCMC, then, relational uncertainty causes no\nincrease in network complexity; instead, the MCMC process includes transitions that change\nthe relational structure, and hence the dependency structure, of the unrolled network.\nAll of the methods just described assume that the RPM has to be partially or completely\nunrolled into a Bayesian network. This is exactly analogous to the method of proposition-\nalization for ﬁrst-order logical inference. (See page 322.) Resolution theorem-provers and\nlogic programming systems avoid propositionalizing by instantiating the logical variables\nonly as needed to make the inference go through; that is, they lift the inference process above\nthe level of ground propositional sentences and make each lifted step do the work of many\nground steps. The same idea applied in probabilistic inference. For example, in the variable\nelimination algorithm, a lifted factor can represent an entire set of ground factors that assign\nprobabilities to random variables in the RPM, where those random variables differ only in the\nconstant symbols used to construct them. The details of this method are beyond the scope of\nthis book, but references are given at the end of the chapter.\n14.6.3\nOpen-universe probability models\nWe argued earlier that database semantics was appropriate for situations in which we know\nexactly the set of relevant objects that exist and can identify them unambiguously. (In partic-\nular, all observations about an object are correctly associated with the constant symbol that\nnames it.) In many real-world settings, however, these assumptions are simply untenable. We\ngave the examples of multiple ISBNs and sibyl attacks in the book-recommendation domain\n(to which we will return in a moment), but the phenomenon is far more pervasive: Section 14.6.\nRelational and First-Order Probability Models\n545\n• A vision system doesn’t know what exists, if anything, around the next corner, and may\nnot know if the object it sees now is the same one it saw a few minutes ago.\n• A text-understanding system does not know in advance the entities that will be featured\nin a text, and must reason about whether phrases such as “Mary,” “Dr. Smith,” “she,”\n“his cardiologist,” “his mother,” and so on refer to the same object.\n• An intelligence analyst hunting for spies never knows how many spies there really are\nand can only guess whether various pseudonyms, phone numbers, and sightings belong",
  "“his cardiologist,” “his mother,” and so on refer to the same object.\n• An intelligence analyst hunting for spies never knows how many spies there really are\nand can only guess whether various pseudonyms, phone numbers, and sightings belong\nto the same individual.\nIn fact, a major part of human cognition seems to require learning what objects exist and\nbeing able to connect observations—which almost never come with unique IDs attached—to\nhypothesized objects in the world.\nFor these reasons, we need to be able to write so-called open-universe probability\nOPEN UNIVERSE\nmodels or OUPMs based on the standard semantics of ﬁrst-order logic, as illustrated at the\ntop of Figure 14.18. A language for OUPMs provides a way of writing such models easily\nwhile guaranteeing a unique, consistent probability distribution over the inﬁnite space of\npossible worlds.\nThe basic idea is to understand how ordinary Bayesian networks and RPMs manage\nto deﬁne a unique probability model and to transfer that insight to the ﬁrst-order setting. In\nessence, a Bayes net generates each possible world, event by event, in the topological order\ndeﬁned by the network structure, where each event is an assignment of a value to a variable.\nAn RPM extends this to entire sets of events, deﬁned by the possible instantiations of the\nlogical variables in a given predicate or function. OUPMs go further by allowing generative\nsteps that add objects to the possible world under construction, where the number and type\nof objects may depend on the objects that are already in that world. That is, the event being\ngenerated is not the assignment of a value to a variable, but the very existence of objects.\nOne way to do this in OUPMs is to add statements that deﬁne conditional distributions\nover the numbers of objects of various kinds. For example, in the book-recommendation\ndomain, we might want to distinguish between customers (real people) and their login IDs.\nSuppose we expect somewhere between 100 and 10,000 distinct customers (whom we cannot\nobserve directly). We can express this as a prior log-normal distribution9 as follows:\n# Customer ∼LogNormal[6.9, 2.32]() .\nWe expect honest customers to have just one ID, whereas dishonest customers might have\nanywhere between 10 and 1000 IDs:\n# LoginID(Owner = c) ∼\nif Honest(c) then Exactly(1)\nelse LogNormal[6.9, 2.32]() .\nThis statement deﬁnes the number of login IDs for a given owner, who is a customer. The",
  "anywhere between 10 and 1000 IDs:\n# LoginID(Owner = c) ∼\nif Honest(c) then Exactly(1)\nelse LogNormal[6.9, 2.32]() .\nThis statement deﬁnes the number of login IDs for a given owner, who is a customer. The\nOwner function is called an origin function because it says where each generated object\nORIGIN FUNCTION\ncame from. In the formal semantics of BLOG (as distinct from ﬁrst-order logic), the domain\nelements in each possible world are actually generation histories (e.g., “the fourth login ID of\nthe seventh customer”) rather than simple tokens.\n9 A distribution LogNormal[μ, σ2](x) is equivalent to a distribution N[μ, σ2](x) over loge(x). 546\nChapter\n14.\nProbabilistic Reasoning\nSubject to technical conditions of acyclicity and well-foundedness similar to those for\nRPMs, open-universe models of this kind deﬁne a unique distribution over possible worlds.\nFurthermore, there exist inference algorithms such that, for every such well-deﬁned model\nand every ﬁrst-order query, the answer returned approaches the true posterior arbitrarily\nclosely in the limit. There are some tricky issues involved in designing these algorithms.\nFor example, an MCMC algorithm cannot sample directly in the space of possible worlds\nwhen the size of those worlds is unbounded; instead, it samples ﬁnite, partial worlds, rely-\ning on the fact that only ﬁnitely many objects can be relevant to the query in distinct ways.\nMoreover, transitions must allow for merging two objects into one or splitting one into two.\n(Details are given in the references at the end of the chapter.) Despite these complications,\nthe basic principle established in Equation (14.13) still holds: the probability of any sentence\nis well deﬁned and can be calculated.\nResearch in this area is still at an early stage, but already it is becoming clear that ﬁrst-\norder probabilistic reasoning yields a tremendous increase in the effectiveness of AI systems\nat handling uncertain information. Potential applications include those mentioned above—\ncomputer vision, text understanding, and intelligence analysis—as well as many other kinds\nof sensor interpretation.\n14.7\nOTHER APPROACHES TO UNCERTAIN REASONING\nOther sciences (e.g., physics, genetics, and economics) have long favored probability as a\nmodel for uncertainty. In 1819, Pierre Laplace said, “Probability theory is nothing but com-\nmon sense reduced to calculation.” In 1850, James Maxwell said, “The true logic for this",
  "model for uncertainty. In 1819, Pierre Laplace said, “Probability theory is nothing but com-\nmon sense reduced to calculation.” In 1850, James Maxwell said, “The true logic for this\nworld is the calculus of Probabilities, which takes account of the magnitude of the probabil-\nity which is, or ought to be, in a reasonable man’s mind.”\nGiven this long tradition, it is perhaps surprising that AI has considered many alterna-\ntives to probability. The earliest expert systems of the 1970s ignored uncertainty and used\nstrict logical reasoning, but it soon became clear that this was impractical for most real-world\ndomains. The next generation of expert systems (especially in medical domains) used prob-\nabilistic techniques. Initial results were promising, but they did not scale up because of the\nexponential number of probabilities required in the full joint distribution. (Efﬁcient Bayesian\nnetwork algorithms were unknown then.) As a result, probabilistic approaches fell out of\nfavor from roughly 1975 to 1988, and a variety of alternatives to probability were tried for a\nvariety of reasons:\n• One common view is that probability theory is essentially numerical, whereas human\njudgmental reasoning is more “qualitative.” Certainly, we are not consciously aware\nof doing numerical calculations of degrees of belief. (Neither are we aware of doing\nuniﬁcation, yet we seem to be capable of some kind of logical reasoning.) It might be\nthat we have some kind of numerical degrees of belief encoded directly in strengths\nof connections and activations in our neurons. In that case, the difﬁculty of conscious\naccess to those strengths is not surprising. One should also note that qualitative reason- Section 14.7.\nOther Approaches to Uncertain Reasoning\n547\ning mechanisms can be built directly on top of probability theory, so the “no numbers”\nargument against probability has little force. Nonetheless, some qualitative schemes\nhave a good deal of appeal in their own right. One of the best studied is default rea-\nsoning, which treats conclusions not as “believed to a certain degree,” but as “believed\nuntil a better reason is found to believe something else.” Default reasoning is covered\nin Chapter 12.\n• Rule-based approaches to uncertainty have also been tried. Such approaches hope to\nbuild on the success of logical rule-based systems, but add a sort of “fudge factor” to\neach rule to accommodate uncertainty. These methods were developed in the mid-1970s",
  "build on the success of logical rule-based systems, but add a sort of “fudge factor” to\neach rule to accommodate uncertainty. These methods were developed in the mid-1970s\nand formed the basis for a large number of expert systems in medicine and other areas.\n• One area that we have not addressed so far is the question of ignorance, as opposed\nto uncertainty. Consider the ﬂipping of a coin. If we know that the coin is fair, then\na probability of 0.5 for heads is reasonable. If we know that the coin is biased, but\nwe do not know which way, then 0.5 for heads is again reasonable. Obviously, the\ntwo cases are different, yet the outcome probability seems not to distinguish them. The\nDempster–Shafer theory uses interval-valued degrees of belief to represent an agent’s\nknowledge of the probability of a proposition.\n• Probability makes the same ontological commitment as logic: that propositions are true\nor false in the world, even if the agent is uncertain as to which is the case. Researchers\nin fuzzy logic have proposed an ontology that allows vagueness: that a proposition can\nbe “sort of” true. Vagueness and uncertainty are in fact orthogonal issues.\nThe next three subsections treat some of these approaches in slightly more depth. We will not\nprovide detailed technical material, but we cite references for further study.\n14.7.1\nRule-based methods for uncertain reasoning\nRule-based systems emerged from early work on practical and intuitive systems for logical\ninference. Logical systems in general, and logical rule-based systems in particular, have three\ndesirable properties:\n• Locality: In logical systems, whenever we have a rule of the form A ⇒B, we can\nLOCALITY\nconclude B, given evidence A, without worrying about any other rules. In probabilistic\nsystems, we need to consider all the evidence.\n• Detachment: Once a logical proof is found for a proposition B, the proposition can be\nDETACHMENT\nused regardless of how it was derived. That is, it can be detached from its justiﬁcation.\nIn dealing with probabilities, on the other hand, the source of the evidence for a belief\nis important for subsequent reasoning.\n• Truth-functionality: In logic, the truth of complex sentences can be computed from\nTRUTH-\nFUNCTIONALITY\nthe truth of the components. Probability combination does not work this way, except\nunder strong global independence assumptions.\nThere have been several attempts to devise uncertain reasoning schemes that retain these",
  "TRUTH-\nFUNCTIONALITY\nthe truth of the components. Probability combination does not work this way, except\nunder strong global independence assumptions.\nThere have been several attempts to devise uncertain reasoning schemes that retain these\nadvantages. The idea is to attach degrees of belief to propositions and rules and to devise\npurely local schemes for combining and propagating those degrees of belief. The schemes 548\nChapter\n14.\nProbabilistic Reasoning\nare also truth-functional; for example, the degree of belief in A ∨B is a function of the belief\nin A and the belief in B.\nThe bad news for rule-based systems is that the properties of locality, detachment, and\ntruth-functionality are simply not appropriate for uncertain reasoning. Let us look at truth-\nfunctionality ﬁrst. Let H1 be the event that a fair coin ﬂip comes up heads, let T1 be the event\nthat the coin comes up tails on that same ﬂip, and let H2 be the event that the coin comes\nup heads on a second ﬂip. Clearly, all three events have the same probability, 0.5, and so a\ntruth-functional system must assign the same belief to the disjunction of any two of them.\nBut we can see that the probability of the disjunction depends on the events themselves and\nnot just on their probabilities:\nP(A)\nP(B)\nP(A ∨B)\nP(H1) = 0.5 P(H1 ∨H1) = 0.50\nP(H1) = 0.5 P(T1) = 0.5\nP(H1 ∨T1) = 1.00\nP(H2) = 0.5 P(H1 ∨H2) = 0.75\nIt gets worse when we chain evidence together. Truth-functional systems have rules of the\nform A %→B that allow us to compute the belief in B as a function of the belief in the rule\nand the belief in A. Both forward- and backward-chaining systems can be devised. The belief\nin the rule is assumed to be constant and is usually speciﬁed by the knowledge engineer—for\nexample, as A %→0.9 B.\nConsider the wet-grass situation from Figure 14.12(a) (page 529). If we wanted to be\nable to do both causal and diagnostic reasoning, we would need the two rules\nRain %→WetGrass\nand\nWetGrass %→Rain .\nThese two rules form a feedback loop: evidence for Rain increases the belief in WetGrass,\nwhich in turn increases the belief in Rain even more. Clearly, uncertain reasoning systems\nneed to keep track of the paths along which evidence is propagated.\nIntercausal reasoning (or explaining away) is also tricky. Consider what happens when\nwe have the two rules\nSprinkler %→WetGrass\nand\nWetGrass %→Rain .\nSuppose we see that the sprinkler is on. Chaining forward through our rules, this increases the",
  "Intercausal reasoning (or explaining away) is also tricky. Consider what happens when\nwe have the two rules\nSprinkler %→WetGrass\nand\nWetGrass %→Rain .\nSuppose we see that the sprinkler is on. Chaining forward through our rules, this increases the\nbelief that the grass will be wet, which in turn increases the belief that it is raining. But this\nis ridiculous: the fact that the sprinkler is on explains away the wet grass and should reduce\nthe belief in rain. A truth-functional system acts as if it also believes Sprinkler %→Rain.\nGiven these difﬁculties, how can truth-functional systems be made useful in practice?\nThe answer lies in restricting the task and in carefully engineering the rule base so that un-\ndesirable interactions do not occur. The most famous example of a truth-functional system\nfor uncertain reasoning is the certainty factors model, which was developed for the MYCIN\nCERTAINTY FACTOR\nmedical diagnosis program and was widely used in expert systems of the late 1970s and\n1980s. Almost all uses of certainty factors involved rule sets that were either purely diagnos-\ntic (as in MYCIN) or purely causal. Furthermore, evidence was entered only at the “roots”\nof the rule set, and most rule sets were singly connected. Heckerman (1986) has shown that, Section 14.7.\nOther Approaches to Uncertain Reasoning\n549\nunder these circumstances, a minor variation on certainty-factor inference was exactly equiv-\nalent to Bayesian inference on polytrees. In other circumstances, certainty factors could yield\ndisastrously incorrect degrees of belief through overcounting of evidence. As rule sets be-\ncame larger, undesirable interactions between rules became more common, and practitioners\nfound that the certainty factors of many other rules had to be “tweaked” when new rules were\nadded. For these reasons, Bayesian networks have largely supplanted rule-based methods for\nuncertain reasoning.\n14.7.2\nRepresenting ignorance: Dempster–Shafer theory\nThe Dempster–Shafer theory is designed to deal with the distinction between uncertainty\nDEMPSTER–SHAFER\nTHEORY\nand ignorance. Rather than computing the probability of a proposition, it computes the\nprobability that the evidence supports the proposition. This measure of belief is called a\nbelief function, written Bel(X).\nBELIEF FUNCTION\nWe return to coin ﬂipping for an example of belief functions.\nSuppose you pick a\ncoin from a magician’s pocket. Given that the coin might or might not be fair, what belief",
  "belief function, written Bel(X).\nBELIEF FUNCTION\nWe return to coin ﬂipping for an example of belief functions.\nSuppose you pick a\ncoin from a magician’s pocket. Given that the coin might or might not be fair, what belief\nshould you ascribe to the event that it comes up heads? Dempster–Shafer theory says that\nbecause you have no evidence either way, you have to say that the belief Bel(Heads) = 0\nand also that Bel(¬Heads) = 0. This makes Dempster–Shafer reasoning systems skeptical\nin a way that has some intuitive appeal. Now suppose you have an expert at your disposal\nwho testiﬁes with 90% certainty that the coin is fair (i.e., he is 90% sure that P(Heads) =\n0.5). Then Dempster–Shafer theory gives Bel(Heads) = 0.9 × 0.5 = 0.45 and likewise\nBel(¬Heads) = 0.45. There is still a 10 percentage point “gap” that is not accounted for by\nthe evidence.\nThe mathematical underpinnings of Dempster–Shafer theory have a similar ﬂavor to\nthose of probability theory; the main difference is that, instead of assigning probabilities\nto possible worlds, the theory assigns masses to sets of possible world, that is, to events.\nMASS\nThe masses still must add to 1 over all possible events. Bel(A) is deﬁned to be the sum of\nmasses for all events that are subsets of (i.e., that entail) A, including A itself. With this\ndeﬁnition, Bel(A) and Bel(¬A) sum to at most 1, and the gap—the interval between Bel(A)\nand 1 −Bel(¬A)—is often interpreted as bounding the probability of A.\nAs with default reasoning, there is a problem in connecting beliefs to actions. Whenever\nthere is a gap in the beliefs, then a decision problem can be deﬁned such that a Dempster–\nShafer system is unable to make a decision. In fact, the notion of utility in the Dempster–\nShafer model is not yet well understood because the meanings of masses and beliefs them-\nselves have yet to be understood. Pearl (1988) has argued that Bel(A) should be interpreted\nnot as a degree of belief in A but as the probability assigned to all the possible worlds (now\ninterpreted as logical theories) in which A is provable. While there are cases in which this\nquantity might be of interest, it is not the same as the probability that A is true.\nA Bayesian analysis of the coin-ﬂipping example would suggest that no new formalism\nis necessary to handle such cases. The model would have two variables: the Bias of the coin\n(a number between 0 and 1, where 0 is a coin that always shows tails and 1 a coin that always",
  "is necessary to handle such cases. The model would have two variables: the Bias of the coin\n(a number between 0 and 1, where 0 is a coin that always shows tails and 1 a coin that always\nshows heads) and the outcome of the next Flip. The prior probability distribution for Bias 550\nChapter\n14.\nProbabilistic Reasoning\nwould reﬂect our beliefs based on the source of the coin (the magician’s pocket): some small\nprobability that it is fair and some probability that it is heavily biased toward heads or tails.\nThe conditional distribution P(Flip | Bias) simply deﬁnes how the bias operates. If P(Bias)\nis symmetric about 0.5, then our prior probability for the ﬂip is\nP(Flip = heads) =\n\u001a 1\n0\nP(Bias = x)P(Flip = heads | Bias = x) dx = 0.5 .\nThis is the same prediction as if we believe strongly that the coin is fair, but that does not\nmean that probability theory treats the two situations identically. The difference arises after\nthe ﬂips in computing the posterior distribution for Bias. If the coin came from a bank, then\nseeing it come up heads three times running would have almost no effect on our strong prior\nbelief in its fairness; but if the coin comes from the magician’s pocket, the same evidence\nwill lead to a stronger posterior belief that the coin is biased toward heads. Thus, a Bayesian\napproach expresses our “ignorance” in terms of how our beliefs would change in the face of\nfuture information gathering.\n14.7.3\nRepresenting vagueness: Fuzzy sets and fuzzy logic\nFuzzy set theory is a means of specifying how well an object satisﬁes a vague description.\nFUZZY SET THEORY\nFor example, consider the proposition “Nate is tall.” Is this true if Nate is 5′ 10′′? Most\npeople would hesitate to answer “true” or “false,” preferring to say, “sort of.” Note that this\nis not a question of uncertainty about the external world—we are sure of Nate’s height. The\nissue is that the linguistic term “tall” does not refer to a sharp demarcation of objects into two\nclasses—there are degrees of tallness. For this reason, fuzzy set theory is not a method for\nuncertain reasoning at all. Rather, fuzzy set theory treats Tall as a fuzzy predicate and says\nthat the truth value of Tall(Nate) is a number between 0 and 1, rather than being just true\nor false. The name “fuzzy set” derives from the interpretation of the predicate as implicitly\ndeﬁning a set of its members—a set that does not have sharp boundaries.",
  "or false. The name “fuzzy set” derives from the interpretation of the predicate as implicitly\ndeﬁning a set of its members—a set that does not have sharp boundaries.\nFuzzy logic is a method for reasoning with logical expressions describing membership\nFUZZY LOGIC\nin fuzzy sets. For example, the complex sentence Tall(Nate) ∧Heavy(Nate) has a fuzzy\ntruth value that is a function of the truth values of its components. The standard rules for\nevaluating the fuzzy truth, T, of a complex sentence are\nT(A ∧B) = min(T(A), T(B))\nT(A ∨B) = max(T(A), T(B))\nT(¬A) = 1 −T(A) .\nFuzzy logic is therefore a truth-functional system—a fact that causes serious difﬁculties.\nFor example, suppose that T(Tall(Nate)) = 0.6 and T(Heavy(Nate)) = 0.4. Then we have\nT(Tall(Nate) ∧Heavy(Nate)) = 0.4, which seems reasonable, but we also get the result\nT(Tall(Nate) ∧¬Tall(Nate)) = 0.4, which does not. Clearly, the problem arises from the\ninability of a truth-functional approach to take into account the correlations or anticorrelations\namong the component propositions.\nFuzzy control is a methodology for constructing control systems in which the mapping\nFUZZY CONTROL\nbetween real-valued input and output parameters is represented by fuzzy rules. Fuzzy con-\ntrol has been very successful in commercial products such as automatic transmissions, video Section 14.8.\nSummary\n551\ncameras, and electric shavers. Critics (see, e.g., Elkan, 1993) argue that these applications\nare successful because they have small rule bases, no chaining of inferences, and tunable\nparameters that can be adjusted to improve the system’s performance. The fact that they are\nimplemented with fuzzy operators might be incidental to their success; the key is simply to\nprovide a concise and intuitive way to specify a smoothly interpolated, real-valued function.\nThere have been attempts to provide an explanation of fuzzy logic in terms of probabil-\nity theory. One idea is to view assertions such as “Nate is Tall” as discrete observations made\nconcerning a continuous hidden variable, Nate’s actual Height. The probability model speci-\nﬁes P(Observer says Nate is tall | Height), perhaps using a probit distribution as described\non page 522. A posterior distribution over Nate’s height can then be calculated in the usual\nway, for example, if the model is part of a hybrid Bayesian network. Such an approach is not\ntruth-functional, of course. For example, the conditional distribution",
  "way, for example, if the model is part of a hybrid Bayesian network. Such an approach is not\ntruth-functional, of course. For example, the conditional distribution\nP(Observer says Nate is tall and heavy | Height, Weight)\nallows for interactions between height and weight in the causing of the observation. Thus,\nsomeone who is eight feet tall and weighs 190 pounds is very unlikely to be called “tall and\nheavy,” even though “eight feet” counts as “tall” and “190 pounds” counts as “heavy.”\nFuzzy predicates can also be given a probabilistic interpretation in terms of random\nsets—that is, random variables whose possible values are sets of objects. For example, Tall\nRANDOM SET\nis a random set whose possible values are sets of people. The probability P(Tall = S1),\nwhere S1 is some particular set of people, is the probability that exactly that set would be\nidentiﬁed as “tall” by an observer. Then the probability that “Nate is tall” is the sum of the\nprobabilities of all the sets of which Nate is a member.\nBoth the hybrid Bayesian network approach and the random sets approach appear to\ncapture aspects of fuzziness without introducing degrees of truth. Nonetheless, there remain\nmany open issues concerning the proper representation of linguistic observations and contin-\nuous quantities—issues that have been neglected by most outside the fuzzy community.\n14.8\nSUMMARY\nThis chapter has described Bayesian networks, a well-developed representation for uncertain\nknowledge. Bayesian networks play a role roughly analogous to that of propositional logic\nfor deﬁnite knowledge.\n• A Bayesian network is a directed acyclic graph whose nodes correspond to random\nvariables; each node has a conditional distribution for the node, given its parents.\n• Bayesian networks provide a concise way to represent conditional independence rela-\ntionships in the domain.\n• A Bayesian network speciﬁes a full joint distribution; each joint entry is deﬁned as the\nproduct of the corresponding entries in the local conditional distributions. A Bayesian\nnetwork is often exponentially smaller than an explicitly enumerated joint distribution.\n• Many conditional distributions can be represented compactly by canonical families of 552\nChapter\n14.\nProbabilistic Reasoning\ndistributions. Hybrid Bayesian networks, which include both discrete and continuous\nvariables, use a variety of canonical distributions.\n• Inference in Bayesian networks means computing the probability distribution of a set",
  "Probabilistic Reasoning\ndistributions. Hybrid Bayesian networks, which include both discrete and continuous\nvariables, use a variety of canonical distributions.\n• Inference in Bayesian networks means computing the probability distribution of a set\nof query variables, given a set of evidence variables. Exact inference algorithms, such\nas variable elimination, evaluate sums of products of conditional probabilities as efﬁ-\nciently as possible.\n• In polytrees (singly connected networks), exact inference takes time linear in the size\nof the network. In the general case, the problem is intractable.\n• Stochastic approximation techniques such as likelihood weighting and Markov chain\nMonte Carlo can give reasonable estimates of the true posterior probabilities in a net-\nwork and can cope with much larger networks than can exact algorithms.\n• Probability theory can be combined with representational ideas from ﬁrst-order logic to\nproduce very powerful systems for reasoning under uncertainty. Relational probabil-\nity models (RPMs) include representational restrictions that guarantee a well-deﬁned\nprobability distribution that can be expressed as an equivalent Bayesian network. Open-\nuniverse probability models handle existence and identity uncertainty, deﬁning prob-\nabilty distributions over the inﬁnite space of ﬁrst-order possible worlds.\n• Various alternative systems for reasoning under uncertainty have been suggested. Gen-\nerally speaking, truth-functional systems are not well suited for such reasoning.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe use of networks to represent probabilistic information began early in the 20th century,\nwith the work of Sewall Wright on the probabilistic analysis of genetic inheritance and an-\nimal growth factors (Wright, 1921, 1934). I. J. Good (1961), in collaboration with Alan\nTuring, developed probabilistic representations and Bayesian inference methods that could\nbe regarded as a forerunner of modern Bayesian networks—although the paper is not often\ncited in this context.10 The same paper is the original source for the noisy-OR model.\nThe inﬂuence diagram representation for decision problems, which incorporated a\nDAG representation for random variables, was used in decision analysis in the late 1970s\n(see Chapter 16), but only enumeration was used for evaluation. Judea Pearl developed the\nmessage-passing method for carrying out inference in tree networks (Pearl, 1982a) and poly-",
  "(see Chapter 16), but only enumeration was used for evaluation. Judea Pearl developed the\nmessage-passing method for carrying out inference in tree networks (Pearl, 1982a) and poly-\ntree networks (Kim and Pearl, 1983) and explained the importance of causal rather than di-\nagnostic probability models, in contrast to the certainty-factor systems then in vogue.\nThe ﬁrst expert system using Bayesian networks was CONVINCE (Kim, 1983). Early\napplications in medicine included the MUNIN system for diagnosing neuromuscular disorders\n(Andersen et al., 1989) and the PATHFINDER system for pathology (Heckerman, 1991). The\nCPCS system (Pradhan et al., 1994) is a Bayesian network for internal medicine consisting\n10 I. J. Good was chief statistician for Turing’s code-breaking team in World War II. In 2001: A Space Odyssey\n(Clarke, 1968a), Good and Minsky are credited with making the breakthrough that led to the development of the\nHAL 9000 computer. Bibliographical and Historical Notes\n553\nof 448 nodes, 906 links and 8,254 conditional probability values. (The front cover shows a\nportion of the network.)\nApplications in engineering include the Electric Power Research Institute’s work on\nmonitoring power generators (Morjaria et al., 1995), NASA’s work on displaying time-\ncritical information at Mission Control in Houston (Horvitz and Barry, 1995), and the general\nﬁeld of network tomography, which aims to infer unobserved local properties of nodes and\nlinks in the Internet from observations of end-to-end message performance (Castro et al.,\n2004). Perhaps the most widely used Bayesian network systems have been the diagnosis-\nand-repair modules (e.g., the Printer Wizard) in Microsoft Windows (Breese and Heckerman,\n1996) and the Ofﬁce Assistant in Microsoft Ofﬁce (Horvitz et al., 1998). Another impor-\ntant application area is biology: Bayesian networks have been used for identifying human\ngenes by reference to mouse genes (Zhang et al., 2003), inferring cellular networks Friedman\n(2004), and many other tasks in bioinformatics. We could go on, but instead we’ll refer you\nto Pourret et al. (2008), a 400-page guide to applications of Bayesian networks.\nRoss Shachter (1986), working in the inﬂuence diagram community, developed the ﬁrst\ncomplete algorithm for general Bayesian networks. His method was based on goal-directed\nreduction of the network using posterior-preserving transformations. Pearl (1986) developed",
  "complete algorithm for general Bayesian networks. His method was based on goal-directed\nreduction of the network using posterior-preserving transformations. Pearl (1986) developed\na clustering algorithm for exact inference in general Bayesian networks, utilizing a conversion\nto a directed polytree of clusters in which message passing was used to achieve consistency\nover variables shared between clusters. A similar approach, developed by the statisticians\nDavid Spiegelhalter and Steffen Lauritzen (Lauritzen and Spiegelhalter, 1988), is based on\nconversion to an undirected form of graphical model called a Markov network. This ap-\nMARKOV NETWORK\nproach is implemented in the HUGIN system, an efﬁcient and widely used tool for uncertain\nreasoning (Andersen et al., 1989). Boutilier et al. (1996) show how to exploit context-speciﬁc\nindependence in clustering algorithms.\nThe basic idea of variable elimination—that repeated computations within the overall\nsum-of-products expression can be avoided by caching—appeared in the symbolic probabilis-\ntic inference (SPI) algorithm (Shachter et al., 1990). The elimination algorithm we describe\nis closest to that developed by Zhang and Poole (1994). Criteria for pruning irrelevant vari-\nables were developed by Geiger et al. (1990) and by Lauritzen et al. (1990); the criterion we\ngive is a simple special case of these. Dechter (1999) shows how the variable elimination idea\nis essentially identical to nonserial dynamic programming (Bertele and Brioschi, 1972), an\nNONSERIAL DYNAMIC\nPROGRAMMING\nalgorithmic approach that can be applied to solve a range of inference problems in Bayesian\nnetworks—for example, ﬁnding the most likely explanation for a set of observations. This\nconnects Bayesian network algorithms to related methods for solving CSPs and gives a direct\nmeasure of the complexity of exact inference in terms of the tree width of the network. Wexler\nand Meek (2009) describe a method of preventing exponential growth in the size of factors\ncomputed in variable elimination; their algorithm breaks down large factors into products of\nsmaller factors and simultaneously computes an error bound for the resulting approximation.\nThe inclusion of continuous random variables in Bayesian networks was considered\nby Pearl (1988) and Shachter and Kenley (1989); these papers discussed networks contain-\ning only continuous variables with linear Gaussian distributions. The inclusion of discrete",
  "by Pearl (1988) and Shachter and Kenley (1989); these papers discussed networks contain-\ning only continuous variables with linear Gaussian distributions. The inclusion of discrete\nvariables has been investigated by Lauritzen and Wermuth (1989) and implemented in the 554\nChapter\n14.\nProbabilistic Reasoning\ncHUGIN system (Olesen, 1993). Further analysis of linear Gaussian models, with connec-\ntions to many other models used in statistics, appears in Roweis and Ghahramani (1999) The\nprobit distribution is usually attributed to Gaddum (1933) and Bliss (1934), although it had\nbeen discovered several times in the 19th century. Bliss’s work was expanded considerably\nby Finney (1947). The probit has been used widely for modeling discrete choice phenomena\nand can be extended to handle more than two choices (Daganzo, 1979). The logit model was\nintroduced by Berkson (1944); initially much derided, it eventually became more popular\nthan the probit model. Bishop (1995) gives a simple justiﬁcation for its use.\nCooper (1990) showed that the general problem of inference in unconstrained Bayesian\nnetworks is NP-hard, and Paul Dagum and Mike Luby (1993) showed the corresponding\napproximation problem to be NP-hard. Space complexity is also a serious problem in both\nclustering and variable elimination methods. The method of cutset conditioning, which was\ndeveloped for CSPs in Chapter 6, avoids the construction of exponentially large tables. In a\nBayesian network, a cutset is a set of nodes that, when instantiated, reduces the remaining\nnodes to a polytree that can be solved in linear time and space. The query is answered by\nsumming over all the instantiations of the cutset, so the overall space requirement is still lin-\near (Pearl, 1988). Darwiche (2001) describes a recursive conditioning algorithm that allows\na complete range of space/time tradeoffs.\nThe development of fast approximation algorithms for Bayesian network inference is\na very active area, with contributions from statistics, computer science, and physics. The\nrejection sampling method is a general technique that is long known to statisticians; it was\nﬁrst applied to Bayesian networks by Max Henrion (1988), who called it logic sampling.\nLikelihood weighting, which was developed by Fung and Chang (1989) and Shachter and\nPeot (1989), is an example of the well-known statistical method of importance sampling.\nCheng and Druzdzel (2000) describe an adaptive version of likelihood weighting that works",
  "Peot (1989), is an example of the well-known statistical method of importance sampling.\nCheng and Druzdzel (2000) describe an adaptive version of likelihood weighting that works\nwell even when the evidence has very low prior likelihood.\nMarkov chain Monte Carlo (MCMC) algorithms began with the Metropolis algorithm,\ndue to Metropolis et al. (1953), which was also the source of the simulated annealing algo-\nrithm described in Chapter 4. The Gibbs sampler was devised by Geman and Geman (1984)\nfor inference in undirected Markov networks. The application of MCMC to Bayesian net-\nworks is due to Pearl (1987). The papers collected by Gilks et al. (1996) cover a wide variety\nof applications of MCMC, several of which were developed in the well-known BUGS pack-\nage (Gilks et al., 1994).\nThere are two very important families of approximation methods that we did not cover\nin the chapter. The ﬁrst is the family of variational approximation methods, which can be\nVARIATIONAL\nAPPROXIMATION\nused to simplify complex calculations of all kinds. The basic idea is to propose a reduced\nversion of the original problem that is simple to work with, but that resembles the original\nproblem as closely as possible. The reduced problem is described by some variational pa-\nrameters λ that are adjusted to minimize a distance function D between the original and\nVARIATIONAL\nPARAMETER\nthe reduced problem, often by solving the system of equations ∂D/∂λ = 0. In many cases,\nstrict upper and lower bounds can be obtained. Variational methods have long been used in\nstatistics (Rustagi, 1976). In statistical physics, the mean-ﬁeld method is a particular vari-\nMEAN FIELD\national approximation in which the individual variables making up the model are assumed Bibliographical and Historical Notes\n555\nto be completely independent. This idea was applied to solve large undirected Markov net-\nworks (Peterson and Anderson, 1987; Parisi, 1988). Saul et al. (1996) developed the math-\nematical foundations for applying variational methods to Bayesian networks and obtained\naccurate lower-bound approximations for sigmoid networks with the use of mean-ﬁeld meth-\nods. Jaakkola and Jordan (1996) extended the methodology to obtain both lower and upper\nbounds. Since these early papers, variational methods have been applied to many speciﬁc\nfamilies of models. The remarkable paper by Wainwright and Jordan (2008) provides a uni-\nfying theoretical analysis of the literature on variational methods.",
  "bounds. Since these early papers, variational methods have been applied to many speciﬁc\nfamilies of models. The remarkable paper by Wainwright and Jordan (2008) provides a uni-\nfying theoretical analysis of the literature on variational methods.\nA second important family of approximation algorithms is based on Pearl’s polytree\nmessage-passing algorithm (1982a). This algorithm can be applied to general networks, as\nsuggested by Pearl (1988). The results might be incorrect, or the algorithm might fail to ter-\nminate, but in many cases, the values obtained are close to the true values. Little attention\nwas paid to this so-called belief propagation (or BP) approach until McEliece et al. (1998)\nBELIEF\nPROPAGATION\nobserved that message passing in a multiply connected Bayesian network was exactly the\ncomputation performed by the turbo decoding algorithm (Berrou et al., 1993), which pro-\nTURBO DECODING\nvided a major breakthrough in the design of efﬁcient error-correcting codes. The implication\nis that BP is both fast and accurate on the very large and very highly connected networks used\nfor decoding and might therefore be useful more generally. Murphy et al. (1999) presented a\npromising empirical study of BP’s performance, and Weiss and Freeman (2001) established\nstrong convergence results for BP on linear Gaussian networks. Weiss (2000b) shows how an\napproximation called loopy belief propagation works, and when the approximation is correct.\nYedidia et al. (2005) made further connections between loopy propagation and ideas from\nstatistical physics.\nThe connection between probability and ﬁrst-order languages was ﬁrst studied by Car-\nnap (1950). Gaifman (1964) and Scott and Krauss (1966) deﬁned a language in which proba-\nbilities could be associated with ﬁrst-order sentences and for which models were probability\nmeasures on possible worlds. Within AI, this idea was developed for propositional logic\nby Nilsson (1986) and for ﬁrst-order logic by Halpern (1990). The ﬁrst extensive inves-\ntigation of knowledge representation issues in such languages was carried out by Bacchus\n(1990). The basic idea is that each sentence in the knowledge base expressed a constraint on\nthe distribution over possible worlds; one sentence entails another if it expresses a stronger\nconstraint. For example, the sentence ∀x\nP(Hungry(x)) > 0.2 rules out distributions\nin which any object is hungry with probability less than 0.2; thus, it entails the sentence",
  "constraint. For example, the sentence ∀x\nP(Hungry(x)) > 0.2 rules out distributions\nin which any object is hungry with probability less than 0.2; thus, it entails the sentence\n∀x P(Hungry(x)) > 0.1. It turns out that writing a consistent set of sentences in these\nlanguages is quite difﬁcult and constructing a unique probability model nearly impossible\nunless one adopts the representation approach of Bayesian networks by writing suitable sen-\ntences about conditional probabilities.\nBeginning in the early 1990s, researchers working on complex applications noticed\nthe expressive limitations of Bayesian networks and developed various languages for writing\n“templates” with logical variables, from which large networks could be constructed automat-\nically for each problem instance (Breese, 1992; Wellman et al., 1992). The most important\nsuch language was BUGS (Bayesian inference Using Gibbs Sampling) (Gilks et al., 1994),\nwhich combined Bayesian networks with the indexed random variable notation common in\nINDEXED RANDOM\nVARIABLE 556\nChapter\n14.\nProbabilistic Reasoning\nstatistics. (In BUGS, an indexed random variable looks like X[i], where i has a deﬁned integer\nrange.) These languages inherited the key property of Bayesian networks: every well-formed\nknowledge base deﬁnes a unique, consistent probability model. Languages with well-deﬁned\nsemantics based on unique names and domain closure drew on the representational capa-\nbilities of logic programming (Poole, 1993; Sato and Kameya, 1997; Kersting et al., 2000)\nand semantic networks (Koller and Pfeffer, 1998; Pfeffer, 2000). Pfeffer (2007) went on to\ndevelop IBAL, which represents ﬁrst-order probability models as probabilistic programs in a\nprogramming language extended with a randomization primitive. Another important thread\nwas the combination of relational and ﬁrst-order notations with (undirected) Markov net-\nworks (Taskar et al., 2002; Domingos and Richardson, 2004), where the emphasis has been\nless on knowledge representation and more on learning from large data sets.\nInitially, inference in these models was performed by generating an equivalent Bayesian\nnetwork. Pfeffer et al. (1999) introduced a variable elimination algorithm that cached each\ncomputed factor for reuse by later computations involving the same relations but different\nobjects, thereby realizing some of the computational gains of lifting. The ﬁrst truly lifted",
  "computed factor for reuse by later computations involving the same relations but different\nobjects, thereby realizing some of the computational gains of lifting. The ﬁrst truly lifted\ninference algorithm was a lifted form of variable elimination described by Poole (2003) and\nsubsequently improved by de Salvo Braz et al. (2007). Further advances, including cases\nwhere certain aggregate probabilities can be computed in closed form, are described by Milch\net al. (2008) and Kisynski and Poole (2009). Pasula and Russell (2001) studied the application\nof MCMC to avoid building the complete equivalent Bayes net in cases of relational and\nidentity uncertainty. Getoor and Taskar (2007) collect many important papers on ﬁrst-order\nprobability models and their use in machine learning.\nProbabilistic reasoning about identity uncertainty has two distinct origins. In statis-\ntics, the problem of record linkage arises when data records do not contain standard unique\nRECORD LINKAGE\nidentiﬁers—for example, various citations of this book might name its ﬁrst author “Stuart\nRussell” or “S. J. Russell” or even “Stewart Russle,” and other authors may use the some of\nthe same names. Literally hundreds of companies exist solely to solve record linkage prob-\nlems in ﬁnancial, medical, census, and other data. Probabilistic analysis goes back to work\nby Dunn (1946); the Fellegi–Sunter model (1969), which is essentially naive Bayes applied\nto matching, still dominates current practice. The second origin for work on identity uncer-\ntainty is multitarget tracking (Sittler, 1964), which we cover in Chapter 15. For most of its\nhistory, work in symbolic AI assumed erroneously that sensors could supply sentences with\nunique identiﬁers for objects. The issue was studied in the context of language understanding\nby Charniak and Goldman (1992) and in the context of surveillance by (Huang and Russell,\n1998) and Pasula et al. (1999). Pasula et al. (2003) developed a complex generative model\nfor authors, papers, and citation strings, involving both relational and identity uncertainty,\nand demonstrated high accuracy for citation information extraction. The ﬁrst formally de-\nﬁned language for open-universe probability models was BLOG (Milch et al., 2005), which\ncame with a complete (albeit slow) MCMC inference algorithm for all well-deﬁned mdoels.\n(The program code faintly visible on the front cover of this book is part of a BLOG model",
  "came with a complete (albeit slow) MCMC inference algorithm for all well-deﬁned mdoels.\n(The program code faintly visible on the front cover of this book is part of a BLOG model\nfor detecting nuclear explosions from seismic signals as part of the UN Comprehensive Test\nBan Treaty veriﬁcation regime.) Laskey (2008) describes another open-universe modeling\nlanguage called multi-entity Bayesian networks. Bibliographical and Historical Notes\n557\nAs explained in Chapter 13, early probabilistic systems fell out of favor in the early\n1970s, leaving a partial vacuum to be ﬁlled by alternative methods. Certainty factors were\ninvented for use in the medical expert system MYCIN (Shortliffe, 1976), which was intended\nboth as an engineering solution and as a model of human judgment under uncertainty. The\ncollection Rule-Based Expert Systems (Buchanan and Shortliffe, 1984) provides a complete\noverview of MYCIN and its descendants (see also Steﬁk, 1995). David Heckerman (1986)\nshowed that a slightly modiﬁed version of certainty factor calculations gives correct proba-\nbilistic results in some cases, but results in serious overcounting of evidence in other cases.\nThe PROSPECTOR expert system (Duda et al., 1979) used a rule-based approach in which the\nrules were justiﬁed by a (seldom tenable) global independence assumption.\nDempster–Shafer theory originates with a paper by Arthur Dempster (1968) proposing\na generalization of probability to interval values and a combination rule for using them. Later\nwork by Glenn Shafer (1976) led to the Dempster-Shafer theory’s being viewed as a compet-\ning approach to probability. Pearl (1988) and Ruspini et al. (1992) analyze the relationship\nbetween the Dempster–Shafer theory and standard probability theory.\nFuzzy sets were developed by LotﬁZadeh (1965) in response to the perceived difﬁculty\nof providing exact inputs to intelligent systems. The text by Zimmermann (2001) provides\na thorough introduction to fuzzy set theory; papers on fuzzy applications are collected in\nZimmermann (1999). As we mentioned in the text, fuzzy logic has often been perceived\nincorrectly as a direct competitor to probability theory, whereas in fact it addresses a different\nset of issues. Possibility theory (Zadeh, 1978) was introduced to handle uncertainty in fuzzy\nPOSSIBILITY THEORY\nsystems and has much in common with probability. Dubois and Prade (1994) survey the\nconnections between possibility theory and probability theory.",
  "POSSIBILITY THEORY\nsystems and has much in common with probability. Dubois and Prade (1994) survey the\nconnections between possibility theory and probability theory.\nThe resurgence of probability depended mainly on Pearl’s development of Bayesian\nnetworks as a method for representing and using conditional independence information. This\nresurgence did not come without a ﬁght; Peter Cheeseman’s (1985) pugnacious “In Defense\nof Probability” and his later article “An Inquiry into Computer Understanding” (Cheeseman,\n1988, with commentaries) give something of the ﬂavor of the debate. Eugene Charniak\nhelped present the ideas to AI researchers with a popular article, “Bayesian networks with-\nout tears”11 (1991), and book (1993). The book by Dean and Wellman (1991) also helped\nintroduce Bayesian networks to AI researchers. One of the principal philosophical objections\nof the logicists was that the numerical calculations that probability theory was thought to re-\nquire were not apparent to introspection and presumed an unrealistic level of precision in our\nuncertain knowledge. The development of qualitative probabilistic networks (Wellman,\n1990a) provided a purely qualitative abstraction of Bayesian networks, using the notion of\npositive and negative inﬂuences between variables. Wellman shows that in many cases such\ninformation is sufﬁcient for optimal decision making without the need for the precise spec-\niﬁcation of probability values. Goldszmidt and Pearl (1996) take a similar approach. Work\nby Adnan Darwiche and Matt Ginsberg (1992) extracts the basic properties of conditioning\nand evidence combination from probability theory and shows that they can also be applied in\nlogical and default reasoning. Often, programs speak louder than words, and the ready avail-\n11 The title of the original version of the article was “Pearl for swine.” 558\nChapter\n14.\nProbabilistic Reasoning\nability of high-quality software such as the Bayes Net toolkit (Murphy, 2001) accelerated the\nadoption of the technology.\nThe most important single publication in the growth of Bayesian networks was undoubt-\nedly the text Probabilistic Reasoning in Intelligent Systems (Pearl, 1988). Several excellent\ntexts (Lauritzen, 1996; Jensen, 2001; Korb and Nicholson, 2003; Jensen, 2007; Darwiche,\n2009; Koller and Friedman, 2009) provide thorough treatments of the topics we have cov-\nered in this chapter. New research on probabilistic reasoning appears both in mainstream",
  "2009; Koller and Friedman, 2009) provide thorough treatments of the topics we have cov-\nered in this chapter. New research on probabilistic reasoning appears both in mainstream\nAI journals, such as Artiﬁcial Intelligence and the Journal of AI Research, and in more spe-\ncialized journals, such as the International Journal of Approximate Reasoning. Many papers\non graphical models, which include Bayesian networks, appear in statistical journals. The\nproceedings of the conferences on Uncertainty in Artiﬁcial Intelligence (UAI), Neural Infor-\nmation Processing Systems (NIPS), and Artiﬁcial Intelligence and Statistics (AISTATS) are\nexcellent sources for current research.\nEXERCISES\n14.1\nWe have a bag of three biased coins a, b, and c with probabilities of coming up heads\nof 20%, 60%, and 80%, respectively. One coin is drawn randomly from the bag (with equal\nlikelihood of drawing each of the three coins), and then the coin is ﬂipped three times to\ngenerate the outcomes X1, X2, and X3.\na. Draw the Bayesian network corresponding to this setup and deﬁne the necessary CPTs.\nb. Calculate which coin was most likely to have been drawn from the bag if the observed\nﬂips come out heads twice and tails once.\n14.2\nEquation (14.1) on page 513 deﬁnes the joint distribution represented by a Bayesian\nnetwork in terms of the parameters θ(Xi | Parents(Xi)). This exercise asks you to derive the\nequivalence between the parameters and the conditional probabilities P(Xi | Parents(Xi))\nfrom this deﬁnition.\na. Consider a simple network X →Y →Z with three Boolean variables. Use Equa-\ntions (13.3) and (13.6) (pages 485 and 492) to express the conditional probability\nP(z | y) as the ratio of two sums, each over entries in the joint distribution P(X, Y, Z).\nb. Now use Equation (14.1) to write this expression in terms of the network parameters\nθ(X), θ(Y | X), and θ(Z | Y ).\nc. Next, expand out the summations in your expression from part (b), writing out explicitly\nthe terms for the true and false values of each summed variable. Assuming that all\nnetwork parameters satisfy the constraint \u0002\nxi θ(xi | parents(Xi)) = 1, show that the\nresulting expression reduces to θ(x | y).\nd. Generalize this derivation to show that θ(Xi | Parents(Xi)) = P(Xi | Parents(Xi))\nfor any Bayesian network. Exercises\n559\n14.3\nThe operation of arc reversal in a Bayesian network allows us to change the direction\nARC REVERSAL\nof an arc X →Y while preserving the joint probability distribution that the network repre-",
  "for any Bayesian network. Exercises\n559\n14.3\nThe operation of arc reversal in a Bayesian network allows us to change the direction\nARC REVERSAL\nof an arc X →Y while preserving the joint probability distribution that the network repre-\nsents (Shachter, 1986). Arc reversal may require introducing new arcs: all the parents of X\nalso become parents of Y , and all parents of Y also become parents of X.\na. Assume that X and Y start with m and n parents, respectively, and that all variables\nhave k values. By calculating the change in size for the CPTs of X and Y , show that the\ntotal number of parameters in the network cannot decrease during arc reversal. (Hint:\nthe parents of X and Y need not be disjoint.)\nb. Under what circumstances can the total number remain constant?\nc. Let the parents of X be U ∪V and the parents of Y be V ∪W, where U and W are\ndisjoint. The formulas for the new CPTs after arc reversal are as follows:\nP(Y | U, V, W) =\n\f\nx\nP(Y | V, W, x)P(x | U, V)\nP(X | U, V, W, Y ) = P(Y | X, V, W)P(X | U, V)/P(Y | U, V, W) .\nProve that the new network expresses the same joint distribution over all variables as\nthe original network.\n14.4\nConsider the Bayesian network in Figure 14.2.\na. If no evidence is observed, are Burglary and Earthquake independent? Prove this from\nthe numerical semantics and from the topological semantics.\nb. If we observe Alarm = true, are Burglary and Earthquake independent? Justify your\nanswer by calculating whether the probabilities involved satisfy the deﬁnition of condi-\ntional independence.\n14.5\nSuppose that in a Bayesian network containing an unobserved variable Y , all the vari-\nables in the Markov blanket MB(Y ) have been observed.\na. Prove that removing the node Y from the network will not affect the posterior distribu-\ntion for any other unobserved variable in the network.\nb. Discuss whether we can remove Y if we are planning to use (i) rejection sampling and\n(ii) likelihood weighting.\n14.6\nLet Hx be a random variable denoting the handedness of an individual x, with possible\nvalues l or r. A common hypothesis is that left- or right-handedness is inherited by a simple\nmechanism; that is, perhaps there is a gene Gx, also with values l or r, and perhaps actual\nhandedness turns out mostly the same (with some probability s) as the gene an individual\npossesses. Furthermore, perhaps the gene itself is equally likely to be inherited from either",
  "handedness turns out mostly the same (with some probability s) as the gene an individual\npossesses. Furthermore, perhaps the gene itself is equally likely to be inherited from either\nof an individual’s parents, with a small nonzero probability m of a random mutation ﬂipping\nthe handedness.\na. Which of the three networks in Figure 14.20 claim that P(Gfather, Gmother, Gchild) =\nP(Gfather)P(Gmother)P(Gchild)?\nb. Which of the three networks make independence claims that are consistent with the\nhypothesis about the inheritance of handedness? 560\nChapter\n14.\nProbabilistic Reasoning\nHmother\nHfather\nHchild\nmother\nG\nfather\nG\nchild\nG\nHmother\nHfather\nHchild\nmother\nG\nfather\nG\nchild\nG\nHmother\nHfather\nHchild\nmother\nG\nfather\nG\nchild\nG\n(a)\n(b)\n(c)\nFigure 14.20\nThree possible structures for a Bayesian network describing genetic inheri-\ntance of handedness.\nc. Which of the three networks is the best description of the hypothesis?\nd. Write down the CPT for the Gchild node in network (a), in terms of s and m.\ne. Suppose that P(Gfather = l) = P(Gmother = l) = q. In network (a), derive an expres-\nsion for P(Gchild = l) in terms of m and q only, by conditioning on its parent nodes.\nf. Under conditions of genetic equilibrium, we expect the distribution of genes to be the\nsame across generations. Use this to calculate the value of q, and, given what you know\nabout handedness in humans, explain why the hypothesis described at the beginning of\nthis question must be wrong.\n14.7\nThe Markov blanket of a variable is deﬁned on page 517. Prove that a variable\nis independent of all other variables in the network, given its Markov blanket and derive\nEquation (14.12) (page 538).\nRadio\nBattery\nIgnition\nGas\nStarts\nMoves\nFigure 14.21\nA Bayesian network describing some features of a car’s electrical system\nand engine. Each variable is Boolean, and the true value indicates that the corresponding\naspect of the vehicle is in working order. Exercises\n561\n14.8\nConsider the network for car diagnosis shown in Figure 14.21.\na. Extend the network with the Boolean variables IcyWeather and StarterMotor.\nb. Give reasonable conditional probability tables for all the nodes.\nc. How many independent values are contained in the joint probability distribution for\neight Boolean nodes, assuming that no conditional independence relations are known\nto hold among them?\nd. How many independent probability values do your network tables contain?",
  "eight Boolean nodes, assuming that no conditional independence relations are known\nto hold among them?\nd. How many independent probability values do your network tables contain?\ne. The conditional distribution for Starts could be described as a noisy-AND distribution.\nDeﬁne this family in general and relate it to the noisy-OR distribution.\n14.9\nConsider the family of linear Gaussian networks, as deﬁned on page 520.\na. In a two-variable network, let X1 be the parent of X2, let X1 have a Gaussian prior,\nand let P(X2 | X1) be a linear Gaussian distribution. Show that the joint distribution\nP(X1, X2) is a multivariate Gaussian, and calculate its covariance matrix.\nb. Prove by induction that the joint distribution for a general linear Gaussian network on\nX1, . . . , Xn is also a multivariate Gaussian.\n14.10\nThe probit distribution deﬁned on page 522 describes the probability distribution for\na Boolean child, given a single continuous parent.\na. How might the deﬁnition be extended to cover multiple continuous parents?\nb. How might it be extended to handle a multivalued child variable? Consider both cases\nwhere the child’s values are ordered (as in selecting a gear while driving, depending\non speed, slope, desired acceleration, etc.) and cases where they are unordered (as in\nselecting bus, train, or car to get to work). (Hint: Consider ways to divide the possible\nvalues into two sets, to mimic a Boolean variable.)\n14.11\nIn your local nuclear power station, there is an alarm that senses when a temperature\ngauge exceeds a given threshold. The gauge measures the temperature of the core. Consider\nthe Boolean variables A (alarm sounds), FA (alarm is faulty), and FG (gauge is faulty) and\nthe multivalued nodes G (gauge reading) and T (actual core temperature).\na. Draw a Bayesian network for this domain, given that the gauge is more likely to fail\nwhen the core temperature gets too high.\nb. Is your network a polytree? Why or why not?\nc. Suppose there are just two possible actual and measured temperatures, normal and high;\nthe probability that the gauge gives the correct temperature is x when it is working, but\ny when it is faulty. Give the conditional probability table associated with G.\nd. Suppose the alarm works correctly unless it is faulty, in which case it never sounds.\nGive the conditional probability table associated with A.\ne. Suppose the alarm and gauge are working and the alarm sounds. Calculate an expres-",
  "d. Suppose the alarm works correctly unless it is faulty, in which case it never sounds.\nGive the conditional probability table associated with A.\ne. Suppose the alarm and gauge are working and the alarm sounds. Calculate an expres-\nsion for the probability that the temperature of the core is too high, in terms of the\nvarious conditional probabilities in the network. 562\nChapter\n14.\nProbabilistic Reasoning\nN\nN\n(i)\n(ii)\n(iii)\nF1\nF1\nF1\nM1\nM1\nM1\nF2\nF2\nF2\nM2\nM2\nM2\nN\nFigure 14.22\nThree possible networks for the telescope problem.\n14.12\nTwo astronomers in different parts of the world make measurements M1 and M2 of\nthe number of stars N in some small region of the sky, using their telescopes. Normally, there\nis a small possibility e of error by up to one star in each direction. Each telescope can also\n(with a much smaller probability f) be badly out of focus (events F1 and F2), in which case\nthe scientist will undercount by three or more stars (or if N is less than 3, fail to detect any\nstars at all). Consider the three networks shown in Figure 14.22.\na. Which of these Bayesian networks are correct (but not necessarily efﬁcient) represen-\ntations of the preceding information?\nb. Which is the best network? Explain.\nc. Write out a conditional distribution for P(M1 | N), for the case where N ∈{1, 2, 3} and\nM1 ∈{0, 1, 2, 3, 4}. Each entry in the conditional distribution should be expressed as a\nfunction of the parameters e and/or f.\nd. Suppose M1 = 1 and M2 = 3. What are the possible numbers of stars if you assume no\nprior constraint on the values of N?\ne. What is the most likely number of stars, given these observations? Explain how to\ncompute this, or if it is not possible to compute, explain what additional information is\nneeded and how it would affect the result.\n14.13\nConsider the network shown in Figure 14.22(ii), and assume that the two telescopes\nwork identically. N ∈{1, 2, 3} and M1, M2 ∈{0, 1, 2, 3, 4}, with the symbolic CPTs as de-\nscribed in Exercise 14.12. Using the enumeration algorithm (Figure 14.9 on page 525), cal-\nculate the probability distribution P(N | M1 = 2, M2 = 2).\n14.14\nConsider the Bayes net shown in Figure 14.23.\na. Which of the following are asserted by the network structure?\n(i) P(B, I, M) = P(B)P(I)P(M).\n(ii) P(J | G) = P(J | G, I).\n(iii) P(M | G, B, I) = P(M | G, B, I, J). Exercises\n563\nB\nI\nM\nG\nJ\nP(B)\n.9\nB  M      P(I)\n                  .9\n                  .5\n                  .5\n                  .1\nG\nP(J)\n.9\n.0\nt\n.9\nB\nM\nP(G)\n.0\n.0",
  "(ii) P(J | G) = P(J | G, I).\n(iii) P(M | G, B, I) = P(M | G, B, I, J). Exercises\n563\nB\nI\nM\nG\nJ\nP(B)\n.9\nB  M      P(I)\n                  .9\n                  .5\n                  .5\n                  .1\nG\nP(J)\n.9\n.0\nt\n.9\nB\nM\nP(G)\n.0\n.0\n.0\n.0\n.8\n.2\n.1\nI\nt\nt\nf\nt\nt\nf\nt\nt\nf\nf\nt\nf\nt\nt\nf\nf\nt\nf\nf\nt\nf\nf\nf\nt\nt\nf\nt\nf\nt\nf\nf\nP(M)\n.1\nt\nf\nFigure 14.23\nA simple Bayes net with Boolean variables B = BrokeElectionLaw,\nI = Indicted, M = PoliticallyMotivatedProsecutor , G = FoundGuilty, J = Jailed.\nb. Calculate the value of P(b, i, ¬m, g, j).\nc. Calculate the probability that someone goes to jail given that they broke the law, have\nbeen indicted, and face a politically motivated prosecutor.\nd. A context-speciﬁc independence (see page 542) allows a variable to be independent\nof some of its parents given certain values of others. In addition to the usual conditional\nindependences given by the graph structure, what context-speciﬁc independences exist\nin the Bayes net in Figure 14.23?\ne. Suppose we want to add the variable P = PresidentialPardon to the network; draw the\nnew network and brieﬂy explain any links you add.\n14.15\nConsider the variable elimination algorithm in Figure 14.11 (page 528).\na. Section 14.4 applies variable elimination to the query\nP(Burglary | JohnCalls = true, MaryCalls = true) .\nPerform the calculations indicated and check that the answer is correct.\nb. Count the number of arithmetic operations performed, and compare it with the number\nperformed by the enumeration algorithm.\nc. Suppose a network has the form of a chain: a sequence of Boolean variables X1, . . . , Xn\nwhere Parents(Xi) = {Xi−1} for i = 2, . . . , n. What is the complexity of computing\nP(X1 | Xn = true) using enumeration? Using variable elimination?\nd. Prove that the complexity of running variable elimination on a polytree network is linear\nin the size of the tree for any variable ordering consistent with the network structure.\n14.16\nInvestigate the complexity of exact inference in general Bayesian networks:\na. Prove that any 3-SAT problem can be reduced to exact inference in a Bayesian network\nconstructed to represent the particular problem and hence that exact inference is NP- 564\nChapter\n14.\nProbabilistic Reasoning\nhard. (Hint: Consider a network with one variable for each proposition symbol, one for\neach clause, and one for the conjunction of clauses.)\nb. The problem of counting the number of satisfying assignments for a 3-SAT problem is",
  "hard. (Hint: Consider a network with one variable for each proposition symbol, one for\neach clause, and one for the conjunction of clauses.)\nb. The problem of counting the number of satisfying assignments for a 3-SAT problem is\n#P-complete. Show that exact inference is at least as hard as this.\n14.17\nConsider the problem of generating a random sample from a speciﬁed distribution\non a single variable. Assume you have a random number generator that returns a random\nnumber uniformly distributed between 0 and 1.\na. Let X be a discrete variable with P(X = xi) = pi for i ∈{1, . . . , k}. The cumulative\ndistribution of X gives the probability that X ∈{x1, . . . , xj} for each possible j. (See\nCUMULATIVE\nDISTRIBUTION\nalso Appendix A.) Explain how to calculate the cumulative distribution in O(k) time\nand how to generate a single sample of X from it. Can the latter be done in less than\nO(k) time?\nb. Now suppose we want to generate N samples of X, where N ≫k. Explain how to do\nthis with an expected run time per sample that is constant (i.e., independent of k).\nc. Now consider a continuous-valued variable with a parameterized distribution (e.g.,\nGaussian). How can samples be generated from such a distribution?\nd. Suppose you want to query a continuous-valued variable and you are using a sampling\nalgorithm such as LIKELIHOODWEIGHTING to do the inference. How would you have\nto modify the query-answering process?\n14.18\nConsider the query P(Rain | Sprinkler = true, WetGrass = true) in Figure 14.12(a)\n(page 529) and how Gibbs sampling can answer it.\na. How many states does the Markov chain have?\nb. Calculate the transition matrix Q containing q(y →y′) for all y, y′.\nc. What does Q2, the square of the transition matrix, represent?\nd. What about Qn as n →∞?\ne. Explain how to do probabilistic inference in Bayesian networks, assuming that Qn is\navailable. Is this a practical way to do inference?\n14.19\nThis exercise explores the stationary distribution for Gibbs sampling methods.\na. The convex composition [α, q1; 1 −α, q2] of q1 and q2 is a transition probability distri-\nbution that ﬁrst chooses one of q1 and q2 with probabilities α and 1 −α, respectively,\nand then applies whichever is chosen. Prove that if q1 and q2 are in detailed balance\nwith π, then their convex composition is also in detailed balance with π. (Note: this\nresult justiﬁes a variant of GIBBS-ASK in which variables are chosen at random rather\nthan sampled in a ﬁxed sequence.)",
  "with π, then their convex composition is also in detailed balance with π. (Note: this\nresult justiﬁes a variant of GIBBS-ASK in which variables are chosen at random rather\nthan sampled in a ﬁxed sequence.)\nb. Prove that if each of q1 and q2 has π as its stationary distribution, then the sequential\ncomposition q = q1 ◦q2 also has π as its stationary distribution.\n14.20\nThe Metropolis–Hastings algorithm is a member of the MCMC family; as such, it is\nMETROPOLIS–\nHASTINGS\ndesigned to generate samples x (eventually) according to target probabilities π(x). (Typically Exercises\n565\nwe are interested in sampling from π(x) = P(x | e).) Like simulated annealing, Metropolis–\nHastings operates in two stages. First, it samples a new state x′ from a proposal distribution\nPROPOSAL\nDISTRIBUTION\nq(x′ | x), given the current state x. Then, it probabilistically accepts or rejects x′ according to\nthe acceptance probability\nACCEPTANCE\nPROBABILITY\nα(x′ | x) = min\n\r\n1, π(x′)q(x | x′)\nπ(x)q(x′ | x)\n\u000e\n.\nIf the proposal is rejected, the state remains at x.\na. Consider an ordinary Gibbs sampling step for a speciﬁc variable Xi. Show that this\nstep, considered as a proposal, is guaranteed to be accepted by Metropolis–Hastings.\n(Hence, Gibbs sampling is a special case of Metropolis–Hastings.)\nb. Show that the two-step process above, viewed as a transition probability distribution, is\nin detailed balance with π.\n14.21\nThree soccer teams A, B, and C, play each other once. Each match is between two\nteams, and can be won, drawn, or lost. Each team has a ﬁxed, unknown degree of quality—\nan integer ranging from 0 to 3—and the outcome of a match depends probabilistically on the\ndifference in quality between the two teams.\na. Construct a relational probability model to describe this domain, and suggest numerical\nvalues for all the necessary probability distributions.\nb. Construct the equivalent Bayesian network for the three matches.\nc. Suppose that in the ﬁrst two matches A beats B and draws with C. Using an exact\ninference algorithm of your choice, compute the posterior distribution for the outcome\nof the third match.\nd. Suppose there are n teams in the league and we have the results for all but the last\nmatch. How does the complexity of predicting the last game vary with n?\ne. Investigate the application of MCMC to this problem. How quickly does it converge in\npractice and how well does it scale? 15\nPROBABILISTIC\nREASONING OVER TIME",
  "match. How does the complexity of predicting the last game vary with n?\ne. Investigate the application of MCMC to this problem. How quickly does it converge in\npractice and how well does it scale? 15\nPROBABILISTIC\nREASONING OVER TIME\nIn which we try to interpret the present, understand the past, and perhaps predict\nthe future, even when very little is crystal clear.\nAgents in partially observable environments must be able to keep track of the current state, to\nthe extent that their sensors allow. In Section 4.4 we showed a methodology for doing that: an\nagent maintains a belief state that represents which states of the world are currently possible.\nFrom the belief state and a transition model, the agent can predict how the world might\nevolve in the next time step. From the percepts observed and a sensor model, the agent can\nupdate the belief state. This is a pervasive idea: in Chapter 4 belief states were represented by\nexplicitly enumerated sets of states, whereas in Chapters 7 and 11 they were represented by\nlogical formulas. Those approaches deﬁned belief states in terms of which world states were\npossible, but could say nothing about which states were likely or unlikely. In this chapter, we\nuse probability theory to quantify the degree of belief in elements of the belief state.\nAs we show in Section 15.1, time itself is handled in the same way as in Chapter 7: a\nchanging world is modeled using a variable for each aspect of the world state at each point in\ntime. The transition and sensor models may be uncertain: the transition model describes the\nprobability distribution of the variables at time t, given the state of the world at past times,\nwhile the sensor model describes the probability of each percept at time t, given the current\nstate of the world. Section 15.2 deﬁnes the basic inference tasks and describes the gen-\neral structure of inference algorithms for temporal models. Then we describe three speciﬁc\nkinds of models: hidden Markov models, Kalman ﬁlters, and dynamic Bayesian net-\nworks (which include hidden Markov models and Kalman ﬁlters as special cases). Finally,\nSection 15.6 examines the problems faced when keeping track of more than one thing.\n15.1\nTIME AND UNCERTAINTY\nWe have developed our techniques for probabilistic reasoning in the context of static worlds,\nin which each random variable has a single ﬁxed value. For example, when repairing a car,\nwe assume that whatever is broken remains broken during the process of diagnosis; our job",
  "in which each random variable has a single ﬁxed value. For example, when repairing a car,\nwe assume that whatever is broken remains broken during the process of diagnosis; our job\nis to infer the state of the car from observed evidence, which also remains ﬁxed.\n566 Section 15.1.\nTime and Uncertainty\n567\nNow consider a slightly different problem: treating a diabetic patient. As in the case of\ncar repair, we have evidence such as recent insulin doses, food intake, blood sugar measure-\nments, and other physical signs. The task is to assess the current state of the patient, including\nthe actual blood sugar level and insulin level. Given this information, we can make a deci-\nsion about the patient’s food intake and insulin dose. Unlike the case of car repair, here the\ndynamic aspects of the problem are essential. Blood sugar levels and measurements thereof\ncan change rapidly over time, depending on recent food intake and insulin doses, metabolic\nactivity, the time of day, and so on. To assess the current state from the history of evidence\nand to predict the outcomes of treatment actions, we must model these changes.\nThe same considerations arise in many other contexts, such as tracking the location of\na robot, tracking the economic activity of a nation, and making sense of a spoken or written\nsequence of words. How can dynamic situations like these be modeled?\n15.1.1\nStates and observations\nWe view the world as a series of snapshots, or time slices, each of which contains a set of\nTIME SLICE\nrandom variables, some observable and some not.1 For simplicity, we will assume that the\nsame subset of variables is observable in each time slice (although this is not strictly necessary\nin anything that follows). We will use Xt to denote the set of state variables at time t, which\nare assumed to be unobservable, and Et to denote the set of observable evidence variables.\nThe observation at time t is Et = et for some set of values et.\nConsider the following example: You are the security guard stationed at a secret under-\nground installation. You want to know whether it’s raining today, but your only access to the\noutside world occurs each morning when you see the director coming in with, or without, an\numbrella. For each day t, the set Et thus contains a single evidence variable Umbrellat or Ut\nfor short (whether the umbrella appears), and the set Xt contains a single state variable Raint",
  "umbrella. For each day t, the set Et thus contains a single evidence variable Umbrellat or Ut\nfor short (whether the umbrella appears), and the set Xt contains a single state variable Raint\nor Rt for short (whether it is raining). Other problems can involve larger sets of variables. In\nthe diabetes example, we might have evidence variables, such as MeasuredBloodSugar t and\nPulseRatet, and state variables, such as BloodSugar t and StomachContents t. (Notice that\nBloodSugar t and MeasuredBloodSugar t are not the same variable; this is how we deal with\nnoisy measurements of actual quantities.)\nThe interval between time slices also depends on the problem. For diabetes monitoring,\na suitable interval might be an hour rather than a day. In this chapter we assume the interval\nbetween slices is ﬁxed, so we can label times by integers. We will assume that the state\nsequence starts at t = 0; for various uninteresting reasons, we will assume that evidence starts\narriving at t = 1 rather than t = 0. Hence, our umbrella world is represented by state variables\nR0, R1, R2, . . . and evidence variables U1, U2, . . .. We will use the notation a:b to denote\nthe sequence of integers from a to b (inclusive), and the notation Xa:b to denote the set of\nvariables from Xa to Xb. For example, U1:3 corresponds to the variables U1, U2, U3.\n1 Uncertainty over continuous time can be modeled by stochastic differential equations (SDEs). The models\nstudied in this chapter can be viewed as discrete-time approximations to SDEs. 568\nChapter\n15.\nProbabilistic Reasoning over Time\nXt–2\nXt–1\nXt\n(a)\n(b)\nXt+1\nXt+2\nXt–2\nXt–1\nXt\nXt+1\nXt+2\nFigure 15.1\n(a) Bayesian network structure corresponding to a ﬁrst-order Markov process\nwith state deﬁned by the variables Xt. (b) A second-order Markov process.\n15.1.2\nTransition and sensor models\nWith the set of state and evidence variables for a given problem decided on, the next step is\nto specify how the world evolves (the transition model) and how the evidence variables get\ntheir values (the sensor model).\nThe transition model speciﬁes the probability distribution over the latest state variables,\ngiven the previous values, that is, P(Xt | X0:t−1). Now we face a problem: the set X0:t−1 is\nunbounded in size as t increases. We solve the problem by making a Markov assumption—\nMARKOV\nASSUMPTION\nthat the current state depends on only a ﬁnite ﬁxed number of previous states. Processes sat-",
  "unbounded in size as t increases. We solve the problem by making a Markov assumption—\nMARKOV\nASSUMPTION\nthat the current state depends on only a ﬁnite ﬁxed number of previous states. Processes sat-\nisfying this assumption were ﬁrst studied in depth by the Russian statistician Andrei Markov\n(1856–1922) and are called Markov processes or Markov chains. They come in various ﬂa-\nMARKOV PROCESS\nvors; the simplest is the ﬁrst-order Markov process, in which the current state depends only\nFIRST-ORDER\nMARKOV PROCESS\non the previous state and not on any earlier states. In other words, a state provides enough\ninformation to make the future conditionally independent of the past, and we have\nP(Xt | X0:t−1) = P(Xt | Xt−1) .\n(15.1)\nHence, in a ﬁrst-order Markov process, the transition model is the conditional distribution\nP(Xt | Xt−1). The transition model for a second-order Markov process is the conditional\ndistribution P(Xt | Xt−2, Xt−1). Figure 15.1 shows the Bayesian network structures corre-\nsponding to ﬁrst-order and second-order Markov processes.\nEven with the Markov assumption there is still a problem: there are inﬁnitely many\npossible values of t. Do we need to specify a different distribution for each time step? We\navoid this problem by assuming that changes in the world state are caused by a stationary\nprocess—that is, a process of change that is governed by laws that do not themselves change\nSTATIONARY\nPROCESS\nover time. (Don’t confuse stationary with static: in a static process, the state itself does not\nchange.) In the umbrella world, then, the conditional probability of rain, P(Rt | Rt−1), is the\nsame for all t, and we only have to specify one conditional probability table.\nNow for the sensor model. The evidence variables Et could depend on previous vari-\nables as well as the current state variables, but any state that’s worth its salt should sufﬁce to\ngenerate the current sensor values. Thus, we make a sensor Markov assumption as follows:\nSENSOR MARKOV\nASSUMPTION\nP(Et | X0:t, E0:t−1) = P(Et | Xt) .\n(15.2)\nThus, P(Et | Xt) is our sensor model (sometimes called the observation model). Figure 15.2\nshows both the transition model and the sensor model for the umbrella example. Notice the Section 15.1.\nTime and Uncertainty\n569\nRaint\nUmbrellat\nRaint–1\nUmbrellat–1\nRaint+1\nUmbrellat+1\nRt -1\nt\nP(R )\n0.3\nf\n0.7\nt\nt\nR\nt\nP(U  )\n0.9\nt\n0.2\nf\nFigure 15.2\nBayesian network structure and conditional distributions describing the\numbrella world.",
  "Time and Uncertainty\n569\nRaint\nUmbrellat\nRaint–1\nUmbrellat–1\nRaint+1\nUmbrellat+1\nRt -1\nt\nP(R )\n0.3\nf\n0.7\nt\nt\nR\nt\nP(U  )\n0.9\nt\n0.2\nf\nFigure 15.2\nBayesian network structure and conditional distributions describing the\numbrella world.\nThe transition model is P(Raint | Raint−1) and the sensor model is\nP(Umbrellat | Raint).\ndirection of the dependence between state and sensors: the arrows go from the actual state\nof the world to sensor values because the state of the world causes the sensors to take on\nparticular values: the rain causes the umbrella to appear. (The inference process, of course,\ngoes in the other direction; the distinction between the direction of modeled dependencies\nand the direction of inference is one of the principal advantages of Bayesian networks.)\nIn addition to specifying the transition and sensor models, we need to say how every-\nthing gets started—the prior probability distribution at time 0, P(X0). With that, we have a\nspeciﬁcation of the complete joint distribution over all the variables, using Equation (14.2).\nFor any t,\nP(X0:t, E1:t) = P(X0)\nt\u0019\ni = 1\nP(Xi | Xi−1) P(Ei | Xi) .\n(15.3)\nThe three terms on the right-hand side are the initial state model P(X0), the transition model\nP(Xi | Xi−1), and the sensor model P(Ei | Xi).\nThe structure in Figure 15.2 is a ﬁrst-order Markov process—the probability of rain is\nassumed to depend only on whether it rained the previous day. Whether such an assumption\nis reasonable depends on the domain itself. The ﬁrst-order Markov assumption says that the\nstate variables contain all the information needed to characterize the probability distribution\nfor the next time slice. Sometimes the assumption is exactly true—for example, if a particle\nis executing a random walk along the x-axis, changing its position by ±1 at each time step,\nthen using the x-coordinate as the state gives a ﬁrst-order Markov process. Sometimes the\nassumption is only approximate, as in the case of predicting rain only on the basis of whether\nit rained the previous day. There are two ways to improve the accuracy of the approximation:\n1. Increasing the order of the Markov process model. For example, we could make a\nsecond-order model by adding Raint−2 as a parent of Raint, which might give slightly\nmore accurate predictions. For example, in Palo Alto, California, it very rarely rains\nmore than two days in a row.\n2. Increasing the set of state variables. For example, we could add Seasont to allow 570\nChapter\n15.",
  "more accurate predictions. For example, in Palo Alto, California, it very rarely rains\nmore than two days in a row.\n2. Increasing the set of state variables. For example, we could add Seasont to allow 570\nChapter\n15.\nProbabilistic Reasoning over Time\nus to incorporate historical records of rainy seasons, or we could add Temperature t,\nHumidityt and Pressuret (perhaps at a range of locations) to allow us to use a physical\nmodel of rainy conditions.\nExercise 15.1 asks you to show that the ﬁrst solution—increasing the order—can always be\nreformulated as an increase in the set of state variables, keeping the order ﬁxed. Notice that\nadding state variables might improve the system’s predictive power but also increases the\nprediction requirements: we now have to predict the new variables as well. Thus, we are\nlooking for a “self-sufﬁcient” set of variables, which really means that we have to understand\nthe “physics” of the process being modeled. The requirement for accurate modeling of the\nprocess is obviously lessened if we can add new sensors (e.g., measurements of temperature\nand pressure) that provide information directly about the new state variables.\nConsider, for example, the problem of tracking a robot wandering randomly on the X–Y\nplane. One might propose that the position and velocity are a sufﬁcient set of state variables:\none can simply use Newton’s laws to calculate the new position, and the velocity may change\nunpredictably. If the robot is battery-powered, however, then battery exhaustion would tend to\nhave a systematic effect on the change in velocity. Because this in turn depends on how much\npower was used by all previous maneuvers, the Markov property is violated. We can restore\nthe Markov property by including the charge level Batteryt as one of the state variables that\nmake up Xt. This helps in predicting the motion of the robot, but in turn requires a model\nfor predicting Batteryt from Batteryt−1 and the velocity. In some cases, that can be done\nreliably, but more often we ﬁnd that error accumulates over time. In that case, accuracy can\nbe improved by adding a new sensor for the battery level.\n15.2\nINFERENCE IN TEMPORAL MODELS\nHaving set up the structure of a generic temporal model, we can formulate the basic inference\ntasks that must be solved:\n• Filtering: This is the task of computing the belief state—the posterior distribution\nFILTERING\nBELIEF STATE\nover the most recent state—given all evidence to date. Filtering2 is also called state",
  "tasks that must be solved:\n• Filtering: This is the task of computing the belief state—the posterior distribution\nFILTERING\nBELIEF STATE\nover the most recent state—given all evidence to date. Filtering2 is also called state\nestimation. In our example, we wish to compute P(Xt | e1:t). In the umbrella example,\nSTATE ESTIMATION\nthis would mean computing the probability of rain today, given all the observations of\nthe umbrella carrier made so far. Filtering is what a rational agent does to keep track\nof the current state so that rational decisions can be made. It turns out that an almost\nidentical calculation provides the likelihood of the evidence sequence, P(e1:t).\n• Prediction: This is the task of computing the posterior distribution over the future state,\nPREDICTION\ngiven all evidence to date. That is, we wish to compute P(Xt+k | e1:t) for some k > 0.\nIn the umbrella example, this might mean computing the probability of rain three days\nfrom now, given all the observations to date. Prediction is useful for evaluating possible\ncourses of action based on their expected outcomes.\n2 The term “ﬁltering” refers to the roots of this problem in early work on signal processing, where the problem\nis to ﬁlter out the noise in a signal by estimating its underlying properties. Section 15.2.\nInference in Temporal Models\n571\n• Smoothing: This is the task of computing the posterior distribution over a past state,\nSMOOTHING\ngiven all evidence up to the present. That is, we wish to compute P(Xk | e1:t) for some k\nsuch that 0 ≤k < t. In the umbrella example, it might mean computing the probability\nthat it rained last Wednesday, given all the observations of the umbrella carrier made\nup to today. Smoothing provides a better estimate of the state than was available at the\ntime, because it incorporates more evidence.3\n• Most likely explanation: Given a sequence of observations, we might wish to ﬁnd the\nsequence of states that is most likely to have generated those observations. That is, we\nwish to compute argmaxx1:t P(x1:t | e1:t). For example, if the umbrella appears on each\nof the ﬁrst three days and is absent on the fourth, then the most likely explanation is that\nit rained on the ﬁrst three days and did not rain on the fourth. Algorithms for this task\nare useful in many applications, including speech recognition—where the aim is to ﬁnd\nthe most likely sequence of words, given a series of sounds—and the reconstruction of\nbit strings transmitted over a noisy channel.",
  "are useful in many applications, including speech recognition—where the aim is to ﬁnd\nthe most likely sequence of words, given a series of sounds—and the reconstruction of\nbit strings transmitted over a noisy channel.\nIn addition to these inference tasks, we also have\n• Learning: The transition and sensor models, if not yet known, can be learned from\nobservations. Just as with static Bayesian networks, dynamic Bayes net learning can be\ndone as a by-product of inference. Inference provides an estimate of what transitions\nactually occurred and of what states generated the sensor readings, and these estimates\ncan be used to update the models. The updated models provide new estimates, and the\nprocess iterates to convergence. The overall process is an instance of the expectation-\nmaximization or EM algorithm. (See Section 20.3.)\nNote that learning requires smoothing, rather than ﬁltering, because smoothing provides bet-\nter estimates of the states of the process. Learning with ﬁltering can fail to converge correctly;\nconsider, for example, the problem of learning to solve murders: unless you are an eyewit-\nness, smoothing is always required to infer what happened at the murder scene from the\nobservable variables.\nThe remainder of this section describes generic algorithms for the four inference tasks,\nindependent of the particular kind of model employed. Improvements speciﬁc to each model\nare described in subsequent sections.\n15.2.1\nFiltering and prediction\nAs we pointed out in Section 7.7.3, a useful ﬁltering algorithm needs to maintain a current\nstate estimate and update it, rather than going back over the entire history of percepts for each\nupdate. (Otherwise, the cost of each update increases as time goes by.) In other words, given\nthe result of ﬁltering up to time t, the agent needs to compute the result for t + 1 from the\nnew evidence et+1,\nP(Xt+1 | e1:t+1) = f(et+1, P(Xt | e1:t)) ,\nfor some function f. This process is called recursive estimation. We can view the calculation\nRECURSIVE\nESTIMATION\n3 In particular, when tracking a moving object with inaccurate position observations, smoothing gives a smoother\nestimated trajectory than ﬁltering—hence the name. 572\nChapter\n15.\nProbabilistic Reasoning over Time\nas being composed of two parts: ﬁrst, the current state distribution is projected forward from\nt to t+1; then it is updated using the new evidence et+1. This two-part process emerges quite\nsimply when the formula is rearranged:",
  "as being composed of two parts: ﬁrst, the current state distribution is projected forward from\nt to t+1; then it is updated using the new evidence et+1. This two-part process emerges quite\nsimply when the formula is rearranged:\nP(Xt+1 | e1:t+1) = P(Xt+1 | e1:t, et+1)\n(dividing up the evidence)\n= α P(et+1 | Xt+1, e1:t) P(Xt+1 | e1:t)\n(using Bayes’ rule)\n= α P(et+1 | Xt+1) P(Xt+1 | e1:t)\n(by the sensor Markov assumption).\n(15.4)\nHere and throughout this chapter, α is a normalizing constant used to make probabilities sum\nup to 1. The second term, P(Xt+1 | e1:t) represents a one-step prediction of the next state,\nand the ﬁrst term updates this with the new evidence; notice that P(et+1 | Xt+1) is obtainable\ndirectly from the sensor model. Now we obtain the one-step prediction for the next state by\nconditioning on the current state Xt:\nP(Xt+1 | e1:t+1) = α P(et+1 | Xt+1)\n\f\nxt\nP(Xt+1 | xt, e1:t)P(xt | e1:t)\n= α P(et+1 | Xt+1)\n\f\nxt\nP(Xt+1 | xt)P(xt | e1:t)\n(Markov assumption).\n(15.5)\nWithin the summation, the ﬁrst factor comes from the transition model and the second comes\nfrom the current state distribution. Hence, we have the desired recursive formulation. We can\nthink of the ﬁltered estimate P(Xt | e1:t) as a “message” f1:t that is propagated forward along\nthe sequence, modiﬁed by each transition and updated by each new observation. The process\nis given by\nf1:t+1 = α FORWARD(f1:t, et+1) ,\nwhere FORWARD implements the update described in Equation (15.5) and the process begins\nwith f1:0 = P(X0). When all the state variables are discrete, the time for each update is\nconstant (i.e., independent of t), and the space required is also constant. (The constants\ndepend, of course, on the size of the state space and the speciﬁc type of the temporal model\nin question.) The time and space requirements for updating must be constant if an agent with\nlimited memory is to keep track of the current state distribution over an unbounded sequence\nof observations.\nLet us illustrate the ﬁltering process for two steps in the basic umbrella example (Fig-\nure 15.2.) That is, we will compute P(R2 | u1:2) as follows:\n• On day 0, we have no observations, only the security guard’s prior beliefs; let’s assume\nthat consists of P(R0) = ⟨0.5, 0.5⟩.\n• On day 1, the umbrella appears, so U1 = true. The prediction from t = 0 to t = 1 is\nP(R1) =\n\f\nr0\nP(R1 | r0)P(r0)\n= ⟨0.7, 0.3⟩× 0.5 + ⟨0.3, 0.7⟩× 0.5 = ⟨0.5, 0.5⟩.",
  "that consists of P(R0) = ⟨0.5, 0.5⟩.\n• On day 1, the umbrella appears, so U1 = true. The prediction from t = 0 to t = 1 is\nP(R1) =\n\f\nr0\nP(R1 | r0)P(r0)\n= ⟨0.7, 0.3⟩× 0.5 + ⟨0.3, 0.7⟩× 0.5 = ⟨0.5, 0.5⟩.\nThen the update step simply multiplies by the probability of the evidence for t = 1 and\nnormalizes, as shown in Equation (15.4):\nP(R1 | u1) = α P(u1 | R1)P(R1) = α ⟨0.9, 0.2⟩⟨0.5, 0.5⟩\n= α ⟨0.45, 0.1⟩≈⟨0.818, 0.182⟩. Section 15.2.\nInference in Temporal Models\n573\n• On day 2, the umbrella appears, so U2 = true. The prediction from t = 1 to t = 2 is\nP(R2 | u1) =\n\f\nr1\nP(R2 | r1)P(r1 | u1)\n= ⟨0.7, 0.3⟩× 0.818 + ⟨0.3, 0.7⟩× 0.182 ≈⟨0.627, 0.373⟩,\nand updating it with the evidence for t = 2 gives\nP(R2 | u1, u2) = α P(u2 | R2)P(R2 | u1) = α ⟨0.9, 0.2⟩⟨0.627, 0.373⟩\n= α ⟨0.565, 0.075⟩≈⟨0.883, 0.117⟩.\nIntuitively, the probability of rain increases from day 1 to day 2 because rain persists. Exer-\ncise 15.2(a) asks you to investigate this tendency further.\nThe task of prediction can be seen simply as ﬁltering without the addition of new\nevidence. In fact, the ﬁltering process already incorporates a one-step prediction, and it is\neasy to derive the following recursive computation for predicting the state at t + k + 1 from\na prediction for t + k:\nP(Xt+k+1 | e1:t) =\n\f\nxt+k\nP(Xt+k+1 | xt+k)P(xt+k | e1:t) .\n(15.6)\nNaturally, this computation involves only the transition model and not the sensor model.\nIt is interesting to consider what happens as we try to predict further and further into\nthe future. As Exercise 15.2(b) shows, the predicted distribution for rain converges to a\nﬁxed point ⟨0.5, 0.5⟩, after which it remains constant for all time. This is the stationary\ndistribution of the Markov process deﬁned by the transition model. (See also page 537.) A\ngreat deal is known about the properties of such distributions and about the mixing time—\nMIXING TIME\nroughly, the time taken to reach the ﬁxed point. In practical terms, this dooms to failure any\nattempt to predict the actual state for a number of steps that is more than a small fraction of\nthe mixing time, unless the stationary distribution itself is strongly peaked in a small area of\nthe state space. The more uncertainty there is in the transition model, the shorter will be the\nmixing time and the more the future is obscured.\nIn addition to ﬁltering and prediction, we can use a forward recursion to compute the\nlikelihood of the evidence sequence, P(e1:t). This is a useful quantity if we want to compare",
  "mixing time and the more the future is obscured.\nIn addition to ﬁltering and prediction, we can use a forward recursion to compute the\nlikelihood of the evidence sequence, P(e1:t). This is a useful quantity if we want to compare\ndifferent temporal models that might have produced the same evidence sequence (e.g., two\ndifferent models for the persistence of rain). For this recursion, we use a likelihood message\nℓ1:t(Xt) = P(Xt, e1:t). It is a simple exercise to show that the message calculation is identical\nto that for ﬁltering:\nℓ1:t+1 = FORWARD(ℓ1:t, et+1) .\nHaving computed ℓ1:t, we obtain the actual likelihood by summing out Xt:\nL1:t = P(e1:t) =\n\f\nxt\nℓ1:t(xt) .\n(15.7)\nNotice that the likelihood message represents the probabilities of longer and longer evidence\nsequences as time goes by and so becomes numerically smaller and smaller, leading to under-\nﬂow problems with ﬂoating-point arithmetic. This is an important problem in practice, but\nwe shall not go into solutions here. 574\nChapter\n15.\nProbabilistic Reasoning over Time\nX1\nE1\nX0\nXk\nEk\nXt\nEt\nFigure 15.3\nSmoothing computes P(Xk | e1:t), the posterior distribution of the state at\nsome past time k given a complete sequence of observations from 1 to t.\n15.2.2\nSmoothing\nAs we said earlier, smoothing is the process of computing the distribution over past states\ngiven evidence up to the present; that is, P(Xk | e1:t) for 0 ≤k < t. (See Figure 15.3.)\nIn anticipation of another recursive message-passing approach, we can split the computation\ninto two parts—the evidence up to k and the evidence from k + 1 to t,\nP(Xk | e1:t) = P(Xk | e1:k, ek+1:t)\n= α P(Xk | e1:k)P(ek+1:t | Xk, e1:k)\n(using Bayes’ rule)\n= α P(Xk | e1:k)P(ek+1:t | Xk)\n(using conditional independence)\n= α f1:k × bk+1:t .\n(15.8)\nwhere “×” represents pointwise multiplication of vectors. Here we have deﬁned a “back-\nward” message bk+1:t = P(ek+1:t | Xk), analogous to the forward message f1:k. The forward\nmessage f1:k can be computed by ﬁltering forward from 1 to k, as given by Equation (15.5).\nIt turns out that the backward message bk+1:t can be computed by a recursive process that\nruns backward from t:\nP(ek+1:t | Xk) =\n\f\nxk+1\nP(ek+1:t | Xk, xk+1)P(xk+1 | Xk)\n(conditioning on Xk+1)\n=\n\f\nxk+1\nP(ek+1:t | xk+1)P(xk+1 | Xk)\n(by conditional independence)\n=\n\f\nxk+1\nP(ek+1, ek+2:t | xk+1)P(xk+1 | Xk)\n=\n\f\nxk+1\nP(ek+1 | xk+1)P(ek+2:t | xk+1)P(xk+1 | Xk) ,\n(15.9)\nwhere the last step follows by the conditional independence of ek+1 and ek+2:t, given Xk+1.",
  "(by conditional independence)\n=\n\f\nxk+1\nP(ek+1, ek+2:t | xk+1)P(xk+1 | Xk)\n=\n\f\nxk+1\nP(ek+1 | xk+1)P(ek+2:t | xk+1)P(xk+1 | Xk) ,\n(15.9)\nwhere the last step follows by the conditional independence of ek+1 and ek+2:t, given Xk+1.\nOf the three factors in this summation, the ﬁrst and third are obtained directly from the model,\nand the second is the “recursive call.” Using the message notation, we have\nbk+1:t = BACKWARD(bk+2:t, ek+1) ,\nwhere BACKWARD implements the update described in Equation (15.9). As with the forward\nrecursion, the time and space needed for each update are constant and thus independent of t.\nWe can now see that the two terms in Equation (15.8) can both be computed by recur-\nsions through time, one running forward from 1 to k and using the ﬁltering equation (15.5) Section 15.2.\nInference in Temporal Models\n575\nand the other running backward from t to k + 1 and using Equation (15.9). Note that the\nbackward phase is initialized with bt+1:t = P(et+1:t | Xt) = P( | Xt)1, where 1 is a vector of\n1s. (Because et+1:t is an empty sequence, the probability of observing it is 1.)\nLet us now apply this algorithm to the umbrella example, computing the smoothed\nestimate for the probability of rain at time k = 1, given the umbrella observations on days 1\nand 2. From Equation (15.8), this is given by\nP(R1 | u1, u2) = α P(R1 | u1) P(u2 | R1) .\n(15.10)\nThe ﬁrst term we already know to be ⟨.818, .182⟩, from the forward ﬁltering process de-\nscribed earlier. The second term can be computed by applying the backward recursion in\nEquation (15.9):\nP(u2 | R1) =\n\f\nr2\nP(u2 | r2)P( | r2)P(r2 | R1)\n= (0.9 × 1 × ⟨0.7, 0.3⟩) + (0.2 × 1 × ⟨0.3, 0.7⟩) = ⟨0.69, 0.41⟩.\nPlugging this into Equation (15.10), we ﬁnd that the smoothed estimate for rain on day 1 is\nP(R1 | u1, u2) = α ⟨0.818, 0.182⟩× ⟨0.69, 0.41⟩≈⟨0.883, 0.117⟩.\nThus, the smoothed estimate for rain on day 1 is higher than the ﬁltered estimate (0.818) in\nthis case. This is because the umbrella on day 2 makes it more likely to have rained on day\n2; in turn, because rain tends to persist, that makes it more likely to have rained on day 1.\nBoth the forward and backward recursions take a constant amount of time per step;\nhence, the time complexity of smoothing with respect to evidence e1:t is O(t). This is the\ncomplexity for smoothing at a particular time step k. If we want to smooth the whole se-\nquence, one obvious method is simply to run the whole smoothing process once for each",
  "complexity for smoothing at a particular time step k. If we want to smooth the whole se-\nquence, one obvious method is simply to run the whole smoothing process once for each\ntime step to be smoothed. This results in a time complexity of O(t2). A better approach\nuses a simple application of dynamic programming to reduce the complexity to O(t). A clue\nappears in the preceding analysis of the umbrella example, where we were able to reuse the\nresults of the forward-ﬁltering phase. The key to the linear-time algorithm is to record the\nresults of forward ﬁltering over the whole sequence. Then we run the backward recursion\nfrom t down to 1, computing the smoothed estimate at each step k from the computed back-\nward message bk+1:t and the stored forward message f1:k. The algorithm, aptly called the\nforward–backward algorithm, is shown in Figure 15.4.\nFORWARD–\nBACKWARD\nALGORITHM\nThe alert reader will have spotted that the Bayesian network structure shown in Fig-\nure 15.3 is a polytree as deﬁned on page 528. This means that a straightforward application\nof the clustering algorithm also yields a linear-time algorithm that computes smoothed es-\ntimates for the entire sequence. It is now understood that the forward–backward algorithm\nis in fact a special case of the polytree propagation algorithm used with clustering methods\n(although the two were developed independently).\nThe forward–backward algorithm forms the computational backbone for many applica-\ntions that deal with sequences of noisy observations. As described so far, it has two practical\ndrawbacks. The ﬁrst is that its space complexity can be too high when the state space is large\nand the sequences are long. It uses O(|f|t) space where |f| is the size of the representation of\nthe forward message. The space requirement can be reduced to O(|f| log t) with a concomi- 576\nChapter\n15.\nProbabilistic Reasoning over Time\ntant increase in the time complexity by a factor of log t, as shown in Exercise 15.3. In some\ncases (see Section 15.3), a constant-space algorithm can be used.\nThe second drawback of the basic algorithm is that it needs to be modiﬁed to work\nin an online setting where smoothed estimates must be computed for earlier time slices as\nnew observations are continuously added to the end of the sequence. The most common\nrequirement is for ﬁxed-lag smoothing, which requires computing the smoothed estimate\nFIXED-LAG\nSMOOTHING",
  "new observations are continuously added to the end of the sequence. The most common\nrequirement is for ﬁxed-lag smoothing, which requires computing the smoothed estimate\nFIXED-LAG\nSMOOTHING\nP(Xt−d | e1:t) for ﬁxed d. That is, smoothing is done for the time slice d steps behind the\ncurrent time t; as t increases, the smoothing has to keep up. Obviously, we can run the\nforward–backward algorithm over the d-step “window” as each new observation is added,\nbut this seems inefﬁcient. In Section 15.3, we will see that ﬁxed-lag smoothing can, in some\ncases, be done in constant time per update, independent of the lag d.\n15.2.3\nFinding the most likely sequence\nSuppose that [true, true, false, true, true] is the umbrella sequence for the security guard’s\nﬁrst ﬁve days on the job. What is the weather sequence most likely to explain this? Does\nthe absence of the umbrella on day 3 mean that it wasn’t raining, or did the director forget\nto bring it? If it didn’t rain on day 3, perhaps (because weather tends to persist) it didn’t\nrain on day 4 either, but the director brought the umbrella just in case. In all, there are 25\npossible weather sequences we could pick. Is there a way to ﬁnd the most likely one, short of\nenumerating all of them?\nWe could try this linear-time procedure: use smoothing to ﬁnd the posterior distribution\nfor the weather at each time step; then construct the sequence, using at each step the weather\nthat is most likely according to the posterior. Such an approach should set off alarm bells\nin the reader’s head, because the posterior distributions computed by smoothing are distri-\nfunction FORWARD-BACKWARD(ev,prior) returns a vector of probability distributions\ninputs: ev, a vector of evidence values for steps 1, . . . , t\nprior, the prior distribution on the initial state, P(X0)\nlocal variables: fv, a vector of forward messages for steps 0, . . . , t\nb, a representation of the backward message, initially all 1s\nsv, a vector of smoothed estimates for steps 1, . . . , t\nfv[0] ←prior\nfor i = 1 to t do\nfv[i] ←FORWARD(fv[i −1], ev[i])\nfor i = t downto 1 do\nsv[i] ←NORMALIZE(fv[i] × b)\nb ←BACKWARD(b, ev[i])\nreturn sv\nFigure 15.4\nThe forward–backward algorithm for smoothing: computing posterior prob-\nabilities of a sequence of states given a sequence of observations.\nThe FORWARD and\nBACKWARD operators are deﬁned by Equations (15.5) and (15.9), respectively. Section 15.2.\nInference in Temporal Models\n577\nRain1\nm1:1\ntrue\nRain5\nm1:5\ntrue\nRain4\nm1:4\ntrue\nRain3",
  "The FORWARD and\nBACKWARD operators are deﬁned by Equations (15.5) and (15.9), respectively. Section 15.2.\nInference in Temporal Models\n577\nRain1\nm1:1\ntrue\nRain5\nm1:5\ntrue\nRain4\nm1:4\ntrue\nRain3\nm1:3\nfalse\nRain2\nm1:2\ntrue\nUmbrellat\n(a)\n(b)\n.8182\n.1818\n.0210\n.0024\n.0334\n.0173\n.0361\n.1237\n.5155\n.0491\ntrue\nfalse\ntrue\nfalse\ntrue\nfalse\ntrue\nfalse\ntrue\nfalse\nFigure 15.5\n(a) Possible state sequences for Raint can be viewed as paths through a graph\nof the possible states at each time step. (States are shown as rectangles to avoid confusion\nwith nodes in a Bayes net.) (b) Operation of the Viterbi algorithm for the umbrella obser-\nvation sequence [true, true, false, true, true]. For each t, we have shown the values of the\nmessage m1:t, which gives the probability of the best sequence reaching each state at time t.\nAlso, for each state, the bold arrow leading into it indicates its best predecessor as measured\nby the product of the preceding sequence probability and the transition probability. Following\nthe bold arrows back from the most likely state in m1:5 gives the most likely sequence.\nbutions over single time steps, whereas to ﬁnd the most likely sequence we must consider\njoint probabilities over all the time steps. The results can in fact be quite different. (See\nExercise 15.4.)\nThere is a linear-time algorithm for ﬁnding the most likely sequence, but it requires a\nlittle more thought. It relies on the same Markov property that yielded efﬁcient algorithms for\nﬁltering and smoothing. The easiest way to think about the problem is to view each sequence\nas a path through a graph whose nodes are the possible states at each time step. Such a\ngraph is shown for the umbrella world in Figure 15.5(a). Now consider the task of ﬁnding\nthe most likely path through this graph, where the likelihood of any path is the product of\nthe transition probabilities along the path and the probabilities of the given observations at\neach state. Let’s focus in particular on paths that reach the state Rain5 = true. Because of\nthe Markov property, it follows that the most likely path to the state Rain5 = true consists of\nthe most likely path to some state at time 4 followed by a transition to Rain5 = true; and the\nstate at time 4 that will become part of the path to Rain5 = true is whichever maximizes the\nlikelihood of that path. In other words, there is a recursive relationship between most likely\npaths to each state xt+1 and most likely paths to each state xt. We can write this relationship",
  "likelihood of that path. In other words, there is a recursive relationship between most likely\npaths to each state xt+1 and most likely paths to each state xt. We can write this relationship\nas an equation connecting the probabilities of the paths:\nmax\nx1...xt P(x1, . . . , xt, Xt+1 | e1:t+1)\n= α P(et+1 | Xt+1) max\nxt\n\r\nP(Xt+1 | xt) max\nx1...xt−1 P(x1, . . . , xt−1, xt | e1:t)\n\u000e\n.\n(15.11)\nEquation (15.11) is identical to the ﬁltering equation (15.5) except that 578\nChapter\n15.\nProbabilistic Reasoning over Time\n1. The forward message f1:t = P(Xt | e1:t) is replaced by the message\nm1:t =\nmax\nx1...xt−1 P(x1, . . . , xt−1, Xt | e1:t) ,\nthat is, the probabilities of the most likely path to each state xt; and\n2. the summation over xt in Equation (15.5) is replaced by the maximization over xt in\nEquation (15.11).\nThus, the algorithm for computing the most likely sequence is similar to ﬁltering: it runs for-\nward along the sequence, computing the m message at each time step, using Equation (15.11).\nThe progress of this computation is shown in Figure 15.5(b). At the end, it will have the\nprobability for the most likely sequence reaching each of the ﬁnal states. One can thus easily\nselect the most likely sequence overall (the states outlined in bold). In order to identify the\nactual sequence, as opposed to just computing its probability, the algorithm will also need to\nrecord, for each state, the best state that leads to it; these are indicated by the bold arrows in\nFigure 15.5(b). The optimal sequence is identiﬁed by following these bold arrows backwards\nfrom the best ﬁnal state.\nThe algorithm we have just described is called the Viterbi algorithm, after its inventor.\nVITERBI ALGORITHM\nLike the ﬁltering algorithm, its time complexity is linear in t, the length of the sequence.\nUnlike ﬁltering, which uses constant space, its space requirement is also linear in t. This\nis because the Viterbi algorithm needs to keep the pointers that identify the best sequence\nleading to each state.\n15.3\nHIDDEN MARKOV MODELS\nThe preceding section developed algorithms for temporal probabilistic reasoning using a gen-\neral framework that was independent of the speciﬁc form of the transition and sensor models.\nIn this and the next two sections, we discuss more concrete models and applications that\nillustrate the power of the basic algorithms and in some cases allow further improvements.\nWe begin with the hidden Markov model, or HMM. An HMM is a temporal proba-\nHIDDEN MARKOV\nMODEL",
  "illustrate the power of the basic algorithms and in some cases allow further improvements.\nWe begin with the hidden Markov model, or HMM. An HMM is a temporal proba-\nHIDDEN MARKOV\nMODEL\nbilistic model in which the state of the process is described by a single discrete random vari-\nable. The possible values of the variable are the possible states of the world. The umbrella\nexample described in the preceding section is therefore an HMM, since it has just one state\nvariable: Raint. What happens if you have a model with two or more state variables? You can\nstill ﬁt it into the HMM framework by combining the variables into a single “megavariable”\nwhose values are all possible tuples of values of the individual state variables. We will see\nthat the restricted structure of HMMs allows for a simple and elegant matrix implementation\nof all the basic algorithms.4\n4 The reader unfamiliar with basic operations on vectors and matrices might wish to consult Appendix A before\nproceeding with this section. Section 15.3.\nHidden Markov Models\n579\n15.3.1\nSimpliﬁed matrix algorithms\nWith a single, discrete state variable Xt, we can give concrete form to the representations\nof the transition model, the sensor model, and the forward and backward messages. Let the\nstate variable Xt have values denoted by integers 1, . . . , S, where S is the number of possible\nstates. The transition model P(Xt | Xt−1) becomes an S × S matrix T, where\nTij = P(Xt = j | Xt−1 = i) .\nThat is, Tij is the probability of a transition from state i to state j. For example, the transition\nmatrix for the umbrella world is\nT = P(Xt | Xt−1) =\n\r 0.7 0.3\n0.3 0.7\n\u000e\n.\nWe also put the sensor model in matrix form. In this case, because the value of the evidence\nvariable Et is known at time t (call it et), we need only specify, for each state, how likely it\nis that the state causes et to appear: we need P(et | Xt = i) for each state i. For mathematical\nconvenience we place these values into an S × S diagonal matrix, Ot whose ith diagonal\nentry is P(et | Xt = i) and whose other entries are 0. For example, on day 1 in the umbrella\nworld of Figure 15.5, U1 = true, and on day 3, U3 = false, so, from Figure 15.2, we have\nO1 =\n\r 0.9\n0\n0\n0.2\n\u000e\n;\nO3 =\n\r 0.1\n0\n0\n0.8\n\u000e\n.\nNow, if we use column vectors to represent the forward and backward messages, all the com-\nputations become simple matrix–vector operations. The forward equation (15.5) becomes\nf1:t+1 = α Ot+1T⊤f1:t\n(15.12)\nand the backward equation (15.9) becomes",
  ".\nNow, if we use column vectors to represent the forward and backward messages, all the com-\nputations become simple matrix–vector operations. The forward equation (15.5) becomes\nf1:t+1 = α Ot+1T⊤f1:t\n(15.12)\nand the backward equation (15.9) becomes\nbk+1:t = TOk+1bk+2:t .\n(15.13)\nFrom these equations, we can see that the time complexity of the forward–backward algo-\nrithm (Figure 15.4) applied to a sequence of length t is O(S2t), because each step requires\nmultiplying an S-element vector by an S × S matrix. The space requirement is O(St), be-\ncause the forward pass stores t vectors of size S.\nBesides providing an elegant description of the ﬁltering and smoothing algorithms for\nHMMs, the matrix formulation reveals opportunities for improved algorithms. The ﬁrst is\na simple variation on the forward–backward algorithm that allows smoothing to be carried\nout in constant space, independently of the length of the sequence. The idea is that smooth-\ning for any particular time slice k requires the simultaneous presence of both the forward and\nbackward messages, f1:k and bk+1:t, according to Equation (15.8). The forward–backward al-\ngorithm achieves this by storing the fs computed on the forward pass so that they are available\nduring the backward pass. Another way to achieve this is with a single pass that propagates\nboth f and b in the same direction. For example, the “forward” message f can be propagated\nbackward if we manipulate Equation (15.12) to work in the other direction:\nf1:t = α′(T⊤)−1O−1\nt+1f1:t+1 .\nThe modiﬁed smoothing algorithm works by ﬁrst running the standard forward pass to com-\npute ft:t (forgetting all the intermediate results) and then running the backward pass for both 580\nChapter\n15.\nProbabilistic Reasoning over Time\nfunction FIXED-LAG-SMOOTHING(et,hmm,d) returns a distribution over Xt−d\ninputs: et, the current evidence for time step t\nhmm, a hidden Markov model with S × S transition matrix T\nd, the length of the lag for smoothing\npersistent: t, the current time, initially 1\nf, the forward message P(Xt|e1:t), initially hmm.PRIOR\nB, the d-step backward transformation matrix, initially the identity matrix\net−d:t, double-ended list of evidence from t −d to t, initially empty\nlocal variables: Ot−d, Ot, diagonal matrices containing the sensor model information\nadd et to the end of et−d:t\nOt ←diagonal matrix containing P(et|Xt)\nif t > d then\nf ←FORWARD(f, et)\nremove et−d−1 from the beginning of et−d:t\nOt−d ←diagonal matrix containing P(et−d|Xt−d)",
  "add et to the end of et−d:t\nOt ←diagonal matrix containing P(et|Xt)\nif t > d then\nf ←FORWARD(f, et)\nremove et−d−1 from the beginning of et−d:t\nOt−d ←diagonal matrix containing P(et−d|Xt−d)\nB ←O−1\nt−dT−1BTOt\nelse B ←BTOt\nt ←t + 1\nif t > d then return NORMALIZE(f × B1) else return null\nFigure 15.6\nAn algorithm for smoothing with a ﬁxed time lag of d steps, implemented\nas an online algorithm that outputs the new smoothed estimate given the observation for a\nnew time step. Notice that the ﬁnal output NORMALIZE(f × B1) is just α f × b, by Equa-\ntion (15.14).\nb and f together, using them to compute the smoothed estimate at each step. Since only one\ncopy of each message is needed, the storage requirements are constant (i.e., independent of\nt, the length of the sequence). There are two signiﬁcant restrictions on this algorithm: it re-\nquires that the transition matrix be invertible and that the sensor model have no zeroes—that\nis, that every observation be possible in every state.\nA second area in which the matrix formulation reveals an improvement is in online\nsmoothing with a ﬁxed lag. The fact that smoothing can be done in constant space suggests\nthat there should exist an efﬁcient recursive algorithm for online smoothing—that is, an al-\ngorithm whose time complexity is independent of the length of the lag. Let us suppose that\nthe lag is d; that is, we are smoothing at time slice t −d, where the current time is t. By\nEquation (15.8), we need to compute\nα f1:t−d × bt−d+1:t\nfor slice t −d. Then, when a new observation arrives, we need to compute\nα f1:t−d+1 × bt−d+2:t+1\nfor slice t −d + 1. How can this be done incrementally? First, we can compute f1:t−d+1 from\nf1:t−d, using the standard ﬁltering process, Equation (15.5). Section 15.3.\nHidden Markov Models\n581\nComputing the backward message incrementally is trickier, because there is no simple\nrelationship between the old backward message bt−d+1:t and the new backward message\nbt−d+2:t+1. Instead, we will examine the relationship between the old backward message\nbt−d+1:t and the backward message at the front of the sequence, bt+1:t. To do this, we apply\nEquation (15.13) d times to get\nbt−d+1:t =\n\u001f\nt\u0019\ni = t−d+1\nTOi\n \nbt+1:t = Bt−d+1:t1 ,\n(15.14)\nwhere the matrix Bt−d+1:t is the product of the sequence of T and O matrices. B can be\nthought of as a “transformation operator” that transforms a later backward message into an\nearlier one. A similar equation holds for the new backward messages after the next observa-",
  "thought of as a “transformation operator” that transforms a later backward message into an\nearlier one. A similar equation holds for the new backward messages after the next observa-\ntion arrives:\nbt−d+2:t+1 =\n\u001f\nt+1\n\u0019\ni = t−d+2\nTOi\n \nbt+2:t+1 = Bt−d+2:t+11 .\n(15.15)\nExamining the product expressions in Equations (15.14) and (15.15), we see that they have a\nsimple relationship: to get the second product, “divide” the ﬁrst product by the ﬁrst element\nTOt−d+1, and multiply by the new last element TOt+1. In matrix language, then, there is a\nsimple relationship between the old and new B matrices:\nBt−d+2:t+1 = O−1\nt−d+1T−1Bt−d+1:tTOt+1 .\n(15.16)\nThis equation provides an incremental update for the B matrix, which in turn (through Equa-\ntion (15.15)) allows us to compute the new backward message bt−d+2:t+1. The complete\nalgorithm, which requires storing and updating f and B, is shown in Figure 15.6.\n15.3.2\nHidden Markov model example: Localization\nOn page 145, we introduced a simple form of the localization problem for the vacuum world.\nIn that version, the robot had a single nondeterministic Move action and its sensors reported\nperfectly whether or not obstacles lay immediately to the north, south, east, and west; the\nrobot’s belief state was the set of possible locations it could be in.\nHere we make the problem slightly more realistic by including a simple probability\nmodel for the robot’s motion and by allowing for noise in the sensors. The state variable Xt\nrepresents the location of the robot on the discrete grid; the domain of this variable is the\nset of empty squares {s1, . . . , sn}. Let NEIGHBORS(s) be the set of empty squares that are\nadjacent to s and let N(s) be the size of that set. Then the transition model for Move action\nsays that the robot is equally likely to end up at any neighboring square:\nP(Xt+1 = j | Xt = i) = Tij = (1/N(i) if j ∈NEIGHBORS(i) else 0) .\nWe don’t know where the robot starts, so we will assume a uniform distribution over all the\nsquares; that is, P(X0 = i) = 1/n. For the particular environment we consider (Figure 15.7),\nn = 42 and the transition matrix T has 42 × 42 = 1764 entries.\nThe sensor variable Et has 16 possible values, each a four-bit sequence giving the pres-\nence or absence of an obstacle in a particular compass direction. We will use the notation 582\nChapter\n15.\nProbabilistic Reasoning over Time\n(a) Posterior distribution over robot location after E1 = NSW",
  "ence or absence of an obstacle in a particular compass direction. We will use the notation 582\nChapter\n15.\nProbabilistic Reasoning over Time\n(a) Posterior distribution over robot location after E1 = NSW\n(b) Posterior distribution over robot location after E1 = NSW, E2 = NS\nFigure 15.7\nPosterior distribution over robot location: (a) one observation E1 = NSW;\n(b) after a second observation E2 = NS. The size of each disk corresponds to the probability\nthat the robot is at that location. The sensor error rate is ϵ = 0.2.\nNS, for example, to mean that the north and south sensors report an obstacle and the east and\nwest do not. Suppose that each sensor’s error rate is ϵ and that errors occur independently for\nthe four sensor directions. In that case, the probability of getting all four bits right is (1 −ϵ)4\nand the probability of getting them all wrong is ϵ4. Furthermore, if dit is the discrepancy—the\nnumber of bits that are different—between the true values for square i and the actual reading\net, then the probability that a robot in square i would receive a sensor reading et is\nP(Et = et | Xt = i) = Otii = (1 −ϵ)4−ditϵdit .\nFor example, the probability that a square with obstacles to the north and south would produce\na sensor reading NSE is (1 −ϵ)3ϵ1.\nGiven the matrices T and Ot, the robot can use Equation (15.12) to compute the pos-\nterior distribution over locations—that is, to work out where it is. Figure 15.7 shows the\ndistributions P(X1 | E1 = NSW) and P(X2 | E1 = NSW, E2 = NS). This is the same maze\nwe saw before in Figure 4.18 (page 146), but there we used logical ﬁltering to ﬁnd the loca-\ntions that were possible, assuming perfect sensing. Those same locations are still the most\nlikely with noisy sensing, but now every location has some nonzero probability.\nIn addition to ﬁltering to estimate its current location, the robot can use smoothing\n(Equation (15.13)) to work out where it was at any given past time—for example, where it\nbegan at time 0—and it can use the Viterbi algorithm to work out the most likely path it has Section 15.3.\nHidden Markov Models\n583\n 0.5\n 1\n 1.5\n 2\n 2.5\n 3\n 3.5\n 4\n 4.5\n 5\n 5.5\n 6\n 0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\nLocalization error\nNumber of observations\nε = 0.20\nε = 0.10\nε = 0.05\nε = 0.02\nε = 0.00\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\nPath accuracy\nNumber of observations\nε = 0.00\nε = 0.02\nε = 0.05\nε = 0.10\nε = 0.20\n(a)\n(b)\nFigure 15.8",
  "ε = 0.20\nε = 0.10\nε = 0.05\nε = 0.02\nε = 0.00\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\nPath accuracy\nNumber of observations\nε = 0.00\nε = 0.02\nε = 0.05\nε = 0.10\nε = 0.20\n(a)\n(b)\nFigure 15.8\nPerformance of HMM localization as a function of the length of the observa-\ntion sequence for various different values of the sensor error probability ϵ; data averaged over\n400 runs. (a) The localization error, deﬁned as the Manhattan distance from the true location.\n(b) The Viterbi path accuracy, deﬁned as the fraction of correct states on the Viterbi path.\ntaken to get where it is now. Figure 15.8 shows the localization error and Viterbi path accuracy\nfor various values of the per-bit sensor error rate ϵ. Even when ϵ is 20%—which means that\nthe overall sensor reading is wrong 59% of the time—the robot is usually able to work out its\nlocation within two squares after 25 observations. This is because of the algorithm’s ability\nto integrate evidence over time and to take into account the probabilistic constraints imposed\non the location sequence by the transition model. When ϵ is 10%, the performance after\na half-dozen observations is hard to distinguish from the performance with perfect sensing.\nExercise 15.7 asks you to explore how robust the HMM localization algorithm is to errors in\nthe prior distribution P(X0) and in the transition model itself. Broadly speaking, high levels\nof localization and path accuracy are maintained even in the face of substantial errors in the\nmodels used.\nThe state variable for the example we have considered in this section is a physical\nlocation in the world. Other problems can, of course, include other aspects of the world.\nExercise 15.8 asks you to consider a version of the vacuum robot that has the policy of going\nstraight for as long as it can; only when it encounters an obstacle does it change to a new\n(randomly selected) heading. To model this robot, each state in the model consists of a\n(location, heading) pair. For the environment in Figure 15.7, which has 42 empty squares,\nthis leads to 168 states and a transition matrix with 1682 = 28, 224 entries—still a manageable\nnumber. If we add the possibility of dirt in the squares, the number of states is multiplied by\n242 and the transition matrix ends up with more than 1029 entries—no longer a manageable\nnumber; Section 15.5 shows how to use dynamic Bayesian networks to model domains with",
  "242 and the transition matrix ends up with more than 1029 entries—no longer a manageable\nnumber; Section 15.5 shows how to use dynamic Bayesian networks to model domains with\nmany state variables. If we allow the robot to move continuously rather than in a discrete\ngrid, the number of states becomes inﬁnite; the next section shows how to handle this case. 584\nChapter\n15.\nProbabilistic Reasoning over Time\n15.4\nKALMAN FILTERS\nImagine watching a small bird ﬂying through dense jungle foliage at dusk: you glimpse\nbrief, intermittent ﬂashes of motion; you try hard to guess where the bird is and where it will\nappear next so that you don’t lose it. Or imagine that you are a World War II radar operator\npeering at a faint, wandering blip that appears once every 10 seconds on the screen. Or, going\nback further still, imagine you are Kepler trying to reconstruct the motions of the planets\nfrom a collection of highly inaccurate angular observations taken at irregular and imprecisely\nmeasured intervals. In all these cases, you are doing ﬁltering: estimating state variables (here,\nposition and velocity) from noisy observations over time. If the variables were discrete, we\ncould model the system with a hidden Markov model. This section examines methods for\nhandling continuous variables, using an algorithm called Kalman ﬁltering, after one of its\nKALMAN FILTERING\ninventors, Rudolf E. Kalman.\nThe bird’s ﬂight might be speciﬁed by six continuous variables at each time point; three\nfor position (Xt, Yt, Zt) and three for velocity ( ˙Xt, ˙Yt, ˙Zt). We will need suitable conditional\ndensities to represent the transition and sensor models; as in Chapter 14, we will use linear\nGaussian distributions. This means that the next state Xt+1 must be a linear function of the\ncurrent state Xt, plus some Gaussian noise, a condition that turns out to be quite reasonable in\npractice. Consider, for example, the X-coordinate of the bird, ignoring the other coordinates\nfor now. Let the time interval between observations be Δ, and assume constant velocity\nduring the interval; then the position update is given by Xt+Δ = Xt+ ˙X Δ. Adding Gaussian\nnoise (to account for wind variation, etc.), we obtain a linear Gaussian transition model:\nP(Xt+Δ = xt+Δ | Xt = xt, ˙Xt = ˙xt) = N(xt + ˙xt Δ, σ2)(xt+Δ) .\nThe Bayesian network structure for a system with position vector Xt and velocity ˙Xt is shown\nin Figure 15.9. Note that this is a very speciﬁc form of linear Gaussian model; the general",
  "P(Xt+Δ = xt+Δ | Xt = xt, ˙Xt = ˙xt) = N(xt + ˙xt Δ, σ2)(xt+Δ) .\nThe Bayesian network structure for a system with position vector Xt and velocity ˙Xt is shown\nin Figure 15.9. Note that this is a very speciﬁc form of linear Gaussian model; the general\nform will be described later in this section and covers a vast array of applications beyond the\nsimple motion examples of the ﬁrst paragraph. The reader might wish to consult Appendix A\nfor some of the mathematical properties of Gaussian distributions; for our immediate pur-\nposes, the most important is that a multivariate Gaussian distribution for d variables is\nMULTIVARIATE\nGAUSSIAN\nspeciﬁed by a d-element mean μ and a d × d covariance matrix Σ.\n15.4.1\nUpdating Gaussian distributions\nIn Chapter 14 on page 521, we alluded to a key property of the linear Gaussian family of dis-\ntributions: it remains closed under the standard Bayesian network operations. Here, we make\nthis claim precise in the context of ﬁltering in a temporal probability model. The required\nproperties correspond to the two-step ﬁltering calculation in Equation (15.5):\n1. If the current distribution P(Xt | e1:t) is Gaussian and the transition model P(Xt+1 | xt)\nis linear Gaussian, then the one-step predicted distribution given by\nP(Xt+1 | e1:t) =\n\u001a\nxt\nP(Xt+1 | xt)P(xt | e1:t) dxt\n(15.17)\nis also a Gaussian distribution. Section 15.4.\nKalman Filters\n585\nt\nZ\nt+1\nZ\nt\nX\nt+1\nX\nt\nX\nt+1\nX\nFigure 15.9\nBayesian network structure for a linear dynamical system with position Xt,\nvelocity ˙Xt, and position measurement Zt.\n2. If the prediction P(Xt+1 | e1:t) is Gaussian and the sensor model P(et+1 | Xt+1) is linear\nGaussian, then, after conditioning on the new evidence, the updated distribution\nP(Xt+1 | e1:t+1) = α P(et+1 | Xt+1)P(Xt+1 | e1:t)\n(15.18)\nis also a Gaussian distribution.\nThus, the FORWARD operator for Kalman ﬁltering takes a Gaussian forward message f1:t,\nspeciﬁed by a mean μt and covariance matrix Σt, and produces a new multivariate Gaussian\nforward message f1:t+1, speciﬁed by a mean μt+1 and covariance matrix Σt+1. So, if we\nstart with a Gaussian prior f1:0 = P(X0) = N(μ0, Σ0), ﬁltering with a linear Gaussian model\nproduces a Gaussian state distribution for all time.\nThis seems to be a nice, elegant result, but why is it so important? The reason is that,\nexcept for a few special cases such as this, ﬁltering with continuous or hybrid (discrete and\ncontinuous) networks generates state distributions whose representation grows without bound",
  "except for a few special cases such as this, ﬁltering with continuous or hybrid (discrete and\ncontinuous) networks generates state distributions whose representation grows without bound\nover time. This statement is not easy to prove in general, but Exercise 15.10 shows what\nhappens for a simple example.\n15.4.2\nA simple one-dimensional example\nWe have said that the FORWARD operator for the Kalman ﬁlter maps a Gaussian into a new\nGaussian. This translates into computing a new mean and covariance matrix from the previ-\nous mean and covariance matrix. Deriving the update rule in the general (multivariate) case\nrequires rather a lot of linear algebra, so we will stick to a very simple univariate case for now;\nand later give the results for the general case. Even for the univariate case, the calculations\nare somewhat tedious, but we feel that they are worth seeing because the usefulness of the\nKalman ﬁlter is tied so intimately to the mathematical properties of Gaussian distributions.\nThe temporal model we consider describes a random walk of a single continuous state\nvariable Xt with a noisy observation Zt. An example might be the “consumer conﬁdence” in-\ndex, which can be modeled as undergoing a random Gaussian-distributed change each month\nand is measured by a random consumer survey that also introduces Gaussian sampling noise. 586\nChapter\n15.\nProbabilistic Reasoning over Time\nThe prior distribution is assumed to be Gaussian with variance σ2\n0:\nP(x0) = α e\n−1\n2\n„\n(x0−μ0)2\nσ2\n0\n«\n.\n(For simplicity, we use the same symbol α for all normalizing constants in this section.) The\ntransition model adds a Gaussian perturbation of constant variance σ2\nx to the current state:\nP(xt+1 | xt) = α e\n−1\n2\n„\n(xt+1−xt)2\nσ2x\n«\n.\nThe sensor model assumes Gaussian noise with variance σ2\nz:\nP(zt | xt) = α e\n−1\n2\n„\n(zt−xt)2\nσ2z\n«\n.\nNow, given the prior P(X0), the one-step predicted distribution comes from Equation (15.17):\nP(x1) =\n\u001a ∞\n−∞\nP(x1 | x0)P(x0) dx0 = α\n\u001a ∞\n−∞\ne\n−1\n2\n„\n(x1−x0)2\nσ2x\n«\ne\n−1\n2\n„\n(x0−μ0)2\nσ2\n0\n«\ndx0\n= α\n\u001a ∞\n−∞\ne\n−1\n2\n„\nσ2\n0(x1−x0)2+σ2x(x0−μ0)2\nσ2\n0σ2x\n«\ndx0 .\nThis integral looks rather complicated. The key to progress is to notice that the exponent is the\nsum of two expressions that are quadratic in x0 and hence is itself a quadratic in x0. A simple\ntrick known as completing the square allows the rewriting of any quadratic ax2\n0 + bx0 + c\nCOMPLETING THE\nSQUARE\nas the sum of a squared term a(x0 −−b\n2a )2 and a residual term c −b2\n4a that is independent of",
  "trick known as completing the square allows the rewriting of any quadratic ax2\n0 + bx0 + c\nCOMPLETING THE\nSQUARE\nas the sum of a squared term a(x0 −−b\n2a )2 and a residual term c −b2\n4a that is independent of\nx0. The residual term can be taken outside the integral, giving us\nP(x1) = α e−1\n2\n“\nc−b2\n4a\n” \u001a ∞\n−∞\ne−1\n2(a(x0−−b\n2a )2) dx0 .\nNow the integral is just the integral of a Gaussian over its full range, which is simply 1. Thus,\nwe are left with only the residual term from the quadratic. Then, we notice that the residual\nterm is a quadratic in x1; in fact, after simpliﬁcation, we obtain\nP(x1) = α e\n−1\n2\n„\n(x1−μ0)2\nσ2\n0+σ2x\n«\n.\nThat is, the one-step predicted distribution is a Gaussian with the same mean μ0 and a variance\nequal to the sum of the original variance σ2\n0 and the transition variance σ2\nx.\nTo complete the update step, we need to condition on the observation at the ﬁrst time\nstep, namely, z1. From Equation (15.18), this is given by\nP(x1 | z1) = α P(z1 | x1)P(x1)\n= α e\n−1\n2\n„\n(z1−x1)2\nσ2z\n«\ne\n−1\n2\n„\n(x1−μ0)2\nσ2\n0+σ2x\n«\n.\nOnce again, we combine the exponents and complete the square (Exercise 15.11), obtaining\nP(x1 | z1) = α e\n−1\n2\n0\nB\nB\n@\n(x1−(σ2\n0+σ2x)z1+σ2zμ0\nσ2\n0+σ2x+σ2z\n)2\n(σ2\n0+σ2x)σ2z/(σ2\n0+σ2x+σ2z)\n1\nC\nC\nA\n.\n(15.19) Section 15.4.\nKalman Filters\n587\n 0\n 0.05\n 0.1\n 0.15\n 0.2\n 0.25\n 0.3\n 0.35\n 0.4\n 0.45\n-10\n-5\n 0\n 5\n 10\nP(x)\nx position\nP(x0)\nP(x1)\nP(x1 | z1 = 2.5)\n*z1\nFigure 15.10\nStages in the Kalman ﬁlter update cycle for a random walk with a prior\ngiven by μ0 = 0.0 and σ0 = 1.0, transition noise given by σx = 2.0, sensor noise given by\nσz = 1.0, and a ﬁrst observation z1 = 2.5 (marked on the x-axis). Notice how the prediction\nP(x1) is ﬂattened out, relative to P(x0), by the transition noise. Notice also that the mean\nof the posterior distribution P(x1 | z1) is slightly to the left of the observation z1 because the\nmean is a weighted average of the prediction and the observation.\nThus, after one update cycle, we have a new Gaussian distribution for the state variable.\nFrom the Gaussian formula in Equation (15.19), we see that the new mean and standard\ndeviation can be calculated from the old mean and standard deviation as follows:\nμt+1 = (σ2\nt + σ2\nx)zt+1 + σ2\nzμt\nσ2\nt + σ2x + σ2z\nand\nσ2\nt+1 = (σ2\nt + σ2\nx)σ2\nz\nσ2\nt + σ2x + σ2z\n.\n(15.20)\nFigure 15.10 shows one update cycle for particular values of the transition and sensor models.\nEquation (15.20) plays exactly the same role as the general ﬁltering equation (15.5) or",
  "and\nσ2\nt+1 = (σ2\nt + σ2\nx)σ2\nz\nσ2\nt + σ2x + σ2z\n.\n(15.20)\nFigure 15.10 shows one update cycle for particular values of the transition and sensor models.\nEquation (15.20) plays exactly the same role as the general ﬁltering equation (15.5) or\nthe HMM ﬁltering equation (15.12). Because of the special nature of Gaussian distributions,\nhowever, the equations have some interesting additional properties. First, we can interpret\nthe calculation for the new mean μt+1 as simply a weighted mean of the new observation\nzt+1 and the old mean μt. If the observation is unreliable, then σ2\nz is large and we pay more\nattention to the old mean; if the old mean is unreliable (σ2\nt is large) or the process is highly\nunpredictable (σ2\nx is large), then we pay more attention to the observation. Second, notice\nthat the update for the variance σ2\nt+1 is independent of the observation. We can therefore\ncompute in advance what the sequence of variance values will be. Third, the sequence of\nvariance values converges quickly to a ﬁxed value that depends only on σ2\nx and σ2\nz, thereby\nsubstantially simplifying the subsequent calculations. (See Exercise 15.12.)\n15.4.3\nThe general case\nThe preceding derivation illustrates the key property of Gaussian distributions that allows\nKalman ﬁltering to work: the fact that the exponent is a quadratic form. This is true not just\nfor the univariate case; the full multivariate Gaussian distribution has the form\nN(μ, Σ)(x) = α e−1\n2\n“\n(x−μ)⊤Σ\n−1(x−μ)\n”\n. 588\nChapter\n15.\nProbabilistic Reasoning over Time\nMultiplying out the terms in the exponent makes it clear that the exponent is also a quadratic\nfunction of the values xi in x. As in the univariate case, the ﬁltering update preserves the\nGaussian nature of the state distribution.\nLet us ﬁrst deﬁne the general temporal model used with Kalman ﬁltering. Both the tran-\nsition model and the sensor model allow for a linear transformation with additive Gaussian\nnoise. Thus, we have\nP(xt+1 | xt) = N(Fxt, Σx)(xt+1)\nP(zt | xt) = N(Hxt, Σz)(zt) ,\n(15.21)\nwhere F and Σx are matrices describing the linear transition model and transition noise co-\nvariance, and H and Σz are the corresponding matrices for the sensor model. Now the update\nequations for the mean and covariance, in their full, hairy horribleness, are\nμt+1 = Fμt + Kt+1(zt+1 −HFμt)\nΣt+1 = (I −Kt+1H)(FΣtF⊤+ Σx) ,\n(15.22)\nwhere Kt+1 = (FΣtF⊤+ Σx)H⊤(H(FΣtF⊤+ Σx)H⊤+ Σz)−1 is called the Kalman gain",
  "equations for the mean and covariance, in their full, hairy horribleness, are\nμt+1 = Fμt + Kt+1(zt+1 −HFμt)\nΣt+1 = (I −Kt+1H)(FΣtF⊤+ Σx) ,\n(15.22)\nwhere Kt+1 = (FΣtF⊤+ Σx)H⊤(H(FΣtF⊤+ Σx)H⊤+ Σz)−1 is called the Kalman gain\nmatrix. Believe it or not, these equations make some intuitive sense. For example, consider\nKALMAN GAIN\nMATRIX\nthe update for the mean state estimate μ. The term Fμt is the predicted state at t + 1, so\nHFμt is the predicted observation. Therefore, the term zt+1 −HFμt represents the error in\nthe predicted observation. This is multiplied by Kt+1 to correct the predicted state; hence,\nKt+1 is a measure of how seriously to take the new observation relative to the prediction. As\nin Equation (15.20), we also have the property that the variance update is independent of the\nobservations. The sequence of values for Σt and Kt can therefore be computed ofﬂine, and\nthe actual calculations required during online tracking are quite modest.\nTo illustrate these equations at work, we have applied them to the problem of tracking\nan object moving on the X–Y plane. The state variables are X = (X, Y, ˙X, ˙Y )⊤, so F, Σx,\nH, and Σz are 4 × 4 matrices. Figure 15.11(a) shows the true trajectory, a series of noisy\nobservations, and the trajectory estimated by Kalman ﬁltering, along with the covariances\nindicated by the one-standard-deviation contours. The ﬁltering process does a good job of\ntracking the actual motion, and, as expected, the variance quickly reaches a ﬁxed point.\nWe can also derive equations for smoothing as well as ﬁltering with linear Gaussian\nmodels. The smoothing results are shown in Figure 15.11(b). Notice how the variance in the\nposition estimate is sharply reduced, except at the ends of the trajectory (why?), and that the\nestimated trajectory is much smoother.\n15.4.4\nApplicability of Kalman ﬁltering\nThe Kalman ﬁlter and its elaborations are used in a vast array of applications. The “classical”\napplication is in radar tracking of aircraft and missiles. Related applications include acoustic\ntracking of submarines and ground vehicles and visual tracking of vehicles and people. In a\nslightly more esoteric vein, Kalman ﬁlters are used to reconstruct particle trajectories from\nbubble-chamber photographs and ocean currents from satellite surface measurements. The\nrange of application is much larger than just the tracking of motion: any system characterized\nby continuous state variables and noisy measurements will do. Such systems include pulp",
  "range of application is much larger than just the tracking of motion: any system characterized\nby continuous state variables and noisy measurements will do. Such systems include pulp\nmills, chemical plants, nuclear reactors, plant ecosystems, and national economies. Section 15.4.\nKalman Filters\n589\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n6\n7\n8\n9\n10\n11\n12\nX\nY\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n6\n7\n8\n9\n10\n11\n12\nX\nY\ntrue\nobserved\nsmoothed\ntrue\nobserved\nsmoothed\n(a)\n(b)\n2D filtering\n2D smoothing\nFigure 15.11\n(a) Results of Kalman ﬁltering for an object moving on the X–Y plane,\nshowing the true trajectory (left to right), a series of noisy observations, and the trajectory\nestimated by Kalman ﬁltering. Variance in the position estimate is indicated by the ovals. (b)\nThe results of Kalman smoothing for the same observation sequence.\nThe fact that Kalman ﬁltering can be applied to a system does not mean that the re-\nsults will be valid or useful. The assumptions made—a linear Gaussian transition and sensor\nmodels—are very strong. The extended Kalman ﬁlter (EKF) attempts to overcome nonlin-\nEXTENDED KALMAN\nFILTER (EKF)\nearities in the system being modeled. A system is nonlinear if the transition model cannot\nNONLINEAR\nbe described as a matrix multiplication of the state vector, as in Equation (15.21). The EKF\nworks by modeling the system as locally linear in xt in the region of xt = μt, the mean of the\ncurrent state distribution. This works well for smooth, well-behaved systems and allows the\ntracker to maintain and update a Gaussian state distribution that is a reasonable approximation\nto the true posterior. A detailed example is given in Chapter 25.\nWhat does it mean for a system to be “unsmooth” or “poorly behaved”? Technically,\nit means that there is signiﬁcant nonlinearity in system response within the region that is\n“close” (according to the covariance Σt) to the current mean μt. To understand this idea\nin nontechnical terms, consider the example of trying to track a bird as it ﬂies through the\njungle. The bird appears to be heading at high speed straight for a tree trunk. The Kalman\nﬁlter, whether regular or extended, can make only a Gaussian prediction of the location of the\nbird, and the mean of this Gaussian will be centered on the trunk, as shown in Figure 15.12(a).\nA reasonable model of the bird, on the other hand, would predict evasive action to one side or\nthe other, as shown in Figure 15.12(b). Such a model is highly nonlinear, because the bird’s",
  "A reasonable model of the bird, on the other hand, would predict evasive action to one side or\nthe other, as shown in Figure 15.12(b). Such a model is highly nonlinear, because the bird’s\ndecision varies sharply depending on its precise location relative to the trunk.\nTo handle examples like these, we clearly need a more expressive language for repre-\nsenting the behavior of the system being modeled. Within the control theory community, for\nwhich problems such as evasive maneuvering by aircraft raise the same kinds of difﬁculties,\nthe standard solution is the switching Kalman ﬁlter. In this approach, multiple Kalman ﬁl-\nSWITCHING KALMAN\nFILTER 590\nChapter\n15.\nProbabilistic Reasoning over Time\n(a)\n(b)\nFigure 15.12\nA bird ﬂying toward a tree (top views). (a) A Kalman ﬁlter will predict the\nlocation of the bird using a single Gaussian centered on the obstacle. (b) A more realistic\nmodel allows for the bird’s evasive action, predicting that it will ﬂy to one side or the other.\nters run in parallel, each using a different model of the system—for example, one for straight\nﬂight, one for sharp left turns, and one for sharp right turns. A weighted sum of predictions\nis used, where the weight depends on how well each ﬁlter ﬁts the current data. We will see\nin the next section that this is simply a special case of the general dynamic Bayesian net-\nwork model, obtained by adding a discrete “maneuver” state variable to the network shown\nin Figure 15.9. Switching Kalman ﬁlters are discussed further in Exercise 15.10.\n15.5\nDYNAMIC BAYESIAN NETWORKS\nA dynamic Bayesian network, or DBN, is a Bayesian network that represents a temporal\nDYNAMIC BAYESIAN\nNETWORK\nprobability model of the kind described in Section 15.1. We have already seen examples of\nDBNs: the umbrella network in Figure 15.2 and the Kalman ﬁlter network in Figure 15.9. In\ngeneral, each slice of a DBN can have any number of state variables Xt and evidence variables\nEt. For simplicity, we assume that the variables and their links are exactly replicated from\nslice to slice and that the DBN represents a ﬁrst-order Markov process, so that each variable\ncan have parents only in its own slice or the immediately preceding slice.\nIt should be clear that every hidden Markov model can be represented as a DBN with\na single state variable and a single evidence variable. It is also the case that every discrete-\nvariable DBN can be represented as an HMM; as explained in Section 15.3, we can combine",
  "a single state variable and a single evidence variable. It is also the case that every discrete-\nvariable DBN can be represented as an HMM; as explained in Section 15.3, we can combine\nall the state variables in the DBN into a single state variable whose values are all possible\ntuples of values of the individual state variables. Now, if every HMM is a DBN and every\nDBN can be translated into an HMM, what’s the difference? The difference is that, by de- Section 15.5.\nDynamic Bayesian Networks\n591\ncomposing the state of a complex system into its constituent variables, the can take advantage\nof sparseness in the temporal probability model. Suppose, for example, that a DBN has 20\nBoolean state variables, each of which has three parents in the preceding slice. Then the\nDBN transition model has 20 × 23 = 160 probabilities, whereas the corresponding HMM has\n220 states and therefore 240, or roughly a trillion, probabilities in the transition matrix. This\nis bad for at least three reasons: ﬁrst, the HMM itself requires much more space; second,\nthe huge transition matrix makes HMM inference much more expensive; and third, the prob-\nlem of learning such a huge number of parameters makes the pure HMM model unsuitable\nfor large problems. The relationship between DBNs and HMMs is roughly analogous to the\nrelationship between ordinary Bayesian networks and full tabulated joint distributions.\nWe have already explained that every Kalman ﬁlter model can be represented in a\nDBN with continuous variables and linear Gaussian conditional distributions (Figure 15.9).\nIt should be clear from the discussion at the end of the preceding section that not every DBN\ncan be represented by a Kalman ﬁlter model. In a Kalman ﬁlter, the current state distribution\nis always a single multivariate Gaussian distribution—that is, a single “bump” in a particular\nlocation. DBNs, on the other hand, can model arbitrary distributions. For many real-world\napplications, this ﬂexibility is essential. Consider, for example, the current location of my\nkeys. They might be in my pocket, on the bedside table, on the kitchen counter, dangling\nfrom the front door, or locked in the car. A single Gaussian bump that included all these\nplaces would have to allocate signiﬁcant probability to the keys being in mid-air in the front\nhall. Aspects of the real world such as purposive agents, obstacles, and pockets introduce\n“nonlinearities” that require combinations of discrete and continuous variables in order to get",
  "hall. Aspects of the real world such as purposive agents, obstacles, and pockets introduce\n“nonlinearities” that require combinations of discrete and continuous variables in order to get\nreasonable models.\n15.5.1\nConstructing DBNs\nTo construct a DBN, one must specify three kinds of information: the prior distribution over\nthe state variables, P(X0); the transition model P(Xt+1 | Xt); and the sensor model P(Et | Xt).\nTo specify the transition and sensor models, one must also specify the topology of the con-\nnections between successive slices and between the state and evidence variables. Because\nthe transition and sensor models are assumed to be stationary—the same for all t—it is most\nconvenient simply to specify them for the ﬁrst slice. For example, the complete DBN speci-\nﬁcation for the umbrella world is given by the three-node network shown in Figure 15.13(a).\nFrom this speciﬁcation, the complete DBN with an unbounded number of time slices can be\nconstructed as needed by copying the ﬁrst slice.\nLet us now consider a more interesting example: monitoring a battery-powered robot\nmoving in the X–Y plane, as introduced at the end of Section 15.1. First, we need state\nvariables, which will include both Xt = (Xt, Yt) for position and ˙Xt = ( ˙Xt, ˙Yt) for velocity.\nWe assume some method of measuring position—perhaps a ﬁxed camera or onboard GPS\n(Global Positioning System)—yielding measurements Zt. The position at the next time step\ndepends on the current position and velocity, as in the standard Kalman ﬁlter model. The\nvelocity at the next step depends on the current velocity and the state of the battery. We\nadd Batteryt to represent the actual battery charge level, which has as parents the previous 592\nChapter\n15.\nProbabilistic Reasoning over Time\nRain0\nRain1\nUmbrella1\n0.9\nt\n0.2\nf\nP(U  )\n1\nR1\n0.3\nf\n0.7\nt\nP(R )\n1\nR0\n0.7\nP(R )\n0\nZ1\nX1\nX1\ntX\nX0\nX0\n1\nBattery\nBattery 0\n1\nBMeter\n(a)\n(b)\nFigure 15.13\n(a) Speciﬁcation of the prior, transition model, and sensor model for the\numbrella DBN. All subsequent slices are assumed to be copies of slice 1. (b) A simple DBN\nfor robot motion in the X–Y plane.\nbattery level and the velocity, and we add BMetert, which measures the battery charge level.\nThis gives us the basic model shown in Figure 15.13(b).\nIt is worth looking in more depth at the nature of the sensor model for BMetert. Let\nus suppose, for simplicity, that both Batteryt and BMetert can take on discrete values 0",
  "This gives us the basic model shown in Figure 15.13(b).\nIt is worth looking in more depth at the nature of the sensor model for BMetert. Let\nus suppose, for simplicity, that both Batteryt and BMetert can take on discrete values 0\nthrough 5. If the meter is always accurate, then the CPT P(BMetert | Batteryt) should have\nprobabilities of 1.0 “along the diagonal” and probabilities of 0.0 elsewhere. In reality, noise\nalways creeps into measurements. For continuous measurements, a Gaussian distribution\nwith a small variance might be used.5 For our discrete variables, we can approximate a\nGaussian using a distribution in which the probability of error drops off in the appropriate\nway, so that the probability of a large error is very small. We use the term Gaussian error\nmodel to cover both the continuous and discrete versions.\nGAUSSIAN ERROR\nMODEL\nAnyone with hands-on experience of robotics, computerized process control, or other\nforms of automatic sensing will readily testify to the fact that small amounts of measurement\nnoise are often the least of one’s problems. Real sensors fail. When a sensor fails, it does\nnot necessarily send a signal saying, “Oh, by the way, the data I’m about to send you is a\nload of nonsense.” Instead, it simply sends the nonsense. The simplest kind of failure is\ncalled a transient failure, where the sensor occasionally decides to send some nonsense. For\nTRANSIENT FAILURE\nexample, the battery level sensor might have a habit of sending a zero when someone bumps\nthe robot, even if the battery is fully charged.\nLet’s see what happens when a transient failure occurs with a Gaussian error model that\ndoesn’t accommodate such failures. Suppose, for example, that the robot is sitting quietly and\nobserves 20 consecutive battery readings of 5. Then the battery meter has a temporary seizure\n5 Strictly speaking, a Gaussian distribution is problematic because it assigns nonzero probability to large nega-\ntive charge levels. The beta distribution is sometimes a better choice for a variable whose range is restricted. Section 15.5.\nDynamic Bayesian Networks\n593\nand the next reading is BMeter21 = 0. What will the simple Gaussian error model lead us to\nbelieve about Battery21? According to Bayes’ rule, the answer depends on both the sensor\nmodel P(BMeter21 = 0 | Battery 21) and the prediction P(Battery 21 | BMeter1:20). If the\nprobability of a large sensor error is signiﬁcantly less likely than the probability of a transition",
  "model P(BMeter21 = 0 | Battery 21) and the prediction P(Battery 21 | BMeter1:20). If the\nprobability of a large sensor error is signiﬁcantly less likely than the probability of a transition\nto Battery21 = 0, even if the latter is very unlikely, then the posterior distribution will assign\na high probability to the battery’s being empty. A second reading of 0 at t = 22 will make\nthis conclusion almost certain. If the transient failure then disappears and the reading returns\nto 5 from t = 23 onwards, the estimate for the battery level will quickly return to 5, as if by\nmagic. This course of events is illustrated in the upper curve of Figure 15.14(a), which shows\nthe expected value of Batteryt over time, using a discrete Gaussian error model.\nDespite the recovery, there is a time (t = 22) when the robot is convinced that its battery\nis empty; presumably, then, it should send out a mayday signal and shut down. Alas, its\noversimpliﬁed sensor model has led it astray. How can this be ﬁxed? Consider a familiar\nexample from everyday human driving: on sharp curves or steep hills, one’s “fuel tank empty”\nwarning light sometimes turns on. Rather than looking for the emergency phone, one simply\nrecalls that the fuel gauge sometimes gives a very large error when the fuel is sloshing around\nin the tank. The moral of the story is the following: for the system to handle sensor failure\nproperly, the sensor model must include the possibility of failure.\nThe simplest kind of failure model for a sensor allows a certain probability that the\nsensor will return some completely incorrect value, regardless of the true state of the world.\nFor example, if the battery meter fails by returning 0, we might say that\nP(BMetert = 0 | Batteryt = 5) = 0.03 ,\nwhich is presumably much larger than the probability assigned by the simple Gaussian error\nmodel. Let’s call this the transient failure model. How does it help when we are faced\nTRANSIENT FAILURE\nMODEL\nwith a reading of 0? Provided that the predicted probability of an empty battery, according\nto the readings so far, is much less than 0.03, then the best explanation of the observation\nBMeter21 = 0 is that the sensor has temporarily failed. Intuitively, we can think of the belief\nabout the battery level as having a certain amount of “inertia” that helps to overcome tempo-\nrary blips in the meter reading. The upper curve in Figure 15.14(b) shows that the transient",
  "about the battery level as having a certain amount of “inertia” that helps to overcome tempo-\nrary blips in the meter reading. The upper curve in Figure 15.14(b) shows that the transient\nfailure model can handle transient failures without a catastrophic change in beliefs.\nSo much for temporary blips. What about a persistent sensor failure? Sadly, failures of\nthis kind are all too common. If the sensor returns 20 readings of 5 followed by 20 readings\nof 0, then the transient sensor failure model described in the preceding paragraph will result\nin the robot gradually coming to believe that its battery is empty when in fact it may be that\nthe meter has failed. The lower curve in Figure 15.14(b) shows the belief “trajectory” for\nthis case. By t = 25—ﬁve readings of 0—the robot is convinced that its battery is empty.\nObviously, we would prefer the robot to believe that its battery meter is broken—if indeed\nthis is the more likely event.\nUnsurprisingly, to handle persistent failure, we need a persistent failure model that\nPERSISTENT\nFAILURE MODEL\ndescribes how the sensor behaves under normal conditions and after failure. To do this, we\nneed to augment the state of the system with an additional variable, say, BMBroken, that\ndescribes the status of the battery meter. The persistence of failure must be modeled by an 594\nChapter\n15.\nProbabilistic Reasoning over Time\n-1\n0\n1\n2\n3\n4\n5\n15\n20\n25\n30\nE(Batteryt)\nTime step t\nE(Batteryt |...5555005555...)\nE(Batteryt |...5555000000...)\n-1\n0\n1\n2\n3\n4\n5\n15\n20\n25\n30\nE(Batteryt)\nTime step\nE(Batteryt |...5555005555...)\nE(Batteryt |...5555000000...)\n(a)\n(b)\nFigure 15.14\n(a) Upper curve: trajectory of the expected value of Batteryt for an observa-\ntion sequence consisting of all 5s except for 0s at t = 21 and t = 22, using a simple Gaussian\nerror model. Lower curve: trajectory when the observation remains at 0 from t = 21 onwards.\n(b) The same experiment run with the transient failure model. Notice that the transient fail-\nure is handled well, but the persistent failure results in excessive pessimism about the battery\ncharge.\n1\nBattery\nBattery0\n1\nBMeter\n0\nBMBroken\n1\nBMBroken\nf\nt\n0\nB\n1\nP(B )\n1.000\n0.001\n-1\n0\n1\n2\n3\n4\n5\n15\n20\n25\n30\nE(Batteryt)\nTime step\nE(Batteryt |...5555005555...)\nE(Batteryt |...5555000000...)\nP(BMBrokent |...5555000000...)\nP(BMBrokent |...5555005555...)\n(a)\n(b)\nFigure 15.15\n(a) A DBN fragment showing the sensor status variable required for mod-",
  "15\n20\n25\n30\nE(Batteryt)\nTime step\nE(Batteryt |...5555005555...)\nE(Batteryt |...5555000000...)\nP(BMBrokent |...5555000000...)\nP(BMBrokent |...5555005555...)\n(a)\n(b)\nFigure 15.15\n(a) A DBN fragment showing the sensor status variable required for mod-\neling persistent failure of the battery sensor. (b) Upper curves: trajectories of the expected\nvalue of Batteryt for the “transient failure” and “permanent failure” observations sequences.\nLower curves: probability trajectories for BMBroken given the two observation sequences.\narc linking BMBroken0 to BMBroken1. This persistence arc has a CPT that gives a small\nPERSISTENCE ARC\nprobability of failure in any given time step, say, 0.001, but speciﬁes that the sensor stays\nbroken once it breaks. When the sensor is OK, the sensor model for BMeter is identical to\nthe transient failure model; when the sensor is broken, it says BMeter is always 0, regardless\nof the actual battery charge. Section 15.5.\nDynamic Bayesian Networks\n595\n0.3\nf\n0.7\nt\nP(R  )\n1\nR0\n0.7\nP(R0)\n0.2\nf\n0.9\nt\nP(U  )\n1\nR1\nUmbrella1\nRain0\nRain1\n0.7\nP(R0)\n4\n0.2\nf\n0.9\nt\nP(U  )\nR4\nf\nt\n0.3\n0.7\nP(R  )\n4\nR3\nUmbrella4\nRain4\n0.2\nf\n0.9\nt\nP(U  )\n3\nR3\nf\nt\nR\n0.3\n0.7\nP(R  )\n3\n2\nUmbrella3\nRain3\n0.2\nf\n0.9\nt\nP(U  )\n2\nR2\nf\nt\nR\n0.3\n0.7\nP(R  )\n2\n1\nUmbrella2\nRain2\n0.2\nf\n0.9\nt\nP(U  )\n1\nR1\nf\nt\nR\n0.3\n0.7\nP(R  )\n1\n0\nUmbrella1\nRain0\nRain1\nFigure 15.16\nUnrolling a dynamic Bayesian network: slices are replicated to accommo-\ndate the observation sequence Umbrella1:3. Further slices have no effect on inferences within\nthe observation period.\nThe persistent failure model for the battery sensor is shown in Figure 15.15(a). Its\nperformance on the two data sequences (temporary blip and persistent failure) is shown in\nFigure 15.15(b). There are several things to notice about these curves. First, in the case\nof the temporary blip, the probability that the sensor is broken rises signiﬁcantly after the\nsecond 0 reading, but immediately drops back to zero once a 5 is observed. Second, in the\ncase of persistent failure, the probability that the sensor is broken rises quickly to almost 1\nand stays there. Finally, once the sensor is known to be broken, the robot can only assume\nthat its battery discharges at the “normal” rate, as shown by the gradually descending level of\nE(Battery t | . . . ).\nSo far, we have merely scratched the surface of the problem of representing complex\nprocesses. The variety of transition models is huge, encompassing topics as disparate as",
  "E(Battery t | . . . ).\nSo far, we have merely scratched the surface of the problem of representing complex\nprocesses. The variety of transition models is huge, encompassing topics as disparate as\nmodeling the human endocrine system and modeling multiple vehicles driving on a freeway.\nSensor modeling is also a vast subﬁeld in itself, but even subtle phenomena, such as sensor\ndrift, sudden decalibration, and the effects of exogenous conditions (such as weather) on\nsensor readings, can be handled by explicit representation within dynamic Bayesian networks.\n15.5.2\nExact inference in DBNs\nHaving sketched some ideas for representing complex processes as DBNs, we now turn to\nthe question of inference. In a sense, this question has already been answered: dynamic\nBayesian networks are Bayesian networks, and we already have algorithms for inference in\nBayesian networks. Given a sequence of observations, one can construct the full Bayesian\nnetwork representation of a DBN by replicating slices until the network is large enough to\naccommodate the observations, as in Figure 15.16. This technique, mentioned in Chapter 14\nin the context of relational probability models, is called unrolling. (Technically, the DBN is\nequivalent to the semi-inﬁnite network obtained by unrolling forever. Slices added beyond\nthe last observation have no effect on inferences within the observation period and can be\nomitted.) Once the DBN is unrolled, one can use any of the inference algorithms—variable\nelimination, clustering methods, and so on—described in Chapter 14.\nUnfortunately, a naive application of unrolling would not be particularly efﬁcient. If\nwe want to perform ﬁltering or smoothing with a long sequence of observations e1:t, the 596\nChapter\n15.\nProbabilistic Reasoning over Time\nunrolled network would require O(t) space and would thus grow without bound as more\nobservations were added. Moreover, if we simply run the inference algorithm anew each\ntime an observation is added, the inference time per update will also increase as O(t).\nLooking back to Section 15.2.1, we see that constant time and space per ﬁltering update\ncan be achieved if the computation can be done recursively. Essentially, the ﬁltering update\nin Equation (15.5) works by summing out the state variables of the previous time step to get\nthe distribution for the new time step. Summing out variables is exactly what the variable\nelimination (Figure 14.11) algorithm does, and it turns out that running variable elimination",
  "the distribution for the new time step. Summing out variables is exactly what the variable\nelimination (Figure 14.11) algorithm does, and it turns out that running variable elimination\nwith the variables in temporal order exactly mimics the operation of the recursive ﬁltering\nupdate in Equation (15.5). The modiﬁed algorithm keeps at most two slices in memory at\nany one time: starting with slice 0, we add slice 1, then sum out slice 0, then add slice 2, then\nsum out slice 1, and so on. In this way, we can achieve constant space and time per ﬁltering\nupdate. (The same performance can be achieved by suitable modiﬁcations to the clustering\nalgorithm.) Exercise 15.17 asks you to verify this fact for the umbrella network.\nSo much for the good news; now for the bad news: It turns out that the “constant” for\nthe per-update time and space complexity is, in almost all cases, exponential in the number of\nstate variables. What happens is that, as the variable elimination proceeds, the factors grow\nto include all the state variables (or, more precisely, all those state variables that have parents\nin the previous time slice). The maximum factor size is O(dn+k) and the total update cost per\nstep is O(ndn+k), where d is the domain size of the variables and k is the maximum number\nof parents of any state variable.\nOf course, this is much less than the cost of HMM updating, which is O(d2n), but it\nis still infeasible for large numbers of variables. This grim fact is somewhat hard to accept.\nWhat it means is that even though we can use DBNs to represent very complex temporal\nprocesses with many sparsely connected variables, we cannot reason efﬁciently and exactly\nabout those processes. The DBN model itself, which represents the prior joint distribution\nover all the variables, is factorable into its constituent CPTs, but the posterior joint distribu-\ntion conditioned on an observation sequence—that is, the forward message—is generally not\nfactorable. So far, no one has found a way around this problem, despite the fact that many\nimportant areas of science and engineering would beneﬁt enormously from its solution. Thus,\nwe must fall back on approximate methods.\n15.5.3\nApproximate inference in DBNs\nSection 14.5 described two approximation algorithms: likelihood weighting (Figure 14.15)\nand Markov chain Monte Carlo (MCMC, Figure 14.16). Of the two, the former is most easily\nadapted to the DBN context. (An MCMC ﬁltering algorithm is described brieﬂy in the notes",
  "and Markov chain Monte Carlo (MCMC, Figure 14.16). Of the two, the former is most easily\nadapted to the DBN context. (An MCMC ﬁltering algorithm is described brieﬂy in the notes\nat the end of the chapter.) We will see, however, that several improvements are required over\nthe standard likelihood weighting algorithm before a practical method emerges.\nRecall that likelihood weighting works by sampling the nonevidence nodes of the net-\nwork in topological order, weighting each sample by the likelihood it accords to the observed\nevidence variables. As with the exact algorithms, we could apply likelihood weighting di-\nrectly to an unrolled DBN, but this would suffer from the same problems of increasing time Section 15.5.\nDynamic Bayesian Networks\n597\nand space requirements per update as the observation sequence grows. The problem is that\nthe standard algorithm runs each sample in turn, all the way through the network. Instead,\nwe can simply run all N samples together through the DBN, one slice at a time. The mod-\niﬁed algorithm ﬁts the general pattern of ﬁltering algorithms, with the set of N samples as\nthe forward message. The ﬁrst key innovation, then, is to use the samples themselves as an\napproximate representation of the current state distribution. This meets the requirement of a\n“constant” time per update, although the constant depends on the number of samples required\nto maintain an accurate approximation. There is also no need to unroll the DBN, because we\nneed to have in memory only the current slice and the next slice.\nIn our discussion of likelihood weighting in Chapter 14, we pointed out that the al-\ngorithm’s accuracy suffers if the evidence variables are “downstream” from the variables\nbeing sampled, because in that case the samples are generated without any inﬂuence from\nthe evidence. Looking at the typical structure of a DBN—say, the umbrella DBN in Fig-\nure 15.16—we see that indeed the early state variables will be sampled without the beneﬁt of\nthe later evidence. In fact, looking more carefully, we see that none of the state variables has\nany evidence variables among its ancestors! Hence, although the weight of each sample will\ndepend on the evidence, the actual set of samples generated will be completely independent\nof the evidence. For example, even if the boss brings in the umbrella every day, the sam-\npling process could still hallucinate endless days of sunshine. What this means in practice is",
  "of the evidence. For example, even if the boss brings in the umbrella every day, the sam-\npling process could still hallucinate endless days of sunshine. What this means in practice is\nthat the fraction of samples that remain reasonably close to the actual series of events (and\ntherefore have nonnegligible weights) drops exponentially with t, the length of the observa-\ntion sequence. In other words, to maintain a given level of accuracy, we need to increase the\nnumber of samples exponentially with t. Given that a ﬁltering algorithm that works in real\ntime can use only a ﬁxed number of samples, what happens in practice is that the error blows\nup after a very small number of update steps.\nClearly, we need a better solution. The second key innovation is to focus the set of\nsamples on the high-probability regions of the state space. This can be done by throwing\naway samples that have very low weight, according to the observations, while replicating\nthose that have high weight. In that way, the population of samples will stay reasonably close\nto reality. If we think of samples as a resource for modeling the posterior distribution, then it\nmakes sense to use more samples in regions of the state space where the posterior is higher.\nA family of algorithms called particle ﬁltering is designed to do just that. Particle\nPARTICLE FILTERING\nﬁltering works as follows: First, a population of N initial-state samples is created by sampling\nfrom the prior distribution P(X0). Then the update cycle is repeated for each time step:\n1. Each sample is propagated forward by sampling the next state value xt+1 given the\ncurrent value xt for the sample, based on the transition model P(Xt+1 | xt).\n2. Each sample is weighted by the likelihood it assigns to the new evidence, P(et+1 | xt+1).\n3. The population is resampled to generate a new population of N samples. Each new\nsample is selected from the current population; the probability that a particular sample\nis selected is proportional to its weight. The new samples are unweighted.\nThe algorithm is shown in detail in Figure 15.17, and its operation for the umbrella DBN is\nillustrated in Figure 15.18. 598\nChapter\n15.\nProbabilistic Reasoning over Time\nfunction PARTICLE-FILTERING(e,N ,dbn) returns a set of samples for the next time step\ninputs: e, the new incoming evidence\nN , the number of samples to be maintained\ndbn, a DBN with prior P(X0), transition model P(X1|X0), sensor model P(E1|X1)",
  "function PARTICLE-FILTERING(e,N ,dbn) returns a set of samples for the next time step\ninputs: e, the new incoming evidence\nN , the number of samples to be maintained\ndbn, a DBN with prior P(X0), transition model P(X1|X0), sensor model P(E1|X1)\npersistent: S, a vector of samples of size N , initially generated from P(X0)\nlocal variables: W , a vector of weights of size N\nfor i = 1 to N do\nS[i] ←sample from P(X1 | X0 = S[i])\n/* step 1 */\nW [i] ←P(e | X1 = S[i])\n/* step 2 */\nS ←WEIGHTED-SAMPLE-WITH-REPLACEMENT(N ,S,W )\n/* step 3 */\nreturn S\nFigure 15.17\nThe particle ﬁltering algorithm implemented as a recursive update op-\neration with state (the set of samples).\nEach of the sampling operations involves sam-\npling the relevant slice variables in topological order, much as in PRIOR-SAMPLE. The\nWEIGHTED-SAMPLE-WITH-REPLACEMENT operation can be implemented to run in O(N)\nexpected time. The step numbers refer to the description in the text.\ntrue\nfalse\n(a) Propagate\n(c) Resample\nRaint\nRaint+1\nRaint+1\nRaint+1\n(b) Weight\nFigure 15.18\nThe particle ﬁltering update cycle for the umbrella DBN with N = 10, show-\ning the sample populations of each state. (a) At time t, 8 samples indicate rain and 2 indicate\n¬rain. Each is propagated forward by sampling the next state through the transition model.\nAt time t + 1, 6 samples indicate rain and 4 indicate ¬rain. (b) ¬umbrella is observed at\nt + 1. Each sample is weighted by its likelihood for the observation, as indicated by the size\nof the circles. (c) A new set of 10 samples is generated by weighted random selection from\nthe current set, resulting in 2 samples that indicate rain and 8 that indicate ¬rain.\nWe can show that this algorithm is consistent—gives the correct probabilities as N tends\nto inﬁnity—by considering what happens during one update cycle. We assume that the sample\npopulation starts with a correct representation of the forward message f1:t = P(Xt | e1:t) at\ntime t. Writing N(xt | e1:t) for the number of samples occupying state xt after observations\ne1:t have been processed, we therefore have\nN(xt | e1:t)/N = P(xt | e1:t)\n(15.23)\nfor large N. Now we propagate each sample forward by sampling the state variables at t + 1,\ngiven the values for the sample at t. The number of samples reaching state xt+1 from each Section 15.6.\nKeeping Track of Many Objects\n599\nxt is the transition probability times the population of xt; hence, the total number of samples\nreaching xt+1 is\nN(xt+1 | e1:t) =\n\f\nxt",
  "Keeping Track of Many Objects\n599\nxt is the transition probability times the population of xt; hence, the total number of samples\nreaching xt+1 is\nN(xt+1 | e1:t) =\n\f\nxt\nP(xt+1 | xt)N(xt | e1:t) .\nNow we weight each sample by its likelihood for the evidence at t+1. A sample in state xt+1\nreceives weight P(et+1 | xt+1). The total weight of the samples in xt+1 after seeing et+1 is\ntherefore\nW(xt+1 | e1:t+1) = P(et+1 | xt+1)N(xt+1 | e1:t) .\nNow for the resampling step. Since each sample is replicated with probability proportional\nto its weight, the number of samples in state xt+1 after resampling is proportional to the total\nweight in xt+1 before resampling:\nN(xt+1 | e1:t+1)/N = α W(xt+1 | e1:t+1)\n= α P(et+1 | xt+1)N(xt+1 | e1:t)\n= α P(et+1 | xt+1)\n\f\nxt\nP(xt+1 | xt)N(xt | e1:t)\n= α NP(et+1 | xt+1)\n\f\nxt\nP(xt+1 | xt)P(xt | e1:t)\n(by 15.23)\n= α′P(et+1 | xt+1)\n\f\nxt\nP(xt+1 | xt)P(xt | e1:t)\n= P(xt+1 | e1:t+1)\n(by 15.5).\nTherefore the sample population after one update cycle correctly represents the forward mes-\nsage at time t + 1.\nParticle ﬁltering is consistent, therefore, but is it efﬁcient? In practice, it seems that the\nanswer is yes: particle ﬁltering seems to maintain a good approximation to the true posterior\nusing a constant number of samples. Under certain assumptions—in particular, that the prob-\nabilities in the transition and sensor models are strictly greater than 0 and less than 1—it is\npossible to prove that the approximation maintains bounded error with high probability. On\nthe practical side, the range of applications has grown to include many ﬁelds of science and\nengineering; some references are given at the end of the chapter.\n15.6\nKEEPING TRACK OF MANY OBJECTS\nThe preceding sections have considered—without mentioning it—state estimation problems\ninvolving a single object. In this section, we see what happens when two or more objects\ngenerate the observations. What makes this case different from plain old state estimation is\nthat there is now the possibility of uncertainty about which object generated which observa-\ntion. This is the identity uncertainty problem of Section 14.6.3 (page 544), now viewed in a\ntemporal context. In the control theory literature, this is the data association problem—that\nDATA ASSOCIATION\nis, the problem of associating observation data with the objects that generated them. 600\nChapter\n15.\nProbabilistic Reasoning over Time\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n(d)\n(c)\n(b)\n(a)",
  "DATA ASSOCIATION\nis, the problem of associating observation data with the objects that generated them. 600\nChapter\n15.\nProbabilistic Reasoning over Time\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n2\n1\n3\n5\n4\n(d)\n(c)\n(b)\n(a)\ntrack termination\nfalse alarm\ndetection\nfailure\ntrack\ninitiation\nFigure 15.19\n(a) Observations made of object locations in 2D space over ﬁve time steps.\nEach observation is labeled with the time step but does not identify the object that produced\nit. (b–c) Possible hypotheses about the underlying object tracks. (d) A hypothesis for the\ncase in which false alarms, detection failures, and track initiation/termination are possible.\nThe data association problem was studied originally in the context of radar tracking,\nwhere reﬂected pulses are detected at ﬁxed time intervals by a rotating radar antenna. At each\ntime step, multiple blips may appear on the screen, but there is no direct observation of which\nblips at time t belong to which blips at time t −1. Figure 15.19(a) shows a simple example\nwith two blips per time step for ﬁve steps. Let the two blip locations at time t be e1\nt and e2\nt .\n(The labeling of blips within a time step as “1” and “2” is completely arbitrary and carries no\ninformation.) Let us assume, for the time being, that exactly two aircraft, A and B, generated\nthe blips; their true positions are XA\nt and XB\nt . Just to keep things simple, we’ll also assume\nthat the each aircraft moves independently according to a known transition model—e.g., a\nlinear Gaussian model as used in the Kalman ﬁlter (Section 15.4).\nSuppose we try to write down the overall probability model for this scenario, just as\nwe did for general temporal processes in Equation (15.3) on page 569. As usual, the joint\ndistribution factors into contributions for each time step as follows:\nP(xA\n0:t, xB\n0:t, e1\n1:t, e2\n1:t) =\nP(xA\n0 )P(xB\n0 )\nt\u0019\ni = 1\nP(xA\ni | xA\ni−1)P(xB\ni | xB\ni−1) P(e1\ni , e2\ni | xA\ni , xB\ni ) .\n(15.24)\nWe would like to factor the observation term P(e1\ni , e2\ni | xA\ni , xB\ni ) into a product of two terms,\none for each object, but this would require knowing which observation was generated by\nwhich object. Instead, we have to sum over all possible ways of associating the observations Section 15.6.\nKeeping Track of Many Objects\n601\nwith the objects. Some of those ways are shown in Figure 15.19(b–c); in general, for n\nobjects and T time steps, there are (n!)T ways of doing it—an awfully large number.",
  "Keeping Track of Many Objects\n601\nwith the objects. Some of those ways are shown in Figure 15.19(b–c); in general, for n\nobjects and T time steps, there are (n!)T ways of doing it—an awfully large number.\nMathematically speaking, the “way of associating the observations with the objects”\nis a collection of unobserved random variable that identify the source of each observation.\nWe’ll write ωt to denote the one-to-one mapping from objects to observations at time t, with\nωt(A) and ωt(B) denoting the speciﬁc observations (1 or 2) that ωt assigns to A and B.\n(For n objects, ωt will have n! possible values; here, n! = 2.) Because the labels “1” ad\n“2” on the observations are assigned arbitrarily, the prior on ωt is uniform and ωt is inde-\npendent of the states of the objects, xA\nt and xB\nt ). So we can condition the observation term\nP(e1\ni , e2\ni | xA\ni , xB\ni ) on ωt and then simplify:\nP(e1\ni , e2\ni | xA\ni , xB\ni ) =\n\f\nωi\nP(e1\ni , e2\ni | xA\ni , xB\ni , ωi)P(ωi | xA\ni , xB\ni )\n=\n\f\nωi\nP(eωi(A)\ni\n| xA\ni )P(eωi(B)\ni\n| xB\ni )P(ωi | xA\ni , xB\ni )\n= 1\n2\n\f\nωi\nP(eωi(A)\ni\n| xA\ni )P(eωi(B)\ni\n| xB\ni ) .\nPlugging this into Equation (15.24), we get an expression that is only in terms of transition\nand sensor models for individual objects and observations.\nAs for all probability models, inference means summing out the variables other than\nthe query and the evidence. For ﬁltering in HMMs and DBNs, we were able to sum out the\nstate variables from 1 to t−1 by a simple dynamic programming trick; for Kalman ﬁlters, we\ntook advantage of special properties of Gaussians. For data association, we are less fortunate.\nThere is no (known) efﬁcient exact algorithm, for the same reason that there is none for the\nswitching Kalman ﬁlter (page 589): the ﬁltering distribution P(xA\nt | e1\n1:t, e2\n1:t) for object A\nends up as a mixture of exponentially many distributions, one for each way of picking a\nsequence of observations to assign to A.\nAs a result of the complexity of exact inference, many different approximate methods\nhave been used. The simplest approach is to choose a single “best” assignment at each time\nstep, given the predicted positions of the objects at the current time step. This assignment\nassociates observations with objects and enables the track of each object to be updated and\na prediction made for the next time step. For choosing the “best” assignment, it is common\nto use the so-called nearest-neighbor ﬁlter, which repeatedly chooses the closest pairing\nNEAREST-NEIGHBOR",
  "a prediction made for the next time step. For choosing the “best” assignment, it is common\nto use the so-called nearest-neighbor ﬁlter, which repeatedly chooses the closest pairing\nNEAREST-NEIGHBOR\nFILTER\nof predicted position and observation and adds that pairing to the assignment. The nearest-\nneighbor ﬁlter works well when the objects are well separated in state space and the prediction\nuncertainty and observation error are small—in other words, when there is no possibility of\nconfusion. When there is more uncertainty as to the correct assignment, a better approach\nis to choose the assignment that maximizes the joint probability of the current observations\ngiven the predicted positions. This can be done very efﬁciently using the Hungarian algo-\nrithm (Kuhn, 1955), even though there are n! assignments to choose from.\nHUNGARIAN\nALGORITHM\nAny method that commits to a single best assignment at each time step fails miserably\nunder more difﬁcult conditions. In particular, if the algorithm commits to an incorrect as-\nsignment, the prediction at the next time step may be signiﬁcantly wrong, leading to more 602\nChapter\n15.\nProbabilistic Reasoning over Time\n(a)\n(b)\nFigure 15.20\nImages from (a) upstream and (b) downstream surveillance cameras roughly\ntwo miles apart on Highway 99 in Sacramento, California. The boxed vehicle has been\nidentiﬁed at both cameras.\nincorrect assignments, and so on. Two modern approaches turn out to be much more effec-\ntive. A particle ﬁltering algorithm (see page 598) for data association works by maintaining\na large collection of possible current assignments. An MCMC algorithm explores the space\nof assignment histories—for example, Figure 15.19(b–c) might be states in the MCMC state\nspace—and can change its mind about previous assignment decisions. Current MCMC data\nassociation methods can handle many hundreds of objects in real time while giving a good\napproximation to the true posterior distributions.\nThe scenario described so far involved n known objects generating n observations at\neach time step. Real application of data association are typically much more complicated.\nOften, the reported observations include false alarms (also known as clutter), which are not\nFALSE ALARM\nCLUTTER\ncaused by real objects. Detection failures can occur, meaning that no observation is reported\nDETECTION FAILURE\nfor a real object. Finally, new objects arrive and old ones disappear. These phenomena, which",
  "FALSE ALARM\nCLUTTER\ncaused by real objects. Detection failures can occur, meaning that no observation is reported\nDETECTION FAILURE\nfor a real object. Finally, new objects arrive and old ones disappear. These phenomena, which\ncreate even more possible worlds to worry about, are illustrated in Figure 15.19(d).\nFigure 15.20 shows two images from widely separated cameras on a California freeway.\nIn this application, we are interested in two goals: estimating the time it takes, under current\ntrafﬁc conditions, to go from one place to another in the freeway system; and measuring\ndemand, i.e., how many vehicles travel between any two points in the system at particular\ntimes of the day and on particular days of the week. Both goals require solving the data\nassociation problem over a wide area with many cameras and tens of thousands of vehicles\nper hour. With visual surveillance, false alarms are caused by moving shadows, articulated\nvehicles, reﬂections in puddles, etc.; detection failures are caused by occlusion, fog, darkness,\nand lack of visual contrast; and vehicles are constantly entering and leaving the freeway\nsystem. Furthermore, the appearance of any given vehicle can change dramatically between\ncameras depending on lighting conditions and vehicle pose in the image, and the transition\nmodel changes as trafﬁc jams come and go. Despite these problems, modern data association\nalgorithms have been successful in estimating trafﬁc parameters in real-world settings. Section 15.7.\nSummary\n603\nData association is an essential foundation for keeping track of a complex world, be-\ncause without it there is no way to combine multiple observations of any given object. When\nobjects in the world interact with each other in complex activities, understanding the world\nrequires combining data association with the relational and open-universe probability models\nof Section 14.6.3. This is currently an active area of research.\n15.7\nSUMMARY\nThis chapter has addressed the general problem of representing and reasoning about proba-\nbilistic temporal processes. The main points are as follows:\n• The changing state of the world is handled by using a set of random variables to repre-\nsent the state at each point in time.\n• Representations can be designed to satisfy the Markov property, so that the future\nis independent of the past given the present. Combined with the assumption that the\nprocess is stationary—that is, the dynamics do not change over time—this greatly",
  "• Representations can be designed to satisfy the Markov property, so that the future\nis independent of the past given the present. Combined with the assumption that the\nprocess is stationary—that is, the dynamics do not change over time—this greatly\nsimpliﬁes the representation.\n• A temporal probability model can be thought of as containing a transition model de-\nscribing the state evolution and a sensor model describing the observation process.\n• The principal inference tasks in temporal models are ﬁltering, prediction, smooth-\ning, and computing the most likely explanation. Each of these can be achieved using\nsimple, recursive algorithms whose run time is linear in the length of the sequence.\n• Three families of temporal models were studied in more depth: hidden Markov mod-\nels, Kalman ﬁlters, and dynamic Bayesian networks (which include the other two as\nspecial cases).\n• Unless special assumptions are made, as in Kalman ﬁlters, exact inference with many\nstate variables is intractable. In practice, the particle ﬁltering algorithm seems to be an\neffective approximation algorithm.\n• When trying to keep track of many objects, uncertainty arises as to which observations\nbelong to which objects—the data association problem. The number of association\nhypotheses is typically intractably large, but MCMC and particle ﬁltering algorithms\nfor data association work well in practice.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nMany of the basic ideas for estimating the state of dynamical systems came from the mathe-\nmatician C. F. Gauss (1809), who formulated a deterministic least-squares algorithm for the\nproblem of estimating orbits from astronomical observations. A. A. Markov (1913) devel-\noped what was later called the Markov assumption in his analysis of stochastic processes; 604\nChapter\n15.\nProbabilistic Reasoning over Time\nhe estimated a ﬁrst-order Markov chain on letters from the text of Eugene Onegin. The gen-\neral theory of Markov chains and their mixing times is covered by Levin et al. (2008).\nSigniﬁcant classiﬁed work on ﬁltering was done during World War II by Wiener (1942)\nfor continuous-time processes and by Kolmogorov (1941) for discrete-time processes. Al-\nthough this work led to important technological developments over the next 20 years, its\nuse of a frequency-domain representation made many calculations quite cumbersome. Di-\nrect state-space modeling of the stochastic process turned out to be simpler, as shown by",
  "use of a frequency-domain representation made many calculations quite cumbersome. Di-\nrect state-space modeling of the stochastic process turned out to be simpler, as shown by\nPeter Swerling (1959) and Rudolf Kalman (1960). The latter paper described what is now\nknown as the Kalman ﬁlter for forward inference in linear systems with Gaussian noise;\nKalman’s results had, however, been obtained previously by the Danish statistician Thorvold\nThiele (1880) and by the Russian mathematician Ruslan Stratonovich (1959), whom Kalman\nmet in Moscow in 1960. After a visit to NASA Ames Research Center in 1960, Kalman\nsaw the applicability of the method to the tracking of rocket trajectories, and the ﬁlter was\nlater implemented for the Apollo missions. Important results on smoothing were derived by\nRauch et al. (1965), and the impressively named Rauch–Tung–Striebel smoother is still a\nstandard technique today. Many early results are gathered in Gelb (1974). Bar-Shalom and\nFortmann (1988) give a more modern treatment with a Bayesian ﬂavor, as well as many ref-\nerences to the vast literature on the subject. Chatﬁeld (1989) and Box et al. (1994) cover the\ncontrol theory approach to time series analysis.\nThe hidden Markov model and associated algorithms for inference and learning, in-\ncluding the forward–backward algorithm, were developed by Baum and Petrie (1966). The\nViterbi algorithm ﬁrst appeared in (Viterbi, 1967). Similar ideas also appeared independently\nin the Kalman ﬁltering community (Rauch et al., 1965). The forward–backward algorithm\nwas one of the main precursors of the general formulation of the EM algorithm (Dempster\net al., 1977); see also Chapter 20. Constant-space smoothing appears in Binder et al. (1997b),\nas does the divide-and-conquer algorithm developed in Exercise 15.3. Constant-time ﬁxed-\nlag smoothing for HMMs ﬁrst appeared in Russell and Norvig (2003). HMMs have found\nmany applications in language processing (Charniak, 1993), speech recognition (Rabiner and\nJuang, 1993), machine translation (Och and Ney, 2003), computational biology (Krogh et al.,\n1994; Baldi et al., 1994), ﬁnancial economics Bhar and Hamori (2004) and other ﬁelds. There\nhave been several extensions to the basic HMM model, for example the Hierarchical HMM\n(Fine et al., 1998) and Layered HMM (Oliver et al., 2004) introduce structure back into the\nmodel, replacing the single state variable of HMMs.\nDynamic Bayesian networks (DBNs) can be viewed as a sparse encoding of a Markov",
  "(Fine et al., 1998) and Layered HMM (Oliver et al., 2004) introduce structure back into the\nmodel, replacing the single state variable of HMMs.\nDynamic Bayesian networks (DBNs) can be viewed as a sparse encoding of a Markov\nprocess and were ﬁrst used in AI by Dean and Kanazawa (1989b), Nicholson and Brady\n(1992), and Kjaerulff (1992). The last work extends the HUGIN Bayes net system to ac-\ncommodate dynamic Bayesian networks. The book by Dean and Wellman (1991) helped\npopularize DBNs and the probabilistic approach to planning and control within AI. Murphy\n(2002) provides a thorough analysis of DBNs.\nDynamic Bayesian networks have become popular for modeling a variety of com-\nplex motion processes in computer vision (Huang et al., 1994; Intille and Bobick, 1999).\nLike HMMs, they have found applications in speech recognition (Zweig and Russell, 1998;\nRichardson et al., 2000; Stephenson et al., 2000; Neﬁan et al., 2002; Livescu et al., 2003), ge- Bibliographical and Historical Notes\n605\nnomics (Murphy and Mian, 1999; Perrin et al., 2003; Husmeier, 2003) and robot localization\n(Theocharous et al., 2004). The link between HMMs and DBNs, and between the forward–\nbackward algorithm and Bayesian network propagation, was made explicitly by Smyth et\nal. (1997). A further uniﬁcation with Kalman ﬁlters (and other statistical models) appears in\nRoweis and Ghahramani (1999). Procedures exist for learning the parameters (Binder et al.,\n1997a; Ghahramani, 1998) and structures (Friedman et al., 1998) of DBNs.\nThe particle ﬁltering algorithm described in Section 15.5 has a particularly interesting\nhistory. The ﬁrst sampling algorithms for particle ﬁltering (also called sequential Monte Carlo\nmethods) were developed in the control theory community by Handschin and Mayne (1969),\nand the resampling idea that is the core of particle ﬁltering appeared in a Russian control\njournal (Zaritskii et al., 1975). It was later reinvented in statistics as sequential importance-\nsampling resampling, or SIR (Rubin, 1988; Liu and Chen, 1998), in control theory as parti-\ncle ﬁltering (Gordon et al., 1993; Gordon, 1994), in AI as survival of the ﬁttest (Kanazawa\net al., 1995), and in computer vision as condensation (Isard and Blake, 1996). The paper by\nKanazawa et al. (1995) includes an improvement called evidence reversal whereby the state\nEVIDENCE\nREVERSAL\nat time t + 1 is sampled conditional on both the state at time t and the evidence at time t + 1.",
  "Kanazawa et al. (1995) includes an improvement called evidence reversal whereby the state\nEVIDENCE\nREVERSAL\nat time t + 1 is sampled conditional on both the state at time t and the evidence at time t + 1.\nThis allows the evidence to inﬂuence sample generation directly and was proved by Doucet\n(1997) and Liu and Chen (1998) to reduce the approximation error. Particle ﬁltering has been\napplied in many areas, including tracking complex motion patterns in video (Isard and Blake,\n1996), predicting the stock market (de Freitas et al., 2000), and diagnosing faults on plane-\ntary rovers (Verma et al., 2004). A variant called the Rao-Blackwellized particle ﬁlter or\nRAO-\nBLACKWELLIZED\nPARTICLE FILTER\nRBPF (Doucet et al., 2000; Murphy and Russell, 2001) applies particle ﬁltering to a subset\nof state variables and, for each particle, performs exact inference on the remaining variables\nconditioned on the value sequence in the particle. In some cases RBPF works well with thou-\nsands of state variables. An application of RBPF to localization and mapping in robotics is\ndescribed in Chapter 25. The book by Doucet et al. (2001) collects many important papers on\nsequential Monte Carlo (SMC) algorithms, of which particle ﬁltering is the most important\nSEQUENTIAL MONTE\nCARLO\ninstance. Pierre Del Moral and colleagues have performed extensive theoretical analyses of\nSMC algorithms (Del Moral, 2004; Del Moral et al., 2006).\nMCMC methods (see Section 14.5.2) can be applied to the ﬁltering problem; for ex-\nample, Gibbs sampling can be applied directly to an unrolled DBN. To avoid the problem of\nincreasing update times as the unrolled network grows, the decayed MCMC ﬁlter (Marthi\nDECAYED MCMC\net al., 2002) prefers to sample more recent state variables, with a probability that decays as\n1/k2 for a variable k steps into the past. Decayed MCMC is a provably nondivergent ﬁlter.\nNondivergence theorems can also be obtained for certain types of assumed-density ﬁlter.\nASSUMED-DENSITY\nFILTER\nAn assumed-density ﬁlter assumes that the posterior distribution over states at time t belongs\nto a particular ﬁnitely parameterized family; if the projection and update steps take it outside\nthis family, the distribution is projected back to give the best approximation within the fam-\nily. For DBNs, the Boyen–Koller algorithm (Boyen et al., 1999) and the factored frontier\nFACTORED\nFRONTIER\nalgorithm (Murphy and Weiss, 2001) assume that the posterior distribution can be approxi-",
  "ily. For DBNs, the Boyen–Koller algorithm (Boyen et al., 1999) and the factored frontier\nFACTORED\nFRONTIER\nalgorithm (Murphy and Weiss, 2001) assume that the posterior distribution can be approxi-\nmated well by a product of small factors. Variational techniques (see Chapter 14) have also\nbeen developed for temporal models. Ghahramani and Jordan (1997) discuss an approxima-\ntion algorithm for the factorial HMM, a DBN in which two or more independently evolving\nFACTORIAL HMM 606\nChapter\n15.\nProbabilistic Reasoning over Time\nMarkov chains are linked by a shared observation stream. Jordan et al. (1998) cover a number\nof other applications.\nData association for multitarget tracking was ﬁrst described in a probabilistic setting\nby Sittler (1964). The ﬁrst practical algorithm for large-scale problems was the “multiple\nhypothesis tracker” or MHT algorithm (Reid, 1979). Many important papers are collected by\nBar-Shalom and Fortmann (1988) and Bar-Shalom (1992). The development of an MCMC\nalgorithm for data association is due to Pasula et al. (1999), who applied it to trafﬁc surveil-\nlance problems. Oh et al. (2009) provide a formal analysis and extensive experimental com-\nparisons to other methods. Schulz et al. (2003) describe a data association method based on\nparticle ﬁltering. Ingemar Cox analyzed the complexity of data association (Cox, 1993; Cox\nand Hingorani, 1994) and brought the topic to the attention of the vision community. He also\nnoted the applicability of the polynomial-time Hungarian algorithm to the problem of ﬁnd-\ning most-likely assignments, which had long been considered an intractable problem in the\ntracking community. The algorithm itself was published by Kuhn (1955), based on transla-\ntions of papers published in 1931 by two Hungarian mathematicians, D´enes K¨onig and Jen¨o\nEgerv´ary. The basic theorem had been derived previously, however, in an unpublished Latin\nmanuscript by the famous Prussian mathematician Carl Gustav Jacobi (1804–1851).\nEXERCISES\n15.1\nShow that any second-order Markov process can be rewritten as a ﬁrst-order Markov\nprocess with an augmented set of state variables. Can this always be done parsimoniously,\ni.e., without increasing the number of parameters needed to specify the transition model?\n15.2\nIn this exercise, we examine what happens to the probabilities in the umbrella world\nin the limit of long time sequences.\na. Suppose we observe an unending sequence of days on which the umbrella appears.",
  "15.2\nIn this exercise, we examine what happens to the probabilities in the umbrella world\nin the limit of long time sequences.\na. Suppose we observe an unending sequence of days on which the umbrella appears.\nShow that, as the days go by, the probability of rain on the current day increases mono-\ntonically toward a ﬁxed point. Calculate this ﬁxed point.\nb. Now consider forecasting further and further into the future, given just the ﬁrst two\numbrella observations. First, compute the probability P(r2+k|u1, u2) for k = 1 . . . 20\nand plot the results. You should see that the probability converges towards a ﬁxed point.\nProve that the exact value of this ﬁxed point is 0.5.\n15.3\nThis exercise develops a space-efﬁcient variant of the forward–backward algorithm\ndescribed in Figure 15.4 (page 576). We wish to compute P(Xk|e1:t) for k = 1, . . . , t. This\nwill be done with a divide-and-conquer approach.\na. Suppose, for simplicity, that t is odd, and let the halfway point be h = (t + 1)/2. Show\nthat P(Xk|e1:t) can be computed for k = 1, . . . , h given just the initial forward message\nf1:0, the backward message bh+1:t, and the evidence e1:h.\nb. Show a similar result for the second half of the sequence. Exercises\n607\nc. Given the results of (a) and (b), a recursive divide-and-conquer algorithm can be con-\nstructed by ﬁrst running forward along the sequence and then backward from the end,\nstoring just the required messages at the middle and the ends. Then the algorithm is\ncalled on each half. Write out the algorithm in detail.\nd. Compute the time and space complexity of the algorithm as a function of t, the length of\nthe sequence. How does this change if we divide the input into more than two pieces?\n15.4\nOn page 577, we outlined a ﬂawed procedure for ﬁnding the most likely state sequence,\ngiven an observation sequence. The procedure involves ﬁnding the most likely state at each\ntime step, using smoothing, and returning the sequence composed of these states. Show that,\nfor some temporal probability models and observation sequences, this procedure returns an\nimpossible state sequence (i.e., the posterior probability of the sequence is zero).\n15.5\nEquation (15.12) describes the ﬁltering process for the matrix formulation of HMMs.\nGive a similar equation for the calculation of likelihoods, which was described generically in\nEquation (15.7).\n15.6\nConsider the vacuum worlds of Figure 4.18 (perfect sensing) and Figure 15.7 (noisy",
  "Give a similar equation for the calculation of likelihoods, which was described generically in\nEquation (15.7).\n15.6\nConsider the vacuum worlds of Figure 4.18 (perfect sensing) and Figure 15.7 (noisy\nsensing). Suppose that the robot receives an observation sequence such that, with perfect\nsensing, there is exactly one possible location it could be in. Is this location necessarily the\nmost probable location under noisy sensing for sufﬁciently small noise probability ϵ? Prove\nyour claim or ﬁnd a counterexample.\n15.7\nIn Section 15.3.2, the prior distribution over locations is uniform and the transition\nmodel assumes an equal probability of moving to any neighboring square. What if those\nassumptions are wrong? Suppose that the initial location is actually chosen uniformly from\nthe northwest quadrant of the room and the Move action actually tends to move southeast.\nKeeping the HMM model ﬁxed, explore the effect on localization and path accuracy as the\nsoutheasterly tendency increases, for different values of ϵ.\n15.8\nConsider a version of the vacuum robot (page 582) that has the policy of going straight\nfor as long as it can; only when it encounters an obstacle does it change to a new (randomly\nselected) heading. To model this robot, each state in the model consists of a (location, head-\ning) pair. Implement this model and see how well the Viterbi algorithm can track a robot with\nthis model. The robot’s policy is more constrained than the random-walk robot; does that\nmean that predictions of the most likely path are more accurate?\n15.9\nThis exercise is concerned with ﬁltering in an environment with no landmarks. Con-\nsider a vacuum robot in an empty room, represented by an n×m rectangular grid. The robot’s\nlocation is hidden; the only evidence available to the observer is a noisy location sensor that\ngives an approximation to the robot’s location. If the robot is at location (x, y) then with\nprobability .1 the sensor gives the correct location, with probability .05 each it reports one\nof the 8 locations immediately surrounding (x, y), with probability .025 each it reports one\nof the 16 locations that surround those 8, and with the remaining probability of .1 it reports\n“no reading.” The robot’s policy is to pick a direction and follow it with probability .8 on\neach step; the robot switches to a randomly selected new heading with probability .2 (or with 608\nChapter\n15.\nProbabilistic Reasoning over Time\nXt\nSt\nZt\nZt+1\nXt+1\nSt+1\nFigure 15.21",
  "each step; the robot switches to a randomly selected new heading with probability .2 (or with 608\nChapter\n15.\nProbabilistic Reasoning over Time\nXt\nSt\nZt\nZt+1\nXt+1\nSt+1\nFigure 15.21\nA Bayesian network representation of a switching Kalman ﬁlter.\nThe\nswitching variable St is a discrete state variable whose value determines the transition\nmodel for the continuous state variables Xt. For any discrete state i, the transition model\nP(Xt+1|Xt, St = i) is a linear Gaussian model, just as in a regular Kalman ﬁlter. The tran-\nsition model for the discrete state, P(St+1|St), can be thought of as a matrix, as in a hidden\nMarkov model.\nprobability 1 if it encounters a wall). Implement this as an HMM and do ﬁltering to track the\nrobot. How accurately can we track the robot’s path?\n15.10\nOften, we wish to monitor a continuous-state system whose behavior switches unpre-\ndictably among a set of k distinct “modes.” For example, an aircraft trying to evade a missile\ncan execute a series of distinct maneuvers that the missile may attempt to track. A Bayesian\nnetwork representation of such a switching Kalman ﬁlter model is shown in Figure 15.21.\na. Suppose that the discrete state St has k possible values and that the prior continuous\nstate estimate P(X0) is a multivariate Gaussian distribution. Show that the prediction\nP(X1) is a mixture of Gaussians—that is, a weighted sum of Gaussians such that the\nweights sum to 1.\nb. Show that if the current continuous state estimate P(Xt|e1:t) is a mixture of m Gaus-\nsians, then in the general case the updated state estimate P(Xt+1|e1:t+1) will be a mix-\nture of km Gaussians.\nc. What aspect of the temporal process do the weights in the Gaussian mixture represent?\nThe results in (a) and (b) show that the representation of the posterior grows without limit even\nfor switching Kalman ﬁlters, which are among the simplest hybrid dynamic models.\n15.11\nComplete the missing step in the derivation of Equation (15.19) on page 586, the ﬁrst\nupdate step for the one-dimensional Kalman ﬁlter.\n15.12\nLet us examine the behavior of the variance update in Equation (15.20) (page 587).\na. Plot the value of σ2\nt as a function of t, given various values for σ2\nx and σ2\nz.\nb. Show that the update has a ﬁxed point σ2 such that σ2\nt →σ2 as t →∞, and calculate\nthe value of σ2.\nc. Give a qualitative explanation for what happens as σ2\nx →0 and as σ2\nz →0. Exercises\n609\n15.13\nA professor wants to know if students are getting enough sleep. Each day, the pro-",
  "t →σ2 as t →∞, and calculate\nthe value of σ2.\nc. Give a qualitative explanation for what happens as σ2\nx →0 and as σ2\nz →0. Exercises\n609\n15.13\nA professor wants to know if students are getting enough sleep. Each day, the pro-\nfessor observes whether the students sleep in class, and whether they have red eyes. The\nprofessor has the following domain theory:\n• The prior probability of getting enough sleep, with no observations, is 0.7.\n• The probability of getting enough sleep on night t is 0.8 given that the student got\nenough sleep the previous night, and 0.3 if not.\n• The probability of having red eyes is 0.2 if the student got enough sleep, and 0.7 if not.\n• The probability of sleeping in class is 0.1 if the student got enough sleep, and 0.3 if not.\nFormulate this information as a dynamic Bayesian network that the professor could use to\nﬁlter or predict from a sequence of observations. Then reformulate it as a hidden Markov\nmodel that has only a single observation variable. Give the complete probability tables for\nthe model.\n15.14\nFor the DBN speciﬁed in Exercise 15.13 and for the evidence values\ne1 = not red eyes, not sleeping in class\ne2 = red eyes, not sleeping in class\ne3 = red eyes, sleeping in class\nperform the following computations:\na. State estimation: Compute P(EnoughSleept|e1:t) for each of t = 1, 2, 3.\nb. Smoothing: Compute P(EnoughSleept|e1:3) for each of t = 1, 2, 3.\nc. Compare the ﬁltered and smoothed probabilities for t = 1 and t = 2.\n15.15\nSuppose that a particular student shows up with red eyes and sleeps in class every day.\nGiven the model described in Exercise 15.13, explain why the probability that the student had\nenough sleep the previous night converges to a ﬁxed point rather than continuing to go down\nas we gather more days of evidence. What is the ﬁxed point? Answer this both numerically\n(by computation) and analytically.\n15.16\nThis exercise analyzes in more detail the persistent-failure model for the battery sen-\nsor in Figure 15.15(a) (page 594).\na. Figure 15.15(b) stops at t = 32. Describe qualitatively what should happen as t →∞\nif the sensor continues to read 0.\nb. Suppose that the external temperature affects the battery sensor in such a way that tran-\nsient failures become more likely as temperature increases. Show how to augment the\nDBN structure in Figure 15.15(a), and explain any required changes to the CPTs.\nc. Given the new network structure, can battery readings be used by the robot to infer the\ncurrent temperature?",
  "DBN structure in Figure 15.15(a), and explain any required changes to the CPTs.\nc. Given the new network structure, can battery readings be used by the robot to infer the\ncurrent temperature?\n15.17\nConsider applying the variable elimination algorithm to the umbrella DBN unrolled\nfor three slices, where the query is P(R3|u1, u2, u3). Show that the space complexity of the\nalgorithm—the size of the largest factor—is the same, regardless of whether the rain variables\nare eliminated in forward or backward order. 16\nMAKING SIMPLE\nDECISIONS\nIn which we see how an agent should make decisions so that it gets what it wants—\non average, at least.\nIn this chapter, we ﬁll in the details of how utility theory combines with probability theory to\nyield a decision-theoretic agent—an agent that can make rational decisions based on what it\nbelieves and what it wants. Such an agent can make decisions in contexts in which uncertainty\nand conﬂicting goals leave a logical agent with no way to decide: a goal-based agent has a\nbinary distinction between good (goal) and bad (non-goal) states, while a decision-theoretic\nagent has a continuous measure of outcome quality.\nSection 16.1 introduces the basic principle of decision theory: the maximization of\nexpected utility. Section 16.2 shows that the behavior of any rational agent can be captured\nby supposing a utility function that is being maximized. Section 16.3 discusses the nature of\nutility functions in more detail, and in particular their relation to individual quantities such as\nmoney. Section 16.4 shows how to handle utility functions that depend on several quantities.\nIn Section 16.5, we describe the implementation of decision-making systems. In particular,\nwe introduce a formalism called a decision network (also known as an inﬂuence diagram)\nthat extends Bayesian networks by incorporating actions and utilities. The remainder of the\nchapter discusses issues that arise in applications of decision theory to expert systems.\n16.1\nCOMBINING BELIEFS AND DESIRES UNDER UNCERTAINTY\nDecision theory, in its simplest form, deals with choosing among actions based on the desir-\nability of their immediate outcomes; that is, the environment is assumed to be episodic in the\nsense deﬁned on page 43. (This assumption is relaxed in Chapter 17.) In Chapter 3 we used\nthe notation RESULT(s0, a) for the state that is the deterministic outcome of taking action a",
  "sense deﬁned on page 43. (This assumption is relaxed in Chapter 17.) In Chapter 3 we used\nthe notation RESULT(s0, a) for the state that is the deterministic outcome of taking action a\nin state s0. In this chapter we deal with nondeterministic partially observable environments.\nSince the agent may not know the current state, we omit it and deﬁne RESULT(a) as a random\nvariable whose values are the possible outcome states. The probability of outcome s′, given\nevidence observations e, is written\nP(RESULT(a) = s′ | a, e) ,\n610 Section 16.2.\nThe Basis of Utility Theory\n611\nwhere the a on the right-hand side of the conditioning bar stands for the event that action a is\nexecuted.1\nThe agent’s preferences are captured by a utility function, U(s), which assigns a single\nUTILITY FUNCTION\nnumber to express the desirability of a state. The expected utility of an action given the evi-\nEXPECTED UTILITY\ndence, EU (a|e), is just the average utility value of the outcomes, weighted by the probability\nthat the outcome occurs:\nEU (a|e) =\n\f\ns′\nP(RESULT(a) = s′ | a, e) U(s′) .\n(16.1)\nThe principle of maximum expected utility (MEU) says that a rational agent should choose\nMAXIMUM EXPECTED\nUTILITY\nthe action that maximizes the agent’s expected utility:\naction = argmax\na\nEU (a|e)\nIn a sense, the MEU principle could be seen as deﬁning all of AI. All an intelligent agent has\nto do is calculate the various quantities, maximize utility over its actions, and away it goes.\nBut this does not mean that the AI problem is solved by the deﬁnition!\nThe MEU principle formalizes the general notion that the agent should “do the right\nthing,” but goes only a small distance toward a full operationalization of that advice. Es-\ntimating the state of the world requires perception, learning, knowledge representation, and\ninference. Computing P(RESULT(a) | a, e) requires a complete causal model of the world\nand, as we saw in Chapter 14, NP-hard inference in (very large) Bayesian networks. Comput-\ning the outcome utilities U(s′) often requires searching or planning, because an agent may\nnot know how good a state is until it knows where it can get to from that state. So, decision\ntheory is not a panacea that solves the AI problem—but it does provide a useful framework.\nThe MEU principle has a clear relation to the idea of performance measures introduced\nin Chapter 2. The basic idea is simple. Consider the environments that could lead to an",
  "The MEU principle has a clear relation to the idea of performance measures introduced\nin Chapter 2. The basic idea is simple. Consider the environments that could lead to an\nagent having a given percept history, and consider the different agents that we could design.\nIf an agent acts so as to maximize a utility function that correctly reﬂects the performance\nmeasure, then the agent will achieve the highest possible performance score (averaged over\nall the possible environments). This is the central justiﬁcation for the MEU principle itself.\nWhile the claim may seem tautological, it does in fact embody a very important transition\nfrom a global, external criterion of rationality—the performance measure over environment\nhistories—to a local, internal criterion involving the maximization of a utility function applied\nto the next state.\n16.2\nTHE BASIS OF UTILITY THEORY\nIntuitively, the principle of Maximum Expected Utility (MEU) seems like a reasonable way\nto make decisions, but it is by no means obvious that it is the only rational way. After all,\nwhy should maximizing the average utility be so special? What’s wrong with an agent that\n1 Classical decision theory leaves the current state S0 implicit, but we could make it explicit by writing\nP(RESULT(a) = s′ | a, e) = P\ns P(RESULT(s, a) = s′ | a)P(S0 = s | e). 612\nChapter\n16.\nMaking Simple Decisions\nmaximizes the weighted sum of the cubes of the possible utilities, or tries to minimize the\nworst possible loss? Could an agent act rationally just by expressing preferences between\nstates, without giving them numeric values? Finally, why should a utility function with the\nrequired properties exist at all? We shall see.\n16.2.1\nConstraints on rational preferences\nThese questions can be answered by writing down some constraints on the preferences that a\nrational agent should have and then showing that the MEU principle can be derived from the\nconstraints. We use the following notation to describe an agent’s preferences:\nA ≻B\nthe agent prefers A over B.\nA ∼B\nthe agent is indifferent between A and B.\nA ≻∼B\nthe agent prefers A over B or is indifferent between them.\nNow the obvious question is, what sorts of things are A and B? They could be states of the\nworld, but more often than not there is uncertainty about what is really being offered. For\nexample, an airline passenger who is offered “the pasta dish or the chicken” does not know",
  "world, but more often than not there is uncertainty about what is really being offered. For\nexample, an airline passenger who is offered “the pasta dish or the chicken” does not know\nwhat lurks beneath the tinfoil cover.2 The pasta could be delicious or congealed, the chicken\njuicy or overcooked beyond recognition. We can think of the set of outcomes for each action\nas a lottery—think of each action as a ticket. A lottery L with possible outcomes S1, . . . , Sn\nLOTTERY\nthat occur with probabilities p1, . . . , pn is written\nL = [p1, S1; p2, S2; . . . pn, Sn] .\nIn general, each outcome Si of a lottery can be either an atomic state or another lottery. The\nprimary issue for utility theory is to understand how preferences between complex lotteries\nare related to preferences between the underlying states in those lotteries. To address this\nissue we list six constraints that we require any reasonable preference relation to obey:\n• Orderability: Given any two lotteries, a rational agent must either prefer one to the\nORDERABILITY\nother or else rate the two as equally preferable. That is, the agent cannot avoid deciding.\nAs we said on page 490, refusing to bet is like refusing to allow time to pass.\nExactly one of (A ≻B), (B ≻A), or (A ∼B) holds.\n• Transitivity: Given any three lotteries, if an agent prefers A to B and prefers B to C,\nTRANSITIVITY\nthen the agent must prefer A to C.\n(A ≻B) ∧(B ≻C) ⇒(A ≻C) .\n• Continuity: If some lottery B is between A and C in preference, then there is some\nCONTINUITY\nprobability p for which the rational agent will be indifferent between getting B for sure\nand the lottery that yields A with probability p and C with probability 1 −p.\nA ≻B ≻C ⇒∃p [p, A; 1 −p, C] ∼B .\n• Substitutability: If an agent is indifferent between two lotteries A and B, then the\nSUBSTITUTABILITY\nagent is indifferent between two more complex lotteries that are the same except that B\n2 We apologize to readers whose local airlines no longer offer food on long ﬂights. Section 16.2.\nThe Basis of Utility Theory\n613\nis substituted for A in one of them. This holds regardless of the probabilities and the\nother outcome(s) in the lotteries.\nA ∼B ⇒[p, A; 1 −p, C] ∼[p, B; 1 −p, C] .\nThis also holds if we substitute ≻for ∼in this axiom.\n• Monotonicity: Suppose two lotteries have the same two possible outcomes, A and B.\nMONOTONICITY\nIf an agent prefers A to B, then the agent must prefer the lottery that has a higher\nprobability for A (and vice versa).",
  "• Monotonicity: Suppose two lotteries have the same two possible outcomes, A and B.\nMONOTONICITY\nIf an agent prefers A to B, then the agent must prefer the lottery that has a higher\nprobability for A (and vice versa).\nA ≻B ⇒(p > q ⇔[p, A; 1 −p, B] ≻[q, A; 1 −q, B]) .\n• Decomposability: Compound lotteries can be reduced to simpler ones using the laws\nDECOMPOSABILITY\nof probability. This has been called the “no fun in gambling” rule because it says that\ntwo consecutive lotteries can be compressed into a single equivalent lottery, as shown\nin Figure 16.1(b).3\n[p, A; 1 −p, [q, B; 1 −q, C]] ∼[p, A; (1 −p)q, B; (1 −p)(1 −q), C] .\nThese constraints are known as the axioms of utility theory. Each axiom can be motivated\nby showing that an agent that violates it will exhibit patently irrational behavior in some\nsituations. For example, we can motivate transitivity by making an agent with nontransitive\npreferences give us all its money. Suppose that the agent has the nontransitive preferences\nA ≻B ≻C ≻A, where A, B, and C are goods that can be freely exchanged. If the agent\ncurrently has A, then we could offer to trade C for A plus one cent. The agent prefers C,\nand so would be willing to make this trade. We could then offer to trade B for C, extracting\nanother cent, and ﬁnally trade A for B. This brings us back where we started from, except\nthat the agent has given us three cents (Figure 16.1(a)). We can keep going around the cycle\nuntil the agent has no money at all. Clearly, the agent has acted irrationally in this case.\n16.2.2\nPreferences lead to utility\nNotice that the axioms of utility theory are really axioms about preferences—they say nothing\nabout a utility function. But in fact from the axioms of utility we can derive the following\nconsequences (for the proof, see von Neumann and Morgenstern, 1944):\n• Existence of Utility Function: If an agent’s preferences obey the axioms of utility, then\nthere exists a function U such that U(A) > U(B) if and only if A is preferred to B,\nand U(A) = U(B) if and only if the agent is indifferent between A and B.\nU(A) > U(B) ⇔A ≻B\nU(A) = U(B) ⇔A ∼B\n• Expected Utility of a Lottery: The utility of a lottery is the sum of the probability of\neach outcome times the utility of that outcome.\nU([p1, S1; . . . ; pn, Sn]) =\n\f\ni\npiU(Si) .\n3 We can account for the enjoyment of gambling by encoding gambling events into the state description; for\nexample, “Have $10 and gambled” could be preferred to “Have $10 and didn’t gamble.” 614",
  "U([p1, S1; . . . ; pn, Sn]) =\n\f\ni\npiU(Si) .\n3 We can account for the enjoyment of gambling by encoding gambling events into the state description; for\nexample, “Have $10 and gambled” could be preferred to “Have $10 and didn’t gamble.” 614\nChapter\n16.\nMaking Simple Decisions\n1¢\n1¢\n1¢\nA\nB\nC\np\nq\nA\nB\nC\np\n(1–p)\n(1–p)(1–q)\n(1–q)\nA\nB\nC\nis equivalent to\n(a)\n(b)\n(1–p)q\nFigure 16.1\n(a) A cycle of exchanges showing that the nontransitive preferences A ≻\nB ≻C ≻A result in irrational behavior. (b) The decomposability axiom.\nIn other words, once the probabilities and utilities of the possible outcome states are speciﬁed,\nthe utility of a compound lottery involving those states is completely determined. Because the\noutcome of a nondeterministic action is a lottery, it follows that an agent can act rationally—\nthat is, consistently with its preferences—only by choosing an action that maximizes expected\nutility according to Equation (16.1).\nThe preceding theorems establish that a utility function exists for any rational agent, but\nthey do not establish that it is unique. It is easy to see, in fact, that an agent’s behavior would\nnot change if its utility function U(S) were transformed according to\nU′(S) = aU(S) + b ,\n(16.2)\nwhere a and b are constants and a > 0; an afﬁne transformation.4 This fact was noted in\nChapter 5 for two-player games of chance; here, we see that it is completely general.\nAs in game-playing, in a deterministic environment an agent just needs a preference\nranking on states—the numbers don’t matter. This is called a value function or ordinal\nVALUE FUNCTION\nutility function.\nORDINAL UTILITY\nFUNCTION\nIt is important to remember that the existence of a utility function that describes an\nagent’s preference behavior does not necessarily mean that the agent is explicitly maximizing\nthat utility function in its own deliberations. As we showed in Chapter 2, rational behavior can\nbe generated in any number of ways. By observing a rational agent’s preferences, however,\nan observer can construct the utility function that represents what the agent is actually trying\nto achieve (even if the agent doesn’t know it).\n4 In this sense, utilities resemble temperatures: a temperature in Fahrenheit is 1.8 times the Celsius temperature\nplus 32. You get the same results in either measurement system. Section 16.3.\nUtility Functions\n615\n16.3\nUTILITY FUNCTIONS\nUtility is a function that maps from lotteries to real numbers. We know there are some axioms",
  "plus 32. You get the same results in either measurement system. Section 16.3.\nUtility Functions\n615\n16.3\nUTILITY FUNCTIONS\nUtility is a function that maps from lotteries to real numbers. We know there are some axioms\non utilities that all rational agents must obey. Is that all we can say about utility functions?\nStrictly speaking, that is it: an agent can have any preferences it likes. For example, an agent\nmight prefer to have a prime number of dollars in its bank account; in which case, if it had $16\nit would give away $3. This might be unusual, but we can’t call it irrational. An agent might\nprefer a dented 1973 Ford Pinto to a shiny new Mercedes. Preferences can also interact: for\nexample, the agent might prefer prime numbers of dollars only when it owns the Pinto, but\nwhen it owns the Mercedes, it might prefer more dollars to fewer. Fortunately, the preferences\nof real agents are usually more systematic, and thus easier to deal with.\n16.3.1\nUtility assessment and utility scales\nIf we want to build a decision-theoretic system that helps the agent make decisions or acts\non his or her behalf, we must ﬁrst work out what the agent’s utility function is. This process,\noften called preference elicitation, involves presenting choices to the agent and using the\nPREFERENCE\nELICITATION\nobserved preferences to pin down the underlying utility function.\nEquation (16.2) says that there is no absolute scale for utilities, but it is helpful, nonethe-\nless, to establish some scale on which utilities can be recorded and compared for any particu-\nlar problem. A scale can be established by ﬁxing the utilities of any two particular outcomes,\njust as we ﬁx a temperature scale by ﬁxing the freezing point and boiling point of water.\nTypically, we ﬁx the utility of a “best possible prize” at U(S) = u⊤and a “worst possible\ncatastrophe” at U(S) = u⊥. Normalized utilities use a scale with u⊥= 0 and u⊤= 1.\nNORMALIZED\nUTILITIES\nGiven a utility scale between u⊤and u⊥, we can assess the utility of any particular\nprize S by asking the agent to choose between S and a standard lottery [p, u⊤; (1−p), u⊥].\nSTANDARD LOTTERY\nThe probability p is adjusted until the agent is indifferent between S and the standard lottery.\nAssuming normalized utilities, the utility of S is given by p. Once this is done for each prize,\nthe utilities for all lotteries involving those prizes are determined.\nIn medical, transportation, and environmental decision problems, among others, peo-",
  "the utilities for all lotteries involving those prizes are determined.\nIn medical, transportation, and environmental decision problems, among others, peo-\nple’s lives are at stake. In such cases, u⊥is the value assigned to immediate death (or perhaps\nmany deaths). Although nobody feels comfortable with putting a value on human life, it is a\nfact that tradeoffs are made all the time. Aircraft are given a complete overhaul at intervals\ndetermined by trips and miles ﬂown, rather than after every trip. Cars are manufactured in\na way that trades off costs against accident survival rates. Paradoxically, a refusal to “put a\nmonetary value on life” means that life is often undervalued. Ross Shachter relates an ex-\nperience with a government agency that commissioned a study on removing asbestos from\nschools. The decision analysts performing the study assumed a particular dollar value for the\nlife of a school-age child, and argued that the rational choice under that assumption was to\nremove the asbestos. The agency, morally outraged at the idea of setting the value of a life,\nrejected the report out of hand. It then decided against asbestos removal—implicitly asserting\na lower value for the life of a child than that assigned by the analysts. 616\nChapter\n16.\nMaking Simple Decisions\nSome attempts have been made to ﬁnd out the value that people place on their own\nlives. One common “currency” used in medical and safety analysis is the micromort, a\nMICROMORT\none in a million chance of death. If you ask people how much they would pay to avoid a\nrisk—for example, to avoid playing Russian roulette with a million-barreled revolver—they\nwill respond with very large numbers, perhaps tens of thousands of dollars, but their actual\nbehavior reﬂects a much lower monetary value for a micromort. For example, driving in a car\nfor 230 miles incurs a risk of one micromort; over the life of your car—say, 92,000 miles—\nthat’s 400 micromorts. People appear to be willing to pay about $10,000 (at 2009 prices)\nmore for a safer car that halves the risk of death, or about $50 per micromort. A number\nof studies have conﬁrmed a ﬁgure in this range across many individuals and risk types. Of\ncourse, this argument holds only for small risks. Most people won’t agree to kill themselves\nfor $50 million.\nAnother measure is the QALY, or quality-adjusted life year. Patients with a disability\nQALY\nare willing to accept a shorter life expectancy to be restored to full health. For example,",
  "for $50 million.\nAnother measure is the QALY, or quality-adjusted life year. Patients with a disability\nQALY\nare willing to accept a shorter life expectancy to be restored to full health. For example,\nkidney patients on average are indifferent between living two years on a dialysis machine and\none year at full health.\n16.3.2\nThe utility of money\nUtility theory has its roots in economics, and economics provides one obvious candidate\nfor a utility measure: money (or more speciﬁcally, an agent’s total net assets). The almost\nuniversal exchangeability of money for all kinds of goods and services suggests that money\nplays a signiﬁcant role in human utility functions.\nIt will usually be the case that an agent prefers more money to less, all other things being\nequal. We say that the agent exhibits a monotonic preference for more money. This does\nMONOTONIC\nPREFERENCE\nnot mean that money behaves as a utility function, because it says nothing about preferences\nbetween lotteries involving money.\nSuppose you have triumphed over the other competitors in a television game show. The\nhost now offers you a choice: either you can take the $1,000,000 prize or you can gamble it\non the ﬂip of a coin. If the coin comes up heads, you end up with nothing, but if it comes\nup tails, you get $2,500,000. If you’re like most people, you would decline the gamble and\npocket the million. Are you being irrational?\nAssuming the coin is fair, the expected monetary value (EMV) of the gamble is 1\n2($0)\nEXPECTED\nMONETARY VALUE\n+ 1\n2($2,500,000) = $1,250,000, which is more than the original $1,000,000. But that does\nnot necessarily mean that accepting the gamble is a better decision. Suppose we use Sn to\ndenote the state of possessing total wealth $n, and that your current wealth is $k. Then the\nexpected utilities of the two actions of accepting and declining the gamble are\nEU (Accept) =\n1\n2U(Sk) + 1\n2U(Sk+2,500,000) ,\nEU (Decline) = U(Sk+1,000,000) .\nTo determine what to do, we need to assign utilities to the outcome states. Utility is not\ndirectly proportional to monetary value, because the utility for your ﬁrst million is very high\n(or so they say), whereas the utility for an additional million is smaller. Suppose you assign\na utility of 5 to your current ﬁnancial status (Sk), a 9 to the state Sk+2,500,000, and an 8 to the Section 16.3.\nUtility Functions\n617\nU\n$\n$\n-150,000\n800,000\n(a)\n(b)\no\no\no\no\no\no\no\no\no\no\no\no\no\no\no\nU\nFigure 16.2",
  "a utility of 5 to your current ﬁnancial status (Sk), a 9 to the state Sk+2,500,000, and an 8 to the Section 16.3.\nUtility Functions\n617\nU\n$\n$\n-150,000\n800,000\n(a)\n(b)\no\no\no\no\no\no\no\no\no\no\no\no\no\no\no\nU\nFigure 16.2\nThe utility of money. (a) Empirical data for Mr. Beard over a limited range.\n(b) A typical curve for the full range.\nstate Sk+1,000,000. Then the rational action would be to decline, because the expected utility\nof accepting is only 7 (less than the 8 for declining). On the other hand, a billionaire would\nmost likely have a utility function that is locally linear over the range of a few million more,\nand thus would accept the gamble.\nIn a pioneering study of actual utility functions, Grayson (1960) found that the utility of\nmoney was almost exactly proportional to the logarithm of the amount. (This idea was ﬁrst\nsuggested by Bernoulli (1738); see Exercise 16.3.) One particular utility curve, for a certain\nMr. Beard, is shown in Figure 16.2(a). The data obtained for Mr. Beard’s preferences are\nconsistent with a utility function\nU(Sk+n) = −263.31 + 22.09 log(n + 150, 000)\nfor the range between n = −$150, 000 and n = $800, 000.\nWe should not assume that this is the deﬁnitive utility function for monetary value, but\nit is likely that most people have a utility function that is concave for positive wealth. Going\ninto debt is bad, but preferences between different levels of debt can display a reversal of\nthe concavity associated with positive wealth. For example, someone already $10,000,000 in\ndebt might well accept a gamble on a fair coin with a gain of $10,000,000 for heads and a\nloss of $20,000,000 for tails.5 This yields the S-shaped curve shown in Figure 16.2(b).\nIf we restrict our attention to the positive part of the curves, where the slope is decreas-\ning, then for any lottery L, the utility of being faced with that lottery is less than the utility of\nbeing handed the expected monetary value of the lottery as a sure thing:\nU(L) < U(SEMV (L)) .\nThat is, agents with curves of this shape are risk-averse: they prefer a sure thing with a\nRISK-AVERSE\npayoff that is less than the expected monetary value of a gamble. On the other hand, in the\n“desperate” region at large negative wealth in Figure 16.2(b), the behavior is risk-seeking.\nRISK-SEEKING\n5 Such behavior might be called desperate, but it is rational if one is already in a desperate situation. 618\nChapter\n16.\nMaking Simple Decisions",
  "RISK-SEEKING\n5 Such behavior might be called desperate, but it is rational if one is already in a desperate situation. 618\nChapter\n16.\nMaking Simple Decisions\nThe value an agent will accept in lieu of a lottery is called the certainty equivalent of the\nCERTAINTY\nEQUIVALENT\nlottery. Studies have shown that most people will accept about $400 in lieu of a gamble that\ngives $1000 half the time and $0 the other half—that is, the certainty equivalent of the lottery\nis $400, while the EMV is $500. The difference between the EMV of a lottery and its certainty\nequivalent is called the insurance premium. Risk aversion is the basis for the insurance\nINSURANCE\nPREMIUM\nindustry, because it means that insurance premiums are positive. People would rather pay a\nsmall insurance premium than gamble the price of their house against the chance of a ﬁre.\nFrom the insurance company’s point of view, the price of the house is very small compared\nwith the ﬁrm’s total reserves. This means that the insurer’s utility curve is approximately\nlinear over such a small region, and the gamble costs the company almost nothing.\nNotice that for small changes in wealth relative to the current wealth, almost any curve\nwill be approximately linear. An agent that has a linear curve is said to be risk-neutral. For\nRISK-NEUTRAL\ngambles with small sums, therefore, we expect risk neutrality. In a sense, this justiﬁes the\nsimpliﬁed procedure that proposed small gambles to assess probabilities and to justify the\naxioms of probability in Section 13.2.3.\n16.3.3\nExpected utility and post-decision disappointment\nThe rational way to choose the best action, a∗, is to maximize expected utility:\na∗= argmax\na\nEU (a|e) .\nIf we have calculated the expected utility correctly according to our probability model, and if\nthe probability model correctly reﬂects the underlying stochastic processes that generate the\noutcomes, then, on average, we will get the utility we expect if the whole process is repeated\nmany times.\nIn reality, however, our model usually oversimpliﬁes the real situation, either because\nwe don’t know enough (e.g., when making a complex investment decision) or because the\ncomputation of the true expected utility is too difﬁcult (e.g., when estimating the utility of\nsuccessor states of the root node in backgammon). In that case, we are really working with\nestimates !\nEU (a|e) of the true expected utility. We will assume, kindly perhaps, that the",
  "successor states of the root node in backgammon). In that case, we are really working with\nestimates !\nEU (a|e) of the true expected utility. We will assume, kindly perhaps, that the\nestimates are unbiased, that is, the expected value of the error, E(!\nEU (a|e) −EU (a|e))), is\nUNBIASED\nzero. In that case, it still seems reasonable to choose the action with the highest estimated\nutility and to expect to receive that utility, on average, when the action is executed.\nUnfortunately, the real outcome will usually be signiﬁcantly worse than we estimated,\neven though the estimate was unbiased! To see why, consider a decision problem in which\nthere are k choices, each of which has true estimated utility of 0. Suppose that the error in\neach utility estimate has zero mean and standard deviation of 1, shown as the bold curve in\nFigure 16.3. Now, as we actually start to generate the estimates, some of the errors will be\nnegative (pessimistic) and some will be positive (optimistic). Because we select the action\nwith the highest utility estimate, we are obviously favoring the overly optimistic estimates,\nand that is the source of the bias. It is a straightforward matter to calculate the distribution\nof the maximum of the k estimates (see Exercise 16.11) and hence quantify the extent of\nour disappointment. The curve in Figure 16.3 for k = 3 has a mean around 0.85, so the\naverage disappointment will be about 85% of the standard deviation in the utility estimates. Section 16.3.\nUtility Functions\n619\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n-5\n-4\n-3\n-2\n-1\n 0\n 1\n 2\n 3\n 4\n 5\nError in utility estimate\nk=3\nk=10\nk=30\nFigure 16.3\nPlot of the error in each of k utility estimates and of the distribution of the\nmaximum of k estimates for k = 3, 10, and 30.\nWith more choices, extremely optimistic estimates are more likely to arise: for k = 30, the\ndisappointment will be around twice the standard deviation in the estimates.\nThis tendency for the estimated expected utility of the best choice to be too high is\ncalled the optimizer’s curse (Smith and Winkler, 2006). It afﬂicts even the most seasoned\nOPTIMIZER’S CURSE\ndecision analysts and statisticians. Serious manifestations include believing that an exciting\nnew drug that has cured 80% patients in a trial will cure 80% of patients (it’s been chosen\nfrom k = thousands of candidate drugs) or that a mutual fund advertised as having above-\naverage returns will continue to have them (it’s been chosen to appear in the advertisement",
  "from k = thousands of candidate drugs) or that a mutual fund advertised as having above-\naverage returns will continue to have them (it’s been chosen to appear in the advertisement\nout of k = dozens of funds in the company’s overall portfolio). It can even be the case that\nwhat appears to be the best choice may not be, if the variance in the utility estimate is high:\na drug, selected from thousands tried, that has cured 9 of 10 patients is probably worse than\none that has cured 800 of 1000.\nThe optimizer’s curse crops up everywhere because of the ubiquity of utility-maximizing\nselection processes, so taking the utility estimates at face value is a bad idea. We can avoid the\ncurse by using an explicit probability model P(!\nEU | EU ) of the error in the utility estimates.\nGiven this model and a prior P(EU ) on what we might reasonably expect the utilities to be,\nwe treat the utility estimate, once obtained, as evidence and compute the posterior distribution\nfor the true utility using Bayes’ rule.\n16.3.4\nHuman judgment and irrationality\nDecision theory is a normative theory: it describes how a rational agent should act. A\nNORMATIVE THEORY\ndescriptive theory, on the other hand, describes how actual agents—for example, humans—\nDESCRIPTIVE\nTHEORY\nreally do act. The application of economic theory would be greatly enhanced if the two\ncoincided, but there appears to be some experimental evidence to the contrary. The evidence\nsuggests that humans are “predictably irrational” (Ariely, 2009). 620\nChapter\n16.\nMaking Simple Decisions\nThe best-known problem is the Allais paradox (Allais, 1953). People are given a choice\nbetween lotteries A and B and then between C and D, which have the following prizes:\nA :\n80% chance of $4000\nC : 20% chance of $4000\nB : 100% chance of $3000\nD : 25% chance of $3000\nMost people consistently prefer B over A (taking the sure thing), and C over D (taking the\nhigher EMV). The normative analysis disagrees! We can see this most easily if we use the\nfreedom implied by Equation (16.2) to set U($0) = 0. In that case, then B ≻A implies\nthat U($3000) > 0.8 U($4000), whereas C ≻D implies exactly the reverse. In other\nwords, there is no utility function that is consistent with these choices. One explanation for\nthe apparently irrational preferences is the certainty effect (Kahneman and Tversky, 1979):\nCERTAINTY EFFECT\npeople are strongly attracted to gains that are certain. There are several reasons why this may",
  "the apparently irrational preferences is the certainty effect (Kahneman and Tversky, 1979):\nCERTAINTY EFFECT\npeople are strongly attracted to gains that are certain. There are several reasons why this may\nbe so. First, people may prefer to reduce their computational burden; by choosing certain\noutcomes, they don’t have to compute with probabilities. But the effect persists even when\nthe computations involved are very easy ones. Second, people may distrust the legitimacy of\nthe stated probabilities. I trust that a coin ﬂip is roughly 50/50 if I have control over the coin\nand the ﬂip, but I may distrust the result if the ﬂip is done by someone with a vested interest\nin the outcome.6 In the presence of distrust, it might be better to go for the sure thing.7 Third,\npeople may be accounting for their emotional state as well as their ﬁnancial state. People\nknow they would experience regret if they gave up a certain reward (B) for an 80% chance at\nREGRET\na higher reward and then lost. In other words, if A is chosen, there is a 20% chance of getting\nno money and feeling like a complete idiot, which is worse than just getting no money. So\nperhaps people who choose B over A and C over D are not being irrational; they are just\nsaying that they are willing to give up $200 of EMV to avoid a 20% chance of feeling like an\nidiot.\nA related problem is the Ellsberg paradox. Here the prizes are ﬁxed, but the probabilities\nare underconstrained. Your payoff will depend on the color of a ball chosen from an urn. You\nare told that the urn contains 1/3 red balls, and 2/3 either black or yellow balls, but you don’t\nknow how many black and how many yellow. Again, you are asked whether you prefer lottery\nA or B; and then C or D:\nA : $100 for a red ball\nC : $100 for a red or yellow ball\nB : $100 for a black ball\nD : $100 for a black or yellow ball .\nIt should be clear that if you think there are more red than black balls then you should prefer\nA over B and C over D; if you think there are fewer red than black you should prefer the\nopposite. But it turns out that most people prefer A over B and also prefer D over C, even\nthough there is no state of the world for which this is rational. It seems that people have\nambiguity aversion: A gives you a 1/3 chance of winning, while B could be anywhere\nAMBIGUITY\nAVERSION\nbetween 0 and 2/3. Similarly, D gives you a 2/3 chance, while C could be anywhere between",
  "ambiguity aversion: A gives you a 1/3 chance of winning, while B could be anywhere\nAMBIGUITY\nAVERSION\nbetween 0 and 2/3. Similarly, D gives you a 2/3 chance, while C could be anywhere between\n1/3 and 3/3. Most people elect the known probability rather than the unknown unknowns.\n6 For example, the mathematician/magician Persi Diaconis can make a coin ﬂip come out the way he wants\nevery time (Landhuis, 2004).\n7 Even the sure thing may not be certain. Despite cast-iron promises, we have not yet received that $27,000,000\nfrom the Nigerian bank account of a previously unknown deceased relative. Section 16.3.\nUtility Functions\n621\nYet another problem is that the exact wording of a decision problem can have a big\nimpact on the agent’s choices; this is called the framing effect. Experiments show that people\nFRAMING EFFECT\nlike a medical procedure that it is described as having a “90% survival rate” about twice as\nmuch as one described as having a “10% death rate,” even though these two statements mean\nexactly the same thing. This discrepancy in judgment has been found in multiple experiments\nand is about the same whether the subjects were patients in a clinic, statistically sophisticated\nbusiness school students, or experienced doctors.\nPeople feel more comfortable making relative utility judgments rather than absolute\nones. I may have little idea how much I might enjoy the various wines offered by a restaurant.\nThe restaurant takes advantage of this by offering a $200 bottle that it knows nobody will buy,\nbut which serves to skew upward the customer’s estimate of the value of all wines and make\nthe $55 bottle seem like a bargain. This is called the anchoring effect.\nANCHORING EFFECT\nIf human informants insist on contradictory preference judgments, there is nothing that\nautomated agents can do to be consistent with them. Fortunately, preference judgments made\nby humans are often open to revision in the light of further consideration. Paradoxes like\nthe Allais paradox are greatly reduced (but not eliminated) if the choices are explained bet-\nter. In work at the Harvard Business School on assessing the utility of money, Keeney and\nRaiffa (1976, p. 210) found the following:\nSubjects tend to be too risk-averse in the small and therefore . . . the ﬁtted utility functions\nexhibit unacceptably large risk premiums for lotteries with a large spread. . . . Most of the\nsubjects, however, can reconcile their inconsistencies and feel that they have learned an",
  "exhibit unacceptably large risk premiums for lotteries with a large spread. . . . Most of the\nsubjects, however, can reconcile their inconsistencies and feel that they have learned an\nimportant lesson about how they want to behave. As a consequence, some subjects cancel\ntheir automobile collision insurance and take out more term insurance on their lives.\nThe evidence for human irrationality is also questioned by researchers in the ﬁeld of evo-\nlutionary psychology, who point to the fact that our brain’s decision-making mechanisms\nEVOLUTIONARY\nPSYCHOLOGY\ndid not evolve to solve word problems with probabilities and prizes stated as decimal num-\nbers. Let us grant, for the sake of argument, that the brain has built-in neural mechanism\nfor computing with probabilities and utilities, or something functionally equivalent; if so, the\nrequired inputs would be obtained through accumulated experience of outcomes and rewards\nrather than through linguistic presentations of numerical values. It is far from obvious that we\ncan directly access the brain’s built-in neural mechanisms by presenting decision problems in\nlinguistic/numerical form. The very fact that different wordings of the same decision prob-\nlem elicit different choices suggests that the decision problem itself is not getting through.\nSpurred by this observation, psychologists have tried presenting problems in uncertain rea-\nsoning and decision making in “evolutionarily appropriate” forms; for example, instead of\nsaying “90% survival rate,” the experimenter might show 100 stick-ﬁgure animations of the\noperation, where the patient dies in 10 of them and survives in 90. (Boredom is a complicat-\ning factor in these experiments!) With decision problems posed in this way, people seem to\nbe much closer to rational behavior than previously suspected. 622\nChapter\n16.\nMaking Simple Decisions\n16.4\nMULTIATTRIBUTE UTILITY FUNCTIONS\nDecision making in the ﬁeld of public policy involves high stakes, in both money and lives.\nFor example, in deciding what levels of harmful emissions to allow from a power plant, pol-\nicy makers must weigh the prevention of death and disability against the beneﬁt of the power\nand the economic burden of mitigating the emissions. Siting a new airport requires consid-\neration of the disruption caused by construction; the cost of land; the distance from centers\nof population; the noise of ﬂight operations; safety issues arising from local topography and",
  "eration of the disruption caused by construction; the cost of land; the distance from centers\nof population; the noise of ﬂight operations; safety issues arising from local topography and\nweather conditions; and so on. Problems like these, in which outcomes are characterized by\ntwo or more attributes, are handled by multiattribute utility theory.\nMULTIATTRIBUTE\nUTILITY THEORY\nWe will call the attributes X = X1, . . . , Xn; a complete vector of assignments will be\nx = ⟨x1, . . . , xn⟩, where each xi is either a numeric value or a discrete value with an assumed\nordering on values. We will assume that higher values of an attribute correspond to higher\nutilities, all other things being equal. For example, if we choose AbsenceOfNoise as an\nattribute in the airport problem, then the greater its value, the better the solution.8 We begin by\nexamining cases in which decisions can be made without combining the attribute values into\na single utility value. Then we look at cases in which the utilities of attribute combinations\ncan be speciﬁed very concisely.\n16.4.1\nDominance\nSuppose that airport site S1 costs less, generates less noise pollution, and is safer than site S2.\nOne would not hesitate to reject S2. We then say that there is strict dominance of S1 over\nSTRICT DOMINANCE\nS2. In general, if an option is of lower value on all attributes than some other option, it need\nnot be considered further. Strict dominance is often very useful in narrowing down the ﬁeld\nof choices to the real contenders, although it seldom yields a unique choice. Figure 16.4(a)\nshows a schematic diagram for the two-attribute case.\nThat is ﬁne for the deterministic case, in which the attribute values are known for sure.\nWhat about the general case, where the outcomes are uncertain? A direct analog of strict\ndominance can be constructed, where, despite the uncertainty, all possible concrete outcomes\nfor S1 strictly dominate all possible outcomes for S2. (See Figure 16.4(b).) Of course, this\nwill probably occur even less often than in the deterministic case.\nFortunately, there is a more useful generalization called stochastic dominance, which\nSTOCHASTIC\nDOMINANCE\noccurs very frequently in real problems. Stochastic dominance is easiest to understand in\nthe context of a single attribute. Suppose we believe that the cost of siting the airport at S1 is\nuniformly distributed between $2.8 billion and $4.8 billion and that the cost at S2 is uniformly",
  "the context of a single attribute. Suppose we believe that the cost of siting the airport at S1 is\nuniformly distributed between $2.8 billion and $4.8 billion and that the cost at S2 is uniformly\ndistributed between $3 billion and $5.2 billion. Figure 16.5(a) shows these distributions, with\ncost plotted as a negative value. Then, given only the information that utility decreases with\n8 In some cases, it may be necessary to subdivide the range of values so that utility varies monotonically within\neach range. For example, if the RoomTemperature attribute has a utility peak at 70◦F, we would split it into two\nattributes measuring the difference from the ideal, one colder and one hotter. Utility would then be monotonically\nincreasing in each attribute. Section 16.4.\nMultiattribute Utility Functions\n623\n(a) \nA\nB\nC\nD\nA\nB\nC\n(b) \nThis region\ndominates A \nX2\nX2\nX1\nX1\nFigure 16.4\nStrict dominance. (a) Deterministic: Option A is strictly dominated by B but\nnot by C or D. (b) Uncertain: A is strictly dominated by B but not by C.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n-6\n-5.5\n-5\n-4.5\n-4\n-3.5\n-3\n-2.5\n-2\nProbability\nNegative cost\nS1\nS2\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n-6\n-5.5\n-5\n-4.5\n-4\n-3.5\n-3\n-2.5\n-2\nProbability\nNegative cost\nS1\nS2\n(a)\n(b)\nFigure 16.5\nStochastic dominance. (a) S1 stochastically dominates S2 on cost. (b) Cu-\nmulative distributions for the negative cost of S1 and S2.\ncost, we can say that S1 stochastically dominates S2 (i.e., S2 can be discarded). It is important\nto note that this does not follow from comparing the expected costs. For example, if we knew\nthe cost of S1 to be exactly $3.8 billion, then we would be unable to make a decision without\nadditional information on the utility of money. (It might seem odd that more information on\nthe cost of S1 could make the agent less able to decide. The paradox is resolved by noting\nthat in the absence of exact cost information, the decision is easier to make but is more likely\nto be wrong.)\nThe exact relationship between the attribute distributions needed to establish stochastic\ndominance is best seen by examining the cumulative distributions, shown in Figure 16.5(b).\n(See also Appendix A.) The cumulative distribution measures the probability that the cost is\nless than or equal to any given amount—that is, it integrates the original distribution. If the\ncumulative distribution for S1 is always to the right of the cumulative distribution for S2, 624\nChapter\n16.\nMaking Simple Decisions",
  "less than or equal to any given amount—that is, it integrates the original distribution. If the\ncumulative distribution for S1 is always to the right of the cumulative distribution for S2, 624\nChapter\n16.\nMaking Simple Decisions\nthen, stochastically speaking, S1 is cheaper than S2. Formally, if two actions A1 and A2 lead\nto probability distributions p1(x) and p2(x) on attribute X, then A1 stochastically dominates\nA2 on X if\n∀x\nx\n\u001a\n−∞\np1(x′) dx′ ≤\nx\n\u001a\n−∞\np2(x′) dx′ .\nThe relevance of this deﬁnition to the selection of optimal decisions comes from the following\nproperty: if A1 stochastically dominates A2, then for any monotonically nondecreasing utility\nfunction U(x), the expected utility of A1 is at least as high as the expected utility of A2.\nHence, if an action is stochastically dominated by another action on all attributes, then it can\nbe discarded.\nThe stochastic dominance condition might seem rather technical and perhaps not so\neasy to evaluate without extensive probability calculations. In fact, it can be decided very\neasily in many cases. Suppose, for example, that the construction transportation cost depends\non the distance to the supplier. The cost itself is uncertain, but the greater the distance, the\ngreater the cost. If S1 is closer than S2, then S1 will dominate S2 on cost. Although we\nwill not present them here, there exist algorithms for propagating this kind of qualitative\ninformation among uncertain variables in qualitative probabilistic networks, enabling a\nQUALITATIVE\nPROBABILISTIC\nNETWORKS\nsystem to make rational decisions based on stochastic dominance, without using any numeric\nvalues.\n16.4.2\nPreference structure and multiattribute utility\nSuppose we have n attributes, each of which has d distinct possible values. To specify the\ncomplete utility function U(x1, . . . , xn), we need dn values in the worst case. Now, the worst\ncase corresponds to a situation in which the agent’s preferences have no regularity at all. Mul-\ntiattribute utility theory is based on the supposition that the preferences of typical agents have\nmuch more structure than that. The basic approach is to identify regularities in the preference\nbehavior we would expect to see and to use what are called representation theorems to show\nREPRESENTATION\nTHEOREM\nthat an agent with a certain kind of preference structure has a utility function\nU(x1, . . . , xn) = F[f1(x1), . . . , fn(xn)] ,",
  "behavior we would expect to see and to use what are called representation theorems to show\nREPRESENTATION\nTHEOREM\nthat an agent with a certain kind of preference structure has a utility function\nU(x1, . . . , xn) = F[f1(x1), . . . , fn(xn)] ,\nwhere F is, we hope, a simple function such as addition. Notice the similarity to the use of\nBayesian networks to decompose the joint probability of several random variables.\nPreferences without uncertainty\nLet us begin with the deterministic case. Remember that for deterministic environments the\nagent has a value function V (x1, . . . , xn); the aim is to represent this function concisely.\nThe basic regularity that arises in deterministic preference structures is called preference\nindependence. Two attributes X1 and X2 are preferentially independent of a third attribute\nPREFERENCE\nINDEPENDENCE\nX3 if the preference between outcomes ⟨x1, x2, x3⟩and ⟨x′\n1, x′\n2, x3⟩does not depend on the\nparticular value x3 for attribute X3.\nGoing back to the airport example, where we have (among other attributes) Noise,\nCost, and Deaths to consider, one may propose that Noise and Cost are preferentially inde- Section 16.4.\nMultiattribute Utility Functions\n625\npendent of Deaths. For example, if we prefer a state with 20,000 people residing in the ﬂight\npath and a construction cost of $4 billion over a state with 70,000 people residing in the ﬂight\npath and a cost of $3.7 billion when the safety level is 0.06 deaths per million passenger miles\nin both cases, then we would have the same preference when the safety level is 0.12 or 0.03;\nand the same independence would hold for preferences between any other pair of values for\nNoise and Cost. It is also apparent that Cost and Deaths are preferentially independent of\nNoise and that Noise and Deaths are preferentially independent of Cost. We say that the\nset of attributes {Noise, Cost, Deaths} exhibits mutual preferential independence (MPI).\nMUTUAL\nPREFERENTIAL\nINDEPENDENCE\nMPI says that, whereas each attribute may be important, it does not affect the way in which\none trades off the other attributes against each other.\nMutual preferential independence is something of a mouthful, but thanks to a remark-\nable theorem due to the economist G´erard Debreu (1960), we can derive from it a very simple\nform for the agent’s value function: If attributes X1, . . . , Xn are mutually preferentially in-\ndependent, then the agent’s preference behavior can be described as maximizing the function",
  "form for the agent’s value function: If attributes X1, . . . , Xn are mutually preferentially in-\ndependent, then the agent’s preference behavior can be described as maximizing the function\nV (x1, . . . , xn) =\n\f\ni\nVi(xi) ,\nwhere each Vi is a value function referring only to the attribute Xi. For example, it might\nwell be the case that the airport decision can be made using a value function\nV (noise, cost, deaths) = −noise × 104 −cost −deaths × 1012 .\nA value function of this type is called an additive value function. Additive functions are an\nADDITIVE VALUE\nFUNCTION\nextremely natural way to describe an agent’s preferences and are valid in many real-world\nsituations. For n attributes, assessing an additive value function requires assessing n separate\none-dimensional value functions rather than one n-dimensional function; typically, this repre-\nsents an exponential reduction in the number of preference experiments that are needed. Even\nwhen MPI does not strictly hold, as might be the case at extreme values of the attributes, an\nadditive value function might still provide a good approximation to the agent’s preferences.\nThis is especially true when the violations of MPI occur in portions of the attribute ranges\nthat are unlikely to occur in practice.\nTo understand MPI better, it helps to look at cases where it doesn’t hold. Suppose you\nare at a medieval market, considering the purchase of some hunting dogs, some chickens,\nand some wicker cages for the chickens. The hunting dogs are very valuable, but if you\ndon’t have enough cages for the chickens, the dogs will eat the chickens; hence, the tradeoff\nbetween dogs and chickens depends strongly on the number of cages, and MPI is violated.\nThe existence of these kinds of interactions among various attributes makes it much harder to\nassess the overall value function.\nPreferences with uncertainty\nWhen uncertainty is present in the domain, we also need to consider the structure of prefer-\nences between lotteries and to understand the resulting properties of utility functions, rather\nthan just value functions. The mathematics of this problem can become quite complicated,\nso we present just one of the main results to give a ﬂavor of what can be done. The reader is\nreferred to Keeney and Raiffa (1976) for a thorough survey of the ﬁeld. 626\nChapter\n16.\nMaking Simple Decisions\nThe basic notion of utility independence extends preference independence to cover\nUTILITY\nINDEPENDENCE",
  "referred to Keeney and Raiffa (1976) for a thorough survey of the ﬁeld. 626\nChapter\n16.\nMaking Simple Decisions\nThe basic notion of utility independence extends preference independence to cover\nUTILITY\nINDEPENDENCE\nlotteries: a set of attributes X is utility independent of a set of attributes Y if preferences be-\ntween lotteries on the attributes in X are independent of the particular values of the attributes\nin Y. A set of attributes is mutually utility independent (MUI) if each of its subsets is\nMUTUALLY UTILITY\nINDEPENDENT\nutility-independent of the remaining attributes. Again, it seems reasonable to propose that\nthe airport attributes are MUI.\nMUI implies that the agent’s behavior can be described using a multiplicative utility\nfunction (Keeney, 1974). The general form of a multiplicative utility function is best seen by\nMULTIPLICATIVE\nUTILITY FUNCTION\nlooking at the case for three attributes. For conciseness, we use Ui to mean Ui(xi):\nU = k1U1 + k2U2 + k3U3 + k1k2U1U2 + k2k3U2U3 + k3k1U3U1\n+ k1k2k3U1U2U3 .\nAlthough this does not look very simple, it contains just three single-attribute utility functions\nand three constants. In general, an n-attribute problem exhibiting MUI can be modeled using\nn single-attribute utilities and n constants. Each of the single-attribute utility functions can\nbe developed independently of the other attributes, and this combination will be guaranteed\nto generate the correct overall preferences. Additional assumptions are required to obtain a\npurely additive utility function.\n16.5\nDECISION NETWORKS\nIn this section, we look at a general mechanism for making rational decisions. The notation\nis often called an inﬂuence diagram (Howard and Matheson, 1984), but we will use the\nINFLUENCE DIAGRAM\nmore descriptive term decision network. Decision networks combine Bayesian networks\nDECISION NETWORK\nwith additional node types for actions and utilities. We use airport siting as an example.\n16.5.1\nRepresenting a decision problem with a decision network\nIn its most general form, a decision network represents information about the agent’s current\nstate, its possible actions, the state that will result from the agent’s action, and the utility of\nthat state. It therefore provides a substrate for implementing utility-based agents of the type\nﬁrst introduced in Section 2.4.5. Figure 16.6 shows a decision network for the airport siting\nproblem. It illustrates the three types of nodes used:",
  "that state. It therefore provides a substrate for implementing utility-based agents of the type\nﬁrst introduced in Section 2.4.5. Figure 16.6 shows a decision network for the airport siting\nproblem. It illustrates the three types of nodes used:\n• Chance nodes (ovals) represent random variables, just as they do in Bayesian networks.\nCHANCE NODES\nThe agent could be uncertain about the construction cost, the level of air trafﬁc and the\npotential for litigation, and the Deaths, Noise, and total Cost variables, each of which\nalso depends on the site chosen. Each chance node has associated with it a conditional\ndistribution that is indexed by the state of the parent nodes. In decision networks, the\nparent nodes can include decision nodes as well as chance nodes. Note that each of\nthe current-state chance nodes could be part of a large Bayesian network for assessing\nconstruction costs, air trafﬁc levels, or litigation potentials.\n• Decision nodes (rectangles) represent points where the decision maker has a choice of\nDECISION NODES Section 16.5.\nDecision Networks\n627\nU\nAirport Site\nDeaths\nNoise\nCost\nLitigation\nConstruction\nAir Traffic\nFigure 16.6\nA simple decision network for the airport-siting problem.\nactions. In this case, the AirportSite action can take on a different value for each site\nunder consideration. The choice inﬂuences the cost, safety, and noise that will result.\nIn this chapter, we assume that we are dealing with a single decision node. Chapter 17\ndeals with cases in which more than one decision must be made.\n• Utility nodes (diamonds) represent the agent’s utility function.9 The utility node has\nUTILITY NODES\nas parents all variables describing the outcome that directly affect utility. Associated\nwith the utility node is a description of the agent’s utility as a function of the parent\nattributes. The description could be just a tabulation of the function, or it might be a\nparameterized additive or linear function of the attribute values.\nA simpliﬁed form is also used in many cases. The notation remains identical, but the\nchance nodes describing the outcome state are omitted. Instead, the utility node is connected\ndirectly to the current-state nodes and the decision node. In this case, rather than representing\na utility function on outcome states, the utility node represents the expected utility associated\nwith each action, as deﬁned in Equation (16.1) on page 611; that is, the node is associated",
  "a utility function on outcome states, the utility node represents the expected utility associated\nwith each action, as deﬁned in Equation (16.1) on page 611; that is, the node is associated\nwith an action-utility function (also known as a Q-function in reinforcement learning, as\nACTION-UTILITY\nFUNCTION\ndescribed in Chapter 21). Figure 16.7 shows the action-utility representation of the airport\nsiting problem.\nNotice that, because the Noise, Deaths, and Cost chance nodes in Figure 16.6 refer to\nfuture states, they can never have their values set as evidence variables. Thus, the simpliﬁed\nversion that omits these nodes can be used whenever the more general form can be used.\nAlthough the simpliﬁed form contains fewer nodes, the omission of an explicit description\nof the outcome of the siting decision means that it is less ﬂexible with respect to changes in\ncircumstances. For example, in Figure 16.6, a change in aircraft noise levels can be reﬂected\nby a change in the conditional probability table associated with the Noise node, whereas a\nchange in the weight accorded to noise pollution in the utility function can be reﬂected by\n9 These nodes are also called value nodes in the literature. 628\nChapter\n16.\nMaking Simple Decisions\nU\nAirport Site\nLitigation\nConstruction\nAir Traffic\nFigure 16.7\nA simpliﬁed representation of the airport-siting problem. Chance nodes cor-\nresponding to outcome states have been factored out.\na change in the utility table. In the action-utility diagram, Figure 16.7, on the other hand,\nall such changes have to be reﬂected by changes to the action-utility table. Essentially, the\naction-utility formulation is a compiled version of the original formulation.\n16.5.2\nEvaluating decision networks\nActions are selected by evaluating the decision network for each possible setting of the deci-\nsion node. Once the decision node is set, it behaves exactly like a chance node that has been\nset as an evidence variable. The algorithm for evaluating decision networks is the following:\n1. Set the evidence variables for the current state.\n2. For each possible value of the decision node:\n(a) Set the decision node to that value.\n(b) Calculate the posterior probabilities for the parent nodes of the utility node, using\na standard probabilistic inference algorithm.\n(c) Calculate the resulting utility for the action.\n3. Return the action with the highest utility.\nThis is a straightforward extension of the Bayesian network algorithm and can be incorpo-",
  "a standard probabilistic inference algorithm.\n(c) Calculate the resulting utility for the action.\n3. Return the action with the highest utility.\nThis is a straightforward extension of the Bayesian network algorithm and can be incorpo-\nrated directly into the agent design given in Figure 13.1 on page 484. We will see in Chap-\nter 17 that the possibility of executing several actions in sequence makes the problem much\nmore interesting.\n16.6\nTHE VALUE OF INFORMATION\nIn the preceding analysis, we have assumed that all relevant information, or at least all avail-\nable information, is provided to the agent before it makes its decision. In practice, this is Section 16.6.\nThe Value of Information\n629\nhardly ever the case. One of the most important parts of decision making is knowing what\nquestions to ask. For example, a doctor cannot expect to be provided with the results of all\npossible diagnostic tests and questions at the time a patient ﬁrst enters the consulting room.10\nTests are often expensive and sometimes hazardous (both directly and because of associated\ndelays). Their importance depends on two factors: whether the test results would lead to a\nsigniﬁcantly better treatment plan, and how likely the various test results are.\nThis section describes information value theory, which enables an agent to choose\nINFORMATION VALUE\nTHEORY\nwhat information to acquire. We assume that, prior to selecting a “real” action represented\nby the decision node, the agent can acquire the value of any of the potentially observable\nchance variables in the model. Thus, information value theory involves a simpliﬁed form\nof sequential decision making—simpliﬁed because the observation actions affect only the\nagent’s belief state, not the external physical state. The value of any particular observation\nmust derive from the potential to affect the agent’s eventual physical action; and this potential\ncan be estimated directly from the decision model itself.\n16.6.1\nA simple example\nSuppose an oil company is hoping to buy one of n indistinguishable blocks of ocean-drilling\nrights. Let us assume further that exactly one of the blocks contains oil worth C dollars, while\nthe others are worthless. The asking price of each block is C/n dollars. If the company is\nrisk-neutral, then it will be indifferent between buying a block and not buying one.\nNow suppose that a seismologist offers the company the results of a survey of block",
  "risk-neutral, then it will be indifferent between buying a block and not buying one.\nNow suppose that a seismologist offers the company the results of a survey of block\nnumber 3, which indicates deﬁnitively whether the block contains oil. How much should\nthe company be willing to pay for the information? The way to answer this question is to\nexamine what the company would do if it had the information:\n• With probability 1/n, the survey will indicate oil in block 3. In this case, the company\nwill buy block 3 for C/n dollars and make a proﬁt of C −C/n = (n −1)C/n dollars.\n• With probability (n−1)/n, the survey will show that the block contains no oil, in which\ncase the company will buy a different block. Now the probability of ﬁnding oil in one\nof the other blocks changes from 1/n to 1/(n −1), so the company makes an expected\nproﬁt of C/(n −1) −C/n = C/n(n −1) dollars.\nNow we can calculate the expected proﬁt, given the survey information:\n1\nn × (n −1)C\nn\n+ n −1\nn\n×\nC\nn(n −1) = C/n .\nTherefore, the company should be willing to pay the seismologist up to C/n dollars for the\ninformation: the information is worth as much as the block itself.\nThe value of information derives from the fact that with the information, one’s course\nof action can be changed to suit the actual situation. One can discriminate according to the\nsituation, whereas without the information, one has to do what’s best on average over the\npossible situations. In general, the value of a given piece of information is deﬁned to be the\ndifference in expected value between best actions before and after information is obtained.\n10 In the United States, the only question that is always asked beforehand is whether the patient has insurance. 630\nChapter\n16.\nMaking Simple Decisions\n16.6.2\nA general formula for perfect information\nIt is simple to derive a general mathematical formula for the value of information. We assume\nthat exact evidence can be obtained about the value of some random variable Ej (that is, we\nlearn Ej = ej), so the phrase value of perfect information (VPI) is used.11\nVALUE OF PERFECT\nINFORMATION\nLet the agent’s initial evidence be e. Then the value of the current best action α is\ndeﬁned by\nEU (α|e) = max\na\n\f\ns′\nP(RESULT(a) = s′ | a, e) U(s′) ,\nand the value of the new best action (after the new evidence Ej = ej is obtained) will be\nEU (αej|e, ej) = max\na\n\f\ns′\nP(RESULT(a) = s′ | a, e, ej) U(s′) .",
  "deﬁned by\nEU (α|e) = max\na\n\f\ns′\nP(RESULT(a) = s′ | a, e) U(s′) ,\nand the value of the new best action (after the new evidence Ej = ej is obtained) will be\nEU (αej|e, ej) = max\na\n\f\ns′\nP(RESULT(a) = s′ | a, e, ej) U(s′) .\nBut Ej is a random variable whose value is currently unknown, so to determine the value of\ndiscovering Ej, given current information e we must average over all possible values ejk that\nwe might discover for Ej, using our current beliefs about its value:\nVPI e(Ej) =\n\u001f\f\nk\nP(Ej = ejk|e) EU (αejk|e, Ej = ejk)\n \n−EU (α|e) .\nTo get some intuition for this formula, consider the simple case where there are only two\nactions, a1 and a2, from which to choose. Their current expected utilities are U1 and U2. The\ninformation Ej = ejk will yield some new expected utilities U′\n1 and U′\n2 for the actions, but\nbefore we obtain Ej, we will have some probability distributions over the possible values of\nU′\n1 and U′\n2 (which we assume are independent).\nSuppose that a1 and a2 represent two different routes through a mountain range in\nwinter. a1 is a nice, straight highway through a low pass, and a2 is a winding dirt road over\nthe top. Just given this information, a1 is clearly preferable, because it is quite possible that\na2 is blocked by avalanches, whereas it is unlikely that anything blocks a1. U1 is therefore\nclearly higher than U2. It is possible to obtain satellite reports Ej on the actual state of each\nroad that would give new expectations, U′\n1 and U′\n2, for the two crossings. The distributions\nfor these expectations are shown in Figure 16.8(a). Obviously, in this case, it is not worth the\nexpense of obtaining satellite reports, because it is unlikely that the information derived from\nthem will change the plan. With no change, information has no value.\nNow suppose that we are choosing between two different winding dirt roads of slightly\ndifferent lengths and we are carrying a seriously injured passenger. Then, even when U1\nand U2 are quite close, the distributions of U′\n1 and U′\n2 are very broad. There is a signiﬁcant\npossibility that the second route will turn out to be clear while the ﬁrst is blocked, and in this\n11 There is no loss of expressiveness in requiring perfect information. Suppose we wanted to model the case\nin which we become somewhat more certain about a variable. We can do that by introducing another variable\nabout which we learn perfect information. For example, suppose we initially have broad uncertainty about the",
  "in which we become somewhat more certain about a variable. We can do that by introducing another variable\nabout which we learn perfect information. For example, suppose we initially have broad uncertainty about the\nvariable Temperature. Then we gain the perfect knowledge Thermometer = 37; this gives us imperfect\ninformation about the true Temperature, and the uncertainty due to measurement error is encoded in the sensor\nmodel P(Thermometer | Temperature). See Exercise 16.17 for another example. Section 16.6.\nThe Value of Information\n631\n(c)\nP(U | Ej)\nU1\nU2\nU\n(b)\nP(U | Ej)\nU1\nU2\nU\n(a)\nP(U | Ej)\nU1\nU2\nU\nFigure 16.8\nThree generic cases for the value of information. In (a), a1 will almost cer-\ntainly remain superior to a2, so the information is not needed. In (b), the choice is unclear and\nthe information is crucial. In (c), the choice is unclear, but because it makes little difference,\nthe information is less valuable. (Note: The fact that U2 has a high peak in (c) means that its\nexpected value is known with higher certainty than U1.)\ncase the difference in utilities will be very high. The VPI formula indicates that it might be\nworthwhile getting the satellite reports. Such a situation is shown in Figure 16.8(b).\nFinally, suppose that we are choosing between the two dirt roads in summertime, when\nblockage by avalanches is unlikely. In this case, satellite reports might show one route to be\nmore scenic than the other because of ﬂowering alpine meadows, or perhaps wetter because\nof errant streams. It is therefore quite likely that we would change our plan if we had the\ninformation. In this case, however, the difference in value between the two routes is still\nlikely to be very small, so we will not bother to obtain the reports. This situation is shown in\nFigure 16.8(c).\nIn sum, information has value to the extent that it is likely to cause a change of plan\nand to the extent that the new plan will be signiﬁcantly better than the old plan.\n16.6.3\nProperties of the value of information\nOne might ask whether it is possible for information to be deleterious: can it actually have\nnegative expected value? Intuitively, one should expect this to be impossible. After all, one\ncould in the worst case just ignore the information and pretend that one has never received it.\nThis is conﬁrmed by the following theorem, which applies to any decision-theoretic agent:\nThe expected value of information is nonnegative:\n∀e, Ej VPI e(Ej) ≥0 .",
  "This is conﬁrmed by the following theorem, which applies to any decision-theoretic agent:\nThe expected value of information is nonnegative:\n∀e, Ej VPI e(Ej) ≥0 .\nThe theorem follows directly from the deﬁnition of VPI, and we leave the proof as an exercise\n(Exercise 16.18). It is, of course, a theorem about expected value, not actual value. Additional\ninformation can easily lead to a plan that turns out to be worse than the original plan if the\ninformation happens to be misleading. For example, a medical test that gives a false positive\nresult may lead to unnecessary surgery; but that does not mean that the test shouldn’t be done. 632\nChapter\n16.\nMaking Simple Decisions\nIt is important to remember that VPI depends on the current state of information, which\nis why it is subscripted. It can change as more information is acquired. For any given piece\nof evidence Ej, the value of acquiring it can go down (e.g., if another variable strongly\nconstrains the posterior for Ej) or up (e.g., if another variable provides a clue on which Ej\nbuilds, enabling a new and better plan to be devised). Thus, VPI is not additive. That is,\nVPI e(Ej, Ek) ̸= VPI e(Ej) + VPI e(Ek)\n(in general) .\nVPI is, however, order independent. That is,\nVPI e(Ej, Ek) = VPI e(Ej) + VPI e,ej(Ek) = VPI e(Ek) + VPI e,ek(Ej) .\nOrder independence distinguishes sensing actions from ordinary actions and simpliﬁes the\nproblem of calculating the value of a sequence of sensing actions.\n16.6.4\nImplementation of an information-gathering agent\nA sensible agent should ask questions in a reasonable order, should avoid asking questions\nthat are irrelevant, should take into account the importance of each piece of information in\nrelation to its cost, and should stop asking questions when that is appropriate. All of these\ncapabilities can be achieved by using the value of information as a guide.\nFigure 16.9 shows the overall design of an agent that can gather information intel-\nligently before acting. For now, we assume that with each observable evidence variable\nEj, there is an associated cost, Cost(Ej), which reﬂects the cost of obtaining the evidence\nthrough tests, consultants, questions, or whatever. The agent requests what appears to be the\nmost efﬁcient observation in terms of utility gain per unit cost. We assume that the result of\nthe action Request(Ej) is that the next percept provides the value of Ej. If no observation is\nworth its cost, the agent selects a “real” action.",
  "most efﬁcient observation in terms of utility gain per unit cost. We assume that the result of\nthe action Request(Ej) is that the next percept provides the value of Ej. If no observation is\nworth its cost, the agent selects a “real” action.\nThe agent algorithm we have described implements a form of information gathering\nthat is called myopic. This is because it uses the VPI formula shortsightedly, calculating the\nMYOPIC\nvalue of information as if only a single evidence variable will be acquired. Myopic control\nis based on the same heuristic idea as greedy search and often works well in practice. (For\nexample, it has been shown to outperform expert physicians in selecting diagnostic tests.)\nfunction INFORMATION-GATHERING-AGENT(percept) returns an action\npersistent: D, a decision network\nintegrate percept into D\nj ←the value that maximizes VPI (Ej) / Cost(Ej)\nif VPI (Ej) > Cost(Ej)\nreturn REQUEST(Ej)\nelse return the best action from D\nFigure 16.9\nDesign of a simple information-gathering agent. The agent works by repeat-\nedly selecting the observation with the highest information value, until the cost of the next\nobservation is greater than its expected beneﬁt. Section 16.7.\nDecision-Theoretic Expert Systems\n633\nHowever, if there is no single evidence variable that will help a lot, a myopic agent might\nhastily take an action when it would have been better to request two or more variables ﬁrst\nand then take action. A better approach in this situation would be to construct a conditional\nplan (as described in Section 11.3.2) that asks for variable values and takes different next\nsteps depending on the answer.\nOne ﬁnal consideration is the effect a series of questions will have on a human respon-\ndent. People may respond better to a series of questions if they “make sense,” so some expert\nsystems are built to take this into account, asking questions in an order that maximizes the\ntotal utility of the system and human rather than an order that maximizes value of information.\n16.7\nDECISION-THEORETIC EXPERT SYSTEMS\nThe ﬁeld of decision analysis, which evolved in the 1950s and 1960s, studies the application\nDECISION ANALYSIS\nof decision theory to actual decision problems. It is used to help make rational decisions in\nimportant domains where the stakes are high, such as business, government, law, military\nstrategy, medical diagnosis and public health, engineering design, and resource management.",
  "important domains where the stakes are high, such as business, government, law, military\nstrategy, medical diagnosis and public health, engineering design, and resource management.\nThe process involves a careful study of the possible actions and outcomes, as well as the\npreferences placed on each outcome. It is traditional in decision analysis to talk about two\nroles: the decision maker states preferences between outcomes, and the decision analyst\nDECISION MAKER\nDECISION ANALYST\nenumerates the possible actions and outcomes and elicits preferences from the decision maker\nto determine the best course of action. Until the early 1980s, the main purpose of decision\nanalysis was to help humans make decisions that actually reﬂect their own preferences. As\nmore and more decision processes become automated, decision analysis is increasingly used\nto ensure that the automated processes are behaving as desired.\nEarly expert system research concentrated on answering questions, rather than on mak-\ning decisions. Those systems that did recommend actions rather than providing opinions on\nmatters of fact generally did so using condition-action rules, rather than with explicit rep-\nresentations of outcomes and preferences. The emergence of Bayesian networks in the late\n1980s made it possible to build large-scale systems that generated sound probabilistic infer-\nences from evidence. The addition of decision networks means that expert systems can be\ndeveloped that recommend optimal decisions, reﬂecting the preferences of the agent as well\nas the available evidence.\nA system that incorporates utilities can avoid one of the most common pitfalls associ-\nated with the consultation process: confusing likelihood and importance. A common strategy\nin early medical expert systems, for example, was to rank possible diagnoses in order of like-\nlihood and report the most likely. Unfortunately, this can be disastrous! For the majority of\npatients in general practice, the two most likely diagnoses are usually “There’s nothing wrong\nwith you” and “You have a bad cold,” but if the third most likely diagnosis for a given patient\nis lung cancer, that’s a serious matter. Obviously, a testing or treatment plan should depend\nboth on probabilities and utilities. Current medical expert systems can take into account the\nvalue of information to recommend tests, and then describe a differential diagnosis. 634\nChapter\n16.\nMaking Simple Decisions",
  "both on probabilities and utilities. Current medical expert systems can take into account the\nvalue of information to recommend tests, and then describe a differential diagnosis. 634\nChapter\n16.\nMaking Simple Decisions\nWe now describe the knowledge engineering process for decision-theoretic expert sys-\ntems. As an example we consider the problem of selecting a medical treatment for a kind of\ncongenital heart disease in children (see Lucas, 1996).\nAbout 0.8% of children are born with a heart anomaly, the most common being aortic\ncoarctation (a constriction of the aorta). It can be treated with surgery, angioplasty (expand-\nAORTIC\nCOARCTATION\ning the aorta with a balloon placed inside the artery), or medication. The problem is to decide\nwhat treatment to use and when to do it: the younger the infant, the greater the risks of certain\ntreatments, but one mustn’t wait too long. A decision-theoretic expert system for this problem\ncan be created by a team consisting of at least one domain expert (a pediatric cardiologist)\nand one knowledge engineer. The process can be broken down into the following steps:\nCreate a causal model. Determine the possible symptoms, disorders, treatments, and\noutcomes. Then draw arcs between them, indicating what disorders cause what symptoms,\nand what treatments alleviate what disorders. Some of this will be well known to the domain\nexpert, and some will come from the literature. Often the model will match well with the\ninformal graphical descriptions given in medical textbooks.\nSimplify to a qualitative decision model. Since we are using the model to make\ntreatment decisions and not for other purposes (such as determining the joint probability of\ncertain symptom/disorder combinations), we can often simplify by removing variables that\nare not involved in treatment decisions. Sometimes variables will have to be split or joined\nto match the expert’s intuitions. For example, the original aortic coarctation model had a\nTreatment variable with values surgery, angioplasty, and medication, and a separate variable\nfor Timing of the treatment. But the expert had a hard time thinking of these separately, so\nthey were combined, with Treatment taking on values such as surgery in 1 month. This gives\nus the model of Figure 16.10.\nAssign probabilities. Probabilities can come from patient databases, literature studies,\nor the expert’s subjective assessments. Note that a diagnostic system will reason from symp-",
  "us the model of Figure 16.10.\nAssign probabilities. Probabilities can come from patient databases, literature studies,\nor the expert’s subjective assessments. Note that a diagnostic system will reason from symp-\ntoms and other observations to the disease or other cause of the problems. Thus, in the early\nyears of building these systems, experts were asked for the probability of a cause given an\neffect. In general they found this difﬁcult to do, and were better able to assess the probability\nof an effect given a cause. So modern systems usually assess causal knowledge and encode it\ndirectly in the Bayesian network structure of the model, leaving the diagnostic reasoning to\nthe Bayesian network inference algorithms (Shachter and Heckerman, 1987).\nAssign utilities. When there are a small number of possible outcomes, they can be\nenumerated and evaluated individually using the methods of Section 16.3.1. We would create\na scale from best to worst outcome and give each a numeric value, for example 0 for death\nand 1 for complete recovery. We would then place the other outcomes on this scale. This\ncan be done by the expert, but it is better if the patient (or in the case of infants, the patient’s\nparents) can be involved, because different people have different preferences. If there are ex-\nponentially many outcomes, we need some way to combine them using multiattribute utility\nfunctions. For example, we may say that the costs of various complications are additive.\nVerify and reﬁne the model. To evaluate the system we need a set of correct (input,\noutput) pairs; a so-called gold standard to compare against. For medical expert systems\nGOLD STANDARD\nthis usually means assembling the best available doctors, presenting them with a few cases, Section 16.7.\nDecision-Theoretic Expert Systems\n635\nTachypnea\nDyspnea\nHeart\nFailure\nAge\nTachycardia\nFailure\nTo Thrive\nIntercostal\nRecession\nHepato-\nmegaly\nPulmonary\nCrepitations\nCardiomegaly\nTreatment\nIntermediate\nResult\nLate\nResult\nParaplegia\nAortic\nAneurysm\nParadoxical\nHypertension\nPostcoarctectomy\nSyndrome\nSex\nCVA\nAortic\nDissection\nMyocardial\nInfarction\nU\nFigure 16.10\nInﬂuence diagram for aortic coarctation (courtesy of Peter Lucas).\nand asking them for their diagnosis and recommended treatment plan. We then see how\nwell the system matches their recommendations. If it does poorly, we try to isolate the parts\nthat are going wrong and ﬁx them. It can be useful to run the system “backward.” Instead",
  "well the system matches their recommendations. If it does poorly, we try to isolate the parts\nthat are going wrong and ﬁx them. It can be useful to run the system “backward.” Instead\nof presenting the system with symptoms and asking for a diagnosis, we can present it with\na diagnosis such as “heart failure,” examine the predicted probability of symptoms such as\ntachycardia, and compare with the medical literature.\nPerform sensitivity analysis. This important step checks whether the best decision is\nSENSITIVITY\nANALYSIS\nsensitive to small changes in the assigned probabilities and utilities by systematically varying\nthose parameters and running the evaluation again. If small changes lead to signiﬁcantly\ndifferent decisions, then it could be worthwhile to spend more resources to collect better\ndata. If all variations lead to the same decision, then the agent will have more conﬁdence that\nit is the right decision. Sensitivity analysis is particularly important, because one of the main 636\nChapter\n16.\nMaking Simple Decisions\ncriticisms of probabilistic approaches to expert systems is that it is too difﬁcult to assess the\nnumerical probabilities required. Sensitivity analysis often reveals that many of the numbers\nneed be speciﬁed only very approximately. For example, we might be uncertain about the\nconditional probability P(tachycardia | dyspnea), but if the optimal decision is reasonably\nrobust to small variations in the probability, then our ignorance is less of a concern.\n16.8\nSUMMARY\nThis chapter shows how to combine utility theory with probability to enable an agent to select\nactions that will maximize its expected performance.\n• Probability theory describes what an agent should believe on the basis of evidence,\nutility theory describes what an agent wants, and decision theory puts the two together\nto describe what an agent should do.\n• We can use decision theory to build a system that makes decisions by considering all\npossible actions and choosing the one that leads to the best expected outcome. Such a\nsystem is known as a rational agent.\n• Utility theory shows that an agent whose preferences between lotteries are consistent\nwith a set of simple axioms can be described as possessing a utility function; further-\nmore, the agent selects actions as if maximizing its expected utility.\n• Multiattribute utility theory deals with utilities that depend on several distinct at-\ntributes of states. Stochastic dominance is a particularly useful technique for making",
  "more, the agent selects actions as if maximizing its expected utility.\n• Multiattribute utility theory deals with utilities that depend on several distinct at-\ntributes of states. Stochastic dominance is a particularly useful technique for making\nunambiguous decisions, even without precise utility values for attributes.\n• Decision networks provide a simple formalism for expressing and solving decision\nproblems. They are a natural extension of Bayesian networks, containing decision and\nutility nodes in addition to chance nodes.\n• Sometimes, solving a problem involves ﬁnding more information before making a de-\ncision. The value of information is deﬁned as the expected improvement in utility\ncompared with making a decision without the information.\n• Expert systems that incorporate utility information have additional capabilities com-\npared with pure inference systems. In addition to being able to make decisions, they\ncan use the value of information to decide which questions to ask, if any; they can rec-\nommend contingency plans; and they can calculate the sensitivity of their decisions to\nsmall changes in probability and utility assessments.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe book L’art de Penser, also known as the Port-Royal Logic (Arnauld, 1662) states:\nTo judge what one must do to obtain a good or avoid an evil, it is necessary to consider\nnot only the good and the evil in itself, but also the probability that it happens or does not\nhappen; and to view geometrically the proportion that all these things have together. Bibliographical and Historical Notes\n637\nModern texts talk of utility rather than good and evil, but this statement correctly notes that\none should multiply utility by probability (“view geometrically”) to give expected utility,\nand maximize that over all outcomes (“all these things”) to “judge what one must do.” It\nis remarkable how much this got right, 350 years ago, and only 8 years after Pascal and\nFermat showed how to use probability correctly. The Port-Royal Logic also marked the ﬁrst\npublication of Pascal’s wager.\nDaniel Bernoulli (1738), investigating the St. Petersburg paradox (see Exercise 16.3),\nwas the ﬁrst to realize the importance of preference measurement for lotteries, writing “the\nvalue of an item must not be based on its price, but rather on the utility that it yields” (ital-\nics his). Utilitarian philosopher Jeremy Bentham (1823) proposed the hedonic calculus for",
  "value of an item must not be based on its price, but rather on the utility that it yields” (ital-\nics his). Utilitarian philosopher Jeremy Bentham (1823) proposed the hedonic calculus for\nweighing “pleasures” and “pains,” arguing that all decisions (not just monetary ones) could\nbe reduced to utility comparisons.\nThe derivation of numerical utilities from preferences was ﬁrst carried out by Ram-\nsey (1931); the axioms for preference in the present text are closer in form to those rediscov-\nered in Theory of Games and Economic Behavior (von Neumann and Morgenstern, 1944).\nA good presentation of these axioms, in the course of a discussion on risk preference, is given\nby Howard (1977). Ramsey had derived subjective probabilities (not just utilities) from an\nagent’s preferences; Savage (1954) and Jeffrey (1983) carry out more recent constructions\nof this kind. Von Winterfeldt and Edwards (1986) provide a modern perspective on decision\nanalysis and its relationship to human preference structures. The micromort utility measure\nis discussed by Howard (1989). A 1994 survey by the Economist set the value of a life at\nbetween $750,000 and $2.6 million. However, Richard Thaler (1992) found irrational fram-\ning effects on the price one is willing to pay to avoid a risk of death versus the price one is\nwilling to be paid to accept a risk. For a 1/1000 chance, a respondent wouldn’t pay more\nthan $200 to remove the risk, but wouldn’t accept $50,000 to take on the risk. How much are\npeople willing to pay for a QALY? When it comes down to a speciﬁc case of saving oneself\nor a family member, the number is approximately “whatever I’ve got.” But we can ask at a\nsocietal level: suppose there is a vaccine that would yield X QALYs but costs Y dollars; is it\nworth it? In this case people report a wide range of values from around $10,000 to $150,000\nper QALY (Prades et al., 2008). QALYs are much more widely used in medical and social\npolicy decision making than are micromorts; see (Russell, 1990) for a typical example of an\nargument for a major change in public health policy on grounds of increased expected utility\nmeasured in QALYs.\nThe optimizer’s curse was brought to the attention of decision analysts in a forceful\nway by Smith and Winkler (2006), who pointed out that the ﬁnancial beneﬁts to the client\nprojected by analysts for their proposed course of action almost never materialized. They",
  "way by Smith and Winkler (2006), who pointed out that the ﬁnancial beneﬁts to the client\nprojected by analysts for their proposed course of action almost never materialized. They\ntrace this directly to the bias introduced by selecting an optimal action and show that a more\ncomplete Bayesian analysis eliminates the problem. The same underlying concept has been\ncalled post-decision disappointment by Harrison and March (1984) and was noted in the\nPOST-DECISION\nDISAPPOINTMENT\ncontext of analyzing capital investment projects by Brown (1974). The optimizer’s curse is\nalso closely related to the winner’s curse (Capen et al., 1971; Thaler, 1992), which applies\nWINNER’S CURSE\nto competitive bidding in auctions: whoever wins the auction is very likely to have overes-\ntimated the value of the object in question. Capen et al. quote a petroleum engineer on the 638\nChapter\n16.\nMaking Simple Decisions\ntopic of bidding for oil-drilling rights: “If one wins a tract against two or three others he may\nfeel ﬁne about his good fortune. But how should he feel if he won against 50 others? Ill.”\nFinally, behind both curses is the general phenomenon of regression to the mean, whereby\nREGRESSION TO THE\nMEAN\nindividuals selected on the basis of exceptional characteristics previously exhibited will, with\nhigh probability, become less exceptional in future.\nThe Allais paradox, due to Nobel Prize-winning economist Maurice Allais (1953) was\ntested experimentally (Tversky and Kahneman, 1982; Conlisk, 1989) to show that people\nare consistently inconsistent in their judgments. The Ellsberg paradox on ambiguity aver-\nsion was introduced in the Ph.D. thesis of Daniel Ellsberg (Ellsberg, 1962), who went on to\nbecome a military analyst at the RAND Corporation and to leak documents known as The\nPentagon Papers, which contributed to the end of the Vietnam war and the resignation of\nPresident Nixon. Fox and Tversky (1995) describe a further study of ambiguity aversion.\nMark Machina (2005) gives an overview of choice under uncertainty and how it can vary\nfrom expected utility theory.\nThere has been a recent outpouring of more-or-less popular books on human irrational-\nity. The best known is Predictably Irrational (Ariely, 2009); others include Sway (Brafman\nand Brafman, 2009), Nudge (Thaler and Sunstein, 2009), Kluge (Marcus, 2009), How We\nDecide (Lehrer, 2009) and On Being Certain (Burton, 2009). They complement the classic",
  "and Brafman, 2009), Nudge (Thaler and Sunstein, 2009), Kluge (Marcus, 2009), How We\nDecide (Lehrer, 2009) and On Being Certain (Burton, 2009). They complement the classic\n(Kahneman et al., 1982) and the article that started it all (Kahneman and Tversky, 1979).\nThe ﬁeld of evolutionary psychology (Buss, 2005), on the other hand, has run counter to this\nliterature, arguing that humans are quite rational in evolutionarily appropriate contexts. Its\nadherents point out that irrationality is penalized by deﬁnition in an evolutionary context and\nshow that in some cases it is an artifact of the experimental setup (Cummins and Allen, 1998).\nThere has been a recent resurgence of interest in Bayesian models of cognition, overturning\ndecades of pessimism (Oaksford and Chater, 1998; Elio, 2002; Chater and Oaksford, 2008).\nKeeney and Raiffa (1976) give a thorough introduction to multiattribute utility the-\nory. They describe early computer implementations of methods for eliciting the necessary\nparameters for a multiattribute utility function and include extensive accounts of real appli-\ncations of the theory. In AI, the principal reference for MAUT is Wellman’s (1985) paper,\nwhich includes a system called URP (Utility Reasoning Package) that can use a collection\nof statements about preference independence and conditional independence to analyze the\nstructure of decision problems. The use of stochastic dominance together with qualitative\nprobability models was investigated extensively by Wellman (1988, 1990a). Wellman and\nDoyle (1992) provide a preliminary sketch of how a complex set of utility-independence re-\nlationships might be used to provide a structured model of a utility function, in much the\nsame way that Bayesian networks provide a structured model of joint probability distribu-\ntions. Bacchus and Grove (1995, 1996) and La Mura and Shoham (1999) give further results\nalong these lines.\nDecision theory has been a standard tool in economics, ﬁnance, and management sci-\nence since the 1950s. Until the 1980s, decision trees were the main tool used for representing\nsimple decision problems. Smith (1988) gives an overview of the methodology of deci-\nsion analysis. Inﬂuence diagrams were introduced by Howard and Matheson (1984), based\non earlier work at SRI (Miller et al., 1976). Howard and Matheson’s method involved the Bibliographical and Historical Notes\n639\nderivation of a decision tree from a decision network, but in general the tree is of exponential",
  "on earlier work at SRI (Miller et al., 1976). Howard and Matheson’s method involved the Bibliographical and Historical Notes\n639\nderivation of a decision tree from a decision network, but in general the tree is of exponential\nsize. Shachter (1986) developed a method for making decisions based directly on a decision\nnetwork, without the creation of an intermediate decision tree. This algorithm was also one\nof the ﬁrst to provide complete inference for multiply connected Bayesian networks. Zhang\net al. (1994) showed how to take advantage of conditional independence of information to re-\nduce the size of trees in practice; they use the term decision network for networks that use this\napproach (although others use it as a synonym for inﬂuence diagram). Nilsson and Lauritzen\n(2000) link algorithms for decision networks to ongoing developments in clustering algo-\nrithms for Bayesian networks. Koller and Milch (2003) show how inﬂuence diagrams can be\nused to solve games that involve gathering information by opposing players, and Detwarasiti\nand Shachter (2005) show how inﬂuence diagrams can be used as an aid to decision making\nfor a team that shares goals but is unable to share all information perfectly. The collection\nby Oliver and Smith (1990) has a number of useful articles on decision networks, as does the\n1990 special issue of the journal Networks. Papers on decision networks and utility modeling\nalso appear regularly in the journals Management Science and Decision Analysis.\nThe theory of information value was explored ﬁrst in the context of statistical experi-\nments, where a quasi-utility (entropy reduction) was used (Lindley, 1956). The Russian con-\ntrol theorist Ruslan Stratonovich (1965) developed the more general theory presented here, in\nwhich information has value by virtue of its ability to affect decisions. Stratonovich’s work\nwas not known in the West, where Ron Howard (1966) pioneered the same idea. His paper\nends with the remark “If information value theory and associated decision theoretic structures\ndo not in the future occupy a large part of the education of engineers, then the engineering\nprofession will ﬁnd that its traditional role of managing scientiﬁc and economic resources for\nthe beneﬁt of man has been forfeited to another profession.” To date, the implied revolution\nin managerial methods has not occurred.\nRecent work by Krause and Guestrin (2009) shows that computing the exact non-",
  "the beneﬁt of man has been forfeited to another profession.” To date, the implied revolution\nin managerial methods has not occurred.\nRecent work by Krause and Guestrin (2009) shows that computing the exact non-\nmyopic value of information is intractable even in polytree networks. There are other cases—\nmore restricted than general value of information—in which the myopic algorithm does pro-\nvide a provably good approximation to the optimal sequence of observations (Krause et al.,\n2008). In some cases—for example, looking for treasure buried in one of n places—ranking\nexperiments in order of success probability divided by cost gives an optimal solution (Kadane\nand Simon, 1977).\nSurprisingly few early AI researchers adopted decision-theoretic tools after the early\napplications in medical decision making described in Chapter 13. One of the few exceptions\nwas Jerry Feldman, who applied decision theory to problems in vision (Feldman and Yaki-\nmovsky, 1974) and planning (Feldman and Sproull, 1977). After the resurgence of interest in\nprobabilistic methods in AI in the 1980s, decision-theoretic expert systems gained widespread\nacceptance (Horvitz et al., 1988; Cowell et al., 2002). In fact, from 1991 onward, the cover\ndesign of the journal Artiﬁcial Intelligence has depicted a decision network, although some\nartistic license appears to have been taken with the direction of the arrows. 640\nChapter\n16.\nMaking Simple Decisions\nEXERCISES\n16.1\n(Adapted from David Heckerman.) This exercise concerns the Almanac Game, which\nis used by decision analysts to calibrate numeric estimation. For each of the questions that\nfollow, give your best guess of the answer, that is, a number that you think is as likely to be\ntoo high as it is to be too low. Also give your guess at a 25th percentile estimate, that is, a\nnumber that you think has a 25% chance of being too high, and a 75% chance of being too\nlow. Do the same for the 75th percentile. (Thus, you should give three estimates in all—low,\nmedian, and high—for each question.)\na. Number of passengers who ﬂew between New York and Los Angeles in 1989.\nb. Population of Warsaw in 1992.\nc. Year in which Coronado discovered the Mississippi River.\nd. Number of votes received by Jimmy Carter in the 1976 presidential election.\ne. Age of the oldest living tree, as of 2002.\nf. Height of the Hoover Dam in feet.\ng. Number of eggs produced in Oregon in 1985.\nh. Number of Buddhists in the world in 1992.",
  "e. Age of the oldest living tree, as of 2002.\nf. Height of the Hoover Dam in feet.\ng. Number of eggs produced in Oregon in 1985.\nh. Number of Buddhists in the world in 1992.\ni. Number of deaths due to AIDS in the United States in 1981.\nj. Number of U.S. patents granted in 1901.\nThe correct answers appear after the last exercise of this chapter. From the point of view of\ndecision analysis, the interesting thing is not how close your median guesses came to the real\nanswers, but rather how often the real answer came within your 25% and 75% bounds. If it\nwas about half the time, then your bounds are accurate. But if you’re like most people, you\nwill be more sure of yourself than you should be, and fewer than half the answers will fall\nwithin the bounds. With practice, you can calibrate yourself to give realistic bounds, and thus\nbe more useful in supplying information for decision making. Try this second set of questions\nand see if there is any improvement:\na. Year of birth of Zsa Zsa Gabor.\nb. Maximum distance from Mars to the sun in miles.\nc. Value in dollars of exports of wheat from the United States in 1992.\nd. Tons handled by the port of Honolulu in 1991.\ne. Annual salary in dollars of the governor of California in 1993.\nf. Population of San Diego in 1990.\ng. Year in which Roger Williams founded Providence, Rhode Island.\nh. Height of Mt. Kilimanjaro in feet.\ni. Length of the Brooklyn Bridge in feet.\nj. Number of deaths due to automobile accidents in the United States in 1992. Exercises\n641\n16.2\nChris considers four used cars before buying the one with maximum expected utility.\nPat considers ten cars and does the same. All other things being equal, which one is more\nlikely to have the better car? Which is more likely to be disappointed with their car’s quality?\nBy how much (in terms of standard deviations of expected quality)?\n16.3\nIn 1713, Nicolas Bernoulli stated a puzzle, now called the St. Petersburg paradox,\nwhich works as follows. You have the opportunity to play a game in which a fair coin is\ntossed repeatedly until it comes up heads. If the ﬁrst heads appears on the nth toss, you win\n2n dollars.\na. Show that the expected monetary value of this game is inﬁnite.\nb. How much would you, personally, pay to play the game?\nc. Nicolas’s cousin Daniel Bernoulli resolved the apparent paradox in 1738 by suggesting\nthat the utility of money is measured on a logarithmic scale (i.e., U(Sn) = a log2 n+b,",
  "b. How much would you, personally, pay to play the game?\nc. Nicolas’s cousin Daniel Bernoulli resolved the apparent paradox in 1738 by suggesting\nthat the utility of money is measured on a logarithmic scale (i.e., U(Sn) = a log2 n+b,\nwhere Sn is the state of having $n). What is the expected utility of the game under this\nassumption?\nd. What is the maximum amount that it would be rational to pay to play the game, assum-\ning that one’s initial wealth is $k ?\n16.4\nWrite a computer program to automate the process in Exercise 16.9. Try your pro-\ngram out on several people of different net worth and political outlook. Comment on the\nconsistency of your results, both for an individual and across individuals.\n16.5\nThe Surprise Candy Company makes candy in two ﬂavors: 70% are strawberry ﬂa-\nvor and 30% are anchovy ﬂavor. Each new piece of candy starts out with a round shape;\nas it moves along the production line, a machine randomly selects a certain percentage to\nbe trimmed into a square; then, each piece is wrapped in a wrapper whose color is chosen\nrandomly to be red or brown. 80% of the strawberry candies are round and 80% have a red\nwrapper, while 90% of the anchovy candies are square and 90% have a brown wrapper. All\ncandies are sold individually in sealed, identical, black boxes.\nNow you, the customer, have just bought a Surprise candy at the store but have not yet\nopened the box. Consider the three Bayes nets in Figure 16.11.\n(i)\n(ii)\n(iii)\nFlavor\nWrapper\nShape\nWrapper\nShape\nFlavor\nWrapper\nShape\nFlavor\nFigure 16.11\nThree proposed Bayes nets for the Surprise Candy problem, Exercise 16.5.\na. Which network(s) can correctly represent P(Flavor, Wrapper, Shape)?\nb. Which network is the best representation for this problem? 642\nChapter\n16.\nMaking Simple Decisions\nc. Does network (i) assert that P(Wrapper|Shape) = P(Wrapper)?\nd. What is the probability that your candy has a red wrapper?\ne. In the box is a round candy with a red wrapper. What is the probability that its ﬂavor is\nstrawberry?\nf. A unwrapped strawberry candy is worth s on the open market and an unwrapped an-\nchovy candy is worth a. Write an expression for the value of an unopened candy box.\ng. A new law prohibits trading of unwrapped candies, but it is still legal to trade wrapped\ncandies (out of the box). Is an unopened candy box now worth more than less than, or\nthe same as before?\n16.6\nProve that the judgments B ≻A and C ≻D in the Allais paradox (page 620) violate\nthe axiom of substitutability.\n16.7",
  "candies (out of the box). Is an unopened candy box now worth more than less than, or\nthe same as before?\n16.6\nProve that the judgments B ≻A and C ≻D in the Allais paradox (page 620) violate\nthe axiom of substitutability.\n16.7\nConsider the Allais paradox described on page 620: an agent who prefers B over\nA (taking the sure thing), and C over D (taking the higher EMV) is not acting rationally,\naccording to utility theory. Do you think this indicates a problem for the agent, a problem for\nthe theory, or no problem at all? Explain.\n16.8\nTickets to a lottery cost $1. There are two possible prizes: a $10 payoff with probabil-\nity 1/50, and a $1,000,000 payoff with probability 1/2,000,000. What is the expected mone-\ntary value of a lottery ticket? When (if ever) is it rational to buy a ticket? Be precise—show an\nequation involving utilities. You may assume current wealth of $k and that U(Sk) = 0. You\nmay also assume that U(Sk+10) = 10 × U(Sk+1), but you may not make any assumptions\nabout U(Sk+1,000,000). Sociological studies show that people with lower income buy a dis-\nproportionate number of lottery tickets. Do you think this is because they are worse decision\nmakers or because they have a different utility function? Consider the value of contemplating\nthe possibility of winning the lottery versus the value of contemplating becoming an action\nhero while watching an adventure movie.\n16.9\nAssess your own utility for different incremental amounts of money by running a series\nof preference tests between some deﬁnite amount M1 and a lottery [p, M2; (1−p), 0]. Choose\ndifferent values of M1 and M2, and vary p until you are indifferent between the two choices.\nPlot the resulting utility function.\n16.10\nHow much is a micromort worth to you? Devise a protocol to determine this. Ask\nquestions based both on paying to avoid risk and being paid to accept risk.\n16.11\nLet continuous variables X1, . . . , Xk be independently distributed according to the\nsame probability density function f(x). Prove that the density function for max{X1, . . . , Xk}\nis given by kf(x)(F(x))k−1, where F is the cumulative distribution for f.\n16.12\nEconomists often make use of an exponential utility function for money: U(x) =\n−ex/R, where R is a positive constant representing an individual’s risk tolerance. Risk toler-\nance reﬂects how likely an individual is to accept a lottery with a particular expected monetary",
  "−ex/R, where R is a positive constant representing an individual’s risk tolerance. Risk toler-\nance reﬂects how likely an individual is to accept a lottery with a particular expected monetary\nvalue (EMV) versus some certain payoff. As R (which is measured in the same units as x)\nbecomes larger, the individual becomes less risk-averse. Exercises\n643\na. Assume Mary has an exponential utility function with R = $500. Mary is given the\nchoice between receiving $500 with certainty (probability 1) or participating in a lot-\ntery which has a 60% probability of winning $5000 and a 40% probability of winning\nnothing. Assuming Marry acts rationally, which option would she choose? Show how\nyou derived your answer.\nb. Consider the choice between receiving $100 with certainty (probability 1) or participat-\ning in a lottery which has a 50% probability of winning $500 and a 50% probability of\nwinning nothing. Approximate the value of R (to 3 signiﬁcant digits) in an exponential\nutility function that would cause an individual to be indifferent to these two alternatives.\n(You might ﬁnd it helpful to write a short program to help you solve this problem.)\n16.13\nRepeat Exercise 16.16, using the action-utility representation shown in Figure 16.7.\n16.14\nFor either of the airport-siting diagrams from Exercises 16.16 and 16.13, to which\nconditional probability table entry is the utility most sensitive, given the available evidence?\n16.15\nConsider a student who has the choice to buy or not buy a textbook for a course. We’ll\nmodel this as a decision problem with one Boolean decision node, B, indicating whether the\nagent chooses to buy the book, and two Boolean chance nodes, M, indicating whether the\nstudent has mastered the material in the book, and P, indicating whether the student passes\nthe course. Of course, there is also a utility node, U. A certain student, Sam, has an additive\nutility function: 0 for not buying the book and -$100 for buying it; and $2000 for passing the\ncourse and 0 for not passing. Sam’s conditional probability estimates are as follows:\nP(p|b, m) = 0.9\nP(m|b) = 0.9\nP(p|b, ¬m) = 0.5\nP(m|¬b) = 0.7\nP(p|¬b, m) = 0.8\nP(p|¬b, ¬m) = 0.3\nYou might think that P would be independent of B given M, But this course has an open-\nbook ﬁnal—so having the book helps.\na. Draw the decision network for this problem.\nb. Compute the expected utility of buying the book and of not buying it.\nc. What should Sam do?\n16.16",
  "book ﬁnal—so having the book helps.\na. Draw the decision network for this problem.\nb. Compute the expected utility of buying the book and of not buying it.\nc. What should Sam do?\n16.16\nThis exercise completes the analysis of the airport-siting problem in Figure 16.6.\na. Provide reasonable variable domains, probabilities, and utilities for the network, assum-\ning that there are three possible sites.\nb. Solve the decision problem.\nc. What happens if changes in technology mean that each aircraft generates half the noise?\nd. What if noise avoidance becomes three times more important?\ne. Calculate the VPI for AirTraﬃc, Litigation, and Construction in your model. 644\nChapter\n16.\nMaking Simple Decisions\n16.17\n(Adapted from Pearl (1988).) A used-car buyer can decide to carry out various tests\nwith various costs (e.g., kick the tires, take the car to a qualiﬁed mechanic) and then, depend-\ning on the outcome of the tests, decide which car to buy. We will assume that the buyer is\ndeciding whether to buy car c1, that there is time to carry out at most one test, and that t1 is\nthe test of c1 and costs $50.\nA car can be in good shape (quality q+) or bad shape (quality q−), and the tests might\nhelp indicate what shape the car is in. Car c1 costs $1,500, and its market value is $2,000 if it\nis in good shape; if not, $700 in repairs will be needed to make it in good shape. The buyer’s\nestimate is that c1 has a 70% chance of being in good shape.\na. Draw the decision network that represents this problem.\nb. Calculate the expected net gain from buying c1, given no test.\nc. Tests can be described by the probability that the car will pass or fail the test given that\nthe car is in good or bad shape. We have the following information:\nP(pass(c1, t1)|q+(c1)) = 0.8\nP(pass(c1, t1)|q−(c1)) = 0.35\nUse Bayes’ theorem to calculate the probability that the car will pass (or fail) its test and\nhence the probability that it is in good (or bad) shape given each possible test outcome.\nd. Calculate the optimal decisions given either a pass or a fail, and their expected utilities.\ne. Calculate the value of information of the test, and derive an optimal conditional plan\nfor the buyer.\n16.18\nRecall the deﬁnition of value of information in Section 16.6.\na. Prove that the value of information is nonnegative and order independent.\nb. Explain why it is that some people would prefer not to get some information—for ex-\nample, not wanting to know the sex of their baby when an ultrasound is done.",
  "a. Prove that the value of information is nonnegative and order independent.\nb. Explain why it is that some people would prefer not to get some information—for ex-\nample, not wanting to know the sex of their baby when an ultrasound is done.\nc. A function f on sets is submodular if, for any element x and any sets A and B such\nSUBMODULARITY\nthat A ⊆B, adding x to A gives a greater increase in f than adding x to B:\nA ⊆B ⇒(f(A ∪{x}) −f(A)) ≥(f(B ∪{x}) −f(B)) .\nSubmodularity captures the intuitive notion of diminishing returns. Is the value of in-\nformation, viewed as a function f on sets of possible observations, submodular? Prove\nthis or ﬁnd a counterexample.\nThe answers to Exercise 16.1 (where M stands for million): First set: 3M, 1.6M, 1541, 41M,\n4768, 221, 649M, 295M, 132, 25,546. Second set: 1917, 155M, 4,500M, 11M, 120,000,\n1.1M, 1636, 19,340, 1,595, 41,710. 17\nMAKING COMPLEX\nDECISIONS\nIn which we examine methods for deciding what to do today, given that we may\ndecide again tomorrow.\nIn this chapter, we address the computational issues involved in making decisions in a stochas-\ntic environment. Whereas Chapter 16 was concerned with one-shot or episodic decision\nproblems, in which the utility of each action’s outcome was well known, we are concerned\nhere with sequential decision problems, in which the agent’s utility depends on a sequence\nSEQUENTIAL\nDECISION PROBLEM\nof decisions. Sequential decision problems incorporate utilities, uncertainty, and sensing,\nand include search and planning problems as special cases. Section 17.1 explains how se-\nquential decision problems are deﬁned, and Sections 17.2 and 17.3 explain how they can\nbe solved to produce optimal behavior that balances the risks and rewards of acting in an\nuncertain environment. Section 17.4 extends these ideas to the case of partially observable\nenvironments, and Section 17.4.3 develops a complete design for decision-theoretic agents in\npartially observable environments, combining dynamic Bayesian networks from Chapter 15\nwith decision networks from Chapter 16.\nThe second part of the chapter covers environments with multiple agents. In such en-\nvironments, the notion of optimal behavior is complicated by the interactions among the\nagents. Section 17.5 introduces the main ideas of game theory, including the idea that ra-\ntional agents might need to behave randomly. Section 17.6 looks at how multiagent systems\ncan be designed so that multiple agents can achieve a common goal.\n17.1",
  "tional agents might need to behave randomly. Section 17.6 looks at how multiagent systems\ncan be designed so that multiple agents can achieve a common goal.\n17.1\nSEQUENTIAL DECISION PROBLEMS\nSuppose that an agent is situated in the 4 × 3 environment shown in Figure 17.1(a). Beginning\nin the start state, it must choose an action at each time step. The interaction with the environ-\nment terminates when the agent reaches one of the goal states, marked +1 or –1. Just as for\nsearch problems, the actions available to the agent in each state are given by ACTIONS(s),\nsometimes abbreviated to A(s); in the 4 × 3 environment, the actions in every state are Up,\nDown, Left, and Right. We assume for now that the environment is fully observable, so that\nthe agent always knows where it is.\n645 646\nChapter\n17.\nMaking Complex Decisions\n1\n2\n3\n1\n2\n3\n4\nSTART\n0.8\n0.1\n0.1\n(a)\n(b)\n–1\n+ 1\nFigure 17.1\n(a) A simple 4 ×3 environment that presents the agent with a sequential\ndecision problem. (b) Illustration of the transition model of the environment: the “intended”\noutcome occurs with probability 0.8, but with probability 0.2 the agent moves at right angles\nto the intended direction. A collision with a wall results in no movement. The two terminal\nstates have reward +1 and –1, respectively, and all other states have a reward of –0.04.\nIf the environment were deterministic, a solution would be easy: [Up, Up, Right, Right,\nRight]. Unfortunately, the environment won’t always go along with this solution, because the\nactions are unreliable. The particular model of stochastic motion that we adopt is illustrated\nin Figure 17.1(b). Each action achieves the intended effect with probability 0.8, but the rest\nof the time, the action moves the agent at right angles to the intended direction. Furthermore,\nif the agent bumps into a wall, it stays in the same square. For example, from the start square\n(1,1), the action Up moves the agent to (1,2) with probability 0.8, but with probability 0.1, it\nmoves right to (2,1), and with probability 0.1, it moves left, bumps into the wall, and stays in\n(1,1). In such an environment, the sequence [Up, Up, Right, Right, Right] goes up around\nthe barrier and reaches the goal state at (4,3) with probability 0.85 = 0.32768. There is also a\nsmall chance of accidentally reaching the goal by going the other way around with probability\n0.14 × 0.8, for a grand total of 0.32776. (See also Exercise 17.1.)",
  "small chance of accidentally reaching the goal by going the other way around with probability\n0.14 × 0.8, for a grand total of 0.32776. (See also Exercise 17.1.)\nAs in Chapter 3, the transition model (or just “model,” whenever no confusion can\narise) describes the outcome of each action in each state. Here, the outcome is stochastic,\nso we write P(s′ | s, a) to denote the probability of reaching state s′ if action a is done in\nstate s. We will assume that transitions are Markovian in the sense of Chapter 15, that is, the\nprobability of reaching s′ from s depends only on s and not on the history of earlier states. For\nnow, you can think of P(s′ | s, a) as a big three-dimensional table containing probabilities.\nLater, in Section 17.4.3, we will see that the transition model can be represented as a dynamic\nBayesian network, just as in Chapter 15.\nTo complete the deﬁnition of the task environment, we must specify the utility function\nfor the agent. Because the decision problem is sequential, the utility function will depend\non a sequence of states—an environment history—rather than on a single state. Later in\nthis section, we investigate how such utility functions can be speciﬁed in general; for now,\nwe simply stipulate that in each state s, the agent receives a reward R(s), which may be\nREWARD\npositive or negative, but must be bounded. For our particular example, the reward is −0.04\nin all states except the terminal states (which have rewards +1 and –1). The utility of an Section 17.1.\nSequential Decision Problems\n647\nenvironment history is just (for now) the sum of the rewards received. For example, if the\nagent reaches the +1 state after 10 steps, its total utility will be 0.6. The negative reward of\n–0.04 gives the agent an incentive to reach (4,3) quickly, so our environment is a stochastic\ngeneralization of the search problems of Chapter 3. Another way of saying this is that the\nagent does not enjoy living in this environment and so wants to leave as soon as possible.\nTo sum up: a sequential decision problem for a fully observable, stochastic environment\nwith a Markovian transition model and additive rewards is called a Markov decision process,\nMARKOV DECISION\nPROCESS\nor MDP, and consists of a set of states (with an initial state s0); a set ACTIONS(s) of actions\nin each state; a transition model P(s′ | s, a); and a reward function R(s).1\nThe next question is, what does a solution to the problem look like? We have seen that",
  "in each state; a transition model P(s′ | s, a); and a reward function R(s).1\nThe next question is, what does a solution to the problem look like? We have seen that\nany ﬁxed action sequence won’t solve the problem, because the agent might end up in a state\nother than the goal. Therefore, a solution must specify what the agent should do for any state\nthat the agent might reach. A solution of this kind is called a policy. It is traditional to denote\nPOLICY\na policy by π, and π(s) is the action recommended by the policy π for state s. If the agent\nhas a complete policy, then no matter what the outcome of any action, the agent will always\nknow what to do next.\nEach time a given policy is executed starting from the initial state, the stochastic nature\nof the environment may lead to a different environment history. The quality of a policy is\ntherefore measured by the expected utility of the possible environment histories generated\nby that policy. An optimal policy is a policy that yields the highest expected utility. We\nOPTIMAL POLICY\nuse π∗to denote an optimal policy. Given π∗, the agent decides what to do by consulting\nits current percept, which tells it the current state s, and then executing the action π∗(s). A\npolicy represents the agent function explicitly and is therefore a description of a simple reﬂex\nagent, computed from the information used for a utility-based agent.\nAn optimal policy for the world of Figure 17.1 is shown in Figure 17.2(a). Notice\nthat, because the cost of taking a step is fairly small compared with the penalty for ending\nup in (4,2) by accident, the optimal policy for the state (3,1) is conservative. The policy\nrecommends taking the long way round, rather than taking the shortcut and thereby risking\nentering (4,2).\nThe balance of risk and reward changes depending on the value of R(s) for the nonter-\nminal states. Figure 17.2(b) shows optimal policies for four different ranges of R(s). When\nR(s) ≤−1.6284, life is so painful that the agent heads straight for the nearest exit, even if\nthe exit is worth –1. When −0.4278 ≤R(s) ≤−0.0850, life is quite unpleasant; the agent\ntakes the shortest route to the +1 state and is willing to risk falling into the –1 state by acci-\ndent. In particular, the agent takes the shortcut from (3,1). When life is only slightly dreary\n(−0.0221 < R(s) < 0), the optimal policy takes no risks at all. In (4,1) and (3,2), the agent",
  "dent. In particular, the agent takes the shortcut from (3,1). When life is only slightly dreary\n(−0.0221 < R(s) < 0), the optimal policy takes no risks at all. In (4,1) and (3,2), the agent\nheads directly away from the –1 state so that it cannot fall in by accident, even though this\nmeans banging its head against the wall quite a few times. Finally, if R(s) > 0, then life is\npositively enjoyable and the agent avoids both exits. As long as the actions in (4,1), (3,2),\n1 Some deﬁnitions of MDPs allow the reward to depend on the action and outcome too, so the reward function\nis R(s, a, s′). This simpliﬁes the description of some environments but does not change the problem in any\nfundamental way, as shown in Exercise 17.4. 648\nChapter\n17.\nMaking Complex Decisions\n1\n2\n3\n1\n2\n3\n+ 1\n–1\n4\n–1\n+1\n R(s) < –1.6284\n(a)\n(b)\n– 0.0221 < R(s) < 0 \n–1\n+1\n–1\n+1\n–1\n+1\nR(s) > 0 \n– 0.4278 < R(s) < – 0.0850\nFigure 17.2\n(a) An optimal policy for the stochastic environment with R(s) = −0.04 in\nthe nonterminal states. (b) Optimal policies for four different ranges of R(s).\nand (3,3) are as shown, every policy is optimal, and the agent obtains inﬁnite total reward be-\ncause it never enters a terminal state. Surprisingly, it turns out that there are six other optimal\npolicies for various ranges of R(s); Exercise 17.5 asks you to ﬁnd them.\nThe careful balancing of risk and reward is a characteristic of MDPs that does not\narise in deterministic search problems; moreover, it is a characteristic of many real-world\ndecision problems. For this reason, MDPs have been studied in several ﬁelds, including\nAI, operations research, economics, and control theory. Dozens of algorithms have been\nproposed for calculating optimal policies. In sections 17.2 and 17.3 we describe two of the\nmost important algorithm families. First, however, we must complete our investigation of\nutilities and policies for sequential decision problems.\n17.1.1\nUtilities over time\nIn the MDP example in Figure 17.1, the performance of the agent was measured by a sum of\nrewards for the states visited. This choice of performance measure is not arbitrary, but it is\nnot the only possibility for the utility function on environment histories, which we write as\nUh([s0, s1, . . . , sn]). Our analysis draws on multiattribute utility theory (Section 16.4) and\nis somewhat technical; the impatient reader may wish to skip to the next section.\nThe ﬁrst question to answer is whether there is a ﬁnite horizon or an inﬁnite horizon",
  "is somewhat technical; the impatient reader may wish to skip to the next section.\nThe ﬁrst question to answer is whether there is a ﬁnite horizon or an inﬁnite horizon\nFINITE HORIZON\nINFINITE HORIZON\nfor decision making. A ﬁnite horizon means that there is a ﬁxed time N after which nothing\nmatters—the game is over, so to speak. Thus, Uh([s0, s1, . . . , sN+k]) = Uh([s0, s1, . . . , sN])\nfor all k > 0. For example, suppose an agent starts at (3,1) in the 4 × 3 world of Figure 17.1,\nand suppose that N = 3. Then, to have any chance of reaching the +1 state, the agent must\nhead directly for it, and the optimal action is to go Up. On the other hand, if N = 100,\nthen there is plenty of time to take the safe route by going Left. So, with a ﬁnite horizon, Section 17.1.\nSequential Decision Problems\n649\nthe optimal action in a given state could change over time. We say that the optimal policy\nfor a ﬁnite horizon is nonstationary. With no ﬁxed time limit, on the other hand, there is\nNONSTATIONARY\nPOLICY\nno reason to behave differently in the same state at different times. Hence, the optimal ac-\ntion depends only on the current state, and the optimal policy is stationary. Policies for the\nSTATIONARY POLICY\ninﬁnite-horizon case are therefore simpler than those for the ﬁnite-horizon case, and we deal\nmainly with the inﬁnite-horizon case in this chapter. (We will see later that for partially ob-\nservable environments, the inﬁnite-horizon case is not so simple.) Note that “inﬁnite horizon”\ndoes not necessarily mean that all state sequences are inﬁnite; it just means that there is no\nﬁxed deadline. In particular, there can be ﬁnite state sequences in an inﬁnite-horizon MDP\ncontaining a terminal state.\nThe next question we must decide is how to calculate the utility of state sequences. In\nthe terminology of multiattribute utility theory, each state si can be viewed as an attribute of\nthe state sequence [s0, s1, s2 . . .]. To obtain a simple expression in terms of the attributes, we\nwill need to make some sort of preference-independence assumption. The most natural as-\nsumption is that the agent’s preferences between state sequences are stationary. Stationarity\nSTATIONARY\nPREFERENCE\nfor preferences means the following: if two state sequences [s0, s1, s2, . . .] and [s′\n0, s′\n1, s′\n2, . . .]\nbegin with the same state (i.e., s0 = s′\n0), then the two sequences should be preference-ordered\nthe same way as the sequences [s1, s2, . . .] and [s′\n1, s′",
  "0, s′\n1, s′\n2, . . .]\nbegin with the same state (i.e., s0 = s′\n0), then the two sequences should be preference-ordered\nthe same way as the sequences [s1, s2, . . .] and [s′\n1, s′\n2, . . .]. In English, this means that if you\nprefer one future to another starting tomorrow, then you should still prefer that future if it\nwere to start today instead. Stationarity is a fairly innocuous-looking assumption with very\nstrong consequences: it turns out that under stationarity there are just two coherent ways to\nassign utilities to sequences:\n1. Additive rewards: The utility of a state sequence is\nADDITIVE REWARD\nUh([s0, s1, s2, . . .]) = R(s0) + R(s1) + R(s2) + · · · .\nThe 4 × 3 world in Figure 17.1 uses additive rewards. Notice that additivity was used\nimplicitly in our use of path cost functions in heuristic search algorithms (Chapter 3).\n2. Discounted rewards: The utility of a state sequence is\nDISCOUNTED\nREWARD\nUh([s0, s1, s2, . . .]) = R(s0) + γR(s1) + γ2R(s2) + · · · ,\nwhere the discount factor γ is a number between 0 and 1. The discount factor describes\nDISCOUNT FACTOR\nthe preference of an agent for current rewards over future rewards. When γ is close\nto 0, rewards in the distant future are viewed as insigniﬁcant. When γ is 1, discounted\nrewards are exactly equivalent to additive rewards, so additive rewards are a special\ncase of discounted rewards. Discounting appears to be a good model of both animal\nand human preferences over time. A discount factor of γ is equivalent to an interest rate\nof (1/γ) −1.\nFor reasons that will shortly become clear, we assume discounted rewards in the remainder\nof the chapter, although sometimes we allow γ = 1.\nLurking beneath our choice of inﬁnite horizons is a problem: if the environment does\nnot contain a terminal state, or if the agent never reaches one, then all environment histories\nwill be inﬁnitely long, and utilities with additive, undiscounted rewards will generally be 650\nChapter\n17.\nMaking Complex Decisions\ninﬁnite. While we can agree that+∞is better than −∞, comparing two state sequences with\n+∞utility is more difﬁcult. There are three solutions, two of which we have seen already:\n1. With discounted rewards, the utility of an inﬁnite sequence is ﬁnite. In fact, if γ < 1\nand rewards are bounded by ±Rmax, we have\nUh([s0, s1, s2, . . .]) =\n∞\n\f\nt = 0\nγtR(st) ≤\n∞\n\f\nt = 0\nγtRmax = Rmax/(1 −γ) ,\n(17.1)\nusing the standard formula for the sum of an inﬁnite geometric series.",
  "and rewards are bounded by ±Rmax, we have\nUh([s0, s1, s2, . . .]) =\n∞\n\f\nt = 0\nγtR(st) ≤\n∞\n\f\nt = 0\nγtRmax = Rmax/(1 −γ) ,\n(17.1)\nusing the standard formula for the sum of an inﬁnite geometric series.\n2. If the environment contains terminal states and if the agent is guaranteed to get to one\neventually, then we will never need to compare inﬁnite sequences. A policy that is\nguaranteed to reach a terminal state is called a proper policy. With proper policies, we\nPROPER POLICY\ncan use γ = 1 (i.e., additive rewards). The ﬁrst three policies shown in Figure 17.2(b)\nare proper, but the fourth is improper. It gains inﬁnite total reward by staying away from\nthe terminal states when the reward for the nonterminal states is positive. The existence\nof improper policies can cause the standard algorithms for solving MDPs to fail with\nadditive rewards, and so provides a good reason for using discounted rewards.\n3. Inﬁnite sequences can be compared in terms of the average reward obtained per time\nAVERAGE REWARD\nstep. Suppose that square (1,1) in the 4 × 3 world has a reward of 0.1 while the other\nnonterminal states have a reward of 0.01. Then a policy that does its best to stay in\n(1,1) will have higher average reward than one that stays elsewhere. Average reward is\na useful criterion for some problems, but the analysis of average-reward algorithms is\nbeyond the scope of this book.\nIn sum, discounted rewards present the fewest difﬁculties in evaluating state sequences.\n17.1.2\nOptimal policies and the utilities of states\nHaving decided that the utility of a given state sequence is the sum of discounted rewards\nobtained during the sequence, we can compare policies by comparing the expected utilities\nobtained when executing them. We assume the agent is in some initial state s and deﬁne St\n(a random variable) to be the state the agent reaches at time t when executing a particular\npolicy π. (Obviously, S0 = s, the state the agent is in now.) The probability distribution over\nstate sequences S1, S2, . . . , is determined by the initial state s, the policy π, and the transition\nmodel for the environment.\nThe expected utility obtained by executing π starting in s is given by\nUπ(s) = E\n\" ∞\n\f\nt = 0\nγtR(St)\n#\n,\n(17.2)\nwhere the expectation is with respect to the probability distribution over state sequences de-\ntermined by s and π. Now, out of all the policies the agent could choose to execute starting in",
  "Uπ(s) = E\n\" ∞\n\f\nt = 0\nγtR(St)\n#\n,\n(17.2)\nwhere the expectation is with respect to the probability distribution over state sequences de-\ntermined by s and π. Now, out of all the policies the agent could choose to execute starting in\ns, one (or more) will have higher expected utilities than all the others. We’ll use π∗\ns to denote\none of these policies:\nπ∗\ns = argmax\nπ\nUπ(s) .\n(17.3) Section 17.1.\nSequential Decision Problems\n651\nRemember that π∗\ns is a policy, so it recommends an action for every state; its connection\nwith s in particular is that it’s an optimal policy when s is the starting state. A remarkable\nconsequence of using discounted utilities with inﬁnite horizons is that the optimal policy is\nindependent of the starting state. (Of course, the action sequence won’t be independent;\nremember that a policy is a function specifying an action for each state.) This fact seems\nintuitively obvious: if policy π∗\na is optimal starting in a and policy π∗\nb is optimal starting in b,\nthen, when they reach a third state c, there’s no good reason for them to disagree with each\nother, or with π∗\nc, about what to do next.2 So we can simply write π∗for an optimal policy.\nGiven this deﬁnition, the true utility of a state is just U π∗(s)—that is, the expected\nsum of discounted rewards if the agent executes an optimal policy. We write this as U(s),\nmatching the notation used in Chapter 16 for the utility of an outcome. Notice that U(s) and\nR(s) are quite different quantities; R(s) is the “short term” reward for being in s, whereas\nU(s) is the “long term” total reward from s onward. Figure 17.3 shows the utilities for the\n4 × 3 world. Notice that the utilities are higher for states closer to the +1 exit, because fewer\nsteps are required to reach the exit.\n1\n2\n3\n1\n2\n3\n–1\n+ 1\n4\n0.611\n0.812\n0.655\n0.762\n0.918\n0.705\n0.660\n0.868\n 0.388\nFigure 17.3\nThe utilities of the states in the 4 × 3 world, calculated with γ = 1 and\nR(s) = −0.04 for nonterminal states.\nThe utility function U(s) allows the agent to select actions by using the principle of\nmaximum expected utility from Chapter 16—that is, choose the action that maximizes the\nexpected utility of the subsequent state:\nπ∗(s) = argmax\na∈A(s)\n\f\ns′\nP(s′ | s, a)U(s′) .\n(17.4)\nThe next two sections describe algorithms for ﬁnding optimal policies.\n2 Although this seems obvious, it does not hold for ﬁnite-horizon policies or for other ways of combining",
  "π∗(s) = argmax\na∈A(s)\n\f\ns′\nP(s′ | s, a)U(s′) .\n(17.4)\nThe next two sections describe algorithms for ﬁnding optimal policies.\n2 Although this seems obvious, it does not hold for ﬁnite-horizon policies or for other ways of combining\nrewards over time. The proof follows directly from the uniqueness of the utility function on states, as shown in\nSection 17.2. 652\nChapter\n17.\nMaking Complex Decisions\n17.2\nVALUE ITERATION\nIn this section, we present an algorithm, called value iteration, for calculating an optimal\nVALUE ITERATION\npolicy. The basic idea is to calculate the utility of each state and then use the state utilities to\nselect an optimal action in each state.\n17.2.1\nThe Bellman equation for utilities\nSection 17.1.2 deﬁned the utility of being in a state as the expected sum of discounted rewards\nfrom that point onwards. From this, it follows that there is a direct relationship between the\nutility of a state and the utility of its neighbors: the utility of a state is the immediate reward\nfor that state plus the expected discounted utility of the next state, assuming that the agent\nchooses the optimal action. That is, the utility of a state is given by\nU(s) = R(s) + γ max\na∈A(s)\n\f\ns′\nP(s′ | s, a)U(s′) .\n(17.5)\nThis is called the Bellman equation, after Richard Bellman (1957). The utilities of the\nBELLMAN EQUATION\nstates—deﬁned by Equation (17.2) as the expected utility of subsequent state sequences—are\nsolutions of the set of Bellman equations. In fact, they are the unique solutions, as we show\nin Section 17.2.3.\nLet us look at one of the Bellman equations for the 4 × 3 world. The equation for the\nstate (1,1) is\nU(1, 1) = −0.04 + γ max[ 0.8U(1, 2) + 0.1U(2, 1) + 0.1U(1, 1),\n(Up)\n0.9U(1, 1) + 0.1U(1, 2),\n(Left)\n0.9U(1, 1) + 0.1U(2, 1),\n(Down)\n0.8U(2, 1) + 0.1U(1, 2) + 0.1U(1, 1) ].\n(Right)\nWhen we plug in the numbers from Figure 17.3, we ﬁnd that Up is the best action.\n17.2.2\nThe value iteration algorithm\nThe Bellman equation is the basis of the value iteration algorithm for solving MDPs. If there\nare n possible states, then there are n Bellman equations, one for each state. The n equations\ncontain n unknowns—the utilities of the states. So we would like to solve these simultaneous\nequations to ﬁnd the utilities. There is one problem: the equations are nonlinear, because the\n“max” operator is not a linear operator. Whereas systems of linear equations can be solved\nquickly using linear algebra techniques, systems of nonlinear equations are more problematic.",
  "“max” operator is not a linear operator. Whereas systems of linear equations can be solved\nquickly using linear algebra techniques, systems of nonlinear equations are more problematic.\nOne thing to try is an iterative approach. We start with arbitrary initial values for the utilities,\ncalculate the right-hand side of the equation, and plug it into the left-hand side—thereby\nupdating the utility of each state from the utilities of its neighbors. We repeat this until we\nreach an equilibrium. Let Ui(s) be the utility value for state s at the ith iteration. The iteration\nstep, called a Bellman update, looks like this:\nBELLMAN UPDATE\nUi+1(s) ←R(s) + γ max\na∈A(s)\n\f\ns′\nP(s′ | s, a)Ui(s′) ,\n(17.6) Section 17.2.\nValue Iteration\n653\nfunction VALUE-ITERATION(mdp,ϵ) returns a utility function\ninputs: mdp, an MDP with states S, actions A(s), transition model P(s′ | s, a),\nrewards R(s), discount γ\nϵ, the maximum error allowed in the utility of any state\nlocal variables: U , U ′, vectors of utilities for states in S, initially zero\nδ, the maximum change in the utility of any state in an iteration\nrepeat\nU ←U ′; δ ←0\nfor each state s in S do\nU ′[s] ←R(s) + γ\nmax\na ∈A(s)\n\f\ns′\nP(s′ | s, a) U [s′]\nif |U ′[s] −U [s]| > δ then δ ←|U ′[s] −U [s]|\nuntil δ < ϵ(1 −γ)/γ\nreturn U\nFigure 17.4\nThe value iteration algorithm for calculating utilities of states. The termina-\ntion condition is from Equation (17.8).\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n5\n10\n15\n20\n25\n30\nUtility estimates\nNumber of iterations\n(4,3)\n(3,3)\n(1,1)\n(3,1)\n(4,1)\n1\n10\n100\n1000\n10000\n100000\n1e+06\n1e+07\n0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1\nIterations required\nDiscount factor γ\nc = 0.0001\nc = 0.001\nc = 0.01\nc = 0.1\n(a)\n(b)\nFigure 17.5\n(a) Graph showing the evolution of the utilities of selected states using value\niteration. (b) The number of value iterations k required to guarantee an error of at most\nϵ = c · Rmax, for different values of c, as a function of the discount factor γ.\nwhere the update is assumed to be applied simultaneously to all the states at each iteration.\nIf we apply the Bellman update inﬁnitely often, we are guaranteed to reach an equilibrium\n(see Section 17.2.3), in which case the ﬁnal utility values must be solutions to the Bellman\nequations. In fact, they are also the unique solutions, and the corresponding policy (obtained\nusing Equation (17.4)) is optimal. The algorithm, called VALUE-ITERATION, is shown in\nFigure 17.4.",
  "equations. In fact, they are also the unique solutions, and the corresponding policy (obtained\nusing Equation (17.4)) is optimal. The algorithm, called VALUE-ITERATION, is shown in\nFigure 17.4.\nWe can apply value iteration to the 4 × 3 world in Figure 17.1(a). Starting with initial\nvalues of zero, the utilities evolve as shown in Figure 17.5(a). Notice how the states at differ- 654\nChapter\n17.\nMaking Complex Decisions\nent distances from (4,3) accumulate negative reward until a path is found to (4,3), whereupon\nthe utilities start to increase. We can think of the value iteration algorithm as propagating\ninformation through the state space by means of local updates.\n17.2.3\nConvergence of value iteration\nWe said that value iteration eventually converges to a unique set of solutions of the Bellman\nequations. In this section, we explain why this happens. We introduce some useful mathe-\nmatical ideas along the way, and we obtain some methods for assessing the error in the utility\nfunction returned when the algorithm is terminated early; this is useful because it means that\nwe don’t have to run forever. This section is quite technical.\nThe basic concept used in showing that value iteration converges is the notion of a con-\ntraction. Roughly speaking, a contraction is a function of one argument that, when applied to\nCONTRACTION\ntwo different inputs in turn, produces two output values that are “closer together,” by at least\nsome constant factor, than the original inputs. For example, the function “divide by two” is\na contraction, because, after we divide any two numbers by two, their difference is halved.\nNotice that the “divide by two” function has a ﬁxed point, namely zero, that is unchanged by\nthe application of the function. From this example, we can discern two important properties\nof contractions:\n• A contraction has only one ﬁxed point; if there were two ﬁxed points they would not\nget closer together when the function was applied, so it would not be a contraction.\n• When the function is applied to any argument, the value must get closer to the ﬁxed\npoint (because the ﬁxed point does not move), so repeated application of a contraction\nalways reaches the ﬁxed point in the limit.\nNow, suppose we view the Bellman update (Equation (17.6)) as an operator B that is applied\nsimultaneously to update the utility of every state. Let Ui denote the vector of utilities for all\nthe states at the ith iteration. Then the Bellman update equation can be written as",
  "simultaneously to update the utility of every state. Let Ui denote the vector of utilities for all\nthe states at the ith iteration. Then the Bellman update equation can be written as\nUi+1 ←B Ui .\nNext, we need a way to measure distances between utility vectors. We will use the max norm,\nMAX NORM\nwhich measures the “length” of a vector by the absolute value of its biggest component:\n||U|| = max\ns\n|U(s)| .\nWith this deﬁnition, the “distance” between two vectors, ||U −U′||, is the maximum dif-\nference between any two corresponding elements. The main result of this section is the\nfollowing: Let Ui and U′\ni be any two utility vectors. Then we have\n||B Ui −B U′\ni|| ≤γ ||Ui −U′\ni|| .\n(17.7)\nThat is, the Bellman update is a contraction by a factor of γ on the space of utility vectors.\n(Exercise 17.6 provides some guidance on proving this claim.) Hence, from the properties of\ncontractions in general, it follows that value iteration always converges to a unique solution\nof the Bellman equations whenever γ < 1. Section 17.2.\nValue Iteration\n655\nWe can also use the contraction property to analyze the rate of convergence to a solu-\ntion. In particular, we can replace U′\ni in Equation (17.7) with the true utilities U, for which\nB U = U. Then we obtain the inequality\n||B Ui −U|| ≤γ ||Ui −U|| .\nSo, if we view ||Ui −U|| as the error in the estimate Ui, we see that the error is reduced by a\nfactor of at least γ on each iteration. This means that value iteration converges exponentially\nfast. We can calculate the number of iterations required to reach a speciﬁed error bound ϵ\nas follows: First, recall from Equation (17.1) that the utilities of all states are bounded by\n±Rmax/(1 −γ). This means that the maximum initial error ||U0 −U|| ≤2Rmax/(1 −γ).\nSuppose we run for N iterations to reach an error of at most ϵ. Then, because the error is\nreduced by at least γ each time, we require γN · 2Rmax/(1 −γ) ≤ϵ. Taking logs, we ﬁnd\nN = ⌈log(2Rmax/ϵ(1 −γ))/ log(1/γ)⌉\niterations sufﬁce. Figure 17.5(b) shows how N varies with γ, for different values of the ratio\nϵ/Rmax. The good news is that, because of the exponentially fast convergence, N does not\ndepend much on the ratio ϵ/Rmax. The bad news is that N grows rapidly as γ becomes close\nto 1. We can get fast convergence if we make γ small, but this effectively gives the agent a\nshort horizon and could miss the long-term effects of the agent’s actions.\nThe error bound in the preceding paragraph gives some idea of the factors inﬂuencing",
  "short horizon and could miss the long-term effects of the agent’s actions.\nThe error bound in the preceding paragraph gives some idea of the factors inﬂuencing\nthe run time of the algorithm, but is sometimes overly conservative as a method of deciding\nwhen to stop the iteration. For the latter purpose, we can use a bound relating the error\nto the size of the Bellman update on any given iteration. From the contraction property\n(Equation (17.7)), it can be shown that if the update is small (i.e., no state’s utility changes by\nmuch), then the error, compared with the true utility function, also is small. More precisely,\nif\n||Ui+1 −Ui|| < ϵ(1 −γ)/γ\nthen\n||Ui+1 −U|| < ϵ .\n(17.8)\nThis is the termination condition used in the VALUE-ITERATION algorithm of Figure 17.4.\nSo far, we have analyzed the error in the utility function returned by the value iteration\nalgorithm. What the agent really cares about, however, is how well it will do if it makes its\ndecisions on the basis of this utility function. Suppose that after i iterations of value iteration,\nthe agent has an estimate Ui of the true utility U and obtains the MEU policy πi based on\none-step look-ahead using Ui (as in Equation (17.4)). Will the resulting behavior be nearly\nas good as the optimal behavior? This is a crucial question for any real agent, and it turns out\nthat the answer is yes. Uπi(s) is the utility obtained if πi is executed starting in s, and the\npolicy loss ||Uπi −U|| is the most the agent can lose by executing πi instead of the optimal\nPOLICY LOSS\npolicy π∗. The policy loss of πi is connected to the error in Ui by the following inequality:\nif\n||Ui −U|| < ϵ\nthen\n||Uπi −U|| < 2ϵγ/(1 −γ) .\n(17.9)\nIn practice, it often occurs that πi becomes optimal long before Ui has converged. Figure 17.6\nshows how the maximum error in Ui and the policy loss approach zero as the value iteration\nprocess proceeds for the 4 × 3 environment with γ = 0.9. The policy πi is optimal when i = 4,\neven though the maximum error in Ui is still 0.46.\nNow we have everything we need to use value iteration in practice. We know that\nit converges to the correct utilities, we can bound the error in the utility estimates if we 656\nChapter\n17.\nMaking Complex Decisions\nstop after a ﬁnite number of iterations, and we can bound the policy loss that results from\nexecuting the corresponding MEU policy. As a ﬁnal note, all of the results in this section",
  "Chapter\n17.\nMaking Complex Decisions\nstop after a ﬁnite number of iterations, and we can bound the policy loss that results from\nexecuting the corresponding MEU policy. As a ﬁnal note, all of the results in this section\ndepend on discounting with γ < 1. If γ = 1 and the environment contains terminal states,\nthen a similar set of convergence results and error bounds can be derived whenever certain\ntechnical conditions are satisﬁed.\n17.3\nPOLICY ITERATION\nIn the previous section, we observed that it is possible to get an optimal policy even when\nthe utility function estimate is inaccurate. If one action is clearly better than all others, then\nthe exact magnitude of the utilities on the states involved need not be precise. This insight\nsuggests an alternative way to ﬁnd optimal policies. The policy iteration algorithm alternates\nPOLICY ITERATION\nthe following two steps, beginning from some initial policy π0:\n• Policy evaluation: given a policy πi, calculate Ui = Uπi, the utility of each state if πi\nPOLICY EVALUATION\nwere to be executed.\n• Policy improvement: Calculate a new MEU policy πi+1, using one-step look-ahead\nPOLICY\nIMPROVEMENT\nbased on Ui (as in Equation (17.4)).\nThe algorithm terminates when the policy improvement step yields no change in the utilities.\nAt this point, we know that the utility function Ui is a ﬁxed point of the Bellman update, so\nit is a solution to the Bellman equations, and πi must be an optimal policy. Because there are\nonly ﬁnitely many policies for a ﬁnite state space, and each iteration can be shown to yield a\nbetter policy, policy iteration must terminate. The algorithm is shown in Figure 17.7.\nThe policy improvement step is obviously straightforward, but how do we implement\nthe POLICY-EVALUATION routine? It turns out that doing so is much simpler than solving\nthe standard Bellman equations (which is what value iteration does), because the action in\neach state is ﬁxed by the policy. At the ith iteration, the policy πi speciﬁes the action πi(s) in\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n2\n4\n6\n8\n10\n12\n14\nMax error/Policy loss\nNumber of iterations\nMax error\nPolicy loss\nFigure 17.6\nThe maximum error ||Ui −U|| of the utility estimates and the policy loss\n||U πi −U||, as a function of the number of iterations of value iteration. Section 17.3.\nPolicy Iteration\n657\nstate s. This means that we have a simpliﬁed version of the Bellman equation (17.5) relating\nthe utility of s (under πi) to the utilities of its neighbors:\nUi(s) = R(s) + γ\n\f\ns′",
  "Policy Iteration\n657\nstate s. This means that we have a simpliﬁed version of the Bellman equation (17.5) relating\nthe utility of s (under πi) to the utilities of its neighbors:\nUi(s) = R(s) + γ\n\f\ns′\nP(s′ | s, πi(s))Ui(s′) .\n(17.10)\nFor example, suppose πi is the policy shown in Figure 17.2(a). Then we have πi(1, 1) = Up,\nπi(1, 2) = Up, and so on, and the simpliﬁed Bellman equations are\nUi(1, 1) = −0.04 + 0.8Ui(1, 2) + 0.1Ui(1, 1) + 0.1Ui(2, 1) ,\nUi(1, 2) = −0.04 + 0.8Ui(1, 3) + 0.2Ui(1, 2) ,\n...\nThe important point is that these equations are linear, because the “max” operator has been\nremoved. For n states, we have n linear equations with n unknowns, which can be solved\nexactly in time O(n3) by standard linear algebra methods.\nFor small state spaces, policy evaluation using exact solution methods is often the most\nefﬁcient approach. For large state spaces, O(n3) time might be prohibitive. Fortunately, it\nis not necessary to do exact policy evaluation. Instead, we can perform some number of\nsimpliﬁed value iteration steps (simpliﬁed because the policy is ﬁxed) to give a reasonably\ngood approximation of the utilities. The simpliﬁed Bellman update for this process is\nUi+1(s) ←R(s) + γ\n\f\ns′\nP(s′ | s, πi(s))Ui(s′) ,\nand this is repeated k times to produce the next utility estimate. The resulting algorithm is\ncalled modiﬁed policy iteration. It is often much more efﬁcient than standard policy iteration\nMODIFIED POLICY\nITERATION\nor value iteration.\nfunction POLICY-ITERATION(mdp) returns a policy\ninputs: mdp, an MDP with states S, actions A(s), transition model P(s′ | s, a)\nlocal variables: U , a vector of utilities for states in S, initially zero\nπ, a policy vector indexed by state, initially random\nrepeat\nU ←POLICY-EVALUATION(π,U ,mdp)\nunchanged? ←true\nfor each state s in S do\nif\nmax\na ∈A(s)\n\f\ns′\nP(s′ | s, a) U [s′] >\n\f\ns′\nP(s′ | s, π[s]) U [s′] then do\nπ[s] ←argmax\na ∈A(s)\n\f\ns′\nP(s′ | s, a) U [s′]\nunchanged? ←false\nuntil unchanged?\nreturn π\nFigure 17.7\nThe policy iteration algorithm for calculating an optimal policy. 658\nChapter\n17.\nMaking Complex Decisions\nThe algorithms we have described so far require updating the utility or policy for all\nstates at once. It turns out that this is not strictly necessary. In fact, on each iteration, we\ncan pick any subset of states and apply either kind of updating (policy improvement or sim-\npliﬁed value iteration) to that subset. This very general algorithm is called asynchronous",
  "can pick any subset of states and apply either kind of updating (policy improvement or sim-\npliﬁed value iteration) to that subset. This very general algorithm is called asynchronous\npolicy iteration. Given certain conditions on the initial policy and initial utility function,\nASYNCHRONOUS\nPOLICY ITERATION\nasynchronous policy iteration is guaranteed to converge to an optimal policy. The freedom\nto choose any states to work on means that we can design much more efﬁcient heuristic\nalgorithms—for example, algorithms that concentrate on updating the values of states that\nare likely to be reached by a good policy. This makes a lot of sense in real life: if one has no\nintention of throwing oneself off a cliff, one should not spend time worrying about the exact\nvalue of the resulting states.\n17.4\nPARTIALLY OBSERVABLE MDPS\nThe description of Markov decision processes in Section 17.1 assumed that the environment\nwas fully observable. With this assumption, the agent always knows which state it is in.\nThis, combined with the Markov assumption for the transition model, means that the optimal\npolicy depends only on the current state. When the environment is only partially observable,\nthe situation is, one might say, much less clear. The agent does not necessarily know which\nstate it is in, so it cannot execute the action π(s) recommended for that state. Furthermore, the\nutility of a state s and the optimal action in s depend not just on s, but also on how much the\nagent knows when it is in s. For these reasons, partially observable MDPs (or POMDPs—\nPARTIALLY\nOBSERVABLE MDP\npronounced “pom-dee-pees”) are usually viewed as much more difﬁcult than ordinary MDPs.\nWe cannot avoid POMDPs, however, because the real world is one.\n17.4.1\nDeﬁnition of POMDPs\nTo get a handle on POMDPs, we must ﬁrst deﬁne them properly. A POMDP has the same\nelements as an MDP—the transition model P(s′ | s, a), actions A(s), and reward function\nR(s)—but, like the partially observable search problems of Section 4.4, it also has a sensor\nmodel P(e | s). Here, as in Chapter 15, the sensor model speciﬁes the probability of perceiv-\ning evidence e in state s.3 For example, we can convert the 4 × 3 world of Figure 17.1 into\na POMDP by adding a noisy or partial sensor instead of assuming that the agent knows its\nlocation exactly. Such a sensor might measure the number of adjacent walls, which happens\nto be 2 in all the nonterminal squares except for those in the third column, where the value",
  "location exactly. Such a sensor might measure the number of adjacent walls, which happens\nto be 2 in all the nonterminal squares except for those in the third column, where the value\nis 1; a noisy version might give the wrong value with probability 0.1.\nIn Chapters 4 and 11, we studied nondeterministic and partially observable planning\nproblems and identiﬁed the belief state—the set of actual states the agent might be in—as a\nkey concept for describing and calculating solutions. In POMDPs, the belief state b becomes a\nprobability distribution over all possible states, just as in Chapter 15. For example, the initial\n3 As with the reward function for MDPs, the sensor model can also depend on the action and outcome state, but\nagain this change is not fundamental. Section 17.4.\nPartially Observable MDPs\n659\nbelief state for the 4 × 3 POMDP could be the uniform distribution over the nine nonterminal\nstates, i.e., ⟨1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 0, 0⟩. We write b(s) for the probability assigned to the\nactual state s by belief state b. The agent can calculate its current belief state as the conditional\nprobability distribution over the actual states given the sequence of percepts and actions so\nfar. This is essentially the ﬁltering task described in Chapter 15. The basic recursive ﬁltering\nequation (15.5 on page 572) shows how to calculate the new belief state from the previous\nbelief state and the new evidence. For POMDPs, we also have an action to consider, but the\nresult is essentially the same. If b(s) was the previous belief state, and the agent does action\na and then perceives evidence e, then the new belief state is given by\nb′(s′) = α P(e | s′)\n\f\ns\nP(s′ | s, a)b(s) ,\nwhere α is a normalizing constant that makes the belief state sum to 1. By analogy with the\nupdate operator for ﬁltering (page 572), we can write this as\nb′ = FORWARD(b, a, e) .\n(17.11)\nIn the 4 × 3 POMDP, suppose the agent moves Left and its sensor reports 1 adjacent wall; then\nit’s quite likely (although not guaranteed, because both the motion and the sensor are noisy)\nthat the agent is now in (3,1). Exercise 17.13 asks you to calculate the exact probability values\nfor the new belief state.\nThe fundamental insight required to understand POMDPs is this: the optimal action\ndepends only on the agent’s current belief state. That is, the optimal policy can be described\nby a mapping π∗(b) from belief states to actions. It does not depend on the actual state the",
  "depends only on the agent’s current belief state. That is, the optimal policy can be described\nby a mapping π∗(b) from belief states to actions. It does not depend on the actual state the\nagent is in. This is a good thing, because the agent does not know its actual state; all it knows\nis the belief state. Hence, the decision cycle of a POMDP agent can be broken down into the\nfollowing three steps:\n1. Given the current belief state b, execute the action a = π∗(b).\n2. Receive percept e.\n3. Set the current belief state to FORWARD(b, a, e) and repeat.\nNow we can think of POMDPs as requiring a search in belief-state space, just like the meth-\nods for sensorless and contingency problems in Chapter 4. The main difference is that the\nPOMDP belief-state space is continuous, because a POMDP belief state is a probability dis-\ntribution. For example, a belief state for the 4 × 3 world is a point in an 11-dimensional\ncontinuous space. An action changes the belief state, not just the physical state. Hence, the\naction is evaluated at least in part according to the information the agent acquires as a result.\nPOMDPs therefore include the value of information (Section 16.6) as one component of the\ndecision problem.\nLet’s look more carefully at the outcome of actions. In particular, let’s calculate the\nprobability that an agent in belief state b reaches belief state b′ after executing action a. Now,\nif we knew the action and the subsequent percept, then Equation (17.11) would provide a\ndeterministic update to the belief state: b′ = FORWARD(b, a, e). Of course, the subsequent\npercept is not yet known, so the agent might arrive in one of several possible belief states b′,\ndepending on the percept that is received. The probability of perceiving e, given that a was 660\nChapter\n17.\nMaking Complex Decisions\nperformed starting in belief state b, is given by summing over all the actual states s′ that the\nagent might reach:\nP(e|a, b) =\n\f\ns′\nP(e|a, s′, b)P(s′|a, b)\n=\n\f\ns′\nP(e | s′)P(s′|a, b)\n=\n\f\ns′\nP(e | s′)\n\f\ns\nP(s′ | s, a)b(s) .\nLet us write the probability of reaching b′ from b, given action a, as P(b′ | b, a)). Then that\ngives us\nP(b′ | b, a) = P(b′|a, b) =\n\f\ne\nP(b′|e, a, b)P(e|a, b)\n=\n\f\ne\nP(b′|e, a, b)\n\f\ns′\nP(e | s′)\n\f\ns\nP(s′ | s, a)b(s) ,\n(17.12)\nwhere P(b′|e, a, b) is 1 if b′ = FORWARD(b, a, e) and 0 otherwise.\nEquation (17.12) can be viewed as deﬁning a transition model for the belief-state space.",
  "e\nP(b′|e, a, b)P(e|a, b)\n=\n\f\ne\nP(b′|e, a, b)\n\f\ns′\nP(e | s′)\n\f\ns\nP(s′ | s, a)b(s) ,\n(17.12)\nwhere P(b′|e, a, b) is 1 if b′ = FORWARD(b, a, e) and 0 otherwise.\nEquation (17.12) can be viewed as deﬁning a transition model for the belief-state space.\nWe can also deﬁne a reward function for belief states (i.e., the expected reward for the actual\nstates the agent might be in):\nρ(b) =\n\f\ns\nb(s)R(s) .\nTogether, P(b′ | b, a) and ρ(b) deﬁne an observable MDP on the space of belief states. Fur-\nthermore, it can be shown that an optimal policy for this MDP, π∗(b), is also an optimal policy\nfor the original POMDP. In other words, solving a POMDP on a physical state space can be\nreduced to solving an MDP on the corresponding belief-state space. This fact is perhaps less\nsurprising if we remember that the belief state is always observable to the agent, by deﬁnition.\nNotice that, although we have reduced POMDPs to MDPs, the MDP we obtain has a\ncontinuous (and usually high-dimensional) state space. None of the MDP algorithms de-\nscribed in Sections 17.2 and 17.3 applies directly to such MDPs. The next two subsec-\ntions describe a value iteration algorithm designed speciﬁcally for POMDPs and an online\ndecision-making algorithm, similar to those developed for games in Chapter 5.\n17.4.2\nValue iteration for POMDPs\nSection 17.2 described a value iteration algorithm that computed one utility value for each\nstate. With inﬁnitely many belief states, we need to be more creative. Consider an optimal\npolicy π∗and its application in a speciﬁc belief state b: the policy generates an action, then,\nfor each subsequent percept, the belief state is updated and a new action is generated, and so\non. For this speciﬁc b, therefore, the policy is exactly equivalent to a conditional plan, as de-\nﬁned in Chapter 4 for nondeterministic and partially observable problems. Instead of thinking\nabout policies, let us think about conditional plans and how the expected utility of executing\na ﬁxed conditional plan varies with the initial belief state. We make two observations: Section 17.4.\nPartially Observable MDPs\n661\n1. Let the utility of executing a ﬁxed conditional plan p starting in physical state s be αp(s).\nThen the expected utility of executing p in belief state b is just \u0002\ns b(s)αp(s), or b · αp\nif we think of them both as vectors. Hence, the expected utility of a ﬁxed conditional\nplan varies linearly with b; that is, it corresponds to a hyperplane in belief space.",
  "s b(s)αp(s), or b · αp\nif we think of them both as vectors. Hence, the expected utility of a ﬁxed conditional\nplan varies linearly with b; that is, it corresponds to a hyperplane in belief space.\n2. At any given belief state b, the optimal policy will choose to execute the conditional\nplan with highest expected utility; and the expected utility of b under the optimal policy\nis just the utility of that conditional plan:\nU(b) = Uπ∗(b) = max\np\nb · αp .\nIf the optimal policy π∗chooses to execute p starting at b, then it is reasonable to expect\nthat it might choose to execute p in belief states that are very close to b; in fact, if we\nbound the depth of the conditional plans, then there are only ﬁnitely many such plans\nand the continuous space of belief states will generally be divided into regions, each\ncorresponding to a particular conditional plan that is optimal in that region.\nFrom these two observations, we see that the utility function U(b) on belief states, being the\nmaximum of a collection of hyperplanes, will be piecewise linear and convex.\nTo illustrate this, we use a simple two-state world. The states are labeled 0 and 1, with\nR(0) = 0 and R(1) = 1. There are two actions: Stay stays put with probability 0.9 and Go\nswitches to the other state with probability 0.9. For now we will assume the discount factor\nγ = 1. The sensor reports the correct state with probability 0.6. Obviously, the agent should\nStay when it thinks it’s in state 1 and Go when it thinks it’s in state 0.\nThe advantage of a two-state world is that the belief space can be viewed as one-\ndimensional, because the two probabilities must sum to 1. In Figure 17.8(a), the x-axis\nrepresents the belief state, deﬁned by b(1), the probability of being in state 1. Now let us con-\nsider the one-step plans [Stay] and [Go], each of which receives the reward for the current\nstate followed by the (discounted) reward for the state reached after the action:\nα[Stay](0) = R(0) + γ(0.9R(0) + 0.1R(1)) = 0.1\nα[Stay](1) = R(1) + γ(0.9R(1) + 0.1R(0)) = 1.9\nα[Go](0) = R(0) + γ(0.9R(1) + 0.1R(0)) = 0.9\nα[Go](1) = R(1) + γ(0.9R(0) + 0.1R(1)) = 1.1\nThe hyperplanes (lines, in this case) for b·α[Stay] and b·α[Go] are shown in Figure 17.8(a) and\ntheir maximum is shown in bold. The bold line therefore represents the utility function for\nthe ﬁnite-horizon problem that allows just one action, and in each “piece” of the piecewise",
  "their maximum is shown in bold. The bold line therefore represents the utility function for\nthe ﬁnite-horizon problem that allows just one action, and in each “piece” of the piecewise\nlinear utility function the optimal action is the ﬁrst action of the corresponding conditional\nplan. In this case, the optimal one-step policy is to Stay when b(1) > 0.5 and Go otherwise.\nOnce we have utilities αp(s) for all the conditional plans p of depth 1 in each physical\nstate s, we can compute the utilities for conditional plans of depth 2 by considering each\npossible ﬁrst action, each possible subsequent percept, and then each way of choosing a\ndepth-1 plan to execute for each percept:\n[Stay; if Percept = 0 then Stay else Stay]\n[Stay; if Percept = 0 then Stay else Go] . . . 662\nChapter\n17.\nMaking Complex Decisions\n 0\n 0.5\n 1\n 1.5\n 2\n 2.5\n 3\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nUtility\nProbability of state 1\n[Stay]\n[Go]\n 0\n 0.5\n 1\n 1.5\n 2\n 2.5\n 3\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nUtility\nProbability of state 1\n(a)\n(b)\n 0\n 0.5\n 1\n 1.5\n 2\n 2.5\n 3\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nUtility\nProbability of state 1\n 4.5\n 5\n 5.5\n 6\n 6.5\n 7\n 7.5\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nUtility\nProbability of state 1\n(c)\n(d)\nFigure 17.8\n(a) Utility of two one-step plans as a function of the initial belief state b(1)\nfor the two-state world, with the corresponding utility function shown in bold. (b) Utilities\nfor 8 distinct two-step plans. (c) Utilities for four undominated two-step plans. (d) Utility\nfunction for optimal eight-step plans.\nThere are eight distinct depth-2 plans in all, and their utilities are shown in Figure 17.8(b).\nNotice that four of the plans, shown as dashed lines, are suboptimal across the entire belief\nspace—we say these plans are dominated, and they need not be considered further. There\nDOMINATED PLAN\nare four undominated plans, each of which is optimal in a speciﬁc region, as shown in Fig-\nure 17.8(c). The regions partition the belief-state space.\nWe repeat the process for depth 3, and so on. In general, let p be a depth-d conditional\nplan whose initial action is a and whose depth-d −1 subplan for percept e is p.e; then\nαp(s) = R(s) + γ\n\u001f\f\ns′\nP(s′ | s, a)\n\f\ne\nP(e | s′)αp.e(s′)\n \n.\n(17.13)\nThis recursion naturally gives us a value iteration algorithm, which is sketched in Figure 17.9.\nThe structure of the algorithm and its error analysis are similar to those of the basic value iter-\nation algorithm in Figure 17.4 on page 653; the main difference is that instead of computing",
  "The structure of the algorithm and its error analysis are similar to those of the basic value iter-\nation algorithm in Figure 17.4 on page 653; the main difference is that instead of computing\none utility number for each state, POMDP-VALUE-ITERATION maintains a collection of Section 17.4.\nPartially Observable MDPs\n663\nfunction POMDP-VALUE-ITERATION(pomdp,ϵ) returns a utility function\ninputs: pomdp, a POMDP with states S, actions A(s), transition model P(s′ | s, a),\nsensor model P(e | s), rewards R(s), discount γ\nϵ, the maximum error allowed in the utility of any state\nlocal variables: U , U ′, sets of plans p with associated utility vectors αp\nU ′ ←a set containing just the empty plan [ ], with α[ ](s) = R(s)\nrepeat\nU ←U ′\nU ′ ←the set of all plans consisting of an action and, for each possible next percept,\na plan in U with utility vectors computed according to Equation (17.13)\nU ′ ←REMOVE-DOMINATED-PLANS(U ′)\nuntil MAX-DIFFERENCE(U , U ′) < ϵ(1 −γ)/γ\nreturn U\nFigure 17.9\nA high-level sketch of the value iteration algorithm for POMDPs.\nThe\nREMOVE-DOMINATED-PLANS step and MAX-DIFFERENCE test are typically implemented\nas linear programs.\nundominated plans with their utility hyperplanes. The algorithm’s complexity depends pri-\nmarily on how many plans get generated. Given |A| actions and |E| possible observations, it\nis easy to show that there are |A|O(|E|d−1) distinct depth-d plans. Even for the lowly two-state\nworld with d = 8, the exact number is 2255. The elimination of dominated plans is essential\nfor reducing this doubly exponential growth: the number of undominated plans with d = 8 is\njust 144. The utility function for these 144 plans is shown in Figure 17.8(d).\nNotice that even though state 0 has lower utility than state 1, the intermediate belief\nstates have even lower utility because the agent lacks the information needed to choose a\ngood action. This is why information has value in the sense deﬁned in Section 16.6 and\noptimal policies in POMDPs often include information-gathering actions.\nGiven such a utility function, an executable policy can be extracted by looking at which\nhyperplane is optimal at any given belief state b and executing the ﬁrst action of the corre-\nsponding plan. In Figure 17.8(d), the corresponding optimal policy is still the same as for\ndepth-1 plans: Stay when b(1) > 0.5 and Go otherwise.\nIn practice, the value iteration algorithm in Figure 17.9 is hopelessly inefﬁcient for",
  "sponding plan. In Figure 17.8(d), the corresponding optimal policy is still the same as for\ndepth-1 plans: Stay when b(1) > 0.5 and Go otherwise.\nIn practice, the value iteration algorithm in Figure 17.9 is hopelessly inefﬁcient for\nlarger problems—even the 4 × 3 POMDP is too hard. The main reason is that, given n con-\nditional plans at level d, the algorithm constructs |A| · n|E| conditional plans at level d + 1\nbefore eliminating the dominated ones. Since the 1970s, when this algorithm was developed,\nthere have been several advances including more efﬁcient forms of value iteration and various\nkinds of policy iteration algorithms. Some of these are discussed in the notes at the end of the\nchapter. For general POMDPs, however, ﬁnding optimal policies is very difﬁcult (PSPACE-\nhard, in fact—i.e., very hard indeed). Problems with a few dozen states are often infeasible.\nThe next section describes a different, approximate method for solving POMDPs, one based\non look-ahead search. 664\nChapter\n17.\nMaking Complex Decisions\nXt–1\nAt–1\nRt–1\nAt\nRt\nAt+2\nRt+2\nAt+1\nRt+1\nAt–2\nEt–1\nXt+1\nEt+1\nXt+2\nEt+2\nXt+3\nEt+3\nUt+3\nXt\nEt\nFigure 17.10\nThe generic structure of a dynamic decision network. Variables with known\nvalues are shaded. The current time is t and the agent must decide what to do—that is, choose\na value for At. The network has been unrolled into the future for three steps and represents\nfuture rewards, as well as the utility of the state at the look-ahead horizon.\n17.4.3\nOnline agents for POMDPs\nIn this section, we outline a simple approach to agent design for partially observable, stochas-\ntic environments. The basic elements of the design are already familiar:\n• The transition and sensor models are represented by a dynamic Bayesian network\n(DBN), as described in Chapter 15.\n• The dynamic Bayesian network is extended with decision and utility nodes, as used in\ndecision networks in Chapter 16. The resulting model is called a dynamic decision\nnetwork, or DDN.\nDYNAMIC DECISION\nNETWORK\n• A ﬁltering algorithm is used to incorporate each new percept and action and to update\nthe belief state representation.\n• Decisions are made by projecting forward possible action sequences and choosing the\nbest one.\nDBNs are factored representations in the terminology of Chapter 2; they typically have\nan exponential complexity advantage over atomic representations and can model quite sub-\nstantial real-world problems. The agent design is therefore a practical implementation of the",
  "an exponential complexity advantage over atomic representations and can model quite sub-\nstantial real-world problems. The agent design is therefore a practical implementation of the\nutility-based agent sketched in Chapter 2.\nIn the DBN, the single state St becomes a set of state variables Xt, and there may be\nmultiple evidence variables Et. We will use At to refer to the action at time t, so the transition\nmodel becomes P(Xt+1|Xt, At) and the sensor model becomes P(Et|Xt). We will use Rt to\nrefer to the reward received at time t and Ut to refer to the utility of the state at time t. (Both\nof these are random variables.) With this notation, a dynamic decision network looks like the\none shown in Figure 17.10.\nDynamic decision networks can be used as inputs for any POMDP algorithm, including\nthose for value and policy iteration methods. In this section, we focus on look-ahead methods\nthat project action sequences forward from the current belief state in much the same way as do\nthe game-playing algorithms of Chapter 5. The network in Figure 17.10 has been projected\nthree steps into the future; the current and future decisions A and the future observations Section 17.4.\nPartially Observable MDPs\n665\n. . .\n...\n...\n...\n...\n...\n...\n. . .\n...\n...\n...\n...\n...\n...\n. . .\n...\n. . .\n...\n...\n. . .\n...\n. . .\nAt in P(Xt | E1:t)\nAt+1 in P(Xt+1 | E1:t+1)\nAt+2 in P(Xt+2 | E1:t+2)\nU(Xt+3)\nEt+1\nEt+2\nEt+3\n10\n4\n6\n3\nFigure 17.11\nPart of the look-ahead solution of the DDN in Figure 17.10. Each decision\nwill be taken in the belief state indicated.\nE and rewards R are all unknown. Notice that the network includes nodes for the rewards\nfor Xt+1 and Xt+2, but the utility for Xt+3. This is because the agent must maximize the\n(discounted) sum of all future rewards, and U(Xt+3) represents the reward for Xt+3 and all\nsubsequent rewards. As in Chapter 5, we assume that U is available only in some approximate\nform: if exact utility values were available, look-ahead beyond depth 1 would be unnecessary.\nFigure 17.11 shows part of the search tree corresponding to the three-step look-ahead\nDDN in Figure 17.10. Each of the triangular nodes is a belief state in which the agent makes\na decision At+i for i = 0, 1, 2, . . .. The round (chance) nodes correspond to choices by the\nenvironment, namely, what evidence Et+i arrives. Notice that there are no chance nodes\ncorresponding to the action outcomes; this is because the belief-state update for an action is",
  "environment, namely, what evidence Et+i arrives. Notice that there are no chance nodes\ncorresponding to the action outcomes; this is because the belief-state update for an action is\ndeterministic regardless of the actual outcome.\nThe belief state at each triangular node can be computed by applying a ﬁltering al-\ngorithm to the sequence of percepts and actions leading to it. In this way, the algorithm\ntakes into account the fact that, for decision At+i, the agent will have available percepts\nEt+1, . . . , Et+i, even though at time t it does not know what those percepts will be. In this\nway, a decision-theoretic agent automatically takes into account the value of information and\nwill execute information-gathering actions where appropriate.\nA decision can be extracted from the search tree by backing up the utility values from\nthe leaves, taking an average at the chance nodes and taking the maximum at the decision\nnodes. This is similar to the EXPECTIMINIMAX algorithm for game trees with chance nodes,\nexcept that (1) there can also be rewards at non-leaf states and (2) the decision nodes corre-\nspond to belief states rather than actual states. The time complexity of an exhaustive search\nto depth d is O(|A|d · |E|d), where |A| is the number of available actions and |E| is the num-\nber of possible percepts. (Notice that this is far less than the number of depth-d conditional 666\nChapter\n17.\nMaking Complex Decisions\nplans generated by value iteration.) For problems in which the discount factor γ is not too\nclose to 1, a shallow search is often good enough to give near-optimal decisions. It is also\npossible to approximate the averaging step at the chance nodes, by sampling from the set of\npossible percepts instead of summing over all possible percepts. There are various other ways\nof ﬁnding good approximate solutions quickly, but we defer them to Chapter 21.\nDecision-theoretic agents based on dynamic decision networks have a number of advan-\ntages compared with other, simpler agent designs presented in earlier chapters. In particular,\nthey handle partially observable, uncertain environments and can easily revise their “plans” to\nhandle unexpected evidence. With appropriate sensor models, they can handle sensor failure\nand can plan to gather information. They exhibit “graceful degradation” under time pressure\nand in complex environments, using various approximation techniques. So what is missing?",
  "and can plan to gather information. They exhibit “graceful degradation” under time pressure\nand in complex environments, using various approximation techniques. So what is missing?\nOne defect of our DDN-based algorithm is its reliance on forward search through state space,\nrather than using the hierarchical and other advanced planning techniques described in Chap-\nter 11. There have been attempts to extend these techniques into the probabilistic domain, but\nso far they have proved to be inefﬁcient. A second, related problem is the basically proposi-\ntional nature of the DDN language. We would like to be able to extend some of the ideas for\nﬁrst-order probabilistic languages to the problem of decision making. Current research has\nshown that this extension is possible and has signiﬁcant beneﬁts, as discussed in the notes at\nthe end of the chapter.\n17.5\nDECISIONS WITH MULTIPLE AGENTS: GAME THEORY\nThis chapter has concentrated on making decisions in uncertain environments. But what if\nthe uncertainty is due to other agents and the decisions they make? And what if the decisions\nof those agents are in turn inﬂuenced by our decisions? We addressed this question once\nbefore, when we studied games in Chapter 5. There, however, we were primarily concerned\nwith turn-taking games in fully observable environments, for which minimax search can be\nused to ﬁnd optimal moves. In this section we study the aspects of game theory that analyze\nGAME THEORY\ngames with simultaneous moves and other sources of partial observability. (Game theorists\nuse the terms perfect information and imperfect information rather than fully and partially\nobservable.) Game theory can be used in at least two ways:\n1. Agent design: Game theory can analyze the agent’s decisions and compute the expected\nutility for each decision (under the assumption that other agents are acting optimally\naccording to game theory). For example, in the game two-ﬁnger Morra, two players,\nO and E, simultaneously display one or two ﬁngers. Let the total number of ﬁngers\nbe f. If f is odd, O collects f dollars from E; and if f is even, E collects f dollars\nfrom O. Game theory can determine the best strategy against a rational player and the\nexpected return for each player.4\n4 Morra is a recreational version of an inspection game. In such games, an inspector chooses a day to inspect a\nfacility (such as a restaurant or a biological weapons plant), and the facility operator chooses a day to hide all the",
  "4 Morra is a recreational version of an inspection game. In such games, an inspector chooses a day to inspect a\nfacility (such as a restaurant or a biological weapons plant), and the facility operator chooses a day to hide all the\nnasty stuff. The inspector wins if the days are different, and the facility operator wins if they are the same. Section 17.5.\nDecisions with Multiple Agents: Game Theory\n667\n2. Mechanism design: When an environment is inhabited by many agents, it might be\npossible to deﬁne the rules of the environment (i.e., the game that the agents must\nplay) so that the collective good of all agents is maximized when each agent adopts the\ngame-theoretic solution that maximizes its own utility. For example, game theory can\nhelp design the protocols for a collection of Internet trafﬁc routers so that each router\nhas an incentive to act in such a way that global throughput is maximized. Mechanism\ndesign can also be used to construct intelligent multiagent systems that solve complex\nproblems in a distributed fashion.\n17.5.1\nSingle-move games\nWe start by considering a restricted set of games: ones where all players take action simulta-\nneously and the result of the game is based on this single set of actions. (Actually, it is not\ncrucial that the actions take place at exactly the same time; what matters is that no player has\nknowledge of the other players’ choices.) The restriction to a single move (and the very use\nof the word “game”) might make this seem trivial, but in fact, game theory is serious busi-\nness. It is used in decision-making situations including the auctioning of oil drilling rights\nand wireless frequency spectrum rights, bankruptcy proceedings, product development and\npricing decisions, and national defense—situations involving billions of dollars and hundreds\nof thousands of lives. A single-move game is deﬁned by three components:\n• Players or agents who will be making decisions. Two-player games have received the\nPLAYER\nmost attention, although n-player games for n > 2 are also common. We give players\ncapitalized names, like Alice and Bob or O and E.\n• Actions that the players can choose. We will give actions lowercase names, like one or\nACTION\ntestify. The players may or may not have the same set of actions available.\n• A payoff function that gives the utility to each player for each combination of actions\nPAYOFF FUNCTION\nby all the players. For single-move games the payoff function can be represented by a",
  "• A payoff function that gives the utility to each player for each combination of actions\nPAYOFF FUNCTION\nby all the players. For single-move games the payoff function can be represented by a\nmatrix, a representation known as the strategic form (also called normal form). The\nSTRATEGIC FORM\npayoff matrix for two-ﬁnger Morra is as follows:\nO: one\nO: two\nE: one\nE = +2, O = −2\nE = −3, O = +3\nE: two\nE = −3, O = +3\nE = +4, O = −4\nFor example, the lower-right corner shows that when player O chooses action two and\nE also chooses two, the payoff is +4 for E and −4 for O.\nEach player in a game must adopt and then execute a strategy (which is the name used in\nSTRATEGY\ngame theory for a policy). A pure strategy is a deterministic policy; for a single-move game,\nPURE STRATEGY\na pure strategy is just a single action. For many games an agent can do better with a mixed\nstrategy, which is a randomized policy that selects actions according to a probability distri-\nMIXED STRATEGY\nbution. The mixed strategy that chooses action a with probability p and action b otherwise\nis written [p: a; (1 −p): b]. For example, a mixed strategy for two-ﬁnger Morra might be\n[0.5: one; 0.5: two]. A strategy proﬁle is an assignment of a strategy to each player; given\nSTRATEGY PROFILE\nthe strategy proﬁle, the game’s outcome is a numeric value for each player.\nOUTCOME 668\nChapter\n17.\nMaking Complex Decisions\nA solution to a game is a strategy proﬁle in which each player adopts a rational strategy.\nSOLUTION\nWe will see that the most important issue in game theory is to deﬁne what “rational” means\nwhen each agent chooses only part of the strategy proﬁle that determines the outcome. It is\nimportant to realize that outcomes are actual results of playing a game, while solutions are\ntheoretical constructs used to analyze a game. We will see that some games have a solution\nonly in mixed strategies. But that does not mean that a player must literally be adopting a\nmixed strategy to be rational.\nConsider the following story: Two alleged burglars, Alice and Bob, are caught red-\nhanded near the scene of a burglary and are interrogated separately. A prosecutor offers each\na deal: if you testify against your partner as the leader of a burglary ring, you’ll go free for\nbeing the cooperative one, while your partner will serve 10 years in prison. However, if you\nboth testify against each other, you’ll both get 5 years. Alice and Bob also know that if both",
  "being the cooperative one, while your partner will serve 10 years in prison. However, if you\nboth testify against each other, you’ll both get 5 years. Alice and Bob also know that if both\nrefuse to testify they will serve only 1 year each for the lesser charge of possessing stolen\nproperty. Now Alice and Bob face the so-called prisoner’s dilemma: should they testify\nPRISONER’S\nDILEMMA\nor refuse? Being rational agents, Alice and Bob each want to maximize their own expected\nutility. Let’s assume that Alice is callously unconcerned about her partner’s fate, so her utility\ndecreases in proportion to the number of years she will spend in prison, regardless of what\nhappens to Bob. Bob feels exactly the same way. To help reach a rational decision, they both\nconstruct the following payoff matrix:\nAlice:testify\nAlice:refuse\nBob:testify\nA = −5, B = −5\nA = −10, B = 0\nBob:refuse\nA = 0, B = −10\nA = −1, B = −1\nAlice analyzes the payoff matrix as follows: “Suppose Bob testiﬁes. Then I get 5 years if I\ntestify and 10 years if I don’t, so in that case testifying is better. On the other hand, if Bob\nrefuses, then I get 0 years if I testify and 1 year if I refuse, so in that case as well testifying is\nbetter. So in either case, it’s better for me to testify, so that’s what I must do.”\nAlice has discovered that testify is a dominant strategy for the game. We say that a\nDOMINANT\nSTRATEGY\nstrategy s for player p strongly dominates strategy s′ if the outcome for s is better for p than\nSTRONG\nDOMINATION\nthe outcome for s′, for every choice of strategies by the other player(s). Strategy s weakly\ndominates s′ if s is better than s′ on at least one strategy proﬁle and no worse on any other.\nWEAK DOMINATION\nA dominant strategy is a strategy that dominates all others. It is irrational to play a dominated\nstrategy, and irrational not to play a dominant strategy if one exists. Being rational, Alice\nchooses the dominant strategy. We need just a bit more terminology: we say that an outcome\nis Pareto optimal5 if there is no other outcome that all players would prefer. An outcome is\nPARETO OPTIMAL\nPareto dominated by another outcome if all players would prefer the other outcome.\nPARETO DOMINATED\nIf Alice is clever as well as rational, she will continue to reason as follows: Bob’s\ndominant strategy is also to testify. Therefore, he will testify and we will both get ﬁve years.\nWhen each player has a dominant strategy, the combination of those strategies is called a",
  "dominant strategy is also to testify. Therefore, he will testify and we will both get ﬁve years.\nWhen each player has a dominant strategy, the combination of those strategies is called a\ndominant strategy equilibrium. In general, a strategy proﬁle forms an equilibrium if no\nDOMINANT\nSTRATEGY\nEQUILIBRIUM\nEQUILIBRIUM\nplayer can beneﬁt by switching strategies, given that every other player sticks with the same\n5 Pareto optimality is named after the economist Vilfredo Pareto (1848–1923). Section 17.5.\nDecisions with Multiple Agents: Game Theory\n669\nstrategy. An equilibrium is essentially a local optimum in the space of policies; it is the top\nof a peak that slopes downward along every dimension, where a dimension corresponds to a\nplayer’s strategy choices.\nThe mathematician John Nash (1928–) proved that every game has at least one equi-\nlibrium. The general concept of equilibrium is now called Nash equilibrium in his honor.\nClearly, a dominant strategy equilibrium is a Nash equilibrium (Exercise 17.16), but some\nNASH EQUILIBRIUM\ngames have Nash equilibria but no dominant strategies.\nThe dilemma in the prisoner’s dilemma is that the equilibrium outcome is worse for\nboth players than the outcome they would get if they both refused to testify. In other words,\n(testify, testify) is Pareto dominated by the (-1, -1) outcome of (refuse, refuse). Is there any\nway for Alice and Bob to arrive at the (-1, -1) outcome? It is certainly an allowable option\nfor both of them to refuse to testify, but is is hard to see how rational agents can get there,\ngiven the deﬁnition of the game. Either player contemplating playing refuse will realize that\nhe or she would do better by playing testify. That is the attractive power of an equilibrium\npoint. Game theorists agree that being a Nash equilibrium is a necessary condition for being\na solution—although they disagree whether it is a sufﬁcient condition.\nIt is easy enough to get to the (refuse, refuse) solution if we modify the game. For\nexample, we could change to a repeated game in which the players know that they will meet\nagain. Or the agents might have moral beliefs that encourage cooperation and fairness. That\nmeans they have a different utility function, necessitating a different payoff matrix, making\nit a different game. We will see later that agents with limited computational powers, rather\nthan the ability to reason absolutely rationally, can reach non-equilibrium outcomes, as can an",
  "it a different game. We will see later that agents with limited computational powers, rather\nthan the ability to reason absolutely rationally, can reach non-equilibrium outcomes, as can an\nagent that knows that the other agent has limited rationality. In each case, we are considering\na different game than the one described by the payoff matrix above.\nNow let’s look at a game that has no dominant strategy. Acme, a video game console\nmanufacturer, has to decide whether its next game machine will use Blu-ray discs or DVDs.\nMeanwhile, the video game software producer Best needs to decide whether to produce its\nnext game on Blu-ray or DVD. The proﬁts for both will be positive if they agree and negative\nif they disagree, as shown in the following payoff matrix:\nAcme:bluray\nAcme:dvd\nBest:bluray\nA = +9, B = +9\nA = −4, B = −1\nBest:dvd\nA = −3, B = −1\nA = +5, B = +5\nThere is no dominant strategy equilibrium for this game, but there are two Nash equilibria:\n(bluray, bluray) and (dvd, dvd). We know these are Nash equilibria because if either player\nunilaterally moves to a different strategy, that player will be worse off. Now the agents have\na problem: there are multiple acceptable solutions, but if each agent aims for a different\nsolution, then both agents will suffer. How can they agree on a solution? One answer is\nthat both should choose the Pareto-optimal solution (bluray, bluray); that is, we can restrict\nthe deﬁnition of “solution” to the unique Pareto-optimal Nash equilibrium provided that one\nexists. Every game has at least one Pareto-optimal solution, but a game might have several,\nor they might not be equilibrium points. For example, if (bluray, bluray) had payoff (5,\n5), then there would be two equal Pareto-optimal equilibrium points. To choose between 670\nChapter\n17.\nMaking Complex Decisions\nthem the agents can either guess or communicate, which can be done either by establishing\na convention that orders the solutions before the game begins or by negotiating to reach a\nmutually beneﬁcial solution during the game (which would mean including communicative\nactions as part of a sequential game). Communication thus arises in game theory for exactly\nthe same reasons that it arose in multiagent planning in Section 11.4. Games in which players\nneed to communicate like this are called coordination games.\nCOORDINATION\nGAME\nA game can have more than one Nash equilibrium; how do we know that every game",
  "need to communicate like this are called coordination games.\nCOORDINATION\nGAME\nA game can have more than one Nash equilibrium; how do we know that every game\nmust have at least one? Some games have no pure-strategy Nash equilibria. Consider, for\nexample, any pure-strategy proﬁle for two-ﬁnger Morra (page 666). If the total number of\nﬁngers is even, then O will want to switch; on the other hand (so to speak), if the total is odd,\nthen E will want to switch. Therefore, no pure strategy proﬁle can be an equilibrium and we\nmust look to mixed strategies instead.\nBut which mixed strategy? In 1928, von Neumann developed a method for ﬁnding the\noptimal mixed strategy for two-player, zero-sum games—games in which the sum of the\nZERO-SUM GAME\npayoffs is always zero.6 Clearly, Morra is such a game. For two-player, zero-sum games, we\nknow that the payoffs are equal and opposite, so we need consider the payoffs of only one\nplayer, who will be the maximizer (just as in Chapter 5). For Morra, we pick the even player\nE to be the maximizer, so we can deﬁne the payoff matrix by the values UE(e, o)—the payoff\nto E if E does e and O does o. (For convenience we call player E “her” and O “him.”) Von\nNeumann’s method is called the the maximin technique, and it works as follows:\nMAXIMIN\n• Suppose we change the rules as follows: ﬁrst E picks her strategy and reveals it to\nO. Then O picks his strategy, with knowledge of E’s strategy. Finally, we evaluate\nthe expected payoff of the game based on the chosen strategies. This gives us a turn-\ntaking game to which we can apply the standard minimax algorithm from Chapter 5.\nLet’s suppose this gives an outcome UE,O. Clearly, this game favors O, so the true\nutility U of the original game (from E’s point of view) is at least UE,O. For example,\nif we just look at pure strategies, the minimax game tree has a root value of −3 (see\nFigure 17.12(a)), so we know that U ≥−3.\n• Now suppose we change the rules to force O to reveal his strategy ﬁrst, followed by E.\nThen the minimax value of this game is UO,E, and because this game favors E we know\nthat U is at most UO,E. With pure strategies, the value is +2 (see Figure 17.12(b)), so\nwe know U ≤+2.\nCombining these two arguments, we see that the true utility U of the solution to the original\ngame must satisfy\nUE,O ≤U ≤UO,E\nor in this case,\n−3 ≤U ≤2 .\nTo pinpoint the value of U, we need to turn our analysis to mixed strategies. First, observe the",
  "game must satisfy\nUE,O ≤U ≤UO,E\nor in this case,\n−3 ≤U ≤2 .\nTo pinpoint the value of U, we need to turn our analysis to mixed strategies. First, observe the\nfollowing: once the ﬁrst player has revealed his or her strategy, the second player might as\nwell choose a pure strategy. The reason is simple: if the second player plays a mixed strategy,\n[p: one; (1−p): two], its expected utility is a linear combination (p · uone + (1−p)· utwo) of\n6 or a constant—see page 162. Section 17.5.\nDecisions with Multiple Agents: Game Theory\n671\none\none\none\ntwo\ntwo\ntwo\nE\nO\none\none\none\ntwo\ntwo\ntwo\nO\nE\none\ntwo\nE\nO\none\ntwo\nO\nE\n+4\n+3\n+2\n+1\n 0\n–1\n–2\n–3\n1\ntwo\none\nU\np\n+4\n+3\n+2\n+1\n 0\n–1\n–2\n–3\n1\ntwo\none\nU\nq\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n[p: one; (1 – p): two]\n[q: one; (1 – q): two]\n2p – 3(1 – p)\n2q – 3(1 – q)\n3p + 4(1 – p)\n3q + 4(1 – q)\n2\n-3\n-3\n-3\n-3\n-3\n4\n2\n2\n2\n-3\n-3\n4\n4\nFigure 17.12\n(a) and (b): Minimax game trees for two-ﬁnger Morra if the players take\nturns playing pure strategies. (c) and (d): Parameterized game trees where the ﬁrst player\nplays a mixed strategy. The payoffs depend on the probability parameter (p or q) in the\nmixed strategy. (e) and (f): For any particular value of the probability parameter, the second\nplayer will choose the “better” of the two actions, so the value of the ﬁrst player’s mixed\nstrategy is given by the heavy lines. The ﬁrst player will choose the probability parameter for\nthe mixed strategy at the intersection point.\nthe utilities of the pure strategies, uone and utwo. This linear combination can never be better\nthan the better of uone and utwo, so the second player can just choose the better one.\nWith this observation in mind, the minimax trees can be thought of as having inﬁnitely\nmany branches at the root, corresponding to the inﬁnitely many mixed strategies the ﬁrst 672\nChapter\n17.\nMaking Complex Decisions\nplayer can choose. Each of these leads to a node with two branches corresponding to the\npure strategies for the second player. We can depict these inﬁnite trees ﬁnitely by having one\n“parameterized” choice at the root:\n• If E chooses ﬁrst, the situation is as shown in Figure 17.12(c). E chooses the strategy\n[p: one; (1−p): two] at the root, and then O chooses a pure strategy (and hence a move)\ngiven the value of p. If O chooses one, the expected payoff (to E) is 2p−3(1−p) = 5p−\n3; if O chooses two, the expected payoff is −3p + 4(1 −p) = 4 −7p. We can draw\nthese two payoffs as straight lines on a graph, where p ranges from 0 to 1 on the x-axis,",
  "3; if O chooses two, the expected payoff is −3p + 4(1 −p) = 4 −7p. We can draw\nthese two payoffs as straight lines on a graph, where p ranges from 0 to 1 on the x-axis,\nas shown in Figure 17.12(e). O, the minimizer, will always choose the lower of the two\nlines, as shown by the heavy lines in the ﬁgure. Therefore, the best that E can do at the\nroot is to choose p to be at the intersection point, which is where\n5p −3 = 4 −7p\n⇒\np = 7/12 .\nThe utility for E at this point is UE,O = −1/12.\n• If O moves ﬁrst, the situation is as shown in Figure 17.12(d). O chooses the strategy\n[q: one; (1 −q): two] at the root, and then E chooses a move given the value of q. The\npayoffs are 2q−3(1−q) = 5q−3 and −3q+4(1−q) = 4−7q.7 Again, Figure 17.12(f)\nshows that the best O can do at the root is to choose the intersection point:\n5q −3 = 4 −7q\n⇒\nq = 7/12 .\nThe utility for E at this point is UO,E = −1/12.\nNow we know that the true utility of the original game lies between −1/12 and −1/12, that\nis, it is exactly −1/12! (The moral is that it is better to be O than E if you are playing this\ngame.) Furthermore, the true utility is attained by the mixed strategy [7/12: one; 5/12: two],\nwhich should be played by both players. This strategy is called the maximin equilibrium of\nMAXIMIN\nEQUILIBRIUM\nthe game, and is a Nash equilibrium. Note that each component strategy in an equilibrium\nmixed strategy has the same expected utility. In this case, both one and two have the same\nexpected utility, −1/12, as the mixed strategy itself.\nOur result for two-ﬁnger Morra is an example of the general result by von Neumann:\nevery two-player zero-sum game has a maximin equilibrium when you allow mixed strategies.\nFurthermore, every Nash equilibrium in a zero-sum game is a maximin for both players. A\nplayer who adopts the maximin strategy has two guarantees: First, no other strategy can do\nbetter against an opponent who plays well (although some other strategies might be better at\nexploiting an opponent who makes irrational mistakes). Second, the player continues to do\njust as well even if the strategy is revealed to the opponent.\nThe general algorithm for ﬁnding maximin equilibria in zero-sum games is somewhat\nmore involved than Figures 17.12(e) and (f) might suggest. When there are n possible actions,\na mixed strategy is a point in n-dimensional space and the lines become hyperplanes. It’s\nalso possible for some pure strategies for the second player to be dominated by others, so",
  "a mixed strategy is a point in n-dimensional space and the lines become hyperplanes. It’s\nalso possible for some pure strategies for the second player to be dominated by others, so\nthat they are not optimal against any strategy for the ﬁrst player. After removing all such\nstrategies (which might have to be done repeatedly), the optimal choice at the root is the\n7 It is a coincidence that these equations are the same as those for p; the coincidence arises because\nUE(one, two) = UE(two, one) = −3. This also explains why the optimal strategy is the same for both players. Section 17.5.\nDecisions with Multiple Agents: Game Theory\n673\nhighest (or lowest) intersection point of the remaining hyperplanes. Finding this choice is\nan example of a linear programming problem: maximizing an objective function subject to\nlinear constraints. Such problems can be solved by standard techniques in time polynomial\nin the number of actions (and in the number of bits used to specify the reward function, if you\nwant to get technical).\nThe question remains, what should a rational agent actually do in playing a single game\nof Morra? The rational agent will have derived the fact that [7/12: one; 5/12: two] is the\nmaximin equilibrium strategy, and will assume that this is mutual knowledge with a rational\nopponent. The agent could use a 12-sided die or a random number generator to pick randomly\naccording to this mixed strategy, in which case the expected payoff would be -1/12 for E. Or\nthe agent could just decide to play one, or two. In either case, the expected payoff remains\n-1/12 for E. Curiously, unilaterally choosing a particular action does not harm one’s expected\npayoff, but allowing the other agent to know that one has made such a unilateral decision does\naffect the expected payoff, because then the opponent can adjust his strategy accordingly.\nFinding equilibria in non-zero-sum games is somewhat more complicated. The general\napproach has two steps: (1) Enumerate all possible subsets of actions that might form mixed\nstrategies. For example, ﬁrst try all strategy proﬁles where each player uses a single action,\nthen those where each player uses either one or two actions, and so on. This is exponential\nin the number of actions, and so only applies to relatively small games. (2) For each strategy\nproﬁle enumerated in (1), check to see if it is an equilibrium. This is done by solving a set of",
  "in the number of actions, and so only applies to relatively small games. (2) For each strategy\nproﬁle enumerated in (1), check to see if it is an equilibrium. This is done by solving a set of\nequations and inequalities that are similar to the ones used in the zero-sum case. For two play-\ners these equations are linear and can be solved with basic linear programming techniques,\nbut for three or more players they are nonlinear and may be very difﬁcult to solve.\n17.5.2\nRepeated games\nSo far we have looked only at games that last a single move. The simplest kind of multiple-\nmove game is the repeated game, in which players face the same choice repeatedly, but each\nREPEATED GAME\ntime with knowledge of the history of all players’ previous choices. A strategy proﬁle for a\nrepeated game speciﬁes an action choice for each player at each time step for every possible\nhistory of previous choices. As with MDPs, payoffs are additive over time.\nLet’s consider the repeated version of the prisoner’s dilemma. Will Alice and Bob work\ntogether and refuse to testify, knowing they will meet again? The answer depends on the\ndetails of the engagement. For example, suppose Alice and Bob know that they must play\nexactly 100 rounds of prisoner’s dilemma. Then they both know that the 100th round will not\nbe a repeated game—that is, its outcome can have no effect on future rounds—and therefore\nthey will both choose the dominant strategy, testify, in that round. But once the 100th round\nis determined, the 99th round can have no effect on subsequent rounds, so it too will have\na dominant strategy equilibrium at (testify, testify). By induction, both players will choose\ntestify on every round, earning a total jail sentence of 500 years each.\nWe can get different solutions by changing the rules of the interaction. For example,\nsuppose that after each round there is a 99% chance that the players will meet again. Then\nthe expected number of rounds is still 100, but neither player knows for sure which round 674\nChapter\n17.\nMaking Complex Decisions\nwill be the last. Under these conditions, more cooperative behavior is possible. For example,\none equilibrium strategy is for each player to refuse unless the other player has ever played\ntestify. This strategy could be called perpetual punishment. Suppose both players have\nPERPETUAL\nPUNISHMENT\nadopted this strategy, and this is mutual knowledge. Then as long as neither player has played",
  "testify. This strategy could be called perpetual punishment. Suppose both players have\nPERPETUAL\nPUNISHMENT\nadopted this strategy, and this is mutual knowledge. Then as long as neither player has played\ntestify, then at any point in time the expected future total payoff for each player is\n∞\n\f\nt=0\n0.99t · (−1) = −100 .\nA player who deviates from the strategy and chooses testify will gain a score of 0 rather than\n−1 on the very next move, but from then on both players will play testify and the player’s\ntotal expected future payoff becomes\n0 +\n∞\n\f\nt=1\n0.99t · (−5) = −495 .\nTherefore, at every step, there is no incentive to deviate from (refuse, refuse). Perpetual\npunishment is the “mutually assured destruction” strategy of the prisoner’s dilemma: once\neither player decides to testify, it ensures that both players suffer a great deal. But it works\nas a deterrent only if the other player believes you have adopted this strategy—or at least that\nyou might have adopted it.\nOther strategies are more forgiving. The most famous, called tit-for-tat, calls for start-\nTIT-FOR-TAT\ning with refuse and then echoing the other player’s previous move on all subsequent moves.\nSo Alice would refuse as long as Bob refuses and would testify the move after Bob testiﬁed,\nbut would go back to refusing if Bob did. Although very simple, this strategy has proven to\nbe highly robust and effective against a wide variety of strategies.\nWe can also get different solutions by changing the agents, rather than changing the\nrules of engagement. Suppose the agents are ﬁnite-state machines with n states and they\nare playing a game with m > n total steps. The agents are thus incapable of representing\nthe number of remaining steps, and must treat it as an unknown. Therefore, they cannot do\nthe induction, and are free to arrive at the more favorable (refuse, refuse) equilibrium. In\nthis case, ignorance is bliss—or rather, having your opponent believe that you are ignorant is\nbliss. Your success in these repeated games depends on the other player’s perception of you\nas a bully or a simpleton, and not on your actual characteristics.\n17.5.3\nSequential games\nIn the general case, a game consists of a sequence of turns that need not be all the same. Such\ngames are best represented by a game tree, which game theorists call the extensive form. The\nEXTENSIVE FORM\ntree includes all the same information we saw in Section 5.1: an initial state S0, a function",
  "games are best represented by a game tree, which game theorists call the extensive form. The\nEXTENSIVE FORM\ntree includes all the same information we saw in Section 5.1: an initial state S0, a function\nPLAYER(s) that tells which player has the move, a function ACTIONS(s) enumerating the\npossible actions, a function RESULT(s, a) that deﬁnes the transition to a new state, and a\npartial function UTILITY(s, p), which is deﬁned only on terminal states, to give the payoff\nfor each player.\nTo represent stochastic games, such as backgammon, we add a distinguished player,\nchance, that can take random actions. Chance’s “strategy” is part of the deﬁnition of the Section 17.5.\nDecisions with Multiple Agents: Game Theory\n675\ngame, speciﬁed as a probability distribution over actions (the other players get to choose\ntheir own strategy). To represent games with nondeterministic actions, such as billiards, we\nbreak the action into two pieces: the player’s action itself has a deterministic result, and then\nchance has a turn to react to the action in its own capricious way. To represent simultaneous\nmoves, as in the prisoner’s dilemma or two-ﬁnger Morra, we impose an arbitrary order on the\nplayers, but we have the option of asserting that the earlier player’s actions are not observable\nto the subsequent players: e.g., Alice must choose refuse or testify ﬁrst, then Bob chooses,\nbut Bob does not know what choice Alice made at that time (we can also represent the fact\nthat the move is revealed later). However, we assume the players always remember all their\nown previous actions; this assumption is called perfect recall.\nThe key idea of extensive form that sets it apart from the game trees of Chapter 5 is\nthe representation of partial observability. We saw in Section 5.6 that a player in a partially\nobservable game such as Kriegspiel can create a game tree over the space of belief states.\nWith that tree, we saw that in some cases a player can ﬁnd a sequence of moves (a strategy)\nthat leads to a forced checkmate regardless of what actual state we started in, and regardless of\nwhat strategy the opponent uses. However, the techniques of Chapter 5 could not tell a player\nwhat to do when there is no guaranteed checkmate. If the player’s best strategy depends\non the opponent’s strategy and vice versa, then minimax (or alpha–beta) by itself cannot\nﬁnd a solution. The extensive form does allow us to ﬁnd solutions because it represents the",
  "on the opponent’s strategy and vice versa, then minimax (or alpha–beta) by itself cannot\nﬁnd a solution. The extensive form does allow us to ﬁnd solutions because it represents the\nbelief states (game theorists call them information sets) of all players at once. From that\nINFORMATION SETS\nrepresentation we can ﬁnd equilibrium solutions, just as we did with normal-form games.\nAs a simple example of a sequential game, place two agents in the 4 × 3 world of Fig-\nure 17.1 and have them move simultaneously until one agent reaches an exit square, and gets\nthe payoff for that square. If we specify that no movement occurs when the two agents try\nto move into the same square simultaneously (a common problem at many trafﬁc intersec-\ntions), then certain pure strategies can get stuck forever. Thus, agents need a mixed strategy\nto perform well in this game: randomly choose between moving ahead and staying put. This\nis exactly what is done to resolve packet collisions in Ethernet networks.\nNext we’ll consider a very simple variant of poker. The deck has only four cards, two\naces and two kings. One card is dealt to each player. The ﬁrst player then has the option\nto raise the stakes of the game from 1 point to 2, or to check. If player 1 checks, the game\nis over. If he raises, then player 2 has the option to call, accepting that the game is worth 2\npoints, or fold, conceding the 1 point. If the game does not end with a fold, then the payoff\ndepends on the cards: it is zero for both players if they have the same card; otherwise the\nplayer with the king pays the stakes to the player with the ace.\nThe extensive-form tree for this game is shown in Figure 17.13. Nonterminal states are\nshown as circles, with the player to move inside the circle; player 0 is chance. Each action is\ndepicted as an arrow with a label, corresponding to a raise, check, call, or fold, or, for chance,\nthe four possible deals (“AK” means that player 1 gets an ace and player 2 a king). Terminal\nstates are rectangles labeled by their payoff to player 1 and player 2. Information sets are\nshown as labeled dashed boxes; for example, I1,1 is the information set where it is player\n1’s turn, and he knows he has an ace (but does not know what player 2 has). In information\nset I2,1, it is player 2’s turn and she knows that she has an ace and that player 1 has raised, 676\nChapter\n17.\nMaking Complex Decisions\n0 \n1 \n1 \n1 \n1 \n2 \n2 \n2 \n2 \n0,0!\n+1,-1!\n0,0!\n-1,+1!\n1/6: AA\nr \nk \nr \nk \nr \nk \nr \nk \n+1,-1!\n+1,-1!\n+1,-1!",
  "set I2,1, it is player 2’s turn and she knows that she has an ace and that player 1 has raised, 676\nChapter\n17.\nMaking Complex Decisions\n0 \n1 \n1 \n1 \n1 \n2 \n2 \n2 \n2 \n0,0!\n+1,-1!\n0,0!\n-1,+1!\n1/6: AA\nr \nk \nr \nk \nr \nk \nr \nk \n+1,-1!\n+1,-1!\n+1,-1!\n+1,-1!\n0,0!\n+2,-2\n0,0\n-2,+2\nc \nf \nc \nf \nc \nf \nc \nf \n1/3: KA\n1/3: AK\n1/6: KK\n2\nI1,1 \nI1,2 \nI2,1 \nI2,2 \nI2,1 \nFigure 17.13\nExtensive form of a simpliﬁed version of poker.\nbut does not know what card player 1 has. (Due to the limits of two-dimensional paper, this\ninformation set is shown as two boxes rather than one.)\nOne way to solve an extensive game is to convert it to a normal-form game. Recall that\nthe normal form is a matrix, each row of which is labeled with a pure strategy for player 1, and\neach column by a pure strategy for player 2. In an extensive game a pure strategy for player\ni corresponds to an action for each information set involving that player. So in Figure 17.13,\none pure strategy for player 1 is “raise when in I1,1 (that is, when I have an ace), and check\nwhen in I1,2 (when I have a king).” In the payoff matrix below, this strategy is called rk.\nSimilarly, strategy cf for player 2 means “call when I have an ace and fold when I have a\nking.” Since this is a zero-sum game, the matrix below gives only the payoff for player 1;\nplayer 2 always has the opposite payoff:\n2:cc\n2:cf\n2:ff\n2:fc\n1:rr\n0\n-1/6\n1\n7/6\n1:kr\n-1/3\n-1/6\n5/6\n2/3\n1:rk\n1/3\n0\n1/6\n1/2\n1:kk\n0\n0\n0\n0\nThis game is so simple that it has two pure-strategy equilibria, shown in bold: cf for player\n2 and rk or kk for player 1. But in general we can solve extensive games by converting\nto normal form and then ﬁnding a solution (usually a mixed strategy) using standard linear\nprogramming methods. That works in theory. But if a player has I information sets and\na actions per set, then that player will have aI pure strategies. In other words, the size of\nthe normal-form matrix is exponential in the number of information sets, so in practice the Section 17.5.\nDecisions with Multiple Agents: Game Theory\n677\napproach works only for very small game trees, on the order of a dozen states. A game like\nTexas hold’em poker has about 1018 states, making this approach completely infeasible.\nWhat are the alternatives? In Chapter 5 we saw how alpha–beta search could handle\ngames of perfect information with huge game trees by generating the tree incrementally, by\npruning some branches, and by heuristically evaluating nonterminal nodes. But that approach",
  "games of perfect information with huge game trees by generating the tree incrementally, by\npruning some branches, and by heuristically evaluating nonterminal nodes. But that approach\ndoes not work well for games with imperfect information, for two reasons: ﬁrst, it is harder\nto prune, because we need to consider mixed strategies that combine multiple branches, not a\npure strategy that always chooses the best branch. Second, it is harder to heuristically evaluate\na nonterminal node, because we are dealing with information sets, not individual states.\nKoller et al. (1996) come to the rescue with an alternative representation of extensive\ngames, called the sequence form, that is only linear in the size of the tree, rather than ex-\nSEQUENCE FORM\nponential. Rather than represent strategies, it represents paths through the tree; the number\nof paths is equal to the number of terminal nodes. Standard linear programming methods\ncan again be applied to this representation. The resulting system can solve poker variants\nwith 25,000 states in a minute or two. This is an exponential speedup over the normal-form\napproach, but still falls far short of handling full poker, with 1018 states.\nIf we can’t handle 1018 states, perhaps we can simplify the problem by changing the\ngame to a simpler form. For example, if I hold an ace and am considering the possibility that\nthe next card will give me a pair of aces, then I don’t care about the suit of the next card; any\nsuit will do equally well. This suggests forming an abstraction of the game, one in which\nABSTRACTION\nsuits are ignored. The resulting game tree will be smaller by a factor of 4! = 24. Suppose I\ncan solve this smaller game; how will the solution to that game relate to the original game?\nIf no player is going for a ﬂush (or blufﬁng so), then the suits don’t matter to any player, and\nthe solution for the abstraction will also be a solution for the original game. However, if any\nplayer is contemplating a ﬂush, then the abstraction will be only an approximate solution (but\nit is possible to compute bounds on the error).\nThere are many opportunities for abstraction. For example, at the point in a game where\neach player has two cards, if I hold a pair of queens, then the other players’ hands could be\nabstracted into three classes: better (only a pair of kings or a pair of aces), same (pair of\nqueens) or worse (everything else). However, this abstraction might be too coarse. A better",
  "abstracted into three classes: better (only a pair of kings or a pair of aces), same (pair of\nqueens) or worse (everything else). However, this abstraction might be too coarse. A better\nabstraction would divide worse into, say, medium pair (nines through jacks), low pair, and\nno pair. These examples are abstractions of states; it is also possible to abstract actions. For\nexample, instead of having a bet action for each integer from 1 to 1000, we could restrict the\nbets to 100, 101, 102 and 103. Or we could cut out one of the rounds of betting altogether.\nWe can also abstract over chance nodes, by considering only a subset of the possible deals.\nThis is equivalent to the rollout technique used in Go programs. Putting all these abstractions\ntogether, we can reduce the 1018 states of poker to 107 states, a size that can be solved with\ncurrent techniques.\nPoker programs based on this approach can easily defeat novice and some experienced\nhuman players, but are not yet at the level of master players. Part of the problem is that\nthe solution these programs approximate—the equilibrium solution—is optimal only against\nan opponent who also plays the equilibrium strategy. Against fallible human players it is\nimportant to be able to exploit an opponent’s deviation from the equilibrium strategy. As 678\nChapter\n17.\nMaking Complex Decisions\nGautam Rao (aka “The Count”), the world’s leading online poker player, said (Billings et al.,\n2003), “You have a very strong program. Once you add opponent modeling to it, it will kill\neveryone.” However, good models of human fallability remain elusive.\nIn a sense, extensive game form is the one of the most complete representations we have\nseen so far: it can handle partially observable, multiagent, stochastic, sequential, dynamic\nenvironments—most of the hard cases from the list of environment properties on page 42.\nHowever, there are two limitations of game theory. First, it does not deal well with continuous\nstates and actions (although there have been some extensions to the continuous case; for\nexample, the theory of Cournot competition uses game theory to solve problems where two\nCOURNOT\nCOMPETITION\ncompanies choose prices for their products from a continuous space). Second, game theory\nassumes the game is known. Parts of the game may be speciﬁed as unobservable to some of\nthe players, but it must be known what parts are unobservable. In cases in which the players",
  "assumes the game is known. Parts of the game may be speciﬁed as unobservable to some of\nthe players, but it must be known what parts are unobservable. In cases in which the players\nlearn the unknown structure of the game over time, the model begins to break down. Let’s\nexamine each source of uncertainty, and whether each can be represented in game theory.\nActions: There is no easy way to represent a game where the players have to discover\nwhat actions are available. Consider the game between computer virus writers and security\nexperts. Part of the problem is anticipating what action the virus writers will try next.\nStrategies: Game theory is very good at representing the idea that the other players’\nstrategies are initially unknown—as long as we assume all agents are rational. The theory\nitself does not say what to do when the other players are less than fully rational. The notion\nof a Bayes–Nash equilibrium partially addresses this point: it is an equilibrium with respect\nBAYES–NASH\nEQUILIBRIUM\nto a player’s prior probability distribution over the other players’ strategies—in other words,\nit expresses a player’s beliefs about the other players’ likely strategies.\nChance: If a game depends on the roll of a die, it is easy enough to model a chance node\nwith uniform distribution over the outcomes. But what if it is possible that the die is unfair?\nWe can represent that with another chance node, higher up in the tree, with two branches for\n“die is fair” and “die is unfair,” such that the corresponding nodes in each branch are in the\nsame information set (that is, the players don’t know if the die is fair or not). And what if we\nsuspect the other opponent does know? Then we add another chance node, with one branch\nrepresenting the case where the opponent does know, and one where he doesn’t.\nUtilities: What if we don’t know our opponent’s utilities? Again, that can be modeled\nwith a chance node, such that the other agent knows its own utilities in each branch, but we\ndon’t. But what if we don’t know our own utilities? For example, how do I know if it is\nrational to order the Chef’s salad if I don’t know how much I will like it? We can model that\nwith yet another chance node specifying an unobservable “intrinsic quality” of the salad.\nThus, we see that game theory is good at representing most sources of uncertainty—but\nat the cost of doubling the size of the tree every time we add another node; a habit which",
  "Thus, we see that game theory is good at representing most sources of uncertainty—but\nat the cost of doubling the size of the tree every time we add another node; a habit which\nquickly leads to intractably large trees. Because of these and other problems, game theory\nhas been used primarily to analyze environments that are at equilibrium, rather than to control\nagents within an environment. Next we shall see how it can help design environments. Section 17.6.\nMechanism Design\n679\n17.6\nMECHANISM DESIGN\nIn the previous section, we asked, “Given a game, what is a rational strategy?” In this sec-\ntion, we ask, “Given that agents pick rational strategies, what game should we design?” More\nspeciﬁcally, we would like to design a game whose solutions, consisting of each agent pursu-\ning its own rational strategy, result in the maximization of some global utility function. This\nproblem is called mechanism design, or sometimes inverse game theory. Mechanism de-\nMECHANISM DESIGN\nsign is a staple of economics and political science. Capitalism 101 says that if everyone tries\nto get rich, the total wealth of society will increase. But the examples we will discuss show\nthat proper mechanism design is necessary to keep the invisible hand on track. For collections\nof agents, mechanism design allows us to construct smart systems out of a collection of more\nlimited systems—even uncooperative systems—in much the same way that teams of humans\ncan achieve goals beyond the reach of any individual.\nExamples of mechanism design include auctioning off cheap airline tickets, routing\nTCP packets between computers, deciding how medical interns will be assigned to hospitals,\nand deciding how robotic soccer players will cooperate with their teammates. Mechanism\ndesign became more than an academic subject in the 1990s when several nations, faced with\nthe problem of auctioning off licenses to broadcast in various frequency bands, lost hundreds\nof millions of dollars in potential revenue as a result of poor mechanism design. Formally,\na mechanism consists of (1) a language for describing the set of allowable strategies that\nMECHANISM\nagents may adopt, (2) a distinguished agent, called the center, that collects reports of strategy\nCENTER\nchoices from the agents in the game, and (3) an outcome rule, known to all agents, that the\ncenter uses to determine the payoffs to each agent, given their strategy choices.\n17.6.1\nAuctions",
  "CENTER\nchoices from the agents in the game, and (3) an outcome rule, known to all agents, that the\ncenter uses to determine the payoffs to each agent, given their strategy choices.\n17.6.1\nAuctions\nLet’s consider auctions ﬁrst. An auction is a mechanism for selling some goods to members\nAUCTION\nof a pool of bidders. For simplicity, we concentrate on auctions with a single item for sale.\nEach bidder i has a utility value vi for having the item. In some cases, each bidder has a\nprivate value for the item. For example, the ﬁrst item sold on eBay was a broken laser\npointer, which sold for $14.83 to a collector of broken laser pointers. Thus, we know that the\ncollector has vi ≥$14.83, but most other people would have vj ≪$14.83. In other cases,\nsuch as auctioning drilling rights for an oil tract, the item has a common value—the tract\nwill produce some amount of money, X, and all bidders value a dollar equally—but there is\nuncertainty as to what the actual value of X is. Different bidders have different information,\nand hence different estimates of the item’s true value. In either case, bidders end up with their\nown vi. Given vi, each bidder gets a chance, at the appropriate time or times in the auction,\nto make a bid bi. The highest bid, bmax wins the item, but the price paid need not be bmax;\nthat’s part of the mechanism design.\nThe best-known auction mechanism is the ascending-bid,8 or English auction, in\nASCENDING-BID\nENGLISH AUCTION\nwhich the center starts by asking for a minimum (or reserve) bid bmin. If some bidder is\n8 The word “auction” comes from the Latin augere, to increase. 680\nChapter\n17.\nMaking Complex Decisions\nwilling to pay that amount, the center then asks for bmin + d, for some increment d, and\ncontinues up from there. The auction ends when nobody is willing to bid anymore; then the\nlast bidder wins the item, paying the price he bid.\nHow do we know if this is a good mechanism? One goal is to maximize expected\nrevenue for the seller. Another goal is to maximize a notion of global utility. These goals\noverlap to some extent, because one aspect of maximizing global utility is to ensure that the\nwinner of the auction is the agent who values the item the most (and thus is willing to pay\nthe most). We say an auction is efﬁcient if the goods go to the agent who values them most.\nEFFICIENT\nThe ascending-bid auction is usually both efﬁcient and revenue maximizing, but if the reserve",
  "the most). We say an auction is efﬁcient if the goods go to the agent who values them most.\nEFFICIENT\nThe ascending-bid auction is usually both efﬁcient and revenue maximizing, but if the reserve\nprice is set too high, the bidder who values it most may not bid, and if the reserve is set too\nlow, the seller loses net revenue.\nProbably the most important things that an auction mechanism can do is encourage a\nsufﬁcient number of bidders to enter the game and discourage them from engaging in collu-\nsion. Collusion is an unfair or illegal agreement by two or more bidders to manipulate prices.\nCOLLUSION\nIt can happen in secret backroom deals or tacitly, within the rules of the mechanism.\nFor example, in 1999, Germany auctioned ten blocks of cell-phone spectrum with a\nsimultaneous auction (bids were taken on all ten blocks at the same time), using the rule that\nany bid must be a minimum of a 10% raise over the previous bid on a block. There were only\ntwo credible bidders, and the ﬁrst, Mannesman, entered the bid of 20 million deutschmark\non blocks 1-5 and 18.18 million on blocks 6-10. Why 18.18M? One of T-Mobile’s managers\nsaid they “interpreted Mannesman’s ﬁrst bid as an offer.” Both parties could compute that\na 10% raise on 18.18M is 19.99M; thus Mannesman’s bid was interpreted as saying “we\ncan each get half the blocks for 20M; let’s not spoil it by bidding the prices up higher.”\nAnd in fact T-Mobile bid 20M on blocks 6-10 and that was the end of the bidding. The\nGerman government got less than they expected, because the two competitors were able to\nuse the bidding mechanism to come to a tacit agreement on how not to compete. From\nthe government’s point of view, a better result could have been obtained by any of these\nchanges to the mechanism: a higher reserve price; a sealed-bid ﬁrst-price auction, so that\nthe competitors could not communicate through their bids; or incentives to bring in a third\nbidder. Perhaps the 10% rule was an error in mechanism design, because it facilitated the\nprecise signaling from Mannesman to T-Mobile.\nIn general, both the seller and the global utility function beneﬁt if there are more bid-\nders, although global utility can suffer if you count the cost of wasted time of bidders that\nhave no chance of winning. One way to encourage more bidders is to make the mechanism\neasier for them. After all, if it requires too much research or computation on the part of the",
  "have no chance of winning. One way to encourage more bidders is to make the mechanism\neasier for them. After all, if it requires too much research or computation on the part of the\nbidders, they may decide to take their money elsewhere. So it is desirable that the bidders\nhave a dominant strategy. Recall that “dominant” means that the strategy works against all\nother strategies, which in turn means that an agent can adopt it without regard for the other\nstrategies. An agent with a dominant strategy can just bid, without wasting time contemplat-\ning other agents’ possible strategies. A mechanism where agents have a dominant strategy\nis called a strategy-proof mechanism. If, as is usually the case, that strategy involves the\nSTRATEGY-PROOF\nbidders revealing their true value, vi, then it is called a truth-revealing, or truthful, auction;\nTRUTH-REVEALING\nthe term incentive compatible is also used. The revelation principle states that any mecha-\nREVELATION\nPRINCIPLE Section 17.6.\nMechanism Design\n681\nnism can be transformed into an equivalent truth-revealing mechanism, so part of mechanism\ndesign is ﬁnding these equivalent mechanisms.\nIt turns out that the ascending-bid auction has most of the desirable properties. The\nbidder with the highest value vi gets the goods at a price of bo + d, where bo is the highest\nbid among all the other agents and d is the auctioneer’s increment.9 Bidders have a simple\ndominant strategy: keep bidding as long as the current cost is below your vi. The mechanism\nis not quite truth-revealing, because the winning bidder reveals only that his vi ≥bo + d; we\nhave a lower bound on vi but not an exact amount.\nA disadvantage (from the point of view of the seller) of the ascending-bid auction is\nthat it can discourage competition. Suppose that in a bid for cell-phone spectrum there is\none advantaged company that everyone agrees would be able to leverage existing customers\nand infrastructure, and thus can make a larger proﬁt than anyone else. Potential competitors\ncan see that they have no chance in an ascending-bid auction, because the advantaged com-\npany can always bid higher. Thus, the competitors may not enter at all, and the advantaged\ncompany ends up winning at the reserve price.\nAnother negative property of the English auction is its high communication costs. Either\nthe auction takes place in one room or all bidders have to have high-speed, secure communi-",
  "company ends up winning at the reserve price.\nAnother negative property of the English auction is its high communication costs. Either\nthe auction takes place in one room or all bidders have to have high-speed, secure communi-\ncation lines; in either case they have to have the time available to go through several rounds of\nbidding. An alternative mechanism, which requires much less communication, is the sealed-\nbid auction. Each bidder makes a single bid and communicates it to the auctioneer, without\nSEALED-BID\nAUCTION\nthe other bidders seeing it. With this mechanism, there is no longer a simple dominant strat-\negy. If your value is vi and you believe that the maximum of all the other agents’ bids will\nbe bo, then you should bid bo + ϵ, for some small ϵ, if that is less than vi. Thus, your bid\ndepends on your estimation of the other agents’ bids, requiring you to do more work. Also,\nnote that the agent with the highest vi might not win the auction. This is offset by the fact\nthat the auction is more competitive, reducing the bias toward an advantaged bidder.\nA small change in the mechanism for sealed-bid auctions produces the sealed-bid\nsecond-price auction, also known as a Vickrey auction.10 In such auctions, the winner pays\nSEALED-BID\nSECOND-PRICE\nAUCTION\nVICKREY AUCTION\nthe price of the second-highest bid, bo, rather than paying his own bid. This simple modiﬁ-\ncation completely eliminates the complex deliberations required for standard (or ﬁrst-price)\nsealed-bid auctions, because the dominant strategy is now simply to bid vi; the mechanism is\ntruth-revealing. Note that the utility of agent i in terms of his bid bi, his value vi, and the best\nbid among the other agents, bo, is\nui =\n\u0018 (vi −bo)\nif bi > bo\n0\notherwise.\nTo see that bi = vi is a dominant strategy, note that when (vi −bo) is positive, any bid\nthat wins the auction is optimal, and bidding vi in particular wins the auction. On the other\nhand, when (vi −bo) is negative, any bid that loses the auction is optimal, and bidding vi in\n9 There is actually a small chance that the agent with highest vi fails to get the goods, in the case in which\nbo < vi < bo + d. The chance of this can be made arbitrarily small by decreasing the increment d.\n10 Named after William Vickrey (1914–1996), who won the 1996 Nobel Prize in economics for this work and\ndied of a heart attack three days later. 682\nChapter\n17.\nMaking Complex Decisions",
  "10 Named after William Vickrey (1914–1996), who won the 1996 Nobel Prize in economics for this work and\ndied of a heart attack three days later. 682\nChapter\n17.\nMaking Complex Decisions\nparticular loses the auction. So bidding vi is optimal for all possible values of bo, and in fact,\nvi is the only bid that has this property. Because of its simplicity and the minimal computation\nrequirements for both seller and bidders, the Vickrey auction is widely used in constructing\ndistributed AI systems. Also, Internet search engines conduct over a billion auctions a day\nto sell advertisements along with their search results, and online auction sites handle $100\nbillion a year in goods, all using variants of the Vickrey auction. Note that the expected value\nto the seller is bo, which is the same expected return as the limit of the English auction as\nthe increment d goes to zero. This is actually a very general result: the revenue equivalence\ntheorem states that, with a few minor caveats, any auction mechanism where risk-neutral\nREVENUE\nEQUIVALENCE\nTHEOREM\nbidders have values vi known only to themselves (but know a probability distribution from\nwhich those values are sampled), will yield the same expected revenue. This principle means\nthat the various mechanisms are not competing on the basis of revenue generation, but rather\non other qualities.\nAlthough the second-price auction is truth-revealing, it turns out that extending the idea\nto multiple goods and using a next-price auction is not truth-revealing. Many Internet search\nengines use a mechanism where they auction k slots for ads on a page. The highest bidder\nwins the top spot, the second highest gets the second spot, and so on. Each winner pays the\nprice bid by the next-lower bidder, with the understanding that payment is made only if the\nsearcher actually clicks on the ad. The top slots are considered more valuable because they\nare more likely to be noticed and clicked on. Imagine that three bidders, b1, b2 and b3, have\nvaluations for a click of v1 = 200, v2 = 180, and v3 = 100, and thatk = 2 slots are available,\nwhere it is known that the top spot is clicked on 5% of the time and the bottom spot 2%. If\nall bidders bid truthfully, then b1 wins the top slot and pays 180, and has an expected return\nof (200 −180) × 0.05 = 1. The second slot goes to b2. But b1 can see that if she were to bid\nanything in the range 101–179, she would concede the top slot to b2, win the second slot, and",
  "of (200 −180) × 0.05 = 1. The second slot goes to b2. But b1 can see that if she were to bid\nanything in the range 101–179, she would concede the top slot to b2, win the second slot, and\nyield an expected return of (200−100)×.02 = 2. Thus, b1 can double her expected return by\nbidding less than her true value in this case. In general, bidders in this multislot auction must\nspend a lot of energy analyzing the bids of others to determine their best strategy; there is no\nsimple dominant strategy. Aggarwal et al. (2006) show that there is a unique truthful auction\nmechanism for this multislot problem, in which the winner of slot j pays the full price for\nslot j just for those additional clicks that are available at slot j and not at slot j + 1. The\nwinner pays the price for the lower slot for the remaining clicks. In our example, b1 would\nbid 200 truthfully, and would pay 180 for the additional .05 −.02 = .03 clicks in the top slot,\nbut would pay only the cost of the bottom slot, 100, for the remaining .02 clicks. Thus, the\ntotal return to b1 would be (200 −180) × .03 + (200 −100) × .02 = 2.6.\nAnother example of where auctions can come into play within AI is when a collection\nof agents are deciding whether to cooperate on a joint plan. Hunsberger and Grosz (2000)\nshow that this can be accomplished efﬁciently with an auction in which the agents bid for\nroles in the joint plan. Section 17.6.\nMechanism Design\n683\n17.6.2\nCommon goods\nNow let’s consider another type of game, in which countries set their policy for controlling\nair pollution. Each country has a choice: they can reduce pollution at a cost of -10 points for\nimplementing the necessary changes, or they can continue to pollute, which gives them a net\nutility of -5 (in added health costs, etc.) and also contributes -1 points to every other country\n(because the air is shared across countries). Clearly, the dominant strategy for each country\nis “continue to pollute,” but if there are 100 countries and each follows this policy, then each\ncountry gets a total utility of -104, whereas if every country reduced pollution, they would\neach have a utility of -10. This situation is called the tragedy of the commons: if nobody\nTRAGEDY OF THE\nCOMMONS\nhas to pay for using a common resource, then it tends to be exploited in a way that leads to\na lower total utility for all agents. It is similar to the prisoner’s dilemma: there is another",
  "TRAGEDY OF THE\nCOMMONS\nhas to pay for using a common resource, then it tends to be exploited in a way that leads to\na lower total utility for all agents. It is similar to the prisoner’s dilemma: there is another\nsolution to the game that is better for all parties, but there appears to be no way for rational\nagents to arrive at that solution.\nThe standard approach for dealing with the tragedy of the commons is to change the\nmechanism to one that charges each agent for using the commons. More generally, we need\nto ensure that all externalities—effects on global utility that are not recognized in the in-\nEXTERNALITIES\ndividual agents’ transactions—are made explicit. Setting the prices correctly is the difﬁcult\npart. In the limit, this approach amounts to creating a mechanism in which each agent is\neffectively required to maximize global utility, but can do so by making a local decision. For\nthis example, a carbon tax would be an example of a mechanism that charges for use of the\ncommons in a way that, if implemented well, maximizes global utility.\nAs a ﬁnal example, consider the problem of allocating some common goods. Suppose a\ncity decides it wants to install some free wireless Internet transceivers. However, the number\nof transceivers they can afford is less than the number of neighborhoods that want them. The\ncity wants to allocate the goods efﬁciently, to the neighborhoods that would value them the\nmost. That is, they want to maximize the global utility V = \u0002\ni vi. The problem is that if\nthey just ask each neighborhood council “how much do you value this free gift?” they would\nall have an incentive to lie, and report a high value. It turns out there is a mechanism, known\nas the Vickrey-Clarke-Groves, or VCG, mechanism, that makes it a dominant strategy for\nVICKREY-CLARKE-\nGROVES\nVCG\neach agent to report its true utility and that achieves an efﬁcient allocation of the goods. The\ntrick is that each agent pays a tax equivalent to the loss in global utility that occurs because\nof the agent’s presence in the game. The mechanism works like this:\n1. The center asks each agent to report its value for receiving an item. Call this bi.\n2. The center allocates the goods to a subset of the bidders. We call this subset A, and use\nthe notation bi(A) to mean the result to i under this allocation: bi if i is in A (that is, i\nis a winner), and 0 otherwise. The center chooses A to maximize total reported utility\nB = \u0002\ni bi(A).",
  "the notation bi(A) to mean the result to i under this allocation: bi if i is in A (that is, i\nis a winner), and 0 otherwise. The center chooses A to maximize total reported utility\nB = \u0002\ni bi(A).\n3. The center calculates (for each i) the sum of the reported utilities for all the winners\nexcept i. We use the notation B−i = \u0002\nj̸=i bj(A). The center also computes (for each\ni) the allocation that would maximize total global utility if i were not in the game; call\nthat sum W−i.\n4. Each agent i pays a tax equal to W−i −B−i. 684\nChapter\n17.\nMaking Complex Decisions\nIn this example, the VCG rule means that each winner would pay a tax equal to the highest\nreported value among the losers. That is, if I report my value as 5, and that causes someone\nwith value 2 to miss out on an allocation, then I pay a tax of 2. All winners should be happy\nbecause they pay a tax that is less than their value, and all losers are as happy as they can be,\nbecause they value the goods less than the required tax.\nWhy is it that this mechanism is truth-revealing? First, consider the payoff to agent i,\nwhich is the value of getting an item, minus the tax:\nvi(A) −(W−i −B−i) .\n(17.14)\nHere we distinguish the agent’s true utility, vi, from his reported utility bi (but we are trying\nto show that a dominant strategy is bi = vi). Agent i knows that the center will maximize\nglobal utility using the reported values,\n\f\nj\nbj(A) = bi(A) +\n\f\nj̸=i\nbj(A)\nwhereas agent i wants the center to maximize (17.14), which can be rewritten as\nvi(A) +\n\f\nj̸=i\nbj(A) −W−i .\nSince agent i cannot affect the value of W−i (it depends only on the other agents), the only\nway i can make the center optimize what i wants is to report the true utility, bi = vi.\n17.7\nSUMMARY\nThis chapter shows how to use knowledge about the world to make decisions even when the\noutcomes of an action are uncertain and the rewards for acting might not be reaped until many\nactions have passed. The main points are as follows:\n• Sequential decision problems in uncertain environments, also called Markov decision\nprocesses, or MDPs, are deﬁned by a transition model specifying the probabilistic\noutcomes of actions and a reward function specifying the reward in each state.\n• The utility of a state sequence is the sum of all the rewards over the sequence, possibly\ndiscounted over time. The solution of an MDP is a policy that associates a decision\nwith every state that the agent might reach. An optimal policy maximizes the utility of",
  "discounted over time. The solution of an MDP is a policy that associates a decision\nwith every state that the agent might reach. An optimal policy maximizes the utility of\nthe state sequences encountered when it is executed.\n• The utility of a state is the expected utility of the state sequences encountered when\nan optimal policy is executed, starting in that state. The value iteration algorithm for\nsolving MDPs works by iteratively solving the equations relating the utility of each state\nto those of its neighbors.\n• Policy iteration alternates between calculating the utilities of states under the current\npolicy and improving the current policy with respect to the current utilities.\n• Partially observable MDPs, or POMDPs, are much more difﬁcult to solve than are\nMDPs. They can be solved by conversion to an MDP in the continuous space of belief Bibliographical and Historical Notes\n685\nstates; both value iteration and policy iteration algorithms have been devised. Optimal\nbehavior in POMDPs includes information gathering to reduce uncertainty and there-\nfore make better decisions in the future.\n• A decision-theoretic agent can be constructed for POMDP environments. The agent\nuses a dynamic decision network to represent the transition and sensor models, to\nupdate its belief state, and to project forward possible action sequences.\n• Game theory describes rational behavior for agents in situations in which multiple\nagents interact simultaneously. Solutions of games are Nash equilibria—strategy pro-\nﬁles in which no agent has an incentive to deviate from the speciﬁed strategy.\n• Mechanism design can be used to set the rules by which agents will interact, in order\nto maximize some global utility through the operation of individually rational agents.\nSometimes, mechanisms exist that achieve this goal without requiring each agent to\nconsider the choices made by other agents.\nWe shall return to the world of MDPs and POMDP in Chapter 21, when we study rein-\nforcement learning methods that allow an agent to improve its behavior from experience in\nsequential, uncertain environments.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nRichard Bellman developed the ideas underlying the modern approach to sequential decision\nproblems while working at the RAND Corporation beginning in 1949. According to his au-\ntobiography (Bellman, 1984), he coined the exciting term “dynamic programming” to hide\nfrom a research-phobic Secretary of Defense, Charles Wilson, the fact that his group was",
  "tobiography (Bellman, 1984), he coined the exciting term “dynamic programming” to hide\nfrom a research-phobic Secretary of Defense, Charles Wilson, the fact that his group was\ndoing mathematics. (This cannot be strictly true, because his ﬁrst paper using the term (Bell-\nman, 1952) appeared before Wilson became Secretary of Defense in 1953.) Bellman’s book,\nDynamic Programming (1957), gave the new ﬁeld a solid foundation and introduced the basic\nalgorithmic approaches. Ron Howard’s Ph.D. thesis (1960) introduced policy iteration and\nthe idea of average reward for solving inﬁnite-horizon problems. Several additional results\nwere introduced by Bellman and Dreyfus (1962). Modiﬁed policy iteration is due to van\nNunen (1976) and Puterman and Shin (1978). Asynchronous policy iteration was analyzed\nby Williams and Baird (1993), who also proved the policy loss bound in Equation (17.9). The\nanalysis of discounting in terms of stationary preferences is due to Koopmans (1972). The\ntexts by Bertsekas (1987), Puterman (1994), and Bertsekas and Tsitsiklis (1996) provide a\nrigorous introduction to sequential decision problems. Papadimitriou and Tsitsiklis (1987)\ndescribe results on the computational complexity of MDPs.\nSeminal work by Sutton (1988) and Watkins (1989) on reinforcement learning methods\nfor solving MDPs played a signiﬁcant role in introducing MDPs into the AI community, as\ndid the later survey by Barto et al. (1995). (Earlier work by Werbos (1977) contained many\nsimilar ideas, but was not taken up to the same extent.) The connection between MDPs and\nAI planning problems was made ﬁrst by Sven Koenig (1991), who showed how probabilistic\nSTRIPS operators provide a compact representation for transition models (see also Wellman, 686\nChapter\n17.\nMaking Complex Decisions\n1990b). Work by Dean et al. (1993) and Tash and Russell (1994) attempted to overcome\nthe combinatorics of large state spaces by using a limited search horizon and abstract states.\nHeuristics based on the value of information can be used to select areas of the state space\nwhere a local expansion of the horizon will yield a signiﬁcant improvement in decision qual-\nity. Agents using this approach can tailor their effort to handle time pressure and generate\nsome interesting behaviors such as using familiar “beaten paths” to ﬁnd their way around the\nstate space quickly without having to recompute optimal decisions at each point.",
  "some interesting behaviors such as using familiar “beaten paths” to ﬁnd their way around the\nstate space quickly without having to recompute optimal decisions at each point.\nAs one might expect, AI researchers have pushed MDPs in the direction of more ex-\npressive representations that can accommodate much larger problems than the traditional\natomic representations based on transition matrices. The use of a dynamic Bayesian network\nto represent transition models was an obvious idea, but work on factored MDPs (Boutilier\nFACTORED MDP\net al., 2000; Koller and Parr, 2000; Guestrin et al., 2003b) extends the idea to structured\nrepresentations of the value function with provable improvements in complexity. Relational\nMDPs (Boutilier et al., 2001; Guestrin et al., 2003a) go one step further, using structured\nRELATIONAL MDP\nrepresentations to handle domains with many related objects.\nThe observation that a partially observable MDP can be transformed into a regular MDP\nover belief states is due to Astrom (1965) and Aoki (1965). The ﬁrst complete algorithm for\nthe exact solution of POMDPs—essentially the value iteration algorithm presented in this\nchapter—was proposed by Edward Sondik (1971) in his Ph.D. thesis. (A later journal paper\nby Smallwood and Sondik (1973) contains some errors, but is more accessible.) Lovejoy\n(1991) surveyed the ﬁrst twenty-ﬁve years of POMDP research, reaching somewhat pes-\nsimistic conclusions about the feasibility of solving large problems. The ﬁrst signiﬁcant\ncontribution within AI was the Witness algorithm (Cassandra et al., 1994; Kaelbling et al.,\n1998), an improved version of POMDP value iteration. Other algorithms soon followed, in-\ncluding an approach due to Hansen (1998) that constructs a policy incrementally in the form\nof a ﬁnite-state automaton. In this policy representation, the belief state corresponds directly\nto a particular state in the automaton. More recent work in AI has focused on point-based\nvalue iteration methods that, at each iteration, generate conditional plans and α-vectors for\na ﬁnite set of belief states rather than for the entire belief space. Lovejoy (1991) proposed\nsuch an algorithm for a ﬁxed grid of points, an approach taken also by Bonet (2002). An\ninﬂuential paper by Pineau et al. (2003) suggested generating reachable points by simulat-\ning trajectories in a somewhat greedy fashion; Spaan and Vlassis (2005) observe that one",
  "inﬂuential paper by Pineau et al. (2003) suggested generating reachable points by simulat-\ning trajectories in a somewhat greedy fashion; Spaan and Vlassis (2005) observe that one\nneed generate plans for only a small, randomly selected subset of points to improve on the\nplans from the previous iteration for all points in the set. Current point-based methods—\nsuch as point-based policy iteration (Ji et al., 2007)—can generate near-optimal solutions for\nPOMDPs with thousands of states. Because POMDPs are PSPACE-hard (Papadimitriou and\nTsitsiklis, 1987), further progress may require taking advantage of various kinds of structure\nwithin a factored representation.\nThe online approach—using look-ahead search to select an action for the current belief\nstate—was ﬁrst examined by Satia and Lave (1973). The use of sampling at chance nodes\nwas explored analytically by Kearns et al. (2000) and Ng and Jordan (2000). The basic\nideas for an agent architecture using dynamic decision networks were proposed by Dean\nand Kanazawa (1989a). The book Planning and Control by Dean and Wellman (1991) goes Bibliographical and Historical Notes\n687\ninto much greater depth, making connections between DBN/DDN models and the classical\ncontrol literature on ﬁltering. Tatman and Shachter (1990) showed how to apply dynamic\nprogramming algorithms to DDN models. Russell (1998) explains various ways in which\nsuch agents can be scaled up and identiﬁes a number of open research issues.\nThe roots of game theory can be traced back to proposals made in the 17th century\nby Christiaan Huygens and Gottfried Leibniz to study competitive and cooperative human\ninteractions scientiﬁcally and mathematically. Throughout the 19th century, several leading\neconomists created simple mathematical examples to analyze particular examples of com-\npetitive situations. The ﬁrst formal results in game theory are due to Zermelo (1913) (who\nhad, the year before, suggested a form of minimax search for games, albeit an incorrect one).\nEmile Borel (1921) introduced the notion of a mixed strategy. John von Neumann (1928)\nproved that every two-person, zero-sum game has a maximin equilibrium in mixed strategies\nand a well-deﬁned value. Von Neumann’s collaboration with the economist Oskar Morgen-\nstern led to the publication in 1944 of the Theory of Games and Economic Behavior, the\ndeﬁning book for game theory. Publication of the book was delayed by the wartime paper",
  "stern led to the publication in 1944 of the Theory of Games and Economic Behavior, the\ndeﬁning book for game theory. Publication of the book was delayed by the wartime paper\nshortage until a member of the Rockefeller family personally subsidized its publication.\nIn 1950, at the age of 21, John Nash published his ideas concerning equilibria in general\n(non-zero-sum) games. His deﬁnition of an equilibrium solution, although originating in the\nwork of Cournot (1838), became known as Nash equilibrium. After a long delay because\nof the schizophrenia he suffered from 1959 onward, Nash was awarded the Nobel Memorial\nPrize in Economics (along with Reinhart Selten and John Harsanyi) in 1994. The Bayes–Nash\nequilibrium is described by Harsanyi (1967) and discussed by Kadane and Larkey (1982).\nSome issues in the use of game theory for agent control are covered by Binmore (1982).\nThe prisoner’s dilemma was invented as a classroom exercise by Albert W. Tucker in\n1950 (based on an example by Merrill Flood and Melvin Dresher) and is covered extensively\nby Axelrod (1985) and Poundstone (1993). Repeated games were introduced by Luce and\nRaiffa (1957), and games of partial information in extensive form by Kuhn (1953). The ﬁrst\npractical algorithm for sequential, partial-information games was developed within AI by\nKoller et al. (1996); the paper by Koller and Pfeffer (1997) provides a readable introduction\nto the ﬁeld and describe a working system for representing and solving sequential games.\nThe use of abstraction to reduce a game tree to a size that can be solved with Koller’s\ntechnique is discussed by Billings et al. (2003). Bowling et al. (2008) show how to use\nimportance sampling to get a better estimate of the value of a strategy. Waugh et al. (2009)\nshow that the abstraction approach is vulnerable to making systematic errors in approximating\nthe equilibrium solution, meaning that the whole approach is on shaky ground: it works for\nsome games but not others. Korb et al. (1999) experiment with an opponent model in the\nform of a Bayesian network. It plays ﬁve-card stud about as well as experienced humans.\n(Zinkevich et al., 2008) show how an approach that minimizes regret can ﬁnd approximate\nequilibria for abstractions with 1012 states, 100 times more than previous methods.\nGame theory and MDPs are combined in the theory of Markov games, also called\nstochastic games (Littman, 1994; Hu and Wellman, 1998). Shapley (1953) actually described",
  "Game theory and MDPs are combined in the theory of Markov games, also called\nstochastic games (Littman, 1994; Hu and Wellman, 1998). Shapley (1953) actually described\nthe value iteration algorithm independently of Bellman, but his results were not widely ap-\npreciated, perhaps because they were presented in the context of Markov games. Evolu- 688\nChapter\n17.\nMaking Complex Decisions\ntionary game theory (Smith, 1982; Weibull, 1995) looks at strategy drift over time: if your\nopponent’s strategy is changing, how should you react? Textbooks on game theory from\nan economics point of view include those by Myerson (1991), Fudenberg and Tirole (1991),\nOsborne (2004), and Osborne and Rubinstein (1994); Mailath and Samuelson (2006) concen-\ntrate on repeated games. From an AI perspective we have Nisan et al. (2007), Leyton-Brown\nand Shoham (2008), and Shoham and Leyton-Brown (2009).\nThe 2007 Nobel Memorial Prize in Economics went to Hurwicz, Maskin, and Myerson\n“for having laid the foundations of mechanism design theory” (Hurwicz, 1973). The tragedy\nof the commons, a motivating problem for the ﬁeld, was presented by Hardin (1968). The rev-\nelation principle is due to Myerson (1986), and the revenue equivalence theorem was devel-\noped independently by Myerson (1981) and Riley and Samuelson (1981). Two economists,\nMilgrom (1997) and Klemperer (2002), write about the multibillion-dollar spectrum auctions\nthey were involved in.\nMechanism design is used in multiagent planning (Hunsberger and Grosz, 2000; Stone\net al., 2009) and scheduling (Rassenti et al., 1982). Varian (1995) gives a brief overview with\nconnections to the computer science literature, and Rosenschein and Zlotkin (1994) present a\nbook-length treatment with applications to distributed AI. Related work on distributed AI also\ngoes under other names, including collective intelligence (Tumer and Wolpert, 2000; Segaran,\n2007) and market-based control (Clearwater, 1996). Since 2001 there has been an annual\nTrading Agents Competition (TAC), in which agents try to make the best proﬁt on a series\nof auctions (Wellman et al., 2001; Arunachalam and Sadeh, 2005). Papers on computational\nissues in auctions often appear in the ACM Conferences on Electronic Commerce.\nEXERCISES\n17.1\nFor the 4 × 3 world shown in Figure 17.1, calculate which squares can be reached\nfrom (1,1) by the action sequence [Up, Up, Right, Right, Right] and with what probabilities.",
  "EXERCISES\n17.1\nFor the 4 × 3 world shown in Figure 17.1, calculate which squares can be reached\nfrom (1,1) by the action sequence [Up, Up, Right, Right, Right] and with what probabilities.\nExplain how this computation is related to the prediction task (see Section 15.2.1) for a hidden\nMarkov model.\n17.2\nSelect a speciﬁc member of the set of policies that are optimal for R(s) > 0 as shown\nin Figure 17.2(b), and calculate the fraction of time the agent spends in each state, in the limit,\nif the policy is executed forever. (Hint: Construct the state-to-state transition probability\nmatrix corresponding to the policy and see Exercise 15.2.)\n17.3\nSuppose that we deﬁne the utility of a state sequence to be the maximum reward ob-\ntained in any state in the sequence. Show that this utility function does not result in stationary\npreferences between state sequences. Is it still possible to deﬁne a utility function on states\nsuch that MEU decision making gives optimal behavior?\n17.4\nSometimes MDPs are formulated with a reward function R(s, a) that depends on the\naction taken or with a reward function R(s, a, s′) that also depends on the outcome state.\na. Write the Bellman equations for these formulations. Exercises\n689\nb. Show how an MDP with reward function R(s, a, s′) can be transformed into a different\nMDP with reward function R(s, a), such that optimal policies in the new MDP corre-\nspond exactly to optimal policies in the original MDP.\nc. Now do the same to convert MDPs with R(s, a) into MDPs with R(s).\n17.5\nFor the environment shown in Figure 17.1, ﬁnd all the threshold values for R(s) such\nthat the optimal policy changes when the threshold is crossed. You will need a way to calcu-\nlate the optimal policy and its value for ﬁxed R(s). (Hint: Prove that the value of any ﬁxed\npolicy varies linearly with R(s).)\n17.6\nEquation (17.7) on page 654 states that the Bellman operator is a contraction.\na. Show that, for any functions f and g,\n| max\na\nf(a) −max\na\ng(a)| ≤max\na\n|f(a) −g(a)| .\nb. Write out an expression for |(B Ui −B U′\ni)(s)| and then apply the result from (a) to\ncomplete the proof that the Bellman operator is a contraction.\n17.7\nThis exercise considers two-player MDPs that correspond to zero-sum, turn-taking\ngames like those in Chapter 5. Let the players be A and B, and let R(s) be the reward for\nplayer A in state s. (The reward for B is always equal and opposite.)\na. Let UA(s) be the utility of state s when it is A’s turn to move in s, and let UB(s) be the",
  "player A in state s. (The reward for B is always equal and opposite.)\na. Let UA(s) be the utility of state s when it is A’s turn to move in s, and let UB(s) be the\nutility of state s when it is B’s turn to move in s. All rewards and utilities are calculated\nfrom A’s point of view (just as in a minimax game tree). Write down Bellman equations\ndeﬁning UA(s) and UB(s).\nb. Explain how to do two-player value iteration with these equations, and deﬁne a suitable\ntermination criterion.\nc. Consider the game described in Figure 5.17 on page 197. Draw the state space (rather\nthan the game tree), showing the moves by A as solid lines and moves by B as dashed\nlines. Mark each state with R(s). You will ﬁnd it helpful to arrange the states (sA, sB)\non a two-dimensional grid, using sA and sB as “coordinates.”\nd. Now apply two-player value iteration to solve this game, and derive the optimal policy.\n17.8\nConsider the 3 × 3 world shown in Figure 17.14(a). The transition model is the same\nas in the 4 × 3 Figure 17.1: 80% of the time the agent goes in the direction it selects; the rest\nof the time it moves at right angles to the intended direction.\nImplement value iteration for this world for each value of r below. Use discounted\nrewards with a discount factor of 0.99. Show the policy obtained in each case. Explain\nintuitively why the value of r leads to each policy.\na. r = 100\nb. r = −3\nc. r = 0\nd. r = +3 690\nChapter\n17.\nMaking Complex Decisions\n-50\n+1\n+1\n+1\n+1\n+1\n+1\n+1\n+50\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n· · ·\n· · ·\n· · ·\nStart\nr\n-1\n-1\n-1\n+10\n-1\n-1\n-1\n-1\n(a)\n(b)\nFigure 17.14\n(a) 3 × 3 world for Exercise 17.8. The reward for each state is indicated.\nThe upper right square is a terminal state. (b) 101 ×3 world for Exercise 17.9 (omitting 93\nidentical columns in the middle). The start state has reward 0.\n17.9\nConsider the 101 × 3 world shown in Figure 17.14(b). In the start state the agent has\na choice of two deterministic actions, Up or Down, but in the other states the agent has one\ndeterministic action, Right. Assuming a discounted reward function, for what values of the\ndiscount γ should the agent choose Up and for which Down? Compute the utility of each\naction as a function of γ. (Note that this simple example actually reﬂects many real-world\nsituations in which one must weigh the value of an immediate action versus the potential\ncontinual long-term consequences, such as choosing to dump pollutants into a lake.)\n17.10",
  "situations in which one must weigh the value of an immediate action versus the potential\ncontinual long-term consequences, such as choosing to dump pollutants into a lake.)\n17.10\nConsider an undiscounted MDP having three states, (1, 2, 3), with rewards −1, −2,\n0, respectively. State 3 is a terminal state. In states 1 and 2 there are two possible actions: a\nand b. The transition model is as follows:\n• In state 1, action a moves the agent to state 2 with probability 0.8 and makes the agent\nstay put with probability 0.2.\n• In state 2, action a moves the agent to state 1 with probability 0.8 and makes the agent\nstay put with probability 0.2.\n• In either state 1 or state 2, action b moves the agent to state 3 with probability 0.1 and\nmakes the agent stay put with probability 0.9.\nAnswer the following questions:\na. What can be determined qualitatively about the optimal policy in states 1 and 2?\nb. Apply policy iteration, showing each step in full, to determine the optimal policy and\nthe values of states 1 and 2. Assume that the initial policy has action b in both states.\nc. What happens to policy iteration if the initial policy has action a in both states? Does\ndiscounting help? Does the optimal policy depend on the discount factor?\n17.11\nConsider the 4 × 3 world shown in Figure 17.1.\na. Implement an environment simulator for this environment, such that the speciﬁc geog-\nraphy of the environment is easily altered. Some code for doing this is already in the\nonline code repository. Exercises\n691\nb. Create an agent that uses policy iteration, and measure its performance in the environ-\nment simulator from various starting states. Perform several experiments from each\nstarting state, and compare the average total reward received per run with the utility of\nthe state, as determined by your algorithm.\nc. Experiment with increasing the size of the environment. How does the run time for\npolicy iteration vary with the size of the environment?\n17.12\nHow can the value determination algorithm be used to calculate the expected loss\nexperienced by an agent using a given set of utility estimates U and an estimated model P,\ncompared with an agent using correct values?\n17.13\nLet the initial belief state b0 for the 4 × 3 POMDP on page 658 be the uniform dis-\ntribution over the nonterminal states, i.e., ⟨1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 0, 0⟩. Calculate the exact\nbelief state b1 after the agent moves Left and its sensor reports 1 adjacent wall. Also calculate",
  "tribution over the nonterminal states, i.e., ⟨1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 1\n9, 0, 0⟩. Calculate the exact\nbelief state b1 after the agent moves Left and its sensor reports 1 adjacent wall. Also calculate\nb2 assuming that the same thing happens again.\n17.14\nWhat is the time complexity of d steps of POMDP value iteration for a sensorless\nenvironment?\n17.15\nConsider a version of the two-state POMDP on page 661 in which the sensor is\n90% reliable in state 0 but provides no information in state 1 (that is, it reports 0 or 1 with\nequal probability). Analyze, either qualitatively or quantitatively, the utility function and the\noptimal policy for this problem.\n17.16\nShow that a dominant strategy equilibrium is a Nash equilibrium, but not vice versa.\n17.17\nIn the children’s game of rock–paper–scissors each player reveals at the same time\na choice of rock, paper, or scissors. Paper wraps rock, rock blunts scissors, and scissors cut\npaper. In the extended version rock–paper–scissors–ﬁre–water, ﬁre beats rock, paper, and\nscissors; rock, paper, and scissors beat water; and water beats ﬁre. Write out the payoff\nmatrix and ﬁnd a mixed-strategy solution to this game.\n17.18\nThe following payoff matrix, from Blinder (1983) by way of Bernstein (1996), shows\na game between politicians and the Federal Reserve.\nFed: contract Fed: do nothing\nFed: expand\nPol: contract\nF = 7, P = 1\nF = 9, P = 4\nF = 6, P = 6\nPol: do nothing F = 8, P = 2\nF = 5, P = 5\nF = 4, P = 9\nPol: expand\nF = 3, P = 3\nF = 2, P = 7\nF = 1, P = 8\nPoliticians can expand or contract ﬁscal policy, while the Fed can expand or contract mon-\netary policy. (And of course either side can choose to do nothing.) Each side also has pref-\nerences for who should do what—neither side wants to look like the bad guys. The payoffs\nshown are simply the rank orderings: 9 for ﬁrst choice through 1 for last choice. Find the\nNash equilibrium of the game in pure strategies. Is this a Pareto-optimal solution? You might\nwish to analyze the policies of recent administrations in this light. 692\nChapter\n17.\nMaking Complex Decisions\n17.19\nA Dutch auction is similar in an English auction, but rather than starting the bidding\nat a low price and increasing, in a Dutch auction the seller starts at a high price and gradually\nlowers the price until some buyer is willing to accept that price. (If multiple bidders accept\nthe price, one is arbitrarily chosen as the winner.) More formally, the seller begins with a",
  "lowers the price until some buyer is willing to accept that price. (If multiple bidders accept\nthe price, one is arbitrarily chosen as the winner.) More formally, the seller begins with a\nprice p and gradually lowers p by increments of d until at least one buyer accepts the price.\nAssuming all bidders act rationally, is it true that for arbitrarily small d, a Dutch auction will\nalways result in the bidder with the highest value for the item obtaining the item? If so, show\nmathematically why. If not, explain how it may be possible for the bidder with highest value\nfor the item not to obtain it.\n17.20\nImagine an auction mechanism that is just like an ascending-bid auction, except that\nat the end, the winning bidder, the one who bid bmax, pays only bmax/2 rather than bmax.\nAssuming all agents are rational, what is the expected revenue to the auctioneer for this\nmechanism, compared with a standard ascending-bid auction?\n17.21\nTeams in the National Hockey League historically received 2 points for winning a\ngame and 0 for losing. If the game is tied, an overtime period is played; if nobody wins in\novertime, the game is a tie and each team gets 1 point. But league ofﬁcials felt that teams\nwere playing too conservatively in overtime (to avoid a loss), and it would be more exciting\nif overtime produced a winner. So in 1999 the ofﬁcials experimented in mechanism design:\nthe rules were changed, giving a team that loses in overtime 1 point, not 0. It is still 2 points\nfor a win and 1 for a tie.\na. Was hockey a zero-sum game before the rule change? After?\nb. Suppose that at a certain time t in a game, the home team has probability p of winning\nin regulation time, probability 0.78 −p of losing, and probability 0.22 of going into\novertime, where they have probability q of winning, .9 −q of losing, and .1 of tying.\nGive equations for the expected value for the home and visiting teams.\nc. Imagine that it were legal and ethical for the two teams to enter into a pact where they\nagree that they will skate to a tie in regulation time, and then both try in earnest to win\nin overtime. Under what conditions, in terms of p and q, would it be rational for both\nteams to agree to this pact?\nd. Longley and Sankaran (2005) report that since the rule change, the percentage of games\nwith a winner in overtime went up 18.2%, as desired, but the percentage of overtime\ngames also went up 3.6%. What does that suggest about possible collusion or conser-\nvative play after the rule change? 18",
  "with a winner in overtime went up 18.2%, as desired, but the percentage of overtime\ngames also went up 3.6%. What does that suggest about possible collusion or conser-\nvative play after the rule change? 18\nLEARNING FROM\nEXAMPLES\nIn which we describe agents that can improve their behavior through diligent\nstudy of their own experiences.\nAn agent is learning if it improves its performance on future tasks after making observations\nLEARNING\nabout the world. Learning can range from the trivial, as exhibited by jotting down a phone\nnumber, to the profound, as exhibited by Albert Einstein, who inferred a new theory of the\nuniverse. In this chapter we will concentrate on one class of learning problem, which seems\nrestricted but actually has vast applicability: from a collection of input–output pairs, learn a\nfunction that predicts the output for new inputs.\nWhy would we want an agent to learn? If the design of the agent can be improved,\nwhy wouldn’t the designers just program in that improvement to begin with? There are three\nmain reasons. First, the designers cannot anticipate all possible situations that the agent\nmight ﬁnd itself in. For example, a robot designed to navigate mazes must learn the layout\nof each new maze it encounters. Second, the designers cannot anticipate all changes over\ntime; a program designed to predict tomorrow’s stock market prices must learn to adapt when\nconditions change from boom to bust. Third, sometimes human programmers have no idea\nhow to program a solution themselves. For example, most people are good at recognizing the\nfaces of family members, but even the best programmers are unable to program a computer\nto accomplish that task, except by using learning algorithms. This chapter ﬁrst gives an\noverview of the various forms of learning, then describes one popular approach, decision-\ntree learning, in Section 18.3, followed by a theoretical analysis of learning in Sections 18.4\nand 18.5. We look at various learning systems used in practice: linear models, nonlinear\nmodels (in particular, neural networks), nonparametric models, and support vector machines.\nFinally we show how ensembles of models can outperform a single model.\n18.1\nFORMS OF LEARNING\nAny component of an agent can be improved by learning from data. The improvements, and\nthe techniques used to make them, depend on four major factors:\n• Which component is to be improved.\n693 694\nChapter\n18.\nLearning from Examples\n• What prior knowledge the agent already has.",
  "the techniques used to make them, depend on four major factors:\n• Which component is to be improved.\n693 694\nChapter\n18.\nLearning from Examples\n• What prior knowledge the agent already has.\n• What representation is used for the data and the component.\n• What feedback is available to learn from.\nComponents to be learned\nChapter 2 described several agent designs. The components of these agents include:\n1. A direct mapping from conditions on the current state to actions.\n2. A means to infer relevant properties of the world from the percept sequence.\n3. Information about the way the world evolves and about the results of possible actions\nthe agent can take.\n4. Utility information indicating the desirability of world states.\n5. Action-value information indicating the desirability of actions.\n6. Goals that describe classes of states whose achievement maximizes the agent’s utility.\nEach of these components can be learned. Consider, for example, an agent training to become\na taxi driver. Every time the instructor shouts “Brake!” the agent might learn a condition–\naction rule for when to brake (component 1); the agent also learns every time the instructor\ndoes not shout. By seeing many camera images that it is told contain buses, it can learn\nto recognize them (2). By trying actions and observing the results—for example, braking\nhard on a wet road—it can learn the effects of its actions (3). Then, when it receives no tip\nfrom passengers who have been thoroughly shaken up during the trip, it can learn a useful\ncomponent of its overall utility function (4).\nRepresentation and prior knowledge\nWe have seen several examples of representations for agent components: propositional and\nﬁrst-order logical sentences for the components in a logical agent; Bayesian networks for\nthe inferential components of a decision-theoretic agent, and so on. Effective learning algo-\nrithms have been devised for all of these representations. This chapter (and most of current\nmachine learning research) covers inputs that form a factored representation—a vector of\nattribute values—and outputs that can be either a continuous numerical value or a discrete\nvalue. Chapter 19 covers functions and prior knowledge composed of ﬁrst-order logic sen-\ntences, and Chapter 20 concentrates on Bayesian networks.\nThere is another way to look at the various types of learning. We say that learning\na (possibly incorrect) general function or rule from speciﬁc input–output pairs is called in-",
  "tences, and Chapter 20 concentrates on Bayesian networks.\nThere is another way to look at the various types of learning. We say that learning\na (possibly incorrect) general function or rule from speciﬁc input–output pairs is called in-\nductive learning. We will see in Chapter 19 that we can also do analytical or deductive\nINDUCTIVE\nLEARNING\nlearning: going from a known general rule to a new rule that is logically entailed, but is\nDEDUCTIVE\nLEARNING\nuseful because it allows more efﬁcient processing.\nFeedback to learn from\nThere are three types of feedback that determine the three main types of learning:\nIn unsupervised learning the agent learns patterns in the input even though no explicit\nUNSUPERVISED\nLEARNING\nfeedback is supplied. The most common unsupervised learning task is clustering: detecting\nCLUSTERING Section 18.2.\nSupervised Learning\n695\npotentially useful clusters of input examples. For example, a taxi agent might gradually\ndevelop a concept of “good trafﬁc days” and “bad trafﬁc days” without ever being given\nlabeled examples of each by a teacher.\nIn reinforcement learning the agent learns from a series of reinforcements—rewards\nREINFORCEMENT\nLEARNING\nor punishments. For example, the lack of a tip at the end of the journey gives the taxi agent an\nindication that it did something wrong. The two points for a win at the end of a chess game\ntells the agent it did something right. It is up to the agent to decide which of the actions prior\nto the reinforcement were most responsible for it.\nIn supervised learning the agent observes some example input–output pairs and learns\nSUPERVISED\nLEARNING\na function that maps from input to output. In component 1 above, the inputs are percepts and\nthe output are provided by a teacher who says “Brake!” or “Turn left.” In component 2, the\ninputs are camera images and the outputs again come from a teacher who says “that’s a bus.”\nIn 3, the theory of braking is a function from states and braking actions to stopping distance\nin feet. In this case the output value is available directly from the agent’s percepts (after the\nfact); the environment is the teacher.\nIn practice, these distinction are not always so crisp. In semi-supervised learning we\nSEMI-SUPERVISED\nLEARNING\nare given a few labeled examples and must make what we can of a large collection of un-\nlabeled examples. Even the labels themselves may not be the oracular truths that we hope",
  "SEMI-SUPERVISED\nLEARNING\nare given a few labeled examples and must make what we can of a large collection of un-\nlabeled examples. Even the labels themselves may not be the oracular truths that we hope\nfor. Imagine that you are trying to build a system to guess a person’s age from a photo. You\ngather some labeled examples by snapping pictures of people and asking their age. That’s\nsupervised learning. But in reality some of the people lied about their age. It’s not just\nthat there is random noise in the data; rather the inaccuracies are systematic, and to uncover\nthem is an unsupervised learning problem involving images, self-reported ages, and true (un-\nknown) ages. Thus, both noise and lack of labels create a continuum between supervised and\nunsupervised learning.\n18.2\nSUPERVISED LEARNING\nThe task of supervised learning is this:\nGiven a training set of N example input–output pairs\nTRAINING SET\n(x1, y1), (x2, y2), . . . (xN, yN) ,\nwhere each yj was generated by an unknown function y = f(x),\ndiscover a function h that approximates the true function f.\nHere x and y can be any value; they need not be numbers. The function h is a hypothesis.1\nHYPOTHESIS\nLearning is a search through the space of possible hypotheses for one that will perform well,\neven on new examples beyond the training set. To measure the accuracy of a hypothesis we\ngive it a test set of examples that are distinct from the training set. We say a hypothesis\nTEST SET\n1 A note on notation: except where noted, we will use j to index the N examples; xj will always be the input and\nyj the output. In cases where the input is speciﬁcally a vector of attribute values (beginning with Section 18.3),\nwe will use xj for the jth example and we will use i to index the n attributes of each example. The elements of\nxj are written xj,1, xj,2, . . . , xj,n. 696\nChapter\n18.\nLearning from Examples\n(c)\n(a)\n(b)\n(d)\nx\nx\nx\nx\nf(x)\nf(x)\nf(x)\nf(x)\nFigure 18.1\n(a) Example (x, f(x)) pairs and a consistent, linear hypothesis. (b) A con-\nsistent, degree-7 polynomial hypothesis for the same data set. (c) A different data set, which\nadmits an exact degree-6 polynomial ﬁt or an approximate linear ﬁt. (d) A simple, exact\nsinusoidal ﬁt to the same data set.\ngeneralizes well if it correctly predicts the value of y for novel examples. Sometimes the\nGENERALIZATION\nfunction f is stochastic—it is not strictly a function of x, and what we have to learn is a\nconditional probability distribution, P(Y | x).",
  "generalizes well if it correctly predicts the value of y for novel examples. Sometimes the\nGENERALIZATION\nfunction f is stochastic—it is not strictly a function of x, and what we have to learn is a\nconditional probability distribution, P(Y | x).\nWhen the output y is one of a ﬁnite set of values (such as sunny, cloudy or rainy),\nthe learning problem is called classiﬁcation, and is called Boolean or binary classiﬁcation\nCLASSIFICATION\nif there are only two values. When y is a number (such as tomorrow’s temperature), the\nlearning problem is called regression. (Technically, solving a regression problem is ﬁnding\nREGRESSION\na conditional expectation or average value of y, because the probability that we have found\nexactly the right real-valued number for y is 0.)\nFigure 18.1 shows a familiar example: ﬁtting a function of a single variable to some data\npoints. The examples are points in the (x, y) plane, where y = f(x). We don’t know what f\nis, but we will approximate it with a function h selected from a hypothesis space, H, which\nHYPOTHESIS SPACE\nfor this example we will take to be the set of polynomials, such as x5+3x2+2. Figure 18.1(a)\nshows some data with an exact ﬁt by a straight line (the polynomial 0.4x + 3). The line is\ncalled a consistent hypothesis because it agrees with all the data. Figure 18.1(b) shows a high-\nCONSISTENT\ndegree polynomial that is also consistent with the same data. This illustrates a fundamental\nproblem in inductive learning: how do we choose from among multiple consistent hypotheses?\nOne answer is to prefer the simplest hypothesis consistent with the data. This principle is\ncalled Ockham’s razor, after the 14th-century English philosopher William of Ockham, who\nOCKHAM’S RAZOR\nused it to argue sharply against all sorts of complications. Deﬁning simplicity is not easy, but\nit seems clear that a degree-1 polynomial is simpler than a degree-7 polynomial, and thus (a)\nshould be preferred to (b). We will make this intuition more precise in Section 18.4.3.\nFigure 18.1(c) shows a second data set. There is no consistent straight line for this\ndata set; in fact, it requires a degree-6 polynomial for an exact ﬁt. There are just 7 data\npoints, so a polynomial with 7 parameters does not seem to be ﬁnding any pattern in the\ndata and we do not expect it to generalize well. A straight line that is not consistent with\nany of the data points, but might generalize fairly well for unseen values of x, is also shown",
  "data and we do not expect it to generalize well. A straight line that is not consistent with\nany of the data points, but might generalize fairly well for unseen values of x, is also shown\nin (c). In general, there is a tradeoff between complex hypotheses that ﬁt the training data\nwell and simpler hypotheses that may generalize better. In Figure 18.1(d) we expand the Section 18.3.\nLearning Decision Trees\n697\nhypothesis space H to allow polynomials over both x and sin(x), and ﬁnd that the data in\n(c) can be ﬁtted exactly by a simple function of the form ax + b + c sin(x). This shows the\nimportance of the choice of hypothesis space. We say that a learning problem is realizable if\nREALIZABLE\nthe hypothesis space contains the true function. Unfortunately, we cannot always tell whether\na given learning problem is realizable, because the true function is not known.\nIn some cases, an analyst looking at a problem is willing to make more ﬁne-grained\ndistinctions about the hypothesis space, to say—even before seeing any data—not just that a\nhypothesis is possible or impossible, but rather how probable it is. Supervised learning can\nbe done by choosing the hypothesis h∗that is most probable given the data:\nh∗= argmax\nh∈H\nP(h|data) .\nBy Bayes’ rule this is equivalent to\nh∗= argmax\nh∈H\nP(data|h) P(h) .\nThen we can say that the prior probability P(h) is high for a degree-1 or -2 polynomial,\nlower for a degree-7 polynomial, and especially low for degree-7 polynomials with large,\nsharp spikes as in Figure 18.1(b). We allow unusual-looking functions when the data say we\nreally need them, but we discourage them by giving them a low prior probability.\nWhy not let H be the class of all Java programs, or Turing machines? After all, every\ncomputable function can be represented by some Turing machine, and that is the best we\ncan do. One problem with this idea is that it does not take into account the computational\ncomplexity of learning. There is a tradeoff between the expressiveness of a hypothesis space\nand the complexity of ﬁnding a good hypothesis within that space. For example, ﬁtting a\nstraight line to data is an easy computation; ﬁtting high-degree polynomials is somewhat\nharder; and ﬁtting Turing machines is in general undecidable. A second reason to prefer\nsimple hypothesis spaces is that presumably we will want to use h after we have learned it,\nand computing h(x) when h is a linear function is guaranteed to be fast, while computing",
  "simple hypothesis spaces is that presumably we will want to use h after we have learned it,\nand computing h(x) when h is a linear function is guaranteed to be fast, while computing\nan arbitrary Turing machine program is not even guaranteed to terminate. For these reasons,\nmost work on learning has focused on simple representations.\nWe will see that the expressiveness–complexity tradeoff is not as simple as it ﬁrst seems:\nit is often the case, as we saw with ﬁrst-order logic in Chapter 8, that an expressive language\nmakes it possible for a simple hypothesis to ﬁt the data, whereas restricting the expressiveness\nof the language means that any consistent hypothesis must be very complex. For example,\nthe rules of chess can be written in a page or two of ﬁrst-order logic, but require thousands of\npages when written in propositional logic.\n18.3\nLEARNING DECISION TREES\nDecision tree induction is one of the simplest and yet most successful forms of machine\nlearning. We ﬁrst describe the representation—the hypothesis space—and then show how to\nlearn a good hypothesis. 698\nChapter\n18.\nLearning from Examples\n18.3.1\nThe decision tree representation\nA decision tree represents a function that takes as input a vector of attribute values and\nDECISION TREE\nreturns a “decision”—a single output value. The input and output values can be discrete or\ncontinuous. For now we will concentrate on problems where the inputs have discrete values\nand the output has exactly two possible values; this is Boolean classiﬁcation, where each\nexample input will be classiﬁed as true (a positive example) or false (a negative example).\nPOSITIVE\nNEGATIVE\nA decision tree reaches its decision by performing a sequence of tests. Each internal\nnode in the tree corresponds to a test of the value of one of the input attributes, Ai, and\nthe branches from the node are labeled with the possible values of the attribute, Ai = vik.\nEach leaf node in the tree speciﬁes a value to be returned by the function. The decision tree\nrepresentation is natural for humans; indeed, many “How To” manuals (e.g., for car repair)\nare written entirely as a single decision tree stretching over hundreds of pages.\nAs an example, we will build a decision tree to decide whether to wait for a table at a\nrestaurant. The aim here is to learn a deﬁnition for the goal predicate WillWait. First we\nGOAL PREDICATE\nlist the attributes that we will consider as part of the input:",
  "restaurant. The aim here is to learn a deﬁnition for the goal predicate WillWait. First we\nGOAL PREDICATE\nlist the attributes that we will consider as part of the input:\n1. Alternate: whether there is a suitable alternative restaurant nearby.\n2. Bar: whether the restaurant has a comfortable bar area to wait in.\n3. Fri/Sat: true on Fridays and Saturdays.\n4. Hungry: whether we are hungry.\n5. Patrons: how many people are in the restaurant (values are None, Some, and Full).\n6. Price: the restaurant’s price range ($, $$, $$$).\n7. Raining: whether it is raining outside.\n8. Reservation: whether we made a reservation.\n9. Type: the kind of restaurant (French, Italian, Thai, or burger).\n10. WaitEstimate: the wait estimated by the host (0–10 minutes, 10–30, 30–60, or >60).\nNote that every variable has a small set of possible values; the value of WaitEstimate, for\nexample, is not an integer, rather it is one of the four discrete values 0–10, 10–30, 30–60, or\n>60. The decision tree usually used by one of us (SR) for this domain is shown in Figure 18.2.\nNotice that the tree ignores the Price and Type attributes. Examples are processed by the tree\nstarting at the root and following the appropriate branch until a leaf is reached. For instance,\nan example with Patrons = Full and WaitEstimate = 0–10 will be classiﬁed as positive\n(i.e., yes, we will wait for a table).\n18.3.2\nExpressiveness of decision trees\nA Boolean decision tree is logically equivalent to the assertion that the goal attribute is true\nif and only if the input attributes satisfy one of the paths leading to a leaf with value true.\nWriting this out in propositional logic, we have\nGoal ⇔(Path1 ∨Path2 ∨· · ·) ,\nwhere each Path is a conjunction of attribute-value tests required to follow that path. Thus,\nthe whole expression is equivalent to disjunctive normal form (see page 283), which means Section 18.3.\nLearning Decision Trees\n699\nthat any function in propositional logic can be expressed as a decision tree. As an example,\nthe rightmost path in Figure 18.2 is\nPath = (Patrons = Full ∧WaitEstimate = 0–10) .\nFor a wide variety of problems, the decision tree format yields a nice, concise result. But\nsome functions cannot be represented concisely. For example, the majority function, which\nreturns true if and only if more than half of the inputs are true, requires an exponentially\nlarge decision tree. In other words, decision trees are good for some kinds of functions and",
  "returns true if and only if more than half of the inputs are true, requires an exponentially\nlarge decision tree. In other words, decision trees are good for some kinds of functions and\nbad for others. Is there any kind of representation that is efﬁcient for all kinds of functions?\nUnfortunately, the answer is no. We can show this in a general way. Consider the set of all\nBoolean functions on n attributes. How many different functions are in this set? This is just\nthe number of different truth tables that we can write down, because the function is deﬁned\nby its truth table. A truth table over n attributes has 2n rows, one for each combination of\nvalues of the attributes. We can consider the “answer” column of the table as a 2n-bit number\nthat deﬁnes the function. That means there are 22n different functions (and there will be more\nthan that number of trees, since more than one tree can compute the same function). This is\na scary number. For example, with just the ten Boolean attributes of our restaurant problem\nthere are 21024 or about 10308 different functions to choose from, and for 20 attributes there\nare over 10300,000. We will need some ingenious algorithms to ﬁnd good hypotheses in such\na large space.\n18.3.3\nInducing decision trees from examples\nAn example for a Boolean decision tree consists of an (x, y) pair, where x is a vector of values\nfor the input attributes, and y is a single Boolean output value. A training set of 12 examples\nNo\n Yes\nNo\n Yes\nNo\n Yes\nNo\n Yes\nNone\nSome\nFull\n>60\n30-60\n10-30\n0-10\nNo\n Yes\nAlternate?\nHungry?\nReservation?\nBar?\nRaining?\nAlternate?\nPatrons?\nFri/Sat?\nNo\nYes\nNo\nYes\nYes\nYes\nNo\n Yes\nNo\nYes\nYes\nNo\nYes\nNo\n Yes\nYes\nNo\nWaitEstimate?\nFigure 18.2\nA decision tree for deciding whether to wait for a table. 700\nChapter\n18.\nLearning from Examples\nExample\nInput Attributes\nGoal\nAlt\nBar\nFri\nHun\nPat\nPrice Rain\nRes\nType\nEst\nWillWait\nx1\nYes\nNo\nNo\nYes\nSome\n$$$\nNo\nYes\nFrench\n0–10\ny1 = Yes\nx2\nYes\nNo\nNo\nYes\nFull\n$\nNo\nNo\nThai\n30–60\ny2 = No\nx3\nNo\nYes\nNo\nNo\nSome\n$\nNo\nNo\nBurger\n0–10\ny3 = Yes\nx4\nYes\nNo\nYes\nYes\nFull\n$\nYes\nNo\nThai\n10–30\ny4 = Yes\nx5\nYes\nNo\nYes\nNo\nFull\n$$$\nNo\nYes\nFrench\n>60\ny5 = No\nx6\nNo\nYes\nNo\nYes\nSome\n$$\nYes\nYes\nItalian\n0–10\ny6 = Yes\nx7\nNo\nYes\nNo\nNo\nNone\n$\nYes\nNo\nBurger\n0–10\ny7 = No\nx8\nNo\nNo\nNo\nYes\nSome\n$$\nYes\nYes\nThai\n0–10\ny8 = Yes\nx9\nNo\nYes\nYes\nNo\nFull\n$\nYes\nNo\nBurger\n>60\ny9 = No\nx10\nYes\nYes\nYes\nYes\nFull\n$$$\nNo\nYes\nItalian\n10–30\ny10 = No\nx11\nNo\nNo\nNo\nNo\nNone\n$\nNo\nNo\nThai\n0–10\ny11 = No\nx12\nYes\nYes\nYes\nYes\nFull\n$\nNo\nNo\nBurger",
  "x8\nNo\nNo\nNo\nYes\nSome\n$$\nYes\nYes\nThai\n0–10\ny8 = Yes\nx9\nNo\nYes\nYes\nNo\nFull\n$\nYes\nNo\nBurger\n>60\ny9 = No\nx10\nYes\nYes\nYes\nYes\nFull\n$$$\nNo\nYes\nItalian\n10–30\ny10 = No\nx11\nNo\nNo\nNo\nNo\nNone\n$\nNo\nNo\nThai\n0–10\ny11 = No\nx12\nYes\nYes\nYes\nYes\nFull\n$\nNo\nNo\nBurger\n30–60\ny12 = Yes\nFigure 18.3\nExamples for the restaurant domain.\nis shown in Figure 18.3. The positive examples are the ones in which the goal WillWait is\ntrue (x1, x3, . . .); the negative examples are the ones in which it is false (x2, x5, . . .).\nWe want a tree that is consistent with the examples and is as small as possible. Un-\nfortunately, no matter how we measure size, it is an intractable problem to ﬁnd the smallest\nconsistent tree; there is no way to efﬁciently search through the 22n trees. With some simple\nheuristics, however, we can ﬁnd a good approximate solution: a small (but not smallest) con-\nsistent tree. The DECISION-TREE-LEARNING algorithm adopts a greedy divide-and-conquer\nstrategy: always test the most important attribute ﬁrst. This test divides the problem up into\nsmaller subproblems that can then be solved recursively. By “most important attribute,” we\nmean the one that makes the most difference to the classiﬁcation of an example. That way, we\nhope to get to the correct classiﬁcation with a small number of tests, meaning that all paths in\nthe tree will be short and the tree as a whole will be shallow.\nFigure 18.4(a) shows that Type is a poor attribute, because it leaves us with four possible\noutcomes, each of which has the same number of positive as negative examples. On the other\nhand, in (b) we see that Patrons is a fairly important attribute, because if the value is None or\nSome, then we are left with example sets for which we can answer deﬁnitively (No and Yes,\nrespectively). If the value is Full, we are left with a mixed set of examples. In general, after\nthe ﬁrst attribute test splits up the examples, each outcome is a new decision tree learning\nproblem in itself, with fewer examples and one less attribute. There are four cases to consider\nfor these recursive problems:\n1. If the remaining examples are all positive (or all negative), then we are done: we can\nanswer Yes or No. Figure 18.4(b) shows examples of this happening in the None and\nSome branches.\n2. If there are some positive and some negative examples, then choose the best attribute to\nsplit them. Figure 18.4(b) shows Hungry being used to split the remaining examples.",
  "Some branches.\n2. If there are some positive and some negative examples, then choose the best attribute to\nsplit them. Figure 18.4(b) shows Hungry being used to split the remaining examples.\n3. If there are no examples left, it means that no example has been observed for this com- Section 18.3.\nLearning Decision Trees\n701\n(a)\nNone\nSome\nFull\nPatrons?\nYes\nNo\nHungry?\n(b)\nNo\nYes\n12\n1\n3\n4\n6\n8\n2\n5\n7\n9\n10 11\nFrench\nItalian\nThai\nBurger\nType?\n12\n1\n3\n4\n6\n8\n2\n5\n7\n9\n10 11\n1\n5\n6\n10\n4\n8\n2\n11\n12\n3\n7\n9\n7\n11\n1\n3\n6\n8\n12\n4\n2\n5\n9\n10\n12\n4\n2\n10\n5\n9\nFigure 18.4\nSplitting the examples by testing on attributes. At each node we show the\npositive (light boxes) and negative (dark boxes) examples remaining. (a) Splitting on Type\nbrings us no nearer to distinguishing between positive and negative examples. (b) Splitting\non Patrons does a good job of separating positive and negative examples. After splitting on\nPatrons, Hungry is a fairly good second test.\nbination of attribute values, and we return a default value calculated from the plurality\nclassiﬁcation of all the examples that were used in constructing the node’s parent. These\nare passed along in the variable parent examples.\n4. If there are no attributes left, but both positive and negative examples, it means that\nthese examples have exactly the same description, but different classiﬁcations. This can\nhappen because there is an error or noise in the data; because the domain is nondeter-\nNOISE\nministic; or because we can’t observe an attribute that would distinguish the examples.\nThe best we can do is return the plurality classiﬁcation of the remaining examples.\nThe DECISION-TREE-LEARNING algorithm is shown in Figure 18.5. Note that the set of\nexamples is crucial for constructing the tree, but nowhere do the examples appear in the tree\nitself. A tree consists of just tests on attributes in the interior nodes, values of attributes on\nthe branches, and output values on the leaf nodes. The details of the IMPORTANCE function\nare given in Section 18.3.4. The output of the learning algorithm on our sample training\nset is shown in Figure 18.6. The tree is clearly different from the original tree shown in\nFigure 18.2. One might conclude that the learning algorithm is not doing a very good job\nof learning the correct function. This would be the wrong conclusion to draw, however. The\nlearning algorithm looks at the examples, not at the correct function, and in fact, its hypothesis",
  "of learning the correct function. This would be the wrong conclusion to draw, however. The\nlearning algorithm looks at the examples, not at the correct function, and in fact, its hypothesis\n(see Figure 18.6) not only is consistent with all the examples, but is considerably simpler\nthan the original tree! The learning algorithm has no reason to include tests for Raining and\nReservation, because it can classify all the examples without them. It has also detected an\ninteresting and previously unsuspected pattern: the ﬁrst author will wait for Thai food on\nweekends. It is also bound to make some mistakes for cases where it has seen no examples.\nFor example, it has never seen a case where the wait is 0–10 minutes but the restaurant is full. 702\nChapter\n18.\nLearning from Examples\nfunction DECISION-TREE-LEARNING(examples,attributes,parent examples) returns\na tree\nif examples is empty then return PLURALITY-VALUE(parent examples)\nelse if all examples have the same classiﬁcation then return the classiﬁcation\nelse if attributes is empty then return PLURALITY-VALUE(examples)\nelse\nA ←argmaxa ∈attributes IMPORTANCE(a, examples)\ntree ←a new decision tree with root test A\nfor each value vk of A do\nexs ←{e : e ∈examples and e.A = vk}\nsubtree ←DECISION-TREE-LEARNING(exs,attributes −A,examples)\nadd a branch to tree with label (A = vk) and subtree subtree\nreturn tree\nFigure 18.5\nThe decision-tree learning algorithm.\nThe function IMPORTANCE is de-\nscribed in Section 18.3.4. The function PLURALITY-VALUE selects the most common output\nvalue among a set of examples, breaking ties randomly.\nNone\nSome\nFull\nPatrons?\nNo\nYes\nNo\n Yes\nHungry?\nNo\nNo\n Yes\nFri/Sat?\nYes\nNo\nYes\nType?\nFrench\nItalian\nThai\nBurger\nYes\nNo\nFigure 18.6\nThe decision tree induced from the 12-example training set.\nIn that case it says not to wait when Hungry is false, but I (SR) would certainly wait. With\nmore training examples the learning program could correct this mistake.\nWe note there is a danger of over-interpreting the tree that the algorithm selects. When\nthere are several variables of similar importance, the choice between them is somewhat arbi-\ntrary: with slightly different input examples, a different variable would be chosen to split on\nﬁrst, and the whole tree would look completely different. The function computed by the tree\nwould still be similar, but the structure of the tree can vary widely.\nWe can evaluate the accuracy of a learning algorithm with a learning curve, as shown\nLEARNING CURVE",
  "would still be similar, but the structure of the tree can vary widely.\nWe can evaluate the accuracy of a learning algorithm with a learning curve, as shown\nLEARNING CURVE\nin Figure 18.7. We have 100 examples at our disposal, which we split into a training set and Section 18.3.\nLearning Decision Trees\n703\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n20\n40\n60\n80\n100\nProportion correct on test set\nTraining set size\nFigure 18.7\nA learning curve for the decision tree learning algorithm on 100 randomly\ngenerated examples in the restaurant domain. Each data point is the average of 20 trials.\na test set. We learn a hypothesis h with the training set and measure its accuracy with the test\nset. We do this starting with a training set of size 1 and increasing one at a time up to size\n99. For each size we actually repeat the process of randomly splitting 20 times, and average\nthe results of the 20 trials. The curve shows that as the training set size grows, the accuracy\nincreases. (For this reason, learning curves are also called happy graphs.) In this graph we\nreach 95% accuracy, and it looks like the curve might continue to increase with more data.\n18.3.4\nChoosing attribute tests\nThe greedy search used in decision tree learning is designed to approximately minimize the\ndepth of the ﬁnal tree. The idea is to pick the attribute that goes as far as possible toward\nproviding an exact classiﬁcation of the examples. A perfect attribute divides the examples\ninto sets, each of which are all positive or all negative and thus will be leaves of the tree. The\nPatrons attribute is not perfect, but it is fairly good. A really useless attribute, such as Type,\nleaves the example sets with roughly the same proportion of positive and negative examples\nas the original set.\nAll we need, then, is a formal measure of “fairly good” and “really useless” and we can\nimplement the IMPORTANCE function of Figure 18.5. We will use the notion of information\ngain, which is deﬁned in terms of entropy, the fundamental quantity in information theory\nENTROPY\n(Shannon and Weaver, 1949).\nEntropy is a measure of the uncertainty of a random variable; acquisition of information\ncorresponds to a reduction in entropy. A random variable with only one value—a coin that\nalways comes up heads—has no uncertainty and thus its entropy is deﬁned as zero; thus, we\ngain no information by observing its value. A ﬂip of a fair coin is equally likely to come up",
  "always comes up heads—has no uncertainty and thus its entropy is deﬁned as zero; thus, we\ngain no information by observing its value. A ﬂip of a fair coin is equally likely to come up\nheads or tails, 0 or 1, and we will soon show that this counts as “1 bit” of entropy. The roll\nof a fair four-sided die has 2 bits of entropy, because it takes two bits to describe one of four\nequally probable choices. Now consider an unfair coin that comes up heads 99% of the time.\nIntuitively, this coin has less uncertainty than the fair coin—if we guess heads we’ll be wrong\nonly 1% of the time—so we would like it to have an entropy measure that is close to zero, but 704\nChapter\n18.\nLearning from Examples\npositive. In general, the entropy of a random variable V with values vk, each with probability\nP(vk), is deﬁned as\nEntropy:\nH(V ) =\n\f\nk\nP(vk) log2\n1\nP(vk) = −\n\f\nk\nP(vk) log2 P(vk) .\nWe can check that the entropy of a fair coin ﬂip is indeed 1 bit:\nH(Fair) = −(0.5 log2 0.5 + 0.5 log2 0.5) = 1 .\nIf the coin is loaded to give 99% heads, we get\nH(Loaded) = −(0.99 log2 0.99 + 0.01 log2 0.01) ≈0.08 bits.\nIt will help to deﬁne B(q) as the entropy of a Boolean random variable that is true with\nprobability q:\nB(q) = −(q log2 q + (1 −q) log2(1 −q)) .\nThus, H(Loaded) = B(0.99) ≈0.08. Now let’s get back to decision tree learning. If a\ntraining set contains p positive examples and n negative examples, then the entropy of the\ngoal attribute on the whole set is\nH(Goal) = B\n\r\np\np + n\n\u000e\n.\nThe restaurant training set in Figure 18.3 has p = n = 6, so the corresponding entropy is\nB(0.5) or exactly 1 bit. A test on a single attribute A might give us only part of this 1 bit. We\ncan measure exactly how much by looking at the entropy remaining after the attribute test.\nAn attribute A with d distinct values divides the training set E into subsets E1, . . . , Ed.\nEach subset Ek has pk positive examples and nk negative examples, so if we go along that\nbranch, we will need an additional B(pk/(pk + nk)) bits of information to answer the ques-\ntion. A randomly chosen example from the training set has the kth value for the attribute with\nprobability (pk + nk)/(p + n), so the expected entropy remaining after testing attribute A is\nRemainder(A) =\nd\n\f\nk=1\npk+nk\np+n B(\npk\npk+nk ) .\nThe information gain from the attribute test on A is the expected reduction in entropy:\nINFORMATION GAIN\nGain(A) = B(\np\np+n) −Remainder(A) .\nIn fact Gain(A) is just what we need to implement the IMPORTANCE function. Returning to",
  "p+n B(\npk\npk+nk ) .\nThe information gain from the attribute test on A is the expected reduction in entropy:\nINFORMATION GAIN\nGain(A) = B(\np\np+n) −Remainder(A) .\nIn fact Gain(A) is just what we need to implement the IMPORTANCE function. Returning to\nthe attributes considered in Figure 18.4, we have\nGain(Patrons) = 1 −\n$ 2\n12B(0\n2) + 4\n12B(4\n4) + 6\n12B(2\n6)\n%\n≈0.541 bits,\nGain(Type) = 1 −\n$ 2\n12B(1\n2) + 2\n12B(1\n2) + 4\n12B(2\n4) + 4\n12B(2\n4)\n%\n= 0 bits,\nconﬁrming our intuition that Patrons is a better attribute to split on. In fact, Patrons has\nthe maximum gain of any of the attributes and would be chosen by the decision-tree learning\nalgorithm as the root. Section 18.3.\nLearning Decision Trees\n705\n18.3.5\nGeneralization and overﬁtting\nOn some problems, the DECISION-TREE-LEARNING algorithm will generate a large tree\nwhen there is actually no pattern to be found. Consider the problem of trying to predict\nwhether the roll of a die will come up as 6 or not. Suppose that experiments are carried out\nwith various dice and that the attributes describing each training example include the color\nof the die, its weight, the time when the roll was done, and whether the experimenters had\ntheir ﬁngers crossed. If the dice are fair, the right thing to learn is a tree with a single node\nthat says “no,” But the DECISION-TREE-LEARNING algorithm will seize on any pattern it\ncan ﬁnd in the input. If it turns out that there are 2 rolls of a 7-gram blue die with ﬁngers\ncrossed and they both come out 6, then the algorithm may construct a path that predicts 6 in\nthat case. This problem is called overﬁtting. A general phenomenon, overﬁtting occurs with\nOVERFITTING\nall types of learners, even when the target function is not at all random. In Figure 18.1(b) and\n(c), we saw polynomial functions overﬁtting the data. Overﬁtting becomes more likely as the\nhypothesis space and the number of input attributes grows, and less likely as we increase the\nnumber of training examples.\nFor decision trees, a technique called decision tree pruning combats overﬁtting. Prun-\nDECISION TREE\nPRUNING\ning works by eliminating nodes that are not clearly relevant. We start with a full tree, as\ngenerated by DECISION-TREE-LEARNING. We then look at a test node that has only leaf\nnodes as descendants. If the test appears to be irrelevant—detecting only noise in the data—\nthen we eliminate the test, replacing it with a leaf node. We repeat this process, considering",
  "nodes as descendants. If the test appears to be irrelevant—detecting only noise in the data—\nthen we eliminate the test, replacing it with a leaf node. We repeat this process, considering\neach test with only leaf descendants, until each one has either been pruned or accepted as is.\nThe question is, how do we detect that a node is testing an irrelevant attribute? Suppose\nwe are at a node consisting of p positive and n negative examples. If the attribute is irrelevant,\nwe would expect that it would split the examples into subsets that each have roughly the same\nproportion of positive examples as the whole set, p/(p + n), and so the information gain will\nbe close to zero.2 Thus, the information gain is a good clue to irrelevance. Now the question\nis, how large a gain should we require in order to split on a particular attribute?\nWe can answer this question by using a statistical signiﬁcance test. Such a test begins\nSIGNIFICANCE TEST\nby assuming that there is no underlying pattern (the so-called null hypothesis). Then the ac-\nNULL HYPOTHESIS\ntual data are analyzed to calculate the extent to which they deviate from a perfect absence of\npattern. If the degree of deviation is statistically unlikely (usually taken to mean a 5% prob-\nability or less), then that is considered to be good evidence for the presence of a signiﬁcant\npattern in the data. The probabilities are calculated from standard distributions of the amount\nof deviation one would expect to see in random sampling.\nIn this case, the null hypothesis is that the attribute is irrelevant and, hence, that the\ninformation gain for an inﬁnitely large sample would be zero. We need to calculate the\nprobability that, under the null hypothesis, a sample of size v = n + p would exhibit the\nobserved deviation from the expected distribution of positive and negative examples. We can\nmeasure the deviation by comparing the actual numbers of positive and negative examples in\n2 The gain will be strictly positive except for the unlikely case where all the proportions are exactly the same.\n(See Exercise 18.5.) 706\nChapter\n18.\nLearning from Examples\neach subset, pk and nk, with the expected numbers, ˆpk and ˆnk, assuming true irrelevance:\nˆpk = p × pk + nk\np + n\nˆnk = n × pk + nk\np + n\n.\nA convenient measure of the total deviation is given by\nΔ =\nd\n\f\nk=1\n(pk −ˆpk)2\nˆpk\n+ (nk −ˆnk)2\nˆnk\n.\nUnder the null hypothesis, the value of Δ is distributed according to the χ2 (chi-squared)",
  "ˆpk = p × pk + nk\np + n\nˆnk = n × pk + nk\np + n\n.\nA convenient measure of the total deviation is given by\nΔ =\nd\n\f\nk=1\n(pk −ˆpk)2\nˆpk\n+ (nk −ˆnk)2\nˆnk\n.\nUnder the null hypothesis, the value of Δ is distributed according to the χ2 (chi-squared)\ndistribution with v −1 degrees of freedom. We can use a χ2 table or a standard statistical\nlibrary routine to see if a particular Δ value conﬁrms or rejects the null hypothesis. For\nexample, consider the restaurant type attribute, with four values and thus three degrees of\nfreedom. A value of Δ = 7.82 or more would reject the null hypothesis at the 5% level (and a\nvalue of Δ = 11.35 or more would reject at the 1% level). Exercise 18.8 asks you to extend the\nDECISION-TREE-LEARNING algorithm to implement this form of pruning, which is known\nas χ2 pruning.\nχ2 PRUNING\nWith pruning, noise in the examples can be tolerated. Errors in the example’s label (e.g.,\nan example (x, Yes) that should be (x, No)) give a linear increase in prediction error, whereas\nerrors in the descriptions of examples (e.g., Price = $ when it was actually Price = $$) have\nan asymptotic effect that gets worse as the tree shrinks down to smaller sets. Pruned trees\nperform signiﬁcantly better than unpruned trees when the data contain a large amount of\nnoise. Also, the pruned trees are often much smaller and hence easier to understand.\nOne ﬁnal warning: You might think that χ2 pruning and information gain look similar,\nso why not combine them using an approach called early stopping—have the decision tree\nEARLY STOPPING\nalgorithm stop generating nodes when there is no good attribute to split on, rather than going\nto all the trouble of generating nodes and then pruning them away. The problem with early\nstopping is that it stops us from recognizing situations where there is no one good attribute,\nbut there are combinations of attributes that are informative. For example, consider the XOR\nfunction of two binary attributes. If there are roughly equal number of examples for all four\ncombinations of input values, then neither attribute will be informative, yet the correct thing\nto do is to split on one of the attributes (it doesn’t matter which one), and then at the second\nlevel we will get splits that are informative. Early stopping would miss this, but generate-\nand-then-prune handles it correctly.\n18.3.6\nBroadening the applicability of decision trees\nIn order to extend decision tree induction to a wider variety of problems, a number of issues",
  "and-then-prune handles it correctly.\n18.3.6\nBroadening the applicability of decision trees\nIn order to extend decision tree induction to a wider variety of problems, a number of issues\nmust be addressed. We will brieﬂy mention several, suggesting that a full understanding is\nbest obtained by doing the associated exercises:\n• Missing data: In many domains, not all the attribute values will be known for every\nexample. The values might have gone unrecorded, or they might be too expensive to\nobtain. This gives rise to two problems: First, given a complete decision tree, how\nshould one classify an example that is missing one of the test attributes? Second, how Section 18.3.\nLearning Decision Trees\n707\nshould one modify the information-gain formula when some examples have unknown\nvalues for the attribute? These questions are addressed in Exercise 18.9.\n• Multivalued attributes: When an attribute has many possible values, the information\ngain measure gives an inappropriate indication of the attribute’s usefulness. In the ex-\ntreme case, an attribute such as ExactTime has a different value for every example,\nwhich means each subset of examples is a singleton with a unique classiﬁcation, and\nthe information gain measure would have its highest value for this attribute. But choos-\ning this split ﬁrst is unlikely to yield the best tree. One solution is to use the gain ratio\nGAIN RATIO\n(Exercise 18.10). Another possibility is to allow a Boolean test of the form A = vk, that\nis, picking out just one of the possible values for an attribute, leaving the remaining\nvalues to possibly be tested later in the tree.\n• Continuous and integer-valued input attributes: Continuous or integer-valued at-\ntributes such as Height and Weight, have an inﬁnite set of possible values. Rather than\ngenerate inﬁnitely many branches, decision-tree learning algorithms typically ﬁnd the\nsplit point that gives the highest information gain. For example, at a given node in\nSPLIT POINT\nthe tree, it might be the case that testing on Weight > 160 gives the most informa-\ntion. Efﬁcient methods exist for ﬁnding good split points: start by sorting the values\nof the attribute, and then consider only split points that are between two examples in\nsorted order that have different classiﬁcations, while keeping track of the running totals\nof positive and negative examples on each side of the split point. Splitting is the most\nexpensive part of real-world decision tree learning applications.",
  "sorted order that have different classiﬁcations, while keeping track of the running totals\nof positive and negative examples on each side of the split point. Splitting is the most\nexpensive part of real-world decision tree learning applications.\n• Continuous-valued output attributes: If we are trying to predict a numerical output\nvalue, such as the price of an apartment, then we need a regression tree rather than a\nREGRESSION TREE\nclassiﬁcation tree. A regression tree has at each leaf a linear function of some subset\nof numerical attributes, rather than a single value. For example, the branch for two-\nbedroom apartments might end with a linear function of square footage, number of\nbathrooms, and average income for the neighborhood. The learning algorithm must\ndecide when to stop splitting and begin applying linear regression (see Section 18.6)\nover the attributes.\nA decision-tree learning system for real-world applications must be able to handle all of\nthese problems. Handling continuous-valued variables is especially important, because both\nphysical and ﬁnancial processes provide numerical data. Several commercial packages have\nbeen built that meet these criteria, and they have been used to develop thousands of ﬁelded\nsystems. In many areas of industry and commerce, decision trees are usually the ﬁrst method\ntried when a classiﬁcation method is to be extracted from a data set. One important property\nof decision trees is that it is possible for a human to understand the reason for the output of the\nlearning algorithm. (Indeed, this is a legal requirement for ﬁnancial decisions that are subject\nto anti-discrimination laws.) This is a property not shared by some other representations,\nsuch as neural networks. 708\nChapter\n18.\nLearning from Examples\n18.4\nEVALUATING AND CHOOSING THE BEST HYPOTHESIS\nWe want to learn a hypothesis that ﬁts the future data best. To make that precise we need\nto deﬁne “future data” and “best.” We make the stationarity assumption: that there is a\nSTATIONARITY\nASSUMPTION\nprobability distribution over examples that remains stationary over time. Each example data\npoint (before we see it) is a random variable Ej whose observed value ej = (xj, yj) is sampled\nfrom that distribution, and is independent of the previous examples:\nP(Ej|Ej−1, Ej−2, . . .) = P(Ej) ,\nand each example has an identical prior probability distribution:\nP(Ej) = P(Ej−1) = P(Ej−2) = · · · .",
  "from that distribution, and is independent of the previous examples:\nP(Ej|Ej−1, Ej−2, . . .) = P(Ej) ,\nand each example has an identical prior probability distribution:\nP(Ej) = P(Ej−1) = P(Ej−2) = · · · .\nExamples that satisfy these assumptions are called independent and identically distributed or\ni.i.d.. An i.i.d. assumption connects the past to the future; without some such connection, all\nI.I.D.\nbets are off—the future could be anything. (We will see later that learning can still occur if\nthere are slow changes in the distribution.)\nThe next step is to deﬁne “best ﬁt.” We deﬁne the error rate of a hypothesis as the\nERROR RATE\nproportion of mistakes it makes—the proportion of times that h(x) ̸= y for an (x, y) example.\nNow, just because a hypothesis h has a low error rate on the training set does not mean that\nit will generalize well. A professor knows that an exam will not accurately evaluate students\nif they have already seen the exam questions. Similarly, to get an accurate evaluation of a\nhypothesis, we need to test it on a set of examples it has not seen yet. The simplest approach is\nthe one we have seen already: randomly split the available data into a training set from which\nthe learning algorithm produces h and a test set on which the accuracy of h is evaluated. This\nmethod, sometimes called holdout cross-validation, has the disadvantage that it fails to use\nHOLDOUT\nCROSS-VALIDATION\nall the available data; if we use half the data for the test set, then we are only training on half\nthe data, and we may get a poor hypothesis. On the other hand, if we reserve only 10% of\nthe data for the test set, then we may, by statistical chance, get a poor estimate of the actual\naccuracy.\nWe can squeeze more out of the data and still get an accurate estimate using a technique\ncalled k-fold cross-validation. The idea is that each example serves double duty—as training\nK-FOLD\nCROSS-VALIDATION\ndata and test data. First we split the data into k equal subsets. We then perform k rounds of\nlearning; on each round 1/k of the data is held out as a test set and the remaining examples\nare used as training data. The average test set score of the k rounds should then be a better\nestimate than a single score. Popular values for k are 5 and 10—enough to give an estimate\nthat is statistically likely to be accurate, at a cost of 5 to 10 times longer computation time.\nThe extreme is k = n, also known as leave-one-out cross-validation or LOOCV.\nLEAVE-ONE-OUT\nCROSS-VALIDATION",
  "that is statistically likely to be accurate, at a cost of 5 to 10 times longer computation time.\nThe extreme is k = n, also known as leave-one-out cross-validation or LOOCV.\nLEAVE-ONE-OUT\nCROSS-VALIDATION\nLOOCV\nDespite the best efforts of statistical methodologists, users frequently invalidate their\nresults by inadvertently peeking at the test data. Peeking can happen like this: A learning\nPEEKING\nalgorithm has various “knobs” that can be twiddled to tune its behavior—for example, various\ndifferent criteria for choosing the next attribute in decision tree learning. The researcher\ngenerates hypotheses for various different settings of the knobs, measures their error rates on\nthe test set, and reports the error rate of the best hypothesis. Alas, peeking has occurred! The Section 18.4.\nEvaluating and Choosing the Best Hypothesis\n709\nreason is that the hypothesis was selected on the basis of its test set error rate, so information\nabout the test set has leaked into the learning algorithm.\nPeeking is a consequence of using test-set performance to both choose a hypothesis and\nevaluate it. The way to avoid this is to really hold the test set out—lock it away until you\nare completely done with learning and simply wish to obtain an independent evaluation of\nthe ﬁnal hypothesis. (And then, if you don’t like the results . . . you have to obtain, and lock\naway, a completely new test set if you want to go back and ﬁnd a better hypothesis.) If the\ntest set is locked away, but you still want to measure performance on unseen data as a way of\nselecting a good hypothesis, then divide the available data (without the test set) into a training\nset and a validation set. The next section shows how to use validation sets to ﬁnd a good\nVALIDATION SET\ntradeoff between hypothesis complexity and goodness of ﬁt.\n18.4.1\nModel selection: Complexity versus goodness of ﬁt\nIn Figure 18.1 (page 696) we showed that higher-degree polynomials can ﬁt the training data\nbetter, but when the degree is too high they will overﬁt, and perform poorly on validation data.\nChoosing the degree of the polynomial is an instance of the problem of model selection. You\nMODEL SELECTION\ncan think of the task of ﬁnding the best hypothesis as two tasks: model selection deﬁnes the\nhypothesis space and then optimization ﬁnds the best hypothesis within that space.\nOPTIMIZATION\nIn this section we explain how to select among models that are parameterized by size.",
  "hypothesis space and then optimization ﬁnds the best hypothesis within that space.\nOPTIMIZATION\nIn this section we explain how to select among models that are parameterized by size.\nFor example, with polynomials we have size = 1 for linear functions, size = 2 for quadratics,\nand so on. For decision trees, the size could be the number of nodes in the tree. In all cases\nwe want to ﬁnd the value of the size parameter that best balances underﬁtting and overﬁtting\nto give the best test set accuracy.\nAn algorithm to perform model selection and optimization is shown in Figure 18.8. It\nis a wrapper that takes a learning algorithm as an argument (DECISION-TREE-LEARNING,\nWRAPPER\nfor example). The wrapper enumerates models according to a parameter, size. For each size,\nit uses cross validation on Learner to compute the average error rate on the training and\ntest sets. We start with the smallest, simplest models (which probably underﬁt the data), and\niterate, considering more complex models at each step, until the models start to overﬁt. In\nFigure 18.9 we see typical curves: the training set error decreases monotonically (although\nthere may in general be slight random variation), while the validation set error decreases at\nﬁrst, and then increases when the model begins to overﬁt. The cross-validation procedure\npicks the value of size with the lowest validation set error; the bottom of the U-shaped curve.\nWe then generate a hypothesis of that size, using all the data (without holding out any of it).\nFinally, of course, we should evaluate the returned hypothesis on a separate test set.\nThis approach requires that the learning algorithm accept a parameter, size, and deliver\na hypothesis of that size. As we said, for decision tree learning, the size can be the number of\nnodes. We can modify DECISION-TREE-LEARNER so that it takes the number of nodes as\nan input, builds the tree breadth-ﬁrst rather than depth-ﬁrst (but at each level it still chooses\nthe highest gain attribute ﬁrst), and stops when it reaches the desired number of nodes. 710\nChapter\n18.\nLearning from Examples\nfunction CROSS-VALIDATION-WRAPPER(Learner,k,examples) returns a hypothesis\nlocal variables: errT, an array, indexed by size, storing training-set error rates\nerrV , an array, indexed by size, storing validation-set error rates\nfor size = 1 to ∞do\nerrT[size],errV [size] ←CROSS-VALIDATION(Learner, size,k, examples)\nif errT has converged then do\nbest size ←the value of size with minimum errV [size]",
  "errV , an array, indexed by size, storing validation-set error rates\nfor size = 1 to ∞do\nerrT[size],errV [size] ←CROSS-VALIDATION(Learner, size,k, examples)\nif errT has converged then do\nbest size ←the value of size with minimum errV [size]\nreturn Learner(best size,examples)\nfunction CROSS-VALIDATION(Learner,size,k,examples) returns two values:\naverage training set error rate, average validation set error rate\nfold errT ←0; fold errV ←0\nfor fold = 1 to k do\ntraining set,validation set ←PARTITION(examples,fold,k)\nh ←Learner(size,training set)\nfold errT ←fold errT + ERROR-RATE(h,training set)\nfold errV ←fold errV +ERROR-RATE(h,validation set)\nreturn fold errT/k, fold errV /k\nFigure 18.8\nAn algorithm to select the model that has the lowest error rate on validation\ndata by building models of increasing complexity, and choosing the one with best empir-\nical error rate on validation data. Here errT means error rate on the training data, and\nerrV means error rate on the validation data. Learner(size, examples) returns a hypoth-\nesis whose complexity is set by the parameter size, and which is trained on the examples.\nPARTITION(examples, fold, k) splits examples into two subsets: a validation set of size N/k\nand a training set with all the other examples. The split is different for each value of fold.\n18.4.2\nFrom error rates to loss\nSo far, we have been trying to minimize error rate. This is clearly better than maximizing\nerror rate, but it is not the full story. Consider the problem of classifying email messages\nas spam or non-spam. It is worse to classify non-spam as spam (and thus potentially miss\nan important message) then to classify spam as non-spam (and thus suffer a few seconds of\nannoyance). So a classiﬁer with a 1% error rate, where almost all the errors were classifying\nspam as non-spam, would be better than a classiﬁer with only a 0.5% error rate, if most of\nthose errors were classifying non-spam as spam. We saw in Chapter 16 that decision-makers\nshould maximize expected utility, and utility is what learners should maximize as well. In\nmachine learning it is traditional to express utilities by means of a loss function. The loss\nLOSS FUNCTION\nfunction L(x, y, ˆy) is deﬁned as the amount of utility lost by predicting h(x) = ˆy when the\ncorrect answer is f(x) = y:\nL(x, y, ˆy) = Utility(result of using y given an input x)\n−Utility(result of using ˆy given an input x) Section 18.4.\nEvaluating and Choosing the Best Hypothesis\n711\n 0\n 10\n 20\n 30\n 40\n 50\n 60",
  "correct answer is f(x) = y:\nL(x, y, ˆy) = Utility(result of using y given an input x)\n−Utility(result of using ˆy given an input x) Section 18.4.\nEvaluating and Choosing the Best Hypothesis\n711\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n 10\nError rate\nTree size\nValidation Set Error\nTraining Set Error\nFigure 18.9\nError rates on training data (lower, dashed line) and validation data (upper,\nsolid line) for different size decision trees. We stop when the training set error rate asymp-\ntotes, and then choose the tree with minimal error on the validation set; in this case the tree\nof size 7 nodes.\nThis is the most general formulation of the loss function. Often a simpliﬁed version is used,\nL(y, ˆy), that is independent of x. We will use the simpliﬁed version for the rest of this\nchapter, which means we can’t say that it is worse to misclassify a letter from Mom than it\nis to misclassify a letter from our annoying cousin, but we can say it is 10 times worse to\nclassify non-spam as spam than vice-versa:\nL(spam, nospam) = 1,\nL(nospam, spam) = 10 .\nNote that L(y, y) is always zero; by deﬁnition there is no loss when you guess exactly right.\nFor functions with discrete outputs, we can enumerate a loss value for each possible mis-\nclassiﬁcation, but we can’t enumerate all the possibilities for real-valued data. If f(x) is\n137.035999, we would be fairly happy with h(x) = 137.036, but just how happy should we\nbe? In general small errors are better than large ones; two functions that implement that idea\nare the absolute value of the difference (called the L1 loss), and the square of the difference\n(called the L2 loss). If we are content with the idea of minimizing error rate, we can use\nthe L0/1 loss function, which has a loss of 1 for an incorrect answer and is appropriate for\ndiscrete-valued outputs:\nAbsolute value loss: L1(y, ˆy) = |y −ˆy|\nSquared error loss:\nL2(y, ˆy) = (y −ˆy)2\n0/1 loss:\nL0/1(y, ˆy) = 0 if y = ˆy, else 1\nThe learning agent can theoretically maximize its expected utility by choosing the hypoth-\nesis that minimizes expected loss over all input–output pairs it will see. It is meaningless\nto talk about this expectation without deﬁning a prior probability distribution, P(X, Y ) over\nexamples. Let E be the set of all possible input–output examples. Then the expected gener-\nalization loss for a hypothesis h (with respect to loss function L) is\nGENERALIZATION\nLOSS 712\nChapter\n18.\nLearning from Examples\nGenLossL(h) =\n\f\n(x,y)∈E",
  "examples. Let E be the set of all possible input–output examples. Then the expected gener-\nalization loss for a hypothesis h (with respect to loss function L) is\nGENERALIZATION\nLOSS 712\nChapter\n18.\nLearning from Examples\nGenLossL(h) =\n\f\n(x,y)∈E\nL(y, h(x)) P(x, y) ,\nand the best hypothesis, h∗, is the one with the minimum expected generalization loss:\nh∗= argmin\nh∈H\nGenLossL(h) .\nBecause P(x, y) is not known, the learning agent can only estimate generalization loss with\nempirical loss on a set of examples, E:\nEMPIRICAL LOSS\nEmpLossL,E(h) = 1\nN\n\f\n(x,y)∈E\nL(y, h(x)) .\nThe estimated best hypothesis ˆh∗is then the one with minimum empirical loss:\nˆh∗= argmin\nh∈H\nEmpLoss L,E(h) .\nThere are four reasons why ˆh∗may differ from the true function, f: unrealizability, variance,\nnoise, and computational complexity. First, f may not be realizable—may not be in H—or\nmay be present in such a way that other hypotheses are preferred. Second, a learning algo-\nrithm will return different hypotheses for different sets of examples, even if those sets are\ndrawn from the same true function f, and those hypotheses will make different predictions\non new examples. The higher the variance among the predictions, the higher the probability\nof signiﬁcant error. Note that even when the problem is realizable, there will still be random\nvariance, but that variance decreases towards zero as the number of training examples in-\ncreases. Third, f may be nondeterministic or noisy—it may return different values for f(x)\nNOISE\neach time x occurs. By deﬁnition, noise cannot be predicted; in many cases, it arises because\nthe observed labels y are the result of attributes of the environment not listed in x. And ﬁnally,\nwhen H is complex, it can be computationally intractable to systematically search the whole\nhypothesis space. The best we can do is a local search (hill climbing or greedy search) that\nexplores only part of the space. That gives us an approximation error. Combining the sources\nof error, we’re left with an estimation of an approximation of the true function f.\nTraditional methods in statistics and the early years of machine learning concentrated\non small-scale learning, where the number of training examples ranged from dozens to the\nSMALL-SCALE\nLEARNING\nlow thousands. Here the generalization error mostly comes from the approximation error of\nnot having the true f in the hypothesis space, and from estimation error of not having enough",
  "SMALL-SCALE\nLEARNING\nlow thousands. Here the generalization error mostly comes from the approximation error of\nnot having the true f in the hypothesis space, and from estimation error of not having enough\ntraining examples to limit variance. In recent years there has been more emphasis on large-\nscale learning, often with millions of examples. Here the generalization error is dominated\nLARGE-SCALE\nLEARNING\nby limits of computation: there is enough data and a rich enough model that we could ﬁnd an\nh that is very close to the true f, but the computation to ﬁnd it is too complex, so we settle\nfor a sub-optimal approximation.\n18.4.3\nRegularization\nIn Section 18.4.1, we saw how to do model selection with cross-validation on model size. An\nalternative approach is to search for a hypothesis that directly minimizes the weighted sum of Section 18.5.\nThe Theory of Learning\n713\nempirical loss and the complexity of the hypothesis, which we will call the total cost:\nCost(h) = EmpLoss(h) + λ Complexity(h)\nˆh∗= argmin\nh∈H\nCost(h) .\nHere λ is a parameter, a positive number that serves as a conversion rate between loss and\nhypothesis complexity (which after all are not measured on the same scale). This approach\ncombines loss and complexity into one metric, allowing us to ﬁnd the best hypothesis all at\nonce. Unfortunately we still need to do a cross-validation search to ﬁnd the hypothesis that\ngeneralizes best, but this time it is with different values of λ rather than size. We select the\nvalue of λ that gives us the best validation set score.\nThis process of explicitly penalizing complex hypotheses is called regularization (be-\nREGULARIZATION\ncause it looks for a function that is more regular, or less complex). Note that the cost function\nrequires us to make two choices: the loss function and the complexity measure, which is\ncalled a regularization function. The choice of regularization function depends on the hy-\npothesis space. For example, a good regularization function for polynomials is the sum of\nthe squares of the coefﬁcients—keeping the sum small would guide us away from the wiggly\npolynomials in Figure 18.1(b) and (c). We will show an example of this type of regularization\nin Section 18.6.\nAnother way to simplify models is to reduce the dimensions that the models work with.\nA process of feature selection can be performed to discard attributes that appear to be irrel-\nFEATURE SELECTION\nevant. χ2 pruning is a kind of feature selection.",
  "Another way to simplify models is to reduce the dimensions that the models work with.\nA process of feature selection can be performed to discard attributes that appear to be irrel-\nFEATURE SELECTION\nevant. χ2 pruning is a kind of feature selection.\nIt is in fact possible to have the empirical loss and the complexity measured on the\nsame scale, without the conversion factor λ: they can both be measured in bits. First encode\nthe hypothesis as a Turing machine program, and count the number of bits. Then count\nthe number of bits required to encode the data, where a correctly predicted example costs\nzero bits and the cost of an incorrectly predicted example depends on how large the error is.\nThe minimum description length or MDL hypothesis minimizes the total number of bits\nMINIMUM\nDESCRIPTION\nLENGTH\nrequired. This works well in the limit, but for smaller problems there is a difﬁculty in that\nthe choice of encoding for the program—for example, how best to encode a decision tree\nas a bit string—affects the outcome. In Chapter 20 (page 805), we describe a probabilistic\ninterpretation of the MDL approach.\n18.5\nTHE THEORY OF LEARNING\nThe main unanswered question in learning is this: How can we be sure that our learning\nalgorithm has produced a hypothesis that will predict the correct value for previously unseen\ninputs? In formal terms, how do we know that the hypothesis h is close to the target function\nf if we don’t know what f is? These questions have been pondered for several centuries.\nIn more recent decades, other questions have emerged: how many examples do we need\nto get a good h? What hypothesis space should we use? If the hypothesis space is very\ncomplex, can we even ﬁnd the best h, or do we have to settle for a local maximum in the 714\nChapter\n18.\nLearning from Examples\nspace of hypotheses? How complex should h be? How do we avoid overﬁtting? This section\nexamines these questions.\nWe’ll start with the question of how many examples are needed for learning. We saw\nfrom the learning curve for decision tree learning on the restaurant problem (Figure 18.7 on\npage 703) that improves with more training data. Learning curves are useful, but they are\nspeciﬁc to a particular learning algorithm on a particular problem. Are there some more gen-\neral principles governing the number of examples needed in general? Questions like this are\naddressed by computational learning theory, which lies at the intersection of AI, statistics,\nCOMPUTATIONAL\nLEARNING THEORY",
  "eral principles governing the number of examples needed in general? Questions like this are\naddressed by computational learning theory, which lies at the intersection of AI, statistics,\nCOMPUTATIONAL\nLEARNING THEORY\nand theoretical computer science. The underlying principle is that any hypothesis that is seri-\nously wrong will almost certainly be “found out” with high probability after a small number\nof examples, because it will make an incorrect prediction. Thus, any hypothesis that is consis-\ntent with a sufﬁciently large set of training examples is unlikely to be seriously wrong: that is,\nit must be probably approximately correct. Any learning algorithm that returns hypotheses\nPROBABLY\nAPPROXIMATELY\nCORRECT\nthat are probably approximately correct is called a PAC learning algorithm; we can use this\nPAC LEARNING\napproach to provide bounds on the performance of various learning algorithms.\nPAC-learning theorems, like all theorems, are logical consequences of axioms. When\na theorem (as opposed to, say, a political pundit) states something about the future based on\nthe past, the axioms have to provide the “juice” to make that connection. For PAC learning,\nthe juice is provided by the stationarity assumption introduced on page 708, which says that\nfuture examples are going to be drawn from the same ﬁxed distribution P(E) = P(X, Y )\nas past examples. (Note that we do not have to know what distribution that is, just that it\ndoesn’t change.) In addition, to keep things simple, we will assume that the true function f\nis deterministic and is a member of the hypothesis class H that is being considered.\nThe simplest PAC theorems deal with Boolean functions, for which the 0/1 loss is ap-\npropriate. The error rate of a hypothesis h, deﬁned informally earlier, is deﬁned formally\nhere as the expected generalization error for examples drawn from the stationary distribution:\nerror(h) = GenLossL0/1(h) =\n\f\nx,y\nL0/1(y, h(x)) P(x, y) .\nIn other words, error(h) is the probability that h misclassiﬁes a new example. This is the\nsame quantity being measured experimentally by the learning curves shown earlier.\nA hypothesis h is called approximately correct if error(h) ≤ϵ, where ϵ is a small\nconstant. We will show that we can ﬁnd an N such that, after seeing N examples, with high\nprobability, all consistent hypotheses will be approximately correct. One can think of an\napproximately correct hypothesis as being “close” to the true function in hypothesis space: it",
  "probability, all consistent hypotheses will be approximately correct. One can think of an\napproximately correct hypothesis as being “close” to the true function in hypothesis space: it\nlies inside what is called the ϵ-ball around the true function f. The hypothesis space outside\nϵ-BALL\nthis ball is called Hbad.\nWe can calculate the probability that a “seriously wrong” hypothesis hb ∈Hbad is\nconsistent with the ﬁrst N examples as follows. We know that error(hb) > ϵ. Thus, the\nprobability that it agrees with a given example is at most 1 −ϵ. Since the examples are\nindependent, the bound for N examples is\nP(hb agrees with N examples) ≤(1 −ϵ)N . Section 18.5.\nThe Theory of Learning\n715\nThe probability that Hbad contains at least one consistent hypothesis is bounded by the sum\nof the individual probabilities:\nP(Hbad contains a consistent hypothesis) ≤|Hbad|(1 −ϵ)N ≤|H|(1 −ϵ)N ,\nwhere we have used the fact that |Hbad| ≤|H|. We would like to reduce the probability of\nthis event below some small number δ:\n|H|(1 −ϵ)N ≤δ .\nGiven that 1 −ϵ ≤e−ϵ, we can achieve this if we allow the algorithm to see\nN ≥1\nϵ\n\r\nln 1\nδ + ln |H|\n\u000e\n(18.1)\nexamples. Thus, if a learning algorithm returns a hypothesis that is consistent with this many\nexamples, then with probability at least 1 −δ, it has error at most ϵ. In other words, it is\nprobably approximately correct. The number of required examples, as a function of ϵ and δ,\nis called the sample complexity of the hypothesis space.\nSAMPLE\nCOMPLEXITY\nAs we saw earlier, if H is the set of all Boolean functions on n attributes, then |H| =\n22n. Thus, the sample complexity of the space grows as 2n. Because the number of possible\nexamples is also 2n, this suggests that PAC-learning in the class of all Boolean functions\nrequires seeing all, or nearly all, of the possible examples. A moment’s thought reveals the\nreason for this: H contains enough hypotheses to classify any given set of examples in all\npossible ways. In particular, for any set of N examples, the set of hypotheses consistent with\nthose examples contains equal numbers of hypotheses that predict xN+1 to be positive and\nhypotheses that predict xN+1 to be negative.\nTo obtain real generalization to unseen examples, then, it seems we need to restrict\nthe hypothesis space H in some way; but of course, if we do restrict the space, we might\neliminate the true function altogether. There are three ways to escape this dilemma. The ﬁrst,",
  "the hypothesis space H in some way; but of course, if we do restrict the space, we might\neliminate the true function altogether. There are three ways to escape this dilemma. The ﬁrst,\nwhich we will cover in Chapter 19, is to bring prior knowledge to bear on the problem. The\nsecond, which we introduced in Section 18.4.3, is to insist that the algorithm return not just\nany consistent hypothesis, but preferably a simple one (as is done in decision tree learning). In\ncases where ﬁnding simple consistent hypotheses is tractable, the sample complexity results\nare generally better than for analyses based only on consistency. The third escape, which\nwe pursue next, is to focus on learnable subsets of the entire hypothesis space of Boolean\nfunctions. This approach relies on the assumption that the restricted language contains a\nhypothesis h that is close enough to the true function f; the beneﬁts are that the restricted\nhypothesis space allows for effective generalization and is typically easier to search. We now\nexamine one such restricted language in more detail.\n18.5.1\nPAC learning example: Learning decision lists\nWe now show how to apply PAC learning to a new hypothesis space: decision lists. A\nDECISION LISTS\ndecision list consists of a series of tests, each of which is a conjunction of literals. If a\ntest succeeds when applied to an example description, the decision list speciﬁes the value\nto be returned. If the test fails, processing continues with the next test in the list. Decision\nlists resemble decision trees, but their overall structure is simpler: they branch only in one 716\nChapter\n18.\nLearning from Examples\nPatrons(x, Some)\nNo\nYes\nYes\nNo\nPatrons(x, Full)\n  Fri/Sat(x)\nYes\nNo\nYes\n^\nFigure 18.10\nA decision list for the restaurant problem.\ndirection. In contrast, the individual tests are more complex. Figure 18.10 shows a decision\nlist that represents the following hypothesis:\nWillWait ⇔(Patrons = Some) ∨(Patrons = Full ∧Fri/Sat) .\nIf we allow tests of arbitrary size, then decision lists can represent any Boolean function\n(Exercise 18.14). On the other hand, if we restrict the size of each test to at most k literals,\nthen it is possible for the learning algorithm to generalize successfully from a small number\nof examples. We call this language k-DL. The example in Figure 18.10 is in 2-DL. It is easy to\nk-DL\nshow (Exercise 18.14) that k-DL includes as a subset the language k-DT, the set of all decision\nk-DT",
  "of examples. We call this language k-DL. The example in Figure 18.10 is in 2-DL. It is easy to\nk-DL\nshow (Exercise 18.14) that k-DL includes as a subset the language k-DT, the set of all decision\nk-DT\ntrees of depth at most k. It is important to remember that the particular language referred to\nby k-DL depends on the attributes used to describe the examples. We will use the notation\nk-DL(n) to denote a k-DL language using n Boolean attributes.\nThe ﬁrst task is to show that k-DL is learnable—that is, that any function in k-DL can\nbe approximated accurately after training on a reasonable number of examples. To do this,\nwe need to calculate the number of hypotheses in the language. Let the language of tests—\nconjunctions of at most k literals using n attributes—be Conj (n, k). Because a decision list\nis constructed of tests, and because each test can be attached to either a Yes or a No outcome\nor can be absent from the decision list, there are at most 3|Conj (n,k)| distinct sets of component\ntests. Each of these sets of tests can be in any order, so\n|k-DL(n)| ≤3|Conj (n,k)||Conj (n, k)|! .\nThe number of conjunctions of k literals from n attributes is given by\n|Conj (n, k)| =\nk\n\f\ni=0\n\r2n\ni\n\u000e\n= O(nk) .\nHence, after some work, we obtain\n|k-DL(n)| = 2O(nk log2(nk)) .\nWe can plug this into Equation (18.1) to show that the number of examples needed for PAC-\nlearning a k-DL function is polynomial in n:\nN ≥1\nϵ\n\r\nln 1\nδ + O(nk log2(nk))\n\u000e\n.\nTherefore, any algorithm that returns a consistent decision list will PAC-learn a k-DL function\nin a reasonable number of examples, for small k.\nThe next task is to ﬁnd an efﬁcient algorithm that returns a consistent decision list.\nWe will use a greedy algorithm called DECISION-LIST-LEARNING that repeatedly ﬁnds a Section 18.6.\nRegression and Classiﬁcation with Linear Models\n717\nfunction DECISION-LIST-LEARNING(examples) returns a decision list, or failure\nif examples is empty then return the trivial decision list No\nt ←a test that matches a nonempty subset examplest of examples\nsuch that the members of examplest are all positive or all negative\nif there is no such t then return failure\nif the examples in examplest are positive then o ←Yes else o ←No\nreturn a decision list with initial test t and outcome o and remaining tests given by\nDECISION-LIST-LEARNING(examples −examplest)\nFigure 18.11\nAn algorithm for learning decision lists.\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n20\n40\n60\n80\n100\nProportion correct on test set\nTraining set size",
  "DECISION-LIST-LEARNING(examples −examplest)\nFigure 18.11\nAn algorithm for learning decision lists.\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n20\n40\n60\n80\n100\nProportion correct on test set\nTraining set size\nDecision tree\nDecision list\nFigure 18.12\nLearning curve for DECISION-LIST-LEARNING algorithm on the restaurant\ndata. The curve for DECISION-TREE-LEARNING is shown for comparison.\ntest that agrees exactly with some subset of the training set. Once it ﬁnds such a test, it\nadds it to the decision list under construction and removes the corresponding examples. It\nthen constructs the remainder of the decision list, using just the remaining examples. This is\nrepeated until there are no examples left. The algorithm is shown in Figure 18.11.\nThis algorithm does not specify the method for selecting the next test to add to the\ndecision list. Although the formal results given earlier do not depend on the selection method,\nit would seem reasonable to prefer small tests that match large sets of uniformly classiﬁed\nexamples, so that the overall decision list will be as compact as possible. The simplest strategy\nis to ﬁnd the smallest test t that matches any uniformly classiﬁed subset, regardless of the size\nof the subset. Even this approach works quite well, as Figure 18.12 suggests.\n18.6\nREGRESSION AND CLASSIFICATION WITH LINEAR MODELS\nNow it is time to move on from decision trees and lists to a different hypothesis space, one\nthat has been used for hundred of years: the class of linear functions of continuous-valued\nLINEAR FUNCTION 718\nChapter\n18.\nLearning from Examples\n 300\n 400\n 500\n 600\n 700\n 800\n 900\n 1000\n 500\n 1000  1500  2000  2500  3000  3500\nHouse price in $1000\nHouse size in square feet\nw0\nw1\nLoss\n(a)\n(b)\nFigure 18.13\n(a) Data points of price versus ﬂoor space of houses for sale in Berkeley,\nCA, in July 2009, along with the linear function hypothesis that minimizes squared error\nloss: y = 0.232x + 246. (b) Plot of the loss function \u0002\nj(w1xj + w0 −yj)2 for various\nvalues of w0, w1. Note that the loss function is convex, with a single global minimum.\ninputs. We’ll start with the simplest case: regression with a univariate linear function, oth-\nerwise known as “ﬁtting a straight line.” Section 18.6.2 covers the multivariate case. Sec-\ntions 18.6.3 and 18.6.4 show how to turn linear functions into classiﬁers by applying hard\nand soft thresholds.\n18.6.1\nUnivariate linear regression",
  "erwise known as “ﬁtting a straight line.” Section 18.6.2 covers the multivariate case. Sec-\ntions 18.6.3 and 18.6.4 show how to turn linear functions into classiﬁers by applying hard\nand soft thresholds.\n18.6.1\nUnivariate linear regression\nA univariate linear function (a straight line) with input x and output y has the form y = w1x+\nw0, where w0 and w1 are real-valued coefﬁcients to be learned. We use the letter w because\nwe think of the coefﬁcients as weights; the value of y is changed by changing the relative\nWEIGHT\nweight of one term or another. We’ll deﬁne w to be the vector [w0, w1], and deﬁne\nhw(x) = w1x + w0 .\nFigure 18.13(a) shows an example of a training set of n points in the x, y plane, each point\nrepresenting the size in square feet and the price of a house offered for sale. The task of\nﬁnding the hw that best ﬁts these data is called linear regression. To ﬁt a line to the data, all\nLINEAR REGRESSION\nwe have to do is ﬁnd the values of the weights [w0, w1] that minimize the empirical loss. It is\ntraditional (going back to Gauss3) to use the squared loss function, L2, summed over all the\ntraining examples:\nLoss(hw) =\nN\n\f\nj = 1\nL2(yj, hw(xj)) =\nN\n\f\nj = 1\n(yj −hw(xj))2 =\nN\n\f\nj = 1\n(yj −(w1xj + w0))2 .\n3 Gauss showed that if the yj values have normally distributed noise, then the most likely values of w1 and w0\nare obtained by minimizing the sum of the squares of the errors. Section 18.6.\nRegression and Classiﬁcation with Linear Models\n719\nWe would like to ﬁnd w∗= argminw Loss(hw). The sum \u0002N\nj = 1(yj −(w1xj + w0))2 is\nminimized when its partial derivatives with respect to w0 and w1 are zero:\n∂\n∂w0\nN\n\f\nj = 1\n(yj −(w1xj + w0))2 = 0 and\n∂\n∂w1\nN\n\f\nj = 1\n(yj −(w1xj + w0))2 = 0 .\n(18.2)\nThese equations have a unique solution:\nw1 = N(\u0002 xjyj) −(\u0002 xj)(\u0002 yj)\nN(\u0002 x2\nj) −(\u0002 xj)2\n; w0 = (\n\f\nyj −w1(\n\f\nxj))/N .\n(18.3)\nFor the example in Figure 18.13(a), the solution is w1 = 0.232, w0 = 246, and the line with\nthose weights is shown as a dashed line in the ﬁgure.\nMany forms of learning involve adjusting weights to minimize a loss, so it helps to\nhave a mental picture of what’s going on in weight space—the space deﬁned by all possible\nWEIGHT SPACE\nsettings of the weights. For univariate linear regression, the weight space deﬁned by w0 and\nw1 is two-dimensional, so we can graph the loss as a function of w0 and w1 in a 3D plot (see\nFigure 18.13(b)). We see that the loss function is convex, as deﬁned on page 133; this is true",
  "w1 is two-dimensional, so we can graph the loss as a function of w0 and w1 in a 3D plot (see\nFigure 18.13(b)). We see that the loss function is convex, as deﬁned on page 133; this is true\nfor every linear regression problem with an L2 loss function, and implies that there are no\nlocal minima. In some sense that’s the end of the story for linear models; if we need to ﬁt\nlines to data, we apply Equation (18.3).4\nTo go beyond linear models, we will need to face the fact that the equations deﬁning\nminimum loss (as in Equation (18.2)) will often have no closed-form solution. Instead, we\nwill face a general optimization search problem in a continuous weight space. As indicated\nin Section 4.2 (page 129), such problems can be addressed by a hill-climbing algorithm that\nfollows the gradient of the function to be optimized. In this case, because we are trying to\nminimize the loss, we will use gradient descent. We choose any starting point in weight\nGRADIENT DESCENT\nspace—here, a point in the (w0, w1) plane—and then move to a neighboring point that is\ndownhill, repeating until we converge on the minimum possible loss:\nw ←any point in the parameter space\nloop until convergence do\nfor each wi in w do\nwi ←wi −α ∂\n∂wi\nLoss(w)\n(18.4)\nThe parameter α, which we called the step size in Section 4.2, is usually called the learning\nrate when we are trying to minimize loss in a learning problem. It can be a ﬁxed constant, or\nLEARNING RATE\nit can decay over time as the learning process proceeds.\nFor univariate regression, the loss function is a quadratic function, so the partial deriva-\ntive will be a linear function. (The only calculus you need to know is that\n∂\n∂xx2 = 2x and\n∂\n∂xx = 1.) Let’s ﬁrst work out the partial derivatives—the slopes—in the simpliﬁed case of\n4 With some caveats: the L2 loss function is appropriate when there is normally-distributed noise that is inde-\npendent of x; all results rely on the stationarity assumption; etc. 720\nChapter\n18.\nLearning from Examples\nonly one training example, (x, y):\n∂\n∂wi\nLoss(w) =\n∂\n∂wi\n(y −hw(x))2\n= 2(y −hw(x)) ×\n∂\n∂wi\n(y −hw(x))\n= 2(y −hw(x)) ×\n∂\n∂wi\n(y −(w1x + w0)) ,\n(18.5)\napplying this to both w0 and w1 we get:\n∂\n∂w0\nLoss(w) = −2(y −hw(x)) ;\n∂\n∂w1\nLoss(w) = −2(y −hw(x)) × x\nThen, plugging this back into Equation (18.4), and folding the 2 into the unspeciﬁed learning\nrate α, we get the following learning rule for the weights:\nw0 ←w0 + α (y −hw(x)) ;\nw1 ←w1 + α (y −hw(x)) × x",
  "∂\n∂w1\nLoss(w) = −2(y −hw(x)) × x\nThen, plugging this back into Equation (18.4), and folding the 2 into the unspeciﬁed learning\nrate α, we get the following learning rule for the weights:\nw0 ←w0 + α (y −hw(x)) ;\nw1 ←w1 + α (y −hw(x)) × x\nThese updates make intuitive sense: if hw(x) > y, i.e., the output of the hypothesis is too\nlarge, reduce w0 a bit, and reduce w1 if x was a positive input but increase w1 if x was a\nnegative input.\nThe preceding equations cover one training example. For N training examples, we want\nto minimize the sum of the individual losses for each example. The derivative of a sum is the\nsum of the derivatives, so we have:\nw0 ←w0 + α\n\f\nj\n(yj −hw(xj)) ;\nw1 ←w1 + α\n\f\nj\n(yj −hw(xj)) × xj .\nThese updates constitute the batch gradient descent learning rule for univariate linear re-\nBATCH GRADIENT\nDESCENT\ngression. Convergence to the unique global minimum is guaranteed (as long as we pick α\nsmall enough) but may be very slow: we have to cycle through all the training data for every\nstep, and there may be many steps.\nThere is another possibility, called stochastic gradient descent, where we consider\nSTOCHASTIC\nGRADIENT DESCENT\nonly a single training point at a time, taking a step after each one using Equation (18.5).\nStochastic gradient descent can be used in an online setting, where new data are coming in\none at a time, or ofﬂine, where we cycle through the same data as many times as is neces-\nsary, taking a step after considering each single example. It is often faster than batch gradient\ndescent. With a ﬁxed learning rate α, however, it does not guarantee convergence; it can os-\ncillate around the minimum without settling down. In some cases, as we see later, a schedule\nof decreasing learning rates (as in simulated annealing) does guarantee convergence.\n18.6.2\nMultivariate linear regression\nWe can easily extend to multivariate linear regression problems, in which each example xj\nMULTIVARIATE\nLINEAR REGRESSION\nis an n-element vector.5 Our hypothesis space is the set of functions of the form\nhsw(xj) = w0 + w1xj,1 + · · · + wnxj,n = w0 +\n\f\ni\nwixj,i .\n5 The reader may wish to consult Appendix A for a brief summary of linear algebra. Section 18.6.\nRegression and Classiﬁcation with Linear Models\n721\nThe w0 term, the intercept, stands out as different from the others. We can ﬁx that by inventing\na dummy input attribute, xj,0, which is deﬁned as always equal to 1. Then h is simply the",
  "Regression and Classiﬁcation with Linear Models\n721\nThe w0 term, the intercept, stands out as different from the others. We can ﬁx that by inventing\na dummy input attribute, xj,0, which is deﬁned as always equal to 1. Then h is simply the\ndot product of the weights and the input vector (or equivalently, the matrix product of the\ntranspose of the weights and the input vector):\nhsw(xj) = w · xj = w⊤xj =\n\f\ni\nwixj,i .\nThe best vector of weights, w∗, minimizes squared-error loss over the examples:\nw∗= argmin\nw\n\f\nj\nL2(yj, w · xj) .\nMultivariate linear regression is actually not much more complicated than the univariate case\nwe just covered. Gradient descent will reach the (unique) minimum of the loss function; the\nupdate equation for each weight wi is\nwi ←wi + α\n\f\nj\nxj,i(yj −hw(xj)) .\n(18.6)\nIt is also possible to solve analytically for the w that minimizes loss. Let y be the vector of\noutputs for the training examples, and X be the data matrix, i.e., the matrix of inputs with\nDATA MATRIX\none n-dimensional example per row. Then the solution\nw∗= (X⊤X)−1X⊤y\nminimizes the squared error.\nWith univariate linear regression we didn’t have to worry about overﬁtting. But with\nmultivariate linear regression in high-dimensional spaces it is possible that some dimension\nthat is actually irrelevant appears by chance to be useful, resulting in overﬁtting.\nThus, it is common to use regularization on multivariate linear functions to avoid over-\nﬁtting. Recall that with regularization we minimize the total cost of a hypothesis, counting\nboth the empirical loss and the complexity of the hypothesis:\nCost(h) = EmpLoss(h) + λ Complexity(h) .\nFor linear functions the complexity can be speciﬁed as a function of the weights. We can\nconsider a family of regularization functions:\nComplexity(hw) = Lq(w) =\n\f\ni\n|wi|q .\nAs with loss functions,6 with q = 1 we have L1 regularization, which minimizes the sum of\nthe absolute values; with q = 2, L2 regularization minimizes the sum of squares. Which reg-\nularization function should you pick? That depends on the speciﬁc problem, but L1 regular-\nization has an important advantage: it tends to produce a sparse model. That is, it often sets\nSPARSE MODEL\nmany weights to zero, effectively declaring the corresponding attributes to be irrelevant—just\nas DECISION-TREE-LEARNING does (although by a different mechanism). Hypotheses that\ndiscard attributes can be easier for a human to understand, and may be less likely to overﬁt.",
  "as DECISION-TREE-LEARNING does (although by a different mechanism). Hypotheses that\ndiscard attributes can be easier for a human to understand, and may be less likely to overﬁt.\n6 It is perhaps confusing that L1 and L2 are used for both loss functions and regularization functions. They need\nnot be used in pairs: you could use L2 loss with L1 regularization, or vice versa. 722\nChapter\n18.\nLearning from Examples\nw1\nw2\nw*\nw1\nw2\nw*\nFigure 18.14\nWhy L1 regularization tends to produce a sparse model. (a) With L1 regu-\nlarization (box), the minimal achievable loss (concentric contours) often occurs on an axis,\nmeaning a weight of zero. (b) With L2 regularization (circle), the minimal loss is likely to\noccur anywhere on the circle, giving no preference to zero weights.\nFigure 18.14 gives an intuitive explanation of why L1 regularization leads to weights of\nzero, while L2 regularization does not. Note that minimizing Loss(w) + λComplexity(w)\nis equivalent to minimizing Loss(w) subject to the constraint that Complexity(w) ≤c, for\nsome constant c that is related to λ. Now, in Figure 18.14(a) the diamond-shaped box repre-\nsents the set of points w in two-dimensional weight space that have L1 complexity less than\nc; our solution will have to be somewhere inside this box. The concentric ovals represent\ncontours of the loss function, with the minimum loss at the center. We want to ﬁnd the point\nin the box that is closest to the minimum; you can see from the diagram that, for an arbitrary\nposition of the minimum and its contours, it will be common for the corner of the box to ﬁnd\nits way closest to the minimum, just because the corners are pointy. And of course the corners\nare the points that have a value of zero in some dimension. In Figure 18.14(b), we’ve done\nthe same for the L2 complexity measure, which represents a circle rather than a diamond.\nHere you can see that, in general, there is no reason for the intersection to appear on one of\nthe axes; thus L2 regularization does not tend to produce zero weights. The result is that the\nnumber of examples required to ﬁnd a good h is linear in the number of irrelevant features for\nL2 regularization, but only logarithmic with L1 regularization. Empirical evidence on many\nproblems supports this analysis.\nAnother way to look at it is that L1 regularization takes the dimensional axes seriously,\nwhile L2 treats them as arbitrary. The L2 function is spherical, which makes it rotationally",
  "problems supports this analysis.\nAnother way to look at it is that L1 regularization takes the dimensional axes seriously,\nwhile L2 treats them as arbitrary. The L2 function is spherical, which makes it rotationally\ninvariant: Imagine a set of points in a plane, measured by their x and y coordinates. Now\nimagine rotating the axes by 45o. You’d get a different set of (x′, y′) values representing\nthe same points. If you apply L2 regularization before and after rotating, you get exactly\nthe same point as the answer (although the point would be described with the new (x′, y′)\ncoordinates). That is appropriate when the choice of axes really is arbitrary—when it doesn’t\nmatter whether your two dimensions are distances north and east; or distances north-east and Section 18.6.\nRegression and Classiﬁcation with Linear Models\n723\nsouth-east. With L1 regularization you’d get a different answer, because the L1 function is not\nrotationally invariant. That is appropriate when the axes are not interchangeable; it doesn’t\nmake sense to rotate “number of bathrooms” 45o towards “lot size.”\n18.6.3\nLinear classiﬁers with a hard threshold\nLinear functions can be used to do classiﬁcation as well as regression. For example, Fig-\nure 18.15(a) shows data points of two classes: earthquakes (which are of interest to seismolo-\ngists) and underground explosions (which are of interest to arms control experts). Each point\nis deﬁned by two input values, x1 and x2, that refer to body and surface wave magnitudes\ncomputed from the seismic signal. Given these training data, the task of classiﬁcation is to\nlearn a hypothesis h that will take new (x1, x2) points and return either 0 for earthquakes or\n1 for explosions.\n 2.5\n 3\n 3.5\n 4\n 4.5\n 5\n 5.5\n 6\n 6.5\n 7\n 7.5\n 4.5\n 5\n 5.5\n 6\n 6.5\n 7\nx2\nx1\n 2.5\n 3\n 3.5\n 4\n 4.5\n 5\n 5.5\n 6\n 6.5\n 7\n 7.5\n 4.5\n 5\n 5.5\n 6\n 6.5\n 7\nx2\nx1\n(a)\n(b)\nFigure 18.15\n(a) Plot of two seismic data parameters, body wave magnitude x1 and sur-\nface wave magnitude x2, for earthquakes (white circles) and nuclear explosions (black cir-\ncles) occurring between 1982 and 1990 in Asia and the Middle East (Kebeasy et al., 1998).\nAlso shown is a decision boundary between the classes. (b) The same domain with more data\npoints. The earthquakes and explosions are no longer linearly separable.\nA decision boundary is a line (or a surface, in higher dimensions) that separates the\nDECISION\nBOUNDARY\ntwo classes. In Figure 18.15(a), the decision boundary is a straight line. A linear decision",
  "A decision boundary is a line (or a surface, in higher dimensions) that separates the\nDECISION\nBOUNDARY\ntwo classes. In Figure 18.15(a), the decision boundary is a straight line. A linear decision\nboundary is called a linear separator and data that admit such a separator are called linearly\nLINEAR SEPARATOR\nseparable. The linear separator in this case is deﬁned by\nLINEAR\nSEPARABILITY\nx2 = 1.7x1 −4.9\nor\n−4.9 + 1.7x1 −x2 = 0 .\nThe explosions, which we want to classify with value 1, are to the right of this line with higher\nvalues of x1 and lower values of x2, so they are points for which −4.9 + 1.7x1 −x2 > 0,\nwhile earthquakes have −4.9 + 1.7x1 −x2 < 0. Using the convention of a dummy input\nx0 = 1, we can write the classiﬁcation hypothesis as\nhw(x) = 1 if w · x ≥0 and 0 otherwise. 724\nChapter\n18.\nLearning from Examples\nAlternatively, we can think of h as the result of passing the linear function w · x through a\nthreshold function:\nTHRESHOLD\nFUNCTION\nhw(x) = Threshold(w · x) where Threshold(z) = 1 if z ≥0 and 0 otherwise.\nThe threshold function is shown in Figure 18.17(a).\nNow that the hypothesis hw(x) has a well-deﬁned mathematical form, we can think\nabout choosing the weights w to minimize the loss. In Sections 18.6.1 and 18.6.2, we did\nthis both in closed form (by setting the gradient to zero and solving for the weights) and\nby gradient descent in weight space. Here, we cannot do either of those things because the\ngradient is zero almost everywhere in weight space except at those points where w · x = 0,\nand at those points the gradient is undeﬁned.\nThere is, however, a simple weight update rule that converges to a solution—that is, a\nlinear separator that classiﬁes the data perfectly–provided the data are linearly separable. For\na single example (x, y), we have\nwi ←wi + α (y −hw(x)) × xi\n(18.7)\nwhich is essentially identical to the Equation (18.6), the update rule for linear regression! This\nrule is called the perceptron learning rule, for reasons that will become clear in Section 18.7.\nPERCEPTRON\nLEARNING RULE\nBecause we are considering a 0/1 classiﬁcation problem, however, the behavior is somewhat\ndifferent. Both the true value y and the hypothesis output hw(x) are either 0 or 1, so there are\nthree possibilities:\n• If the output is correct, i.e., y = hw(x), then the weights are not changed.\n• If y is 1 but hw(x) is 0, then wi is increased when the corresponding input xi is positive",
  "three possibilities:\n• If the output is correct, i.e., y = hw(x), then the weights are not changed.\n• If y is 1 but hw(x) is 0, then wi is increased when the corresponding input xi is positive\nand decreased when xi is negative. This makes sense, because we want to make w · x\nbigger so that hw(x) outputs a 1.\n• If y is 0 but hw(x) is 1, then wi is decreased when the corresponding input xi is positive\nand increased when xi is negative. This makes sense, because we want to make w · x\nsmaller so that hw(x) outputs a 0.\nTypically the learning rule is applied one example at a time, choosing examples at random\n(as in stochastic gradient descent). Figure 18.16(a) shows a training curve for this learning\nTRAINING CURVE\nrule applied to the earthquake/explosion data shown in Figure 18.15(a). A training curve\nmeasures the classiﬁer performance on a ﬁxed training set as the learning process proceeds\non that same training set. The curve shows the update rule converging to a zero-error linear\nseparator. The “convergence” process isn’t exactly pretty, but it always works. This particular\nrun takes 657 steps to converge, for a data set with 63 examples, so each example is presented\nroughly 10 times on average. Typically, the variation across runs is very large.\nWe have said that the perceptron learning rule converges to a perfect linear separator\nwhen the data points are linearly separable, but what if they are not? This situation is all\ntoo common in the real world. For example, Figure 18.15(b) adds back in the data points\nleft out by Kebeasy et al. (1998) when they plotted the data shown in Figure 18.15(a). In\nFigure 18.16(b), we show the perceptron learning rule failing to converge even after 10,000\nsteps: even though it hits the minimum-error solution (three errors) many times, the algo-\nrithm keeps changing the weights. In general, the perceptron rule may not converge to a Section 18.6.\nRegression and Classiﬁcation with Linear Models\n725\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 100  200  300  400  500  600  700\nProportion correct\nNumber of weight updates\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 20000  40000  60000  80000  100000\nProportion correct\nNumber of weight updates\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 20000  40000  60000  80000  100000\nProportion correct\nNumber of weight updates\n(a)\n(b)\n(c)\nFigure 18.16\n(a) Plot of total training-set accuracy vs. number of iterations through the\ntraining set for the perceptron learning rule, given the earthquake/explosion data in Fig-",
  "Proportion correct\nNumber of weight updates\n(a)\n(b)\n(c)\nFigure 18.16\n(a) Plot of total training-set accuracy vs. number of iterations through the\ntraining set for the perceptron learning rule, given the earthquake/explosion data in Fig-\nure 18.15(a). (b) The same plot for the noisy, non-separable data in Figure 18.15(b); note\nthe change in scale of the x-axis. (c) The same plot as in (b), with a learning rate schedule\nα(t) = 1000/(1000 + t).\nstable solution for ﬁxed learning rate α, but if α decays as O(1/t) where t is the iteration\nnumber, then the rule can be shown to converge to a minimum-error solution when examples\nare presented in a random sequence.7 It can also be shown that ﬁnding the minimum-error\nsolution is NP-hard, so one expects that many presentations of the examples will be required\nfor convergence to be achieved. Figure 18.16(b) shows the training process with a learning\nrate schedule α(t) = 1000/(1000 + t): convergence is not perfect after 100,000 iterations,\nbut it is much better than the ﬁxed-α case.\n18.6.4\nLinear classiﬁcation with logistic regression\nWe have seen that passing the output of a linear function through the threshold function\ncreates a linear classiﬁer; yet the hard nature of the threshold causes some problems: the\nhypothesis hw(x) is not differentiable and is in fact a discontinuous function of its inputs and\nits weights; this makes learning with the perceptron rule a very unpredictable adventure. Fur-\nthermore, the linear classiﬁer always announces a completely conﬁdent prediction of 1 or 0,\neven for examples that are very close to the boundary; in many situations, we really need\nmore gradated predictions.\nAll of these issues can be resolved to a large extent by softening the threshold function—\napproximating the hard threshold with a continuous, differentiable function. In Chapter 14\n(page 522), we saw two functions that look like soft thresholds: the integral of the standard\nnormal distribution (used for the probit model) and the logistic function (used for the logit\nmodel). Although the two functions are very similar in shape, the logistic function\nLogistic(z) =\n1\n1 + e−z\n7 Technically, we require that P∞\nt = 1 α(t) = ∞and P∞\nt = 1 α2(t) < ∞. The decay α(t) = O(1/t) satisﬁes\nthese conditions. 726\nChapter\n18.\nLearning from Examples\n 0\n 0.5\n 1\n-8 -6 -4 -2  0  2  4  6  8\n 0\n 0.5\n 1\n-6 -4 -2  0\n 2\n 4\n 6\n-2\n 0\n 2\n 4\n 6\n-4\n-2\n 0\n 2\n 4\n 6\n 8\n 10\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nx1\nx2\n(a)\n(b)\n(c)\nFigure 18.17",
  "these conditions. 726\nChapter\n18.\nLearning from Examples\n 0\n 0.5\n 1\n-8 -6 -4 -2  0  2  4  6  8\n 0\n 0.5\n 1\n-6 -4 -2  0\n 2\n 4\n 6\n-2\n 0\n 2\n 4\n 6\n-4\n-2\n 0\n 2\n 4\n 6\n 8\n 10\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nx1\nx2\n(a)\n(b)\n(c)\nFigure 18.17\n(a) The hard threshold function Threshold(z) with 0/1 output.\nNote\nthat the function is nondifferentiable at z = 0. (b) The logistic function, Logistic(z) =\n1\n1+e−z , also known as the sigmoid function. (c) Plot of a logistic regression hypothesis\nhw(x) = Logistic(w · x) for the data shown in Figure 18.15(b).\nhas more convenient mathematical properties. The function is shown in Figure 18.17(b).\nWith the logistic function replacing the threshold function, we now have\nhw(x) = Logistic(w · x) =\n1\n1 + e−w·x .\nAn example of such a hypothesis for the two-input earthquake/explosion problem is shown in\nFigure 18.17(c). Notice that the output, being a number between 0 and 1, can be interpreted\nas a probability of belonging to the class labeled 1. The hypothesis forms a soft boundary\nin the input space and gives a probability of 0.5 for any input at the center of the boundary\nregion, and approaches 0 or 1 as we move away from the boundary.\nThe process of ﬁtting the weights of this model to minimize loss on a data set is called\nlogistic regression. There is no easy closed-form solution to ﬁnd the optimal value of w with\nLOGISTIC\nREGRESSION\nthis model, but the gradient descent computation is straightforward. Because our hypotheses\nno longer output just 0 or 1, we will use the L2 loss function; also, to keep the formulas\nreadable, we’ll use g to stand for the logistic function, with g′ its derivative.\nFor a single example (x, y), the derivation of the gradient is the same as for linear\nregression (Equation (18.5)) up to the point where the actual form of h is inserted. (For this\nderivation, we will need the chain rule: ∂g(f(x))/∂x = g′(f(x)) ∂f(x)/∂x.) We have\nCHAIN RULE\n∂\n∂wi\nLoss(w) =\n∂\n∂wi\n(y −hw(x))2\n= 2(y −hw(x)) ×\n∂\n∂wi\n(y −hw(x))\n= −2(y −hw(x)) × g′(w · x) × ∂\n∂wi\nw · x\n= −2(y −hw(x)) × g′(w · x) × xi . Section 18.7.\nArtiﬁcial Neural Networks\n727\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 1000\n 2000\n 3000\n 4000\n 5000\nSquared error per example\nNumber of weight updates\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 20000  40000  60000  80000  100000\nSquared error per example\nNumber of weight updates\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 20000  40000  60000  80000  100000\nSquared error per example\nNumber of weight updates\n(a)\n(b)\n(c)\nFigure 18.18",
  "0.9\n 1\n 0\n 20000  40000  60000  80000  100000\nSquared error per example\nNumber of weight updates\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 20000  40000  60000  80000  100000\nSquared error per example\nNumber of weight updates\n(a)\n(b)\n(c)\nFigure 18.18\nRepeat of the experiments in Figure 18.16 using logistic regression and\nsquared error. The plot in (a) covers 5000 iterations rather than 1000, while (b) and (c) use\nthe same scale.\nThe derivative g′ of the logistic function satisﬁes g′(z) = g(z)(1 −g(z)), so we have\ng′(w · x) = g(w · x)(1 −g(w · x)) = hw(x)(1 −hw(x))\nso the weight update for minimizing the loss is\nwi ←wi + α (y −hw(x)) × hw(x)(1 −hw(x)) × xi .\n(18.8)\nRepeating the experiments of Figure 18.16 with logistic regression instead of the linear\nthreshold classiﬁer, we obtain the results shown in Figure 18.18. In (a), the linearly sep-\narable case, logistic regression is somewhat slower to converge, but behaves much more\npredictably. In (b) and (c), where the data are noisy and nonseparable, logistic regression\nconverges far more quickly and reliably. These advantages tend to carry over into real-world\napplications and logistic regression has become one of the most popular classiﬁcation tech-\nniques for problems in medicine, marketing and survey analysis, credit scoring, public health,\nand other applications.\n18.7\nARTIFICIAL NEURAL NETWORKS\nWe turn now to what seems to be a somewhat unrelated topic: the brain. In fact, as we\nwill see, the technical ideas we have discussed so far in this chapter turn out to be useful in\nbuilding mathematical models of the brain’s activity; conversely, thinking about the brain has\nhelped in extending the scope of the technical ideas.\nChapter 1 touched brieﬂy on the basic ﬁndings of neuroscience—in particular, the hy-\npothesis that mental activity consists primarily of electrochemical activity in networks of\nbrain cells called neurons. (Figure 1.2 on page 11 showed a schematic diagram of a typical\nneuron.) Inspired by this hypothesis, some of the earliest AI work aimed to create artiﬁcial\nneural networks. (Other names for the ﬁeld include connectionism, parallel distributed\nNEURAL NETWORK\nprocessing, and neural computation.) Figure 18.19 shows a simple mathematical model\nof the neuron devised by McCulloch and Pitts (1943). Roughly speaking, it “ﬁres” when a\nlinear combination of its inputs exceeds some (hard or soft) threshold—that is, it implements 728\nChapter\n18.\nLearning from Examples\nOutput\nΣ\nInput\nLinks\nActivation",
  "linear combination of its inputs exceeds some (hard or soft) threshold—that is, it implements 728\nChapter\n18.\nLearning from Examples\nOutput\nΣ\nInput\nLinks\nActivation\nFunction\nInput\nFunction\nOutput\nLinks\na0 = 1\naj = g(inj)\naj\ng\ninj\nwi,j\nw0,j\nBias Weight\nai\nFigure 18.19\nA simple mathematical model for a neuron. The unit’s output activation is\naj = g(\u0002n\ni = 0 wi,jai), where ai is the output activation of unit i and wi,j is the weight on the\nlink from unit i to this unit.\na linear classiﬁer of the kind described in the preceding section. A neural network is just a\ncollection of units connected together; the properties of the network are determined by its\ntopology and the properties of the “neurons.”\nSince 1943, much more detailed and realistic models have been developed, both for\nneurons and for larger systems in the brain, leading to the modern ﬁeld of computational\nneuroscience. On the other hand, researchers in AI and statistics became interested in the\nCOMPUTATIONAL\nNEUROSCIENCE\nmore abstract properties of neural networks, such as their ability to perform distributed com-\nputation, to tolerate noisy inputs, and to learn. Although we understand now that other kinds\nof systems—including Bayesian networks—have these properties, neural networks remain\none of the most popular and effective forms of learning system and are worthy of study in\ntheir own right.\n18.7.1\nNeural network structures\nNeural networks are composed of nodes or units (see Figure 18.19) connected by directed\nUNIT\nlinks. A link from unit i to unit j serves to propagate the activation ai from i to j.8 Each link\nLINK\nACTIVATION\nalso has a numeric weight wi,j associated with it, which determines the strength and sign of\nWEIGHT\nthe connection. Just as in linear regression models, each unit has a dummy input a0 = 1 with\nan associated weight w0,j. Each unit j ﬁrst computes a weighted sum of its inputs:\ninj =\nn\n\f\ni = 0\nwi,jai .\nThen it applies an activation function g to this sum to derive the output:\nACTIVATION\nFUNCTION\naj = g(inj) = g\n\u001f n\n\f\ni = 0\nwi,jai\n \n.\n(18.9)\n8 A note on notation: for this section, we are forced to suspend our usual conventions. Input attributes are still\nindexed by i , so that an “external” activation ai is given by input xi; but index j will refer to internal units\nrather than examples. Throughout this section, the mathematical derivations concern a single generic example x,\nomitting the usual summations over examples to obtain results for the whole data set. Section 18.7.",
  "rather than examples. Throughout this section, the mathematical derivations concern a single generic example x,\nomitting the usual summations over examples to obtain results for the whole data set. Section 18.7.\nArtiﬁcial Neural Networks\n729\nThe activation function g is typically either a hard threshold (Figure 18.17(a)), in which case\nthe unit is called a perceptron, or a logistic function (Figure 18.17(b)), in which case the term\nPERCEPTRON\nsigmoid perceptron is sometimes used. Both of these nonlinear activation function ensure\nSIGMOID\nPERCEPTRON\nthe important property that the entire network of units can represent a nonlinear function (see\nExercise 18.22). As mentioned in the discussion of logistic regression (page 725), the logistic\nactivation function has the added advantage of being differentiable.\nHaving decided on the mathematical model for individual “neurons,” the next task is\nto connect them together to form a network. There are two fundamentally distinct ways to\ndo this. A feed-forward network has connections only in one direction—that is, it forms a\nFEED-FORWARD\nNETWORK\ndirected acyclic graph. Every node receives input from “upstream” nodes and delivers output\nto “downstream” nodes; there are no loops. A feed-forward network represents a function of\nits current input; thus, it has no internal state other than the weights themselves. A recurrent\nnetwork, on the other hand, feeds its outputs back into its own inputs. This means that\nRECURRENT\nNETWORK\nthe activation levels of the network form a dynamical system that may reach a stable state or\nexhibit oscillations or even chaotic behavior. Moreover, the response of the network to a given\ninput depends on its initial state, which may depend on previous inputs. Hence, recurrent\nnetworks (unlike feed-forward networks) can support short-term memory. This makes them\nmore interesting as models of the brain, but also more difﬁcult to understand. This section\nwill concentrate on feed-forward networks; some pointers for further reading on recurrent\nnetworks are given at the end of the chapter.\nFeed-forward networks are usually arranged in layers, such that each unit receives input\nLAYERS\nonly from units in the immediately preceding layer. In the next two subsections, we will look\nat single-layer networks, in which every unit connects directly from the network’s inputs to\nits outputs, and multilayer networks, which have one or more layers of hidden units that are\nHIDDEN UNIT",
  "at single-layer networks, in which every unit connects directly from the network’s inputs to\nits outputs, and multilayer networks, which have one or more layers of hidden units that are\nHIDDEN UNIT\nnot connected to the outputs of the network. So far in this chapter, we have considered only\nlearning problems with a single output variable y, but neural networks are often used in cases\nwhere multiple outputs are appropriate. For example, if we want to train a network to add\ntwo input bits, each a 0 or a 1, we will need one output for the sum bit and one for the carry\nbit. Also, when the learning problem involves classiﬁcation into more than two classes—for\nexample, when learning to categorize images of handwritten digits—it is common to use one\noutput unit for each class.\n18.7.2\nSingle-layer feed-forward neural networks (perceptrons)\nA network with all the inputs connected directly to the outputs is called a single-layer neural\nnetwork, or a perceptron network. Figure 18.20 shows a simple two-input, two-output\nPERCEPTRON\nNETWORK\nperceptron network. With such a network, we might hope to learn the two-bit adder function,\nfor example. Here are all the training data we will need:\nx1\nx2\ny3 (carry)\ny4 (sum)\n0\n0\n0\n0\n0\n1\n0\n1\n1\n0\n0\n1\n1\n1\n1\n0 730\nChapter\n18.\nLearning from Examples\nThe ﬁrst thing to notice is that a perceptron network with m outputs is really m separate\nnetworks, because each weight affects only one of the outputs. Thus, there will be m sepa-\nrate training processes. Furthermore, depending on the type of activation function used, the\ntraining processes will be either the perceptron learning rule (Equation (18.7) on page 724)\nor gradient descent rule for the logistic regression (Equation (18.8) on page 727).\nIf you try either method on the two-bit-adder data, something interesting happens. Unit\n3 learns the carry function easily, but unit 4 completely fails to learn the sum function. No,\nunit 4 is not defective! The problem is with the sum function itself. We saw in Section 18.6\nthat linear classiﬁers (whether hard or soft) can represent linear decision boundaries in the in-\nput space. This works ﬁne for the carry function, which is a logical AND (see Figure 18.21(a)).\nThe sum function, however, is an XOR (exclusive OR) of the two inputs. As Figure 18.21(c)\nillustrates, this function is not linearly separable so the perceptron cannot learn it.\nThe linearly separable functions constitute just a small fraction of all Boolean func-",
  "illustrates, this function is not linearly separable so the perceptron cannot learn it.\nThe linearly separable functions constitute just a small fraction of all Boolean func-\ntions; Exercise 18.20 asks you to quantify this fraction. The inability of perceptrons to learn\neven such simple functions as XOR was a signiﬁcant setback to the nascent neural network\nw3,5\n3,6\nw\n4,5\nw\n4,6\nw\n5\n6\nw1,3\n1,4\nw\n2,3\nw\n2,4\nw\n1\n2\n3\n4\nw1,3\n1,4\nw\n2,3\nw\n2,4\nw\n1\n2\n3\n4\n(b)\n(a)\nFigure 18.20\n(a) A perceptron network with two inputs and two output units. (b) A neural\nnetwork with two inputs, one hidden layer of two units, and one output unit. Not shown are\nthe dummy inputs and their associated weights.\n(a) x1 and x2\n1\n0\n0\n1\nx1\nx2\n(b) x1 or x2\n0\n1\n1\n0\nx1\nx2\n(c) x1 xor x2\n?\n0\n1\n1\n0\nx1\nx2\nFigure 18.21\nLinear separability in threshold perceptrons. Black dots indicate a point in\nthe input space where the value of the function is 1, and white dots indicate a point where the\nvalue is 0. The perceptron returns 1 on the region on the non-shaded side of the line. In (c),\nno such line exists that correctly classiﬁes the inputs. Section 18.7.\nArtiﬁcial Neural Networks\n731\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100\nProportion correct on test set\nTraining set size\nPerceptron\nDecision tree\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100\nProportion correct on test set\nTraining set size\nPerceptron\nDecision tree\n(a)\n(b)\nFigure 18.22\nComparing the performance of perceptrons and decision trees. (a) Percep-\ntrons are better at learning the majority function of 11 inputs. (b) Decision trees are better at\nlearning the WillWait predicate in the restaurant example.\ncommunity in the 1960s. Perceptrons are far from useless, however. Section 18.6.4 noted\nthat logistic regression (i.e., training a sigmoid perceptron) is even today a very popular and\neffective tool. Moreover, a perceptron can represent some quite “complex” Boolean func-\ntions very compactly. For example, the majority function, which outputs a 1 only if more\nthan half of its n inputs are 1, can be represented by a perceptron with each wi = 1 and with\nw0 = −n/2. A decision tree would need exponentially many nodes to represent this function.\nFigure 18.22 shows the learning curve for a perceptron on two different problems. On\nthe left, we show the curve for learning the majority function with 11 Boolean inputs (i.e.,\nthe function outputs a 1 if 6 or more inputs are 1). As we would expect, the perceptron learns",
  "the left, we show the curve for learning the majority function with 11 Boolean inputs (i.e.,\nthe function outputs a 1 if 6 or more inputs are 1). As we would expect, the perceptron learns\nthe function quite quickly, because the majority function is linearly separable. On the other\nhand, the decision-tree learner makes no progress, because the majority function is very hard\n(although not impossible) to represent as a decision tree. On the right, we have the restaurant\nexample. The solution problem is easily represented as a decision tree, but is not linearly\nseparable. The best plane through the data correctly classiﬁes only 65%.\n18.7.3\nMultilayer feed-forward neural networks\n(McCulloch and Pitts, 1943) were well aware that a single threshold unit would not solve all\ntheir problems. In fact, their paper proves that such a unit can represent the basic Boolean\nfunctions AND, OR, and NOT and then goes on to argue that any desired functionality can be\nobtained by connecting large numbers of units into (possibly recurrent) networks of arbitrary\ndepth. The problem was that nobody knew how to train such networks.\nThis turns out to be an easy problem if we think of a network the right way: as a\nfunction hw(x) parameterized by the weights w. Consider the simple network shown in Fig-\nure 18.20(b), which has two input units, two hidden units, and two output unit. (In addition,\neach unit has a dummy input ﬁxed at 1.) Given an input vector x = (x1, x2), the activations 732\nChapter\n18.\nLearning from Examples\n-4\n-2\n0\n2\n4\nx1\n-4 -2 0 2 4\nx2\n0\n0.2\n0.4\n0.6\n0.8\n1\nhW(x1, x2)\n-4\n-2\n0\n2\n4\nx1\n-4 -2 0 2 4\nx2\n0\n0.2\n0.4\n0.6\n0.8\n1\nhW(x1, x2)\n(a)\n(b)\nFigure 18.23\n(a) The result of combining two opposite-facing soft threshold functions to\nproduce a ridge. (b) The result of combining two ridges to produce a bump.\nof the input units are set to (a1, a2) = (x1, x2). The output at unit 5 is given by\na5 = g(w0,5,+w3,5 a3 + w4,5 a4)\n= g(w0,5,+w3,5 g(w0,3 + w1,3 a1 + w2,3 a2) + w4,5 g(w04 + w1,4 a1 + w2,4 a2))\n= g(w0,5,+w3,5 g(w0,3 + w1,3 x1 + w2,3 x2) + w4,5 g(w04 + w1,4 x1 + w2,4 x2)).\nThus, we have the output expressed as a function of the inputs and the weights. A similar\nexpression holds for unit 6. As long as we can calculate the derivatives of such expressions\nwith respect to the weights, we can use the gradient-descent loss-minimization method to\ntrain the network. Section 18.7.4 shows exactly how to do this. And because the function",
  "with respect to the weights, we can use the gradient-descent loss-minimization method to\ntrain the network. Section 18.7.4 shows exactly how to do this. And because the function\nrepresented by a network can be highly nonlinear—composed, as it is, of nested nonlinear soft\nthreshold functions—we can see neural networks as a tool for doing nonlinear regression.\nNONLINEAR\nREGRESSION\nBefore delving into learning rules, let us look at the ways in which networks generate\ncomplicated functions. First, remember that each unit in a sigmoid network represents a soft\nthreshold in its input space, as shown in Figure 18.17(c) (page 726). With one hidden layer\nand one output layer, as in Figure 18.20(b), each output unit computes a soft-thresholded\nlinear combination of several such functions. For example, by adding two opposite-facing\nsoft threshold functions and thresholding the result, we can obtain a “ridge” function as shown\nin Figure 18.23(a). Combining two such ridges at right angles to each other (i.e., combining\nthe outputs from four hidden units), we obtain a “bump” as shown in Figure 18.23(b).\nWith more hidden units, we can produce more bumps of different sizes in more places.\nIn fact, with a single, sufﬁciently large hidden layer, it is possible to represent any continuous\nfunction of the inputs with arbitrary accuracy; with two layers, even discontinuous functions\ncan be represented.9 Unfortunately, for any particular network structure, it is harder to char-\nacterize exactly which functions can be represented and which ones cannot.\n9 The proof is complex, but the main point is that the required number of hidden units grows exponentially with\nthe number of inputs. For example, 2n/n hidden units are needed to encode all Boolean functions of n inputs. Section 18.7.\nArtiﬁcial Neural Networks\n733\n18.7.4\nLearning in multilayer networks\nFirst, let us dispense with one minor complication arising in multilayer networks: interactions\namong the learning problems when the network has multiple outputs. In such cases, we\nshould think of the network as implementing a vector function hw rather than a scalar function\nhw; for example, the network in Figure 18.20(b) returns a vector [a5, a6]. Similarly, the\ntarget output will be a vector y. Whereas a perceptron network decomposes into m separate\nlearning problems for an m-output problem, this decomposition fails in a multilayer network.",
  "target output will be a vector y. Whereas a perceptron network decomposes into m separate\nlearning problems for an m-output problem, this decomposition fails in a multilayer network.\nFor example, both a5 and a6 in Figure 18.20(b) depend on all of the input-layer weights, so\nupdates to those weights will depend on errors in both a5 and a6. Fortunately, this dependency\nis very simple in the case of any loss function that is additive across the components of the\nerror vector y −hw(x). For the L2 loss, we have, for any weight w,\n∂\n∂wLoss(w) = ∂\n∂w|y −hw(x)|2 = ∂\n∂w\n\f\nk\n(yk −ak)2 =\n\f\nk\n∂\n∂w(yk −ak)2\n(18.10)\nwhere the index k ranges over nodes in the output layer. Each term in the ﬁnal summation\nis just the gradient of the loss for the kth output, computed as if the other outputs did not\nexist. Hence, we can decompose an m-output learning problem into m learning problems,\nprovided we remember to add up the gradient contributions from each of them when updating\nthe weights.\nThe major complication comes from the addition of hidden layers to the network.\nWhereas the error y −hw at the output layer is clear, the error at the hidden layers seems\nmysterious because the training data do not say what value the hidden nodes should have.\nFortunately, it turns out that we can back-propagate the error from the output layer to the\nBACK-PROPAGATION\nhidden layers. The back-propagation process emerges directly from a derivation of the overall\nerror gradient. First, we will describe the process with an intuitive justiﬁcation; then, we will\nshow the derivation.\nAt the output layer, the weight-update rule is identical to Equation (18.8). We have\nmultiple output units, so let Errk be the kth component of the error vector y −hw. We will\nalso ﬁnd it convenient to deﬁne a modiﬁed error Δk = Errk × g′(ink), so that the weight-\nupdate rule becomes\nwj,k ←wj,k + α × aj × Δk .\n(18.11)\nTo update the connections between the input units and the hidden units, we need to deﬁne a\nquantity analogous to the error term for output nodes. Here is where we do the error back-\npropagation. The idea is that hidden node j is “responsible” for some fraction of the error Δk\nin each of the output nodes to which it connects. Thus, the Δk values are divided according\nto the strength of the connection between the hidden node and the output node and are prop-\nagated back to provide the Δj values for the hidden layer. The propagation rule for the Δ\nvalues is the following:\nΔj = g′(inj)\n\f\nk\nwj,kΔk .\n(18.12) 734",
  "to the strength of the connection between the hidden node and the output node and are prop-\nagated back to provide the Δj values for the hidden layer. The propagation rule for the Δ\nvalues is the following:\nΔj = g′(inj)\n\f\nk\nwj,kΔk .\n(18.12) 734\nChapter\n18.\nLearning from Examples\nfunction BACK-PROP-LEARNING(examples,network) returns a neural network\ninputs: examples, a set of examples, each with input vector x and output vector y\nnetwork, a multilayer network with L layers, weights wi,j, activation function g\nlocal variables: Δ, a vector of errors, indexed by network node\nrepeat\nfor each weight wi,j in network do\nwi,j ←a small random number\nfor each example (x, y) in examples do\n/* Propagate the inputs forward to compute the outputs */\nfor each node i in the input layer do\nai ←xi\nfor ℓ= 2 to L do\nfor each node j in layer ℓdo\ninj ←\u0002\ni wi,j ai\naj ←g(inj)\n/* Propagate deltas backward from output layer to input layer */\nfor each node j in the output layer do\nΔ[j] ←g′(inj) × (yj −aj)\nfor ℓ= L −1 to 1 do\nfor each node i in layer ℓdo\nΔ[i] ←g′(ini) \u0002\nj wi,j Δ[j]\n/* Update every weight in network using deltas */\nfor each weight wi,j in network do\nwi,j ←wi,j + α × ai × Δ[j]\nuntil some stopping criterion is satisﬁed\nreturn network\nFigure 18.24\nThe back-propagation algorithm for learning in multilayer networks.\nNow the weight-update rule for the weights between the inputs and the hidden layer is essen-\ntially identical to the update rule for the output layer:\nwi,j ←wi,j + α × ai × Δj .\nThe back-propagation process can be summarized as follows:\n• Compute the Δ values for the output units, using the observed error.\n• Starting with output layer, repeat the following for each layer in the network, until the\nearliest hidden layer is reached:\n– Propagate the Δ values back to the previous layer.\n– Update the weights between the two layers.\nThe detailed algorithm is shown in Figure 18.24.\nFor the mathematically inclined, we will now derive the back-propagation equations\nfrom ﬁrst principles. The derivation is quite similar to the gradient calculation for logistic Section 18.7.\nArtiﬁcial Neural Networks\n735\nregression (leading up to Equation (18.8) on page 727), except that we have to use the chain\nrule more than once.\nFollowing Equation (18.10), we compute just the gradient for Lossk = (yk −ak)2 at\nthe kth output. The gradient of this loss with respect to weights connecting the hidden layer",
  "rule more than once.\nFollowing Equation (18.10), we compute just the gradient for Lossk = (yk −ak)2 at\nthe kth output. The gradient of this loss with respect to weights connecting the hidden layer\nto the output layer will be zero except for weights wj,k that connect to the kth output unit.\nFor those weights, we have\n∂Lossk\n∂wj,k\n= −2(yk −ak) ∂ak\n∂wj,k\n= −2(yk −ak)∂g(ink)\n∂wj,k\n= −2(yk −ak)g′(ink) ∂ink\n∂wj,k\n= −2(yk −ak)g′(ink)\n∂\n∂wj,k\n⎛\n⎝\f\nj\nwj,kaj\n⎞\n⎠\n= −2(yk −ak)g′(ink)aj = −ajΔk ,\nwith Δk deﬁned as before. To obtain the gradient with respect to the wi,j weights connecting\nthe input layer to the hidden ¡layer, we have to expand out the activations aj and reapply the\nchain rule. We will show the derivation in gory detail because it is interesting to see how the\nderivative operator propagates back through the network:\n∂Lossk\n∂wi,j\n= −2(yk −ak) ∂ak\n∂wi,j\n= −2(yk −ak)∂g(ink)\n∂wi,j\n= −2(yk −ak)g′(ink) ∂ink\n∂wi,j\n= −2Δk\n∂\n∂wi,j\n⎛\n⎝\f\nj\nwj,kaj\n⎞\n⎠\n= −2Δkwj,k\n∂aj\n∂wi,j\n= −2Δkwj,k\n∂g(inj)\n∂wi,j\n= −2Δkwj,kg′(inj) ∂inj\n∂wi,j\n= −2Δkwj,kg′(inj)\n∂\n∂wi,j\n\u001f\f\ni\nwi,jai\n \n= −2Δkwj,kg′(inj)ai = −aiΔj ,\nwhere Δj is deﬁned as before. Thus, we obtain the update rules obtained earlier from intuitive\nconsiderations. It is also clear that the process can be continued for networks with more than\none hidden layer, which justiﬁes the general algorithm given in Figure 18.24.\nHaving made it through (or skipped over) all the mathematics, let’s see how a single-\nhidden-layer network performs on the restaurant problem. First, we need to determine the\nstructure of the network. We have 10 attributes describing each example, so we will need\n10 input units. Should we have one hidden layer or two? How many nodes in each layer?\nShould they be fully connected? There is no good theory that will tell us the answer. (See the\nnext section.) As always, we can use cross-validation: try several different structures and see\nwhich one works best. It turns out that a network with one hidden layer containing four nodes\nis about right for this problem. In Figure 18.25, we show two curves. The ﬁrst is a training\ncurve showing the mean squared error on a given training set of 100 restaurant examples 736\nChapter\n18.\nLearning from Examples\n0\n2\n4\n6\n8\n10\n12\n14\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTotal error on training set\nNumber of epochs\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100\nProportion correct on test set\nTraining set size\nDecision tree\nMultilayer network\n(a)\n(b)\nFigure 18.25",
  "10\n12\n14\n0\n50\n100\n150\n200\n250\n300\n350\n400\nTotal error on training set\nNumber of epochs\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100\nProportion correct on test set\nTraining set size\nDecision tree\nMultilayer network\n(a)\n(b)\nFigure 18.25\n(a) Training curve showing the gradual reduction in error as weights are\nmodiﬁed over several epochs, for a given set of examples in the restaurant domain. (b)\nComparative learning curves showing that decision-tree learning does slightly better on the\nrestaurant problem than back-propagation in a multilayer network.\nduring the weight-updating process. This demonstrates that the network does indeed converge\nto a perfect ﬁt to the training data. The second curve is the standard learning curve for the\nrestaurant data. The neural network does learn well, although not quite as fast as decision-\ntree learning; this is perhaps not surprising, because the data were generated from a simple\ndecision tree in the ﬁrst place.\nNeural networks are capable of far more complex learning tasks of course, although it\nmust be said that a certain amount of twiddling is needed to get the network structure right\nand to achieve convergence to something close to the global optimum in weight space. There\nare literally tens of thousands of published applications of neural networks. Section 18.11.1\nlooks at one such application in more depth.\n18.7.5\nLearning neural network structures\nSo far, we have considered the problem of learning weights, given a ﬁxed network structure;\njust as with Bayesian networks, we also need to understand how to ﬁnd the best network\nstructure. If we choose a network that is too big, it will be able to memorize all the examples\nby forming a large lookup table, but will not necessarily generalize well to inputs that have\nnot been seen before.10 In other words, like all statistical models, neural networks are subject\nto overﬁtting when there are too many parameters in the model. We saw this in Figure 18.1\n(page 696), where the high-parameter models in (b) and (c) ﬁt all the data, but might not\ngeneralize as well as the low-parameter models in (a) and (d).\nIf we stick to fully connected networks, the only choices to be made concern the number\n10 It has been observed that very large networks do generalize well as long as the weights are kept small. This\nrestriction keeps the activation values in the linear region of the sigmoid function g(x) where x is close to zero.",
  "10 It has been observed that very large networks do generalize well as long as the weights are kept small. This\nrestriction keeps the activation values in the linear region of the sigmoid function g(x) where x is close to zero.\nThis, in turn, means that the network behaves like a linear function (Exercise 18.22) with far fewer parameters. Section 18.8.\nNonparametric Models\n737\nof hidden layers and their sizes. The usual approach is to try several and keep the best. The\ncross-validation techniques of Chapter 18 are needed if we are to avoid peeking at the test\nset. That is, we choose the network architecture that gives the highest prediction accuracy on\nthe validation sets.\nIf we want to consider networks that are not fully connected, then we need to ﬁnd\nsome effective search method through the very large space of possible connection topologies.\nThe optimal brain damage algorithm begins with a fully connected network and removes\nOPTIMAL BRAIN\nDAMAGE\nconnections from it. After the network is trained for the ﬁrst time, an information-theoretic\napproach identiﬁes an optimal selection of connections that can be dropped. The network\nis then retrained, and if its performance has not decreased then the process is repeated. In\naddition to removing connections, it is also possible to remove units that are not contributing\nmuch to the result.\nSeveral algorithms have been proposed for growing a larger network from a smaller one.\nOne, the tiling algorithm, resembles decision-list learning. The idea is to start with a single\nTILING\nunit that does its best to produce the correct output on as many of the training examples as\npossible. Subsequent units are added to take care of the examples that the ﬁrst unit got wrong.\nThe algorithm adds only as many units as are needed to cover all the examples.\n18.8\nNONPARAMETRIC MODELS\nLinear regression and neural networks use the training data to estimate a ﬁxed set of param-\neters w. That deﬁnes our hypothesis hw(x), and at that point we can throw away the training\ndata, because they are all summarized by w. A learning model that summarizes data with a\nset of parameters of ﬁxed size (independent of the number of training examples) is called a\nparametric model.\nPARAMETRIC MODEL\nNo matter how much data you throw at a parametric model, it won’t change its mind\nabout how many parameters it needs. When data sets are small, it makes sense to have a strong",
  "parametric model.\nPARAMETRIC MODEL\nNo matter how much data you throw at a parametric model, it won’t change its mind\nabout how many parameters it needs. When data sets are small, it makes sense to have a strong\nrestriction on the allowable hypotheses, to avoid overﬁtting. But when there are thousands or\nmillions or billions of examples to learn from, it seems like a better idea to let the data speak\nfor themselves rather than forcing them to speak through a tiny vector of parameters. If the\ndata say that the correct answer is a very wiggly function, we shouldn’t restrict ourselves to\nlinear or slightly wiggly functions.\nA nonparametric model is one that cannot be characterized by a bounded set of param-\nNONPARAMETRIC\nMODEL\neters. For example, suppose that each hypothesis we generate simply retains within itself all\nof the training examples and uses all of them to predict the next example. Such a hypothesis\nfamily would be nonparametric because the effective number of parameters is unbounded—\nit grows with the number of examples. This approach is called instance-based learning or\nINSTANCE-BASED\nLEARNING\nmemory-based learning. The simplest instance-based learning method is table lookup: take\nTABLE LOOKUP\nall the training examples, put them in a lookup table, and then when asked for h(x), see if x is\nin the table; if it is, return the corresponding y. The problem with this method is that it does\nnot generalize well: when x is not in the table all it can do is return some default value. 738\nChapter\n18.\nLearning from Examples\n 2.5\n 3\n 3.5\n 4\n 4.5\n 5\n 5.5\n 6\n 6.5\n 7\n 7.5\n 4.5\n 5\n 5.5\n 6\n 6.5\n 7\nx1\nx2\n 2.5\n 3\n 3.5\n 4\n 4.5\n 5\n 5.5\n 6\n 6.5\n 7\n 7.5\n 4.5\n 5\n 5.5\n 6\n 6.5\n 7\nx1\nx2\n(k = 1)\n(k = 5)\nFigure 18.26\n(a) A k-nearest-neighbormodel showing the extent of the explosion class for\nthe data in Figure 18.15, with k = 1. Overﬁtting is apparent. (b) With k = 5, the overﬁtting\nproblem goes away for this data set.\n18.8.1\nNearest neighbor models\nWe can improve on table lookup with a slight variation: given a query xq, ﬁnd the k examples\nthat are nearest to xq. This is called k-nearest neighbors lookup. We’ll use the notation\nNEAREST\nNEIGHBORS\nNN (k, xq) to denote the set of k nearest neighbors.\nTo do classiﬁcation, ﬁrst ﬁnd NN (k, xq), then take the plurality vote of the neighbors\n(which is the majority vote in the case of binary classiﬁcation). To avoid ties, k is always\nchosen to be an odd number. To do regression, we can take the mean or median of the k",
  "(which is the majority vote in the case of binary classiﬁcation). To avoid ties, k is always\nchosen to be an odd number. To do regression, we can take the mean or median of the k\nneighbors, or we can solve a linear regression problem on the neighbors.\nIn Figure 18.26, we show the decision boundary of k-nearest-neighbors classiﬁcation\nfor k = 1 and 5 on the earthquake data set from Figure 18.15. Nonparametric methods are\nstill subject to underﬁtting and overﬁtting, just like parametric methods. In this case 1-nearest\nneighbors is overﬁtting; it reacts too much to the black outlier in the upper right and the white\noutlier at (5.4, 3.7). The 5-nearest-neighbors decision boundary is good; higher k would\nunderﬁt. As usual, cross-validation can be used to select the best value of k.\nThe very word “nearest” implies a distance metric. How do we measure the distance\nfrom a query point xq to an example point xj? Typically, distances are measured with a\nMinkowski distance or Lp norm, deﬁned as\nMINKOWSKI\nDISTANCE\nLp(xj, xq) = (\n\f\ni\n|xj,i −xq,i|p)1/p .\nWith p = 2 this is Euclidean distance and with p = 1 it is Manhattan distance. With Boolean\nattribute values, the number of attributes on which the two points differ is called the Ham-\nming distance. Often p = 2 is used if the dimensions are measuring similar properties, such\nHAMMING DISTANCE\nas the width, height and depth of parts on a conveyor belt, and Manhattan distance is used if\nthey are dissimilar, such as age, weight, and gender of a patient. Note that if we use the raw\nnumbers from each dimension then the total distance will be affected by a change in scale\nin any dimension. That is, if we change dimension i from measurements in centimeters to Section 18.8.\nNonparametric Models\n739\nmiles while keeping the other dimensions the same, we’ll get different nearest neighbors. To\navoid this, it is common to apply normalization to the measurements in each dimension. One\nNORMALIZATION\nsimple approach is to compute the mean μi and standard deviation σi of the values in each\ndimension, and rescale them so that xj,i becomes (xj,i −μi)/σi. A more complex metric\nknown as the Mahalanobis distance takes into account the covariance between dimensions.\nMAHALANOBIS\nDISTANCE\nIn low-dimensional spaces with plenty of data, nearest neighbors works very well: we\nare likely to have enough nearby data points to get a good answer. But as the number of",
  "MAHALANOBIS\nDISTANCE\nIn low-dimensional spaces with plenty of data, nearest neighbors works very well: we\nare likely to have enough nearby data points to get a good answer. But as the number of\ndimensions rises we encounter a problem: the nearest neighbors in high-dimensional spaces\nare usually not very near! Consider k-nearest-neighbors on a data set of N points uniformly\ndistributed throughout the interior of an n-dimensional unit hypercube. We’ll deﬁne the k-\nneighborhood of a point as the smallest hypercube that contains the k-nearest neighbors. Let\nℓbe the average side length of a neighborhood. Then the volume of the neighborhood (which\ncontains k points) is ℓn and the volume of the full cube (which contains N points) is 1. So,\non average, ℓn = k/N. Taking nth roots of both sides we get ℓ= (k/N)1/n.\nTo be concrete, let k = 10 and N = 1, 000, 000.\nIn two dimensions (n = 2; a unit\nsquare), the average neighborhood has ℓ= 0.003, a small fraction of the unit square, and\nin 3 dimensions ℓis just 2% of the edge length of the unit cube. But by the time we get to 17\ndimensions, ℓis half the edge length of the unit hypercube, and in 200 dimensions it is 94%.\nThis problem has been called the curse of dimensionality.\nCURSE OF\nDIMENSIONALITY\nAnother way to look at it: consider the points that fall within a thin shell making up the\nouter 1% of the unit hypercube. These are outliers; in general it will be hard to ﬁnd a good\nvalue for them because we will be extrapolating rather than interpolating. In one dimension,\nthese outliers are only 2% of the points on the unit line (those points where x < .01 or\nx > .99), but in 200 dimensions, over 98% of the points fall within this thin shell—almost\nall the points are outliers. You can see an example of a poor nearest-neighbors ﬁt on outliers\nif you look ahead to Figure 18.28(b).\nThe NN (k, xq) function is conceptually trivial: given a set of N examples and a query\nxq, iterate through the examples, measure the distance to xq from each one, and keep the best\nk. If we are satisﬁed with an implementation that takes O(N) execution time, then that is the\nend of the story. But instance-based methods are designed for large data sets, so we would\nlike an algorithm with sublinear run time. Elementary analysis of algorithms tells us that\nexact table lookup is O(N) with a sequential table, O(log N) with a binary tree, and O(1)\nwith a hash table. We will now see that binary trees and hash tables are also applicable for",
  "exact table lookup is O(N) with a sequential table, O(log N) with a binary tree, and O(1)\nwith a hash table. We will now see that binary trees and hash tables are also applicable for\nﬁnding nearest neighbors.\n18.8.2\nFinding nearest neighbors with k-d trees\nA balanced binary tree over data with an arbitrary number of dimensions is called a k-d tree,\nK-D TREE\nfor k-dimensional tree. (In our notation, the number of dimensions is n, so they would be\nn-d trees. The construction of a k-d tree is similar to the construction of a one-dimensional\nbalanced binary tree. We start with a set of examples and at the root node we split them along\nthe ith dimension by testing whether xi ≤m. We chose the value m to be the median of the\nexamples along the ith dimension; thus half the examples will be in the left branch of the tree 740\nChapter\n18.\nLearning from Examples\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 25\n 50\n 75  100  125  150  175  200\nEdge length of neighborhood\nNumber of dimensions\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 25\n 50\n 75  100  125  150  175  200\nProportion of points in exterior shell\nNumber of dimensions\n(a)\n(b)\nFigure 18.27\nThe curse of dimensionality: (a) The length of the average neighborhood for\n10-nearest-neighbors in a unit hypercube with 1,000,000 points, as a function of the number\nof dimensions. (b) The proportion of points that fall within a thin shell consisting of the\nouter 1% of the hypercube, as a function of the number of dimensions. Sampled from 10,000\nrandomly distributed points.\nand half in the right. We then recursively make a tree for the left and right sets of examples,\nstopping when there are fewer than two examples left. To choose a dimension to split on at\neach node of the tree, one can simply select dimension i mod n at level i of the tree. (Note\nthat we may need to split on any given dimension several times as we proceed down the tree.)\nAnother strategy is to split on the dimension that has the widest spread of values.\nExact lookup from a k-d tree is just like lookup from a binary tree (with the slight\ncomplication that you need to pay attention to which dimension you are testing at each node).\nBut nearest neighbor lookup is more complicated. As we go down the branches, splitting\nthe examples in half, in some cases we can discard the other half of the examples. But not\nalways. Sometimes the point we are querying for falls very close to the dividing boundary.",
  "the examples in half, in some cases we can discard the other half of the examples. But not\nalways. Sometimes the point we are querying for falls very close to the dividing boundary.\nThe query point itself might be on the left hand side of the boundary, but one or more of\nthe k nearest neighbors might actually be on the right-hand side. We have to test for this\npossibility by computing the distance of the query point to the dividing boundary, and then\nsearching both sides if we can’t ﬁnd k examples on the left that are closer than this distance.\nBecause of this problem, k-d trees are appropriate only when there are many more examples\nthan dimensions, preferably at least 2n examples. Thus, k-d trees work well with up to 10\ndimensions with thousands of examples or up to 20 dimensions with millions of examples. If\nwe don’t have enough examples, lookup is no faster than a linear scan of the entire data set.\n18.8.3\nLocality-sensitive hashing\nHash tables have the potential to provide even faster lookup than binary trees. But how can\nwe ﬁnd nearest neighbors using a hash table, when hash codes rely on an exact match? Hash\ncodes randomly distribute values among the bins, but we want to have near points grouped\ntogether in the same bin; we want a locality-sensitive hash (LSH).\nLOCALITY-SENSITIVE\nHASH Section 18.8.\nNonparametric Models\n741\nWe can’t use hashes to solve NN (k, xq) exactly, but with a clever use of randomized\nalgorithms, we can ﬁnd an approximate solution. First we deﬁne the approximate near-\nneighbors problem: given a data set of example points and a query point xq, ﬁnd, with high\nAPPROXIMATE\nNEAR-NEIGHBORS\nprobability, an example point (or points) that is near xq. To be more precise, we require that\nif there is a point xj that is within a radius r of xq, then with high probability the algorithm\nwill ﬁnd a point xj′ that is within distance c r of q. If there is no point within radius r then the\nalgorithm is allowed to report failure. The values of c and “high probability” are parameters\nof the algorithm.\nTo solve approximate near neighbors, we will need a hash function g(x) that has the\nproperty that, for any two points xj and xj′, the probability that they have the same hash code\nis small if their distance is more than c r, and is high if their distance is less than r. For\nsimplicity we will treat each point as a bit string. (Any features that are not Boolean can be\nencoded into a set of Boolean features.)",
  "is small if their distance is more than c r, and is high if their distance is less than r. For\nsimplicity we will treat each point as a bit string. (Any features that are not Boolean can be\nencoded into a set of Boolean features.)\nThe intuition we rely on is that if two points are close together in an n-dimensional\nspace, then they will necessarily be close when projected down onto a one-dimensional space\n(a line). In fact, we can discretize the line into bins—hash buckets—so that, with high prob-\nability, near points project down to exactly the same bin. Points that are far away from each\nother will tend to project down into different bins for most projections, but there will always\nbe a few projections that coincidentally project far-apart points into the same bin. Thus, the\nbin for point xq contains many (but not all) points that are near to xq, as well as some points\nthat are far away.\nThe trick of LSH is to create multiple random projections and combine them. A random\nprojection is just a random subset of the bit-string representation. We choose ℓdifferent\nrandom projections and create ℓhash tables, g1(x), . . . , gℓ(x). We then enter all the examples\ninto each hash table. Then when given a query point xq, we fetch the set of points in bin gk(q)\nfor each k, and union these sets together into a set of candidate points, C. Then we compute\nthe actual distance to xq for each of the points in C and return the k closest points. With high\nprobability, each of the points that are near to xq will show up in at least one of the bins, and\nalthough some far-away points will show up as well, we can ignore those. With large real-\nworld problems, such as ﬁnding the near neighbors in a data set of 13 million Web images\nusing 512 dimensions (Torralba et al., 2008), locality-sensitive hashing needs to examine only\na few thousand images out of 13 million to ﬁnd nearest neighbors; a thousand-fold speedup\nover exhaustive or k-d tree approaches.\n18.8.4\nNonparametric regression\nNow we’ll look at nonparametric approaches to regression rather than classiﬁcation. Fig-\nure 18.28 shows an example of some different models. In (a), we have perhaps the simplest\nmethod of all, known informally as “connect-the-dots,” and superciliously as “piecewise-\nlinear nonparametric regression.” This model creates a function h(x) that, when given a\nquery xq, solves the ordinary linear regression problem with just two points: the training",
  "linear nonparametric regression.” This model creates a function h(x) that, when given a\nquery xq, solves the ordinary linear regression problem with just two points: the training\nexamples immediately to the left and right of xq. When noise is low, this trivial method is\nactually not too bad, which is why it is a standard feature of charting software in spreadsheets. 742\nChapter\n18.\nLearning from Examples\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 0\n 2\n 4\n 6\n 8\n 10\n 12\n 14\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 0\n 2\n 4\n 6\n 8\n 10\n 12\n 14\n(a)\n(b)\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 0\n 2\n 4\n 6\n 8\n 10\n 12\n 14\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 0\n 2\n 4\n 6\n 8\n 10\n 12\n 14\n(c)\n(d)\nFigure 18.28\nNonparametric regression models: (a) connect the dots, (b) 3-nearest neigh-\nbors average, (c) 3-nearest-neighbors linear regression, (d) locally weighted regression with\na quadratic kernel of width k = 10.\nBut when the data are noisy, the resulting function is spiky, and does not generalize well.\nk-nearest-neighbors regression (Figure 18.28(b)) improves on connect-the-dots. In-\nNEAREST-\nNEIGHBORS\nREGRESSION\nstead of using just the two examples to the left and right of a query point xq, we use the\nk nearest neighbors (here 3). A larger value of k tends to smooth out the magnitude of\nthe spikes, although the resulting function has discontinuities. In (b), we have the k-nearest-\nneighbors average: h(x) is the mean value of the k points, \u0002 yj/k. Notice that at the outlying\npoints, near x = 0 and x = 14, the estimates are poor because all the evidence comes from one\nside (the interior), and ignores the trend. In (c), we have k-nearest-neighbor linear regression,\nwhich ﬁnds the best line through the k examples. This does a better job of capturing trends at\nthe outliers, but is still discontinuous. In both (b) and (c), we’re left with the question of how\nto choose a good value for k. The answer, as usual, is cross-validation.\nLocally weighted regression (Figure 18.28(d)) gives us the advantages of nearest neigh-\nLOCALLY WEIGHTED\nREGRESSION\nbors, without the discontinuities. To avoid discontinuities in h(x), we need to avoid disconti- Section 18.8.\nNonparametric Models\n743\n 0\n 0.5\n 1\n-10\n-5\n 0\n 5\n 10\nFigure 18.29\nA quadratic kernel, K(d) = max(0, 1 −(2|x|/k)2), with kernel width\nk = 10, centered on the query point x = 0.\nnuities in the set of examples we use to estimate h(x). The idea of locally weighted regression\nis that at each query point xq, the examples that are close to xq are weighted heavily, and the",
  "k = 10, centered on the query point x = 0.\nnuities in the set of examples we use to estimate h(x). The idea of locally weighted regression\nis that at each query point xq, the examples that are close to xq are weighted heavily, and the\nexamples that are farther away are weighted less heavily or not at all. The decrease in weight\nover distance is always gradual, not sudden.\nWe decide how much to weight each example with a function known as a kernel. A\nKERNEL\nkernel function looks like a bump; in Figure 18.29 we see the speciﬁc kernel used to generate\nFigure 18.28(d). We can see that the weight provided by this kernel is highest in the center\nand reaches zero at a distance of ±5. Can we choose just any function for a kernel? No. First,\nnote that we invoke a kernel function K with K(Distance(xj, xq)), where xq is a query point\nthat is a given distance from xj, and we want to know how much to weight that distance.\nSo K should be symmetric around 0 and have a maximum at 0. The area under the kernel\nmust remain bounded as we go to ±∞. Other shapes, such as Gaussians, have been used for\nkernels, but the latest research suggests that the choice of shape doesn’t matter much. We\ndo have to be careful about the width of the kernel. Again, this is a parameter of the model\nthat is best chosen by cross-validation. Just as in choosing the k for nearest neighbors, if the\nkernels are too wide we’ll get underﬁtting and if they are too narrow we’ll get overﬁtting. In\nFigure 18.29(d), the value of k = 10 gives a smooth curve that looks about right—but maybe\nit does not pay enough attention to the outlier at x = 6; a narrower kernel width would be\nmore responsive to individual points.\nDoing locally weighted regression with kernels is now straightforward. For a given\nquery point xq we solve the following weighted regression problem using gradient descent:\nw∗= argmin\nw\n\f\nj\nK(Distance(xq, xj)) (yj −w · xj)2 ,\nwhere Distance is any of the distance metrics discussed for nearest neighbors. Then the\nanswer is h(xq) = w∗· xq.\nNote that we need to solve a new regression problem for every query point—that’s what\nit means to be local. (In ordinary linear regression, we solved the regression problem once,\nglobally, and then used the same hw for any query point.) Mitigating against this extra work 744\nChapter\n18.\nLearning from Examples\nis the fact that each regression problem will be easier to solve, because it involves only the",
  "globally, and then used the same hw for any query point.) Mitigating against this extra work 744\nChapter\n18.\nLearning from Examples\nis the fact that each regression problem will be easier to solve, because it involves only the\nexamples with nonzero weight—the examples whose kernels overlap the query point. When\nkernel widths are small, this may be just a few points.\nMost nonparametric models have the advantage that it is easy to do leave-one-out cross-\nvalidation without having to recompute everything. With a k-nearest-neighbors model, for\ninstance, when given a test example (x, y) we retrieve the k nearest neighbors once, compute\nthe per-example loss L(y, h(x)) from them, and record that as the leave-one-out result for\nevery example that is not one of the neighbors. Then we retrieve the k + 1 nearest neighbors\nand record distinct results for leaving out each of the k neighbors. With N examples the\nwhole process is O(k), not O(kN).\n18.9\nSUPPORT VECTOR MACHINES\nThe support vector machine or SVM framework is currently the most popular approach for\nSUPPORT VECTOR\nMACHINE\n“off-the-shelf” supervised learning: if you don’t have any specialized prior knowledge about\na domain, then the SVM is an excellent method to try ﬁrst. There are three properties that\nmake SVMs attractive:\n1. SVMs construct a maximum margin separator—a decision boundary with the largest\npossible distance to example points. This helps them generalize well.\n2. SVMs create a linear separating hyperplane, but they have the ability to embed the\ndata into a higher-dimensional space, using the so-called kernel trick. Often, data that\nare not linearly separable in the original input space are easily separable in the higher-\ndimensional space. The high-dimensional linear separator is actually nonlinear in the\noriginal space. This means the hypothesis space is greatly expanded over methods that\nuse strictly linear representations.\n3. SVMs are a nonparametric method—they retain training examples and potentially need\nto store them all. On the other hand, in practice they often end up retaining only a\nsmall fraction of the number of examples—sometimes as few as a small constant times\nthe number of dimensions. Thus SVMs combine the advantages of nonparametric and\nparametric models: they have the ﬂexibility to represent complex functions, but they\nare resistant to overﬁtting.\nYou could say that SVMs are successful because of one key insight and one neat trick. We",
  "parametric models: they have the ﬂexibility to represent complex functions, but they\nare resistant to overﬁtting.\nYou could say that SVMs are successful because of one key insight and one neat trick. We\nwill cover each in turn. In Figure 18.30(a), we have a binary classiﬁcation problem with three\ncandidate decision boundaries, each a linear separator. Each of them is consistent with all\nthe examples, so from the point of view of 0/1 loss, each would be equally good. Logistic\nregression would ﬁnd some separating line; the exact location of the line depends on all the\nexample points. The key insight of SVMs is that some examples are more important than\nothers, and that paying attention to them can lead to better generalization.\nConsider the lowest of the three separating lines in (a). It comes very close to 5 of the\nblack examples. Although it classiﬁes all the examples correctly, and thus minimizes loss, it Section 18.9.\nSupport Vector Machines\n745\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n(a)\n(b)\nFigure 18.30\nSupport vector machine classiﬁcation: (a) Two classes of points (black and\nwhite circles) and three candidate linear separators. (b) The maximum margin separator\n(heavy line), is at the midpoint of the margin (area between dashed lines). The support\nvectors (points with large circles) are the examples closest to the separator.\nshould make you nervous that so many examples are close to the line; it may be that other\nblack examples will turn out to fall on the other side of the line.\nSVMs address this issue: Instead of minimizing expected empirical loss on the training\ndata, SVMs attempt to minimize expected generalization loss. We don’t know where the\nas-yet-unseen points may fall, but under the probabilistic assumption that they are drawn\nfrom the same distribution as the previously seen examples, there are some arguments from\ncomputational learning theory (Section 18.5) suggesting that we minimize generalization loss\nby choosing the separator that is farthest away from the examples we have seen so far. We\ncall this separator, shown in Figure 18.30(b) the maximum margin separator. The margin\nMAXIMUM MARGIN\nSEPARATOR\nMARGIN\nis the width of the area bounded by dashed lines in the ﬁgure—twice the distance from the\nseparator to the nearest example point.\nNow, how do we ﬁnd this separator? Before showing the equations, some notation:",
  "MAXIMUM MARGIN\nSEPARATOR\nMARGIN\nis the width of the area bounded by dashed lines in the ﬁgure—twice the distance from the\nseparator to the nearest example point.\nNow, how do we ﬁnd this separator? Before showing the equations, some notation:\nTraditionally SVMs use the convention that class labels are +1 and -1, instead of the +1 and\n0 we have been using so far. Also, where we put the intercept into the weight vector w (and\na corresponding dummy 1 value into xj,0), SVMs do not do that; they keep the intercept\nas a separate parameter, b. With that in mind, the separator is deﬁned as the set of points\n{x : w · x + b = 0}. We could search the space of w and b with gradient descent to ﬁnd the\nparameters that maximize the margin while correctly classifying all the examples.\nHowever, it turns out there is another approach to solving this problem. We won’t\nshow the details, but will just say that there is an alternative representation called the dual 746\nChapter\n18.\nLearning from Examples\nrepresentation, in which the optimal solution is found by solving\nargmax\nα\n\f\nj\nαj −1\n2\n\f\nj,k\nαjαkyjyk(xj · xk)\n(18.13)\nsubject to the constraints αj ≥0 and \u0002\nj αjyj = 0. This is a quadratic programming\nQUADRATIC\nPROGRAMMING\noptimization problem, for which there are good software packages. Once we have found the\nvector α we can get back to w with the equation w = \u0002\nj αjxj, or we can stay in the dual\nrepresentation. There are three important properties of Equation (18.13). First, the expression\nis convex; it has a single global maximum that can be found efﬁciently. Second, the data enter\nthe expression only in the form of dot products of pairs of points. This second property is also\ntrue of the equation for the separator itself; once the optimal αj have been calculated, it is\nh(x) = sign\n⎛\n⎝\f\nj\nαjyj(x · xj) −b\n⎞\n⎠.\n(18.14)\nA ﬁnal important property is that the weights αj associated with each data point are zero ex-\ncept for the support vectors—the points closest to the separator. (They are called “support”\nSUPPORT VECTOR\nvectors because they “hold up” the separating plane.) Because there are usually many fewer\nsupport vectors than examples, SVMs gain some of the advantages of parametric models.\nWhat if the examples are not linearly separable? Figure 18.31(a) shows an input space\ndeﬁned by attributes x = (x1, x2), with positive examples (y = + 1) inside a circular region\nand negative examples (y = −1) outside. Clearly, there is no linear separator for this problem.",
  "deﬁned by attributes x = (x1, x2), with positive examples (y = + 1) inside a circular region\nand negative examples (y = −1) outside. Clearly, there is no linear separator for this problem.\nNow, suppose we re-express the input data—i.e., we map each input vector x to a new vector\nof feature values, F(x). In particular, let us use the three features\nf1 = x2\n1 ,\nf2 = x2\n2 ,\nf3 =\n√\n2x1x2 .\n(18.15)\nWe will see shortly where these came from, but for now, just look at what happens. Fig-\nure 18.31(b) shows the data in the new, three-dimensional space deﬁned by the three features;\nthe data are linearly separable in this space! This phenomenon is actually fairly general: if\ndata are mapped into a space of sufﬁciently high dimension, then they will almost always be\nlinearly separable—if you look at a set of points from enough directions, you’ll ﬁnd a way to\nmake them line up. Here, we used only three dimensions;11 Exercise 18.16 asks you to show\nthat four dimensions sufﬁce for linearly separating a circle anywhere in the plane (not just at\nthe origin), and ﬁve dimensions sufﬁce to linearly separate any ellipse. In general (with some\nspecial cases excepted) if we have N data points then they will always be separable in spaces\nof N −1 dimensions or more (Exercise 18.25).\nNow, we would not usually expect to ﬁnd a linear separator in the input space x, but\nwe can ﬁnd linear separators in the high-dimensional feature space F(x) simply by replacing\nxj·xk in Equation (18.13) with F(xj)·F(xk). This by itself is not remarkable—replacing x by\nF(x) in any learning algorithm has the required effect—but the dot product has some special\nproperties. It turns out that F(xj) · F(xk) can often be computed without ﬁrst computing F\n11 The reader may notice that we could have used just f1 and f2, but the 3D mapping illustrates the idea better. Section 18.9.\nSupport Vector Machines\n747\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\nx2\nx1\n0\n0.5\n1\n1.5\n2\nx1\n2\n0.5\n1\n1.5\n2\n2.5\nx2\n2\n-3\n-2\n-1\n0\n1\n2\n3\n√2x1x2\n(a)\n(b)\nFigure 18.31\n(a) A two-dimensional training set with positive examples as black cir-\ncles and negative examples as white circles. The true decision boundary, x2\n1 + x2\n2 ≤1,\nis also shown.\n(b) The same data after mapping into a three-dimensional input space\n(x2\n1, x2\n2,\n√\n2x1x2). The circular decision boundary in (a) becomes a linear decision boundary\nin three dimensions. Figure 18.30(b) gives a closeup of the separator in (b).",
  "(b) The same data after mapping into a three-dimensional input space\n(x2\n1, x2\n2,\n√\n2x1x2). The circular decision boundary in (a) becomes a linear decision boundary\nin three dimensions. Figure 18.30(b) gives a closeup of the separator in (b).\nfor each point. In our three-dimensional feature space deﬁned by Equation (18.15), a little bit\nof algebra shows that\nF(xj) · F(xk) = (xj · xk)2 .\n(That’s why the\n√\n2 is in f3.) The expression (xj · xk)2 is called a kernel function,12 and\nKERNEL FUNCTION\nis usually written as K(xj, xk). The kernel function can be applied to pairs of input data to\nevaluate dot products in some corresponding feature space. So, we can ﬁnd linear separators\nin the higher-dimensional feature space F(x) simply by replacing xj · xk in Equation (18.13)\nwith a kernel function K(xj, xk). Thus, we can learn in the higher-dimensional space, but we\ncompute only kernel functions rather than the full list of features for each data point.\nThe next step is to see that there’s nothing special about the kernel K(xj, xk) = (xj·xk)2.\nIt corresponds to a particular higher-dimensional feature space, but other kernel functions\ncorrespond to other feature spaces.\nA venerable result in mathematics, Mercer’s theo-\nrem (1909), tells us that any “reasonable”13 kernel function corresponds to some feature\nMERCER’S THEOREM\nspace. These feature spaces can be very large, even for innocuous-looking kernels. For ex-\nample, the polynomial kernel, K(xj, xk) = (1 + xj · xk)d, corresponds to a feature space\nPOLYNOMIAL\nKERNEL\nwhose dimension is exponential in d.\n12 This usage of “kernel function” is slightly different from the kernels in locally weighted regression. Some\nSVM kernels are distance metrics, but not all are.\n13 Here, “reasonable” means that the matrix Kjk = K(xj, xk) is positive deﬁnite. 748\nChapter\n18.\nLearning from Examples\nThis then is the clever kernel trick: Plugging these kernels into Equation (18.13),\nKERNEL TRICK\noptimal linear separators can be found efﬁciently in feature spaces with billions of (or, in\nsome cases, inﬁnitely many) dimensions. The resulting linear separators, when mapped back\nto the original input space, can correspond to arbitrarily wiggly, nonlinear decision bound-\naries between the positive and negative examples.\nIn the case of inherently noisy data, we may not want a linear separator in some high-\ndimensional space. Rather, we’d like a decision surface in a lower-dimensional space that",
  "aries between the positive and negative examples.\nIn the case of inherently noisy data, we may not want a linear separator in some high-\ndimensional space. Rather, we’d like a decision surface in a lower-dimensional space that\ndoes not cleanly separate the classes, but reﬂects the reality of the noisy data. That is pos-\nsible with the soft margin classiﬁer, which allows examples to fall on the wrong side of the\nSOFT MARGIN\ndecision boundary, but assigns them a penalty proportional to the distance required to move\nthem back on the correct side.\nThe kernel method can be applied not only with learning algorithms that ﬁnd optimal\nlinear separators, but also with any other algorithm that can be reformulated to work only\nwith dot products of pairs of data points, as in Equations 18.13 and 18.14. Once this is\ndone, the dot product is replaced by a kernel function and we have a kernelized version\nKERNELIZATION\nof the algorithm. This can be done easily for k-nearest-neighbors and perceptron learning\n(Section 18.7.2), among others.\n18.10\nENSEMBLE LEARNING\nSo far we have looked at learning methods in which a single hypothesis, chosen from a\nhypothesis space, is used to make predictions. The idea of ensemble learning methods is\nENSEMBLE\nLEARNING\nto select a collection, or ensemble, of hypotheses from the hypothesis space and combine\ntheir predictions. For example, during cross-validation we might generate twenty different\ndecision trees, and have them vote on the best classiﬁcation for a new example.\nThe motivation for ensemble learning is simple. Consider an ensemble of K = 5 hy-\npotheses and suppose that we combine their predictions using simple majority voting. For the\nensemble to misclassify a new example, at least three of the ﬁve hypotheses have to misclas-\nsify it. The hope is that this is much less likely than a misclassiﬁcation by a single hypothesis.\nSuppose we assume that each hypothesis hk in the ensemble has an error of p—that is, the\nprobability that a randomly chosen example is misclassiﬁed by hk is p. Furthermore, suppose\nwe assume that the errors made by each hypothesis are independent. In that case, if p is small,\nthen the probability of a large number of misclassiﬁcations occurring is minuscule. For ex-\nample, a simple calculation (Exercise 18.18) shows that using an ensemble of ﬁve hypotheses\nreduces an error rate of 1 in 10 down to an error rate of less than 1 in 100. Now, obviously",
  "ample, a simple calculation (Exercise 18.18) shows that using an ensemble of ﬁve hypotheses\nreduces an error rate of 1 in 10 down to an error rate of less than 1 in 100. Now, obviously\nthe assumption of independence is unreasonable, because hypotheses are likely to be misled\nin the same way by any misleading aspects of the training data. But if the hypotheses are at\nleast a little bit different, thereby reducing the correlation between their errors, then ensemble\nlearning can be very useful.\nAnother way to think about the ensemble idea is as a generic way of enlarging the\nhypothesis space. That is, think of the ensemble itself as a hypothesis and the new hypothesis Section 18.10.\nEnsemble Learning\n749\n+\n+ +\n+\n+\n+\n+\n+\n+\n+\n+ +\n+\n+\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n– –\n–\n–\n–\n–\n–\n–\nFigure 18.32\nIllustration of the increased expressive power obtained by ensemble learn-\ning. We take three linear threshold hypotheses, each of which classiﬁes positively on the\nunshaded side, and classify as positive any example classiﬁed positively by all three. The\nresulting triangular region is a hypothesis not expressible in the original hypothesis space.\nspace as the set of all possible ensembles constructable from hypotheses in the original space.\nFigure 18.32 shows how this can result in a more expressive hypothesis space. If the original\nhypothesis space allows for a simple and efﬁcient learning algorithm, then the ensemble\nmethod provides a way to learn a much more expressive class of hypotheses without incurring\nmuch additional computational or algorithmic complexity.\nThe most widely used ensemble method is called boosting. To understand how it works,\nBOOSTING\nwe need ﬁrst to explain the idea of a weighted training set. In such a training set, each\nWEIGHTED TRAINING\nSET\nexample has an associated weight wj ≥0. The higher the weight of an example, the higher\nis the importance attached to it during the learning of a hypothesis. It is straightforward to\nmodify the learning algorithms we have seen so far to operate with weighted training sets.14\nBoosting starts with wj = 1 for all the examples (i.e., a normal training set). From this\nset, it generates the ﬁrst hypothesis, h1. This hypothesis will classify some of the training ex-\namples correctly and some incorrectly. We would like the next hypothesis to do better on the\nmisclassiﬁed examples, so we increase their weights while decreasing the weights of the cor-",
  "amples correctly and some incorrectly. We would like the next hypothesis to do better on the\nmisclassiﬁed examples, so we increase their weights while decreasing the weights of the cor-\nrectly classiﬁed examples. From this new weighted training set, we generate hypothesis h2.\nThe process continues in this way until we have generated K hypotheses, where K is an input\nto the boosting algorithm. The ﬁnal ensemble hypothesis is a weighted-majority combination\nof all the K hypotheses, each weighted according to how well it performed on the training set.\nFigure 18.33 shows how the algorithm works conceptually. There are many variants of the ba-\nsic boosting idea, with different ways of adjusting the weights and combining the hypotheses.\nOne speciﬁc algorithm, called ADABOOST, is shown in Figure 18.34. ADABOOST has a very\nimportant property: if the input learning algorithm L is a weak learning algorithm—which\nWEAK LEARNING\n14 For learning algorithms in which this is not possible, one can instead create a replicated training set where\nthe jth example appears wj times, using randomization to handle fractional weights. 750\nChapter\n18.\nLearning from Examples\nh\nh1 =\nh2 =\nh3 =\nh4 =\nFigure 18.33\nHow the boosting algorithm works. Each shaded rectangle corresponds to\nan example; the height of the rectangle corresponds to the weight. The checks and crosses\nindicate whether the example was classiﬁed correctly by the current hypothesis. The size of\nthe decision tree indicates the weight of that hypothesis in the ﬁnal ensemble.\nmeans that L always returns a hypothesis with accuracy on the training set that is slightly\nbetter than random guessing (i.e., 50%+ϵ for Boolean classiﬁcation)—then ADABOOST will\nreturn a hypothesis that classiﬁes the training data perfectly for large enough K. Thus, the\nalgorithm boosts the accuracy of the original learning algorithm on the training data. This\nresult holds no matter how inexpressive the original hypothesis space and no matter how\ncomplex the function being learned.\nLet us see how well boosting does on the restaurant data. We will choose as our original\nhypothesis space the class of decision stumps, which are decision trees with just one test, at\nDECISION STUMP\nthe root. The lower curve in Figure 18.35(a) shows that unboosted decision stumps are not\nvery effective for this data set, reaching a prediction performance of only 81% on 100 training\nexamples. When boosting is applied (with K = 5), the performance is better, reaching 93%",
  "very effective for this data set, reaching a prediction performance of only 81% on 100 training\nexamples. When boosting is applied (with K = 5), the performance is better, reaching 93%\nafter 100 examples.\nAn interesting thing happens as the ensemble size K increases. Figure 18.35(b) shows\nthe training set performance (on 100 examples) as a function of K. Notice that the error\nreaches zero when K is 20; that is, a weighted-majority combination of 20 decision stumps\nsufﬁces to ﬁt the 100 examples exactly. As more stumps are added to the ensemble, the error\nremains at zero. The graph also shows that the test set performance continues to increase\nlong after the training set error has reached zero. At K = 20, the test performance is 0.95\n(or 0.05 error), and the performance increases to 0.98 as late as K = 137, before gradually\ndropping to 0.95.\nThis ﬁnding, which is quite robust across data sets and hypothesis spaces, came as quite\na surprise when it was ﬁrst noticed. Ockham’s razor tells us not to make hypotheses more Section 18.10.\nEnsemble Learning\n751\nfunction ADABOOST(examples,L,K) returns a weighted-majority hypothesis\ninputs: examples, set of N labeled examples (x1, y1), . . . , (xN, yN)\nL, a learning algorithm\nK, the number of hypotheses in the ensemble\nlocal variables: w, a vector of N example weights, initially 1/N\nh, a vector of K hypotheses\nz, a vector of K hypothesis weights\nfor k = 1 to K do\nh[k] ←L(examples,w)\nerror ←0\nfor j = 1 to N do\nif h[k](xj) ̸= yj then error ←error + w[j]\nfor j = 1 to N do\nif h[k](xj) = yj then w[j] ←w[j] · error/(1 −error)\nw ←NORMALIZE(w)\nz[k] ←log (1 −error)/error\nreturn WEIGHTED-MAJORITY(h,z)\nFigure 18.34\nThe ADABOOST variant of the boosting method for ensemble learning. The\nalgorithm generates hypotheses by successively reweighting the training examples. The func-\ntion WEIGHTED-MAJORITY generates a hypothesis that returns the output value with the\nhighest vote from the hypotheses in h, with votes weighted by z.\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\n0\n20\n40\n60\n80\n100\nProportion correct on test set\nTraining set size\nBoosted decision stumps\nDecision stump\n 0.6\n 0.65\n 0.7\n 0.75\n 0.8\n 0.85\n 0.9\n 0.95\n 1\n 0\n 50\n 100\n 150\n 200\nTraining/test accuracy\nNumber of hypotheses K\nTraining error\nTest error\n(a)\n(b)\nFigure 18.35\n(a) Graph showing the performance of boosted decision stumps with K = 5\nversus unboosted decision stumps on the restaurant data. (b) The proportion correct on the",
  "Number of hypotheses K\nTraining error\nTest error\n(a)\n(b)\nFigure 18.35\n(a) Graph showing the performance of boosted decision stumps with K = 5\nversus unboosted decision stumps on the restaurant data. (b) The proportion correct on the\ntraining set and the test set as a function of K, the number of hypotheses in the ensemble.\nNotice that the test set accuracy improves slightly even after the training accuracy reaches 1,\ni.e., after the ensemble ﬁts the data exactly. 752\nChapter\n18.\nLearning from Examples\ncomplex than necessary, but the graph tells us that the predictions improve as the ensemble\nhypothesis gets more complex! Various explanations have been proposed for this. One view\nis that boosting approximates Bayesian learning (see Chapter 20), which can be shown to\nbe an optimal learning algorithm, and the approximation improves as more hypotheses are\nadded. Another possible explanation is that the addition of further hypotheses enables the\nensemble to be more deﬁnite in its distinction between positive and negative examples, which\nhelps it when it comes to classifying new examples.\n18.10.1\nOnline Learning\nSo far, everything we have done in this chapter has relied on the assumption that the data are\ni.i.d. (independent and identically distributed). On the one hand, that is a sensible assumption:\nif the future bears no resemblance to the past, then how can we predict anything? On the other\nhand, it is too strong an assumption: it is rare that our inputs have captured all the information\nthat would make the future truly independent of the past.\nIn this section we examine what to do when the data are not i.i.d.; when they can change\nover time. In this case, it matters when we make a prediction, so we will adopt the perspective\ncalled online learning: an agent receives an input xj from nature, predicts the corresponding\nONLINE LEARNING\nyj, and then is told the correct answer. Then the process repeats with xj+1, and so on. One\nmight think this task is hopeless—if nature is adversarial, all the predictions may be wrong.\nIt turns out that there are some guarantees we can make.\nLet us consider the situation where our input consists of predictions from a panel of\nexperts. For example, each day a set of K pundits predicts whether the stock market will go\nup or down, and our task is to pool those predictions and make our own. One way to do this\nis to keep track of how well each expert performs, and choose to believe them in proportion",
  "up or down, and our task is to pool those predictions and make our own. One way to do this\nis to keep track of how well each expert performs, and choose to believe them in proportion\nto their past performance. This is called the randomized weighted majority algorithm. We\nRANDOMIZED\nWEIGHTED\nMAJORITY\nALGORITHM\ncan described it more formally:\n1. Initialize a set of weights {w1, . . . , wK} all to 1.\n2. Receive the predictions {ˆy1, . . . , ˆyK} from the experts.\n3. Randomly choose an expert k∗, in proportion to its weight: P(k) = wk/(\u0002\nk′ wk′).\n4. Predict ˆyk∗.\n5. Receive the correct answer y.\n6. For each expert k such that ˆyk ̸= y, update wk ←βwk\nHere β is a number, 0 < β < 1, that tells how much to penalize an expert for each mistake.\nWe measure the success of this algorithm in terms of regret, which is deﬁned as the\nREGRET\nnumber of additional mistakes we make compared to the expert who, in hindsight, had the\nbest prediction record. Let M∗be the number of mistakes made by the best expert. Then the\nnumber of mistakes, M, made by the random weighted majority algorithm, is bounded by15\nM < M∗ln(1/β) + ln K\n1 −β\n.\n15 See (Blum, 1996) for the proof. Section 18.11.\nPractical Machine Learning\n753\nThis bound holds for any sequence of examples, even ones chosen by adversaries trying to\ndo their worst. To be speciﬁc, when there are K = 10 experts, if we choose β = 1/2 then\nour number of mistakes is bounded by 1.39M∗+ 4.6, and if β = 3/4 by 1.15M∗+ 9.2. In\ngeneral, if β is close to 1 then we are responsive to change over the long run; if the best expert\nchanges, we will pick up on it before too long. However, we pay a penalty at the beginning,\nwhen we start with all experts trusted equally; we may accept the advice of the bad experts\nfor too long. When β is closer to 0, these two factors are reversed. Note that we can choose β\nto get asymptotically close to M∗in the long run; this is called no-regret learning (because\nNO-REGRET\nLEARNING\nthe average amount of regret per trial tends to 0 as the number of trials increases).\nOnline learning is helpful when the data may be changing rapidly over time. It is also\nuseful for applications that involve a large collection of data that is constantly growing, even\nif changes are gradual. For example, with a database of millions of Web images, you wouldn’t\nwant to train, say, a linear regression model on all the data, and then retrain from scratch every",
  "if changes are gradual. For example, with a database of millions of Web images, you wouldn’t\nwant to train, say, a linear regression model on all the data, and then retrain from scratch every\ntime a new image is added. It would be more practical to have an online algorithm that allows\nimages to be added incrementally. For most learning algorithms based on minimizing loss,\nthere is an online version based on minimizing regret. It is a bonus that many of these online\nalgorithms come with guaranteed bounds on regret.\nTo some observers, it is surprising that there are such tight bounds on how well we can\ndo compared to a panel of experts. To others, the really surprising thing is that when panels\nof human experts congregate—predicting stock market prices, sports outcomes, or political\ncontests—the viewing public is so willing to listen to them pontiﬁcate and so unwilling to\nquantify their error rates.\n18.11\nPRACTICAL MACHINE LEARNING\nWe have introduced a wide range of machine learning techniques, each illustrated with simple\nlearning tasks. In this section, we consider two aspects of practical machine learning. The ﬁrst\ninvolves ﬁnding algorithms capable of learning to recognize handwritten digits and squeezing\nevery last drop of predictive performance out of them. The second involves anything but—\npointing out that obtaining, cleaning, and representing the data can be at least as important as\nalgorithm engineering.\n18.11.1\nCase study: Handwritten digit recognition\nRecognizing handwritten digits is an important problem with many applications, including\nautomated sorting of mail by postal code, automated reading of checks and tax returns, and\ndata entry for hand-held computers. It is an area where rapid progress has been made, in part\nbecause of better learning algorithms and in part because of the availability of better training\nsets. The United States National Institute of Science and Technology (NIST) has archived a\ndatabase of 60,000 labeled digits, each 20 × 20 = 400 pixels with 8-bit grayscale values. It\nhas become one of the standard benchmark problems for comparing new learning algorithms.\nSome example digits are shown in Figure 18.36. 754\nChapter\n18.\nLearning from Examples\nFigure 18.36\nExamples from the NIST database of handwritten digits. Top row: examples\nof digits 0–9 that are easy to identify. Bottom row: more difﬁcult examples of the same digits.\nMany different learning approaches have been tried. One of the ﬁrst, and probably the",
  "of digits 0–9 that are easy to identify. Bottom row: more difﬁcult examples of the same digits.\nMany different learning approaches have been tried. One of the ﬁrst, and probably the\nsimplest, is the 3-nearest-neighbor classiﬁer, which also has the advantage of requiring no\ntraining time. As a memory-based algorithm, however, it must store all 60,000 images, and\nits run time performance is slow. It achieved a test error rate of 2.4%.\nA single-hidden-layer neural network was designed for this problem with 400 input\nunits (one per pixel) and 10 output units (one per class). Using cross-validation, it was found\nthat roughly 300 hidden units gave the best performance. With full interconnections between\nlayers, there were a total of 123,300 weights. This network achieved a 1.6% error rate.\nA series of specialized neural networks called LeNet were devised to take advantage\nof the structure of the problem—that the input consists of pixels in a two–dimensional array,\nand that small changes in the position or slant of an image are unimportant. Each network\nhad an input layer of 32 × 32 units, onto which the 20 × 20 pixels were centered so that each\ninput unit is presented with a local neighborhood of pixels. This was followed by three layers\nof hidden units. Each layer consisted of several planes of n × n arrays, where n is smaller\nthan the previous layer so that the network is down-sampling the input, and where the weights\nof every unit in a plane are constrained to be identical, so that the plane is acting as a feature\ndetector: it can pick out a feature such as a long vertical line or a short semi-circular arc. The\noutput layer had 10 units. Many versions of this architecture were tried; a representative one\nhad hidden layers with 768, 192, and 30 units, respectively. The training set was augmented\nby applying afﬁne transformations to the actual inputs: shifting, slightly rotating, and scaling\nthe images. (Of course, the transformations have to be small, or else a 6 will be transformed\ninto a 9!) The best error rate achieved by LeNet was 0.9%.\nA boosted neural network combined three copies of the LeNet architecture, with the\nsecond one trained on a mix of patterns that the ﬁrst one got 50% wrong, and the third one\ntrained on patterns for which the ﬁrst two disagreed. During testing, the three nets voted with\nthe majority ruling. The test error rate was 0.7%.\nA support vector machine (see Section 18.9) with 25,000 support vectors achieved an",
  "trained on patterns for which the ﬁrst two disagreed. During testing, the three nets voted with\nthe majority ruling. The test error rate was 0.7%.\nA support vector machine (see Section 18.9) with 25,000 support vectors achieved an\nerror rate of 1.1%. This is remarkable because the SVM technique, like the simple nearest-\nneighbor approach, required almost no thought or iterated experimentation on the part of the\ndeveloper, yet it still came close to the performance of LeNet, which had had years of devel-\nopment. Indeed, the support vector machine makes no use of the structure of the problem,\nand would perform just as well if the pixels were presented in a permuted order. Section 18.11.\nPractical Machine Learning\n755\nA virtual support vector machine starts with a regular SVM and then improves it\nVIRTUAL SUPPORT\nVECTOR MACHINE\nwith a technique that is designed to take advantage of the structure of the problem. Instead of\nallowing products of all pixel pairs, this approach concentrates on kernels formed from pairs\nof nearby pixels. It also augments the training set with transformations of the examples, just\nas LeNet did. A virtual SVM achieved the best error rate recorded to date, 0.56%.\nShape matching is a technique from computer vision used to align corresponding parts\nof two different images of objects (Belongie et al., 2002). The idea is to pick out a set\nof points in each of the two images, and then compute, for each point in the ﬁrst image,\nwhich point in the second image it corresponds to. From this alignment, we then compute a\ntransformation between the images. The transformation gives us a measure of the distance\nbetween the images. This distance measure is better motivated than just counting the number\nof differing pixels, and it turns out that a 3–nearest neighbor algorithm using this distance\nmeasure performs very well. Training on only 20,000 of the 60,000 digits, and using 100\nsample points per image extracted from a Canny edge detector, a shape matching classiﬁer\nachieved 0.63% test error.\nHumans are estimated to have an error rate of about 0.2% on this problem. This ﬁgure\nis somewhat suspect because humans have not been tested as extensively as have machine\nlearning algorithms. On a similar data set of digits from the United States Postal Service,\nhuman errors were at 2.5%.\nThe following ﬁgure summarizes the error rates, run time performance, memory re-\nquirements, and amount of training time for the seven algorithms we have discussed. It also",
  "human errors were at 2.5%.\nThe following ﬁgure summarizes the error rates, run time performance, memory re-\nquirements, and amount of training time for the seven algorithms we have discussed. It also\nadds another measure, the percentage of digits that must be rejected to achieve 0.5% error.\nFor example, if the SVM is allowed to reject 1.8% of the inputs—that is, pass them on for\nsomeone else to make the ﬁnal judgment—then its error rate on the remaining 98.2% of the\ninputs is reduced from 1.1% to 0.5%.\nThe following table summarizes the error rate and some of the other characteristics of\nthe seven techniques we have discussed.\n3\n300\nBoosted\nVirtual Shape\nNN\nHidden LeNet\nLeNet\nSVM\nSVM\nMatch\nError rate (pct.)\n2.4\n1.6\n0.9\n0.7\n1.1\n0.56\n0.63\nRun time (millisec/digit)\n1000\n10\n30\n50\n2000\n200\nMemory requirements (Mbyte)\n12\n.49\n.012\n.21\n11\nTraining time (days)\n0\n7\n14\n30\n10\n% rejected to reach 0.5% error\n8.1\n3.2\n1.8\n0.5\n1.8\n18.11.2\nCase study: Word senses and house prices\nIn a textbook we need to deal with simple, toy data to get the ideas across: a small data set,\nusually in two dimensions. But in practical applications of machine learning, the data set\nis usually large, multidimensional, and messy. The data are not handed to the analyst in a\nprepackaged set of (x, y) values; rather the analyst needs to go out and acquire the right data.\nThere is a task to be accomplished, and most of the engineering problem is deciding what\ndata are necessary to accomplish the task; a smaller part is choosing and implementing an 756\nChapter\n18.\nLearning from Examples\n 0.75\n 0.8\n 0.85\n 0.9\n 0.95\n 1\n 1\n 10\n 100\n 1000\nProportion correct on test set\nTraining set size (millions of words)\nFigure 18.37\nLearning curves for ﬁve learning algorithms on a common task. Note that\nthere appears to be more room for improvement in the horizontal direction (more training\ndata) than in the vertical direction (different machine learning algorithm). Adapted from\nBanko and Brill (2001).\nappropriate machine learning method to process the data. Figure 18.37 shows a typical real-\nworld example, comparing ﬁve learning algorithms on the task of word-sense classiﬁcation\n(given a sentence such as “The bank folded,” classify the word “bank” as “money-bank” or\n“river-bank”). The point is that machine learning researchers have focused mainly on the\nvertical direction: Can I invent a new learning algorithm that performs better than previously",
  "“river-bank”). The point is that machine learning researchers have focused mainly on the\nvertical direction: Can I invent a new learning algorithm that performs better than previously\npublished algorithms on a standard training set of 1 million words? But the graph shows\nthere is more room for improvement in the horizontal direction: instead of inventing a new\nalgorithm, all I need to do is gather 10 million words of training data; even the worst algorithm\nat 10 million words is performing better than the best algorithm at 1 million. As we gather\neven more data, the curves continue to rise, dwarﬁng the differences between algorithms.\nConsider another problem: the task of estimating the true value of houses that are for\nsale. In Figure 18.13 we showed a toy version of this problem, doing linear regression of\nhouse size to asking price. You probably noticed many limitations of this model. First, it is\nmeasuring the wrong thing: we want to estimate the selling price of a house, not the asking\nprice. To solve this task we’ll need data on actual sales. But that doesn’t mean we should\nthrow away the data about asking price—we can use it as one of the input features. Besides\nthe size of the house, we’ll need more information: the number of rooms, bedrooms and\nbathrooms; whether the kitchen and bathrooms have been recently remodeled; the age of\nthe house; we’ll also need information about the lot, and the neighborhood. But how do\nwe deﬁne neighborhood? By zip code? What if part of one zip code is on the “wrong”\nside of the highway or train tracks, and the other part is desirable? What about the school\ndistrict? Should the name of the school district be a feature, or the average test scores? In\naddition to deciding what features to include, we will have to deal with missing data; different\nareas have different customs on what data are reported, and individual cases will always be\nmissing some data. If the data you want are not available, perhaps you can set up a social\nnetworking site to encourage people to share and correct data. In the end, this process of Section 18.12.\nSummary\n757\ndeciding what features to use, and how to use them, is just as important as choosing between\nlinear regression, decision trees, or some other form of learning.\nThat said, one does have to pick a method (or methods) for a problem. There is no\nguaranteed way to pick the best method, but there are some rough guidelines. Decision",
  "linear regression, decision trees, or some other form of learning.\nThat said, one does have to pick a method (or methods) for a problem. There is no\nguaranteed way to pick the best method, but there are some rough guidelines. Decision\ntrees are good when there are a lot of discrete features and you believe that many of them\nmay be irrelevant. Nonparametric methods are good when you have a lot of data and no prior\nknowledge, and when you don’t want to worry too much about choosing just the right features\n(as long as there are fewer than 20 or so). However, nonparametric methods usually give you\na function h that is more expensive to run. Support vector machines are often considered the\nbest method to try ﬁrst, provided the data set is not too large.\n18.12\nSUMMARY\nThis chapter has concentrated on inductive learning of functions from examples. The main\npoints were as follows:\n• Learning takes many forms, depending on the nature of the agent, the component to be\nimproved, and the available feedback.\n• If the available feedback provides the correct answer for example inputs, then the learn-\ning problem is called supervised learning. The task is to learn a function y = h(x).\nLearning a discrete-valued function is called classiﬁcation; learning a continuous func-\ntion is called regression.\n• Inductive learning involves ﬁnding a hypothesis that agrees well with the examples.\nOckham’s razor suggests choosing the simplest consistent hypothesis. The difﬁculty\nof this task depends on the chosen representation.\n• Decision trees can represent all Boolean functions. The information-gain heuristic\nprovides an efﬁcient method for ﬁnding a simple, consistent decision tree.\n• The performance of a learning algorithm is measured by the learning curve, which\nshows the prediction accuracy on the test set as a function of the training-set size.\n• When there are multiple models to choose from, cross-validation can be used to select\na model that will generalize well.\n• Sometimes not all errors are equal. A loss function tells us how bad each error is; the\ngoal is then to minimize loss over a validation set.\n• Computational learning theory analyzes the sample complexity and computational\ncomplexity of inductive learning. There is a tradeoff between the expressiveness of the\nhypothesis language and the ease of learning.\n• Linear regression is a widely used model. The optimal parameters of a linear regres-\nsion model can be found by gradient descent search, or computed exactly.",
  "hypothesis language and the ease of learning.\n• Linear regression is a widely used model. The optimal parameters of a linear regres-\nsion model can be found by gradient descent search, or computed exactly.\n• A linear classiﬁer with a hard threshold—also known as a perceptron—can be trained\nby a simple weight update rule to ﬁt data that are linearly separable. In other cases,\nthe rule fails to converge. 758\nChapter\n18.\nLearning from Examples\n• Logistic regression replaces the perceptron’s hard threshold with a soft threshold de-\nﬁned by a logistic function. Gradient descent works well even for noisy data that are\nnot linearly separable.\n• Neural networks represent complex nonlinear functions with a network of linear-\nthreshold units. termMultilayer feed-forward neural networks can represent any func-\ntion, given enough units. The back-propagation algorithm implements a gradient de-\nscent in parameter space to minimize the output error.\n• Nonparametric models use all the data to make each prediction, rather than trying to\nsummarize the data ﬁrst with a few parameters. Examples include nearest neighbors\nand locally weighted regression.\n• Support vector machines ﬁnd linear separators with maximum margin to improve\nthe generalization performance of the classiﬁer. Kernel methods implicitly transform\nthe input data into a high-dimensional space where a linear separator may exist, even if\nthe original data are non-separable.\n• Ensemble methods such as boosting often perform better than individual methods. In\nonline learning we can aggregate the opinions of experts to come arbitrarily close to the\nbest expert’s performance, even when the distribution of the data is constantly shifting.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nChapter 1 outlined the history of philosophical investigations into inductive learning. William\nof Ockham16 (1280–1349), the most inﬂuential philosopher of his century and a major con-\ntributor to medieval epistemology, logic, and metaphysics, is credited with a statement called\n“Ockham’s Razor”—in Latin, Entia non sunt multiplicanda praeter necessitatem, and in En-\nglish, “Entities are not to be multiplied beyond necessity.” Unfortunately, this laudable piece\nof advice is nowhere to be found in his writings in precisely these words (although he did\nsay “Pluralitas non est ponenda sine necessitate,” or “plurality shouldn’t be posited without\nnecessity”). A similar sentiment was expressed by Aristotle in 350 B.C. in Physics book I,",
  "say “Pluralitas non est ponenda sine necessitate,” or “plurality shouldn’t be posited without\nnecessity”). A similar sentiment was expressed by Aristotle in 350 B.C. in Physics book I,\nchapter VI: “For the more limited, if adequate, is always preferable.”\nThe ﬁrst notable use of decision trees was in EPAM, the “Elementary Perceiver And\nMemorizer” (Feigenbaum, 1961), which was a simulation of human concept learning. ID3\n(Quinlan, 1979) added the crucial idea of choosing the attribute with maximum entropy; it is\nthe basis for the decision tree algorithm in this chapter. Information theory was developed by\nClaude Shannon to aid in the study of communication (Shannon and Weaver, 1949). (Shan-\nnon also contributed one of the earliest examples of machine learning, a mechanical mouse\nnamed Theseus that learned to navigate through a maze by trial and error.) The χ2 method\nof tree pruning was described by Quinlan (1986). C4.5, an industrial-strength decision tree\npackage, can be found in Quinlan (1993). An independent tradition of decision tree learning\nexists in the statistical literature. Classiﬁcation and Regression Trees (Breiman et al., 1984),\nknown as the “CART book,” is the principal reference.\n16 The name is often misspelled as “Occam,” perhaps from the French rendering, “Guillaume d’Occam.” Bibliographical and Historical Notes\n759\nCross-validation was ﬁrst introduced by Larson (1931), and in a form close to what\nwe show by Stone (1974) and Golub et al. (1979). The regularization procedure is due to\nTikhonov (1963). Guyon and Elisseeff (2003) introduce a journal issue devoted to the prob-\nlem of feature selection. Banko and Brill (2001) and Halevy et al. (2009) discuss the advan-\ntages of using large amounts of data. It was Robert Mercer, a speech researcher who said\nin 1985 “There is no data like more data.” (Lyman and Varian, 2003) estimate that about 5\nexabytes (5 × 1018 bytes) of data was produced in 2002, and that the rate of production is\ndoubling every 3 years.\nTheoretical analysis of learning algorithms began with the work of Gold (1967) on\nidentiﬁcation in the limit. This approach was motivated in part by models of scientiﬁc\ndiscovery from the philosophy of science (Popper, 1962), but has been applied mainly to the\nproblem of learning grammars from example sentences (Osherson et al., 1986).\nWhereas the identiﬁcation-in-the-limit approach concentrates on eventual convergence,",
  "problem of learning grammars from example sentences (Osherson et al., 1986).\nWhereas the identiﬁcation-in-the-limit approach concentrates on eventual convergence,\nthe study of Kolmogorov complexity or algorithmic complexity, developed independently\nKOLMOGOROV\nCOMPLEXITY\nby Solomonoff (1964, 2009) and Kolmogorov (1965), attempts to provide a formal deﬁnition\nfor the notion of simplicity used in Ockham’s razor. To escape the problem that simplicity\ndepends on the way in which information is represented, it is proposed that simplicity be\nmeasured by the length of the shortest program for a universal Turing machine that correctly\nreproduces the observed data. Although there are many possible universal Turing machines,\nand hence many possible “shortest” programs, these programs differ in length by at most a\nconstant that is independent of the amount of data. This beautiful insight, which essentially\nshows that any initial representation bias will eventually be overcome by the data itself, is\nmarred only by the undecidability of computing the length of the shortest program. Approx-\nimate measures such as the minimum description length, or MDL (Rissanen, 1984, 2007)\nMINIMUM\nDESCRIPTION\nLENGTH\ncan be used instead and have produced excellent results in practice. The text by Li and Vi-\ntanyi (1993) is the best source for Kolmogorov complexity.\nThe theory of PAC-learning was inaugurated by Leslie Valiant (1984). His work stressed\nthe importance of computational and sample complexity. With Michael Kearns (1990), Valiant\nshowed that several concept classes cannot be PAC-learned tractably, even though sufﬁcient\ninformation is available in the examples. Some positive results were obtained for classes such\nas decision lists (Rivest, 1987).\nAn independent tradition of sample-complexity analysis has existed in statistics, begin-\nning with the work on uniform convergence theory (Vapnik and Chervonenkis, 1971). The\nUNIFORM\nCONVERGENCE\nTHEORY\nso-called VC dimension provides a measure roughly analogous to, but more general than, the\nVC DIMENSION\nln |H| measure obtained from PAC analysis. The VC dimension can be applied to continuous\nfunction classes, to which standard PAC analysis does not apply. PAC-learning theory and\nVC theory were ﬁrst connected by the “four Germans” (none of whom actually is German):\nBlumer, Ehrenfeucht, Haussler, and Warmuth (1989).\nLinear regression with squared error loss goes back to Legendre (1805) and Gauss",
  "VC theory were ﬁrst connected by the “four Germans” (none of whom actually is German):\nBlumer, Ehrenfeucht, Haussler, and Warmuth (1989).\nLinear regression with squared error loss goes back to Legendre (1805) and Gauss\n(1809), who were both working on predicting orbits around the sun. The modern use of\nmultivariate regression for machine learning is covered in texts such as Bishop (2007). Ng\n(2004) analyzed the differences between L1 and L2 regularization. 760\nChapter\n18.\nLearning from Examples\nThe term logistic function comes from Pierre-Franc¸ois Verhulst (1804–1849), a statis-\ntician who used the curve to model population growth with limited resources, a more realis-\ntic model than the unconstrained geometric growth proposed by Thomas Malthus. Verhulst\ncalled it the courbe logistique, because of its relation to the logarithmic curve. The term re-\ngression is due to Francis Galton, nineteenth century statistician, cousin of Charles Darwin,\nand initiator of the ﬁelds of meteorology, ﬁngerprint analysis, and statistical correlation, who\nused it in the sense of regression to the mean. The term curse of dimensionality comes from\nRichard Bellman (1961).\nLogistic regression can be solved with gradient descent, or with the Newton-Raphson\nmethod (Newton, 1671; Raphson, 1690). A variant of the Newton method called L-BFGS is\nsometimes used for large-dimensional problems; the L stands for “limited memory,” meaning\nthat it avoids creating the full matrices all at once, and instead creates parts of them on the\nﬂy. BFGS are authors’ initials (Byrd et al., 1995).\nNearest-neighbors models date back at least to Fix and Hodges (1951) and have been a\nstandard tool in statistics and pattern recognition ever since. Within AI, they were popularized\nby Stanﬁll and Waltz (1986), who investigated methods for adapting the distance metric to the\ndata. Hastie and Tibshirani (1996) developed a way to localize the metric to each point in the\nspace, depending on the distribution of data around that point. Gionis et al. (1999) introduced\nlocality-sensitive hashing, which has revolutionized the retrieval of similar objects in high-\ndimensional spaces, particularly in computer vision. Andoni and Indyk (2006) provide a\nrecent survey of LSH and related methods.\nThe ideas behind kernel machines come from Aizerman et al. (1964) (who also in-\ntroduced the kernel trick), but the full development of the theory is due to Vapnik and his",
  "recent survey of LSH and related methods.\nThe ideas behind kernel machines come from Aizerman et al. (1964) (who also in-\ntroduced the kernel trick), but the full development of the theory is due to Vapnik and his\ncolleagues (Boser et al., 1992). SVMs were made practical with the introduction of the\nsoft-margin classiﬁer for handling noisy data in a paper that won the 2008 ACM Theory\nand Practice Award (Cortes and Vapnik, 1995), and of the Sequential Minimal Optimization\n(SMO) algorithm for efﬁciently solving SVM problems using quadratic programming (Platt,\n1999). SVMs have proven to be very popular and effective for tasks such as text categoriza-\ntion (Joachims, 2001), computational genomics (Cristianini and Hahn, 2007), and natural lan-\nguage processing, such as the handwritten digit recognition of DeCoste and Sch¨olkopf (2002).\nAs part of this process, many new kernels have been designed that work with strings, trees,\nand other nonnumerical data types. A related technique that also uses the kernel trick to im-\nplicitly represent an exponential feature space is the voted perceptron (Freund and Schapire,\n1999; Collins and Duffy, 2002). Textbooks on SVMs include Cristianini and Shawe-Taylor\n(2000) and Sch¨olkopf and Smola (2002). A friendlier exposition appears in the AI Magazine\narticle by Cristianini and Sch¨olkopf (2002). Bengio and LeCun (2007) show some of the\nlimitations of SVMs and other local, nonparametric methods for learning functions that have\na global structure but do not have local smoothness.\nEnsemble learning is an increasingly popular technique for improving the performance\nof learning algorithms. Bagging (Breiman, 1996), the ﬁrst effective method, combines hy-\nBAGGING\npotheses learned from multiple bootstrap data sets, each generated by subsampling the orig-\ninal data set. The boosting method described in this chapter originated with theoretical work\nby Schapire (1990). The ADABOOST algorithm was developed by Freund and Schapire Bibliographical and Historical Notes\n761\n(1996) and analyzed theoretically by Schapire (2003). Friedman et al. (2000) explain boost-\ning from a statistician’s viewpoint. Online learning is covered in a survey by Blum (1996)\nand a book by Cesa-Bianchi and Lugosi (2006). Dredze et al. (2008) introduce the idea of\nconﬁdence-weighted online learning for classiﬁcation: in addition to keeping a weight for\neach parameter, they also maintain a measure of conﬁdence, so that a new example can have",
  "conﬁdence-weighted online learning for classiﬁcation: in addition to keeping a weight for\neach parameter, they also maintain a measure of conﬁdence, so that a new example can have\na large effect on features that were rarely seen before (and thus had low conﬁdence) and a\nsmall effect on common features that have already been well-estimated.\nThe literature on neural networks is rather too large (approximately 150,000 papers to\ndate) to cover in detail. Cowan and Sharp (1988b, 1988a) survey the early history, beginning\nwith the work of McCulloch and Pitts (1943). (As mentioned in Chapter 1, John McCarthy\nhas pointed to the work of Nicolas Rashevsky (1936, 1938) as the earliest mathematical model\nof neural learning.) Norbert Wiener, a pioneer of cybernetics and control theory (Wiener,\n1948), worked with McCulloch and Pitts and inﬂuenced a number of young researchers in-\ncluding Marvin Minsky, who may have been the ﬁrst to develop a working neural network in\nhardware in 1951 (see Minsky and Papert, 1988, pp. ix–x). Turing (1948) wrote a research\nreport titled Intelligent Machinery that begins with the sentence “I propose to investigate the\nquestion as to whether it is possible for machinery to show intelligent behaviour” and goes on\nto describe a recurrent neural network architecture he called “B-type unorganized machines”\nand an approach to training them. Unfortunately, the report went unpublished until 1969, and\nwas all but ignored until recently.\nFrank Rosenblatt (1957) invented the modern “perceptron” and proved the percep-\ntron convergence theorem (1960), although it had been foreshadowed by purely mathemat-\nical work outside the context of neural networks (Agmon, 1954; Motzkin and Schoenberg,\n1954). Some early work was also done on multilayer networks, including Gamba percep-\ntrons (Gamba et al., 1961) and madalines (Widrow, 1962). Learning Machines (Nilsson,\n1965) covers much of this early work and more. The subsequent demise of early perceptron\nresearch efforts was hastened (or, the authors later claimed, merely explained) by the book\nPerceptrons (Minsky and Papert, 1969), which lamented the ﬁeld’s lack of mathematical\nrigor. The book pointed out that single-layer perceptrons could represent only linearly sepa-\nrable concepts and noted the lack of effective learning algorithms for multilayer networks.\nThe papers in (Hinton and Anderson, 1981), based on a conference in San Diego in",
  "rable concepts and noted the lack of effective learning algorithms for multilayer networks.\nThe papers in (Hinton and Anderson, 1981), based on a conference in San Diego in\n1979, can be regarded as marking a renaissance of connectionism. The two-volume “PDP”\n(Parallel Distributed Processing) anthology (Rumelhart et al., 1986a) and a short article in\nNature (Rumelhart et al., 1986b) attracted a great deal of attention—indeed, the number of\npapers on “neural networks” multiplied by a factor of 200 between 1980–84 and 1990–94.\nThe analysis of neural networks using the physical theory of magnetic spin glasses (Amit\net al., 1985) tightened the links between statistical mechanics and neural network theory—\nproviding not only useful mathematical insights but also respectability. The back-propagation\ntechnique had been invented quite early (Bryson and Ho, 1969) but it was rediscovered several\ntimes (Werbos, 1974; Parker, 1985).\nThe probabilistic interpretation of neural networks has several sources, including Baum\nand Wilczek (1988) and Bridle (1990). The role of the sigmoid function is discussed by\nJordan (1995). Bayesian parameter learning for neural networks was proposed by MacKay 762\nChapter\n18.\nLearning from Examples\n(1992) and is explored further by Neal (1996). The capacity of neural networks to represent\nfunctions was investigated by Cybenko (1988, 1989), who showed that two hidden layers are\nenough to represent any function and a single layer is enough to represent any continuous\nfunction. The “optimal brain damage” method for removing useless connections is by LeCun\net al. (1989), and Sietsma and Dow (1988) show how to remove useless units. The tiling\nalgorithm for growing larger structures is due to M´ezard and Nadal (1989). LeCun et al.\n(1995) survey a number of algorithms for handwritten digit recognition. Improved error rates\nsince then were reported by Belongie et al. (2002) for shape matching and DeCoste and\nSch¨olkopf (2002) for virtual support vectors. At the time of writing, the best test error rate\nreported is 0.39% by Ranzato et al. (2007) using a convolutional neural network.\nThe complexity of neural network learning has been investigated by researchers in com-\nputational learning theory. Early computational results were obtained by Judd (1990), who\nshowed that the general problem of ﬁnding a set of weights consistent with a set of examples\nis NP-complete, even under very restrictive assumptions. Some of the ﬁrst sample complexity",
  "showed that the general problem of ﬁnding a set of weights consistent with a set of examples\nis NP-complete, even under very restrictive assumptions. Some of the ﬁrst sample complexity\nresults were obtained by Baum and Haussler (1989), who showed that the number of exam-\nples required for effective learning grows as roughly W log W, where W is the number of\nweights.17 Since then, a much more sophisticated theory has been developed (Anthony and\nBartlett, 1999), including the important result that the representational capacity of a network\ndepends on the size of the weights as well as on their number, a result that should not be\nsurprising in the light of our discussion of regularization.\nThe most popular kind of neural network that we did not cover is the radial basis\nfunction, or RBF, network. A radial basis function combines a weighted collection of kernels\nRADIAL BASIS\nFUNCTION\n(usually Gaussians, of course) to do function approximation. RBF networks can be trained in\ntwo phases: ﬁrst, an unsupervised clustering approach is used to train the parameters of the\nGaussians—the means and variances—are trained, as in Section 20.3.1. In the second phase,\nthe relative weights of the Gaussians are determined. This is a system of linear equations,\nwhich we know how to solve directly. Thus, both phases of RBF training have a nice beneﬁt:\nthe ﬁrst phase is unsupervised, and thus does not require labeled training data, and the second\nphase, although supervised, is efﬁcient. See Bishop (1995) for more details.\nRecurrent networks, in which units are linked in cycles, were mentioned in the chap-\nter but not explored in depth. Hopﬁeld networks (Hopﬁeld, 1982) are probably the best-\nHOPFIELD NETWORK\nunderstood class of recurrent networks. They use bidirectional connections with symmetric\nweights (i.e., wi,j = wj,i), all of the units are both input and output units, the activation\nfunction g is the sign function, and the activation levels can only be ±1. A Hopﬁeld network\nfunctions as an associative memory: after the network trains on a set of examples, a new\nASSOCIATIVE\nMEMORY\nstimulus will cause it to settle into an activation pattern corresponding to the example in the\ntraining set that most closely resembles the new stimulus. For example, if the training set con-\nsists of a set of photographs, and the new stimulus is a small piece of one of the photographs,\nthen the network activation levels will reproduce the photograph from which the piece was",
  "sists of a set of photographs, and the new stimulus is a small piece of one of the photographs,\nthen the network activation levels will reproduce the photograph from which the piece was\ntaken. Notice that the original photographs are not stored separately in the network; each\n17 This approximately conﬁrmed “Uncle Bernie’s rule.” The rule was named after Bernie Widrow, who recom-\nmended using roughly ten times as many examples as weights. Exercises\n763\nweight is a partial encoding of all the photographs. One of the most interesting theoretical\nresults is that Hopﬁeld networks can reliably store up to 0.138N training examples, where N\nis the number of units in the network.\nBoltzmann machines (Hinton and Sejnowski, 1983, 1986) also use symmetric weights,\nBOLTZMANN\nMACHINE\nbut include hidden units. In addition, they use a stochastic activation function, such that\nthe probability of the output being 1 is some function of the total weighted input. Boltz-\nmann machines therefore undergo state transitions that resemble a simulated annealing search\n(see Chapter 4) for the conﬁguration that best approximates the training set. It turns out that\nBoltzmann machines are very closely related to a special case of Bayesian networks evaluated\nwith a stochastic simulation algorithm. (See Section 14.5.)\nFor neural nets, Bishop (1995), Ripley (1996), and Haykin (2008) are the leading texts.\nThe ﬁeld of computational neuroscience is covered by Dayan and Abbott (2001).\nThe approach taken in this chapter was inﬂuenced by the excellent course notes of David\nCohn, Tom Mitchell, Andrew Moore, and Andrew Ng. There are several top-notch textbooks\nin Machine Learning (Mitchell, 1997; Bishop, 2007) and in the closely allied and overlapping\nﬁelds of pattern recognition (Ripley, 1996; Duda et al., 2001), statistics (Wasserman, 2004;\nHastie et al., 2001), data mining (Hand et al., 2001; Witten and Frank, 2005), computational\nlearning theory (Kearns and Vazirani, 1994; Vapnik, 1998) and information theory (Shannon\nand Weaver, 1949; MacKay, 2002; Cover and Thomas, 2006). Other books concentrate on\nimplementations (Segaran, 2007; Marsland, 2009) and comparisons of algorithms (Michie\net al., 1994). Current research in machine learning is published in the annual proceedings\nof the International Conference on Machine Learning (ICML) and the conference on Neural\nInformation Processing Systems (NIPS), in Machine Learning and the Journal of Machine\nLearning Research, and in mainstream AI journals.",
  "of the International Conference on Machine Learning (ICML) and the conference on Neural\nInformation Processing Systems (NIPS), in Machine Learning and the Journal of Machine\nLearning Research, and in mainstream AI journals.\nEXERCISES\n18.1\nConsider the problem faced by an infant learning to speak and understand a language.\nExplain how this process ﬁts into the general learning model. Describe the percepts and\nactions of the infant, and the types of learning the infant must do. Describe the subfunctions\nthe infant is trying to learn in terms of inputs and outputs, and available example data.\n18.2\nRepeat Exercise 18.1 for the case of learning to play tennis (or some other sport with\nwhich you are familiar). Is this supervised learning or reinforcement learning?\n18.3\nSuppose we generate a training set from a decision tree and then apply decision-tree\nlearning to that training set. Is it the case that the learning algorithm will eventually return\nthe correct tree as the training-set size goes to inﬁnity? Why or why not?\n18.4\nIn the recursive construction of decision trees, it sometimes happens that a mixed set\nof positive and negative examples remains at a leaf node, even after all the attributes have\nbeen used. Suppose that we have p positive examples and n negative examples. 764\nChapter\n18.\nLearning from Examples\na. Show that the solution used by DECISION-TREE-LEARNING, which picks the majority\nclassiﬁcation, minimizes the absolute error over the set of examples at the leaf.\nb. Show that the class probability p/(p + n) minimizes the sum of squared errors.\nCLASS PROBABILITY\n18.5\nSuppose that an attribute splits the set of examples E into subsets Ek and that each\nsubset has pk positive examples and nk negative examples. Show that the attribute has strictly\npositive information gain unless the ratio pk/(pk + nk) is the same for all k.\n18.6\nConsider the following data set comprised of three binary input attributes (A1, A2, and\nA3) and one binary output:\nExample A1 A2 A3 Output y\nx1\n1\n0\n0\n0\nx2\n1\n0\n1\n0\nx3\n0\n1\n0\n0\nx4\n1\n1\n1\n1\nx5\n1\n1\n0\n1\nUse the algorithm in Figure 18.5 (page 702) to learn a decision tree for these data. Show the\ncomputations made to determine the attribute to split at each node.\n18.7\nA decision graph is a generalization of a decision tree that allows nodes (i.e., attributes\nused for splits) to have multiple parents, rather than just a single parent. The resulting graph",
  "18.7\nA decision graph is a generalization of a decision tree that allows nodes (i.e., attributes\nused for splits) to have multiple parents, rather than just a single parent. The resulting graph\nmust still be acyclic. Now, consider the XOR function of three binary input attributes, which\nproduces the value 1 if and only if an odd number of the three input attributes has value 1.\na. Draw a minimal-sized decision tree for the three-input XOR function.\nb. Draw a minimal-sized decision graph for the three-input XOR function.\n18.8\nThis exercise considers χ2 pruning of decision trees (Section 18.3.5).\na. Create a data set with two input attributes, such that the information gain at the root of\nthe tree for both attributes is zero, but there is a decision tree of depth 2 that is consistent\nwith all the data. What would χ2 pruning do on this data set if applied bottom up? If\napplied top down?\nb. Modify DECISION-TREE-LEARNING to include χ2-pruning. You might wish to con-\nsult Quinlan (1986) or Kearns and Mansour (1998) for details.\n18.9\nThe standard DECISION-TREE-LEARNING algorithm described in the chapter does\nnot handle cases in which some examples have missing attribute values.\na. First, we need to ﬁnd a way to classify such examples, given a decision tree that includes\ntests on the attributes for which values can be missing. Suppose that an example x has\na missing value for attribute A and that the decision tree tests for A at a node that x\nreaches. One way to handle this case is to pretend that the example has all possible\nvalues for the attribute, but to weight each value according to its frequency among all\nof the examples that reach that node in the decision tree. The classiﬁcation algorithm\nshould follow all branches at any node for which a value is missing and should multiply Exercises\n765\nthe weights along each path. Write a modiﬁed classiﬁcation algorithm for decision trees\nthat has this behavior.\nb. Now modify the information-gain calculation so that in any given collection of exam-\nples C at a given node in the tree during the construction process, the examples with\nmissing values for any of the remaining attributes are given “as-if” values according to\nthe frequencies of those values in the set C.\n18.10\nIn Section 18.3.6, we noted that attributes with many different possible values can\ncause problems with the gain measure. Such attributes tend to split the examples into numer-",
  "the frequencies of those values in the set C.\n18.10\nIn Section 18.3.6, we noted that attributes with many different possible values can\ncause problems with the gain measure. Such attributes tend to split the examples into numer-\nous small classes or even singleton classes, thereby appearing to be highly relevant according\nto the gain measure. The gain-ratio criterion selects attributes according to the ratio between\ntheir gain and their intrinsic information content—that is, the amount of information con-\ntained in the answer to the question, “What is the value of this attribute?” The gain-ratio crite-\nrion therefore tries to measure how efﬁciently an attribute provides information on the correct\nclassiﬁcation of an example. Write a mathematical expression for the information content of\nan attribute, and implement the gain ratio criterion in DECISION-TREE-LEARNING.\n18.11\nSuppose you are running a learning experiment on a new algorithm for Boolean clas-\nsiﬁcation. You have a data set consisting of 100 positive and 100 negative examples. You\nplan to use leave-one-out cross-validation and compare your algorithm to a baseline function,\na simple majority classiﬁer. (A majority classiﬁer is given a set of training data and then\nalways outputs the class that is in the majority in the training set, regardless of the input.)\nYou expect the majority classiﬁer to score about 50% on leave-one-out cross-validation, but\nto your surprise, it scores zero every time. Can you explain why?\n18.12\nConstruct a decision list to classify the data below. Select tests to be as small as\npossible (in terms of attributes), breaking ties among tests with the same number of attributes\nby selecting the one that classiﬁes the greatest number of examples correctly. If multiple tests\nhave the same number of attributes and classify the same number of examples, then break the\ntie using attributes with lower index numbers (e.g., select A1 over A2).\nExample\nA1\nA2\nA3\nA4\ny\nx1\n1\n0\n0\n0\n1\nx2\n1\n0\n1\n1\n1\nx3\n0\n1\n0\n0\n1\nx4\n0\n1\n1\n0\n0\nx5\n1\n1\n0\n1\n1\nx6\n0\n1\n0\n1\n0\nx7\n0\n0\n1\n1\n1\nx8\n0\n0\n1\n0\n0\n18.13\nProve that a decision list can represent the same function as a decision tree while\nusing at most as many rules as there are leaves in the decision tree for that function. Give an\nexample of a function represented by a decision list using strictly fewer rules than the number\nof leaves in a minimal-sized decision tree for that same function. 766\nChapter\n18.\nLearning from Examples\n18.14",
  "example of a function represented by a decision list using strictly fewer rules than the number\nof leaves in a minimal-sized decision tree for that same function. 766\nChapter\n18.\nLearning from Examples\n18.14\nThis exercise concerns the expressiveness of decision lists (Section 18.5).\na. Show that decision lists can represent any Boolean function, if the size of the tests is\nnot limited.\nb. Show that if the tests can contain at most k literals each, then decision lists can represent\nany function that can be represented by a decision tree of depth k.\n18.15\nSuppose a 7-nearest-neighbors regression search returns {7, 6, 8, 4, 7, 11, 100} as the\n7 nearest y values for a given x value. What is the value of ˆy that minimizes the L1 loss\nfunction on this data? There is a common name in statistics for this value as a function of the\ny values; what is it? Answer the same two questions for the L2 loss function.\n18.16\nFigure 18.31 showed how a circle at the origin can be linearly separated by mapping\nfrom the features (x1, x2) to the two dimensions (x2\n1, x2\n2). But what if the circle is not located\nat the origin? What if it is an ellipse, not a circle? The general equation for a circle (and\nhence the decision boundary) is (x1 −a)2 + (x2 −b)2 −r2 = 0, and the general equation for\nan ellipse is c(x1 −a)2 + d(x2 −b)2 −1 = 0.\na. Expand out the equation for the circle and show what the weights wi would be for the\ndecision boundary in the four-dimensional feature space (x1, x2, x2\n1, x2\n2). Explain why\nthis means that any circle is linearly separable in this space.\nb. Do the same for ellipses in the ﬁve-dimensional feature space (x1, x2, x2\n1, x2\n2, x1x2).\n18.17\nConstruct a support vector machine that computes the XOR function. Use values of\n+1 and –1 (instead of 1 and 0) for both inputs and outputs, so that an example looks like\n([−1, 1], 1) or ([−1, −1], −1). Map the input [x1, x2] into a space consisting of x1 and x1 x2.\nDraw the four input points in this space, and the maximal margin separator. What is the\nmargin? Now draw the separating line back in the original Euclidean input space.\n18.18\nConsider an ensemble learning algorithm that uses simple majority voting among\nK learned hypotheses. Suppose that each hypothesis has error ϵ and that the errors made\nby each hypothesis are independent of the others’. Calculate a formula for the error of the\nensemble algorithm in terms of K and ϵ, and evaluate it for the cases where K = 5, 10, and",
  "by each hypothesis are independent of the others’. Calculate a formula for the error of the\nensemble algorithm in terms of K and ϵ, and evaluate it for the cases where K = 5, 10, and\n20 and ϵ = 0.1, 0.2, and 0.4. If the independence assumption is removed, is it possible for the\nensemble error to be worse than ϵ?\n18.19\nConstruct by hand a neural network that computes the XOR function of two inputs.\nMake sure to specify what sort of units you are using.\n18.20\nRecall from Chapter 18 that there are 22n distinct Boolean functions of n inputs. How\nmany of these are representable by a threshold perceptron?\n18.21\nSection 18.6.4 (page 725) noted that the output of the logistic function could be in-\nterpreted as a probability p assigned by the model to the proposition that f(x) = 1; the prob-\nability that f(x) = 0 is therefore 1 −p. Write down the probability p as a function of x\nand calculate the derivative of log p with respect to each weight wi. Repeat the process for\nlog(1−p). These calculations give a learning rule for minimizing the negative-log-likelihood Exercises\n767\nloss function for a probabilistic hypothesis. Comment on any resemblance to other learning\nrules in the chapter.\n18.22\nSuppose you had a neural network with linear activation functions. That is, for each\nunit the output is some constant c times the weighted sum of the inputs.\na. Assume that the network has one hidden layer. For a given assignment to the weights\nw, write down equations for the value of the units in the output layer as a function of\nw and the input layer x, without any explicit mention of the output of the hidden layer.\nShow that there is a network with no hidden units that computes the same function.\nb. Repeat the calculation in part (a), but this time do it for a network with any number of\nhidden layers.\nc. Suppose a network with one hidden layer and linear activation functions has n input\nand output nodes and h hidden nodes. What effect does the transformation in part (a)\nto a network with no hidden layers have on the total number of weights? Discuss in\nparticular the case h ≪n.\n18.23\nSuppose that a training set contains only a single example, repeated 100 times. In\n80 of the 100 cases, the single output value is 1; in the other 20, it is 0. What will a back-\npropagation network predict for this example, assuming that it has been trained and reaches\na global optimum? (Hint: to ﬁnd the global optimum, differentiate the error function and set\nit to zero.)\n18.24",
  "propagation network predict for this example, assuming that it has been trained and reaches\na global optimum? (Hint: to ﬁnd the global optimum, differentiate the error function and set\nit to zero.)\n18.24\nThe neural network whose learning performance is measured in Figure 18.25 has four\nhidden nodes. This number was chosen somewhat arbitrarily. Use a cross-validation method\nto ﬁnd the best number of hidden nodes.\n18.25\nConsider the problem of separating N data points into positive and negative examples\nusing a linear separator. Clearly, this can always be done for N = 2 points on a line of\ndimension d = 1, regardless of how the points are labeled or where they are located (unless\nthe points are in the same place).\na. Show that it can always be done for N = 3 points on a plane of dimension d = 2, unless\nthey are collinear.\nb. Show that it cannot always be done for N = 4 points on a plane of dimension d = 2.\nc. Show that it can always be done for N = 4 points in a space of dimension d = 3, unless\nthey are coplanar.\nd. Show that it cannot always be done for N = 5 points in a space of dimension d = 3.\ne. The ambitious student may wish to prove that N points in general position (but not\nN + 1) are linearly separable in a space of dimension N −1. 19\nKNOWLEDGE IN\nLEARNING\nIn which we examine the problem of learning when you know something already.\nIn all of the approaches to learning described in the previous chapter, the idea is to construct\na function that has the input–output behavior observed in the data. In each case, the learning\nmethods can be understood as searching a hypothesis space to ﬁnd a suitable function, starting\nfrom only a very basic assumption about the form of the function, such as “second-degree\npolynomial” or “decision tree” and perhaps a preference for simpler hypotheses. Doing this\namounts to saying that before you can learn something new, you must ﬁrst forget (almost)\neverything you know. In this chapter, we study learning methods that can take advantage\nof prior knowledge about the world. In most cases, the prior knowledge is represented\nPRIOR KNOWLEDGE\nas general ﬁrst-order logical theories; thus for the ﬁrst time we bring together the work on\nknowledge representation and learning.\n19.1\nA LOGICAL FORMULATION OF LEARNING\nChapter 18 deﬁned pure inductive learning as a process of ﬁnding a hypothesis that agrees\nwith the observed examples. Here, we specialize this deﬁnition to the case where the hypoth-",
  "19.1\nA LOGICAL FORMULATION OF LEARNING\nChapter 18 deﬁned pure inductive learning as a process of ﬁnding a hypothesis that agrees\nwith the observed examples. Here, we specialize this deﬁnition to the case where the hypoth-\nesis is represented by a set of logical sentences. Example descriptions and classiﬁcations will\nalso be logical sentences, and a new example can be classiﬁed by inferring a classiﬁcation\nsentence from the hypothesis and the example description. This approach allows for incre-\nmental construction of hypotheses, one sentence at a time. It also allows for prior knowledge,\nbecause sentences that are already known can assist in the classiﬁcation of new examples.\nThe logical formulation of learning may seem like a lot of extra work at ﬁrst, but it turns out\nto clarify many of the issues in learning. It enables us to go well beyond the simple learning\nmethods of Chapter 18 by using the full power of logical inference in the service of learning.\n19.1.1\nExamples and hypotheses\nRecall from Chapter 18 the restaurant learning problem: learning a rule for deciding whether\nto wait for a table. Examples were described by attributes such as Alternate, Bar, Fri/Sat,\n768 Section 19.1.\nA Logical Formulation of Learning\n769\nand so on. In a logical setting, an example is described by a logical sentence; the attributes\nbecome unary predicates. Let us generically call the ith example Xi. For instance, the ﬁrst\nexample from Figure 18.3 (page 700) is described by the sentences\nAlternate(X1) ∧¬Bar(X1) ∧¬Fri/Sat(X1) ∧Hungry(X1) ∧. . .\nWe will use the notation Di(Xi) to refer to the description of Xi, where Di can be any logical\nexpression taking a single argument. The classiﬁcation of the example is given by a literal\nusing the goal predicate, in this case\nWillWait(X1)\nor\n¬WillWait(X1) .\nThe complete training set can thus be expressed as the conjunction of all the example descrip-\ntions and goal literals.\nThe aim of inductive learning in general is to ﬁnd a hypothesis that classiﬁes the ex-\namples well and generalizes well to new examples. Here we are concerned with hypotheses\nexpressed in logic; each hypothesis hj will have the form\n∀x Goal(x) ⇔Cj(x) ,\nwhere Cj(x) is a candidate deﬁnition—some expression involving the attribute predicates.\nFor example, a decision tree can be interpreted as a logical expression of this form. Thus, the\ntree in Figure 18.6 (page 702) expresses the following logical deﬁnition (which we will call\nhr for future reference):",
  "For example, a decision tree can be interpreted as a logical expression of this form. Thus, the\ntree in Figure 18.6 (page 702) expresses the following logical deﬁnition (which we will call\nhr for future reference):\n∀r WillWait(r) ⇔Patrons(r, Some)\n∨Patrons(r, Full) ∧Hungry(r) ∧Type(r, French)\n∨Patrons(r, Full) ∧Hungry(r) ∧Type(r, Thai)\n∧Fri/Sat(r)\n∨Patrons(r, Full) ∧Hungry(r) ∧Type(r, Burger) .\n(19.1)\nEach hypothesis predicts that a certain set of examples—namely, those that satisfy its candi-\ndate deﬁnition—will be examples of the goal predicate. This set is called the extension of\nEXTENSION\nthe predicate. Two hypotheses with different extensions are therefore logically inconsistent\nwith each other, because they disagree on their predictions for at least one example. If they\nhave the same extension, they are logically equivalent.\nThe hypothesis space H is the set of all hypotheses {h1, . . . , hn} that the learning algo-\nrithm is designed to entertain. For example, the DECISION-TREE-LEARNING algorithm can\nentertain any decision tree hypothesis deﬁned in terms of the attributes provided; its hypoth-\nesis space therefore consists of all these decision trees. Presumably, the learning algorithm\nbelieves that one of the hypotheses is correct; that is, it believes the sentence\nh1 ∨h2 ∨h3 ∨. . . ∨hn .\n(19.2)\nAs the examples arrive, hypotheses that are not consistent with the examples can be ruled\nout. Let us examine this notion of consistency more carefully. Obviously, if hypothesis hj is\nconsistent with the entire training set, it has to be consistent with each example in the training\nset. What would it mean for it to be inconsistent with an example? There are two possible\nways that this can happen: 770\nChapter\n19.\nKnowledge in Learning\n• An example can be a false negative for the hypothesis, if the hypothesis says it should\nFALSE NEGATIVE\nbe negative but in fact it is positive. For instance, the new example X13 described by\nPatrons(X13, Full) ∧¬Hungry(X13) ∧. . . ∧WillWait(X13)\nwould be a false negative for the hypothesis hr given earlier. From hr and the example\ndescription, we can deduce both WillWait(X13), which is what the example says,\nand ¬WillWait(X13), which is what the hypothesis predicts. The hypothesis and the\nexample are therefore logically inconsistent.\n• An example can be a false positive for the hypothesis, if the hypothesis says it should\nFALSE POSITIVE\nbe positive but in fact it is negative.1",
  "example are therefore logically inconsistent.\n• An example can be a false positive for the hypothesis, if the hypothesis says it should\nFALSE POSITIVE\nbe positive but in fact it is negative.1\nIf an example is a false positive or false negative for a hypothesis, then the example and the\nhypothesis are logically inconsistent with each other. Assuming that the example is a correct\nobservation of fact, then the hypothesis can be ruled out. Logically, this is exactly analogous\nto the resolution rule of inference (see Chapter 9), where the disjunction of hypotheses cor-\nresponds to a clause and the example corresponds to a literal that resolves against one of the\nliterals in the clause. An ordinary logical inference system therefore could, in principle, learn\nfrom the example by eliminating one or more hypotheses. Suppose, for example, that the\nexample is denoted by the sentence I1, and the hypothesis space is h1 ∨h2 ∨h3 ∨h4. Then if\nI1 is inconsistent with h2 and h3, the logical inference system can deduce the new hypothesis\nspace h1 ∨h4.\nWe therefore can characterize inductive learning in a logical setting as a process of\ngradually eliminating hypotheses that are inconsistent with the examples, narrowing down\nthe possibilities. Because the hypothesis space is usually vast (or even inﬁnite in the case of\nﬁrst-order logic), we do not recommend trying to build a learning system using resolution-\nbased theorem proving and a complete enumeration of the hypothesis space. Instead, we will\ndescribe two approaches that ﬁnd logically consistent hypotheses with much less effort.\n19.1.2\nCurrent-best-hypothesis search\nThe idea behind current-best-hypothesis search is to maintain a single hypothesis, and to\nCURRENT-BEST-\nHYPOTHESIS\nadjust it as new examples arrive in order to maintain consistency. The basic algorithm was\ndescribed by John Stuart Mill (1843), and may well have appeared even earlier.\nSuppose we have some hypothesis such as hr, of which we have grown quite fond.\nAs long as each new example is consistent, we need do nothing. Then along comes a false\nnegative example, X13. What do we do? Figure 19.1(a) shows hr schematically as a region:\neverything inside the rectangle is part of the extension of hr. The examples that have actually\nbeen seen so far are shown as “+” or “–”, and we see that hr correctly categorizes all the\nexamples as positive or negative examples of WillWait. In Figure 19.1(b), a new example",
  "been seen so far are shown as “+” or “–”, and we see that hr correctly categorizes all the\nexamples as positive or negative examples of WillWait. In Figure 19.1(b), a new example\n(circled) is a false negative: the hypothesis says it should be negative but it is actually positive.\nThe extension of the hypothesis must be increased to include it. This is called generalization;\nGENERALIZATION\none possible generalization is shown in Figure 19.1(c). Then in Figure 19.1(d), we see a false\npositive: the hypothesis says the new example (circled) should be positive, but it actually is\n1 The terms “false positive” and “false negative” are used in medicine to describe erroneous results from lab\ntests. A result is a false positive if it indicates that the patient has the disease when in fact no disease is present. Section 19.1.\nA Logical Formulation of Learning\n771\n(a)\n(b)\n(c)\n(d)\n(e)\n+\n+\n+\n+\n+\n++\n–\n–\n–\n–\n–\n–\n–\n– –\n–\n+\n+\n+\n+\n+\n++\n–\n–\n–\n–\n–\n–\n–\n– –\n–\n+\n+\n+\n+\n+\n+\n++\n–\n–\n–\n–\n–\n–\n–\n– –\n–\n+\n+\n+\n+\n+\n+\n++\n–\n–\n–\n–\n–\n–\n–\n–\n–\n+\n–\n+\n+\n+\n+\n+\n++\n–\n–\n–\n–\n–\n–\n–\n–\n–\n+\n–\n–\nFigure 19.1\n(a) A consistent hypothesis. (b) A false negative. (c) The hypothesis is gen-\neralized. (d) A false positive. (e) The hypothesis is specialized.\nfunction CURRENT-BEST-LEARNING(examples,h) returns a hypothesis or fail\nif examples is empty then\nreturn h\ne ←FIRST(examples)\nif e is consistent with h then\nreturn CURRENT-BEST-LEARNING(REST(examples), h)\nelse if e is a false positive for h then\nfor each h′ in specializations of h consistent with examples seen so far do\nh′′ ←CURRENT-BEST-LEARNING(REST(examples), h′)\nif h′′ ̸= fail then return h′′\nelse if e is a false negative for h then\nfor each h′ in generalizations of h consistent with examples seen so far do\nh′′ ←CURRENT-BEST-LEARNING(REST(examples), h′)\nif h′′ ̸= fail then return h′′\nreturn fail\nFigure 19.2\nThe current-best-hypothesis learning algorithm. It searches for a consis-\ntent hypothesis that ﬁts all the examples and backtracks when no consistent specializa-\ntion/generalization can be found. To start the algorithm, any hypothesis can be passed in;\nit will be specialized or gneralized as needed.\nnegative. The extension of the hypothesis must be decreased to exclude the example. This is\ncalled specialization; in Figure 19.1(e) we see one possible specialization of the hypothesis.\nSPECIALIZATION\nThe “more general than” and “more speciﬁc than” relations between hypotheses provide the",
  "called specialization; in Figure 19.1(e) we see one possible specialization of the hypothesis.\nSPECIALIZATION\nThe “more general than” and “more speciﬁc than” relations between hypotheses provide the\nlogical structure on the hypothesis space that makes efﬁcient search possible.\nWe can now specify the CURRENT-BEST-LEARNING algorithm, shown in Figure 19.2.\nNotice that each time we consider generalizing or specializing the hypothesis, we must check\nfor consistency with the other examples, because an arbitrary increase/decrease in the exten-\nsion might include/exclude previously seen negative/positive examples. 772\nChapter\n19.\nKnowledge in Learning\nWe have deﬁned generalization and specialization as operations that change the exten-\nsion of a hypothesis. Now we need to determine exactly how they can be implemented as\nsyntactic operations that change the candidate deﬁnition associated with the hypothesis, so\nthat a program can carry them out. This is done by ﬁrst noting that generalization and special-\nization are also logical relationships between hypotheses. If hypothesis h1, with deﬁnition\nC1, is a generalization of hypothesis h2 with deﬁnition C2, then we must have\n∀x C2(x) ⇒C1(x) .\nTherefore in order to construct a generalization of h2, we simply need to ﬁnd a deﬁni-\ntion C1 that is logically implied by C2.\nThis is easily done.\nFor example, if C2(x) is\nAlternate(x) ∧Patrons(x, Some), then one possible generalization is given by C1(x) ≡\nPatrons(x, Some). This is called dropping conditions. Intuitively, it generates a weaker\nDROPPING\nCONDITIONS\ndeﬁnition and therefore allows a larger set of positive examples. There are a number of other\ngeneralization operations, depending on the language being operated on. Similarly, we can\nspecialize a hypothesis by adding extra conditions to its candidate deﬁnition or by removing\ndisjuncts from a disjunctive deﬁnition. Let us see how this works on the restaurant example,\nusing the data in Figure 18.3.\n• The ﬁrst example, X1, is positive. The attribute Alternate(X1) is true, so let the initial\nhypothesis be\nh1 :\n∀x WillWait(x) ⇔Alternate(x) .\n• The second example, X2, is negative. h1 predicts it to be positive, so it is a false positive.\nTherefore, we need to specialize h1. This can be done by adding an extra condition that\nwill rule out X2, while continuing to classify X1 as positive. One possibility is\nh2 :\n∀x WillWait(x) ⇔Alternate(x) ∧Patrons(x, Some) .",
  "Therefore, we need to specialize h1. This can be done by adding an extra condition that\nwill rule out X2, while continuing to classify X1 as positive. One possibility is\nh2 :\n∀x WillWait(x) ⇔Alternate(x) ∧Patrons(x, Some) .\n• The third example, X3, is positive. h2 predicts it to be negative, so it is a false negative.\nTherefore, we need to generalize h2. We drop the Alternate condition, yielding\nh3 :\n∀x WillWait(x) ⇔Patrons(x, Some) .\n• The fourth example, X4, is positive. h3 predicts it to be negative, so it is a false negative.\nWe therefore need to generalize h3. We cannot drop the Patrons condition, because\nthat would yield an all-inclusive hypothesis that would be inconsistent with X2. One\npossibility is to add a disjunct:\nh4 :\n∀x WillWait(x) ⇔Patrons(x, Some)\n∨(Patrons(x, Full) ∧Fri/Sat(x)) .\nAlready, the hypothesis is starting to look reasonable. Obviously, there are other possibilities\nconsistent with the ﬁrst four examples; here are two of them:\nh′\n4 :\n∀x WillWait(x) ⇔¬WaitEstimate(x, 30-60) .\nh′′\n4 :\n∀x WillWait(x) ⇔Patrons(x, Some)\n∨(Patrons(x, Full) ∧WaitEstimate(x, 10-30)) .\nThe CURRENT-BEST-LEARNING algorithm is described nondeterministically, because at any\npoint, there may be several possible specializations or generalizations that can be applied. The Section 19.1.\nA Logical Formulation of Learning\n773\nfunction VERSION-SPACE-LEARNING(examples) returns a version space\nlocal variables: V , the version space: the set of all hypotheses\nV ←the set of all hypotheses\nfor each example e in examples do\nif V is not empty then V ←VERSION-SPACE-UPDATE(V ,e)\nreturn V\nfunction VERSION-SPACE-UPDATE(V ,e) returns an updated version space\nV ←{h ∈V : h is consistent with e}\nFigure 19.3\nThe version space learning algorithm. It ﬁnds a subset of V that is consistent\nwith all the examples.\nchoices that are made will not necessarily lead to the simplest hypothesis, and may lead to an\nunrecoverable situation where no simple modiﬁcation of the hypothesis is consistent with all\nof the data. In such cases, the program must backtrack to a previous choice point.\nThe CURRENT-BEST-LEARNING algorithm and its variants have been used in many\nmachine learning systems, starting with Patrick Winston’s (1970) “arch-learning” program.\nWith a large number of examples and a large space, however, some difﬁculties arise:\n1. Checking all the previous examples over again for each modiﬁcation is very expensive.",
  "With a large number of examples and a large space, however, some difﬁculties arise:\n1. Checking all the previous examples over again for each modiﬁcation is very expensive.\n2. The search process may involve a great deal of backtracking. As we saw in Chapter 18,\nhypothesis space can be a doubly exponentially large place.\n19.1.3\nLeast-commitment search\nBacktracking arises because the current-best-hypothesis approach has to choose a particular\nhypothesis as its best guess even though it does not have enough data yet to be sure of the\nchoice. What we can do instead is to keep around all and only those hypotheses that are\nconsistent with all the data so far. Each new example will either have no effect or will get\nrid of some of the hypotheses. Recall that the original hypothesis space can be viewed as a\ndisjunctive sentence\nh1 ∨h2 ∨h3 . . . ∨hn .\nAs various hypotheses are found to be inconsistent with the examples, this disjunction shrinks,\nretaining only those hypotheses not ruled out. Assuming that the original hypothesis space\ndoes in fact contain the right answer, the reduced disjunction must still contain the right an-\nswer because only incorrect hypotheses have been removed. The set of hypotheses remaining\nis called the version space, and the learning algorithm (sketched in Figure 19.3) is called the\nVERSION SPACE\nversion space learning algorithm (also the candidate elimination algorithm).\nCANDIDATE\nELIMINATION\nOne important property of this approach is that it is incremental: one never has to\ngo back and reexamine the old examples. All remaining hypotheses are guaranteed to be\nconsistent with them already. But there is an obvious problem. We already said that the 774\nChapter\n19.\nKnowledge in Learning\nThis region all inconsistent\nThis region all inconsistent\nMore general\nMore specific\nS1\nG1\nS2\nG2\nG3\n . . . \nGm\n . . .\nSn\nFigure 19.4\nThe version space contains all hypotheses consistent with the examples.\nhypothesis space is enormous, so how can we possibly write down this enormous disjunction?\nThe following simple analogy is very helpful. How do you represent all the real num-\nbers between 1 and 2? After all, there are an inﬁnite number of them! The answer is to use\nan interval representation that just speciﬁes the boundaries of the set: [1,2]. It works because\nwe have an ordering on the real numbers.\nWe also have an ordering on the hypothesis space, namely, generalization/specialization.",
  "an interval representation that just speciﬁes the boundaries of the set: [1,2]. It works because\nwe have an ordering on the real numbers.\nWe also have an ordering on the hypothesis space, namely, generalization/specialization.\nThis is a partial ordering, which means that each boundary will not be a point but rather a\nset of hypotheses called a boundary set. The great thing is that we can represent the entire\nBOUNDARY SET\nversion space using just two boundary sets: a most general boundary (the G-set) and a most\nG-SET\nspeciﬁc boundary (the S-set). Everything in between is guaranteed to be consistent with the\nS-SET\nexamples. Before we prove this, let us recap:\n• The current version space is the set of hypotheses consistent with all the examples so\nfar. It is represented by the S-set and G-set, each of which is a set of hypotheses.\n• Every member of the S-set is consistent with all observations so far, and there are no\nconsistent hypotheses that are more speciﬁc.\n• Every member of the G-set is consistent with all observations so far, and there are no\nconsistent hypotheses that are more general.\nWe want the initial version space (before any examples have been seen) to represent all possi-\nble hypotheses. We do this by setting the G-set to contain True (the hypothesis that contains\neverything), and the S-set to contain False (the hypothesis whose extension is empty).\nFigure 19.4 shows the general structure of the boundary-set representation of the version\nspace. To show that the representation is sufﬁcient, we need the following two properties: Section 19.1.\nA Logical Formulation of Learning\n775\n1. Every consistent hypothesis (other than those in the boundary sets) is more speciﬁc than\nsome member of the G-set, and more general than some member of the S-set. (That is,\nthere are no “stragglers” left outside.) This follows directly from the deﬁnitions of S\nand G. If there were a straggler h, then it would have to be no more speciﬁc than any\nmember of G, in which case it belongs in G; or no more general than any member of\nS, in which case it belongs in S.\n2. Every hypothesis more speciﬁc than some member of the G-set and more general than\nsome member of the S-set is a consistent hypothesis. (That is, there are no “holes” be-\ntween the boundaries.) Any h between S and G must reject all the negative examples\nrejected by each member of G (because it is more speciﬁc), and must accept all the pos-",
  "tween the boundaries.) Any h between S and G must reject all the negative examples\nrejected by each member of G (because it is more speciﬁc), and must accept all the pos-\nitive examples accepted by any member of S (because it is more general). Thus, h must\nagree with all the examples, and therefore cannot be inconsistent. Figure 19.5 shows\nthe situation: there are no known examples outside S but inside G, so any hypothesis\nin the gap must be consistent.\nWe have therefore shown that if S and G are maintained according to their deﬁnitions, then\nthey provide a satisfactory representation of the version space. The only remaining problem\nis how to update S and G for a new example (the job of the VERSION-SPACE-UPDATE\nfunction). This may appear rather complicated at ﬁrst, but from the deﬁnitions and with the\nhelp of Figure 19.4, it is not too hard to reconstruct the algorithm.\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\nS1\nG1\nG2\nFigure 19.5\nThe extensions of the members of G and S. No known examples lie in\nbetween the two sets of boundaries.\nWe need to worry about the members Si and Gi of the S- and G-sets. For each one, the\nnew example may be a false positive or a false negative.\n1. False positive for Si: This means Si is too general, but there are no consistent special-\nizations of Si (by deﬁnition), so we throw it out of the S-set.\n2. False negative for Si: This means Si is too speciﬁc, so we replace it by all its immediate\ngeneralizations, provided they are more speciﬁc than some member of G.\n3. False positive for Gi: This means Gi is too general, so we replace it by all its immediate\nspecializations, provided they are more general than some member of S. 776\nChapter\n19.\nKnowledge in Learning\n4. False negative for Gi: This means Gi is too speciﬁc, but there are no consistent gener-\nalizations of Gi (by deﬁnition) so we throw it out of the G-set.\nWe continue these operations for each new example until one of three things happens:\n1. We have exactly one hypothesis left in the version space, in which case we return it as\nthe unique hypothesis.\n2. The version space collapses—either S or G becomes empty, indicating that there are\nno consistent hypotheses for the training set. This is the same case as the failure of the\nsimple version of the decision tree algorithm.\n3. We run out of examples and have several hypotheses remaining in the version space.\nThis means the version space represents a disjunction of hypotheses. For any new",
  "simple version of the decision tree algorithm.\n3. We run out of examples and have several hypotheses remaining in the version space.\nThis means the version space represents a disjunction of hypotheses. For any new\nexample, if all the disjuncts agree, then we can return their classiﬁcation of the example.\nIf they disagree, one possibility is to take the majority vote.\nWe leave as an exercise the application of the VERSION-SPACE-LEARNING algorithm to the\nrestaurant data.\nThere are two principal drawbacks to the version-space approach:\n• If the domain contains noise or insufﬁcient attributes for exact classiﬁcation, the version\nspace will always collapse.\n• If we allow unlimited disjunction in the hypothesis space, the S-set will always contain\na single most-speciﬁc hypothesis, namely, the disjunction of the descriptions of the\npositive examples seen to date. Similarly, the G-set will contain just the negation of the\ndisjunction of the descriptions of the negative examples.\n• For some hypothesis spaces, the number of elements in the S-set or G-set may grow\nexponentially in the number of attributes, even though efﬁcient learning algorithms exist\nfor those hypothesis spaces.\nTo date, no completely successful solution has been found for the problem of noise. The\nproblem of disjunction can be addressed by allowing only limited forms of disjunction or by\nincluding a generalization hierarchy of more general predicates. For example, instead of\nGENERALIZATION\nHIERARCHY\nusing the disjunction WaitEstimate(x, 30-60) ∨WaitEstimate(x, >60), we might use the\nsingle literal LongWait(x). The set of generalization and specialization operations can be\neasily extended to handle this.\nThe pure version space algorithm was ﬁrst applied in the Meta-DENDRAL system,\nwhich was designed to learn rules for predicting how molecules would break into pieces in\na mass spectrometer (Buchanan and Mitchell, 1978). Meta-DENDRAL was able to generate\nrules that were sufﬁciently novel to warrant publication in a journal of analytical chemistry—\nthe ﬁrst real scientiﬁc knowledge generated by a computer program. It was also used in the\nelegant LEX system (Mitchell et al., 1983), which was able to learn to solve symbolic integra-\ntion problems by studying its own successes and failures. Although version space methods\nare probably not practical in most real-world learning problems, mainly because of noise,",
  "tion problems by studying its own successes and failures. Although version space methods\nare probably not practical in most real-world learning problems, mainly because of noise,\nthey provide a good deal of insight into the logical structure of hypothesis space. Section 19.2.\nKnowledge in Learning\n777\nObservations\nPredictions\nHypotheses\nPrior \nknowledge\nKnowledge-based\ninductive learning\nFigure 19.6\nA cumulative learning process uses, and adds to, its stock of background\nknowledge over time.\n19.2\nKNOWLEDGE IN LEARNING\nThe preceding section described the simplest setting for inductive learning. To understand the\nrole of prior knowledge, we need to talk about the logical relationships among hypotheses,\nexample descriptions, and classiﬁcations. Let Descriptions denote the conjunction of all the\nexample descriptions in the training set, and let Classiﬁcations denote the conjunction of all\nthe example classiﬁcations. Then a Hypothesis that “explains the observations” must satisfy\nthe following property (recall that |= means “logically entails”):\nHypothesis ∧Descriptions |= Classiﬁcations .\n(19.3)\nWe call this kind of relationship an entailment constraint, in which Hypothesis is the “un-\nENTAILMENT\nCONSTRAINT\nknown.” Pure inductive learning means solving this constraint, where Hypothesis is drawn\nfrom some predeﬁned hypothesis space. For example, if we consider a decision tree as a\nlogical formula (see Equation (19.1) on page 769), then a decision tree that is consistent with\nall the examples will satisfy Equation (19.3). If we place no restrictions on the logical form\nof the hypothesis, of course, then Hypothesis = Classiﬁcations also satisﬁes the constraint.\nOckham’s razor tells us to prefer small, consistent hypotheses, so we try to do better than\nsimply memorizing the examples.\nThis simple knowledge-free picture of inductive learning persisted until the early 1980s.\nThe modern approach is to design agents that already know something and are trying to learn\nsome more. This may not sound like a terriﬁcally deep insight, but it makes quite a difference\nto the way we design agents. It might also have some relevance to our theories about how\nscience itself works. The general idea is shown schematically in Figure 19.6.\nAn autonomous learning agent that uses background knowledge must somehow obtain\nthe background knowledge in the ﬁrst place, in order for it to be used in the new learning",
  "science itself works. The general idea is shown schematically in Figure 19.6.\nAn autonomous learning agent that uses background knowledge must somehow obtain\nthe background knowledge in the ﬁrst place, in order for it to be used in the new learning\nepisodes. This method must itself be a learning process. The agent’s life history will there-\nfore be characterized by cumulative, or incremental, development. Presumably, the agent\ncould start out with nothing, performing inductions in vacuo like a good little pure induc-\ntion program. But once it has eaten from the Tree of Knowledge, it can no longer pursue\nsuch naive speculations and should use its background knowledge to learn more and more\neffectively. The question is then how to actually do this. 778\nChapter\n19.\nKnowledge in Learning\n19.2.1\nSome simple examples\nLet us consider some commonsense examples of learning with background knowledge. Many\napparently rational cases of inferential behavior in the face of observations clearly do not\nfollow the simple principles of pure induction.\n• Sometimes one leaps to general conclusions after only one observation. Gary Larson\nonce drew a cartoon in which a bespectacled caveman, Zog, is roasting his lizard on\nthe end of a pointed stick. He is watched by an amazed crowd of his less intellectual\ncontemporaries, who have been using their bare hands to hold their victuals over the ﬁre.\nThis enlightening experience is enough to convince the watchers of a general principle\nof painless cooking.\n• Or consider the case of the traveler to Brazil meeting her ﬁrst Brazilian. On hearing him\nspeak Portuguese, she immediately concludes that Brazilians speak Portuguese, yet on\ndiscovering that his name is Fernando, she does not conclude that all Brazilians are\ncalled Fernando. Similar examples appear in science. For example, when a freshman\nphysics student measures the density and conductance of a sample of copper at a par-\nticular temperature, she is quite conﬁdent in generalizing those values to all pieces of\ncopper. Yet when she measures its mass, she does not even consider the hypothesis that\nall pieces of copper have that mass. On the other hand, it would be quite reasonable to\nmake such a generalization over all pennies.\n• Finally, consider the case of a pharmacologically ignorant but diagnostically sophisti-\ncated medical student observing a consulting session between a patient and an expert",
  "make such a generalization over all pennies.\n• Finally, consider the case of a pharmacologically ignorant but diagnostically sophisti-\ncated medical student observing a consulting session between a patient and an expert\ninternist. After a series of questions and answers, the expert tells the patient to take a\ncourse of a particular antibiotic. The medical student infers the general rule that that\nparticular antibiotic is effective for a particular type of infection.\nThese are all cases in which the use of background knowledge allows much faster learning\nthan one might expect from a pure induction program.\n19.2.2\nSome general schemes\nIn each of the preceding examples, one can appeal to prior knowledge to try to justify the\ngeneralizations chosen. We will now look at what kinds of entailment constraints are operat-\ning in each case. The constraints will involve the Background knowledge, in addition to the\nHypothesis and the observed Descriptions and Classiﬁcations.\nIn the case of lizard toasting, the cavemen generalize by explaining the success of the\npointed stick: it supports the lizard while keeping the hand away from the ﬁre. From this\nexplanation, they can infer a general rule: that any long, rigid, sharp object can be used to toast\nsmall, soft-bodied edibles. This kind of generalization process has been called explanation-\nbased learning, or EBL. Notice that the general rule follows logically from the background\nEXPLANATION-\nBASED\nLEARNING\nknowledge possessed by the cavemen. Hence, the entailment constraints satisﬁed by EBL are\nthe following:\nHypothesis ∧Descriptions |= Classiﬁcations\nBackground |= Hypothesis . Section 19.2.\nKnowledge in Learning\n779\nBecause EBL uses Equation (19.3), it was initially thought to be a way to learn from ex-\namples. But because it requires that the background knowledge be sufﬁcient to explain the\nHypothesis, which in turn explains the observations, the agent does not actually learn any-\nthing factually new from the example. The agent could have derived the example from what\nit already knew, although that might have required an unreasonable amount of computation.\nEBL is now viewed as a method for converting ﬁrst-principles theories into useful, special-\npurpose knowledge. We describe algorithms for EBL in Section 19.3.\nThe situation of our traveler in Brazil is quite different, for she cannot necessarily ex-\nplain why Fernando speaks the way he does, unless she knows her papal bulls. Moreover,",
  "purpose knowledge. We describe algorithms for EBL in Section 19.3.\nThe situation of our traveler in Brazil is quite different, for she cannot necessarily ex-\nplain why Fernando speaks the way he does, unless she knows her papal bulls. Moreover,\nthe same generalization would be forthcoming from a traveler entirely ignorant of colonial\nhistory. The relevant prior knowledge in this case is that, within any given country, most\npeople tend to speak the same language; on the other hand, Fernando is not assumed to be\nthe name of all Brazilians because this kind of regularity does not hold for names. Similarly,\nthe freshman physics student also would be hard put to explain the particular values that she\ndiscovers for the conductance and density of copper. She does know, however, that the mate-\nrial of which an object is composed and its temperature together determine its conductance.\nIn each case, the prior knowledge Background concerns the relevance of a set of features to\nRELEVANCE\nthe goal predicate. This knowledge, together with the observations, allows the agent to infer\na new, general rule that explains the observations:\nHypothesis ∧Descriptions |= Classiﬁcations ,\nBackground ∧Descriptions ∧Classiﬁcations |= Hypothesis .\n(19.4)\nWe call this kind of generalization relevance-based learning, or RBL (although the name is\nRELEVANCE-BASED\nLEARNING\nnot standard). Notice that whereas RBL does make use of the content of the observations, it\ndoes not produce hypotheses that go beyond the logical content of the background knowledge\nand the observations. It is a deductive form of learning and cannot by itself account for the\ncreation of new knowledge starting from scratch.\nIn the case of the medical student watching the expert, we assume that the student’s\nprior knowledge is sufﬁcient to infer the patient’s disease D from the symptoms. This is\nnot, however, enough to explain the fact that the doctor prescribes a particular medicine M.\nThe student needs to propose another rule, namely, that M generally is effective against D.\nGiven this rule and the student’s prior knowledge, the student can now explain why the expert\nprescribes M in this particular case. We can generalize this example to come up with the\nentailment constraint\nBackground ∧Hypothesis ∧Descriptions |= Classiﬁcations .\n(19.5)\nThat is, the background knowledge and the new hypothesis combine to explain the examples.\nAs with pure inductive learning, the learning algorithm should propose hypotheses that are as",
  "Background ∧Hypothesis ∧Descriptions |= Classiﬁcations .\n(19.5)\nThat is, the background knowledge and the new hypothesis combine to explain the examples.\nAs with pure inductive learning, the learning algorithm should propose hypotheses that are as\nsimple as possible, consistent with this constraint. Algorithms that satisfy constraint (19.5)\nare called knowledge-based inductive learning, or KBIL, algorithms.\nKNOWLEDGE-BASED\nINDUCTIVE\nLEARNING\nKBIL algorithms, which are described in detail in Section 19.5, have been studied\nmainly in the ﬁeld of inductive logic programming, or ILP. In ILP systems, prior knowl-\nINDUCTIVE LOGIC\nPROGRAMMING\nedge plays two key roles in reducing the complexity of learning: 780\nChapter\n19.\nKnowledge in Learning\n1. Because any hypothesis generated must be consistent with the prior knowledge as well\nas with the new observations, the effective hypothesis space size is reduced to include\nonly those theories that are consistent with what is already known.\n2. For any given set of observations, the size of the hypothesis required to construct an\nexplanation for the observations can be much reduced, because the prior knowledge\nwill be available to help out the new rules in explaining the observations. The smaller\nthe hypothesis, the easier it is to ﬁnd.\nIn addition to allowing the use of prior knowledge in induction, ILP systems can formulate\nhypotheses in general ﬁrst-order logic, rather than in the restricted attribute-based language\nof Chapter 18. This means that they can learn in environments that cannot be understood by\nsimpler systems.\n19.3\nEXPLANATION-BASED LEARNING\nExplanation-based learning is a method for extracting general rules from individual obser-\nvations. As an example, consider the problem of differentiating and simplifying algebraic\nexpressions (Exercise 9.17). If we differentiate an expression such as X2 with respect to\nX, we obtain 2X. (We use a capital letter for the arithmetic unknown X, to distinguish it\nfrom the logical variable x.) In a logical reasoning system, the goal might be expressed as\nASK(Derivative(X2, X) = d, KB), with solution d = 2X.\nAnyone who knows differential calculus can see this solution “by inspection” as a result\nof practice in solving such problems. A student encountering such problems for the ﬁrst time,\nor a program with no experience, will have a much more difﬁcult job. Application of the\nstandard rules of differentiation eventually yields the expression 1 × (2 × (X(2−1))), and",
  "or a program with no experience, will have a much more difﬁcult job. Application of the\nstandard rules of differentiation eventually yields the expression 1 × (2 × (X(2−1))), and\neventually this simpliﬁes to 2X. In the authors’ logic programming implementation, this\ntakes 136 proof steps, of which 99 are on dead-end branches in the proof. After such an\nexperience, we would like the program to solve the same problem much more quickly the\nnext time it arises.\nThe technique of memoization has long been used in computer science to speed up\nMEMOIZATION\nprograms by saving the results of computation.\nThe basic idea of memo functions is to\naccumulate a database of input–output pairs; when the function is called, it ﬁrst checks the\ndatabase to see whether it can avoid solving the problem from scratch. Explanation-based\nlearning takes this a good deal further, by creating general rules that cover an entire class\nof cases. In the case of differentiation, memoization would remember that the derivative of\nX2 with respect to X is 2X, but would leave the agent to calculate the derivative of Z2 with\nrespect to Z from scratch. We would like to be able to extract the general rule that for any\narithmetic unknown u, the derivative of u2 with respect to u is 2u. (An even more general\nrule for un can also be produced, but the current example sufﬁces to make the point.) In\nlogical terms, this is expressed by the rule\nArithmeticUnknown(u) ⇒Derivative(u2, u) = 2u . Section 19.3.\nExplanation-Based Learning\n781\nIf the knowledge base contains such a rule, then any new case that is an instance of this rule\ncan be solved immediately.\nThis is, of course, merely a trivial example of a very general phenomenon. Once some-\nthing is understood, it can be generalized and reused in other circumstances. It becomes an\n“obvious” step and can then be used as a building block in solving problems still more com-\nplex. Alfred North Whitehead (1911), co-author with Bertrand Russell of Principia Mathe-\nmatica, wrote “Civilization advances by extending the number of important operations that\nwe can do without thinking about them,” perhaps himself applying EBL to his understanding\nof events such as Zog’s discovery. If you have understood the basic idea of the differenti-\nation example, then your brain is already busily trying to extract the general principles of\nexplanation-based learning from it. Notice that you hadn’t already invented EBL before you",
  "ation example, then your brain is already busily trying to extract the general principles of\nexplanation-based learning from it. Notice that you hadn’t already invented EBL before you\nsaw the example. Like the cavemen watching Zog, you (and we) needed an example before\nwe could generate the basic principles. This is because explaining why something is a good\nidea is much easier than coming up with the idea in the ﬁrst place.\n19.3.1\nExtracting general rules from examples\nThe basic idea behind EBL is ﬁrst to construct an explanation of the observation using prior\nknowledge, and then to establish a deﬁnition of the class of cases for which the same expla-\nnation structure can be used. This deﬁnition provides the basis for a rule covering all of the\ncases in the class. The “explanation” can be a logical proof, but more generally it can be any\nreasoning or problem-solving process whose steps are well deﬁned. The key is to be able to\nidentify the necessary conditions for those same steps to apply to another case.\nWe will use for our reasoning system the simple backward-chaining theorem prover\ndescribed in Chapter 9. The proof tree for Derivative(X2, X) = 2X is too large to use as an\nexample, so we will use a simpler problem to illustrate the generalization method. Suppose\nour problem is to simplify 1 × (0 + X). The knowledge base includes the following rules:\nRewrite(u, v) ∧Simplify(v, w) ⇒Simplify(u, w) .\nPrimitive(u) ⇒Simplify(u, u) .\nArithmeticUnknown(u) ⇒Primitive(u) .\nNumber(u) ⇒Primitive(u) .\nRewrite(1 × u, u) .\nRewrite(0 + u, u) .\n...\nThe proof that the answer is X is shown in the top half of Figure 19.7. The EBL method\nactually constructs two proof trees simultaneously. The second proof tree uses a variabilized\ngoal in which the constants from the original goal are replaced by variables. As the original\nproof proceeds, the variabilized proof proceeds in step, using exactly the same rule applica-\ntions. This could cause some of the variables to become instantiated. For example, in order\nto use the rule Rewrite(1× u, u), the variable x in the subgoal Rewrite(x × (y + z), v) must\nbe bound to 1. Similarly, y must be bound to 0 in the subgoal Rewrite(y + z, v′) in order to\nuse the rule Rewrite(0 + u, u). Once we have the generalized proof tree, we take the leaves 782\nChapter\n19.\nKnowledge in Learning\nPrimitive(X)\nArithmeticUnknown(X)\nPrimitive(z)\nArithmeticUnknown(z)\nSimplify(X,w)\nYes, {  }\nYes, {x / 1, v / y+z}\nSimplify(y+z,w)\nRewrite(y+z,v')",
  "Chapter\n19.\nKnowledge in Learning\nPrimitive(X)\nArithmeticUnknown(X)\nPrimitive(z)\nArithmeticUnknown(z)\nSimplify(X,w)\nYes, {  }\nYes, {x / 1, v / y+z}\nSimplify(y+z,w)\nRewrite(y+z,v')\nYes, {y / 0, v'/ z}\n{w / X}\nYes, {  }\nYes, {v / 0+X}\nYes, {v' / X}\nSimplify(z,w)\n{w / z}\nSimplify(1 × (0+X),w)\nRewrite(x × (y+z),v)\nSimplify(x × (y+z),w)\nRewrite(1 × (0+X),v)\nSimplify(0+X,w)\nRewrite(0+X,v')\nFigure 19.7\nProof trees for the simpliﬁcation problem. The ﬁrst tree shows the proof for\nthe original problem instance, from which we can derive\nArithmeticUnknown(z) ⇒Simplify(1 × (0 + z), z) .\nThe second tree shows the proof for a problem instance with all constants replaced by vari-\nables, from which we can derive a variety of other rules.\n(with the necessary bindings) and form a general rule for the goal predicate:\nRewrite(1 × (0 + z), 0 + z) ∧Rewrite(0 + z, z) ∧ArithmeticUnknown(z)\n⇒Simplify(1 × (0 + z), z) .\nNotice that the ﬁrst two conditions on the left-hand side are true regardless of the value of z.\nWe can therefore drop them from the rule, yielding\nArithmeticUnknown(z) ⇒Simplify(1 × (0 + z), z) .\nIn general, conditions can be dropped from the ﬁnal rule if they impose no constraints on the\nvariables on the right-hand side of the rule, because the resulting rule will still be true and\nwill be more efﬁcient. Notice that we cannot drop the condition ArithmeticUnknown(z),\nbecause not all possible values of z are arithmetic unknowns. Values other than arithmetic\nunknowns might require different forms of simpliﬁcation: for example, if z were 2 × 3, then\nthe correct simpliﬁcation of 1 × (0 + (2 × 3)) would be 6 and not 2 × 3.\nTo recap, the basic EBL process works as follows:\n1. Given an example, construct a proof that the goal predicate applies to the example using\nthe available background knowledge. Section 19.3.\nExplanation-Based Learning\n783\n2. In parallel, construct a generalized proof tree for the variabilized goal using the same\ninference steps as in the original proof.\n3. Construct a new rule whose left-hand side consists of the leaves of the proof tree and\nwhose right-hand side is the variabilized goal (after applying the necessary bindings\nfrom the generalized proof).\n4. Drop any conditions from the left-hand side that are true regardless of the values of the\nvariables in the goal.\n19.3.2\nImproving efﬁciency\nThe generalized proof tree in Figure 19.7 actually yields more than one generalized rule. For",
  "4. Drop any conditions from the left-hand side that are true regardless of the values of the\nvariables in the goal.\n19.3.2\nImproving efﬁciency\nThe generalized proof tree in Figure 19.7 actually yields more than one generalized rule. For\nexample, if we terminate, or prune, the growth of the right-hand branch in the proof tree\nwhen it reaches the Primitive step, we get the rule\nPrimitive(z) ⇒Simplify(1 × (0 + z), z) .\nThis rule is as valid as, but more general than, the rule using ArithmeticUnknown, because\nit covers cases where z is a number. We can extract a still more general rule by pruning after\nthe step Simplify(y + z, w), yielding the rule\nSimplify(y + z, w) ⇒Simplify(1 × (y + z), w) .\nIn general, a rule can be extracted from any partial subtree of the generalized proof tree. Now\nwe have a problem: which of these rules do we choose?\nThe choice of which rule to generate comes down to the question of efﬁciency. There\nare three factors involved in the analysis of efﬁciency gains from EBL:\n1. Adding large numbers of rules can slow down the reasoning process, because the in-\nference mechanism must still check those rules even in cases where they do not yield a\nsolution. In other words, it increases the branching factor in the search space.\n2. To compensate for the slowdown in reasoning, the derived rules must offer signiﬁcant\nincreases in speed for the cases that they do cover. These increases come about mainly\nbecause the derived rules avoid dead ends that would otherwise be taken, but also be-\ncause they shorten the proof itself.\n3. Derived rules should be as general as possible, so that they apply to the largest possible\nset of cases.\nA common approach to ensuring that derived rules are efﬁcient is to insist on the operational-\nity of each subgoal in the rule. A subgoal is operational if it is “easy” to solve. For example,\nOPERATIONALITY\nthe subgoal Primitive(z) is easy to solve, requiring at most two steps, whereas the subgoal\nSimplify(y + z, w) could lead to an arbitrary amount of inference, depending on the values\nof y and z. If a test for operationality is carried out at each step in the construction of the\ngeneralized proof, then we can prune the rest of a branch as soon as an operational subgoal is\nfound, keeping just the operational subgoal as a conjunct of the new rule.\nUnfortunately, there is usually a tradeoff between operationality and generality. More\nspeciﬁc subgoals are generally easier to solve but cover fewer cases. Also, operationality",
  "Unfortunately, there is usually a tradeoff between operationality and generality. More\nspeciﬁc subgoals are generally easier to solve but cover fewer cases. Also, operationality\nis a matter of degree: one or two steps is deﬁnitely operational, but what about 10 or 100? 784\nChapter\n19.\nKnowledge in Learning\nFinally, the cost of solving a given subgoal depends on what other rules are available in the\nknowledge base. It can go up or down as more rules are added. Thus, EBL systems really\nface a very complex optimization problem in trying to maximize the efﬁciency of a given\ninitial knowledge base. It is sometimes possible to derive a mathematical model of the effect\non overall efﬁciency of adding a given rule and to use this model to select the best rule to\nadd. The analysis can become very complicated, however, especially when recursive rules\nare involved. One promising approach is to address the problem of efﬁciency empirically,\nsimply by adding several rules and seeing which ones are useful and actually speed things up.\nEmpirical analysis of efﬁciency is actually at the heart of EBL. What we have been\ncalling loosely the “efﬁciency of a given knowledge base” is actually the average-case com-\nplexity on a distribution of problems. By generalizing from past example problems, EBL\nmakes the knowledge base more efﬁcient for the kind of problems that it is reasonable to\nexpect. This works as long as the distribution of past examples is roughly the same as for\nfuture examples—the same assumption used for PAC-learning in Section 18.5. If the EBL\nsystem is carefully engineered, it is possible to obtain signiﬁcant speedups. For example, a\nvery large Prolog-based natural language system designed for speech-to-speech translation\nbetween Swedish and English was able to achieve real-time performance only by the appli-\ncation of EBL to the parsing process (Samuelsson and Rayner, 1991).\n19.4\nLEARNING USING RELEVANCE INFORMATION\nOur traveler in Brazil seems to be able to make a conﬁdent generalization concerning the lan-\nguage spoken by other Brazilians. The inference is sanctioned by her background knowledge,\nnamely, that people in a given country (usually) speak the same language. We can express\nthis in ﬁrst-order logic as follows:2\nNationality(x, n) ∧Nationality(y, n) ∧Language(x, l) ⇒Language(y, l) . (19.6)\n(Literal translation: “If x and y have the same nationality n and x speaks language l, then y",
  "this in ﬁrst-order logic as follows:2\nNationality(x, n) ∧Nationality(y, n) ∧Language(x, l) ⇒Language(y, l) . (19.6)\n(Literal translation: “If x and y have the same nationality n and x speaks language l, then y\nalso speaks it.”) It is not difﬁcult to show that, from this sentence and the observation that\nNationality(Fernando, Brazil) ∧Language(Fernando, Portuguese) ,\nthe following conclusion is entailed (see Exercise 19.1):\nNationality(x, Brazil) ⇒Language(x, Portuguese) .\nSentences such as (19.6) express a strict form of relevance: given nationality, language\nis fully determined. (Put another way: language is a function of nationality.) These sentences\nare called functional dependencies or determinations. They occur so commonly in certain\nFUNCTIONAL\nDEPENDENCY\nDETERMINATION\nkinds of applications (e.g., deﬁning database designs) that a special syntax is used to write\nthem. We adopt the notation of Davies (1985):\nNationality(x, n) ≻Language(x, l) .\n2 We assume for the sake of simplicity that a person speaks only one language. Clearly, the rule would have to\nbe amended for countries such as Switzerland and India. Section 19.4.\nLearning Using Relevance Information\n785\nAs usual, this is simply a syntactic sugaring, but it makes it clear that the determination is\nreally a relationship between the predicates: nationality determines language. The relevant\nproperties determining conductance and density can be expressed similarly:\nMaterial(x, m) ∧Temperature(x, t) ≻Conductance(x, ρ) ;\nMaterial(x, m) ∧Temperature(x, t) ≻Density(x, d) .\nThe corresponding generalizations follow logically from the determinations and observations.\n19.4.1\nDetermining the hypothesis space\nAlthough the determinations sanction general conclusions concerning all Brazilians, or all\npieces of copper at a given temperature, they cannot, of course, yield a general predictive\ntheory for all nationalities, or for all temperatures and materials, from a single example.\nTheir main effect can be seen as limiting the space of hypotheses that the learning agent need\nconsider. In predicting conductance, for example, one need consider only material and tem-\nperature and can ignore mass, ownership, day of the week, the current president, and so on.\nHypotheses can certainly include terms that are in turn determined by material and temper-\nature, such as molecular structure, thermal energy, or free-electron density. Determinations",
  "Hypotheses can certainly include terms that are in turn determined by material and temper-\nature, such as molecular structure, thermal energy, or free-electron density. Determinations\nspecify a sufﬁcient basis vocabulary from which to construct hypotheses concerning the target\npredicate. This statement can be proven by showing that a given determination is logically\nequivalent to a statement that the correct deﬁnition of the target predicate is one of the set of\nall deﬁnitions expressible using the predicates on the left-hand side of the determination.\nIntuitively, it is clear that a reduction in the hypothesis space size should make it eas-\nier to learn the target predicate. Using the basic results of computational learning theory\n(Section 18.5), we can quantify the possible gains. First, recall that for Boolean functions,\nlog(|H|) examples are required to converge to a reasonable hypothesis, where |H| is the\nsize of the hypothesis space. If the learner has n Boolean features with which to construct\nhypotheses, then, in the absence of further restrictions, |H| = O(22n), so the number of ex-\namples is O(2n). If the determination contains d predicates in the left-hand side, the learner\nwill require only O(2d) examples, a reduction of O(2n−d).\n19.4.2\nLearning and using relevance information\nAs we stated in the introduction to this chapter, prior knowledge is useful in learning; but\nit too has to be learned. In order to provide a complete story of relevance-based learning,\nwe must therefore provide a learning algorithm for determinations. The learning algorithm\nwe now present is based on a straightforward attempt to ﬁnd the simplest determination con-\nsistent with the observations. A determination P ≻Q says that if any examples match on\nP, then they must also match on Q. A determination is therefore consistent with a set of\nexamples if every pair that matches on the predicates on the left-hand side also matches on\nthe goal predicate. For example, suppose we have the following examples of conductance\nmeasurements on material samples: 786\nChapter\n19.\nKnowledge in Learning\nfunction MINIMAL-CONSISTENT-DET(E,A) returns a set of attributes\ninputs: E, a set of examples\nA, a set of attributes, of size n\nfor i = 0 to n do\nfor each subset Ai of A of size i do\nif CONSISTENT-DET?(Ai,E) then return Ai\nfunction CONSISTENT-DET?(A,E) returns a truth value\ninputs: A, a set of attributes\nE, a set of examples\nlocal variables: H , a hash table\nfor each example e in E do",
  "for each subset Ai of A of size i do\nif CONSISTENT-DET?(Ai,E) then return Ai\nfunction CONSISTENT-DET?(A,E) returns a truth value\ninputs: A, a set of attributes\nE, a set of examples\nlocal variables: H , a hash table\nfor each example e in E do\nif some example in H has the same values as e for the attributes A\nbut a different classiﬁcation then return false\nstore the class of e in H , indexed by the values for attributes A of the example e\nreturn true\nFigure 19.8\nAn algorithm for ﬁnding a minimal consistent determination.\nSample Mass Temperature Material Size\nConductance\nS1\n12\n26\nCopper\n3\n0.59\nS1\n12\n100\nCopper\n3\n0.57\nS2\n24\n26\nCopper\n6\n0.59\nS3\n12\n26\nLead\n2\n0.05\nS3\n12\n100\nLead\n2\n0.04\nS4\n24\n26\nLead\n4\n0.05\nThe minimal consistent determination is Material ∧Temperature ≻Conductance. There\nis a nonminimal but consistent determination, namely, Mass ∧Size ∧Temperature ≻\nConductance. This is consistent with the examples because mass and size determine density\nand, in our data set, we do not have two different materials with the same density. As usual,\nwe would need a larger sample set in order to eliminate a nearly correct hypothesis.\nThere are several possible algorithms for ﬁnding minimal consistent determinations.\nThe most obvious approach is to conduct a search through the space of determinations, check-\ning all determinations with one predicate, two predicates, and so on, until a consistent deter-\nmination is found. We will assume a simple attribute-based representation, like that used for\ndecision tree learning in Chapter 18. A determination d will be represented by the set of\nattributes on the left-hand side, because the target predicate is assumed to be ﬁxed. The basic\nalgorithm is outlined in Figure 19.8.\nThe time complexity of this algorithm depends on the size of the smallest consistent\ndetermination. Suppose this determination has p attributes out of the n total attributes. Then\nthe algorithm will not ﬁnd it until searching the subsets of A of size p. There are\n\u0014n\np\n\u0015\n= O(np) Section 19.4.\nLearning Using Relevance Information\n787\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n20\n40\n60\n80\n100\n120\n140\nProportion correct on test set\nTraining set size\nRBDTL\nDTL\nFigure 19.9\nA performance comparison between DECISION-TREE-LEARNING and\nRBDTL on randomly generated data for a target function that depends on only 5 of 16\nattributes.\nsuch subsets; hence the algorithm is exponential in the size of the minimal determination. It",
  "A performance comparison between DECISION-TREE-LEARNING and\nRBDTL on randomly generated data for a target function that depends on only 5 of 16\nattributes.\nsuch subsets; hence the algorithm is exponential in the size of the minimal determination. It\nturns out that the problem is NP-complete, so we cannot expect to do better in the general\ncase. In most domains, however, there will be sufﬁcient local structure (see Chapter 14 for a\ndeﬁnition of locally structured domains) that p will be small.\nGiven an algorithm for learning determinations, a learning agent has a way to construct\na minimal hypothesis within which to learn the target predicate. For example, we can combine\nMINIMAL-CONSISTENT-DET with the DECISION-TREE-LEARNING algorithm. This yields\na relevance-based decision-tree learning algorithm RBDTL that ﬁrst identiﬁes a minimal\nset of relevant attributes and then passes this set to the decision tree algorithm for learning.\nUnlike DECISION-TREE-LEARNING, RBDTL simultaneously learns and uses relevance in-\nformation in order to minimize its hypothesis space. We expect that RBDTL will learn faster\nthan DECISION-TREE-LEARNING, and this is in fact the case. Figure 19.9 shows the learning\nperformance for the two algorithms on randomly generated data for a function that depends\non only 5 of 16 attributes. Obviously, in cases where all the available attributes are relevant,\nRBDTL will show no advantage.\nThis section has only scratched the surface of the ﬁeld of declarative bias, which aims\nDECLARATIVE BIAS\nto understand how prior knowledge can be used to identify the appropriate hypothesis space\nwithin which to search for the correct target deﬁnition. There are many unanswered questions:\n• How can the algorithms be extended to handle noise?\n• Can we handle continuous-valued variables?\n• How can other kinds of prior knowledge be used, besides determinations?\n• How can the algorithms be generalized to cover any ﬁrst-order theory, rather than just\nan attribute-based representation?\nSome of these questions are addressed in the next section. 788\nChapter\n19.\nKnowledge in Learning\n19.5\nINDUCTIVE LOGIC PROGRAMMING\nInductive logic programming (ILP) combines inductive methods with the power of ﬁrst-order\nrepresentations, concentrating in particular on the representation of hypotheses as logic pro-\ngrams.3 It has gained popularity for three reasons. First, ILP offers a rigorous approach to",
  "representations, concentrating in particular on the representation of hypotheses as logic pro-\ngrams.3 It has gained popularity for three reasons. First, ILP offers a rigorous approach to\nthe general knowledge-based inductive learning problem. Second, it offers complete algo-\nrithms for inducing general, ﬁrst-order theories from examples, which can therefore learn\nsuccessfully in domains where attribute-based algorithms are hard to apply. An example is\nin learning how protein structures fold (Figure 19.10). The three-dimensional conﬁguration\nof a protein molecule cannot be represented reasonably by a set of attributes, because the\nconﬁguration inherently refers to relationships between objects, not to attributes of a single\nobject. First-order logic is an appropriate language for describing the relationships. Third,\ninductive logic programming produces hypotheses that are (relatively) easy for humans to\nread. For example, the English translation in Figure 19.10 can be scrutinized and criticized\nby working biologists. This means that inductive logic programming systems can participate\nin the scientiﬁc cycle of experimentation, hypothesis generation, debate, and refutation. Such\nparticipation would not be possible for systems that generate “black-box” classiﬁers, such as\nneural networks.\n19.5.1\nAn example\nRecall from Equation (19.5) that the general knowledge-based induction problem is to “solve”\nthe entailment constraint\nBackground ∧Hypothesis ∧Descriptions |= Classiﬁcations\nfor the unknown Hypothesis, given the Background knowledge and examples described by\nDescriptions and Classiﬁcations. To illustrate this, we will use the problem of learning\nfamily relationships from examples. The descriptions will consist of an extended family\ntree, described in terms of Mother, Father, and Married relations and Male and Female\nproperties. As an example, we will use the family tree from Exercise 8.14, shown here in\nFigure 19.11. The corresponding descriptions are as follows:\nFather(Philip, Charles)\nFather(Philip, Anne)\n. . .\nMother(Mum, Margaret) Mother(Mum, Elizabeth)\n. . .\nMarried(Diana, Charles) Married(Elizabeth, Philip) . . .\nMale(Philip)\nMale(Charles)\n. . .\nFemale(Beatrice)\nFemale(Margaret)\n. . .\nThe sentences in Classiﬁcations depend on the target concept being learned. We might want\nto learn Grandparent, BrotherInLaw, or Ancestor, for example. For Grandparent, the",
  "Male(Philip)\nMale(Charles)\n. . .\nFemale(Beatrice)\nFemale(Margaret)\n. . .\nThe sentences in Classiﬁcations depend on the target concept being learned. We might want\nto learn Grandparent, BrotherInLaw, or Ancestor, for example. For Grandparent, the\n3 It might be appropriate at this point for the reader to refer to Chapter 7 for some of the underlying concepts,\nincluding Horn clauses, conjunctive normal form, uniﬁcation, and resolution. Section 19.5.\nInductive Logic Programming\n789\ncomplete set of Classiﬁcations contains 20 × 20 = 400 conjuncts of the form\nGrandparent(Mum, Charles) Grandparent(Elizabeth, Beatrice) . . .\n¬Grandparent(Mum, Harry) ¬Grandparent(Spencer, Peter)\n. . .\nWe could of course learn from a subset of this complete set.\nThe object of an inductive learning program is to come up with a set of sentences for\nthe Hypothesis such that the entailment constraint is satisﬁed. Suppose, for the moment, that\nthe agent has no background knowledge: Background is empty. Then one possible solution\n2mhr - Four-helical up-and-down bundle\nH:1[19-37]\nH:2[41-64]\nH:3[71-84]\nH:4[93-108]\nH:5[111-113]\nH:1[8-17]\nH:2[26-33]\nH:3[40-50]\nH:4[61-64]\nH:5[66-70]\nH:6[79-88]\nH:7[99-106]\nE:1[57-59]\nE:2[96-98]\n1omd - EF-Hand\n(a)\n(b)\nFigure 19.10\n(a) and (b) show positive and negative examples, respectively, of the\n“four-helical up-and-down bundle” concept in the domain of protein folding.\nEach\nexample structure is coded into a logical expression of about 100 conjuncts such as\nTotalLength(D2mhr, 118)∧NumberHelices(D2mhr, 6)∧. . .. From these descriptions and\nfrom classiﬁcations such as Fold(FOUR-HELICAL-UP-AND-DOWN-BUNDLE, D2mhr),\nthe ILP system PROGOL (Muggleton, 1995) learned the following rule:\nFold(FOUR-HELICAL-UP-AND-DOWN-BUNDLE, p) ⇐\nHelix(p, h1) ∧Length(h1, HIGH) ∧Position(p, h1, n)\n∧(1 ≤n ≤3) ∧Adjacent(p, h1, h2) ∧Helix(p, h2) .\nThis kind of rule could not be learned, or even represented, by an attribute-based mechanism\nsuch as we saw in previous chapters. The rule can be translated into English as “ Protein p\nhas fold class “Four-helical up-and-down-bundle”if it contains a long helix h1 at a secondary\nstructure position between 1 and 3 and h1 is next to a second helix.” 790\nChapter\n19.\nKnowledge in Learning\nfor Hypothesis is the following:\nGrandparent(x, y)\n⇔\n[∃z Mother(x, z) ∧Mother(z, y)]\n∨\n[∃z Mother(x, z) ∧Father(z, y)]\n∨\n[∃z Father(x, z) ∧Mother(z, y)]\n∨\n[∃z Father(x, z) ∧Father(z, y)] .",
  "Chapter\n19.\nKnowledge in Learning\nfor Hypothesis is the following:\nGrandparent(x, y)\n⇔\n[∃z Mother(x, z) ∧Mother(z, y)]\n∨\n[∃z Mother(x, z) ∧Father(z, y)]\n∨\n[∃z Father(x, z) ∧Mother(z, y)]\n∨\n[∃z Father(x, z) ∧Father(z, y)] .\nNotice that an attribute-based learning algorithm, such as DECISION-TREE-LEARNING, will\nget nowhere in solving this problem. In order to express Grandparent as an attribute (i.e., a\nunary predicate), we would need to make pairs of people into objects:\nGrandparent(⟨Mum, Charles⟩) . . .\nThen we get stuck in trying to represent the example descriptions. The only possible attributes\nare horrible things such as\nFirstElementIsMotherOfElizabeth(⟨Mum, Charles⟩) .\nThe deﬁnition of Grandparent in terms of these attributes simply becomes a large disjunc-\ntion of speciﬁc cases that does not generalize to new examples at all. Attribute-based learning\nalgorithms are incapable of learning relational predicates. Thus, one of the principal advan-\ntages of ILP algorithms is their applicability to a much wider range of problems, including\nrelational problems.\nThe reader will certainly have noticed that a little bit of background knowledge would\nhelp in the representation of the Grandparent deﬁnition. For example, if Background in-\ncluded the sentence\nParent(x, y) ⇔[Mother(x, y) ∨Father(x, y)] ,\nthen the deﬁnition of Grandparent would be reduced to\nGrandparent(x, y) ⇔[∃z Parent(x, z) ∧Parent(z, y)] .\nThis shows how background knowledge can dramatically reduce the size of hypotheses re-\nquired to explain the observations.\nIt is also possible for ILP algorithms to create new predicates in order to facilitate the\nexpression of explanatory hypotheses. Given the example data shown earlier, it is entirely\nreasonable for the ILP program to propose an additional predicate, which we would call\nBeatrice\nAndrew\nEugenie\nWilliam Harry\nCharles\nDiana\nMum\nGeorge\nPhilip\nElizabeth\nMargaret\nKydd\nSpencer\nPeter\nMark\nZara\nAnne\nSarah\nEdward\nSophie\nLouise\nJames\nFigure 19.11\nA typical family tree. Section 19.5.\nInductive Logic Programming\n791\n“Parent,” in order to simplify the deﬁnitions of the target predicates. Algorithms that can\ngenerate new predicates are called constructive induction algorithms. Clearly, constructive\nCONSTRUCTIVE\nINDUCTION\ninduction is a necessary part of the picture of cumulative learning. It has been one of the\nhardest problems in machine learning, but some ILP techniques provide effective mechanisms\nfor achieving it.",
  "CONSTRUCTIVE\nINDUCTION\ninduction is a necessary part of the picture of cumulative learning. It has been one of the\nhardest problems in machine learning, but some ILP techniques provide effective mechanisms\nfor achieving it.\nIn the rest of this chapter, we will study the two principal approaches to ILP. The ﬁrst\nuses a generalization of decision tree methods, and the second uses techniques based on\ninverting a resolution proof.\n19.5.2\nTop-down inductive learning methods\nThe ﬁrst approach to ILP works by starting with a very general rule and gradually specializing\nit so that it ﬁts the data. This is essentially what happens in decision-tree learning, where a\ndecision tree is gradually grown until it is consistent with the observations. To do ILP we\nuse ﬁrst-order literals instead of attributes, and the hypothesis is a set of clauses instead of a\ndecision tree. This section describes FOIL (Quinlan, 1990), one of the ﬁrst ILP programs.\nSuppose we are trying to learn a deﬁnition of the Grandfather(x, y) predicate, using\nthe same family data as before. As with decision-tree learning, we can divide the examples\ninto positive and negative examples. Positive examples are\n⟨George, Anne⟩, ⟨Philip, Peter⟩, ⟨Spencer, Harry⟩, . . .\nand negative examples are\n⟨George, Elizabeth⟩, ⟨Harry, Zara⟩, ⟨Charles, Philip⟩, . . .\nNotice that each example is a pair of objects, because Grandfather is a binary predicate. In\nall, there are 12 positive examples in the family tree and 388 negative examples (all the other\npairs of people).\nFOIL constructs a set of clauses, each with Grandfather(x, y) as the head. The clauses\nmust classify the 12 positive examples as instances of the Grandfather(x, y) relationship,\nwhile ruling out the 388 negative examples. The clauses are Horn clauses, with the extension\nthat negated literals are allowed in the body of a clause and are interpreted using negation as\nfailure, as in Prolog. The initial clause has an empty body:\n⇒Grandfather(x, y) .\nThis clause classiﬁes every example as positive, so it needs to be specialized. We do this by\nadding literals one at a time to the left-hand side. Here are three potential additions:\nFather(x, y) ⇒Grandfather(x, y) .\nParent(x, z) ⇒Grandfather(x, y) .\nFather(x, z) ⇒Grandfather(x, y) .\n(Notice that we are assuming that a clause deﬁning Parent is already part of the background\nknowledge.) The ﬁrst of these three clauses incorrectly classiﬁes all of the 12 positive exam-",
  "Father(x, z) ⇒Grandfather(x, y) .\n(Notice that we are assuming that a clause deﬁning Parent is already part of the background\nknowledge.) The ﬁrst of these three clauses incorrectly classiﬁes all of the 12 positive exam-\nples as negative and can thus be ignored. The second and third agree with all of the positive\nexamples, but the second is incorrect on a larger fraction of the negative examples—twice as\nmany, because it allows mothers as well as fathers. Hence, we prefer the third clause. 792\nChapter\n19.\nKnowledge in Learning\nNow we need to specialize this clause further, to rule out the cases in which x is the\nfather of some z, but z is not a parent of y. Adding the single literal Parent(z, y) gives\nFather(x, z) ∧Parent(z, y) ⇒Grandfather(x, y) ,\nwhich correctly classiﬁes all the examples. FOIL will ﬁnd and choose this literal, thereby\nsolving the learning task. In general, the solution is a set of Horn clauses, each of which\nimplies the target predicate. For example, if we didn’t have the Parent predicate in our\nvocabulary, then the solution might be\nFather(x, z) ∧Father(z, y) ⇒Grandfather(x, y)\nFather(x, z) ∧Mother(z, y) ⇒Grandfather(x, y) .\nNote that each of these clauses covers some of the positive examples, that together they cover\nall the positive examples, and that NEW-CLAUSE is designed in such a way that no clause\nwill incorrectly cover a negative example. In general FOIL will have to search through many\nunsuccessful clauses before ﬁnding a correct solution.\nThis example is a very simple illustration of how FOIL operates. A sketch of the com-\nplete algorithm is shown in Figure 19.12. Essentially, the algorithm repeatedly constructs a\nclause, literal by literal, until it agrees with some subset of the positive examples and none of\nthe negative examples. Then the positive examples covered by the clause are removed from\nthe training set, and the process continues until no positive examples remain. The two main\nsubroutines to be explained are NEW-LITERALS, which constructs all possible new literals to\nadd to the clause, and CHOOSE-LITERAL, which selects a literal to add.\nNEW-LITERALS takes a clause and constructs all possible “useful” literals that could\nbe added to the clause. Let us use as an example the clause\nFather(x, z) ⇒Grandfather(x, y) .\nThere are three kinds of literals that can be added:\n1. Literals using predicates: the literal can be negated or unnegated, any existing predicate",
  "be added to the clause. Let us use as an example the clause\nFather(x, z) ⇒Grandfather(x, y) .\nThere are three kinds of literals that can be added:\n1. Literals using predicates: the literal can be negated or unnegated, any existing predicate\n(including the goal predicate) can be used, and the arguments must all be variables. Any\nvariable can be used for any argument of the predicate, with one restriction: each literal\nmust include at least one variable from an earlier literal or from the head of the clause.\nLiterals such as Mother(z, u), Married(z, z), ¬Male(y), and Grandfather(v, x) are\nallowed, whereas Married(u, v) is not. Notice that the use of the predicate from the\nhead of the clause allows FOIL to learn recursive deﬁnitions.\n2. Equality and inequality literals: these relate variables already appearing in the clause.\nFor example, we might add z ̸= x. These literals can also include user-speciﬁed con-\nstants. For learning arithmetic we might use 0 and 1, and for learning list functions we\nmight use the empty list [ ].\n3. Arithmetic comparisons: when dealing with functions of continuous variables, literals\nsuch as x > y and y ≤z can be added. As in decision-tree learning, a constant\nthreshold value can be chosen to maximize the discriminatory power of the test.\nThe resulting branching factor in this search space is very large (see Exercise 19.6), but FOIL\ncan also use type information to reduce it. For example, if the domain included numbers as Section 19.5.\nInductive Logic Programming\n793\nfunction FOIL(examples,target) returns a set of Horn clauses\ninputs: examples, set of examples\ntarget, a literal for the goal predicate\nlocal variables: clauses, set of clauses, initially empty\nwhile examples contains positive examples do\nclause ←NEW-CLAUSE(examples,target)\nremove positive examples covered by clause from examples\nadd clause to clauses\nreturn clauses\nfunction NEW-CLAUSE(examples,target) returns a Horn clause\nlocal variables: clause, a clause with target as head and an empty body\nl, a literal to be added to the clause\nextended examples, a set of examples with values for new variables\nextended examples ←examples\nwhile extended examples contains negative examples do\nl ←CHOOSE-LITERAL(NEW-LITERALS(clause),extended examples)\nappend l to the body of clause\nextended examples ←set of examples created by applying EXTEND-EXAMPLE\nto each example in extended examples\nreturn clause\nfunction EXTEND-EXAMPLE(example,literal) returns a set of examples",
  "append l to the body of clause\nextended examples ←set of examples created by applying EXTEND-EXAMPLE\nto each example in extended examples\nreturn clause\nfunction EXTEND-EXAMPLE(example,literal) returns a set of examples\nif example satisﬁes literal\nthen return the set of examples created by extending example with\neach possible constant value for each new variable in literal\nelse return the empty set\nFigure 19.12\nSketch of the FOIL algorithm for learning sets of ﬁrst-order Horn clauses\nfrom examples. NEW-LITERALS and CHOOSE-LITERAL are explained in the text.\nwell as people, type restrictions would prevent NEW-LITERALS from generating literals such\nas Parent(x, n), where x is a person and n is a number.\nCHOOSE-LITERAL uses a heuristic somewhat similar to information gain (see page 704)\nto decide which literal to add. The exact details are not important here, and a number of\ndifferent variations have been tried. One interesting additional feature of FOIL is the use of\nOckham’s razor to eliminate some hypotheses. If a clause becomes longer (according to some\nmetric) than the total length of the positive examples that the clause explains, that clause is\nnot considered as a potential hypothesis. This technique provides a way to avoid overcomplex\nclauses that ﬁt noise in the data.\nFOIL and its relatives have been used to learn a wide variety of deﬁnitions. One of the\nmost impressive demonstrations (Quinlan and Cameron-Jones, 1993) involved solving a long\nsequence of exercises on list-processing functions from Bratko’s (1986) Prolog textbook. In 794\nChapter\n19.\nKnowledge in Learning\neach case, the program was able to learn a correct deﬁnition of the function from a small set\nof examples, using the previously learned functions as background knowledge.\n19.5.3\nInductive learning with inverse deduction\nThe second major approach to ILP involves inverting the normal deductive proof process.\nInverse resolution is based on the observation that if the example Classiﬁcations follow\nINVERSE\nRESOLUTION\nfrom Background ∧Hypothesis ∧Descriptions, then one must be able to prove this fact by\nresolution (because resolution is complete). If we can “run the proof backward,” then we can\nﬁnd a Hypothesis such that the proof goes through. The key, then, is to ﬁnd a way to invert\nthe resolution process.\nWe will show a backward proof process for inverse resolution that consists of individual\nbackward steps. Recall that an ordinary resolution step takes two clauses C1 and C2 and",
  "the resolution process.\nWe will show a backward proof process for inverse resolution that consists of individual\nbackward steps. Recall that an ordinary resolution step takes two clauses C1 and C2 and\nresolves them to produce the resolvent C. An inverse resolution step takes a resolvent C\nand produces two clauses C1 and C2, such that C is the result of resolving C1 and C2.\nAlternatively, it may take a resolvent C and clause C1 and produce a clause C2 such that C\nis the result of resolving C1 and C2.\nThe early steps in an inverse resolution process are shown in Figure 19.13, where we\nfocus on the positive example Grandparent(George, Anne). The process begins at the end\nof the proof (shown at the bottom of the ﬁgure). We take the resolvent C to be empty\nclause (i.e. a contradiction) and C2 to be ¬Grandparent(George, Anne), which is the nega-\ntion of the goal example. The ﬁrst inverse step takes C and C2 and generates the clause\nGrandparent(George, Anne) for C1. The next step takes this clause as C and the clause\nParent(Elizabeth, Anne) as C2, and generates the clause\n¬Parent(Elizabeth, y) ∨Grandparent(George, y)\nas C1. The ﬁnal step treats this clause as the resolvent. With Parent(George, Elizabeth) as\nC2, one possible clause C1 is the hypothesis\nParent(x, z) ∧Parent(z, y) ⇒Grandparent(x, y) .\nNow we have a resolution proof that the hypothesis, descriptions, and background knowledge\nentail the classiﬁcation Grandparent(George, Anne).\nClearly, inverse resolution involves a search. Each inverse resolution step is nonde-\nterministic, because for any C, there can be many or even an inﬁnite number of clauses\nC1 and C2 that resolve to C. For example, instead of choosing ¬Parent(Elizabeth, y) ∨\nGrandparent(George, y) for C1 in the last step of Figure 19.13, the inverse resolution step\nmight have chosen any of the following sentences:\n¬Parent(Elizabeth, Anne) ∨Grandparent(George, Anne) .\n¬Parent(z, Anne) ∨Grandparent(George, Anne) .\n¬Parent(z, y) ∨Grandparent(George, y) .\n...\n(See Exercises 19.4 and 19.5.) Furthermore, the clauses that participate in each step can be\nchosen from the Background knowledge, from the example Descriptions, from the negated Section 19.5.\nInductive Logic Programming\n795\nClassiﬁcations, or from hypothesized clauses that have already been generated in the inverse\nresolution tree. The large number of possibilities means a large branching factor (and there-",
  "Inductive Logic Programming\n795\nClassiﬁcations, or from hypothesized clauses that have already been generated in the inverse\nresolution tree. The large number of possibilities means a large branching factor (and there-\nfore an inefﬁcient search) without additional controls. A number of approaches to taming the\nsearch have been tried in implemented ILP systems:\n1. Redundant choices can be eliminated—for example, by generating only the most spe-\nciﬁc hypotheses possible and by requiring that all the hypothesized clauses be consistent\nwith each other, and with the observations. This last criterion would rule out the clause\n¬Parent(z, y) ∨Grandparent(George, y), listed before.\n2. The proof strategy can be restricted. For example, we saw in Chapter 9 that linear\nresolution is a complete, restricted strategy. Linear resolution produces proof trees that\nhave a linear branching structure—the whole tree follows one line, with only single\nclauses branching off that line (as in Figure 19.13).\n3. The representation language can be restricted, for example by eliminating function sym-\nbols or by allowing only Horn clauses. For instance, PROGOL operates with Horn\nclauses using inverse entailment. The idea is to change the entailment constraint\nINVERSE\nENTAILMENT\nBackground ∧Hypothesis ∧Descriptions |= Classiﬁcations\nto the logically equivalent form\nBackground ∧Descriptions ∧¬Classiﬁcations |= ¬Hypothesis.\nFrom this, one can use a process similar to the normal Prolog Horn-clause deduction,\nwith negation-as-failure to derive Hypothesis. Because it is restricted to Horn clauses,\nthis is an incomplete method, but it can be more efﬁcient than full resolution. It is also\npossible to apply complete inference with inverse entailment (Inoue, 2001).\n4. Inference can be done with model checking rather than theorem proving. The PROGOL\nsystem (Muggleton, 1995) uses a form of model checking to limit the search. That\n{y/Anne}\nParent(Elizabeth,Anne)\nGrandparent(George,Anne)\nGrandparent(George,Anne)\nGrandparent(George,y)\nParent(Elizabeth,y)\n>\n{x/George, z/Elizabeth}\nParent(George,Elizabeth)\n>\nParent(z,y)\nGrandparent(x,y)\n>\nParent(x,z)\n¬\n¬\n¬\n¬\nFigure 19.13\nEarly steps in an inverse resolution process.\nThe shaded clauses are\ngenerated by inverse resolution steps from the clause to the right and the clause below.\nThe unshaded clauses are from the Descriptions and Classiﬁcations (including negated\nClassiﬁcations). 796\nChapter\n19.\nKnowledge in Learning",
  "generated by inverse resolution steps from the clause to the right and the clause below.\nThe unshaded clauses are from the Descriptions and Classiﬁcations (including negated\nClassiﬁcations). 796\nChapter\n19.\nKnowledge in Learning\nis, like answer set programming, it generates possible values for logical variables, and\nchecks for consistency.\n5. Inference can be done with ground propositional clauses rather than in ﬁrst-order logic.\nThe LINUS system (Lavrauc and Duzeroski, 1994) works by translating ﬁrst-order the-\nories into propositional logic, solving them with a propositional learning system, and\nthen translating back. Working with propositional formulas can be more efﬁcient on\nsome problems, as we saw with SATPLAN in Chapter 10.\n19.5.4\nMaking discoveries with inductive logic programming\nAn inverse resolution procedure that inverts a complete resolution strategy is, in principle, a\ncomplete algorithm for learning ﬁrst-order theories. That is, if some unknown Hypothesis\ngenerates a set of examples, then an inverse resolution procedure can generate Hypothesis\nfrom the examples. This observation suggests an interesting possibility: Suppose that the\navailable examples include a variety of trajectories of falling bodies. Would an inverse reso-\nlution program be theoretically capable of inferring the law of gravity? The answer is clearly\nyes, because the law of gravity allows one to explain the examples, given suitable background\nmathematics. Similarly, one can imagine that electromagnetism, quantum mechanics, and the\ntheory of relativity are also within the scope of ILP programs. Of course, they are also within\nthe scope of a monkey with a typewriter; we still need better heuristics and new ways to\nstructure the search space.\nOne thing that inverse resolution systems will do for you is invent new predicates. This\nability is often seen as somewhat magical, because computers are often thought of as “merely\nworking with what they are given.” In fact, new predicates fall directly out of the inverse\nresolution step. The simplest case arises in hypothesizing two new clauses C1 and C2, given\na clause C. The resolution of C1 and C2 eliminates a literal that the two clauses share; hence,\nit is quite possible that the eliminated literal contained a predicate that does not appear in C.\nThus, when working backward, one possibility is to generate a new predicate from which to\nreconstruct the missing literal.",
  "it is quite possible that the eliminated literal contained a predicate that does not appear in C.\nThus, when working backward, one possibility is to generate a new predicate from which to\nreconstruct the missing literal.\nFigure 19.14 shows an example in which the new predicate P is generated in the process\nof learning a deﬁnition for Ancestor. Once generated, P can be used in later inverse resolu-\ntion steps. For example, a later step might hypothesize that Mother(x, y) ⇒P(x, y). Thus,\nthe new predicate P has its meaning constrained by the generation of hypotheses that involve\nit. Another example might lead to the constraint Father(x, y) ⇒P(x, y). In other words,\nthe predicate P is what we usually think of as the Parent relationship. As we mentioned\nearlier, the invention of new predicates can signiﬁcantly reduce the size of the deﬁnition of\nthe goal predicate. Hence, by including the ability to invent new predicates, inverse resolution\nsystems can often solve learning problems that are infeasible with other techniques.\nSome of the deepest revolutions in science come from the invention of new predicates\nand functions—for example, Galileo’s invention of acceleration or Joule’s invention of ther-\nmal energy. Once these terms are available, the discovery of new laws becomes (relatively)\neasy. The difﬁcult part lies in realizing that some new entity, with a speciﬁc relationship\nto existing entities, will allow an entire body of observations to be explained with a much Section 19.6.\nSummary\n797\n{x/George}\nFather(x,y)\nP(x,y)\n>\nFather(George,y)\nAncestor(George,y)\n>\nP(George,y)\nAncestor(George,y)\n>\n¬\n¬\nFigure 19.14\nAn inverse resolution step that generates a new predicate P.\nsimpler and more elegant theory than previously existed.\nAs yet, ILP systems have not made discoveries on the level of Galileo or Joule, but their\ndiscoveries have been deemed publishable in the scientiﬁc literature. For example, in the\nJournal of Molecular Biology, Turcotte et al. (2001) describe the automated discovery of rules\nfor protein folding by the ILP program PROGOL. Many of the rules discovered by PROGOL\ncould have been derived from known principles, but most had not been previously published\nas part of a standard biological database. (See Figure 19.10 for an example.). In related\nwork, Srinivasan et al. (1994) dealt with the problem of discovering molecular-structure-\nbased rules for the mutagenicity of nitroaromatic compounds. These compounds are found in",
  "work, Srinivasan et al. (1994) dealt with the problem of discovering molecular-structure-\nbased rules for the mutagenicity of nitroaromatic compounds. These compounds are found in\nautomobile exhaust fumes. For 80% of the compounds in a standard database, it is possible to\nidentify four important features, and linear regression on these features outperforms ILP. For\nthe remaining 20%, the features alone are not predictive, and ILP identiﬁes relationships that\nallow it to outperform linear regression, neural nets, and decision trees. Most impressively,\nKing et al. (2009) endowed a robot with the ability to perform molecular biology experiments\nand extended ILP techniques to include experiment design, thereby creating an autonomous\nscientist that actually discovered new knowledge about the functional genomics of yeast. For\nall these examples it appears that the ability both to represent relations and to use background\nknowledge contribute to ILP’s high performance. The fact that the rules found by ILP can be\ninterpreted by humans contributes to the acceptance of these techniques in biology journals\nrather than just computer science journals.\nILP has made contributions to other sciences besides biology. One of the most impor-\ntant is natural language processing, where ILP has been used to extract complex relational\ninformation from text. These results are summarized in Chapter 23.\n19.6\nSUMMARY\nThis chapter has investigated various ways in which prior knowledge can help an agent to\nlearn from new experiences. Because much prior knowledge is expressed in terms of rela-\ntional models rather than attribute-based models, we have also covered systems that allow\nlearning of relational models. The important points are:\n• The use of prior knowledge in learning leads to a picture of cumulative learning, in\nwhich learning agents improve their learning ability as they acquire more knowledge.\n• Prior knowledge helps learning by eliminating otherwise consistent hypotheses and by 798\nChapter\n19.\nKnowledge in Learning\n“ﬁlling in” the explanation of examples, thereby allowing for shorter hypotheses. These\ncontributions often result in faster learning from fewer examples.\n• Understanding the different logical roles played by prior knowledge, as expressed by\nentailment constraints, helps to deﬁne a variety of learning techniques.\n• Explanation-based learning (EBL) extracts general rules from single examples by ex-",
  "• Understanding the different logical roles played by prior knowledge, as expressed by\nentailment constraints, helps to deﬁne a variety of learning techniques.\n• Explanation-based learning (EBL) extracts general rules from single examples by ex-\nplaining the examples and generalizing the explanation. It provides a deductive method\nfor turning ﬁrst-principles knowledge into useful, efﬁcient, special-purpose expertise.\n• Relevance-based learning (RBL) uses prior knowledge in the form of determinations\nto identify the relevant attributes, thereby generating a reduced hypothesis space and\nspeeding up learning. RBL also allows deductive generalizations from single examples.\n• Knowledge-based inductive learning (KBIL) ﬁnds inductive hypotheses that explain\nsets of observations with the help of background knowledge.\n• Inductive logic programming (ILP) techniques perform KBIL on knowledge that is\nexpressed in ﬁrst-order logic. ILP methods can learn relational knowledge that is not\nexpressible in attribute-based systems.\n• ILP can be done with a top-down approach of reﬁning a very general rule or through a\nbottom-up approach of inverting the deductive process.\n• ILP methods naturally generate new predicates with which concise new theories can be\nexpressed and show promise as general-purpose scientiﬁc theory formation systems.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nAlthough the use of prior knowledge in learning would seem to be a natural topic for philoso-\nphers of science, little formal work was done until quite recently. Fact, Fiction, and Forecast,\nby the philosopher Nelson Goodman (1954), refuted the earlier supposition that induction\nwas simply a matter of seeing enough examples of some universally quantiﬁed proposition\nand then adopting it as a hypothesis. Consider, for example, the hypothesis “All emeralds are\ngrue,” where grue means “green if observed before time t, but blue if observed thereafter.”\nAt any time up to t, we might have observed millions of instances conﬁrming the rule that\nemeralds are grue, and no disconﬁrming instances, and yet we are unwilling to adopt the rule.\nThis can be explained only by appeal to the role of relevant prior knowledge in the induction\nprocess. Goodman proposes a variety of different kinds of prior knowledge that might be use-\nful, including a version of determinations called overhypotheses. Unfortunately, Goodman’s\nideas were never pursued in machine learning.",
  "process. Goodman proposes a variety of different kinds of prior knowledge that might be use-\nful, including a version of determinations called overhypotheses. Unfortunately, Goodman’s\nideas were never pursued in machine learning.\nThe current-best-hypothesis approach is an old idea in philosophy (Mill, 1843). Early\nwork in cognitive psychology also suggested that it is a natural form of concept learning in\nhumans (Bruner et al., 1957). In AI, the approach is most closely associated with the work\nof Patrick Winston, whose Ph.D. thesis (Winston, 1970) addressed the problem of learning\ndescriptions of complex objects. The version space method (Mitchell, 1977, 1982) takes\na different approach, maintaining the set of all consistent hypotheses and eliminating those\nfound to be inconsistent with new examples. The approach was used in the Meta-DENDRAL Bibliographical and Historical Notes\n799\nexpert system for chemistry (Buchanan and Mitchell, 1978), and later in Mitchell’s (1983)\nLEX system, which learns to solve calculus problems. A third inﬂuential thread was formed\nby the work of Michalski and colleagues on the AQ series of algorithms, which learned sets\nof logical rules (Michalski, 1969; Michalski et al., 1986).\nEBL had its roots in the techniques used by the STRIPS planner (Fikes et al., 1972).\nWhen a plan was constructed, a generalized version of it was saved in a plan library and\nused in later planning as a macro-operator. Similar ideas appeared in Anderson’s ACT*\narchitecture, under the heading of knowledge compilation (Anderson, 1983), and in the\nSOAR architecture, as chunking (Laird et al., 1986). Schema acquisition (DeJong, 1981),\nanalytical generalization (Mitchell, 1982), and constraint-based generalization (Minton,\n1984) were immediate precursors of the rapid growth of interest in EBL stimulated by the\npapers of Mitchell et al. (1986) and DeJong and Mooney (1986). Hirsh (1987) introduced\nthe EBL algorithm described in the text, showing how it could be incorporated directly into a\nlogic programming system. Van Harmelen and Bundy (1988) explain EBL as a variant of the\npartial evaluation method used in program analysis systems (Jones et al., 1993).\nInitial enthusiasm for EBL was tempered by Minton’s ﬁnding (1988) that, without ex-\ntensive extra work, EBL could easily slow down a program signiﬁcantly. Formal probabilistic\nanalysis of the expected payoff of EBL can be found in Greiner (1989) and Subramanian and",
  "tensive extra work, EBL could easily slow down a program signiﬁcantly. Formal probabilistic\nanalysis of the expected payoff of EBL can be found in Greiner (1989) and Subramanian and\nFeldman (1990). An excellent survey of early work on EBL appears in Dietterich (1990).\nInstead of using examples as foci for generalization, one can use them directly to solve\nnew problems, in a process known as analogical reasoning. This form of reasoning ranges\nANALOGICAL\nREASONING\nfrom a form of plausible reasoning based on degree of similarity (Gentner, 1983), through\na form of deductive inference based on determinations but requiring the participation of the\nexample (Davies and Russell, 1987), to a form of “lazy” EBL that tailors the direction of\ngeneralization of the old example to ﬁt the needs of the new problem. This latter form of\nanalogical reasoning is found most commonly in case-based reasoning (Kolodner, 1993)\nand derivational analogy (Veloso and Carbonell, 1993).\nRelevance information in the form of functional dependencies was ﬁrst developed in\nthe database community, where it is used to structure large sets of attributes into manage-\nable subsets.\nFunctional dependencies were used for analogical reasoning by Carbonell\nand Collins (1973) and rediscovered and given a full logical analysis by Davies and Rus-\nsell (Davies, 1985; Davies and Russell, 1987). Their role as prior knowledge in inductive\nlearning was explored by Russell and Grosof (1987). The equivalence of determinations to\na restricted-vocabulary hypothesis space was proved in Russell (1988). Learning algorithms\nfor determinations and the improved performance obtained by RBDTL were ﬁrst shown in\nthe FOCUS algorithm, due to Almuallim and Dietterich (1991). Tadepalli (1993) describes a\nvery ingenious algorithm for learning with determinations that shows large improvements in\nlearning speed.\nThe idea that inductive learning can be performed by inverse deduction can be traced\nto W. S. Jevons (1874), who wrote, “The study both of Formal Logic and of the Theory of\nProbabilities has led me to adopt the opinion that there is no such thing as a distinct method\nof induction as contrasted with deduction, but that induction is simply an inverse employ-\nment of deduction.” Computational investigations began with the remarkable Ph.D. thesis by 800\nChapter\n19.\nKnowledge in Learning\nGordon Plotkin (1971) at Edinburgh. Although Plotkin developed many of the theorems and",
  "ment of deduction.” Computational investigations began with the remarkable Ph.D. thesis by 800\nChapter\n19.\nKnowledge in Learning\nGordon Plotkin (1971) at Edinburgh. Although Plotkin developed many of the theorems and\nmethods that are in current use in ILP, he was discouraged by some undecidability results for\ncertain subproblems in induction. MIS (Shapiro, 1981) reintroduced the problem of learning\nlogic programs, but was seen mainly as a contribution to the theory of automated debug-\nging. Work on rule induction, such as the ID3 (Quinlan, 1986) and CN2 (Clark and Niblett,\n1989) systems, led to FOIL (Quinlan, 1990), which for the ﬁrst time allowed practical induc-\ntion of relational rules. The ﬁeld of relational learning was reinvigorated by Muggleton and\nBuntine (1988), whose CIGOL program incorporated a slightly incomplete version of inverse\nresolution and was capable of generating new predicates. The inverse resolution method also\nappears in (Russell, 1986), with a simple algorithm given in a footnote. The next major sys-\ntem was GOLEM (Muggleton and Feng, 1990), which uses a covering algorithm based on\nPlotkin’s concept of relative least general generalization. ITOU (Rouveirol and Puget, 1989)\nand CLINT (De Raedt, 1992) were other systems of that era. More recently, PROGOL (Mug-\ngleton, 1995) has taken a hybrid (top-down and bottom-up) approach to inverse entailment\nand has been applied to a number of practical problems, particularly in biology and natural\nlanguage processing. Muggleton (2000) describes an extension of PROGOL to handle uncer-\ntainty in the form of stochastic logic programs.\nA formal analysis of ILP methods appears in Muggleton (1991), a large collection of\npapers in Muggleton (1992), and a collection of techniques and applications in the book\nby Lavrauc and Duzeroski (1994). Page and Srinivasan (2002) give a more recent overview of\nthe ﬁeld’s history and challenges for the future. Early complexity results by Haussler (1989)\nsuggested that learning ﬁrst-order sentences was intractible. However, with better understand-\ning of the importance of syntactic restrictions on clauses, positive results have been obtained\neven for clauses with recursion (Duzeroski et al., 1992). Learnability results for ILP are\nsurveyed by Kietz and Duzeroski (1994) and Cohen and Page (1995).\nAlthough ILP now seems to be the dominant approach to constructive induction, it has\nnot been the only approach taken. So-called discovery systems aim to model the process",
  "surveyed by Kietz and Duzeroski (1994) and Cohen and Page (1995).\nAlthough ILP now seems to be the dominant approach to constructive induction, it has\nnot been the only approach taken. So-called discovery systems aim to model the process\nDISCOVERY SYSTEM\nof scientiﬁc discovery of new concepts, usually by a direct search in the space of concept\ndeﬁnitions. Doug Lenat’s Automated Mathematician, or AM (Davis and Lenat, 1982), used\ndiscovery heuristics expressed as expert system rules to guide its search for concepts and\nconjectures in elementary number theory. Unlike most systems designed for mathematical\nreasoning, AM lacked a concept of proof and could only make conjectures. It rediscovered\nGoldbach’s conjecture and the Unique Prime Factorization theorem. AM’s architecture was\ngeneralized in the EURISKO system (Lenat, 1983) by adding a mechanism capable of rewrit-\ning the system’s own discovery heuristics. EURISKO was applied in a number of areas other\nthan mathematical discovery, although with less success than AM. The methodology of AM\nand EURISKO has been controversial (Ritchie and Hanna, 1984; Lenat and Brown, 1984).\nAnother class of discovery systems aims to operate with real scientiﬁc data to ﬁnd new\nlaws. The systems DALTON, GLAUBER, and STAHL (Langley et al., 1987) are rule-based\nsystems that look for quantitative relationships in experimental data from physical systems;\nin each case, the system has been able to recapitulate a well-known discovery from the his-\ntory of science. Discovery systems based on probabilistic techniques—especially clustering\nalgorithms that discover new categories—are discussed in Chapter 20. Exercises\n801\nEXERCISES\n19.1\nShow, by translating into conjunctive normal form and applying resolution, that the\nconclusion drawn on page 784 concerning Brazilians is sound.\n19.2\nFor each of the following determinations, write down the logical representation and\nexplain why the determination is true (if it is):\na. Design and denomination determine the mass of a coin.\nb. For a given program, input determines output.\nc. Climate, food intake, exercise, and metabolism determine weight gain and loss.\nd. Baldness is determined by the baldness (or lack thereof) of one’s maternal grandfather.\n19.3\nWould a probabilistic version of determinations be useful? Suggest a deﬁnition.\n19.4\nFill in the missing values for the clauses C1 or C2 (or both) in the following sets of\nclauses, given that C is the resolvent of C1 and C2:",
  "19.3\nWould a probabilistic version of determinations be useful? Suggest a deﬁnition.\n19.4\nFill in the missing values for the clauses C1 or C2 (or both) in the following sets of\nclauses, given that C is the resolvent of C1 and C2:\na. C = True ⇒P(A, B), C1 = P(x, y) ⇒Q(x, y), C2 =??.\nb. C = True ⇒P(A, B), C1 =??, C2 =??.\nc. C = P(x, y) ⇒P(x, f(y)), C1 =??, C2 =??.\nIf there is more than one possible solution, provide one example of each different kind.\n19.5\nSuppose one writes a logic program that carries out a resolution inference step. That\nis, let Resolve(c1, c2, c) succeed if c is the result of resolving c1 and c2. Normally, Resolve\nwould be used as part of a theorem prover by calling it with c1 and c2 instantiated to par-\nticular clauses, thereby generating the resolvent c. Now suppose instead that we call it with\nc instantiated and c1 and c2 uninstantiated. Will this succeed in generating the appropriate\nresults of an inverse resolution step? Would you need any special modiﬁcations to the logic\nprogramming system for this to work?\n19.6\nSuppose that FOIL is considering adding a literal to a clause using a binary predicate\nP and that previous literals (including the head of the clause) contain ﬁve different variables.\na. How many functionally different literals can be generated? Two literals are functionally\nidentical if they differ only in the names of the new variables that they contain.\nb. Can you ﬁnd a general formula for the number of different literals with a predicate of\narity r when there are n variables previously used?\nc. Why does FOIL not allow literals that contain no previously used variables?\n19.7\nUsing the data from the family tree in Figure 19.11, or a subset thereof, apply the FOIL\nalgorithm to learn a deﬁnition for the Ancestor predicate. 20\nLEARNING\nPROBABILISTIC MODELS\nIn which we view learning as a form of uncertain reasoning from observations.\nChapter 13 pointed out the prevalence of uncertainty in real environments. Agents can handle\nuncertainty by using the methods of probability and decision theory, but ﬁrst they must learn\ntheir probabilistic theories of the world from experience. This chapter explains how they\ncan do that, by formulating the learning task itself as a process of probabilistic inference\n(Section 20.1). We will see that a Bayesian view of learning is extremely powerful, providing\ngeneral solutions to the problems of noise, overﬁtting, and optimal prediction. It also takes",
  "(Section 20.1). We will see that a Bayesian view of learning is extremely powerful, providing\ngeneral solutions to the problems of noise, overﬁtting, and optimal prediction. It also takes\ninto account the fact that a less-than-omniscient agent can never be certain about which theory\nof the world is correct, yet must still make decisions by using some theory of the world.\nWe describe methods for learning probability models—primarily Bayesian networks—\nin Sections 20.2 and 20.3. Some of the material in this chapter is fairly mathematical, al-\nthough the general lessons can be understood without plunging into the details. It may beneﬁt\nthe reader to review Chapters 13 and 14 and peek at Appendix A.\n20.1\nSTATISTICAL LEARNING\nThe key concepts in this chapter, just as in Chapter 18, are data and hypotheses. Here, the\ndata are evidence—that is, instantiations of some or all of the random variables describing the\ndomain. The hypotheses in this chapter are probabilistic theories of how the domain works,\nincluding logical theories as a special case.\nConsider a simple example. Our favorite Surprise candy comes in two ﬂavors: cherry\n(yum) and lime (ugh). The manufacturer has a peculiar sense of humor and wraps each piece\nof candy in the same opaque wrapper, regardless of ﬂavor. The candy is sold in very large\nbags, of which there are known to be ﬁve kinds—again, indistinguishable from the outside:\nh1: 100% cherry,\nh2: 75% cherry + 25% lime,\nh3: 50% cherry + 50% lime,\nh4: 25% cherry + 75% lime,\nh5: 100% lime .\n802 Section 20.1.\nStatistical Learning\n803\nGiven a new bag of candy, the random variable H (for hypothesis) denotes the type of the\nbag, with possible values h1 through h5. H is not directly observable, of course. As the\npieces of candy are opened and inspected, data are revealed—D1, D2, . . ., DN, where each\nDi is a random variable with possible values cherry and lime. The basic task faced by the\nagent is to predict the ﬂavor of the next piece of candy.1 Despite its apparent triviality, this\nscenario serves to introduce many of the major issues. The agent really does need to infer a\ntheory of its world, albeit a very simple one.\nBayesian learning simply calculates the probability of each hypothesis, given the data,\nBAYESIAN LEARNING\nand makes predictions on that basis. That is, the predictions are made by using all the hy-\npotheses, weighted by their probabilities, rather than by using just a single “best” hypothesis.",
  "BAYESIAN LEARNING\nand makes predictions on that basis. That is, the predictions are made by using all the hy-\npotheses, weighted by their probabilities, rather than by using just a single “best” hypothesis.\nIn this way, learning is reduced to probabilistic inference. Let D represent all the data, with\nobserved value d; then the probability of each hypothesis is obtained by Bayes’ rule:\nP(hi | d) = αP(d | hi)P(hi) .\n(20.1)\nNow, suppose we want to make a prediction about an unknown quantity X. Then we have\nP(X | d) =\n\f\ni\nP(X | d, hi)P(hi | d) =\n\f\ni\nP(X | hi)P(hi | d) ,\n(20.2)\nwhere we have assumed that each hypothesis determines a probability distribution over X.\nThis equation shows that predictions are weighted averages over the predictions of the indi-\nvidual hypotheses. The hypotheses themselves are essentially “intermediaries” between the\nraw data and the predictions. The key quantities in the Bayesian approach are the hypothesis\nprior, P(hi), and the likelihood of the data under each hypothesis, P(d | hi).\nHYPOTHESIS PRIOR\nLIKELIHOOD\nFor our candy example, we will assume for the time being that the prior distribution\nover h1, . . . , h5 is given by ⟨0.1, 0.2, 0.4, 0.2, 0.1⟩, as advertised by the manufacturer. The\nlikelihood of the data is calculated under the assumption that the observations are i.i.d. (see\npage 708), so that\nP(d | hi) =\n\u0019\nj\nP(dj | hi) .\n(20.3)\nFor example, suppose the bag is really an all-lime bag (h5) and the ﬁrst 10 candies are all\nlime; then P(d | h3) is 0.510, because half the candies in an h3 bag are lime.2 Figure 20.1(a)\nshows how the posterior probabilities of the ﬁve hypotheses change as the sequence of 10\nlime candies is observed. Notice that the probabilities start out at their prior values, so h3\nis initially the most likely choice and remains so after 1 lime candy is unwrapped. After 2\nlime candies are unwrapped, h4 is most likely; after 3 or more, h5 (the dreaded all-lime bag)\nis the most likely. After 10 in a row, we are fairly certain of our fate. Figure 20.1(b) shows\nthe predicted probability that the next candy is lime, based on Equation (20.2). As we would\nexpect, it increases monotonically toward 1.\n1 Statistically sophisticated readers will recognize this scenario as a variant of the urn-and-ball setup. We ﬁnd\nurns and balls less compelling than candy; furthermore, candy lends itself to other tasks, such as deciding whether\nto trade the bag with a friend—see Exercise 20.2.",
  "urns and balls less compelling than candy; furthermore, candy lends itself to other tasks, such as deciding whether\nto trade the bag with a friend—see Exercise 20.2.\n2 We stated earlier that the bags of candy are very large; otherwise, the i.i.d. assumption fails to hold. Technically,\nit is more correct (but less hygienic) to rewrap each candy after inspection and return it to the bag. 804\nChapter\n20.\nLearning Probabilistic Models\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0\n 2\n 4\n 6\n 8\n 10\nPosterior probability of hypothesis\nNumber of observations in d\nP(h1 | d)\nP(h2 | d)\nP(h3 | d)\nP(h4 | d)\nP(h5 | d)\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 2\n 4\n 6\n 8\n 10\nProbability that next candy is lime\nNumber of observations in d\n(a)\n(b)\nFigure 20.1\n(a) Posterior probabilities P(hi | d1, . . . , dN) from Equation (20.1).\nThe\nnumber of observations N ranges from 1 to 10, and each observation is of a lime candy.\n(b) Bayesian prediction P(dN+1 = lime | d1, . . . , dN) from Equation (20.2).\nThe example shows that the Bayesian prediction eventually agrees with the true hy-\npothesis. This is characteristic of Bayesian learning. For any ﬁxed prior that does not rule\nout the true hypothesis, the posterior probability of any false hypothesis will, under certain\ntechnical conditions, eventually vanish. This happens simply because the probability of gen-\nerating “uncharacteristic” data indeﬁnitely is vanishingly small. (This point is analogous to\none made in the discussion of PAC learning in Chapter 18.) More important, the Bayesian\nprediction is optimal, whether the data set be small or large. Given the hypothesis prior, any\nother prediction is expected to be correct less often.\nThe optimality of Bayesian learning comes at a price, of course. For real learning\nproblems, the hypothesis space is usually very large or inﬁnite, as we saw in Chapter 18. In\nsome cases, the summation in Equation (20.2) (or integration, in the continuous case) can be\ncarried out tractably, but in most cases we must resort to approximate or simpliﬁed methods.\nA very common approximation—one that is usually adopted in science—is to make pre-\ndictions based on a single most probable hypothesis—that is, an hi that maximizes P(hi | d).\nThis is often called a maximum a posteriori or MAP (pronounced “em-ay-pee”) hypothesis.\nMAXIMUM A\nPOSTERIORI\nPredictions made according to an MAP hypothesis hMAP are approximately Bayesian to the\nextent that P(X | d) ≈P(X | hMAP). In our candy example, hMAP = h5 after three lime can-",
  "MAXIMUM A\nPOSTERIORI\nPredictions made according to an MAP hypothesis hMAP are approximately Bayesian to the\nextent that P(X | d) ≈P(X | hMAP). In our candy example, hMAP = h5 after three lime can-\ndies in a row, so the MAP learner then predicts that the fourth candy is lime with probability\n1.0—a much more dangerous prediction than the Bayesian prediction of 0.8 shown in Fig-\nure 20.1(b). As more data arrive, the MAP and Bayesian predictions become closer, because\nthe competitors to the MAP hypothesis become less and less probable.\nAlthough our example doesn’t show it, ﬁnding MAP hypotheses is often much easier\nthan Bayesian learning, because it requires solving an optimization problem instead of a large\nsummation (or integration) problem. We will see examples of this later in the chapter. Section 20.1.\nStatistical Learning\n805\nIn both Bayesian learning and MAP learning, the hypothesis prior P(hi) plays an im-\nportant role. We saw in Chapter 18 that overﬁtting can occur when the hypothesis space\nis too expressive, so that it contains many hypotheses that ﬁt the data set well. Rather than\nplacing an arbitrary limit on the hypotheses to be considered, Bayesian and MAP learning\nmethods use the prior to penalize complexity. Typically, more complex hypotheses have a\nlower prior probability—in part because there are usually many more complex hypotheses\nthan simple hypotheses. On the other hand, more complex hypotheses have a greater capac-\nity to ﬁt the data. (In the extreme case, a lookup table can reproduce the data exactly with\nprobability 1.) Hence, the hypothesis prior embodies a tradeoff between the complexity of a\nhypothesis and its degree of ﬁt to the data.\nWe can see the effect of this tradeoff most clearly in the logical case, where H contains\nonly deterministic hypotheses. In that case, P(d | hi) is 1 if hi is consistent and 0 otherwise.\nLooking at Equation (20.1), we see that hMAP will then be the simplest logical theory that\nis consistent with the data. Therefore, maximum a posteriori learning provides a natural\nembodiment of Ockham’s razor.\nAnother insight into the tradeoff between complexity and degree of ﬁt is obtained by\ntaking the logarithm of Equation (20.1). Choosing hMAP to maximize P(d | hi)P(hi) is\nequivalent to minimizing\n−log2 P(d | hi) −log2 P(hi) .\nUsing the connection between information encoding and probability that we introduced in\nChapter 18.3.4, we see that the −log2 P(hi) term equals the number of bits required to spec-",
  "equivalent to minimizing\n−log2 P(d | hi) −log2 P(hi) .\nUsing the connection between information encoding and probability that we introduced in\nChapter 18.3.4, we see that the −log2 P(hi) term equals the number of bits required to spec-\nify the hypothesis hi. Furthermore, −log2 P(d | hi) is the additional number of bits required\nto specify the data, given the hypothesis. (To see this, consider that no bits are required\nif the hypothesis predicts the data exactly—as with h5 and the string of lime candies—and\nlog2 1 = 0.) Hence, MAP learning is choosing the hypothesis that provides maximum com-\npression of the data. The same task is addressed more directly by the minimum description\nlength, or MDL, learning method. Whereas MAP learning expresses simplicity by assigning\nhigher probabilities to simpler hypotheses, MDL expresses it directly by counting the bits in\na binary encoding of the hypotheses and data.\nA ﬁnal simpliﬁcation is provided by assuming a uniform prior over the space of hy-\npotheses. In that case, MAP learning reduces to choosing an hi that maximizes P(d | hi).\nThis is called a maximum-likelihood (ML) hypothesis, hML. Maximum-likelihood learning\nMAXIMUM-\nLIKELIHOOD\nis very common in statistics, a discipline in which many researchers distrust the subjective\nnature of hypothesis priors. It is a reasonable approach when there is no reason to prefer one\nhypothesis over another a priori—for example, when all hypotheses are equally complex. It\nprovides a good approximation to Bayesian and MAP learning when the data set is large,\nbecause the data swamps the prior distribution over hypotheses, but it has problems (as we\nshall see) with small data sets. 806\nChapter\n20.\nLearning Probabilistic Models\n20.2\nLEARNING WITH COMPLETE DATA\nThe general task of learning a probability model, given data that are assumed to be generated\nfrom that model, is called density estimation. (The term applied originally to probability\nDENSITY ESTIMATION\ndensity functions for continuous variables, but is used now for discrete distributions too.)\nThis section covers the simplest case, where we have complete data. Data are com-\nCOMPLETE DATA\nplete when each data point contains values for every variable in the probability model being\nlearned. We focus on parameter learning—ﬁnding the numerical parameters for a proba-\nPARAMETER\nLEARNING\nbility model whose structure is ﬁxed. For example, we might be interested in learning the",
  "learned. We focus on parameter learning—ﬁnding the numerical parameters for a proba-\nPARAMETER\nLEARNING\nbility model whose structure is ﬁxed. For example, we might be interested in learning the\nconditional probabilities in a Bayesian network with a given structure. We will also look\nbrieﬂy at the problem of learning structure and at nonparametric density estimation.\n20.2.1\nMaximum-likelihood parameter learning: Discrete models\nSuppose we buy a bag of lime and cherry candy from a new manufacturer whose lime–cherry\nproportions are completely unknown; the fraction could be anywhere between 0 and 1. In\nthat case, we have a continuum of hypotheses. The parameter in this case, which we call\nθ, is the proportion of cherry candies, and the hypothesis is hθ. (The proportion of limes is\njust 1 −θ.) If we assume that all proportions are equally likely a priori, then a maximum-\nlikelihood approach is reasonable. If we model the situation with a Bayesian network, we\nneed just one random variable, Flavor (the ﬂavor of a randomly chosen candy from the bag).\nIt has values cherry and lime, where the probability of cherry is θ (see Figure 20.2(a)). Now\nsuppose we unwrap N candies, of which c are cherries and ℓ= N −c are limes. According\nto Equation (20.3), the likelihood of this particular data set is\nP(d | hθ) =\nN\n\u0019\nj = 1\nP(dj | hθ) = θc · (1 −θ)ℓ.\nThe maximum-likelihood hypothesis is given by the value of θ that maximizes this expres-\nsion. The same value is obtained by maximizing the log likelihood,\nLOG LIKELIHOOD\nL(d | hθ) = log P(d | hθ) =\nN\n\f\nj = 1\nlog P(dj | hθ) = c log θ + ℓlog(1 −θ) .\n(By taking logarithms, we reduce the product to a sum over the data, which is usually easier\nto maximize.) To ﬁnd the maximum-likelihood value of θ, we differentiate L with respect to\nθ and set the resulting expression to zero:\ndL(d | hθ)\ndθ\n= c\nθ −\nℓ\n1 −θ = 0\n⇒\nθ =\nc\nc + ℓ= c\nN .\nIn English, then, the maximum-likelihood hypothesis hML asserts that the actual proportion\nof cherries in the bag is equal to the observed proportion in the candies unwrapped so far!\nIt appears that we have done a lot of work to discover the obvious. In fact, though,\nwe have laid out one standard method for maximum-likelihood parameter learning, a method\nwith broad applicability: Section 20.2.\nLearning with Complete Data\n807\nFlavor\nP(F=cherry)\n(a)\nθ\nP(F=cherry)\nFlavor\nWrapper\n(b)\nθ\nF\ncherry\nlime\nP(W=red | F)\nθ1\nθ2\nFigure 20.2\n(a) Bayesian network model for the case of candies with an unknown propor-",
  "with broad applicability: Section 20.2.\nLearning with Complete Data\n807\nFlavor\nP(F=cherry)\n(a)\nθ\nP(F=cherry)\nFlavor\nWrapper\n(b)\nθ\nF\ncherry\nlime\nP(W=red | F)\nθ1\nθ2\nFigure 20.2\n(a) Bayesian network model for the case of candies with an unknown propor-\ntion of cherries and limes. (b) Model for the case where the wrapper color depends (proba-\nbilistically) on the candy ﬂavor.\n1. Write down an expression for the likelihood of the data as a function of the parameter(s).\n2. Write down the derivative of the log likelihood with respect to each parameter.\n3. Find the parameter values such that the derivatives are zero.\nThe trickiest step is usually the last. In our example, it was trivial, but we will see that in\nmany cases we need to resort to iterative solution algorithms or other numerical optimization\ntechniques, as described in Chapter 4. The example also illustrates a signiﬁcant problem\nwith maximum-likelihood learning in general: when the data set is small enough that some\nevents have not yet been observed—for instance, no cherry candies—the maximum-likelihood\nhypothesis assigns zero probability to those events. Various tricks are used to avoid this\nproblem, such as initializing the counts for each event to 1 instead of 0.\nLet us look at another example. Suppose this new candy manufacturer wants to give a\nlittle hint to the consumer and uses candy wrappers colored red and green. The Wrapper for\neach candy is selected probabilistically, according to some unknown conditional distribution,\ndepending on the ﬂavor. The corresponding probability model is shown in Figure 20.2(b).\nNotice that it has three parameters: θ, θ1, and θ2. With these parameters, the likelihood of\nseeing, say, a cherry candy in a green wrapper can be obtained from the standard semantics\nfor Bayesian networks (page 513):\nP(Flavor = cherry, Wrapper = green | hθ,θ1,θ2)\n= P(Flavor = cherry | hθ,θ1,θ2)P(Wrapper = green | Flavor = cherry, hθ,θ1,θ2)\n= θ · (1 −θ1) .\nNow we unwrap N candies, of which c are cherries and ℓare limes. The wrapper counts are\nas follows: rc of the cherries have red wrappers and gc have green, while rℓof the limes have\nred and gℓhave green. The likelihood of the data is given by\nP(d | hθ,θ1,θ2) = θc(1 −θ)ℓ· θrc\n1 (1 −θ1)gc · θrℓ\n2 (1 −θ2)gℓ. 808\nChapter\n20.\nLearning Probabilistic Models\nThis looks pretty horrible, but taking logarithms helps:\nL = [c log θ + ℓlog(1 −θ)] + [rc log θ1 + gc log(1 −θ1)] + [rℓlog θ2 + gℓlog(1 −θ2)] .",
  "1 (1 −θ1)gc · θrℓ\n2 (1 −θ2)gℓ. 808\nChapter\n20.\nLearning Probabilistic Models\nThis looks pretty horrible, but taking logarithms helps:\nL = [c log θ + ℓlog(1 −θ)] + [rc log θ1 + gc log(1 −θ1)] + [rℓlog θ2 + gℓlog(1 −θ2)] .\nThe beneﬁt of taking logs is clear: the log likelihood is the sum of three terms, each of which\ncontains a single parameter. When we take derivatives with respect to each parameter and set\nthem to zero, we get three independent equations, each containing just one parameter:\n∂L\n∂θ =\nc\nθ −\nℓ\n1−θ = 0\n⇒\nθ =\nc\nc+ℓ\n∂L\n∂θ1 =\nrc\nθ1 −\ngc\n1−θ1 = 0\n⇒\nθ1 =\nrc\nrc+gc\n∂L\n∂θ2 =\nrℓ\nθ2 −\ngℓ\n1−θ2 = 0\n⇒\nθ2 =\nrℓ\nrℓ+gℓ.\nThe solution for θ is the same as before. The solution for θ1, the probability that a cherry\ncandy has a red wrapper, is the observed fraction of cherry candies with red wrappers, and\nsimilarly for θ2.\nThese results are very comforting, and it is easy to see that they can be extended to any\nBayesian network whose conditional probabilities are represented as tables. The most impor-\ntant point is that, with complete data, the maximum-likelihood parameter learning problem\nfor a Bayesian network decomposes into separate learning problems, one for each parameter.\n(See Exercise 20.6 for the nontabulated case, where each parameter affects several conditional\nprobabilities.) The second point is that the parameter values for a variable, given its parents,\nare just the observed frequencies of the variable values for each setting of the parent values.\nAs before, we must be careful to avoid zeroes when the data set is small.\n20.2.2\nNaive Bayes models\nProbably the most common Bayesian network model used in machine learning is the naive\nBayes model ﬁrst introduced on page 499. In this model, the “class” variable C (which is to\nbe predicted) is the root and the “attribute” variables Xi are the leaves. The model is “naive”\nbecause it assumes that the attributes are conditionally independent of each other, given the\nclass. (The model in Figure 20.2(b) is a naive Bayes model with class Flavor and just one\nattribute, Wrapper.) Assuming Boolean variables, the parameters are\nθ = P(C = true), θi1 = P(Xi = true | C = true), θi2 = P(Xi = true | C = false).\nThe maximum-likelihood parameter values are found in exactly the same way as for Fig-\nure 20.2(b). Once the model has been trained in this way, it can be used to classify new exam-\nples for which the class variable C is unobserved. With observed attribute values x1, . . . , xn,",
  "ure 20.2(b). Once the model has been trained in this way, it can be used to classify new exam-\nples for which the class variable C is unobserved. With observed attribute values x1, . . . , xn,\nthe probability of each class is given by\nP(C | x1, . . . , xn) = α P(C)\n\u0019\ni\nP(xi | C) .\nA deterministic prediction can be obtained by choosing the most likely class. Figure 20.3\nshows the learning curve for this method when it is applied to the restaurant problem from\nChapter 18. The method learns fairly well but not as well as decision-tree learning; this is\npresumably because the true hypothesis—which is a decision tree—is not representable ex-\nactly using a naive Bayes model. Naive Bayes learning turns out to do surprisingly well in a\nwide range of applications; the boosted version (Exercise 20.4) is one of the most effective Section 20.2.\nLearning with Complete Data\n809\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n20\n40\n60\n80\n100\nProportion correct on test set\nTraining set size\nDecision tree\nNaive Bayes\nFigure 20.3\nThe learning curve for naive Bayes learning applied to the restaurant problem\nfrom Chapter 18; the learning curve for decision-tree learning is shown for comparison.\ngeneral-purpose learning algorithms. Naive Bayes learning scales well to very large prob-\nlems: with n Boolean attributes, there are just 2n + 1 parameters, and no search is required\nto ﬁnd hML, the maximum-likelihood naive Bayes hypothesis. Finally, naive Bayes learning\nsystems have no difﬁculty with noisy or missing data and can give probabilistic predictions\nwhen appropriate.\n20.2.3\nMaximum-likelihood parameter learning: Continuous models\nContinuous probability models such as the linear Gaussian model were introduced in Sec-\ntion 14.3. Because continuous variables are ubiquitous in real-world applications, it is impor-\ntant to know how to learn the parameters of continuous models from data. The principles for\nmaximum-likelihood learning are identical in the continuous and discrete cases.\nLet us begin with a very simple case: learning the parameters of a Gaussian density\nfunction on a single variable. That is, the data are generated as follows:\nP(x) =\n1\n√\n2πσ e−(x−μ)2\n2σ2\n.\nThe parameters of this model are the mean μ and the standard deviation σ. (Notice that the\nnormalizing “constant” depends on σ, so we cannot ignore it.) Let the observed values be\nx1, . . . , xN. Then the log likelihood is\nL =\nN\n\f\nj = 1\nlog\n1\n√\n2πσ\ne−\n(xj−μ)2\n2σ2\n= N(−log\n√\n2π −log σ) −\nN\n\f\nj = 1\n(xj −μ)2\n2σ2\n.",
  "normalizing “constant” depends on σ, so we cannot ignore it.) Let the observed values be\nx1, . . . , xN. Then the log likelihood is\nL =\nN\n\f\nj = 1\nlog\n1\n√\n2πσ\ne−\n(xj−μ)2\n2σ2\n= N(−log\n√\n2π −log σ) −\nN\n\f\nj = 1\n(xj −μ)2\n2σ2\n.\nSetting the derivatives to zero as usual, we obtain\n∂L\n∂μ = −1\nσ2\n\u0002N\nj=1(xj −μ) = 0\n⇒\nμ =\nP\nj xj\nN\n∂L\n∂σ = −N\nσ + 1\nσ3\n\u0002N\nj=1(xj −μ)2 = 0\n⇒\nσ =\n\t P\nj(xj−μ)2\nN\n.\n(20.4)\nThat is, the maximum-likelihood value of the mean is the sample average and the maximum-\nlikelihood value of the standard deviation is the square root of the sample variance. Again,\nthese are comforting results that conﬁrm “commonsense” practice. 810\nChapter\n20.\nLearning Probabilistic Models\n0\n0.2\n0.4\n0.6\n0.8\n1\nx\n0 0.20.40.60.81\ny\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nP(y |x)\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n1\ny\nx\n(a)\n(b)\nFigure 20.4\n(a) A linear Gaussian model described as y = θ1x + θ2 plus Gaussian noise\nwith ﬁxed variance. (b) A set of 50 data points generated from this model.\nNow consider a linear Gaussian model with one continuous parent X and a continuous\nchild Y . As explained on page 520, Y has a Gaussian distribution whose mean depends\nlinearly on the value of X and whose standard deviation is ﬁxed. To learn the conditional\ndistribution P(Y | X), we can maximize the conditional likelihood\nP(y | x) =\n1\n√\n2πσe−(y−(θ1x+θ2))2\n2σ2\n.\n(20.5)\nHere, the parameters are θ1, θ2, and σ. The data are a collection of (xj, yj) pairs, as illustrated\nin Figure 20.4. Using the usual methods (Exercise 20.5), we can ﬁnd the maximum-likelihood\nvalues of the parameters. The point here is different. If we consider just the parameters θ1\nand θ2 that deﬁne the linear relationship between x and y, it becomes clear that maximizing\nthe log likelihood with respect to these parameters is the same as minimizing the numerator\n(y −(θ1x + θ2))2 in the exponent of Equation (20.5). This is the L2 loss, the squared er-\nror between the actual value y and the prediction θ1x + θ2. This is the quantity minimized\nby the standard linear regression procedure described in Section 18.6. Now we can under-\nstand why: minimizing the sum of squared errors gives the maximum-likelihood straight-line\nmodel, provided that the data are generated with Gaussian noise of ﬁxed variance.\n20.2.4\nBayesian parameter learning\nMaximum-likelihood learning gives rise to some very simple procedures, but it has some\nserious deﬁciencies with small data sets. For example, after seeing one cherry candy, the",
  "20.2.4\nBayesian parameter learning\nMaximum-likelihood learning gives rise to some very simple procedures, but it has some\nserious deﬁciencies with small data sets. For example, after seeing one cherry candy, the\nmaximum-likelihood hypothesis is that the bag is 100% cherry (i.e., θ = 1.0). Unless one’s\nhypothesis prior is that bags must be either all cherry or all lime, this is not a reasonable\nconclusion. It is more likely that the bag is a mixture of lime and cherry. The Bayesian\napproach to parameter learning starts by deﬁning a prior probability distribution over the\npossible hypotheses. We call this the hypothesis prior. Then, as data arrives, the posterior\nHYPOTHESIS PRIOR\nprobability distribution is updated. Section 20.2.\nLearning with Complete Data\n811\n0\n0.5\n1\n1.5\n2\n2.5\n0\n0.2\n0.4\n0.6\n0.8\n1\nP(Θ = θ)\nParameter θ\n[1,1]\n[2,2]\n[5,5]\n0\n1\n2\n3\n4\n5\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nP(Θ = θ)\nParameter θ\n[3,1]\n[6,2]\n[30,10]\n(a)\n(b)\nFigure 20.5\nExamples of the beta[a, b] distribution for different values of [a, b].\nThe candy example in Figure 20.2(a) has one parameter, θ: the probability that a ran-\ndomly selected piece of candy is cherry-ﬂavored. In the Bayesian view, θ is the (unknown)\nvalue of a random variable Θ that deﬁnes the hypothesis space; the hypothesis prior is just\nthe prior distribution P(Θ). Thus, P(Θ = θ) is the prior probability that the bag has a fraction\nθ of cherry candies.\nIf the parameter θ can be any value between 0 and 1, then P(Θ) must be a continuous\ndistribution that is nonzero only between 0 and 1 and that integrates to 1. The uniform density\nP(θ) = Uniform[0, 1](θ) is one candidate. (See Chapter 13.) It turns out that the uniform\ndensity is a member of the family of beta distributions. Each beta distribution is deﬁned by\nBETA DISTRIBUTION\ntwo hyperparameters3 a and b such that\nHYPERPARAMETER\nbeta[a, b](θ) = α θa−1(1 −θ)b−1 ,\n(20.6)\nfor θ in the range [0, 1]. The normalization constant α, which makes the distribution integrate\nto 1, depends on a and b. (See Exercise 20.7.) Figure 20.5 shows what the distribution looks\nlike for various values of a and b. The mean value of the distribution is a/(a + b), so larger\nvalues of a suggest a belief that Θ is closer to 1 than to 0. Larger values of a + b make the\ndistribution more peaked, suggesting greater certainty about the value of Θ. Thus, the beta\nfamily provides a useful range of possibilities for the hypothesis prior.",
  "distribution more peaked, suggesting greater certainty about the value of Θ. Thus, the beta\nfamily provides a useful range of possibilities for the hypothesis prior.\nBesides its ﬂexibility, the beta family has another wonderful property: if Θ has a prior\nbeta[a, b], then, after a data point is observed, the posterior distribution for Θ is also a beta\ndistribution. In other words, beta is closed under update. The beta family is called the\nconjugate prior for the family of distributions for a Boolean variable.4 Let’s see how this\nCONJUGATE PRIOR\nworks. Suppose we observe a cherry candy; then we have\n3 They are called hyperparameters because they parameterize a distribution over θ, which is itself a parameter.\n4 Other conjugate priors include the Dirichlet family for the parameters of a discrete multivalued distribution\nand the Normal–Wishart family for the parameters of a Gaussian distribution. See Bernardo and Smith (1994). 812\nChapter\n20.\nLearning Probabilistic Models\nFlavor1\nWrapper1\nFlavor2\nWrapper2\nFlavor3\nWrapper3\nΘ\nΘ1\nΘ2\nFigure 20.6\nA Bayesian network that corresponds to a Bayesian learning process. Poste-\nrior distributions for the parameter variables Θ, Θ1, and Θ2 can be inferred from their prior\ndistributions and the evidence in the Flavor i and Wrapper i variables.\nP(θ | D1 = cherry) = α P(D1 = cherry | θ)P(θ)\n= α′ θ · beta[a, b](θ) = α′ θ · θa−1(1 −θ)b−1\n= α′ θa(1 −θ)b−1 = beta[a + 1, b](θ) .\nThus, after seeing a cherry candy, we simply increment the a parameter to get the posterior;\nsimilarly, after seeing a lime candy, we increment the b parameter. Thus, we can view the a\nand b hyperparameters as virtual counts, in the sense that a prior beta[a, b] behaves exactly\nVIRTUAL COUNTS\nas if we had started out with a uniform prior beta[1, 1] and seen a −1 actual cherry candies\nand b −1 actual lime candies.\nBy examining a sequence of beta distributions for increasing values of a and b, keeping\nthe proportions ﬁxed, we can see vividly how the posterior distribution over the parameter\nΘ changes as data arrive. For example, suppose the actual bag of candy is 75% cherry. Fig-\nure 20.5(b) shows the sequence beta[3, 1], beta[6, 2], beta[30, 10]. Clearly, the distribution\nis converging to a narrow peak around the true value of Θ. For large data sets, then, Bayesian\nlearning (at least in this case) converges to the same answer as maximum-likelihood learning.\nNow let us consider a more complicated case. The network in Figure 20.2(b) has three",
  "learning (at least in this case) converges to the same answer as maximum-likelihood learning.\nNow let us consider a more complicated case. The network in Figure 20.2(b) has three\nparameters, θ, θ1, and θ2, where θ1 is the probability of a red wrapper on a cherry candy and\nθ2 is the probability of a red wrapper on a lime candy. The Bayesian hypothesis prior must\ncover all three parameters—that is, we need to specify P(Θ, Θ1, Θ2). Usually, we assume\nparameter independence:\nPARAMETER\nINDEPENDENCE\nP(Θ, Θ1, Θ2) = P(Θ)P(Θ1)P(Θ2) . Section 20.2.\nLearning with Complete Data\n813\nWith this assumption, each parameter can have its own beta distribution that is updated sep-\narately as data arrive. Figure 20.6 shows how we can incorporate the hypothesis prior and\nany data into one Bayesian network. The nodes Θ, Θ1, Θ2 have no parents. But each time\nwe make an observation of a wrapper and corresponding ﬂavor of a piece of candy, we add a\nnode Flavor i, which is dependent on the ﬂavor parameter Θ:\nP(Flavor i = cherry | Θ = θ) = θ .\nWe also add a node Wrapper i, which is dependent on Θ1 and Θ2:\nP(Wrapper i = red | Flavor i = cherry, Θ1 = θ1) = θ1\nP(Wrapper i = red | Flavor i = lime, Θ2 = θ2) = θ2 .\nNow, the entire Bayesian learning process can be formulated as an inference problem. We\nadd new evidence nodes, then query the unknown nodes (in this case, Θ, Θ1, Θ2). This for-\nmulation of learning and prediction makes it clear that Bayesian learning requires no extra\n“principles of learning.” Furthermore, there is, in essence, just one learning algorithm —the\ninference algorithm for Bayesian networks. Of course, the nature of these networks is some-\nwhat different from those of Chapter 14 because of the potentially huge number of evidence\nvariables representing the training set and the prevalence of continuous-valued parameter\nvariables.\n20.2.5\nLearning Bayes net structures\nSo far, we have assumed that the structure of the Bayes net is given and we are just trying to\nlearn the parameters. The structure of the network represents basic causal knowledge about\nthe domain that is often easy for an expert, or even a naive user, to supply. In some cases,\nhowever, the causal model may be unavailable or subject to dispute—for example, certain\ncorporations have long claimed that smoking does not cause cancer—so it is important to\nunderstand how the structure of a Bayes net can be learned from data. This section gives a\nbrief sketch of the main ideas.",
  "corporations have long claimed that smoking does not cause cancer—so it is important to\nunderstand how the structure of a Bayes net can be learned from data. This section gives a\nbrief sketch of the main ideas.\nThe most obvious approach is to search for a good model. We can start with a model\ncontaining no links and begin adding parents for each node, ﬁtting the parameters with the\nmethods we have just covered and measuring the accuracy of the resulting model. Alterna-\ntively, we can start with an initial guess at the structure and use hill-climbing or simulated\nannealing search to make modiﬁcations, retuning the parameters after each change in the\nstructure. Modiﬁcations can include reversing, adding, or deleting links. We must not in-\ntroduce cycles in the process, so many algorithms assume that an ordering is given for the\nvariables, and that a node can have parents only among those nodes that come earlier in the\nordering (just as in the construction process in Chapter 14). For full generality, we also need\nto search over possible orderings.\nThere are two alternative methods for deciding when a good structure has been found.\nThe ﬁrst is to test whether the conditional independence assertions implicit in the structure are\nactually satisﬁed in the data. For example, the use of a naive Bayes model for the restaurant\nproblem assumes that\nP(Fri/Sat, Bar | WillWait) = P(Fri/Sat | WillWait)P(Bar | WillWait) 814\nChapter\n20.\nLearning Probabilistic Models\nand we can check in the data that the same equation holds between the corresponding condi-\ntional frequencies. But even if the structure describes the true causal nature of the domain,\nstatistical ﬂuctuations in the data set mean that the equation will never be satisﬁed exactly,\nso we need to perform a suitable statistical test to see if there is sufﬁcient evidence that the\nindependence hypothesis is violated. The complexity of the resulting network will depend\non the threshold used for this test—the stricter the independence test, the more links will be\nadded and the greater the danger of overﬁtting.\nAn approach more consistent with the ideas in this chapter is to assess the degree to\nwhich the proposed model explains the data (in a probabilistic sense). We must be careful\nhow we measure this, however. If we just try to ﬁnd the maximum-likelihood hypothesis,\nwe will end up with a fully connected network, because adding more parents to a node can-",
  "how we measure this, however. If we just try to ﬁnd the maximum-likelihood hypothesis,\nwe will end up with a fully connected network, because adding more parents to a node can-\nnot decrease the likelihood (Exercise 20.8). We are forced to penalize model complexity in\nsome way. The MAP (or MDL) approach simply subtracts a penalty from the likelihood of\neach structure (after parameter tuning) before comparing different structures. The Bayesian\napproach places a joint prior over structures and parameters. There are usually far too many\nstructures to sum over (superexponential in the number of variables), so most practitioners\nuse MCMC to sample over structures.\nPenalizing complexity (whether by MAP or Bayesian methods) introduces an important\nconnection between the optimal structure and the nature of the representation for the condi-\ntional distributions in the network. With tabular distributions, the complexity penalty for a\nnode’s distribution grows exponentially with the number of parents, but with, say, noisy-OR\ndistributions, it grows only linearly. This means that learning with noisy-OR (or other com-\npactly parameterized) models tends to produce learned structures with more parents than does\nlearning with tabular distributions.\n20.2.6\nDensity estimation with nonparametric models\nIt is possible to learn a probability model without making any assumptions about its structure\nand parameterization by adopting the nonparametric methods of Section 18.8. The task of\nnonparametric density estimation is typically done in continuous domains, such as that\nNONPARAMETRIC\nDENSITY ESTIMATION\nshown in Figure 20.7(a). The ﬁgure shows a probability density function on a space deﬁned\nby two continuous variables. In Figure 20.7(b) we see a sample of data points from this\ndensity function. The question is, can we recover the model from the samples?\nFirst we will consider k-nearest-neighbors models. (In Chapter 18 we saw nearest-\nneighbor models for classiﬁcation and regression; here we see them for density estimation.)\nGiven a sample of data points, to estimate the unknown probability density at a query point x\nwe can simply measure the density of the data points in the neighborhood of x. Figure 20.7(b)\nshows two query points (small squares). For each query point we have drawn the smallest\ncircle that encloses 10 neighbors—the 10-nearest-neighborhood. We can see that the central\ncircle is large, meaning there is a low density there, and the circle on the right is small,",
  "circle that encloses 10 neighbors—the 10-nearest-neighborhood. We can see that the central\ncircle is large, meaning there is a low density there, and the circle on the right is small,\nmeaning there is a high density there. In Figure 20.8 we show three plots of density estimation\nusing k-nearest-neighbors, for different values of k. It seems clear that (b) is about right,\nwhile (a) is too spiky (k is too small) and (c) is too smooth (k is too big). Section 20.2.\nLearning with Complete Data\n815\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\nDensity\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n(a)\n(b)\nFigure 20.7\n(a) A 3D plot of the mixture of Gaussians from Figure 20.11(a). (b) A 128-\npoint sample of points from the mixture, together with two query points (small squares) and\ntheir 10-nearest-neighborhoods (medium and large circles).\n0 0.20.40.60.8\n0 0.20.40.60.81\nDensity\n0 0.20.40.60.8\n0 0.20.40.60.81\nDensity\n0 0.20.40.60.8\n0 0.20.40.60.81\nDensity\n(a)\n(b)\n(c)\nFigure 20.8\nDensity estimation using k-nearest-neighbors, applied to the data in Fig-\nure 20.7(b), for k = 3, 10, and 40 respectively. k = 3 is too spiky, 40 is too smooth, and\n10 is just about right. The best value for k can be chosen by cross-validation.\n0 0.20.40.60.8\n0 0.20.40.60.81\nDensity\n0 0.20.40.60.8\n0 0.20.40.60.81\nDensity\n0 0.20.40.60.8\n0 0.20.40.60.81\nDensity\n(a)\n(b)\n(c)\nFigure 20.9\nKernel density estimation for the data in Figure 20.7(b), using Gaussian ker-\nnels with w = 0.02, 0.07, and 0.20 respectively. w = 0.07 is about right. 816\nChapter\n20.\nLearning Probabilistic Models\nAnother possibility is to use kernel functions, as we did for locally weighted regres-\nsion. To apply a kernel model to density estimation, assume that each data point generates its\nown little density function, using a Gaussian kernel. The estimated density at a query point x\nis then the average density as given by each kernel function:\nP(x) = 1\nN\nN\n\f\nj=1\nK(x, xj) .\nWe will assume spherical Gaussians with standard deviation w along each axis:\nK(x, xj) =\n1\n(w2√\n2π)d e−\nD(x,xj)2\n2w2\n,\nwhere d is the number of dimensions in x and D is the Euclidean distance function. We\nstill have the problem of choosing a suitable value for kernel width w; Figure 20.9 shows\nvalues that are too small, just right, and too large. A good value of w can be chosen by using\ncross-validation.\n20.3\nLEARNING WITH HIDDEN VARIABLES: THE EM ALGORITHM",
  "values that are too small, just right, and too large. A good value of w can be chosen by using\ncross-validation.\n20.3\nLEARNING WITH HIDDEN VARIABLES: THE EM ALGORITHM\nThe preceding section dealt with the fully observable case. Many real-world problems have\nhidden variables (sometimes called latent variables), which are not observable in the data\nLATENT VARIABLE\nthat are available for learning. For example, medical records often include the observed\nsymptoms, the physician’s diagnosis, the treatment applied, and perhaps the outcome of the\ntreatment, but they seldom contain a direct observation of the disease itself! (Note that the\ndiagnosis is not the disease; it is a causal consequence of the observed symptoms, which are in\nturn caused by the disease.) One might ask, “If the disease is not observed, why not construct\na model without it?” The answer appears in Figure 20.10, which shows a small, ﬁctitious\ndiagnostic model for heart disease. There are three observable predisposing factors and three\nobservable symptoms (which are too depressing to name). Assume that each variable has\nthree possible values (e.g., none, moderate, and severe). Removing the hidden variable\nfrom the network in (a) yields the network in (b); the total number of parameters increases\nfrom 78 to 708. Thus, latent variables can dramatically reduce the number of parameters\nrequired to specify a Bayesian network. This, in turn, can dramatically reduce the amount of\ndata needed to learn the parameters.\nHidden variables are important, but they do complicate the learning problem. In Fig-\nure 20.10(a), for example, it is not obvious how to learn the conditional distribution for\nHeartDisease, given its parents, because we do not know the value of HeartDisease in each\ncase; the same problem arises in learning the distributions for the symptoms. This section\ndescribes an algorithm called expectation–maximization, or EM, that solves this problem\nEXPECTATION–\nMAXIMIZATION\nin a very general way. We will show three examples and then provide a general description.\nThe algorithm seems like magic at ﬁrst, but once the intuition has been developed, one can\nﬁnd applications for EM in a huge range of learning problems. Section 20.3.\nLearning with Hidden Variables: The EM Algorithm\n817\nSmoking\nDiet\nExercise\nSymptom1\nSymptom2\nSymptom3\n(a)\n(b)\nHeartDisease\nSmoking\nDiet\nExercise\nSymptom1\nSymptom2\nSymptom3\n2\n2\n2\n54\n6\n6\n6\n2\n2\n2\n54\n162\n486\nFigure 20.10",
  "Learning with Hidden Variables: The EM Algorithm\n817\nSmoking\nDiet\nExercise\nSymptom1\nSymptom2\nSymptom3\n(a)\n(b)\nHeartDisease\nSmoking\nDiet\nExercise\nSymptom1\nSymptom2\nSymptom3\n2\n2\n2\n54\n6\n6\n6\n2\n2\n2\n54\n162\n486\nFigure 20.10\n(a) A simple diagnostic network for heart disease, which is assumed to be\na hidden variable. Each variable has three possible values and is labeled with the number\nof independent parameters in its conditional distribution; the total number is 78. (b) The\nequivalent network with HeartDisease removed. Note that the symptom variables are no\nlonger conditionally independent given their parents. This network requires 708 parameters.\n20.3.1\nUnsupervised clustering: Learning mixtures of Gaussians\nUnsupervised clustering is the problem of discerning multiple categories in a collection of\nUNSUPERVISED\nCLUSTERING\nobjects. The problem is unsupervised because the category labels are not given. For example,\nsuppose we record the spectra of a hundred thousand stars; are there different types of stars\nrevealed by the spectra, and, if so, how many types and what are their characteristics? We\nare all familiar with terms such as “red giant” and “white dwarf,” but the stars do not carry\nthese labels on their hats—astronomers had to perform unsupervised clustering to identify\nthese categories. Other examples include the identiﬁcation of species, genera, orders, and\nso on in the Linnæan taxonomy and the creation of natural kinds for ordinary objects (see\nChapter 12).\nUnsupervised clustering begins with data. Figure 20.11(b) shows 500 data points, each\nof which speciﬁes the values of two continuous attributes. The data points might correspond\nto stars, and the attributes might correspond to spectral intensities at two particular frequen-\ncies. Next, we need to understand what kind of probability distribution might have generated\nthe data. Clustering presumes that the data are generated from a mixture distribution, P.\nMIXTURE\nDISTRIBUTION\nSuch a distribution has k components, each of which is a distribution in its own right. A\nCOMPONENT\ndata point is generated by ﬁrst choosing a component and then generating a sample from that\ncomponent. Let the random variable C denote the component, with values 1, . . . , k; then the\nmixture distribution is given by\nP(x) =\nk\n\f\ni = 1\nP(C = i) P(x | C = i) ,\nwhere x refers to the values of the attributes for a data point. For continuous data, a natural",
  "mixture distribution is given by\nP(x) =\nk\n\f\ni = 1\nP(C = i) P(x | C = i) ,\nwhere x refers to the values of the attributes for a data point. For continuous data, a natural\nchoice for the component distributions is the multivariate Gaussian, which gives the so-called\nmixture of Gaussians family of distributions. The parameters of a mixture of Gaussians are\nMIXTURE OF\nGAUSSIANS 818\nChapter\n20.\nLearning Probabilistic Models\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n(a)\n(b)\n(c)\nFigure 20.11\n(a) A Gaussian mixture model with three components; the weights (left-to-\nright) are 0.2, 0.3, and 0.5. (b) 500 data points sampled from the model in (a). (c) The model\nreconstructed by EM from the data in (b).\nwi = P(C = i) (the weight of each component), μi (the mean of each component), and Σi\n(the covariance of each component). Figure 20.11(a) shows a mixture of three Gaussians;\nthis mixture is in fact the source of the data in (b) as well as being the model shown in\nFigure 20.7(a) on page 815.\nThe unsupervised clustering problem, then, is to recover a mixture model like the one\nin Figure 20.11(a) from raw data like that in Figure 20.11(b). Clearly, if we knew which com-\nponent generated each data point, then it would be easy to recover the component Gaussians:\nwe could just select all the data points from a given component and then apply (a multivariate\nversion of) Equation (20.4) (page 809) for ﬁtting the parameters of a Gaussian to a set of data.\nOn the other hand, if we knew the parameters of each component, then we could, at least in\na probabilistic sense, assign each data point to a component. The problem is that we know\nneither the assignments nor the parameters.\nThe basic idea of EM in this context is to pretend that we know the parameters of the\nmodel and then to infer the probability that each data point belongs to each component. After\nthat, we reﬁt the components to the data, where each component is ﬁtted to the entire data set\nwith each point weighted by the probability that it belongs to that component. The process\niterates until convergence. Essentially, we are “completing” the data by inferring probability\ndistributions over the hidden variables—which component each data point belongs to—based\non the current model. For the mixture of Gaussians, we initialize the mixture-model parame-\nters arbitrarily and then iterate the following two steps:",
  "distributions over the hidden variables—which component each data point belongs to—based\non the current model. For the mixture of Gaussians, we initialize the mixture-model parame-\nters arbitrarily and then iterate the following two steps:\n1. E-step: Compute the probabilities pij = P(C = i | xj), the probability that datum xj\nwas generated by component i. By Bayes’ rule, we have pij = αP(xj | C = i)P(C = i).\nThe term P(xj | C = i) is just the probability at xj of the ith Gaussian, and the term\nP(C = i) is just the weight parameter for the ith Gaussian. Deﬁne ni = \u0002\nj pij, the\neffective number of data points currently assigned to component i.\n2. M-step: Compute the new mean, covariance, and component weights using the follow-\ning steps in sequence: Section 20.3.\nLearning with Hidden Variables: The EM Algorithm\n819\nμi ←\n\f\nj\npijxj/ni\nΣi ←\n\f\nj\npij(xj −μi)(xj −μi)⊤/ni\nwi ←ni/N\nwhere N is the total number of data points. The E-step, or expectation step, can be viewed\nas computing the expected values pij of the hidden indicator variables Zij, where Zij is 1 if\nINDICATOR VARIABLE\ndatum xj was generated by the ith component and 0 otherwise. The M-step, or maximization\nstep, ﬁnds the new values of the parameters that maximize the log likelihood of the data,\ngiven the expected values of the hidden indicator variables.\nThe ﬁnal model that EM learns when it is applied to the data in Figure 20.11(a) is shown\nin Figure 20.11(c); it is virtually indistinguishable from the original model from which the\ndata were generated. Figure 20.12(a) plots the log likelihood of the data according to the\ncurrent model as EM progresses.\nThere are two points to notice. First, the log likelihood for the ﬁnal learned model\nslightly exceeds that of the original model, from which the data were generated. This might\nseem surprising, but it simply reﬂects the fact that the data were generated randomly and\nmight not provide an exact reﬂection of the underlying model. The second point is that EM\nincreases the log likelihood of the data at every iteration. This fact can be proved in general.\nFurthermore, under certain conditions (that hold in ost cases), EM can be proven to reach\na local maximum in likelihood. (In rare cases, it could reach a saddle point or even a local\nminimum.) In this sense, EM resembles a gradient-based hill-climbing algorithm, but notice\nthat it has no “step size” parameter.\n-200\n-100\n0\n100\n200\n300\n400\n500\n600\n700\n0\n5\n10\n15\n20\nLog-likelihood L\nIteration number\n-2025\n-2020",
  "minimum.) In this sense, EM resembles a gradient-based hill-climbing algorithm, but notice\nthat it has no “step size” parameter.\n-200\n-100\n0\n100\n200\n300\n400\n500\n600\n700\n0\n5\n10\n15\n20\nLog-likelihood L\nIteration number\n-2025\n-2020\n-2015\n-2010\n-2005\n-2000\n-1995\n-1990\n-1985\n-1980\n-1975\n0\n20\n40\n60\n80\n100\n120\nLog-likelihood L\nIteration number\n(a)\n(b)\nFigure 20.12\nGraphs showing the log likelihood of the data, L, as a function of the EM\niteration. The horizontal line shows the log likelihood according to the true model. (a) Graph\nfor the Gaussian mixture model in Figure 20.11. (b) Graph for the Bayesian network in\nFigure 20.13(a). 820\nChapter\n20.\nLearning Probabilistic Models\n(a)\n(b)\nC\nX\nHole\nBag\nP(Bag=1)\nθ\nWrapper\nFlavor\nBag\n1\n2\nP(F=cherry | B)\nθF2\nθF1\nFigure 20.13\n(a) A mixture model for candy. The proportions of different ﬂavors, wrap-\npers, presence of holes depend on the bag, which is not observed. (b) Bayesian network for\na Gaussian mixture. The mean and covariance of the observable variables X depend on the\ncomponent C.\nThings do not always go as well as Figure 20.12(a) might suggest. It can happen, for\nexample, that one Gaussian component shrinks so that it covers just a single data point. Then\nits variance will go to zero and its likelihood will go to inﬁnity! Another problem is that\ntwo components can “merge,” acquiring identical means and variances and sharing their data\npoints. These kinds of degenerate local maxima are serious problems, especially in high\ndimensions. One solution is to place priors on the model parameters and to apply the MAP\nversion of EM. Another is to restart a component with new random parameters if it gets too\nsmall or too close to another component. Sensible initialization also helps.\n20.3.2\nLearning Bayesian networks with hidden variables\nTo learn a Bayesian network with hidden variables, we apply the same insights that worked\nfor mixtures of Gaussians. Figure 20.13 represents a situation in which there are two bags of\ncandies that have been mixed together. Candies are described by three features: in addition\nto the Flavor and the Wrapper, some candies have a Hole in the middle and some do not.\nThe distribution of candies in each bag is described by a naive Bayes model: the features\nare independent, given the bag, but the conditional probability distribution for each feature\ndepends on the bag. The parameters are as follows: θ is the prior probability that a candy",
  "are independent, given the bag, but the conditional probability distribution for each feature\ndepends on the bag. The parameters are as follows: θ is the prior probability that a candy\ncomes from Bag 1; θF 1 and θF 2 are the probabilities that the ﬂavor is cherry, given that the\ncandy comes from Bag 1 or Bag 2 respectively; θW 1 and θW 2 give the probabilities that the\nwrapper is red; and θH1 and θH2 give the probabilities that the candy has a hole. Notice that\nthe overall model is a mixture model. (In fact, we can also model the mixture of Gaussians\nas a Bayesian network, as shown in Figure 20.13(b).) In the ﬁgure, the bag is a hidden\nvariable because, once the candies have been mixed together, we no longer know which bag\neach candy came from. In such a case, can we recover the descriptions of the two bags by Section 20.3.\nLearning with Hidden Variables: The EM Algorithm\n821\nobserving candies from the mixture? Let us work through an iteration of EM for this problem.\nFirst, let’s look at the data. We generated 1000 samples from a model whose true parameters\nare as follows:\nθ = 0.5, θF 1 = θW 1 = θH1 = 0.8, θF 2 = θW 2 = θH2 = 0.3 .\n(20.7)\nThat is, the candies are equally likely to come from either bag; the ﬁrst is mostly cherries\nwith red wrappers and holes; the second is mostly limes with green wrappers and no holes.\nThe counts for the eight possible kinds of candy are as follows:\nW = red\nW = green\nH = 1 H = 0 H = 1 H = 0\nF = cherry\n273\n93\n104\n90\nF = lime\n79\n100\n94\n167\nWe start by initializing the parameters. For numerical simplicity, we arbitrarily choose5\nθ(0) = 0.6, θ(0)\nF 1 = θ(0)\nW 1 = θ(0)\nH1 = 0.6, θ(0)\nF 2 = θ(0)\nW 2 = θ(0)\nH2 = 0.4 .\n(20.8)\nFirst, let us work on the θ parameter. In the fully observable case, we would estimate this\ndirectly from the observed counts of candies from bags 1 and 2. Because the bag is a hidden\nvariable, we calculate the expected counts instead. The expected count ˆN(Bag = 1) is the\nsum, over all candies, of the probability that the candy came from bag 1:\nθ(1) = ˆN(Bag = 1)/N =\nN\n\f\nj = 1\nP(Bag = 1 | ﬂavor j, wrapper j, holesj)/N .\nThese probabilities can be computed by any inference algorithm for Bayesian networks. For\na naive Bayes model such as the one in our example, we can do the inference “by hand,”\nusing Bayes’ rule and applying conditional independence:\nθ(1) = 1\nN\nN\n\f\nj = 1\nP(ﬂavor j | Bag = 1)P(wrapper j | Bag = 1)P(holesj | Bag = 1)P(Bag = 1)\n\u0002",
  "a naive Bayes model such as the one in our example, we can do the inference “by hand,”\nusing Bayes’ rule and applying conditional independence:\nθ(1) = 1\nN\nN\n\f\nj = 1\nP(ﬂavor j | Bag = 1)P(wrapper j | Bag = 1)P(holesj | Bag = 1)P(Bag = 1)\n\u0002\ni P(ﬂavor j | Bag = i)P(wrapper j | Bag = i)P(holesj | Bag = i)P(Bag = i) .\nApplying this formula to, say, the 273 red-wrapped cherry candies with holes, we get a con-\ntribution of\n273\n1000 ·\nθ(0)\nF 1θ(0)\nW 1θ(0)\nH1θ(0)\nθ(0)\nF 1θ(0)\nW 1θ(0)\nH1θ(0) + θ(0)\nF 2θ(0)\nW 2θ(0)\nH2(1 −θ(0))\n≈0.22797 .\nContinuing with the other seven kinds of candy in the table of counts, we obtain θ(1) = 0.6124.\nNow let us consider the other parameters, such as θF 1. In the fully observable case, we\nwould estimate this directly from the observed counts of cherry and lime candies from bag 1.\nThe expected count of cherry candies from bag 1 is given by\n\f\nj:Flavor j = cherry\nP(Bag = 1 | Flavor j = cherry, wrapper j, holesj) .\n5 It is better in practice to choose them randomly, to avoid local maxima due to symmetry. 822\nChapter\n20.\nLearning Probabilistic Models\nAgain, these probabilities can be calculated by any Bayes net algorithm. Completing this\nprocess, we obtain the new values of all the parameters:\nθ(1) = 0.6124, θ(1)\nF 1 = 0.6684, θ(1)\nW 1 = 0.6483, θ(1)\nH1 = 0.6558,\nθ(1)\nF 2 = 0.3887, θ(1)\nW 2 = 0.3817, θ(1)\nH2 = 0.3827 .\n(20.9)\nThe log likelihood of the data increases from about −2044 initially to about −2021 after\nthe ﬁrst iteration, as shown in Figure 20.12(b). That is, the update improves the likelihood\nitself by a factor of about e23 ≈1010. By the tenth iteration, the learned model is a better\nﬁt than the original model (L = −1982.214). Thereafter, progress becomes very slow. This\nis not uncommon with EM, and many practical systems combine EM with a gradient-based\nalgorithm such as Newton–Raphson (see Chapter 4) for the last phase of learning.\nThe general lesson from this example is that the parameter updates for Bayesian net-\nwork learning with hidden variables are directly available from the results of inference on\neach example. Moreover, only local posterior probabilities are needed for each parame-\nter. Here, “local” means that the CPT for each variable Xi can be learned from posterior\nprobabilities involving just Xi and its parents Ui. Deﬁning θijk to be the CPT parameter\nP(Xi = xij | Ui = uik), the update is given by the normalized expected counts as follows:\nθijk ←ˆN(Xi = xij, Ui = uik)/ ˆN(Ui = uik) .",
  "probabilities involving just Xi and its parents Ui. Deﬁning θijk to be the CPT parameter\nP(Xi = xij | Ui = uik), the update is given by the normalized expected counts as follows:\nθijk ←ˆN(Xi = xij, Ui = uik)/ ˆN(Ui = uik) .\nThe expected counts are obtained by summing over the examples, computing the probabilities\nP(Xi = xij, Ui = uik) for each by using any Bayes net inference algorithm. For the exact\nalgorithms—including variable elimination—all these probabilities are obtainable directly as\na by-product of standard inference, with no need for extra computations speciﬁc to learning.\nMoreover, the information needed for learning is available locally for each parameter.\n20.3.3\nLearning hidden Markov models\nOur ﬁnal application of EM involves learning the transition probabilities in hidden Markov\nmodels (HMMs). Recall from Section 15.3 that a hidden Markov model can be represented\nby a dynamic Bayes net with a single discrete state variable, as illustrated in Figure 20.14.\nEach data point consists of an observation sequence of ﬁnite length, so the problem is to\nlearn the transition probabilities from a set of observation sequences (or from just one long\nsequence).\nWe have already worked out how to learn Bayes nets, but there is one complication:\nin Bayes nets, each parameter is distinct; in a hidden Markov model, on the other hand, the\nindividual transition probabilities from state i to state j at time t, θijt = P(Xt+1 = j | Xt = i),\nare repeated across time—that is, θijt = θij for all t. To estimate the transition probability\nfrom state i to state j, we simply calculate the expected proportion of times that the system\nundergoes a transition to state j when in state i:\nθij ←\n\f\nt\nˆN(Xt+1 = j, Xt = i)/\n\f\nt\nˆN(Xt = i) .\nThe expected counts are computed by an HMM inference algorithm. The forward–backward\nalgorithm shown in Figure 15.4 can be modiﬁed very easily to compute the necessary prob-\nabilities. One important point is that the probabilities required are obtained by smoothing Section 20.3.\nLearning with Hidden Variables: The EM Algorithm\n823\n0.3\nf\n0.7\nt\nP(R )\n1\nR0\n0.7\nP(R0)\n0.2\nf\n0.9\nt\nP(U )\n1\nR1\nUmbrella1\nRain0\nRain1\n0.7\nP(R0)\n4\n0.2\nf\n0.9\nt\nP(U )\nR4\nf\nt\n0.3\n0.7\nP(R  )\n4\nR3\nUmbrella4\nRain4\n0.2\nf\n0.9\nt\nP(U )\n3\nR3\nf\nt\nR\n0.3\n0.7\nP(R  )\n3\n2\nUmbrella3\nRain3\n0.2\nf\n0.9\nt\nP(U )\n2\nR2\nf\nt\nR\n0.3\n0.7\nP(R )\n2\n1\nUmbrella2\nRain2\n0.2\nf\n0.9\nt\nP(U )\n1\nR1\nf\nt\nR\n0.3\n0.7\nP(R )\n1\n0\nUmbrella1\nRain0\nRain1\nFigure 20.14",
  "f\nt\n0.3\n0.7\nP(R  )\n4\nR3\nUmbrella4\nRain4\n0.2\nf\n0.9\nt\nP(U )\n3\nR3\nf\nt\nR\n0.3\n0.7\nP(R  )\n3\n2\nUmbrella3\nRain3\n0.2\nf\n0.9\nt\nP(U )\n2\nR2\nf\nt\nR\n0.3\n0.7\nP(R )\n2\n1\nUmbrella2\nRain2\n0.2\nf\n0.9\nt\nP(U )\n1\nR1\nf\nt\nR\n0.3\n0.7\nP(R )\n1\n0\nUmbrella1\nRain0\nRain1\nFigure 20.14\nAn unrolled dynamic Bayesian network that represents a hidden Markov\nmodel (repeat of Figure 15.16).\nrather than ﬁltering; that is, we need to pay attention to subsequent evidence in estimating\nthe probability that a particular transition occurred. The evidence in a murder case is usually\nobtained after the crime (i.e., the transition from state i to state j) has taken place.\n20.3.4\nThe general form of the EM algorithm\nWe have seen several instances of the EM algorithm. Each involves computing expected\nvalues of hidden variables for each example and then recomputing the parameters, using the\nexpected values as if they were observed values. Let x be all the observed values in all the\nexamples, let Z denote all the hidden variables for all the examples, and let θ be all the\nparameters for the probability model. Then the EM algorithm is\nθ(i+1) = argmax\nθ\n\f\nz\nP(Z = z | x, θ(i))L(x, Z = z | θ) .\nThis equation is the EM algorithm in a nutshell. The E-step is the computation of the summa-\ntion, which is the expectation of the log likelihood of the “completed” data with respect to the\ndistribution P(Z = z | x, θ(i)), which is the posterior over the hidden variables, given the data.\nThe M-step is the maximization of this expected log likelihood with respect to the parame-\nters. For mixtures of Gaussians, the hidden variables are the Zijs, where Zij is 1 if example j\nwas generated by component i. For Bayes nets, Zij is the value of unobserved variable Xi in\nexample j. For HMMs, Zjt is the state of the sequence in example j at time t. Starting from\nthe general form, it is possible to derive an EM algorithm for a speciﬁc application once the\nappropriate hidden variables have been identiﬁed.\nAs soon as we understand the general idea of EM, it becomes easy to derive all sorts\nof variants and improvements. For example, in many cases the E-step—the computation of\nposteriors over the hidden variables—is intractable, as in large Bayes nets. It turns out that\none can use an approximate E-step and still obtain an effective learning algorithm. With a\nsampling algorithm such as MCMC (see Section 14.5), the learning process is very intuitive:\neach state (conﬁguration of hidden and observed variables) visited by MCMC is treated ex-",
  "sampling algorithm such as MCMC (see Section 14.5), the learning process is very intuitive:\neach state (conﬁguration of hidden and observed variables) visited by MCMC is treated ex-\nactly as if it were a complete observation. Thus, the parameters can be updated directly after\neach MCMC transition. Other forms of approximate inference, such as variational and loopy\nmethods, have also proved effective for learning very large networks. 824\nChapter\n20.\nLearning Probabilistic Models\n20.3.5\nLearning Bayes net structures with hidden variables\nIn Section 20.2.5, we discussed the problem of learning Bayes net structures with complete\ndata. When unobserved variables may be inﬂuencing the data that are observed, things get\nmore difﬁcult. In the simplest case, a human expert might tell the learning algorithm that cer-\ntain hidden variables exist, leaving it to the algorithm to ﬁnd a place for them in the network\nstructure. For example, an algorithm might try to learn the structure shown in Figure 20.10(a)\non page 817, given the information that HeartDisease (a three-valued variable) should be in-\ncluded in the model. As in the complete-data case, the overall algorithm has an outer loop that\nsearches over structures and an inner loop that ﬁts the network parameters given the structure.\nIf the learning algorithm is not told which hidden variables exist, then there are two\nchoices: either pretend that the data is really complete—which may force the algorithm to\nlearn a parameter-intensive model such as the one in Figure 20.10(b)—or invent new hidden\nvariables in order to simplify the model. The latter approach can be implemented by including\nnew modiﬁcation choices in the structure search: in addition to modifying links, the algorithm\ncan add or delete a hidden variable or change its arity. Of course, the algorithm will not know\nthat the new variable it has invented is called HeartDisease; nor will it have meaningful\nnames for the values. Fortunately, newly invented hidden variables will usually be connected\nto preexisting variables, so a human expert can often inspect the local conditional distributions\ninvolving the new variable and ascertain its meaning.\nAs in the complete-data case, pure maximum-likelihood structure learning will result in\na completely connected network (moreover, one with no hidden variables), so some form of\ncomplexity penalty is required. We can also apply MCMC to sample many possible network",
  "a completely connected network (moreover, one with no hidden variables), so some form of\ncomplexity penalty is required. We can also apply MCMC to sample many possible network\nstructures, thereby approximating Bayesian learning. For example, we can learn mixtures of\nGaussians with an unknown number of components by sampling over the number; the approx-\nimate posterior distribution for the number of Gaussians is given by the sampling frequencies\nof the MCMC process.\nFor the complete-data case, the inner loop to learn the parameters is very fast—just a\nmatter of extracting conditional frequencies from the data set. When there are hidden vari-\nables, the inner loop may involve many iterations of EM or a gradient-based algorithm, and\neach iteration involves the calculation of posteriors in a Bayes net, which is itself an NP-hard\nproblem. To date, this approach has proved impractical for learning complex models. One\npossible improvement is the so-called structural EM algorithm, which operates in much the\nSTRUCTURAL EM\nsame way as ordinary (parametric) EM except that the algorithm can update the structure\nas well as the parameters. Just as ordinary EM uses the current parameters to compute the\nexpected counts in the E-step and then applies those counts in the M-step to choose new\nparameters, structural EM uses the current structure to compute expected counts and then ap-\nplies those counts in the M-step to evaluate the likelihood for potential new structures. (This\ncontrasts with the outer-loop/inner-loop method, which computes new expected counts for\neach potential structure.) In this way, structural EM may make several structural alterations\nto the network without once recomputing the expected counts, and is capable of learning non-\ntrivial Bayes net structures. Nonetheless, much work remains to be done before we can say\nthat the structure-learning problem is solved. Section 20.4.\nSummary\n825\n20.4\nSUMMARY\nStatistical learning methods range from simple calculation of averages to the construction of\ncomplex models such as Bayesian networks. They have applications throughout computer\nscience, engineering, computational biology, neuroscience, psychology, and physics. This\nchapter has presented some of the basic ideas and given a ﬂavor of the mathematical under-\npinnings. The main points are as follows:\n• Bayesian learning methods formulate learning as a form of probabilistic inference,",
  "chapter has presented some of the basic ideas and given a ﬂavor of the mathematical under-\npinnings. The main points are as follows:\n• Bayesian learning methods formulate learning as a form of probabilistic inference,\nusing the observations to update a prior distribution over hypotheses. This approach\nprovides a good way to implement Ockham’s razor, but quickly becomes intractable for\ncomplex hypothesis spaces.\n• Maximum a posteriori (MAP) learning selects a single most likely hypothesis given\nthe data. The hypothesis prior is still used and the method is often more tractable than\nfull Bayesian learning.\n• Maximum-likelihood learning simply selects the hypothesis that maximizes the likeli-\nhood of the data; it is equivalent to MAP learning with a uniform prior. In simple cases\nsuch as linear regression and fully observable Bayesian networks, maximum-likelihood\nsolutions can be found easily in closed form. Naive Bayes learning is a particularly\neffective technique that scales well.\n• When some variables are hidden, local maximum likelihood solutions can be found\nusing the EM algorithm. Applications include clustering using mixtures of Gaussians,\nlearning Bayesian networks, and learning hidden Markov models.\n• Learning the structure of Bayesian networks is an example of model selection. This\nusually involves a discrete search in the space of structures. Some method is required\nfor trading off model complexity against degree of ﬁt.\n• Nonparametric models represent a distribution using the collection of data points.\nThus, the number of parameters grows with the training set. Nearest-neighbors methods\nlook at the examples nearest to the point in question, whereas kernel methods form a\ndistance-weighted combination of all the examples.\nStatistical learning continues to be a very active area of research. Enormous strides have been\nmade in both theory and practice, to the point where it is possible to learn almost any model\nfor which exact or approximate inference is feasible.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe application of statistical learning techniques in AI was an active area of research in the\nearly years (see Duda and Hart, 1973) but became separated from mainstream AI as the\nlatter ﬁeld concentrated on symbolic methods. A resurgence of interest occurred shortly after\nthe introduction of Bayesian network models in the late 1980s; at roughly the same time, 826\nChapter\n20.\nLearning Probabilistic Models",
  "latter ﬁeld concentrated on symbolic methods. A resurgence of interest occurred shortly after\nthe introduction of Bayesian network models in the late 1980s; at roughly the same time, 826\nChapter\n20.\nLearning Probabilistic Models\na statistical view of neural network learning began to emerge. In the late 1990s, there was\na noticeable convergence of interests in machine learning, statistics, and neural networks,\ncentered on methods for creating large probabilistic models from data.\nThe naive Bayes model is one of the oldest and simplest forms of Bayesian network,\ndating back to the 1950s. Its origins were mentioned in Chapter 13. Its surprising success is\npartially explained by Domingos and Pazzani (1997). A boosted form of naive Bayes learn-\ning won the ﬁrst KDD Cup data mining competition (Elkan, 1997). Heckerman (1998) gives\nan excellent introduction to the general problem of Bayes net learning. Bayesian parame-\nter learning with Dirichlet priors for Bayesian networks was discussed by Spiegelhalter et al.\n(1993). The BUGS software package (Gilks et al., 1994) incorporates many of these ideas and\nprovides a very powerful tool for formulating and learning complex probability models. The\nﬁrst algorithms for learning Bayes net structures used conditional independence tests (Pearl,\n1988; Pearl and Verma, 1991). Spirtes et al. (1993) developed a comprehensive approach\nembodied in the TETRAD package for Bayes net learning. Algorithmic improvements since\nthen led to a clear victory in the 2001 KDD Cup data mining competition for a Bayes net\nlearning method (Cheng et al., 2002). (The speciﬁc task here was a bioinformatics prob-\nlem with 139,351 features!) A structure-learning approach based on maximizing likelihood\nwas developed by Cooper and Herskovits (1992) and improved by Heckerman et al. (1994).\nSeveral algorithmic advances since that time have led to quite respectable performance in\nthe complete-data case (Moore and Wong, 2003; Teyssier and Koller, 2005). One important\ncomponent is an efﬁcient data structure, the AD-tree, for caching counts over all possible\ncombinations of variables and values (Moore and Lee, 1997). Friedman and Goldszmidt\n(1996) pointed out the inﬂuence of the representation of local conditional distributions on the\nlearned structure.\nThe general problem of learning probability models with hidden variables and miss-\ning data was addressed by Hartley (1958), who described the general idea of what was later",
  "learned structure.\nThe general problem of learning probability models with hidden variables and miss-\ning data was addressed by Hartley (1958), who described the general idea of what was later\ncalled EM and gave several examples. Further impetus came from the Baum–Welch algo-\nrithm for HMM learning (Baum and Petrie, 1966), which is a special case of EM. The paper\nby Dempster, Laird, and Rubin (1977), which presented the EM algorithm in general form\nand analyzed its convergence, is one of the most cited papers in both computer science and\nstatistics. (Dempster himself views EM as a schema rather than an algorithm, since a good\ndeal of mathematical work may be required before it can be applied to a new family of dis-\ntributions.) McLachlan and Krishnan (1997) devote an entire book to the algorithm and its\nproperties. The speciﬁc problem of learning mixture models, including mixtures of Gaus-\nsians, is covered by Titterington et al. (1985). Within AI, the ﬁrst successful system that used\nEM for mixture modeling was AUTOCLASS (Cheeseman et al., 1988; Cheeseman and Stutz,\n1996). AUTOCLASS has been applied to a number of real-world scientiﬁc classiﬁcation tasks,\nincluding the discovery of new types of stars from spectral data (Goebel et al., 1989) and new\nclasses of proteins and introns in DNA/protein sequence databases (Hunter and States, 1992).\nFor maximum-likelihood parameter learning in Bayes nets with hidden variables, EM\nand gradient-based methods were introduced around the same time by Lauritzen (1995), Rus-\nsell et al. (1995), and Binder et al. (1997a). The structural EM algorithm was developed by\nFriedman (1998) and applied to maximum-likelihood learning of Bayes net structures with Exercises\n827\nlatent variables. Friedman and Koller (2003). describe Bayesian structure learning.\nThe ability to learn the structure of Bayesian networks is closely connected to the issue\nof recovering causal information from data. That is, is it possible to learn Bayes nets in\nsuch a way that the recovered network structure indicates real causal inﬂuences? For many\nyears, statisticians avoided this question, believing that observational data (as opposed to data\ngenerated from experimental trials) could yield only correlational information—after all, any\ntwo variables that appear related might in fact be inﬂuenced by a third, unknown causal\nfactor rather than inﬂuencing each other directly. Pearl (2000) has presented convincing",
  "two variables that appear related might in fact be inﬂuenced by a third, unknown causal\nfactor rather than inﬂuencing each other directly. Pearl (2000) has presented convincing\narguments to the contrary, showing that there are in fact many cases where causality can be\nascertained and developing the causal network formalism to express causes and the effects\nCAUSAL NETWORK\nof intervention as well as ordinary conditional probabilities.\nNonparametric density estimation, also called Parzen window density estimation, was\ninvestigated initially by Rosenblatt (1956) and Parzen (1962). Since that time, a huge litera-\nture has developed investigating the properties of various estimators. Devroye (1987) gives a\nthorough introduction. There is also a rapidly growing literature on nonparametric Bayesian\nmethods, originating with the seminal work of Ferguson (1973) on the Dirichlet process,\nDIRICHLET PROCESS\nwhich can be thought of as a distribution over Dirichlet distributions. These methods are par-\nticularly useful for mixtures with unknown numbers of components. Ghahramani (2005) and\nJordan (2005) provide useful tutorials on the many applications of these ideas to statistical\nlearning. The text by Rasmussen and Williams (2006) covers the Gaussian process, which\nGAUSSIAN PROCESS\ngives a way of deﬁning prior distributions over the space of continuous functions.\nThe material in this chapter brings together work from the ﬁelds of statistics and pattern\nrecognition, so the story has been told many times in many ways. Good texts on Bayesian\nstatistics include those by DeGroot (1970), Berger (1985), and Gelman et al. (1995). Bishop\n(2007) and Hastie et al. (2009) provide an excellent introduction to statistical machine learn-\ning. For pattern classiﬁcation, the classic text for many years has been Duda and Hart (1973),\nnow updated (Duda et al., 2001). The annual NIPS (Neural Information Processing Confer-\nence) conference, whose proceedings are published as the series Advances in Neural Informa-\ntion Processing Systems, is now dominated by Bayesian papers. Papers on learning Bayesian\nnetworks also appear in the Uncertainty in AI and Machine Learning conferences and in sev-\neral statistics conferences. Journals speciﬁc to neural networks include Neural Computation,\nNeural Networks, and the IEEE Transactions on Neural Networks. Speciﬁcally Bayesian\nvenues include the Valencia International Meetings on Bayesian Statistics and the journal\nBayesian Analysis.\nEXERCISES\n20.1",
  "Neural Networks, and the IEEE Transactions on Neural Networks. Speciﬁcally Bayesian\nvenues include the Valencia International Meetings on Bayesian Statistics and the journal\nBayesian Analysis.\nEXERCISES\n20.1\nThe data used for Figure 20.1 on page 804 can be viewed as being generated by h5.\nFor each of the other four hypotheses, generate a data set of length 100 and plot the cor-\nresponding graphs for P(hi | d1, . . . , dN) and P(DN+1 = lime | d1, . . . , dN). Comment on\nyour results. 828\nChapter\n20.\nLearning Probabilistic Models\n20.2\nSuppose that Ann’s utilities for cherry and lime candies are cA and ℓA, whereas Bob’s\nutilities are cB and ℓB. (But once Ann has unwrapped a piece of candy, Bob won’t buy\nit.) Presumably, if Bob likes lime candies much more than Ann, it would be wise for Ann\nto sell her bag of candies once she is sufﬁciently sure of its lime content. On the other hand,\nif Ann unwraps too many candies in the process, the bag will be worth less. Discuss the\nproblem of determining the optimal point at which to sell the bag. Determine the expected\nutility of the optimal procedure, given the prior distribution from Section 20.1.\n20.3\nTwo statisticians go to the doctor and are both given the same prognosis: A 40%\nchance that the problem is the deadly disease A, and a 60% chance of the fatal disease B.\nFortunately, there are anti-A and anti-B drugs that are inexpensive, 100% effective, and free\nof side-effects. The statisticians have the choice of taking one drug, both, or neither. What\nwill the ﬁrst statistician (an avid Bayesian) do? How about the second statistician, who always\nuses the maximum likelihood hypothesis?\nThe doctor does some research and discovers that disease B actually comes in two\nversions, dextro-B and levo-B, which are equally likely and equally treatable by the anti-B\ndrug. Now that there are three hypotheses, what will the two statisticians do?\n20.4\nExplain how to apply the boosting method of Chapter 18 to naive Bayes learning. Test\nthe performance of the resulting algorithm on the restaurant learning problem.\n20.5\nConsider N data points (xj, yj), where the yjs are generated from the xjs according to\nthe linear Gaussian model in Equation (20.5). Find the values of θ1, θ2, and σ that maximize\nthe conditional log likelihood of the data.\n20.6\nConsider the noisy-OR model for fever described in Section 14.3. Explain how to\napply maximum-likelihood learning to ﬁt the parameters of such a model to a set of complete",
  "the conditional log likelihood of the data.\n20.6\nConsider the noisy-OR model for fever described in Section 14.3. Explain how to\napply maximum-likelihood learning to ﬁt the parameters of such a model to a set of complete\ndata. (Hint: use the chain rule for partial derivatives.)\n20.7\nThis exercise investigates properties of the Beta distribution deﬁned in Equation (20.6).\na. By integrating over the range [0, 1], show that the normalization constant for the dis-\ntribution beta[a, b] is given by α = Γ(a + b)/Γ(a)Γ(b) where Γ(x) is the Gamma\nfunction, deﬁned by Γ(x + 1) = x · Γ(x) and Γ(1) = 1. (For integer x, Γ(x + 1) = x!.)\nGAMMA FUNCTION\nb. Show that the mean is a/(a + b).\nc. Find the mode(s) (the most likely value(s) of θ).\nd. Describe the distribution beta[ϵ, ϵ] for very small ϵ. What happens as such a distribution\nis updated?\n20.8\nConsider an arbitrary Bayesian network, a complete data set for that network, and the\nlikelihood for the data set according to the network. Give a simple proof that the likelihood\nof the data cannot decrease if we add a new link to the network and recompute the maximum-\nlikelihood parameter values.\n20.9\nConsider a single Boolean random variable Y (the “classiﬁcation”). Let the prior\nprobability P(Y = true) be π. Let’s try to ﬁnd π, given a training set D = (y1, . . . , yN) with\nN independent samples of Y . Furthermore, suppose p of the N are positive and n of the N\nare negative. Exercises\n829\na. Write down an expression for the likelihood of D (i.e., the probability of seeing this\nparticular sequence of examples, given a ﬁxed value of π) in terms of π, p, and n.\nb. By differentiating the log likelihood L, ﬁnd the value of π that maximizes the likelihood.\nc. Now suppose we add in k Boolean random variables X1, X2, . . . , Xk (the “attributes”)\nthat describe each sample, and suppose we assume that the attributes are conditionally\nindependent of each other given the goal Y . Draw the Bayes net corresponding to this\nassumption.\nd. Write down the likelihood for the data including the attributes, using the following\nadditional notation:\n• αi is P(Xi = true|Y = true).\n• βi is P(Xi = true|Y = false).\n• p+\ni is the count of samples for which Xi = true and Y = true.\n• n+\ni is the count of samples for which Xi = false and Y = true.\n• p−\ni is the count of samples for which Xi = true and Y = false.\n• n−\ni is the count of samples for which Xi = false and Y = false.",
  "• n+\ni is the count of samples for which Xi = false and Y = true.\n• p−\ni is the count of samples for which Xi = true and Y = false.\n• n−\ni is the count of samples for which Xi = false and Y = false.\n[Hint: consider ﬁrst the probability of seeing a single example with speciﬁed values for\nX1, X2, . . . , Xk and Y .]\ne. By differentiating the log likelihood L, ﬁnd the values of αi and βi (in terms of the var-\nious counts) that maximize the likelihood and say in words what these values represent.\nf. Let k = 2, and consider a data set with 4 all four possible examples of theXOR function.\nCompute the maximum likelihood estimates of π, α1, α2, β1, and β2.\ng. Given these estimates of π, α1, α2, β1, and β2, what are the posterior probabilities\nP(Y = true|x1, x2) for each example?\n20.10\nConsider the application of EM to learn the parameters for the network in Fig-\nure 20.13(a), given the true parameters in Equation (20.7).\na. Explain why the EM algorithm would not work if there were just two attributes in the\nmodel rather than three.\nb. Show the calculations for the ﬁrst iteration of EM starting from Equation (20.8).\nc. What happens if we start with all the parameters set to the same value p? (Hint: you\nmay ﬁnd it helpful to investigate this empirically before deriving the general result.)\nd. Write out an expression for the log likelihood of the tabulated candy data on page 821 in\nterms of the parameters, calculate the partial derivatives with respect to each parameter,\nand investigate the nature of the ﬁxed point reached in part (c). 21\nREINFORCEMENT\nLEARNING\nIn which we examine how an agent can learn from success and failure, from re-\nward and punishment.\n21.1\nINTRODUCTION\nChapters 18, 19, and 20 covered methods that learn functions, logical theories, and probability\nmodels from examples. In this chapter, we will study how agents can learn what to do in the\nabsence of labeled examples of what to do.\nConsider, for example, the problem of learning to play chess. A supervised learning\nagent needs to be told the correct move for each position it encounters, but such feedback is\nseldom available. In the absence of feedback from a teacher, an agent can learn a transition\nmodel for its own moves and can perhaps learn to predict the opponent’s moves, but without\nsome feedback about what is good and what is bad, the agent will have no grounds for decid-\ning which move to make. The agent needs to know that something good has happened when",
  "some feedback about what is good and what is bad, the agent will have no grounds for decid-\ning which move to make. The agent needs to know that something good has happened when\nit (accidentally) checkmates the opponent, and that something bad has happened when it is\ncheckmated—or vice versa, if the game is suicide chess. This kind of feedback is called a\nreward, or reinforcement. In games like chess, the reinforcement is received only at the end\nREINFORCEMENT\nof the game. In other environments, the rewards come more frequently. In ping-pong, each\npoint scored can be considered a reward; when learning to crawl, any forward motion is an\nachievement. Our framework for agents regards the reward as part of the input percept, but\nthe agent must be “hardwired” to recognize that part as a reward rather than as just another\nsensory input. Thus, animals seem to be hardwired to recognize pain and hunger as negative\nrewards and pleasure and food intake as positive rewards. Reinforcement has been carefully\nstudied by animal psychologists for over 60 years.\nRewards were introduced in Chapter 17, where they served to deﬁne optimal policies\nin Markov decision processes (MDPs). An optimal policy is a policy that maximizes the\nexpected total reward. The task of reinforcement learning is to use observed rewards to learn\nan optimal (or nearly optimal) policy for the environment. Whereas in Chapter 17 the agent\nhas a complete model of the environment and knows the reward function, here we assume no\n830 Section 21.1.\nIntroduction\n831\nprior knowledge of either. Imagine playing a new game whose rules you don’t know; after a\nhundred or so moves, your opponent announces, “You lose.” This is reinforcement learning\nin a nutshell.\nIn many complex domains, reinforcement learning is the only feasible way to train a\nprogram to perform at high levels. For example, in game playing, it is very hard for a human\nto provide accurate and consistent evaluations of large numbers of positions, which would be\nneeded to train an evaluation function directly from examples. Instead, the program can be\ntold when it has won or lost, and it can use this information to learn an evaluation function\nthat gives reasonably accurate estimates of the probability of winning from any given position.\nSimilarly, it is extremely difﬁcult to program an agent to ﬂy a helicopter; yet given appropriate\nnegative rewards for crashing, wobbling, or deviating from a set course, an agent can learn to\nﬂy by itself.",
  "Similarly, it is extremely difﬁcult to program an agent to ﬂy a helicopter; yet given appropriate\nnegative rewards for crashing, wobbling, or deviating from a set course, an agent can learn to\nﬂy by itself.\nReinforcement learning might be considered to encompass all of AI: an agent is placed\nin an environment and must learn to behave successfully therein. To keep the chapter man-\nageable, we will concentrate on simple environments and simple agent designs. For the most\npart, we will assume a fully observable environment, so that the current state is supplied by\neach percept. On the other hand, we will assume that the agent does not know how the en-\nvironment works or what its actions do, and we will allow for probabilistic action outcomes.\nThus, the agent faces an unknown Markov decision process. We will consider three of the\nagent designs ﬁrst introduced in Chapter 2:\n• A utility-based agent learns a utility function on states and uses it to select actions that\nmaximize the expected outcome utility.\n• A Q-learning agent learns an action-utility function, or Q-function, giving the ex-\nQ-LEARNING\nQ-FUNCTION\npected utility of taking a given action in a given state.\n• A reﬂex agent learns a policy that maps directly from states to actions.\nA utility-based agent must also have a model of the environment in order to make decisions,\nbecause it must know the states to which its actions will lead. For example, in order to make\nuse of a backgammon evaluation function, a backgammon program must know what its legal\nmoves are and how they affect the board position. Only in this way can it apply the utility\nfunction to the outcome states. A Q-learning agent, on the other hand, can compare the\nexpected utilities for its available choices without needing to know their outcomes, so it does\nnot need a model of the environment. On the other hand, because they do not know where\ntheir actions lead, Q-learning agents cannot look ahead; this can seriously restrict their ability\nto learn, as we shall see.\nWe begin in Section 21.2 with passive learning, where the agent’s policy is ﬁxed and\nPASSIVE LEARNING\nthe task is to learn the utilities of states (or state–action pairs); this could also involve learning\na model of the environment. Section 21.3 covers active learning, where the agent must also\nACTIVE LEARNING\nlearn what to do. The principal issue is exploration: an agent must experience as much as\nEXPLORATION",
  "a model of the environment. Section 21.3 covers active learning, where the agent must also\nACTIVE LEARNING\nlearn what to do. The principal issue is exploration: an agent must experience as much as\nEXPLORATION\npossible of its environment in order to learn how to behave in it. Section 21.4 discusses how\nan agent can use inductive learning to learn much faster from its experiences. Section 21.5\ncovers methods for learning direct policy representations in reﬂex agents. An understanding\nof Markov decision processes (Chapter 17) is essential for this chapter. 832\nChapter\n21.\nReinforcement Learning\n21.2\nPASSIVE REINFORCEMENT LEARNING\nTo keep things simple, we start with the case of a passive learning agent using a state-based\nrepresentation in a fully observable environment. In passive learning, the agent’s policy π\nis ﬁxed: in state s, it always executes the action π(s). Its goal is simply to learn how good\nthe policy is—that is, to learn the utility function Uπ(s). We will use as our example the\n4 × 3 world introduced in Chapter 17. Figure 21.1 shows a policy for that world and the\ncorresponding utilities. Clearly, the passive learning task is similar to the policy evaluation\ntask, part of the policy iteration algorithm described in Section 17.3. The main difference\nis that the passive learning agent does not know the transition model P(s′ | s, a), which\nspeciﬁes the probability of reaching state s′ from state s after doing action a; nor does it\nknow the reward function R(s), which speciﬁes the reward for each state.\n–1\n+1\n1\n2\n3\n1\n2\n3\n4\n1\n2\n3\n1\n2\n3\n–1\n+ 1\n4\n0.611\n0.812\n0.655\n0.762\n0.918\n0.705\n0.660\n0.868\n 0.388\n(a)\n(b)\nFigure 21.1\n(a) A policy π for the 4 × 3 world; this policy happens to be optimal with\nrewards of R(s) = −0.04 in the nonterminal states and no discounting. (b) The utilities of\nthe states in the 4 ×3 world, given policy π.\nThe agent executes a set of trials in the environment using its policy π. In each trial, the\nTRIAL\nagent starts in state (1,1) and experiences a sequence of state transitions until it reaches one\nof the terminal states, (4,2) or (4,3). Its percepts supply both the current state and the reward\nreceived in that state. Typical trials might look like this:\n(1, 1)-.04⇝(1, 2)-.04⇝(1, 3)-.04⇝(1, 2)-.04⇝(1, 3)-.04⇝(2, 3)-.04⇝(3, 3)-.04⇝(4, 3)+1\n(1, 1)-.04⇝(1, 2)-.04⇝(1, 3)-.04⇝(2, 3)-.04⇝(3, 3)-.04⇝(3, 2)-.04⇝(3, 3)-.04⇝(4, 3)+1\n(1, 1)-.04⇝(2, 1)-.04⇝(3, 1)-.04⇝(3, 2)-.04⇝(4, 2)-1 .",
  "(1, 1)-.04⇝(1, 2)-.04⇝(1, 3)-.04⇝(1, 2)-.04⇝(1, 3)-.04⇝(2, 3)-.04⇝(3, 3)-.04⇝(4, 3)+1\n(1, 1)-.04⇝(1, 2)-.04⇝(1, 3)-.04⇝(2, 3)-.04⇝(3, 3)-.04⇝(3, 2)-.04⇝(3, 3)-.04⇝(4, 3)+1\n(1, 1)-.04⇝(2, 1)-.04⇝(3, 1)-.04⇝(3, 2)-.04⇝(4, 2)-1 .\nNote that each state percept is subscripted with the reward received. The object is to use the\ninformation about rewards to learn the expected utility Uπ(s) associated with each nontermi-\nnal state s. The utility is deﬁned to be the expected sum of (discounted) rewards obtained if Section 21.2.\nPassive Reinforcement Learning\n833\npolicy π is followed. As in Equation (17.2) on page 650, we write\nUπ(s) = E\n\" ∞\n\f\nt = 0\nγtR(St)\n#\n(21.1)\nwhere R(s) is the reward for a state, St (a random variable) is the state reached at time t when\nexecuting policy π, and S0 = s. We will include a discount factor γ in all of our equations,\nbut for the 4 × 3 world we will set γ = 1.\n21.2.1\nDirect utility estimation\nA simple method for direct utility estimation was invented in the late 1950s in the area of\nDIRECT UTILITY\nESTIMATION\nadaptive control theory by Widrow and Hoff (1960). The idea is that the utility of a state\nADAPTIVE CONTROL\nTHEORY\nis the expected total reward from that state onward (called the expected reward-to-go), and\nREWARD-TO-GO\neach trial provides a sample of this quantity for each state visited. For example, the ﬁrst trial\nin the set of three given earlier provides a sample total reward of 0.72 for state (1,1), two\nsamples of 0.76 and 0.84 for (1,2), two samples of 0.80 and 0.88 for (1,3), and so on. Thus,\nat the end of each sequence, the algorithm calculates the observed reward-to-go for each state\nand updates the estimated utility for that state accordingly, just by keeping a running average\nfor each state in a table. In the limit of inﬁnitely many trials, the sample average will converge\nto the true expectation in Equation (21.1).\nIt is clear that direct utility estimation is just an instance of supervised learning where\neach example has the state as input and the observed reward-to-go as output. This means\nthat we have reduced reinforcement learning to a standard inductive learning problem, as\ndiscussed in Chapter 18. Section 21.4 discusses the use of more powerful kinds of represen-\ntations for the utility function. Learning techniques for those representations can be applied\ndirectly to the observed data.\nDirect utility estimation succeeds in reducing the reinforcement learning problem to",
  "tations for the utility function. Learning techniques for those representations can be applied\ndirectly to the observed data.\nDirect utility estimation succeeds in reducing the reinforcement learning problem to\nan inductive learning problem, about which much is known. Unfortunately, it misses a very\nimportant source of information, namely, the fact that the utilities of states are not indepen-\ndent! The utility of each state equals its own reward plus the expected utility of its successor\nstates. That is, the utility values obey the Bellman equations for a ﬁxed policy (see also\nEquation (17.10)):\nUπ(s) = R(s) + γ\n\f\ns′\nP(s′ | s, π(s))Uπ(s′) .\n(21.2)\nBy ignoring the connections between states, direct utility estimation misses opportunities for\nlearning. For example, the second of the three trials given earlier reaches the state (3,2),\nwhich has not previously been visited. The next transition reaches (3,3), which is known\nfrom the ﬁrst trial to have a high utility. The Bellman equation suggests immediately that\n(3,2) is also likely to have a high utility, because it leads to (3,3), but direct utility estimation\nlearns nothing until the end of the trial. More broadly, we can view direct utility estimation\nas searching for U in a hypothesis space that is much larger than it needs to be, in that it\nincludes many functions that violate the Bellman equations. For this reason, the algorithm\noften converges very slowly. 834\nChapter\n21.\nReinforcement Learning\nfunction PASSIVE-ADP-AGENT(percept) returns an action\ninputs: percept, a percept indicating the current state s′ and reward signal r ′\npersistent: π, a ﬁxed policy\nmdp, an MDP with model P, rewards R, discount γ\nU , a table of utilities, initially empty\nNsa, a table of frequencies for state–action pairs, initially zero\nNs′|sa, a table of outcome frequencies given state–action pairs, initially zero\ns, a, the previous state and action, initially null\nif s′ is new then U [s′] ←r ′; R[s′] ←r ′\nif s is not null then\nincrement Nsa[s,a] and Ns′|sa[s′,s,a]\nfor each t such that Ns′|sa[t,s,a] is nonzero do\nP(t | s, a) ←Ns′|sa[t,s,a] / Nsa[s,a]\nU ←POLICY-EVALUATION(π,U ,mdp)\nif s′.TERMINAL? then s,a ←null else s,a ←s′,π[s′]\nreturn a\nFigure 21.2\nA passive reinforcement learning agent based on adaptive dynamic program-\nming. The POLICY-EVALUATION function solves the ﬁxed-policy Bellman equations, as\ndescribed on page 657.\n21.2.2\nAdaptive dynamic programming",
  "return a\nFigure 21.2\nA passive reinforcement learning agent based on adaptive dynamic program-\nming. The POLICY-EVALUATION function solves the ﬁxed-policy Bellman equations, as\ndescribed on page 657.\n21.2.2\nAdaptive dynamic programming\nAn adaptive dynamic programming (or ADP) agent takes advantage of the constraints\nADAPTIVE DYNAMIC\nPROGRAMMING\namong the utilities of states by learning the transition model that connects them and solv-\ning the corresponding Markov decision process using a dynamic programming method. For\na passive learning agent, this means plugging the learned transition model P(s′ | s, π(s)) and\nthe observed rewards R(s) into the Bellman equations (21.2) to calculate the utilities of the\nstates. As we remarked in our discussion of policy iteration in Chapter 17, these equations\nare linear (no maximization involved) so they can be solved using any linear algebra pack-\nage. Alternatively, we can adopt the approach of modiﬁed policy iteration (see page 657),\nusing a simpliﬁed value iteration process to update the utility estimates after each change to\nthe learned model. Because the model usually changes only slightly with each observation,\nthe value iteration process can use the previous utility estimates as initial values and should\nconverge quite quickly.\nThe process of learning the model itself is easy, because the environment is fully ob-\nservable. This means that we have a supervised learning task where the input is a state–action\npair and the output is the resulting state. In the simplest case, we can represent the tran-\nsition model as a table of probabilities. We keep track of how often each action outcome\noccurs and estimate the transition probability P(s′ | s, a) from the frequency with which s′\nis reached when executing a in s. For example, in the three trials given on page 832, Right\nis executed three times in (1,3) and two out of three times the resulting state is (2,3), so\nP((2, 3) | (1, 3), Right) is estimated to be 2/3. Section 21.2.\nPassive Reinforcement Learning\n835\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n20\n40\n60\n80\n100\nUtility estimates\nNumber of trials\n(1,1)\n(1,3)\n(3,2)\n(3,3)\n(4,3)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0\n20\n40\n60\n80\n100\nRMS error in utility\nNumber of trials\n(a)\n(b)\nFigure 21.3\nThe passive ADP learning curves for the 4×3 world, given the optimal policy\nshown in Figure 21.1. (a) The utility estimates for a selected subset of states, as a function",
  "0\n20\n40\n60\n80\n100\nRMS error in utility\nNumber of trials\n(a)\n(b)\nFigure 21.3\nThe passive ADP learning curves for the 4×3 world, given the optimal policy\nshown in Figure 21.1. (a) The utility estimates for a selected subset of states, as a function\nof the number of trials. Notice the large changes occurring around the 78th trial—this is the\nﬁrst time that the agent falls into the −1 terminal state at (4,2). (b) The root-mean-square\nerror (see Appendix A) in the estimate for U(1, 1), averaged over 20 runs of 100 trials each.\nThe full agent program for a passive ADP agent is shown in Figure 21.2. Its perfor-\nmance on the 4 × 3 world is shown in Figure 21.3. In terms of how quickly its value es-\ntimates improve, the ADP agent is limited only by its ability to learn the transition model.\nIn this sense, it provides a standard against which to measure other reinforcement learning\nalgorithms. It is, however, intractable for large state spaces. In backgammon, for example, it\nwould involve solving roughly 1050 equations in 1050 unknowns.\nA reader familiar with the Bayesian learning ideas of Chapter 20 will have noticed that\nthe algorithm in Figure 21.2 is using maximum-likelihood estimation to learn the transition\nmodel; moreover, by choosing a policy based solely on the estimated model it is acting as\nif the model were correct. This is not necessarily a good idea! For example, a taxi agent\nthat didn’t know about how trafﬁc lights might ignore a red light once or twice without no\nill effects and then formulate a policy to ignore red lights from then on. Instead, it might\nbe a good idea to choose a policy that, while not optimal for the model estimated by maxi-\nmum likelihood, works reasonably well for the whole range of models that have a reasonable\nchance of being the true model. There are two mathematical approaches that have this ﬂavor.\nThe ﬁrst approach, Bayesian reinforcement learning, assumes a prior probability\nBAYESIAN\nREINFORCEMENT\nLEARNING\nP(h) for each hypothesis h about what the true model is; the posterior probability P(h | e) is\nobtained in the usual way by Bayes’ rule given the observations to date. Then, if the agent has\ndecided to stop learning, the optimal policy is the one that gives the highest expected utility.\nLet uπ\nh be the expected utility, averaged over all possible start states, obtained by executing\npolicy π in model h. Then we have\nπ∗= argmax\nπ\n\f\nh\nP(h | e)uπ\nh . 836\nChapter\n21.\nReinforcement Learning",
  "Let uπ\nh be the expected utility, averaged over all possible start states, obtained by executing\npolicy π in model h. Then we have\nπ∗= argmax\nπ\n\f\nh\nP(h | e)uπ\nh . 836\nChapter\n21.\nReinforcement Learning\nIn some special cases, this policy can even be computed! If the agent will continue learning\nin the future, however, then ﬁnding an optimal policy becomes considerably more difﬁcult,\nbecause the agent must consider the effects of future observations on its beliefs about the\ntransition model. The problem becomes a POMDP whose belief states are distributions over\nmodels. This concept provides an analytical foundation for understanding the exploration\nproblem described in Section 21.3.\nThe second approach, derived from robust control theory, allows for a set of possible\nROBUST CONTROL\nTHEORY\nmodels H and deﬁnes an optimal robust policy as one that gives the best outcome in the worst\ncase over H:\nπ∗= argmax\nπ\nmin\nh uπ\nh .\nOften, the set H will be the set of models that exceed some likelihood threshold on P(h | e),\nso the robust and Bayesian approaches are related. Sometimes, the robust solution can be\ncomputed efﬁciently. There are, moreover, reinforcement learning algorithms that tend to\nproduce robust solutions, although we do not cover them here.\n21.2.3\nTemporal-difference learning\nSolving the underlying MDP as in the preceding section is not the only way to bring the\nBellman equations to bear on the learning problem. Another way is to use the observed\ntransitions to adjust the utilities of the observed states so that they agree with the constraint\nequations. Consider, for example, the transition from (1,3) to (2,3) in the second trial on\npage 832. Suppose that, as a result of the ﬁrst trial, the utility estimates are U π(1, 3) = 0.84\nand Uπ(2, 3) = 0.92. Now, if this transition occurred all the time, we would expect the utili-\nties to obey the equation\nUπ(1, 3) = −0.04 + Uπ(2, 3) ,\nso Uπ(1, 3) would be 0.88. Thus, its current estimate of 0.84 might be a little low and should\nbe increased. More generally, when a transition occurs from state s to state s′, we apply the\nfollowing update to Uπ(s):\nUπ(s) ←Uπ(s) + α(R(s) + γ Uπ(s′) −Uπ(s)) .\n(21.3)\nHere, α is the learning rate parameter. Because this update rule uses the difference in utilities\nbetween successive states, it is often called the temporal-difference, or TD, equation.\nTEMPORAL-\nDIFFERENCE\nAll temporal-difference methods work by adjusting the utility estimates towards the",
  "between successive states, it is often called the temporal-difference, or TD, equation.\nTEMPORAL-\nDIFFERENCE\nAll temporal-difference methods work by adjusting the utility estimates towards the\nideal equilibrium that holds locally when the utility estimates are correct. In the case of pas-\nsive learning, the equilibrium is given by Equation (21.2). Now Equation (21.3) does in fact\ncause the agent to reach the equilibrium given by Equation (21.2), but there is some subtlety\ninvolved. First, notice that the update involves only the observed successor s′, whereas the\nactual equilibrium conditions involve all possible next states. One might think that this causes\nan improperly large change in Uπ(s) when a very rare transition occurs; but, in fact, because\nrare transitions occur only rarely, the average value of U π(s) will converge to the correct\nvalue. Furthermore, if we change α from a ﬁxed parameter to a function that decreases as\nthe number of times a state has been visited increases, then U π(s) itself will converge to the Section 21.2.\nPassive Reinforcement Learning\n837\nfunction PASSIVE-TD-AGENT(percept) returns an action\ninputs: percept, a percept indicating the current state s′ and reward signal r ′\npersistent: π, a ﬁxed policy\nU , a table of utilities, initially empty\nNs, a table of frequencies for states, initially zero\ns, a, r, the previous state, action, and reward, initially null\nif s′ is new then U [s′] ←r ′\nif s is not null then\nincrement N s[s]\nU [s] ←U [s] + α(Ns[s])(r + γ U [s′] −U [s])\nif s′.TERMINAL? then s,a,r ←null else s,a,r ←s′,π[s′],r ′\nreturn a\nFigure 21.4\nA passive reinforcement learning agent that learns utility estimates using tem-\nporal differences. The step-size function α(n) is chosen to ensure convergence, as described\nin the text.\ncorrect value.1 This gives us the agent program shown in Figure 21.4. Figure 21.5 illustrates\nthe performance of the passive TD agent on the 4 × 3 world. It does not learn quite as fast as\nthe ADP agent and shows much higher variability, but it is much simpler and requires much\nless computation per observation. Notice that TD does not need a transition model to perform\nits updates. The environment supplies the connection between neighboring states in the form\nof observed transitions.\nThe ADP approach and the TD approach are actually closely related. Both try to make\nlocal adjustments to the utility estimates in order to make each state “agree” with its succes-",
  "of observed transitions.\nThe ADP approach and the TD approach are actually closely related. Both try to make\nlocal adjustments to the utility estimates in order to make each state “agree” with its succes-\nsors. One difference is that TD adjusts a state to agree with its observed successor (Equa-\ntion (21.3)), whereas ADP adjusts the state to agree with all of the successors that might\noccur, weighted by their probabilities (Equation (21.2)). This difference disappears when\nthe effects of TD adjustments are averaged over a large number of transitions, because the\nfrequency of each successor in the set of transitions is approximately proportional to its prob-\nability. A more important difference is that whereas TD makes a single adjustment per ob-\nserved transition, ADP makes as many as it needs to restore consistency between the utility\nestimates U and the environment model P. Although the observed transition makes only a\nlocal change in P, its effects might need to be propagated throughout U . Thus, TD can be\nviewed as a crude but efﬁcient ﬁrst approximation to ADP.\nEach adjustment made by ADP could be seen, from the TD point of view, as a re-\nsult of a “pseudoexperience” generated by simulating the current environment model. It\nis possible to extend the TD approach to use an environment model to generate several\npseudoexperiences—transitions that the TD agent can imagine might happen, given its current\nmodel. For each observed transition, the TD agent can generate a large number of imaginary\n1 The technical conditions are given on page 725. In Figure 21.5 we have used α(n) = 60/(59 + n), which\nsatisﬁes the conditions. 838\nChapter\n21.\nReinforcement Learning\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n100\n200\n300\n400\n500\nUtility estimates\nNumber of trials\n(1,1)\n(1,3)\n(2,1)\n(3,3)\n(4,3)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0\n20\n40\n60\n80\n100\nRMS error in utility\nNumber of trials\n(a)\n(b)\nFigure 21.5\nThe TD learning curves for the 4 × 3 world. (a) The utility estimates for a\nselected subset of states, as a function of the number of trials. (b) The root-mean-square error\nin the estimate for U(1, 1), averaged over 20 runs of 500 trials each. Only the ﬁrst 100 trials\nare shown to enable comparison with Figure 21.3.\ntransitions. In this way, the resulting utility estimates will approximate more and more closely\nthose of ADP—of course, at the expense of increased computation time.\nIn a similar vein, we can generate more efﬁcient versions of ADP by directly approxi-",
  "those of ADP—of course, at the expense of increased computation time.\nIn a similar vein, we can generate more efﬁcient versions of ADP by directly approxi-\nmating the algorithms for value iteration or policy iteration. Even though the value iteration\nalgorithm is efﬁcient, it is intractable if we have, say, 10100 states. However, many of the\nnecessary adjustments to the state values on each iteration will be extremely tiny. One pos-\nsible approach to generating reasonably good answers quickly is to bound the number of\nadjustments made after each observed transition. One can also use a heuristic to rank the pos-\nsible adjustments so as to carry out only the most signiﬁcant ones. The prioritized sweeping\nPRIORITIZED\nSWEEPING\nheuristic prefers to make adjustments to states whose likely successors have just undergone a\nlarge adjustment in their own utility estimates. Using heuristics like this, approximate ADP\nalgorithms usually can learn roughly as fast as full ADP, in terms of the number of training se-\nquences, but can be several orders of magnitude more efﬁcient in terms of computation. (See\nExercise 21.3.) This enables them to handle state spaces that are far too large for full ADP.\nApproximate ADP algorithms have an additional advantage: in the early stages of learning a\nnew environment, the environment model P often will be far from correct, so there is little\npoint in calculating an exact utility function to match it. An approximation algorithm can use\na minimum adjustment size that decreases as the environment model becomes more accurate.\nThis eliminates the very long value iterations that can occur early in learning due to large\nchanges in the model. Section 21.3.\nActive Reinforcement Learning\n839\n21.3\nACTIVE REINFORCEMENT LEARNING\nA passive learning agent has a ﬁxed policy that determines its behavior. An active agent must\ndecide what actions to take. Let us begin with the adaptive dynamic programming agent and\nconsider how it must be modiﬁed to handle this new freedom.\nFirst, the agent will need to learn a complete model with outcome probabilities for all\nactions, rather than just the model for the ﬁxed policy. The simple learning mechanism used\nby PASSIVE-ADP-AGENT will do just ﬁne for this. Next, we need to take into account the\nfact that the agent has a choice of actions. The utilities it needs to learn are those deﬁned by\nthe optimal policy; they obey the Bellman equations given on page 652, which we repeat here\nfor convenience:",
  "fact that the agent has a choice of actions. The utilities it needs to learn are those deﬁned by\nthe optimal policy; they obey the Bellman equations given on page 652, which we repeat here\nfor convenience:\nU(s) = R(s) + γ max\na\n\f\ns′\nP(s′ | s, a)U(s′) .\n(21.4)\nThese equations can be solved to obtain the utility function U using the value iteration or\npolicy iteration algorithms from Chapter 17. The ﬁnal issue is what to do at each step. Having\nobtained a utility function U that is optimal for the learned model, the agent can extract an\noptimal action by one-step look-ahead to maximize the expected utility; alternatively, if it\nuses policy iteration, the optimal policy is already available, so it should simply execute the\naction the optimal policy recommends. Or should it?\n21.3.1\nExploration\nFigure 21.6 shows the results of one sequence of trials for an ADP agent that follows the\nrecommendation of the optimal policy for the learned model at each step. The agent does\nnot learn the true utilities or the true optimal policy! What happens instead is that, in the\n39th trial, it ﬁnds a policy that reaches the +1 reward along the lower route via (2,1), (3,1),\n(3,2), and (3,3). (See Figure 21.6(b).) After experimenting with minor variations, from the\n276th trial onward it sticks to that policy, never learning the utilities of the other states and\nnever ﬁnding the optimal route via (1,2), (1,3), and (2,3). We call this agent the greedy agent.\nGREEDY AGENT\nRepeated experiments show that the greedy agent very seldom converges to the optimal policy\nfor this environment and sometimes converges to really horrendous policies.\nHow can it be that choosing the optimal action leads to suboptimal results? The answer\nis that the learned model is not the same as the true environment; what is optimal in the\nlearned model can therefore be suboptimal in the true environment. Unfortunately, the agent\ndoes not know what the true environment is, so it cannot compute the optimal action for the\ntrue environment. What, then, is to be done?\nWhat the greedy agent has overlooked is that actions do more than provide rewards\naccording to the current learned model; they also contribute to learning the true model by af-\nfecting the percepts that are received. By improving the model, the agent will receive greater\nrewards in the future.2 An agent therefore must make a tradeoff between exploitation to\nEXPLOITATION",
  "fecting the percepts that are received. By improving the model, the agent will receive greater\nrewards in the future.2 An agent therefore must make a tradeoff between exploitation to\nEXPLOITATION\nmaximize its reward—as reﬂected in its current utility estimates—and exploration to maxi-\nEXPLORATION\n2 Notice the direct analogy to the theory of information value in Chapter 16. 840\nChapter\n21.\nReinforcement Learning\n0\n0.5\n1\n1.5\n2\n0\n50 100 150 200 250 300 350 400 450 500\nRMS error, policy loss\nNumber of trials\nRMS error\nPolicy loss\n1\n2\n3\n1\n2\n3\n–1\n+1\n4\n(a)\n(b)\nFigure 21.6\nPerformance of a greedy ADP agent that executes the action recommended\nby the optimal policy for the learned model. (a) RMS error in the utility estimates averaged\nover the nine nonterminal squares. (b) The suboptimal policy to which the greedy agent\nconverges in this particular sequence of trials.\nmize its long-term well-being. Pure exploitation risks getting stuck in a rut. Pure exploration\nto improve one’s knowledge is of no use if one never puts that knowledge into practice. In the\nreal world, one constantly has to decide between continuing in a comfortable existence and\nstriking out into the unknown in the hopes of discovering a new and better life. With greater\nunderstanding, less exploration is necessary.\nCan we be a little more precise than this? Is there an optimal exploration policy? This\nquestion has been studied in depth in the subﬁeld of statistical decision theory that deals with\nso-called bandit problems. (See sidebar.)\nBANDIT PROBLEM\nAlthough bandit problems are extremely difﬁcult to solve exactly to obtain an optimal\nexploration method, it is nonetheless possible to come up with a reasonable scheme that\nwill eventually lead to optimal behavior by the agent. Technically, any such scheme needs\nto be greedy in the limit of inﬁnite exploration, or GLIE. A GLIE scheme must try each\nGLIE\naction in each state an unbounded number of times to avoid having a ﬁnite probability that\nan optimal action is missed because of an unusually bad series of outcomes. An ADP agent\nusing such a scheme will eventually learn the true environment model. A GLIE scheme must\nalso eventually become greedy, so that the agent’s actions become optimal with respect to the\nlearned (and hence the true) model.\nThere are several GLIE schemes; one of the simplest is to have the agent choose a ran-\ndom action a fraction 1/t of the time and to follow the greedy policy otherwise. While this",
  "learned (and hence the true) model.\nThere are several GLIE schemes; one of the simplest is to have the agent choose a ran-\ndom action a fraction 1/t of the time and to follow the greedy policy otherwise. While this\ndoes eventually converge to an optimal policy, it can be extremely slow. A more sensible\napproach would give some weight to actions that the agent has not tried very often, while\ntending to avoid actions that are believed to be of low utility. This can be implemented by\naltering the constraint equation (21.4) so that it assigns a higher utility estimate to relatively Section 21.3.\nActive Reinforcement Learning\n841\nEXPLORATION AND BANDITS\nIn Las Vegas, a one-armed bandit is a slot machine. A gambler can insert a coin,\npull the lever, and collect the winnings (if any). An n-armed bandit has n levers.\nThe gambler must choose which lever to play on each successive coin—the one\nthat has paid off best, or maybe one that has not been tried?\nThe n-armed bandit problem is a formal model for real problems in many vi-\ntally important areas, such as deciding on the annual budget for AI research and\ndevelopment. Each arm corresponds to an action (such as allocating $20 million\nfor the development of new AI textbooks), and the payoff from pulling the arm cor-\nresponds to the beneﬁts obtained from taking the action (immense). Exploration,\nwhether it is exploration of a new research ﬁeld or exploration of a new shopping\nmall, is risky, is expensive, and has uncertain payoffs; on the other hand, failure to\nexplore at all means that one never discovers any actions that are worthwhile.\nTo formulate a bandit problem properly, one must deﬁne exactly what is meant\nby optimal behavior. Most deﬁnitions in the literature assume that the aim is to\nmaximize the expected total reward obtained over the agent’s lifetime. These deﬁ-\nnitions require that the expectation be taken over the possible worlds that the agent\ncould be in, as well as over the possible results of each action sequence in any given\nworld. Here, a “world” is deﬁned by the transition model P(s′ | s, a). Thus, in or-\nder to act optimally, the agent needs a prior distribution over the possible models.\nThe resulting optimization problems are usually wildly intractable.\nIn some cases—for example, when the payoff of each machine is independent\nand discounted rewards are used—it is possible to calculate a Gittins index for\neach slot machine (Gittins, 1989). The index is a function only of the number of",
  "In some cases—for example, when the payoff of each machine is independent\nand discounted rewards are used—it is possible to calculate a Gittins index for\neach slot machine (Gittins, 1989). The index is a function only of the number of\ntimes the slot machine has been played and how much it has paid off. The index for\neach machine indicates how worthwhile it is to invest more; generally speaking, the\nhigher the expected return and the higher the uncertainty in the utility of a given\nchoice, the better. Choosing the machine with the highest index value gives an\noptimal exploration policy. Unfortunately, no way has been found to extend Gittins\nindices to sequential decision problems.\nOne can use the theory of n-armed bandits to argue for the reasonableness\nof the selection strategy in genetic algorithms. (See Chapter 4.) If you consider\neach arm in an n-armed bandit problem to be a possible string of genes, and the\ninvestment of a coin in one arm to be the reproduction of those genes, then it can\nbe proven that genetic algorithms allocate coins optimally, given an appropriate set\nof independence assumptions. 842\nChapter\n21.\nReinforcement Learning\nunexplored state–action pairs. Essentially, this amounts to an optimistic prior over the possi-\nble environments and causes the agent to behave initially as if there were wonderful rewards\nscattered all over the place. Let us use U+(s) to denote the optimistic estimate of the utility\n(i.e., the expected reward-to-go) of the state s, and let N(s, a) be the number of times action\na has been tried in state s. Suppose we are using value iteration in an ADP learning agent;\nthen we need to rewrite the update equation (Equation (17.6) on page 652) to incorporate the\noptimistic estimate. The following equation does this:\nU+(s) ←R(s) + γ max\na\nf\n\r\u0002\ns′ P(s′ | s, a)U+(s′), N(s, a)\n\u000e\n.\n(21.5)\nHere, f(u, n) is called the exploration function. It determines how greed (preference for\nEXPLORATION\nFUNCTION\nhigh values of u) is traded off against curiosity (preference for actions that have not been\ntried often and have low n). The function f(u, n) should be increasing in u and decreasing\nin n. Obviously, there are many possible functions that ﬁt these conditions. One particularly\nsimple deﬁnition is\nf(u, n) =\n\u0018 R+\nif n < Ne\nu\notherwise\nwhere R+ is an optimistic estimate of the best possible reward obtainable in any state and Ne\nis a ﬁxed parameter. This will have the effect of making the agent try each action–state pair",
  "simple deﬁnition is\nf(u, n) =\n\u0018 R+\nif n < Ne\nu\notherwise\nwhere R+ is an optimistic estimate of the best possible reward obtainable in any state and Ne\nis a ﬁxed parameter. This will have the effect of making the agent try each action–state pair\nat least Ne times.\nThe fact that U+ rather than U appears on the right-hand side of Equation (21.5) is\nvery important. As exploration proceeds, the states and actions near the start state might well\nbe tried a large number of times. If we used U, the more pessimistic utility estimate, then\nthe agent would soon become disinclined to explore further aﬁeld. The use of U + means\nthat the beneﬁts of exploration are propagated back from the edges of unexplored regions,\nso that actions that lead toward unexplored regions are weighted more highly, rather than\njust actions that are themselves unfamiliar. The effect of this exploration policy can be seen\nclearly in Figure 21.7, which shows a rapid convergence toward optimal performance, unlike\nthat of the greedy approach. A very nearly optimal policy is found after just 18 trials. Notice\nthat the utility estimates themselves do not converge as quickly. This is because the agent\nstops exploring the unrewarding parts of the state space fairly soon, visiting them only “by\naccident” thereafter. However, it makes perfect sense for the agent not to care about the exact\nutilities of states that it knows are undesirable and can be avoided.\n21.3.2\nLearning an action-utility function\nNow that we have an active ADP agent, let us consider how to construct an active temporal-\ndifference learning agent. The most obvious change from the passive case is that the agent\nis no longer equipped with a ﬁxed policy, so, if it learns a utility function U, it will need to\nlearn a model in order to be able to choose an action based on U via one-step look-ahead.\nThe model acquisition problem for the TD agent is identical to that for the ADP agent. What\nof the TD update rule itself? Perhaps surprisingly, the update rule (21.3) remains unchanged.\nThis might seem odd, for the following reason: Suppose the agent takes a step that normally Section 21.3.\nActive Reinforcement Learning\n843\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\n2\n2.2\n0\n20\n40\n60\n80\n100\nUtility estimates\nNumber of trials\n(1,1)\n(1,2)\n(1,3)\n(2,3)\n(3,2)\n(3,3)\n(4,3)\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n0\n20\n40\n60\n80\n100\nRMS error, policy loss\nNumber of trials\nRMS error\nPolicy loss\n(a)\n(b)\nFigure 21.7\nPerformance of the exploratory ADP agent. using R+ = 2 and Ne = 5. (a)",
  "(1,1)\n(1,2)\n(1,3)\n(2,3)\n(3,2)\n(3,3)\n(4,3)\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n0\n20\n40\n60\n80\n100\nRMS error, policy loss\nNumber of trials\nRMS error\nPolicy loss\n(a)\n(b)\nFigure 21.7\nPerformance of the exploratory ADP agent. using R+ = 2 and Ne = 5. (a)\nUtility estimates for selected states over time. (b) The RMS error in utility values and the\nassociated policy loss.\nleads to a good destination, but because of nondeterminism in the environment the agent ends\nup in a catastrophic state. The TD update rule will take this as seriously as if the outcome had\nbeen the normal result of the action, whereas one might suppose that, because the outcome\nwas a ﬂuke, the agent should not worry about it too much. In fact, of course, the unlikely\noutcome will occur only infrequently in a large set of training sequences; hence in the long\nrun its effects will be weighted proportionally to its probability, as we would hope. Once\nagain, it can be shown that the TD algorithm will converge to the same values as ADP as the\nnumber of training sequences tends to inﬁnity.\nThere is an alternative TD method, called Q-learning, which learns an action-utility\nrepresentation instead of learning utilities. We will use the notation Q(s, a) to denote the\nvalue of doing action a in state s. Q-values are directly related to utility values as follows:\nU(s) = max\na\nQ(s, a) .\n(21.6)\nQ-functions may seem like just another way of storing utility information, but they have a\nvery important property: a TD agent that learns a Q-function does not need a model of the\nform P(s′ | s, a), either for learning or for action selection. For this reason, Q-learning is\ncalled a model-free method. As with utilities, we can write a constraint equation that must\nMODEL-FREE\nhold at equilibrium when the Q-values are correct:\nQ(s, a) = R(s) + γ\n\f\ns′\nP(s′ | s, a) max\na′\nQ(s′, a′) .\n(21.7)\nAs in the ADP learning agent, we can use this equation directly as an update equation for\nan iteration process that calculates exact Q-values, given an estimated model. This does,\nhowever, require that a model also be learned, because the equation uses P(s′ | s, a). The\ntemporal-difference approach, on the other hand, requires no model of state transitions—all 844\nChapter\n21.\nReinforcement Learning\nfunction Q-LEARNING-AGENT(percept) returns an action\ninputs: percept, a percept indicating the current state s′ and reward signal r ′\npersistent: Q, a table of action values indexed by state and action, initially zero",
  "21.\nReinforcement Learning\nfunction Q-LEARNING-AGENT(percept) returns an action\ninputs: percept, a percept indicating the current state s′ and reward signal r ′\npersistent: Q, a table of action values indexed by state and action, initially zero\nNsa, a table of frequencies for state–action pairs, initially zero\ns, a, r, the previous state, action, and reward, initially null\nif TERMINAL?(s) then Q[s,None] ←r ′\nif s is not null then\nincrement Nsa[s,a]\nQ[s,a] ←Q[s,a] + α(Nsa[s, a])(r + γ maxa′ Q[s′, a′] −Q[s,a])\ns,a,r ←s′,argmaxa′ f(Q[s′, a′], Nsa[s′, a′]),r ′\nreturn a\nFigure 21.8\nAn exploratory Q-learning agent. It is an active learner that learns the value\nQ(s, a) of each action in each situation. It uses the same exploration function f as the ex-\nploratory ADP agent, but avoids having to learn the transition model because the Q-value of\na state can be related directly to those of its neighbors.\nit needs are the Q values. The update equation for TD Q-learning is\nQ(s, a) ←Q(s, a) + α(R(s) + γ max\na′\nQ(s′, a′) −Q(s, a)) ,\n(21.8)\nwhich is calculated whenever action a is executed in state s leading to state s′.\nThe complete agent design for an exploratory Q-learning agent using TD is shown in\nFigure 21.8. Notice that it uses exactly the same exploration function f as that used by the\nexploratory ADP agent—hence the need to keep statistics on actions taken (the table N). If\na simpler exploration policy is used—say, acting randomly on some fraction of steps, where\nthe fraction decreases over time—then we can dispense with the statistics.\nQ-learning has a close relative called SARSA (for State-Action-Reward-State-Action).\nSARSA\nThe update rule for SARSA is very similar to Equation (21.8):\nQ(s, a) ←Q(s, a) + α(R(s) + γ Q(s′, a′) −Q(s, a)) ,\n(21.9)\nwhere a′ is the action actually taken in state s′. The rule is applied at the end of each\ns, a, r, s′, a′ quintuplet—hence the name. The difference from Q-learning is quite subtle:\nwhereas Q-learning backs up the best Q-value from the state reached in the observed transi-\ntion, SARSA waits until an action is actually taken and backs up the Q-value for that action.\nNow, for a greedy agent that always takes the action with best Q-value, the two algorithms\nare identical. When exploration is happening, however, they differ signiﬁcantly. Because\nQ-learning uses the best Q-value, it pays no attention to the actual policy being followed—it",
  "are identical. When exploration is happening, however, they differ signiﬁcantly. Because\nQ-learning uses the best Q-value, it pays no attention to the actual policy being followed—it\nis an off-policy learning algorithm, whereas SARSA is an on-policy algorithm. Q-learning is\nOFF-POLICY\nON-POLICY\nmore ﬂexible than SARSA, in the sense that a Q-learning agent can learn how to behave well\neven when guided by a random or adversarial exploration policy. On the other hand, SARSA\nis more realistic: for example, if the overall policy is even partly controlled by other agents, it\nis better to learn a Q-function for what will actually happen rather than what the agent would\nlike to happen. Section 21.4.\nGeneralization in Reinforcement Learning\n845\nBoth Q-learning and SARSA learn the optimal policy for the 4 × 3 world, but do so\nat a much slower rate than the ADP agent. This is because the local updates do not enforce\nconsistency among all the Q-values via the model. The comparison raises a general question:\nis it better to learn a model and a utility function or to learn an action-utility function with\nno model? In other words, what is the best way to represent the agent function? This is\nan issue at the foundations of artiﬁcial intelligence. As we stated in Chapter 1, one of the\nkey historical characteristics of much of AI research is its (often unstated) adherence to the\nknowledge-based approach. This amounts to an assumption that the best way to represent\nthe agent function is to build a representation of some aspects of the environment in which\nthe agent is situated.\nSome researchers, both inside and outside AI, have claimed that the availability of\nmodel-free methods such as Q-learning means that the knowledge-based approach is unnec-\nessary. There is, however, little to go on but intuition. Our intuition, for what it’s worth, is that\nas the environment becomes more complex, the advantages of a knowledge-based approach\nbecome more apparent. This is borne out even in games such as chess, checkers (draughts),\nand backgammon (see next section), where efforts to learn an evaluation function by means\nof a model have met with more success than Q-learning methods.\n21.4\nGENERALIZATION IN REINFORCEMENT LEARNING\nSo far, we have assumed that the utility functions and Q-functions learned by the agents are\nrepresented in tabular form with one output value for each input tuple. Such an approach",
  "21.4\nGENERALIZATION IN REINFORCEMENT LEARNING\nSo far, we have assumed that the utility functions and Q-functions learned by the agents are\nrepresented in tabular form with one output value for each input tuple. Such an approach\nworks reasonably well for small state spaces, but the time to convergence and (for ADP) the\ntime per iteration increase rapidly as the space gets larger. With carefully controlled, approx-\nimate ADP methods, it might be possible to handle 10,000 states or more. This sufﬁces for\ntwo-dimensional maze-like environments, but more realistic worlds are out of the question.\nBackgammon and chess are tiny subsets of the real world, yet their state spaces contain on\nthe order of 1020 and 1040 states, respectively. It would be absurd to suppose that one must\nvisit all these states many times in order to learn how to play the game!\nOne way to handle such problems is to use function approximation, which simply\nFUNCTION\nAPPROXIMATION\nmeans using any sort of representation for the Q-function other than a lookup table. The\nrepresentation is viewed as approximate because it might not be the case that the true utility\nfunction or Q-function can be represented in the chosen form. For example, in Chapter 5 we\ndescribed an evaluation function for chess that is represented as a weighted linear function\nof a set of features (or basis functions) f1, . . . , fn:\nBASIS FUNCTION\nˆUθ(s) = θ1 f1(s) + θ2 f2(s) + · · · + θn fn(s) .\nA reinforcement learning algorithm can learn values for the parameters θ = θ1, . . . , θn such\nthat the evaluation function ˆUθ approximates the true utility function. Instead of, say, 1040\nvalues in a table, this function approximator is characterized by, say, n = 20 parameters—\nan enormous compression. Although no one knows the true utility function for chess, no\none believes that it can be represented exactly in 20 numbers. If the approximation is good 846\nChapter\n21.\nReinforcement Learning\nenough, however, the agent might still play excellent chess.3 Function approximation makes\nit practical to represent utility functions for very large state spaces, but that is not its principal\nbeneﬁt. The compression achieved by a function approximator allows the learning agent to\ngeneralize from states it has visited to states it has not visited. That is, the most important\naspect of function approximation is not that it requires less space, but that it allows for induc-",
  "generalize from states it has visited to states it has not visited. That is, the most important\naspect of function approximation is not that it requires less space, but that it allows for induc-\ntive generalization over input states. To give you some idea of the power of this effect: by\nexamining only one in every 1012 of the possible backgammon states, it is possible to learn a\nutility function that allows a program to play as well as any human (Tesauro, 1992).\nOn the ﬂip side, of course, there is the problem that there could fail to be any function\nin the chosen hypothesis space that approximates the true utility function sufﬁciently well.\nAs in all inductive learning, there is a tradeoff between the size of the hypothesis space and\nthe time it takes to learn the function. A larger hypothesis space increases the likelihood that\na good approximation can be found, but also means that convergence is likely to be delayed.\nLet us begin with the simplest case, which is direct utility estimation. (See Section 21.2.)\nWith function approximation, this is an instance of supervised learning. For example, sup-\npose we represent the utilities for the 4 × 3 world using a simple linear function. The features\nof the squares are just their x and y coordinates, so we have\nˆUθ(x, y) = θ0 + θ1x + θ2y .\n(21.10)\nThus, if (θ0, θ1, θ2) = (0.5, 0.2, 0.1), then ˆUθ(1, 1) = 0.8. Given a collection of trials, we ob-\ntain a set of sample values of ˆUθ(x, y), and we can ﬁnd the best ﬁt, in the sense of minimizing\nthe squared error, using standard linear regression. (See Chapter 18.)\nFor reinforcement learning, it makes more sense to use an online learning algorithm\nthat updates the parameters after each trial. Suppose we run a trial and the total reward\nobtained starting at (1,1) is 0.4. This suggests that ˆUθ(1, 1), currently 0.8, is too large and\nmust be reduced. How should the parameters be adjusted to achieve this? As with neural-\nnetwork learning, we write an error function and compute its gradient with respect to the\nparameters. If uj(s) is the observed total reward from state s onward in the jth trial, then\nthe error is deﬁned as (half) the squared difference of the predicted total and the actual total:\nEj(s) = ( ˆUθ(s) −uj(s))2/2. The rate of change of the error with respect to each parameter\nθi is ∂Ej/∂θi, so to move the parameter in the direction of decreasing the error, we want\nθi ←θi −α ∂Ej(s)\n∂θi\n= θi + α (uj(s) −ˆUθ(s))∂ˆUθ(s)\n∂θi\n.\n(21.11)",
  "Ej(s) = ( ˆUθ(s) −uj(s))2/2. The rate of change of the error with respect to each parameter\nθi is ∂Ej/∂θi, so to move the parameter in the direction of decreasing the error, we want\nθi ←θi −α ∂Ej(s)\n∂θi\n= θi + α (uj(s) −ˆUθ(s))∂ˆUθ(s)\n∂θi\n.\n(21.11)\nThis is called the Widrow–Hoff rule, or the delta rule, for online least-squares. For the\nWIDROW–HOFF RULE\nDELTA RULE\nlinear function approximator ˆUθ(s) in Equation (21.10), we get three simple update rules:\nθ0 ←θ0 + α (uj(s) −ˆUθ(s)) ,\nθ1 ←θ1 + α (uj(s) −ˆUθ(s))x ,\nθ2 ←θ2 + α (uj(s) −ˆUθ(s))y .\n3 We do know that the exact utility function can be represented in a page or two of Lisp, Java, or C++. That is,\nit can be represented by a program that solves the game exactly every time it is called. We are interested only in\nfunction approximators that use a reasonable amount of computation. It might in fact be better to learn a very\nsimple function approximator and combine it with a certain amount of look-ahead search. The tradeoffs involved\nare currently not well understood. Section 21.4.\nGeneralization in Reinforcement Learning\n847\nWe can apply these rules to the example where ˆUθ(1, 1) is 0.8 and uj(1, 1) is 0.4. θ0, θ1,\nand θ2 are all decreased by 0.4α, which reduces the error for (1,1). Notice that changing the\nparameters θ in response to an observed transition between two states also changes the values\nof ˆUθ for every other state! This is what we mean by saying that function approximation\nallows a reinforcement learner to generalize from its experiences.\nWe expect that the agent will learn faster if it uses a function approximator, provided\nthat the hypothesis space is not too large, but includes some functions that are a reasonably\ngood ﬁt to the true utility function. Exercise 21.5 asks you to evaluate the performance of\ndirect utility estimation, both with and without function approximation. The improvement in\nthe 4 × 3 world is noticeable but not dramatic, because this is a very small state space to begin\nwith. The improvement is much greater in a 10 × 10 world with a +1 reward at (10,10). This\nworld is well suited for a linear utility function because the true utility function is smooth\nand nearly linear. (See Exercise 21.8.) If we put the +1 reward at (5,5), the true utility is\nmore like a pyramid and the function approximator in Equation (21.10) will fail miserably.\nAll is not lost, however! Remember that what matters for linear function approximation",
  "more like a pyramid and the function approximator in Equation (21.10) will fail miserably.\nAll is not lost, however! Remember that what matters for linear function approximation\nis that the function be linear in the parameters—the features themselves can be arbitrary\nnonlinear functions of the state variables. Hence, we can include a term such as θ3f3(x, y) =\nθ3",
  "(x −xg)2 + (y −yg)2 that measures the distance to the goal.\nWe can apply these ideas equally well to temporal-difference learners. All we need do is\nadjust the parameters to try to reduce the temporal difference between successive states. The\nnew versions of the TD and Q-learning equations (21.3 on page 836 and 21.8 on page 844)\nare given by\nθi ←θi + α [R(s) + γ ˆUθ(s′) −ˆUθ(s)]∂ˆUθ(s)\n∂θi\n(21.12)\nfor utilities and\nθi ←θi + α [R(s) + γ max\na′\nˆQθ(s′, a′) −ˆQθ(s, a)]∂ˆQθ(s, a)\n∂θi\n(21.13)\nfor Q-values. For passive TD learning, the update rule can be shown to converge to the closest\npossible4 approximation to the true function when the function approximator is linear in the\nparameters. With active learning and nonlinear functions such as neural networks, all bets\nare off: There are some very simple cases in which the parameters can go off to inﬁnity\neven though there are good solutions in the hypothesis space. There are more sophisticated\nalgorithms that can avoid these problems, but at present reinforcement learning with general\nfunction approximators remains a delicate art.\nFunction approximation can also be very helpful for learning a model of the environ-\nment. Remember that learning a model for an observable environment is a supervised learn-\ning problem, because the next percept gives the outcome state. Any of the supervised learning\nmethods in Chapter 18 can be used, with suitable adjustments for the fact that we need to pre-\ndict a complete state description rather than just a Boolean classiﬁcation or a single real value.\nFor a partially observable environment, the learning problem is much more difﬁcult. If we\nknow what the hidden variables are and how they are causally related to each other and to the\n4 The deﬁnition of distance between utility functions is rather technical; see Tsitsiklis and Van Roy (1997). 848\nChapter\n21.\nReinforcement Learning\nobservable variables, then we can ﬁx the structure of a dynamic Bayesian network and use the\nEM algorithm to learn the parameters, as was described in Chapter 20. Inventing the hidden\nvariables and learning the model structure are still open problems. Some practical examples\nare described in Section 21.6.\n21.5\nPOLICY SEARCH\nThe ﬁnal approach we will consider for reinforcement learning problems is called policy\nsearch. In some ways, policy search is the simplest of all the methods in this chapter: the\nPOLICY SEARCH\nidea is to keep twiddling the policy as long as its performance improves, then stop.",
  "search. In some ways, policy search is the simplest of all the methods in this chapter: the\nPOLICY SEARCH\nidea is to keep twiddling the policy as long as its performance improves, then stop.\nLet us begin with the policies themselves. Remember that a policy π is a function that\nmaps states to actions. We are interested primarily in parameterized representations of π that\nhave far fewer parameters than there are states in the state space (just as in the preceding\nsection). For example, we could represent π by a collection of parameterized Q-functions,\none for each action, and take the action with the highest predicted value:\nπ(s) = max\na\nˆQθ(s, a) .\n(21.14)\nEach Q-function could be a linear function of the parameters θ, as in Equation (21.10),\nor it could be a nonlinear function such as a neural network. Policy search will then ad-\njust the parameters θ to improve the policy. Notice that if the policy is represented by Q-\nfunctions, then policy search results in a process that learns Q-functions. This process is\nnot the same as Q-learning! In Q-learning with function approximation, the algorithm ﬁnds\na value of θ such that ˆQθ is “close” to Q∗, the optimal Q-function. Policy search, on the\nother hand, ﬁnds a value of θ that results in good performance; the values found by the two\nmethods may differ very substantially. (For example, the approximate Q-function deﬁned\nby ˆQθ(s, a) = Q∗(s, a)/10 gives optimal performance, even though it is not at all close to\nQ∗.) Another clear instance of the difference is the case where π(s) is calculated using, say,\ndepth-10 look-ahead search with an approximate utility function ˆUθ. A value of θ that gives\ngood results may be a long way from making ˆUθ resemble the true utility function.\nOne problem with policy representations of the kind given in Equation (21.14) is that\nthe policy is a discontinuous function of the parameters when the actions are discrete. (For a\ncontinuous action space, the policy can be a smooth function of the parameters.) That is, there\nwill be values of θ such that an inﬁnitesimal change in θ causes the policy to switch from one\naction to another. This means that the value of the policy may also change discontinuously,\nwhich makes gradient-based search difﬁcult. For this reason, policy search methods often use\na stochastic policy representation πθ(s, a), which speciﬁes the probability of selecting action\nSTOCHASTIC POLICY\na in state s. One popular representation is the softmax function:",
  "a stochastic policy representation πθ(s, a), which speciﬁes the probability of selecting action\nSTOCHASTIC POLICY\na in state s. One popular representation is the softmax function:\nSOFTMAX FUNCTION\nπθ(s, a) = e\nˆQθ(s,a)/\n\f\na′\ne\nˆQθ(s,a′) .\nSoftmax becomes nearly deterministic if one action is much better than the others, but it\nalways gives a differentiable function of θ; hence, the value of the policy (which depends in Section 21.5.\nPolicy Search\n849\na continuous fashion on the action selection probabilities) is a differentiable function of θ.\nSoftmax is a generalization of the logistic function (page 725) to multiple variables.\nNow let us look at methods for improving the policy. We start with the simplest case: a\ndeterministic policy and a deterministic environment. Let ρ(θ) be the policy value, i.e., the\nPOLICY VALUE\nexpected reward-to-go when πθ is executed. If we can derive an expression for ρ(θ) in closed\nform, then we have a standard optimization problem, as described in Chapter 4. We can follow\nthe policy gradient vector ∇θρ(θ) provided ρ(θ) is differentiable. Alternatively, if ρ(θ) is\nPOLICY GRADIENT\nnot available in closed form, we can evaluate πθ simply by executing it and observing the\naccumulated reward. We can follow the empirical gradient by hill climbing—i.e., evaluating\nthe change in policy value for small increments in each parameter. With the usual caveats,\nthis process will converge to a local optimum in policy space.\nWhen the environment (or the policy) is stochastic, things get more difﬁcult. Suppose\nwe are trying to do hill climbing, which requires comparing ρ(θ) and ρ(θ + Δθ) for some\nsmall Δθ. The problem is that the total reward on each trial may vary widely, so estimates\nof the policy value from a small number of trials will be quite unreliable; trying to compare\ntwo such estimates will be even more unreliable. One solution is simply to run lots of trials,\nmeasuring the sample variance and using it to determine that enough trials have been run\nto get a reliable indication of the direction of improvement for ρ(θ). Unfortunately, this is\nimpractical for many real problems where each trial may be expensive, time-consuming, and\nperhaps even dangerous.\nFor the case of a stochastic policy πθ(s, a), it is possible to obtain an unbiased estimate\nof the gradient at θ, ∇θρ(θ), directly from the results of trials executed at θ. For simplicity,\nwe will derive this estimate for the simple case of a nonsequential environment in which the",
  "of the gradient at θ, ∇θρ(θ), directly from the results of trials executed at θ. For simplicity,\nwe will derive this estimate for the simple case of a nonsequential environment in which the\nreward R(a) is obtained immediately after doing action a in the start state s0. In this case,\nthe policy value is just the expected value of the reward, and we have\n∇θρ(θ) = ∇θ\n\f\na\nπθ(s0, a)R(a) =\n\f\na\n(∇θπθ(s0, a))R(a) .\nNow we perform a simple trick so that this summation can be approximated by samples\ngenerated from the probability distribution deﬁned by πθ(s0, a). Suppose that we have N\ntrials in all and the action taken on the jth trial is aj. Then\n∇θρ(θ) =\n\f\na\nπθ(s0, a) · (∇θπθ(s0, a))R(a)\nπθ(s0, a)\n≈1\nN\nN\n\f\nj = 1\n(∇θπθ(s0, aj))R(aj)\nπθ(s0, aj)\n.\nThus, the true gradient of the policy value is approximated by a sum of terms involving\nthe gradient of the action-selection probability in each trial. For the sequential case, this\ngeneralizes to\n∇θρ(θ) ≈1\nN\nN\n\f\nj = 1\n(∇θπθ(s, aj))Rj(s)\nπθ(s, aj)\nfor each state s visited, where aj is executed in s on the jth trial and Rj(s) is the total\nreward received from state s onwards in the jth trial.\nThe resulting algorithm is called\nREINFORCE (Williams, 1992); it is usually much more effective than hill climbing using\nlots of trials at each value of θ. It is still much slower than necessary, however. 850\nChapter\n21.\nReinforcement Learning\nConsider the following task: given two blackjack5 programs, determine which is best.\nOne way to do this is to have each play against a standard “dealer” for a certain number of\nhands and then to measure their respective winnings. The problem with this, as we have seen,\nis that the winnings of each program ﬂuctuate widely depending on whether it receives good\nor bad cards. An obvious solution is to generate a certain number of hands in advance and\nhave each program play the same set of hands. In this way, we eliminate the measurement\nerror due to differences in the cards received. This idea, called correlated sampling, un-\nCORRELATED\nSAMPLING\nderlies a policy-search algorithm called PEGASUS (Ng and Jordan, 2000). The algorithm is\napplicable to domains for which a simulator is available so that the “random” outcomes of\nactions can be repeated. The algorithm works by generating in advance N sequences of ran-\ndom numbers, each of which can be used to run a trial of any policy. Policy search is carried\nout by evaluating each candidate policy using the same set of random sequences to determine",
  "dom numbers, each of which can be used to run a trial of any policy. Policy search is carried\nout by evaluating each candidate policy using the same set of random sequences to determine\nthe action outcomes. It can be shown that the number of random sequences required to ensure\nthat the value of every policy is well estimated depends only on the complexity of the policy\nspace, and not at all on the complexity of the underlying domain.\n21.6\nAPPLICATIONS OF REINFORCEMENT LEARNING\nWe now turn to examples of large-scale applications of reinforcement learning. We consider\napplications in game playing, where the transition model is known and the goal is to learn the\nutility function, and in robotics, where the model is usually unknown.\n21.6.1\nApplications to game playing\nThe ﬁrst signiﬁcant application of reinforcement learning was also the ﬁrst signiﬁcant learn-\ning program of any kind—the checkers program written by Arthur Samuel (1959, 1967).\nSamuel ﬁrst used a weighted linear function for the evaluation of positions, using up to 16\nterms at any one time. He applied a version of Equation (21.12) to update the weights. There\nwere some signiﬁcant differences, however, between his program and current methods. First,\nhe updated the weights using the difference between the current state and the backed-up value\ngenerated by full look-ahead in the search tree. This works ﬁne, because it amounts to view-\ning the state space at a different granularity. A second difference was that the program did\nnot use any observed rewards! That is, the values of terminal states reached in self-play were\nignored. This means that it is theoretically possible for Samuel’s program not to converge, or\nto converge on a strategy designed to lose rather than to win. He managed to avoid this fate\nby insisting that the weight for material advantage should always be positive. Remarkably,\nthis was sufﬁcient to direct the program into areas of weight space corresponding to good\ncheckers play.\nGerry Tesauro’s backgammon program TD-GAMMON (1992) forcefully illustrates the\npotential of reinforcement learning techniques.\nIn earlier work (Tesauro and Sejnowski,\n1989), Tesauro tried learning a neural network representation of Q(s, a) directly from ex-\n5 Also known as twenty-one or pontoon. Section 21.6.\nApplications of Reinforcement Learning\n851\nx\nθ\nFigure 21.9\nSetup for the problem of balancing a long pole on top of a moving cart. The",
  "5 Also known as twenty-one or pontoon. Section 21.6.\nApplications of Reinforcement Learning\n851\nx\nθ\nFigure 21.9\nSetup for the problem of balancing a long pole on top of a moving cart. The\ncart can be jerked left or right by a controller that observes x, θ, ˙x, and ˙θ.\namples of moves labeled with relative values by a human expert. This approach proved\nextremely tedious for the expert. It resulted in a program, called NEUROGAMMON, that was\nstrong by computer standards, but not competitive with human experts. The TD-GAMMON\nproject was an attempt to learn from self-play alone. The only reward signal was given at\nthe end of each game. The evaluation function was represented by a fully connected neural\nnetwork with a single hidden layer containing 40 nodes. Simply by repeated application of\nEquation (21.12), TD-GAMMON learned to play considerably better than NEUROGAMMON,\neven though the input representation contained just the raw board position with no computed\nfeatures. This took about 200,000 training games and two weeks of computer time. Although\nthat may seem like a lot of games, it is only a vanishingly small fraction of the state space.\nWhen precomputed features were added to the input representation, a network with 80 hidden\nnodes was able, after 300,000 training games, to reach a standard of play comparable to that\nof the top three human players worldwide. Kit Woolsey, a top player and analyst, said that\n“There is no question in my mind that its positional judgment is far better than mine.”\n21.6.2\nApplication to robot control\nThe setup for the famous cart–pole balancing problem, also known as the inverted pendu-\nCART–POLE\nlum, is shown in Figure 21.9. The problem is to control the position x of the cart so that\nINVERTED\nPENDULUM\nthe pole stays roughly upright (θ ≈π/2), while staying within the limits of the cart track\nas shown. Several thousand papers in reinforcement learning and control theory have been\npublished on this seemingly simple problem. The cart–pole problem differs from the prob-\nlems described earlier in that the state variables x, θ, ˙x, and ˙θ are continuous. The actions are\nusually discrete: jerk left or jerk right, the so-called bang-bang control regime.\nBANG-BANG\nCONTROL\nThe earliest work on learning for this problem was carried out by Michie and Cham-\nbers (1968). Their BOXES algorithm was able to balance the pole for over an hour after only\nabout 30 trials. Moreover, unlike many subsequent systems, BOXES was implemented with a 852",
  "bers (1968). Their BOXES algorithm was able to balance the pole for over an hour after only\nabout 30 trials. Moreover, unlike many subsequent systems, BOXES was implemented with a 852\nChapter\n21.\nReinforcement Learning\nreal cart and pole, not a simulation. The algorithm ﬁrst discretized the four-dimensional state\nspace into boxes—hence the name. It then ran trials until the pole fell over or the cart hit the\nend of the track. Negative reinforcement was associated with the ﬁnal action in the ﬁnal box\nand then propagated back through the sequence. It was found that the discretization caused\nsome problems when the apparatus was initialized in a position different from those used in\ntraining, suggesting that generalization was not perfect. Improved generalization and faster\nlearning can be obtained using an algorithm that adaptively partitions the state space accord-\ning to the observed variation in the reward, or by using a continuous-state, nonlinear function\napproximator such as a neural network. Nowadays, balancing a triple inverted pendulum is a\ncommon exercise—a feat far beyond the capabilities of most humans.\nStill more impressive is the application of reinforcement learning to helicopter ﬂight\n(Figure 21.10). This work has generally used policy search (Bagnell and Schneider, 2001)\nas well as the PEGASUS algorithm with simulation based on a learned transition model (Ng\net al., 2004). Further details are given in Chapter 25.\nFigure 21.10\nSuperimposed time-lapse images of an autonomous helicopter performing\na very difﬁcult “nose-in circle” maneuver. The helicopter is under the control of a policy\ndeveloped by the PEGASUS policy-search algorithm. A simulator model was developed by\nobserving the effects of various control manipulations on the real helicopter; then the algo-\nrithm was run on the simulator model overnight. A variety of controllers were developed for\ndifferent maneuvers. In all cases, performance far exceeded that of an expert human pilot\nusing remote control. (Image courtesy of Andrew Ng.) Section 21.7.\nSummary\n853\n21.7\nSUMMARY\nThis chapter has examined the reinforcement learning problem: how an agent can become\nproﬁcient in an unknown environment, given only its percepts and occasional rewards. Rein-\nforcement learning can be viewed as a microcosm for the entire AI problem, but it is studied\nin a number of simpliﬁed settings to facilitate progress. The major points are:",
  "forcement learning can be viewed as a microcosm for the entire AI problem, but it is studied\nin a number of simpliﬁed settings to facilitate progress. The major points are:\n• The overall agent design dictates the kind of information that must be learned. The\nthree main designs we covered were the model-based design, using a model P and a\nutility function U ; the model-free design, using an action-utility function Q; and the\nreﬂex design, using a policy π.\n• Utilities can be learned using three approaches:\n1. Direct utility estimation uses the total observed reward-to-go for a given state as\ndirect evidence for learning its utility.\n2. Adaptive dynamic programming (ADP) learns a model and a reward function\nfrom observations and then uses value or policy iteration to obtain the utilities or\nan optimal policy. ADP makes optimal use of the local constraints on utilities of\nstates imposed through the neighborhood structure of the environment.\n3. Temporal-difference (TD) methods update utility estimates to match those of suc-\ncessor states. They can be viewed as simple approximations to the ADP approach\nthat can learn without requiring a transition model. Using a learned model to gen-\nerate pseudoexperiences can, however, result in faster learning.\n• Action-utility functions, or Q-functions, can be learned by an ADP approach or a TD\napproach. With TD, Q-learning requires no model in either the learning or action-\nselection phase. This simpliﬁes the learning problem but potentially restricts the ability\nto learn in complex environments, because the agent cannot simulate the results of\npossible courses of action.\n• When the learning agent is responsible for selecting actions while it learns, it must\ntrade off the estimated value of those actions against the potential for learning useful\nnew information. An exact solution of the exploration problem is infeasible, but some\nsimple heuristics do a reasonable job.\n• In large state spaces, reinforcement learning algorithms must use an approximate func-\ntional representation in order to generalize over states. The temporal-difference signal\ncan be used directly to update parameters in representations such as neural networks.\n• Policy-search methods operate directly on a representation of the policy, attempting\nto improve it based on observed performance. The variation in the performance in a\nstochastic domain is a serious problem; for simulated domains this can be overcome by\nﬁxing the randomness in advance.",
  "to improve it based on observed performance. The variation in the performance in a\nstochastic domain is a serious problem; for simulated domains this can be overcome by\nﬁxing the randomness in advance.\nBecause of its potential for eliminating hand coding of control strategies, reinforcement learn-\ning continues to be one of the most active areas of machine learning research. Applications\nin robotics promise to be particularly valuable; these will require methods for handling con- 854\nChapter\n21.\nReinforcement Learning\ntinuous, high-dimensional, partially observable environments in which successful behaviors\nmay consist of thousands or even millions of primitive actions.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nTuring (1948, 1950) proposed the reinforcement-learning approach, although he was not con-\nvinced of its effectiveness, writing, “the use of punishments and rewards can at best be a part\nof the teaching process.” Arthur Samuel’s work (1959) was probably the earliest successful\nmachine learning research. Although this work was informal and had a number of ﬂaws,\nit contained most of the modern ideas in reinforcement learning, including temporal differ-\nencing and function approximation. Around the same time, researchers in adaptive control\ntheory (Widrow and Hoff, 1960), building on work by Hebb (1949), were training simple net-\nworks using the delta rule. (This early connection between neural networks and reinforcement\nlearning may have led to the persistent misperception that the latter is a subﬁeld of the for-\nmer.) The cart–pole work of Michie and Chambers (1968) can also be seen as a reinforcement\nlearning method with a function approximator. The psychological literature on reinforcement\nlearning is much older; Hilgard and Bower (1975) provide a good survey. Direct evidence for\nthe operation of reinforcement learning in animals has been provided by investigations into\nthe foraging behavior of bees; there is a clear neural correlate of the reward signal in the form\nof a large neuron mapping from the nectar intake sensors directly to the motor cortex (Mon-\ntague et al., 1995). Research using single-cell recording suggests that the dopamine system\nin primate brains implements something resembling value function learning (Schultz et al.,\n1997). The neuroscience text by Dayan and Abbott (2001) describes possible neural imple-\nmentations of temporal-difference learning, while Dayan and Niv (2008) survey the latest",
  "1997). The neuroscience text by Dayan and Abbott (2001) describes possible neural imple-\nmentations of temporal-difference learning, while Dayan and Niv (2008) survey the latest\nevidence from neuroscientiﬁc and behavioral experiments.\nThe connection between reinforcement learning and Markov decision processes was\nﬁrst made by Werbos (1977), but the development of reinforcement learning in AI stems\nfrom work at the University of Massachusetts in the early 1980s (Barto et al., 1981). The\npaper by Sutton (1988) provides a good historical overview. Equation (21.3) in this chapter\nis a special case for λ = 0 of Sutton’s general TD(λ) algorithm. TD(λ) updates the utility\nvalues of all states in a sequence leading up to each transition by an amount that drops off as\nλt for states t steps in the past. TD(1) is identical to the Widrow–Hoff or delta rule. Boyan\n(2002), building on work by Bradtke and Barto (1996), argues that TD(λ) and related algo-\nrithms make inefﬁcient use of experiences; essentially, they are online regression algorithms\nthat converge much more slowly than ofﬂine regression. His LSTD (least-squares temporal\ndifferencing) algorithm is an online algorithm for passive reinforcement learning that gives\nthe same results as ofﬂine regression. Least-squares policy iteration, or LSPI (Lagoudakis\nand Parr, 2003), combines this idea with the policy iteration algorithm, yielding a robust,\nstatistically efﬁcient, model-free algorithm for learning policies.\nThe combination of temporal-difference learning with the model-based generation of\nsimulated experiences was proposed in Sutton’s DYNA architecture (Sutton, 1990). The idea\nof prioritized sweeping was introduced independently by Moore and Atkeson (1993) and Bibliographical and Historical Notes\n855\nPeng and Williams (1993). Q-learning was developed in Watkins’s Ph.D. thesis (1989), while\nSARSA appeared in a technical report by Rummery and Niranjan (1994).\nBandit problems, which model the problem of exploration for nonsequential decisions,\nare analyzed in depth by Berry and Fristedt (1985). Optimal exploration strategies for several\nsettings are obtainable using the technique called Gittins indices (Gittins, 1989). A vari-\nety of exploration methods for sequential decision problems are discussed by Barto et al.\n(1995). Kearns and Singh (1998) and Brafman and Tennenholtz (2000) describe algorithms\nthat explore unknown environments and are guaranteed to converge on near-optimal policies",
  "(1995). Kearns and Singh (1998) and Brafman and Tennenholtz (2000) describe algorithms\nthat explore unknown environments and are guaranteed to converge on near-optimal policies\nin polynomial time. Bayesian reinforcement learning (Dearden et al., 1998, 1999) provides\nanother angle on both model uncertainty and exploration.\nFunction approximation in reinforcement learning goes back to the work of Samuel,\nwho used both linear and nonlinear evaluation functions and also used feature-selection meth-\nods to reduce the feature space. Later methods include the CMAC (Cerebellar Model Artic-\nCMAC\nulation Controller) (Albus, 1975), which is essentially a sum of overlapping local kernel\nfunctions, and the associative neural networks of Barto et al. (1983). Neural networks are\ncurrently the most popular form of function approximator. The best-known application is\nTD-Gammon (Tesauro, 1992, 1995), which was discussed in the chapter. One signiﬁcant\nproblem exhibited by neural-network-based TD learners is that they tend to forget earlier ex-\nperiences, especially those in parts of the state space that are avoided once competence is\nachieved. This can result in catastrophic failure if such circumstances reappear. Function ap-\nproximation based on instance-based learning can avoid this problem (Ormoneit and Sen,\n2002; Forbes, 2002).\nThe convergence of reinforcement learning algorithms using function approximation is\nan extremely technical subject. Results for TD learning have been progressively strength-\nened for the case of linear function approximators (Sutton, 1988; Dayan, 1992; Tsitsiklis and\nVan Roy, 1997), but several examples of divergence have been presented for nonlinear func-\ntions (see Tsitsiklis and Van Roy, 1997, for a discussion). Papavassiliou and Russell (1999)\ndescribe a new type of reinforcement learning that converges with any form of function ap-\nproximator, provided that a best-ﬁt approximation can be found for the observed data.\nPolicy search methods were brought to the fore by Williams (1992), who developed the\nREINFORCE family of algorithms. Later work by Marbach and Tsitsiklis (1998), Sutton et al.\n(2000), and Baxter and Bartlett (2000) strengthened and generalized the convergence results\nfor policy search. The method of correlated sampling for comparing different conﬁgurations\nof a system was described formally by Kahn and Marshall (1953), but seems to have been",
  "for policy search. The method of correlated sampling for comparing different conﬁgurations\nof a system was described formally by Kahn and Marshall (1953), but seems to have been\nknown long before that. Its use in reinforcement learning is due to Van Roy (1998) and Ng\nand Jordan (2000); the latter paper also introduced the PEGASUS algorithm and proved its\nformal properties.\nAs we mentioned in the chapter, the performance of a stochastic policy is a continu-\nous function of its parameters, which helps with gradient-based search methods. This is not\nthe only beneﬁt: Jaakkola et al. (1995) argue that stochastic policies actually work better\nthan deterministic policies in partially observable environments, if both are limited to act-\ning based on the current percept. (One reason is that the stochastic policy is less likely to\nget “stuck” because of some unseen hindrance.) Now, in Chapter 17 we pointed out that 856\nChapter\n21.\nReinforcement Learning\noptimal policies in partially observable MDPs are deterministic functions of the belief state\nrather than the current percept, so we would expect still better results by keeping track of the\nbelief state using the ﬁltering methods of Chapter 15. Unfortunately, belief-state space is\nhigh-dimensional and continuous, and effective algorithms have not yet been developed for\nreinforcement learning with belief states.\nReal-world environments also exhibit enormous complexity in terms of the number\nof primitive actions required to achieve signiﬁcant reward. For example, a robot playing\nsoccer might make a hundred thousand individual leg motions before scoring a goal. One\ncommon method, used originally in animal training, is called reward shaping. This involves\nREWARD SHAPING\nsupplying the agent with additional rewards, called pseudorewards, for “making progress.”\nPSEUDOREWARD\nFor example, in soccer the real reward is for scoring a goal, but pseudorewards might be\ngiven for making contact with the ball or for kicking it toward the goal. Such rewards can\nspeed up learning enormously and are simple to provide, but there is a risk that the agent\nwill learn to maximize the pseudorewards rather than the true rewards; for example, standing\nnext to the ball and “vibrating” causes many contacts with the ball. Ng et al. (1999) show\nthat the agent will still learn the optimal policy provided that the pseudoreward F(s, a, s′)\nsatisﬁes F(s, a, s′) = γΦ(s′) −Φ(s), where Φ is an arbitrary function of the state. Φ can be",
  "that the agent will still learn the optimal policy provided that the pseudoreward F(s, a, s′)\nsatisﬁes F(s, a, s′) = γΦ(s′) −Φ(s), where Φ is an arbitrary function of the state. Φ can be\nconstructed to reﬂect any desirable aspects of the state, such as achievement of subgoals or\ndistance to a goal state.\nThe generation of complex behaviors can also be facilitated by hierarchical reinforce-\nment learning methods, which attempt to solve problems at multiple levels of abstraction—\nHIERARCHICAL\nREINFORCEMENT\nLEARNING\nmuch like the HTN planning methods of Chapter 11. For example, “scoring a goal” can be\nbroken down into “obtain possession,” “dribble towards the goal,” and “shoot;” and each of\nthese can be broken down further into lower-level motor behaviors. The fundamental result\nin this area is due to Forestier and Varaiya (1978), who proved that lower-level behaviors\nof arbitrary complexity can be treated just like primitive actions (albeit ones that can take\nvarying amounts of time) from the point of view of the higher-level behavior that invokes\nthem. Current approaches (Parr and Russell, 1998; Dietterich, 2000; Sutton et al., 2000;\nAndre and Russell, 2002) build on this result to develop methods for supplying an agent\nwith a partial program that constrains the agent’s behavior to have a particular hierarchical\nPARTIAL PROGRAM\nstructure. The partial-programming language for agent programs extends an ordinary pro-\ngramming language by adding primitives for unspeciﬁed choices that must be ﬁlled in by\nlearning. Reinforcement learning is then applied to learn the best behavior consistent with\nthe partial program. The combination of function approximation, shaping, and hierarchical\nreinforcement learning has been shown to solve large-scale problems—for example, policies\nthat execute for 104 steps in state spaces of 10100 states with branching factors of 1030 (Marthi\net al., 2005). One key result (Dietterich, 2000) is that the hierarchical structure provides a\nnatural additive decomposition of the overall utility function into terms that depend on small\nsubsets of the variables deﬁning the state space. This is somewhat analogous to the represen-\ntation theorems underlying the conciseness of Bayes nets (Chapter 14).\nThe topic of distributed and multiagent reinforcement learning was not touched upon in\nthe chapter but is of great current interest. In distributed RL, the aim is to devise methods by",
  "The topic of distributed and multiagent reinforcement learning was not touched upon in\nthe chapter but is of great current interest. In distributed RL, the aim is to devise methods by\nwhich multiple, coordinated agents learn to optimize a common utility function. For example, Bibliographical and Historical Notes\n857\ncan we devise methods whereby separate subagents for robot navigation and robot obstacle\nSUBAGENT\navoidance could cooperatively achieve a combined control system that is globally optimal?\nSome basic results in this direction have been obtained (Guestrin et al., 2002; Russell and\nZimdars, 2003). The basic idea is that each subagent learns its own Q-function from its\nown stream of rewards. For example, a robot-navigation component can receive rewards for\nmaking progress towards the goal, while the obstacle-avoidance component receives negative\nrewards for every collision. Each global decision maximizes the sum of Q-functions and the\nwhole process converges to globally optimal solutions.\nMultiagent RL is distinguished from distributed RL by the presence of agents who\ncannot coordinate their actions (except by explicit communicative acts) and who may not\nshare the same utility function. Thus, multiagent RL deals with sequential game-theoretic\nproblems or Markov games, as deﬁned in Chapter 17. The consequent requirement for ran-\ndomized policies is not a signiﬁcant complication, as we saw on page 848. What does cause\nproblems is the fact that, while an agent is learning to defeat its opponent’s policy, the op-\nponent is changing its policy to defeat the agent. Thus, the environment is nonstationary\n(see page 568). Littman (1994) noted this difﬁculty when introducing the ﬁrst RL algorithms\nfor zero-sum Markov games. Hu and Wellman (2003) present a Q-learning algorithm for\ngeneral-sum games that converges when the Nash equilibrium is unique; when there are mul-\ntiple equilibria, the notion of convergence is not so easy to deﬁne (Shoham et al., 2004).\nSometimes the reward function is not easy to deﬁne. Consider the task of driving a car.\nThere are extreme states (such as crashing the car) that clearly should have a large penalty.\nBut beyond that, it is difﬁcult to be precise about the reward function. However, it is easy\nenough for a human to drive for a while and then tell a robot “do it like that.” The robot then\nhas the task of apprenticeship learning; learning from an example of the task done right,\nAPPRENTICESHIP\nLEARNING",
  "enough for a human to drive for a while and then tell a robot “do it like that.” The robot then\nhas the task of apprenticeship learning; learning from an example of the task done right,\nAPPRENTICESHIP\nLEARNING\nwithout explicit rewards. Ng et al. (2004) and Coates et al. (2009) show how this technique\nworks for learning to ﬂy a helicopter; see Figure 25.25 on page 1002 for an example of the\nacrobatics the resulting policy is capable of. Russell (1998) describes the task of inverse\nreinforcement learning—ﬁguring out what the reward function must be from an example\nINVERSE\nREINFORCEMENT\nLEARNING\npath through that state space. This is useful as a part of apprenticeship learning, or as a part\nof doing science—we can understand an animal or robot by working backwards from what it\ndoes to what its reward function must be.\nThis chapter has dealt only with atomic states—all the agent knows about a state is the\nset of available actions and the utilities of the resulting states (or of state-action pairs). But\nit is also possible to apply reinforcement learning to structured representations rather than\natomic ones; this is called relational reinforcement learning (Tadepalli et al., 2004).\nRELATIONAL\nREINFORCEMENT\nLEARNING\nThe survey by Kaelbling et al. (1996) provides a good entry point to the literature. The\ntext by Sutton and Barto (1998), two of the ﬁeld’s pioneers, focuses on architectures and algo-\nrithms, showing how reinforcement learning weaves together the ideas of learning, planning,\nand acting. The somewhat more technical work by Bertsekas and Tsitsiklis (1996) gives a\nrigorous grounding in the theory of dynamic programming and stochastic convergence. Re-\ninforcement learning papers are published frequently in Machine Learning, in the Journal of\nMachine Learning Research, and in the International Conferences on Machine Learning and\nthe Neural Information Processing Systems meetings. 858\nChapter\n21.\nReinforcement Learning\nEXERCISES\n21.1\nImplement a passive learning agent in a simple environment, such as the 4 × 3 world.\nFor the case of an initially unknown environment model, compare the learning performance\nof the direct utility estimation, TD, and ADP algorithms. Do the comparison for the optimal\npolicy and for several random policies. For which do the utility estimates converge faster?\nWhat happens when the size of the environment is increased? (Try environments with and\nwithout obstacles.)\n21.2",
  "policy and for several random policies. For which do the utility estimates converge faster?\nWhat happens when the size of the environment is increased? (Try environments with and\nwithout obstacles.)\n21.2\nChapter 17 deﬁned a proper policy for an MDP as one that is guaranteed to reach a\nterminal state. Show that it is possible for a passive ADP agent to learn a transition model\nfor which its policy π is improper even if π is proper for the true MDP; with such models,\nthe POLICY-EVALUATION step may fail if γ = 1. Show that this problem cannot arise if\nPOLICY-EVALUATION is applied to the learned model only at the end of a trial.\n21.3\nStarting with the passive ADP agent, modify it to use an approximate ADP algorithm\nas discussed in the text. Do this in two steps:\na. Implement a priority queue for adjustments to the utility estimates. Whenever a state is\nadjusted, all of its predecessors also become candidates for adjustment and should be\nadded to the queue. The queue is initialized with the state from which the most recent\ntransition took place. Allow only a ﬁxed number of adjustments.\nb. Experiment with various heuristics for ordering the priority queue, examining their ef-\nfect on learning rates and computation time.\n21.4\nWrite out the parameter update equations for TD learning with\nˆU(x, y) = θ0 + θ1x + θ2y + θ3\n\t\n(x −xg)2 + (y −yg)2 .\n21.5\nImplement an exploring reinforcement learning agent that uses direct utility estima-\ntion. Make two versions—one with a tabular representation and one using the function ap-\nproximator in Equation (21.10). Compare their performance in three environments:\na. The 4 × 3 world described in the chapter.\nb. A 10 × 10 world with no obstacles and a +1 reward at (10,10).\nc. A 10 × 10 world with no obstacles and a +1 reward at (5,5).\n21.6\nDevise suitable features for reinforcement learning in stochastic grid worlds (general-\nizations of the 4 × 3 world) that contain multiple obstacles and multiple terminal states with\nrewards of +1 or −1.\n21.7\nExtend the standard game-playing environment (Chapter 5) to incorporate a reward\nsignal. Put two reinforcement learning agents into the environment (they may, of course,\nshare the agent program) and have them play against each other. Apply the generalized TD\nupdate rule (Equation (21.12)) to update the evaluation function. You might wish to start with\na simple linear weighted evaluation function and a simple game, such as tic-tac-toe. Exercises\n859\n21.8",
  "update rule (Equation (21.12)) to update the evaluation function. You might wish to start with\na simple linear weighted evaluation function and a simple game, such as tic-tac-toe. Exercises\n859\n21.8\nCompute the true utility function and the best linear approximation in x and y (as in\nEquation (21.10)) for the following environments:\na. A 10 × 10 world with a single +1 terminal state at (10,10).\nb. As in (a), but add a −1 terminal state at (10,1).\nc. As in (b), but add obstacles in 10 randomly selected squares.\nd. As in (b), but place a wall stretching from (5,2) to (5,9).\ne. As in (a), but with the terminal state at (5,5).\nThe actions are deterministic moves in the four directions. In each case, compare the results\nusing three-dimensional plots. For each environment, propose additional features (besides x\nand y) that would improve the approximation and show the results.\n21.9\nImplement the REINFORCE and PEGASUS algorithms and apply them to the 4 × 3\nworld, using a policy family of your own choosing. Comment on the results.\n21.10\nIs reinforcement learning an appropriate abstract model for evolution? What connec-\ntion exists, if any, between hardwired reward signals and evolutionary ﬁtness? 22\nNATURAL LANGUAGE\nPROCESSING\nIn which we see how to make use of the copious knowledge that is expressed in\nnatural language.\nHomo sapiens is set apart from other species by the capacity for language. Somewhere around\n100,000 years ago, humans learned how to speak, and about 7,000 years ago learned to write.\nAlthough chimpanzees, dolphins, and other animals have shown vocabularies of hundreds of\nsigns, only humans can reliably communicate an unbounded number of qualitatively different\nmessages on any topic using discrete signs.\nOf course, there are other attributes that are uniquely human: no other species wears\nclothes, creates representational art, or watches three hours of television a day. But when\nAlan Turing proposed his Test (see Section 1.1.1), he based it on language, not art or TV.\nThere are two main reasons why we want our computer agents to be able to process natural\nlanguages: ﬁrst, to communicate with humans, a topic we take up in Chapter 23, and second,\nto acquire information from written language, the focus of this chapter.\nThere are over a trillion pages of information on the Web, almost all of it in natural\nlanguage. An agent that wants to do knowledge acquisition needs to understand (at least\nKNOWLEDGE\nACQUISITION",
  "There are over a trillion pages of information on the Web, almost all of it in natural\nlanguage. An agent that wants to do knowledge acquisition needs to understand (at least\nKNOWLEDGE\nACQUISITION\npartially) the ambiguous, messy languages that humans use. We examine the problem from\nthe point of view of speciﬁc information-seeking tasks: text classiﬁcation, information re-\ntrieval, and information extraction. One common factor in addressing these tasks is the use of\nlanguage models: models that predict the probability distribution of language expressions.\nLANGUAGE MODEL\n22.1\nLANGUAGE MODELS\nFormal languages, such as the programming languages Java or Python, have precisely deﬁned\nlanguage models. A language can be deﬁned as a set of strings; “print(2 + 2)” is a\nLANGUAGE\nlegal program in the language Python, whereas “2)+(2 print” is not. Since there are an\ninﬁnite number of legal programs, they cannot be enumerated; instead they are speciﬁed by a\nset of rules called a grammar. Formal languages also have rules that deﬁne the meaning or\nGRAMMAR\nsemantics of a program; for example, the rules say that the “meaning” of “2 + 2” is 4, and\nSEMANTICS\nthe meaning of “1/0” is that an error is signaled.\n860 Section 22.1.\nLanguage Models\n861\nNatural languages, such as English or Spanish, cannot be characterized as a deﬁnitive\nset of sentences. Everyone agrees that “Not to be invited is sad” is a sentence of English,\nbut people disagree on the grammaticality of “To be not invited is sad.” Therefore, it is more\nfruitful to deﬁne a natural language model as a probability distribution over sentences rather\nthan a deﬁnitive set. That is, rather than asking if a string of words is or is not a member of\nthe set deﬁning the language, we instead ask for P(S = words)—what is the probability that\na random sentence would be words.\nNatural languages are also ambiguous. “He saw her duck” can mean either that he saw\nAMBIGUITY\na waterfowl belonging to her, or that he saw her move to evade something. Thus, again, we\ncannot speak of a single meaning for a sentence, but rather of a probability distribution over\npossible meanings.\nFinally, natural languages are difﬁcult to deal with because they are very large, and\nconstantly changing. Thus, our language models are, at best, an approximation. We start\nwith the simplest possible approximations and move up from there.\n22.1.1\nN-gram character models\nUltimately, a written text is composed of characters—letters, digits, punctuation, and spaces",
  "with the simplest possible approximations and move up from there.\n22.1.1\nN-gram character models\nUltimately, a written text is composed of characters—letters, digits, punctuation, and spaces\nCHARACTERS\nin English (and more exotic characters in some other languages). Thus, one of the simplest\nlanguage models is a probability distribution over sequences of characters. As in Chapter 15,\nwe write P(c1:N) for the probability of a sequence of N characters, c1 through cN. In one\nWeb collection, P(“the”) = 0.027 and P(“zgq”) = 0.000000002. A sequence of written sym-\nbols of length n is called an n-gram (from the Greek root for writing or letters), with special\ncase “unigram” for 1-gram, “bigram” for 2-gram, and “trigram” for 3-gram. A model of the\nprobability distribution of n-letter sequences is thus called an n-gram model. (But be care-\nN -GRAM MODEL\nful: we can have n-gram models over sequences of words, syllables, or other units; not just\nover characters.)\nAn n-gram model is deﬁned as a Markov chain of order n −1. Recall from page 568\nthat in a Markov chain the probability of character ci depends only on the immediately pre-\nceding characters, not on any other characters. So in a trigram model (Markov chain of\norder 2) we have\nP(ci | c1:i−1) = P(ci | ci−2:i−1) .\nWe can deﬁne the probability of a sequence of characters P(c1:N) under the trigram model\nby ﬁrst factoring with the chain rule and then using the Markov assumption:\nP(c1:N) =\nN\n\u0019\ni = 1\nP(ci | c1:i−1) =\nN\n\u0019\ni = 1\nP(ci | ci−2:i−1) .\nFor a trigram character model in a language with 100 characters, P(Ci|Ci−2:i−1) has a million\nentries, and can be accurately estimated by counting character sequences in a body of text of\n10 million characters or more. We call a body of text a corpus (plural corpora), from the\nCORPUS\nLatin word for body. 862\nChapter\n22.\nNatural Language Processing\nWhat can we do with n-gram character models? One task for which they are well suited\nis language identiﬁcation: given a text, determine what natural language it is written in. This\nLANGUAGE\nIDENTIFICATION\nis a relatively easy task; even with short texts such as “Hello, world” or “Wie geht es dir,” it\nis easy to identify the ﬁrst as English and the second as German. Computer systems identify\nlanguages with greater than 99% accuracy; occasionally, closely related languages, such as\nSwedish and Norwegian, are confused.\nOne approach to language identiﬁcation is to ﬁrst build a trigram character model of",
  "languages with greater than 99% accuracy; occasionally, closely related languages, such as\nSwedish and Norwegian, are confused.\nOne approach to language identiﬁcation is to ﬁrst build a trigram character model of\neach candidate language, P(ci | ci−2:i−1, ℓ), where the variable ℓranges over languages. For\neach ℓthe model is built by counting trigrams in a corpus of that language. (About 100,000\ncharacters of each language are needed.) That gives us a model of P(Text | Language), but\nwe want to select the most probable language given the text, so we apply Bayes’ rule followed\nby the Markov assumption to get the most probable language:\nℓ∗= argmax\nℓ\nP(ℓ| c1:N)\n= argmax\nℓ\nP(ℓ)P(c1:N | ℓ)\n= argmax\nℓ\nP(ℓ)\nN\n\u0019\ni = 1\nP(ci | ci−2:i−1, ℓ)\nThe trigram model can be learned from a corpus, but what about the prior probability P(ℓ)?\nWe may have some estimate of these values; for example, if we are selecting a random Web\npage we know that English is the most likely language and that the probability of Macedonian\nwill be less than 1%. The exact number we select for these priors is not critical because the\ntrigram model usually selects one language that is several orders of magnitude more probable\nthan any other.\nOther tasks for character models include spelling correction, genre classiﬁcation, and\nnamed-entity recognition. Genre classiﬁcation means deciding if a text is a news story, a\nlegal document, a scientiﬁc article, etc. While many features help make this classiﬁcation,\ncounts of punctuation and other character n-gram features go a long way (Kessler et al.,\n1997). Named-entity recognition is the task of ﬁnding names of things in a document and\ndeciding what class they belong to. For example, in the text “Mr. Sopersteen was prescribed\naciphex,” we should recognize that “Mr. Sopersteen” is the name of a person and “aciphex” is\nthe name of a drug. Character-level models are good for this task because they can associate\nthe character sequence “ex ” (“ex” followed by a space) with a drug name and “steen ” with\na person name, and thereby identify words that they have never seen before.\n22.1.2\nSmoothing n-gram models\nThe major complication of n-gram models is that the training corpus provides only an esti-\nmate of the true probability distribution. For common character sequences such as “ th” any\nEnglish corpus will give a good estimate: about 1.5% of all trigrams. On the other hand, “ ht”",
  "mate of the true probability distribution. For common character sequences such as “ th” any\nEnglish corpus will give a good estimate: about 1.5% of all trigrams. On the other hand, “ ht”\nis very uncommon—no dictionary words start with ht. It is likely that the sequence would\nhave a count of zero in a training corpus of standard English. Does that mean we should as-\nsign P(“ th”) = 0? If we did, then the text “The program issues an http request” would have Section 22.1.\nLanguage Models\n863\nan English probability of zero, which seems wrong. We have a problem in generalization: we\nwant our language models to generalize well to texts they haven’t seen yet. Just because we\nhave never seen “ http” before does not mean that our model should claim that it is impossi-\nble. Thus, we will adjust our language model so that sequences that have a count of zero in\nthe training corpus will be assigned a small nonzero probability (and the other counts will be\nadjusted downward slightly so that the probability still sums to 1). The process od adjusting\nthe probability of low-frequency counts is called smoothing.\nSMOOTHING\nThe simplest type of smoothing was suggested by Pierre-Simon Laplace in the 18th cen-\ntury: he said that, in the lack of further information, if a random Boolean variable X has been\nfalse in all n observations so far then the estimate for P(X = true) should be 1/(n+2). That\nis, he assumes that with two more trials, one might be true and one false. Laplace smoothing\n(also called add-one smoothing) is a step in the right direction, but performs relatively poorly.\nA better approach is a backoff model, in which we start by estimating n-gram counts, but for\nBACKOFF MODEL\nany particular sequence that has a low (or zero) count, we back off to (n −1)-grams. Linear\ninterpolation smoothing is a backoff model that combines trigram, bigram, and unigram\nLINEAR\nINTERPOLATION\nSMOOTHING\nmodels by linear interpolation. It deﬁnes the probability estimate as\n*P(ci|ci−2:i−1) = λ3P(ci|ci−2:i−1) + λ2P(ci|ci−1) + λ1P(ci) ,\nwhere λ3 + λ2 + λ1 = 1. The parameter values λi can be ﬁxed, or they can be trained with\nan expectation–maximization algorithm. It is also possible to have the values of λi depend\non the counts: if we have a high count of trigrams, then we weigh them relatively more; if\nonly a low count, then we put more weight on the bigram and unigram models. One camp of\nresearchers has developed ever more sophisticated smoothing models, while the other camp",
  "only a low count, then we put more weight on the bigram and unigram models. One camp of\nresearchers has developed ever more sophisticated smoothing models, while the other camp\nsuggests gathering a larger corpus so that even simple smoothing models work well. Both are\ngetting at the same goal: reducing the variance in the language model.\nOne complication: note that the expression P(ci | ci−2:i−1) asks for P(c1 | c-1:0) when\ni = 1, but there are no characters before c1. We can introduce artiﬁcial characters, for\nexample, deﬁning c0 to be a space character or a special “begin text” character. Or we can\nfall back on lower-order Markov models, in effect deﬁning c-1:0 to be the empty sequence\nand thus P(c1 | c-1:0) = P(c1).\n22.1.3\nModel evaluation\nWith so many possible n-gram models—unigram, bigram, trigram, interpolated smoothing\nwith different values of λ, etc.—how do we know what model to choose? We can evaluate a\nmodel with cross-validation. Split the corpus into a training corpus and a validation corpus.\nDetermine the parameters of the model from the training data. Then evaluate the model on\nthe validation corpus.\nThe evaluation can be a task-speciﬁc metric, such as measuring accuracy on language\nidentiﬁcation. Alternatively we can have a task-independent model of language quality: cal-\nculate the probability assigned to the validation corpus by the model; the higher the proba-\nbility the better. This metric is inconvenient because the probability of a large corpus will\nbe a very small number, and ﬂoating-point underﬂow becomes an issue. A different way of\ndescribing the probability of a sequence is with a measure called perplexity, deﬁned as\nPERPLEXITY 864\nChapter\n22.\nNatural Language Processing\nPerplexity(c1:N) = P(c1:N)−1\nN .\nPerplexity can be thought of as the reciprocal of probability, normalized by sequence length.\nIt can also be thought of as the weighted average branching factor of a model. Suppose there\nare 100 characters in our language, and our model says they are all equally likely. Then for\na sequence of any length, the perplexity will be 100. If some characters are more likely than\nothers, and the model reﬂects that, then the model will have a perplexity less than 100.\n22.1.4\nN-gram word models\nNow we turn to n-gram models over words rather than characters. All the same mechanism\napplies equally to word and character models. The main difference is that the vocabulary—\nVOCABULARY",
  "22.1.4\nN-gram word models\nNow we turn to n-gram models over words rather than characters. All the same mechanism\napplies equally to word and character models. The main difference is that the vocabulary—\nVOCABULARY\nthe set of symbols that make up the corpus and the model—is larger. There are only about\n100 characters in most languages, and sometimes we build character models that are even\nmore restrictive, for example by treating “A” and “a” as the same symbol or by treating all\npunctuation as the same symbol. But with word models we have at least tens of thousands of\nsymbols, and sometimes millions. The wide range is because it is not clear what constitutes a\nword. In English a sequence of letters surrounded by spaces is a word, but in some languages,\nlike Chinese, words are not separated by spaces, and even in English many decisions must be\nmade to have a clear policy on word boundaries: how many words are in “ne’er-do-well”? Or\nin “(Tel:1-800-960-5660x123)”?\nWord n-gram models need to deal with out of vocabulary words. With character mod-\nOUT OF\nVOCABULARY\nels, we didn’t have to worry about someone inventing a new letter of the alphabet.1 But\nwith word models there is always the chance of a new word that was not seen in the training\ncorpus, so we need to model that explicitly in our language model. This can be done by\nadding just one new word to the vocabulary: <UNK>, standing for the unknown word. We\ncan estimate n-gram counts for <UNK> by this trick: go through the training corpus, and\nthe ﬁrst time any individual word appears it is previously unknown, so replace it with the\nsymbol <UNK>. All subsequent appearances of the word remain unchanged. Then compute\nn-gram counts for the corpus as usual, treating <UNK> just like any other word. Then when\nan unknown word appears in a test set, we look up its probability under <UNK>. Sometimes\nmultiple unknown-word symbols are used, for different classes. For example, any string of\ndigits might be replaced with <NUM>, or any email address with <EMAIL>.\nTo get a feeling for what word models can do, we built unigram, bigram, and trigram\nmodels over the words in this book and then randomly sampled sequences of words from the\nmodels. The results are\nUnigram: logical are as are confusion a may right tries agent goal the was . . .\nBigram: systems are very similar computational approach would be represented . . .\nTrigram: planning and scheduling are integrated the success of naive bayes model is . . .",
  "Bigram: systems are very similar computational approach would be represented . . .\nTrigram: planning and scheduling are integrated the success of naive bayes model is . . .\nEven with this small sample, it should be clear that the unigram model is a poor approximation\nof either English or the content of an AI textbook, and that the bigram and trigram models are\n1 With the possible exception of the groundbreaking work of T. Geisel (1955). Section 22.2.\nText Classiﬁcation\n865\nmuch better. The models agree with this assessment: the perplexity was 891 for the unigram\nmodel, 142 for the bigram model and 91 for the trigram model.\nWith the basics of n-gram models—both character- and word-based—established, we\ncan turn now to some language tasks.\n22.2\nTEXT CLASSIFICATION\nWe now consider in depth the task of text classiﬁcation, also known as categorization: given\nTEXT\nCLASSIFICATION\na text of some kind, decide which of a predeﬁned set of classes it belongs to. Language iden-\ntiﬁcation and genre classiﬁcation are examples of text classiﬁcation, as is sentiment analysis\n(classifying a movie or product review as positive or negative) and spam detection (classify-\nSPAM DETECTION\ning an email message as spam or not-spam). Since “not-spam” is awkward, researchers have\ncoined the term ham for not-spam. We can treat spam detection as a problem in supervised\nlearning. A training set is readily available: the positive (spam) examples are in my spam\nfolder, the negative (ham) examples are in my inbox. Here is an excerpt:\nSpam: Wholesale Fashion Watches -57% today. Designer watches for cheap ...\nSpam: You can buy ViagraFr$1.85 All Medications at unbeatable prices! ...\nSpam: WE CAN TREAT ANYTHING YOU SUFFER FROM JUST TRUST US ...\nSpam: Sta.rt earn*ing the salary yo,u d-eserve by o’btaining the prope,r crede’ntials!\nHam: The practical signiﬁcance of hypertree width in identifying more ...\nHam: Abstract: We will motivate the problem of social identity clustering: ...\nHam: Good to see you my friend. Hey Peter, It was good to hear from you. ...\nHam: PDS implies convexity of the resulting optimization problem (Kernel Ridge ...\nFrom this excerpt we can start to get an idea of what might be good features to include in\nthe supervised learning model. Word n-grams such as “for cheap” and “You can buy” seem\nto be indicators of spam (although they would have a nonzero probability in ham as well).\nCharacter-level features also seem important: spam is more likely to be all uppercase and to",
  "to be indicators of spam (although they would have a nonzero probability in ham as well).\nCharacter-level features also seem important: spam is more likely to be all uppercase and to\nhave punctuation embedded in words. Apparently the spammers thought that the word bigram\n“you deserve” would be too indicative of spam, and thus wrote “yo,u d-eserve” instead. A\ncharacter model should detect this. We could either create a full character n-gram model\nof spam and ham, or we could handcraft features such as “number of punctuation marks\nembedded in words.”\nNote that we have two complementary ways of talking about classiﬁcation.\nIn the\nlanguage-modeling approach, we deﬁne one n-gram language model for P(Message | spam)\nby training on the spam folder, and one model for P(Message | ham) by training on the inbox.\nThen we can classify a new message with an application of Bayes’ rule:\nargmax\nc∈{spam,ham}\nP(c | message) =\nargmax\nc∈{spam,ham}\nP(message | c) P(c) .\nwhere P(c) is estimated just by counting the total number of spam and ham messages. This\napproach works well for spam detection, just as it did for language identiﬁcation. 866\nChapter\n22.\nNatural Language Processing\nIn the machine-learning approach we represent the message as a set of feature/value\npairs and apply a classiﬁcation algorithm h to the feature vector X.\nWe can make the\nlanguage-modeling and machine-learning approaches compatible by thinking of the n-grams\nas features. This is easiest to see with a unigram model. The features are the words in the\nvocabulary: “a,” “aardvark,” . . ., and the values are the number of times each word appears\nin the message. That makes the feature vector large and sparse. If there are 100,000 words in\nthe language model, then the feature vector has length 100,000, but for a short email message\nalmost all the features will have count zero. This unigram representation has been called the\nbag of words model. You can think of the model as putting the words of the training corpus\nBAG OF WORDS\nin a bag and then selecting words one at a time. The notion of order of the words is lost; a\nunigram model gives the same probability to any permutation of a text. Higher-order n-gram\nmodels maintain some local notion of word order.\nWith bigrams and trigrams the number of features is squared or cubed, and we can add\nin other, non-n-gram features: the time the message was sent, whether a URL or an image",
  "models maintain some local notion of word order.\nWith bigrams and trigrams the number of features is squared or cubed, and we can add\nin other, non-n-gram features: the time the message was sent, whether a URL or an image\nis part of the message, an ID number for the sender of the message, the sender’s number of\nprevious spam and ham messages, and so on. The choice of features is the most important part\nof creating a good spam detector—more important than the choice of algorithm for processing\nthe features. In part this is because there is a lot of training data, so if we can propose a\nfeature, the data can accurately determine if it is good or not. It is necessary to constantly\nupdate features, because spam detection is an adversarial task; the spammers modify their\nspam in response to the spam detector’s changes.\nIt can be expensive to run algorithms on a very large feature vector, so often a process\nof feature selection is used to keep only the features that best discriminate between spam and\nFEATURE SELECTION\nham. For example, the bigram “of the” is frequent in English, and may be equally frequent in\nspam and ham, so there is no sense in counting it. Often the top hundred or so features do a\ngood job of discriminating between classes.\nOnce we have chosen a set of features, we can apply any of the supervised learning\ntechniques we have seen; popular ones for text categorization include k-nearest-neighbors,\nsupport vector machines, decision trees, naive Bayes, and logistic regression. All of these\nhave been applied to spam detection, usually with accuracy in the 98%–99% range. With a\ncarefully designed feature set, accuracy can exceed 99.9%.\n22.2.1\nClassiﬁcation by data compression\nAnother way to think about classiﬁcation is as a problem in data compression. A lossless\nDATA COMPRESSION\ncompression algorithm takes a sequence of symbols, detects repeated patterns in it, and writes\na description of the sequence that is more compact than the original. For example, the text\n“0.142857142857142857” might be compressed to “0.[142857]*3.” Compression algorithms\nwork by building dictionaries of subsequences of the text, and then referring to entries in the\ndictionary. The example here had only one dictionary entry, “142857.”\nIn effect, compression algorithms are creating a language model. The LZW algorithm\nin particular directly models a maximum-entropy probability distribution. To do classiﬁcation",
  "dictionary. The example here had only one dictionary entry, “142857.”\nIn effect, compression algorithms are creating a language model. The LZW algorithm\nin particular directly models a maximum-entropy probability distribution. To do classiﬁcation\nby compression, we ﬁrst lump together all the spam training messages and compress them as Section 22.3.\nInformation Retrieval\n867\na unit. We do the same for the ham. Then when given a new message to classify, we append\nit to the spam messages and compress the result. We also append it to the ham and compress\nthat. Whichever class compresses better—adds the fewer number of additional bytes for the\nnew message—is the predicted class. The idea is that a spam message will tend to share\ndictionary entries with other spam messages and thus will compress better when appended to\na collection that already contains the spam dictionary.\nExperiments with compression-based classiﬁcation on some of the standard corpora for\ntext classiﬁcation—the 20-Newsgroups data set, the Reuters-10 Corpora, the Industry Sector\ncorpora—indicate that whereas running off-the-shelf compression algorithms like gzip, RAR,\nand LZW can be quite slow, their accuracy is comparable to traditional classiﬁcation algo-\nrithms. This is interesting in its own right, and also serves to point out that there is promise\nfor algorithms that use character n-grams directly with no preprocessing of the text or feature\nselection: they seem to be captiring some real patterns.\n22.3\nINFORMATION RETRIEVAL\nInformation retrieval is the task of ﬁnding documents that are relevant to a user’s need for\nINFORMATION\nRETRIEVAL\ninformation. The best-known examples of information retrieval systems are search engines\non the World Wide Web. A Web user can type a query such as [AI book]2 into a search engine\nand see a list of relevant pages. In this section, we will see how such systems are built. An\ninformation retrieval (henceforth IR) system can be characterized by\nIR\n1. A corpus of documents. Each system must decide what it wants to treat as a document:\na paragraph, a page, or a multipage text.\n2. Queries posed in a query language. A query speciﬁes what the user wants to know.\nQUERY LANGUAGE\nThe query language can be just a list of words, such as [AI book]; or it can specify\na phrase of words that must be adjacent, as in [“AI book”]; it can contain Boolean\noperators as in [AI AND book]; it can include non-Boolean operators such as [AI NEAR\nbook] or [AI book site:www.aaai.org].",
  "a phrase of words that must be adjacent, as in [“AI book”]; it can contain Boolean\noperators as in [AI AND book]; it can include non-Boolean operators such as [AI NEAR\nbook] or [AI book site:www.aaai.org].\n3. A result set. This is the subset of documents that the IR system judges to be relevant to\nRESULT SET\nRELEVANT\nthe query. By relevant, we mean likely to be of use to the person who posed the query,\nfor the particular information need expressed in the query.\n4. A presentation of the result set. This can be as simple as a ranked list of document\nPRESENTATION\ntitles or as complex as a rotating color map of the result set projected onto a three-\ndimensional space, rendered as a two-dimensional display.\nThe earliest IR systems worked on a Boolean keyword model. Each word in the document\nBOOLEAN KEYWORD\nMODEL\ncollection is treated as a Boolean feature that is true of a document if the word occurs in the\ndocument and false if it does not. So the feature “retrieval” is true for the current chapter\nbut false for Chapter 15. The query language is the language of Boolean expressions over\n2 We denote a search query as [query]. Square brackets are used rather than quotation marks so that we can\ndistinguish the query [“two words”] from [two words]. 868\nChapter\n22.\nNatural Language Processing\nfeatures. A document is relevant only if the expression evaluates to true. For example, the\nquery [information AND retrieval] is true for the current chapter and false for Chapter 15.\nThis model has the advantage of being simple to explain and implement. However,\nit has some disadvantages. First, the degree of relevance of a document is a single bit, so\nthere is no guidance as to how to order the relevant documents for presentation. Second,\nBoolean expressions are unfamiliar to users who are not programmers or logicians. Users\nﬁnd it unintuitive that when they want to know about farming in the states of Kansas and\nNebraska they need to issue the query [farming (Kansas OR Nebraska)]. Third, it can be\nhard to formulate an appropriate query, even for a skilled user. Suppose we try [information\nAND retrieval AND models AND optimization] and get an empty result set. We could try\n[information OR retrieval OR models OR optimization], but if that returns too many results,\nit is difﬁcult to know what to try next.\n22.3.1\nIR scoring functions\nMost IR systems have abandoned the Boolean model and use models based on the statistics of",
  "it is difﬁcult to know what to try next.\n22.3.1\nIR scoring functions\nMost IR systems have abandoned the Boolean model and use models based on the statistics of\nword counts. We describe the BM25 scoring function, which comes from the Okapi project\nBM25 SCORING\nFUNCTION\nof Stephen Robertson and Karen Sparck Jones at London’s City College, and has been used\nin search engines such as the open-source Lucene project.\nA scoring function takes a document and a query and returns a numeric score; the most\nrelevant documents have the highest scores. In the BM25 function, the score is a linear\nweighted combination of scores for each of the words that make up the query. Three factors\naffect the weight of a query term: First, the frequency with which a query term appears in\na document (also known as TF for term frequency). For the query [farming in Kansas],\ndocuments that mention “farming” frequently will have higher scores. Second, the inverse\ndocument frequency of the term, or IDF. The word “in” appears in almost every document,\nso it has a high document frequency, and thus a low inverse document frequency, and thus it\nis not as important to the query as “farming” or “Kansas.” Third, the length of the document.\nA million-word document will probably mention all the query words, but may not actually be\nabout the query. A short document that mentions all the words is a much better candidate.\nThe BM25 function takes all three of these into account. We assume we have created\nan index of the N documents in the corpus so that we can look up TF(qi, dj), the count of\nthe number of times word qi appears in document dj. We also assume a table of document\nfrequency counts, DF(qi), that gives the number of documents that contain the word qi.\nThen, given a document dj and a query consisting of the words q1:N, we have\nBM25(dj, q1:N) =\nN\n\f\ni=1\nIDF(qi) ·\nTF(qi, dj) · (k + 1)\nTF(qi, dj) + k · (1 −b + b · |dj|\nL )\n,\nwhere |dj| is the length of document dj in words, and L is the average document length\nin the corpus: L = \u0002\ni |di|/N. We have two parameters, k and b, that can be tuned by\ncross-validation; typical values are k = 2.0 and b = 0.75. IDF(qi) is the inverse document Section 22.3.\nInformation Retrieval\n869\nfrequency of word qi, given by\nIDF(qi) = log N −DF(qi) + 0.5\nDF(qi) + 0.5\n.\nOf course, it would be impractical to apply the BM25 scoring function to every document\nin the corpus. Instead, systems create an index ahead of time that lists, for each vocabulary\nINDEX",
  "IDF(qi) = log N −DF(qi) + 0.5\nDF(qi) + 0.5\n.\nOf course, it would be impractical to apply the BM25 scoring function to every document\nin the corpus. Instead, systems create an index ahead of time that lists, for each vocabulary\nINDEX\nword, the documents that contain the word. This is called the hit list for the word. Then when\nHIT LIST\ngiven a query, we intersect the hit lists of the query words and only score the documents in\nthe intersection.\n22.3.2\nIR system evaluation\nHow do we know whether an IR system is performing well? We undertake an experiment in\nwhich the system is given a set of queries and the result sets are scored with respect to human\nrelevance judgments. Traditionally, there have been two measures used in the scoring: recall\nand precision. We explain them with the help of an example. Imagine that an IR system has\nreturned a result set for a single query, for which we know which documents are and are not\nrelevant, out of a corpus of 100 documents. The document counts in each category are given\nin the following table:\nIn result set Not in result set\nRelevant\n30\n20\nNot relevant\n10\n40\nPrecision measures the proportion of documents in the result set that are actually relevant.\nPRECISION\nIn our example, the precision is 30/(30 + 10) = .75. The false positive rate is 1 −.75 = .25.\nRecall measures the proportion of all the relevant documents in the collection that are in\nRECALL\nthe result set. In our example, recall is 30/(30 + 20) = .60. The false negative rate is 1 −\n.60 = .40. In a very large document collection, such as the World Wide Web, recall is difﬁcult\nto compute, because there is no easy way to examine every page on the Web for relevance.\nAll we can do is either estimate recall by sampling or ignore recall completely and just judge\nprecision. In the case of a Web search engine, there may be thousands of documents in the\nresult set, so it makes more sense to measure precision for several different sizes, such as\n“P@10” (precision in the top 10 results) or “P@50,” rather than to estimate precision in the\nentire result set.\nIt is possible to trade off precision against recall by varying the size of the result set\nreturned. In the extreme, a system that returns every document in the document collection is\nguaranteed a recall of 100%, but will have low precision. Alternately, a system could return\na single document and have low recall, but a decent chance at 100% precision. A summary",
  "guaranteed a recall of 100%, but will have low precision. Alternately, a system could return\na single document and have low recall, but a decent chance at 100% precision. A summary\nof both measures is the F1 score, a single number that is the harmonic mean of precision and\nrecall, 2PR/(P + R).\n22.3.3\nIR reﬁnements\nThere are many possible reﬁnements to the system described here, and indeed Web search\nengines are continually updating their algorithms as they discover new approaches and as the\nWeb grows and changes. 870\nChapter\n22.\nNatural Language Processing\nOne common reﬁnement is a better model of the effect of document length on relevance.\nSinghal et al. (1996) observed that simple document length normalization schemes tend to\nfavor short documents too much and long documents not enough. They propose a pivoted\ndocument length normalization scheme; the idea is that the pivot is the document length at\nwhich the old-style normalization is correct; documents shorter than that get a boost and\nlonger ones get a penalty.\nThe BM25 scoring function uses a word model that treats all words as completely in-\ndependent, but we know that some words are correlated: “couch” is closely related to both\n“couches” and “sofa.” Many IR systems attempt to account for these correlations.\nFor example, if the query is [couch], it would be a shame to exclude from the result set\nthose documents that mention “COUCH” or “couches” but not “couch.” Most IR systems\ndo case folding of “COUCH” to “couch,” and some use a stemming algorithm to reduce\nCASE FOLDING\nSTEMMING\n“couches” to the stem form “couch,” both in the query and the documents. This typically\nyields a small increase in recall (on the order of 2% for English). However, it can harm\nprecision. For example, stemming “stocking” to “stock” will tend to decrease precision for\nqueries about either foot coverings or ﬁnancial instruments, although it could improve recall\nfor queries about warehousing. Stemming algorithms based on rules (e.g., remove “-ing”)\ncannot avoid this problem, but algorithms based on dictionaries (don’t remove “-ing” if the\nword is already listed in the dictionary) can. While stemming has a small effect in English,\nit is more important in other languages. In German, for example, it is not uncommon to\nsee words like “Lebensversicherungsgesellschaftsangestellter” (life insurance company em-\nployee). Languages such as Finnish, Turkish, Inuit, and Yupik have recursive morphological",
  "see words like “Lebensversicherungsgesellschaftsangestellter” (life insurance company em-\nployee). Languages such as Finnish, Turkish, Inuit, and Yupik have recursive morphological\nrules that in principle generate words of unbounded length.\nThe next step is to recognize synonyms, such as “sofa” for “couch.” As with stemming,\nSYNONYM\nthis has the potential for small gains in recall, but can hurt precision. A user who gives the\nquery [Tim Couch] wants to see results about the football player, not sofas. The problem is\nthat “languages abhor absolute synonyms just as nature abhors a vacuum” (Cruse, 1986). That\nis, anytime there are two words that mean the same thing, speakers of the language conspire\nto evolve the meanings to remove the confusion. Related words that are not synonyms also\nplay an important role in ranking—terms like “leather”, “wooden,” or “modern” can serve\nto conﬁrm that the document really is about “couch.” Synonyms and related words can be\nfound in dictionaries or by looking for correlations in documents or in queries—if we ﬁnd\nthat many users who ask the query [new sofa] follow it up with the query [new couch], we\ncan in the future alter [new sofa] to be [new sofa OR new couch].\nAs a ﬁnal reﬁnement, IR can be improved by considering metadata—data outside of\nMETADATA\nthe text of the document. Examples include human-supplied keywords and publication data.\nOn the Web, hypertext links between documents are a crucial source of information.\nLINKS\n22.3.4\nThe PageRank algorithm\nPageRank3 was one of the two original ideas that set Google’s search apart from other Web\nPAGERANK\nsearch engines when it was introduced in 1997. (The other innovation was the use of anchor\n3 The name stands both for Web pages and for coinventor Larry Page (Brin and Page, 1998). Section 22.3.\nInformation Retrieval\n871\nfunction HITS(query) returns pages with hub and authority numbers\npages ←EXPAND-PAGES(RELEVANT-PAGES(query))\nfor each p in pages do\np.AUTHORITY ←1\np.HUB ←1\nrepeat until convergence do\nfor each p in pages do\np.AUTHORITY ←\u0002\ni INLINKi(p).HUB\np.HUB ←\u0002\ni OUTLINKi(p).AUTHORITY\nNORMALIZE(pages)\nreturn pages\nFigure 22.1\nThe HITS algorithm for computing hubs and authorities with respect to a\nquery. RELEVANT-PAGES fetches the pages that match the query, and EXPAND-PAGES adds\nin every page that links to or is linked from one of the relevant pages. NORMALIZE divides\neach page’s score by the sum of the squares of all pages’ scores (separately for both the",
  "in every page that links to or is linked from one of the relevant pages. NORMALIZE divides\neach page’s score by the sum of the squares of all pages’ scores (separately for both the\nauthority and hubs scores).\ntext—the underlined text in a hyperlink—to index a page, even though the anchor text was on\na different page than the one being indexed.) PageRank was invented to solve the problem of\nthe tyranny of TF scores: if the query is [IBM], how do we make sure that IBM’s home page,\nibm.com, is the ﬁrst result, even if another page mentions the term “IBM” more frequently?\nThe idea is that ibm.com has many in-links (links to the page), so it should be ranked higher:\neach in-link is a vote for the quality of the linked-to page. But if we only counted in-links,\nthen it would be possible for a Web spammer to create a network of pages and have them all\npoint to a page of his choosing, increasing the score of that page. Therefore, the PageRank\nalgorithm is designed to weight links from high-quality sites more heavily. What is a high-\nquality site? One that is linked to by other high-quality sites. The deﬁnition is recursive, but\nwe will see that the recursion bottoms out properly. The PageRank for a page p is deﬁned as:\nPR(p) = 1 −d\nN\n+ d\n\f\ni\nPR(ini)\nC(ini) ,\nwhere PR(p) is the PageRank of page p, N is the total number of pages in the corpus, ini\nare the pages that link in to p, and C(ini) is the count of the total number of out-links on\npage ini. The constant d is a damping factor. It can be understood through the random\nsurfer model: imagine a Web surfer who starts at some random page and begins exploring.\nRANDOM SURFER\nMODEL\nWith probability d (we’ll assume d = 0.85) the surfer clicks on one of the links on the page\n(choosing uniformly among them), and with probability 1 −d she gets bored with the page\nand restarts on a random page anywhere on the Web. The PageRank of page p is then the\nprobability that the random surfer will be at page p at any point in time. PageRank can be\ncomputed by an iterative procedure: start with all pages having PR(p) = 1, and iterate the\nalgorithm, updating ranks until they converge. 872\nChapter\n22.\nNatural Language Processing\n22.3.5\nThe HITS algorithm\nThe Hyperlink-Induced Topic Search algorithm, also known as “Hubs and Authorities” or\nHITS, is another inﬂuential link-analysis algorithm (see Figure 22.1). HITS differs from\nPageRank in several ways. First, it is a query-dependent measure: it rates pages with respect",
  "HITS, is another inﬂuential link-analysis algorithm (see Figure 22.1). HITS differs from\nPageRank in several ways. First, it is a query-dependent measure: it rates pages with respect\nto a query. That means that it must be computed anew for each query—a computational\nburden that most search engines have elected not to take on. Given a query, HITS ﬁrst ﬁnds\na set of pages that are relevant to the query. It does that by intersecting hit lists of query\nwords, and then adding pages in the link neighborhood of these pages—pages that link to or\nare linked from one of the pages in the original relevant set.\nEach page in this set is considered an authority on the query to the degree that other\nAUTHORITY\npages in the relevant set point to it. A page is considered a hub to the degree that it points\nHUB\nto other authoritative pages in the relevant set. Just as with PageRank, we don’t want to\nmerely count the number of links; we want to give more value to the high-quality hubs and\nauthorities. Thus, as with PageRank, we iterate a process that updates the authority score of\na page to be the sum of the hub scores of the pages that point to it, and the hub score to be\nthe sum of the authority scores of the pages it points to. If we then normalize the scores and\nrepeat k times, the process will converge.\nBoth PageRank and HITS played important roles in developing our understanding of\nWeb information retrieval. These algorithms and their extensions are used in ranking billions\nof queries daily as search engines steadily develop better ways of extracting yet ﬁner signals\nof search relevance.\n22.3.6\nQuestion answering\nInformation retrieval is the task of ﬁnding documents that are relevant to a query, where the\nquery may be a question, or just a topic area or concept. Question answering is a somewhat\nQUESTION\nANSWERING\ndifferent task, in which the query really is a question, and the answer is not a ranked list\nof documents but rather a short response—a sentence, or even just a phrase. There have\nbeen question-answering NLP (natural language processing) systems since the 1960s, but\nonly since 2001 have such systems used Web information retrieval to radically increase their\nbreadth of coverage.\nThe ASKMSR system (Banko et al., 2002) is a typical Web-based question-answering\nsystem. It is based on the intuition that most questions will be answered many times on the\nWeb, so question answering should be thought of as a problem in precision, not recall. We",
  "system. It is based on the intuition that most questions will be answered many times on the\nWeb, so question answering should be thought of as a problem in precision, not recall. We\ndon’t have to deal with all the different ways that an answer might be phrased—we only\nhave to ﬁnd one of them. For example, consider the query [Who killed Abraham Lincoln?]\nSuppose a system had to answer that question with access only to a single encyclopedia,\nwhose entry on Lincoln said\nJohn Wilkes Booth altered history with a bullet. He will forever be known as the man\nwho ended Abraham Lincoln’s life.\nTo use this passage to answer the question, the system would have to know that ending a life\ncan be a killing, that “He” refers to Booth, and several other linguistic and semantic facts. Section 22.4.\nInformation Extraction\n873\nASKMSR does not attempt this kind of sophistication—it knows nothing about pronoun\nreference, or about killing, or any other verb. It does know 15 different kinds of questions, and\nhow they can be rewritten as queries to a search engine. It knows that [Who killed Abraham\nLincoln] can be rewritten as the query [* killed Abraham Lincoln] and as [Abraham Lincoln\nwas killed by *]. It issues these rewritten queries and examines the results that come back—\nnot the full Web pages, just the short summaries of text that appear near the query terms.\nThe results are broken into 1-, 2-, and 3-grams and tallied for frequency in the result sets and\nfor weight: an n-gram that came back from a very speciﬁc query rewrite (such as the exact\nphrase match query [“Abraham Lincoln was killed by *”]) would get more weight than one\nfrom a general query rewrite, such as [Abraham OR Lincoln OR killed]. We would expect\nthat “John Wilkes Booth” would be among the highly ranked n-grams retrieved, but so would\n“Abraham Lincoln” and “the assassination of” and “Ford’s Theatre.”\nOnce the n-grams are scored, they are ﬁltered by expected type. If the original query\nstarts with “who,” then we ﬁlter on names of people; for “how many” we ﬁlter on numbers, for\n“when,” on a date or time. There is also a ﬁlter that says the answer should not be part of the\nquestion; together these should allow us to return “John Wilkes Booth” (and not “Abraham\nLincoln”) as the highest-scoring response.\nIn some cases the answer will be longer than three words; since the components re-\nsponses only go up to 3-grams, a longer response would have to be pieced together from",
  "Lincoln”) as the highest-scoring response.\nIn some cases the answer will be longer than three words; since the components re-\nsponses only go up to 3-grams, a longer response would have to be pieced together from\nshorter pieces. For example, in a system that used only bigrams, the answer “John Wilkes\nBooth” could be pieced together from high-scoring pieces “John Wilkes” and “Wilkes Booth.”\nAt the Text Retrieval Evaluation Conference (TREC), ASKMSR was rated as one of\nthe top systems, beating out competitors with the ability to do far more complex language\nunderstanding. ASKMSR relies upon the breadth of the content on the Web rather than on\nits own depth of understanding. It won’t be able to handle complex inference patterns like\nassociating “who killed” with “ended the life of.” But it knows that the Web is so vast that it\ncan afford to ignore passages like that and wait for a simple passage it can handle.\n22.4\nINFORMATION EXTRACTION\nInformation extraction is the process of acquiring knowledge by skimming a text and look-\nINFORMATION\nEXTRACTION\ning for occurrences of a particular class of object and for relationships among objects. A\ntypical task is to extract instances of addresses from Web pages, with database ﬁelds for\nstreet, city, state, and zip code; or instances of storms from weather reports, with ﬁelds for\ntemperature, wind speed, and precipitation. In a limited domain, this can be done with high\naccuracy. As the domain gets more general, more complex linguistic models and more com-\nplex learning techniques are necessary. We will see in Chapter 23 how to deﬁne complex\nlanguage models of the phrase structure (noun phrases and verb phrases) of English. But so\nfar there are no complete models of this kind, so for the limited needs of information ex-\ntraction, we deﬁne limited models that approximate the full English model, and concentrate\non just the parts that are needed for the task at hand. The models we describe in this sec- 874\nChapter\n22.\nNatural Language Processing\ntion are approximations in the same way that the simple 1-CNF logical model in Figure 7.21\n(page 271) is an approximations of the full, wiggly, logical model.\nIn this section we describe six different approaches to information extraction, in order\nof increasing complexity on several dimensions: deterministic to stochastic, domain-speciﬁc\nto general, hand-crafted to learned, and small-scale to large-scale.\n22.4.1\nFinite-state automata for information extraction",
  "of increasing complexity on several dimensions: deterministic to stochastic, domain-speciﬁc\nto general, hand-crafted to learned, and small-scale to large-scale.\n22.4.1\nFinite-state automata for information extraction\nThe simplest type of information extraction system is an attribute-based extraction system\nATTRIBUTE-BASED\nEXTRACTION\nthat assumes that the entire text refers to a single object and the task is to extract attributes of\nthat object. For example, we mentioned in Section 12.7 the problem of extracting from the\ntext “IBM ThinkBook 970. Our price: $399.00” the set of attributes {Manufacturer=IBM,\nModel=ThinkBook970, Price=$399.00}. We can address this problem by deﬁning a tem-\nplate (also known as a pattern) for each attribute we would like to extract. The template is\nTEMPLATE\ndeﬁned by a ﬁnite state automaton, the simplest example of which is the regular expression,\nREGULAR\nEXPRESSION\nor regex. Regular expressions are used in Unix commands such as grep, in programming\nlanguages such as Perl, and in word processors such as Microsoft Word. The details vary\nslightly from one tool to another and so are best learned from the appropriate manual, but\nhere we show how to build up a regular expression template for prices in dollars:\n[0-9]\nmatches any digit from 0 to 9\n[0-9]+\nmatches one or more digits\n[.][0-9][0-9]\nmatches a period followed by two digits\n([.][0-9][0-9])?\nmatches a period followed by two digits, or nothing\n[$][0-9]+([.][0-9][0-9])? matches $249.99 or $1.23 or $1000000 or . . .\nTemplates are often deﬁned with three parts: a preﬁx regex, a target regex, and a postﬁx regex.\nFor prices, the target regex is as above, the preﬁx would look for strings such as “price:” and\nthe postﬁx could be empty. The idea is that some clues about an attribute come from the\nattribute value itself and some come from the surrounding text.\nIf a regular expression for an attribute matches the text exactly once, then we can pull\nout the portion of the text that is the value of the attribute. If there is no match, all we can do\nis give a default value or leave the attribute missing; but if there are several matches, we need\na process to choose among them. One strategy is to have several templates for each attribute,\nordered by priority. So, for example, the top-priority template for price might look for the\npreﬁx “our price:”; if that is not found, we look for the preﬁx “price:” and if that is not found,",
  "ordered by priority. So, for example, the top-priority template for price might look for the\npreﬁx “our price:”; if that is not found, we look for the preﬁx “price:” and if that is not found,\nthe empty preﬁx. Another strategy is to take all the matches and ﬁnd some way to choose\namong them. For example, we could take the lowest price that is within 50% of the highest\nprice. That will select $78.00 as the target from the text “List price $99.00, special sale price\n$78.00, shipping $3.00.”\nOne step up from attribute-based extraction systems are relational extraction systems,\nRELATIONAL\nEXTRACTION\nwhich deal with multiple objects and the relations among them. Thus, when these systems\nsee the text “$249.99,” they need to determine not just that it is a price, but also which object\nhas that price. A typical relational-based extraction system is FASTUS, which handles news\nstories about corporate mergers and acquisitions. It can read the story Section 22.4.\nInformation Extraction\n875\nBridgestone Sports Co. said Friday it has set up a joint venture in Taiwan with a local\nconcern and a Japanese trading house to produce golf clubs to be shipped to Japan.\nand extract the relations:\ne ∈JointVentures ∧Product(e, “golf clubs”) ∧Date(e, “Friday”)\n∧Member(e, “Bridgestone Sports Co”) ∧Member(e, “a local concern”)\n∧Member(e, “a Japanese trading house”) .\nA relational extraction system can be built as a series of cascaded ﬁnite-state transducers.\nCASCADED\nFINITE-STATE\nTRANSDUCERS\nThat is, the system consists of a series of small, efﬁcient ﬁnite-state automata (FSAs), where\neach automaton receives text as input, transduces the text into a different format, and passes\nit along to the next automaton. FASTUS consists of ﬁve stages:\n1. Tokenization\n2. Complex-word handling\n3. Basic-group handling\n4. Complex-phrase handling\n5. Structure merging\nFASTUS’s ﬁrst stage is tokenization, which segments the stream of characters into tokens\n(words, numbers, and punctuation). For English, tokenization can be fairly simple; just sep-\narating characters at white space or punctuation does a fairly good job. Some tokenizers also\ndeal with markup languages such as HTML, SGML, and XML.\nThe second stage handles complex words, including collocations such as “set up” and\n“joint venture,” as well as proper names such as “Bridgestone Sports Co.” These are rec-\nognized by a combination of lexical entries and ﬁnite-state grammar rules. For example, a\ncompany name might be recognized by the rule",
  "“joint venture,” as well as proper names such as “Bridgestone Sports Co.” These are rec-\nognized by a combination of lexical entries and ﬁnite-state grammar rules. For example, a\ncompany name might be recognized by the rule\nCapitalizedWord+ (“Company” | “Co” | “Inc” | “Ltd”)\nThe third stage handles basic groups, meaning noun groups and verb groups. The idea is\nto chunk these into units that will be managed by the later stages. We will see how to write\na complex description of noun and verb phrases in Chapter 23, but here we have simple\nrules that only approximate the complexity of English, but have the advantage of being rep-\nresentable by ﬁnite state automata. The example sentence would emerge from this stage as\nthe following sequence of tagged groups:\n1 NG: Bridgestone Sports Co.\n10 NG: a local concern\n2 VG: said\n11 CJ: and\n3 NG: Friday\n12 NG: a Japanese trading house\n4 NG: it\n13 VG: to produce\n5 VG: had set up\n14 NG: golf clubs\n6 NG: a joint venture\n15 VG: to be shipped\n7 PR: in\n16 PR: to\n8 NG: Taiwan\n17 NG: Japan\n9 PR: with\nHere NG means noun group, VG is verb group, PR is preposition, and CJ is conjunction. 876\nChapter\n22.\nNatural Language Processing\nThe fourth stage combines the basic groups into complex phrases. Again, the aim\nis to have rules that are ﬁnite-state and thus can be processed quickly, and that result in\nunambiguous (or nearly unambiguous) output phrases. One type of combination rule deals\nwith domain-speciﬁc events. For example, the rule\nCompany+ SetUp JointVenture (“with” Company+)?\ncaptures one way to describe the formation of a joint venture. This stage is the ﬁrst one in\nthe cascade where the output is placed into a database template as well as being placed in the\noutput stream. The ﬁnal stage merges structures that were built up in the previous step. If\nthe next sentence says “The joint venture will start production in January,” then this step will\nnotice that there are two references to a joint venture, and that they should be merged into\none. This is an instance of the identity uncertainty problem discussed in Section 14.6.3.\nIn general, ﬁnite-state template-based information extraction works well for a restricted\ndomain in which it is possible to predetermine what subjects will be discussed, and how they\nwill be mentioned. The cascaded transducer model helps modularize the necessary knowl-\nedge, easing construction of the system. These systems work especially well when they are",
  "will be mentioned. The cascaded transducer model helps modularize the necessary knowl-\nedge, easing construction of the system. These systems work especially well when they are\nreverse-engineering text that has been generated by a program. For example, a shopping site\non the Web is generated by a program that takes database entries and formats them into Web\npages; a template-based extractor then recovers the original database. Finite-state informa-\ntion extraction is less successful at recovering information in highly variable format, such as\ntext written by humans on a variety of subjects.\n22.4.2\nProbabilistic models for information extraction\nWhen information extraction must be attempted from noisy or varied input, simple ﬁnite-state\napproaches fare poorly. It is too hard to get all the rules and their priorities right; it is better\nto use a probabilistic model rather than a rule-based model. The simplest probabilistic model\nfor sequences with hidden state is the hidden Markov model, or HMM.\nRecall from Section 15.3 that an HMM models a progression through a sequence of\nhidden states, xt, with an observation et at each step. To apply HMMs to information ex-\ntraction, we can either build one big HMM for all the attributes or build a separate HMM\nfor each attribute. We’ll do the second. The observations are the words of the text, and the\nhidden states are whether we are in the target, preﬁx, or postﬁx part of the attribute template,\nor in the background (not part of a template). For example, here is a brief text and the most\nprobable (Viterbi) path for that text for two HMMs, one trained to recognize the speaker in a\ntalk announcement, and one trained to recognize dates. The “-” indicates a background state:\nText:\nThere will be a seminar by\nDr.\nAndrew McCallum on\nFriday\nSpeaker: -\n-\n-\n-\nPRE\nPRE TARGET TARGET TARGET\nPOST -\nDate:\n-\n-\n-\n- -\n-\n-\n-\n-\nPRE\nTARGET\nHMMs have two big advantages over FSAs for extraction. First, HMMs are probabilistic, and\nthus tolerant to noise. In a regular expression, if a single expected character is missing, the\nregex fails to match; with HMMs there is graceful degradation with missing characters/words,\nand we get a probability indicating the degree of match, not just a Boolean match/fail. Second, Section 22.4.\nInformation Extraction\n877\nthat\nby\nspeakers\n/\nhere\nwill\n(\nreceived\nhas\nis\n1.0\n1.0\n0.99\n0.76\n0.24\n0.99\n0.44\n0.56\n:\nwith\n;\nabout\nhow\nwho\nspeaker\nspeak\n5409\nappointment\nseminar\nreminder\ntheater\nartist\nadditionally\ndr",
  "Information Extraction\n877\nthat\nby\nspeakers\n/\nhere\nwill\n(\nreceived\nhas\nis\n1.0\n1.0\n0.99\n0.76\n0.24\n0.99\n0.44\n0.56\n:\nwith\n;\nabout\nhow\nwho\nspeaker\nspeak\n5409\nappointment\nseminar\nreminder\ntheater\nartist\nadditionally\ndr\nprofessor\nrobert\nmichael\nmr\nw\ncavalier\nstevens\nchristel\nl\nPrefix\nTarget\nPostfix\nFigure 22.2\nHidden Markov model for the speaker of a talk announcement. The two\nsquare states are the target (note the second target state has a self-loop, so the target can\nmatch a string of any length), the four circles to the left are the preﬁx, and the one on the\nright is the postﬁx. For each state, only a few of the high-probability words are shown. From\nFreitag and McCallum (2000).\nHMMs can be trained from data; they don’t require laborious engineering of templates, and\nthus they can more easily be kept up to date as text changes over time.\nNote that we have assumed a certain level of structure in our HMM templates: they all\nconsist of one or more target states, and any preﬁx states must precede the targets, postﬁx\nstates most follow the targets, and other states must be background. This structure makes\nit easier to learn HMMs from examples. With a partially speciﬁed structure, the forward–\nbackward algorithm can be used to learn both the transition probabilities P(Xt | Xt−1) be-\ntween states and the observation model, P(Et | Xt), which says how likely each word is in\neach state. For example, the word “Friday” would have high probability in one or more of\nthe target states of the date HMM, and lower probability elsewhere.\nWith sufﬁcient training data, the HMM automatically learns a structure of dates that we\nﬁnd intuitive: the date HMM might have one target state in which the high-probability words\nare “Monday,” “Tuesday,” etc., and which has a high-probability transition to a target state\nwith words “Jan”, “January,” “Feb,” etc. Figure 22.2 shows the HMM for the speaker of a\ntalk announcement, as learned from data. The preﬁx covers expressions such as “Speaker:”\nand “seminar by,” and the target has one state that covers titles and ﬁrst names and another\nstate that covers initials and last names.\nOnce the HMMs have been learned, we can apply them to a text, using the Viterbi\nalgorithm to ﬁnd the most likely path through the HMM states. One approach is to apply\neach attribute HMM separately; in this case you would expect most of the HMMs to spend\nmost of their time in background states. This is appropriate when the extraction is sparse—",
  "each attribute HMM separately; in this case you would expect most of the HMMs to spend\nmost of their time in background states. This is appropriate when the extraction is sparse—\nwhen the number of extracted words is small compared to the length of the text. 878\nChapter\n22.\nNatural Language Processing\nThe other approach is to combine all the individual attributes into one big HMM, which\nwould then ﬁnd a path that wanders through different target attributes, ﬁrst ﬁnding a speaker\ntarget, then a date target, etc. Separate HMMs are better when we expect just one of each\nattribute in a text and one big HMM is better when the texts are more free-form and dense\nwith attributes. With either approach, in the end we have a collection of target attribute\nobservations, and have to decide what to do with them. If every expected attribute has one\ntarget ﬁller then the decision is easy: we have an instance of the desired relation. If there\nare multiple ﬁllers, we need to decide which to choose, as we discussed with template-based\nsystems. HMMs have the advantage of supplying probability numbers that can help make\nthe choice. If some targets are missing, we need to decide if this is an instance of the desired\nrelation at all, or if the targets found are false positives. A machine learning algorithm can be\ntrained to make this choice.\n22.4.3\nConditional random ﬁelds for information extraction\nOne issue with HMMs for the information extraction task is that they model a lot of prob-\nabilities that we don’t really need. An HMM is a generative model; it models the full joint\nprobability of observations and hidden states, and thus can be used to generate samples. That\nis, we can use the HMM model not only to parse a text and recover the speaker and date,\nbut also to generate a random instance of a text containing a speaker and a date. Since we’re\nnot interested in that task, it is natural to ask whether we might be better off with a model\nthat doesn’t bother modeling that possibility. All we need in order to understand a text is a\ndiscriminative model, one that models the conditional probability of the hidden attributes\ngiven the observations (the text). Given a text e1:N, the conditional model ﬁnds the hidden\nstate sequence X1:N that maximizes P(X1:N | e1:N).\nModeling this directly gives us some freedom. We don’t need the independence as-\nsumptions of the Markov model—we can have an xt that is dependent on x1. A framework",
  "state sequence X1:N that maximizes P(X1:N | e1:N).\nModeling this directly gives us some freedom. We don’t need the independence as-\nsumptions of the Markov model—we can have an xt that is dependent on x1. A framework\nfor this type of model is the conditional random ﬁeld, or CRF, which models a conditional\nCONDITIONAL\nRANDOM FIELD\nprobability distribution of a set of target variables given a set of observed variables. Like\nBayesian networks, CRFs can represent many different structures of dependencies among the\nvariables. One common structure is the linear-chain conditional random ﬁeld for repre-\nLINEAR-CHAIN\nCONDITIONAL\nRANDOM FIELD\nsenting Markov dependencies among variables in a temporal sequence. Thus, HMMs are the\ntemporal version of naive Bayes models, and linear-chain CRFs are the temporal version of\nlogistic regression, where the predicted target is an entire state sequence rather than a single\nbinary variable.\nLet e1:N be the observations (e.g., words in a document), and x1:N be the sequence of\nhidden states (e.g., the preﬁx, target, and postﬁx states). A linear-chain conditional random\nﬁeld deﬁnes a conditional probability distribution:\nP(x1:N|e1:N) = α e[\nPN\ni=1 F (xi−1,xi,e,i)] ,\nwhere α is a normalization factor (to make sure the probabilities sum to 1), and F is a feature\nfunction deﬁned as the weighted sum of a collection of k component feature functions:\nF(xi−1, xi, e, i) =\n\f\nk\nλk fk(xi−1, xi, e, i) . Section 22.4.\nInformation Extraction\n879\nThe λk parameter values are learned with a MAP (maximum a posteriori) estimation proce-\ndure that maximizes the conditional likelihood of the training data. The feature functions are\nthe key components of a CRF. The function fk has access to a pair of adjacent states, xi−1 and\nxi, but also the entire observation (word) sequence e, and the current position in the temporal\nsequence, i. This gives us a lot of ﬂexibility in deﬁning features. We can deﬁne a simple\nfeature function, for example one that produces a value of 1 if the current word is ANDREW\nand the current state is SPEAKER:\nf1(xi−1, xi, e, i) =\n\u0018 1 if xi = SPEAKER and ei = ANDREW\n0 otherwise\nHow are features like these used? It depends on their corresponding weights. If λ1 > 0, then\nwhenever f1 is true, it increases the probability of the hidden state sequence x1:N . This is\nanother way of saying “the CRF model should prefer the target state SPEAKER for the word",
  "whenever f1 is true, it increases the probability of the hidden state sequence x1:N . This is\nanother way of saying “the CRF model should prefer the target state SPEAKER for the word\nANDREW.” If on the other hand λ1 < 0, the CRF model will try to avoid this association,\nand if λ1 = 0, this feature is ignored. Parameter values can be set manually or can be learned\nfrom data. Now consider a second feature function:\nf2(xi−1, xi, e, i) =\n\u0018 1 if xi = SPEAKER and ei+1 = SAID\n0 otherwise\nThis feature is true if the current state is SPEAKER and the next word is “said.” One would\ntherefore expect a positive λ2 value to go with the feature. More interestingly, note that both\nf1 and f2 can hold at the same time for a sentence like “Andrew said . . . .” In this case, the\ntwo features overlap each other and both boost the belief in x1 = SPEAKER. Because of the\nindependence assumption, HMMs cannot use overlapping features; CRFs can. Furthermore,\na feature in a CRF can use any part of the sequence e1:N. Features can also be deﬁned over\ntransitions between states. The features we deﬁned here were binary, but in general, a feature\nfunction can be any real-valued function. For domains where we have some knowledge about\nthe types of features we would like to include, the CRF formalism gives us a great deal of\nﬂexibility in deﬁning them. This ﬂexibility can lead to accuracies that are higher than with\nless ﬂexible models such as HMMs.\n22.4.4\nOntology extraction from large corpora\nSo far we have thought of information extraction as ﬁnding a speciﬁc set of relations (e.g.,\nspeaker, time, location) in a speciﬁc text (e.g., a talk announcement). A different applica-\ntion of extraction technology is building a large knowledge base or ontology of facts from\na corpus. This is different in three ways: First it is open-ended—we want to acquire facts\nabout all types of domains, not just one speciﬁc domain. Second, with a large corpus, this\ntask is dominated by precision, not recall—just as with question answering on the Web (Sec-\ntion 22.3.6). Third, the results can be statistical aggregates gathered from multiple sources,\nrather than being extracted from one speciﬁc text.\nFor example, Hearst (1992) looked at the problem of learning an ontology of concept\ncategories and subcategories from a large corpus. (In 1992, a large corpus was a 1000-page\nencyclopedia; today it would be a 100-million-page Web corpus.) The work concentrated on",
  "categories and subcategories from a large corpus. (In 1992, a large corpus was a 1000-page\nencyclopedia; today it would be a 100-million-page Web corpus.) The work concentrated on\ntemplates that are very general (not tied to a speciﬁc domain) and have high precision (are 880\nChapter\n22.\nNatural Language Processing\nalmost always correct when they match) but low recall (do not always match). Here is one of\nthe most productive templates:\nNP such as NP (, NP)* (,)? ((and | or) NP)? .\nHere the bold words and commas must appear literally in the text, but the parentheses are\nfor grouping, the asterisk means repetition of zero or more, and the question mark means\noptional. NP is a variable standing for a noun phrase; Chapter 23 describes how to identify\nnoun phrases; for now just assume that we know some words are nouns and other words (such\nas verbs) that we can reliably assume are not part of a simple noun phrase. This template\nmatches the texts “diseases such as rabies affect your dog” and “supports network protocols\nsuch as DNS,” concluding that rabies is a disease and DNS is a network protocol. Similar\ntemplates can be constructed with the key words “including,” “especially,” and “or other.” Of\ncourse these templates will fail to match many relevant passages, like “Rabies is a disease.”\nThat is intentional. The “NP is a NP” template does indeed sometimes denote a subcategory\nrelation, but it often means something else, as in “There is a God” or “She is a little tired.”\nWith a large corpus we can afford to be picky; to use only the high-precision templates. We’ll\nmiss many statements of a subcategory relationship, but most likely we’ll ﬁnd a paraphrase\nof the statement somewhere else in the corpus in a form we can use.\n22.4.5\nAutomated template construction\nThe subcategory relation is so fundamental that is worthwhile to handcraft a few templates to\nhelp identify instances of it occurring in natural language text. But what about the thousands\nof other relations in the world? There aren’t enough AI grad students in the world to create\nand debug templates for all of them. Fortunately, it is possible to learn templates from a few\nexamples, then use the templates to learn more examples, from which more templates can be\nlearned, and so on. In one of the ﬁrst experiments of this kind, Brin (1999) started with a data\nset of just ﬁve examples:\n(“Isaac Asimov”, “The Robots of Dawn”)\n(“David Brin”, “Startide Rising”)\n(“James Gleick”, “Chaos—Making a New Science”)",
  "learned, and so on. In one of the ﬁrst experiments of this kind, Brin (1999) started with a data\nset of just ﬁve examples:\n(“Isaac Asimov”, “The Robots of Dawn”)\n(“David Brin”, “Startide Rising”)\n(“James Gleick”, “Chaos—Making a New Science”)\n(“Charles Dickens”, “Great Expectations”)\n(“William Shakespeare”, “The Comedy of Errors”)\nClearly these are examples of the author–title relation, but the learning system had no knowl-\nedge of authors or titles. The words in these examples were used in a search over a Web\ncorpus, resulting in 199 matches. Each match is deﬁned as a tuple of seven strings,\n(Author, Title, Order, Preﬁx, Middle, Postﬁx, URL) ,\nwhere Order is true if the author came ﬁrst and false if the title came ﬁrst, Middle is the\ncharacters between the author and title, Preﬁx is the 10 characters before the match, Sufﬁx is\nthe 10 characters after the match, and URL is the Web address where the match was made.\nGiven a set of matches, a simple template-generation scheme can ﬁnd templates to\nexplain the matches. The language of templates was designed to have a close mapping to the\nmatches themselves, to be amenable to automated learning, and to emphasize high precision Section 22.4.\nInformation Extraction\n881\n(possibly at the risk of lower recall). Each template has the same seven components as a\nmatch. The Author and Title are regexes consisting of any characters (but beginning and\nending in letters) and constrained to have a length from half the minimum length of the\nexamples to twice the maximum length. The preﬁx, middle, and postﬁx are restricted to\nliteral strings, not regexes. The middle is the easiest to learn: each distinct middle string in\nthe set of matches is a distinct candidate template. For each such candidate, the template’s\nPreﬁx is then deﬁned as the longest common sufﬁx of all the preﬁxes in the matches, and the\nPostﬁx is deﬁned as the longest common preﬁx of all the postﬁxes in the matches. If either of\nthese is of length zero, then the template is rejected. The URL of the template is deﬁned as\nthe longest preﬁx of the URLs in the matches.\nIn the experiment run by Brin, the ﬁrst 199 matches generated three templates. The\nmost productive template was\n<LI><B> Title </B> by Author (\nURL: www.sff.net/locus/c\nThe three templates were then used to retrieve 4047 more (author, title) examples. The exam-\nples were then used to generate more templates, and so on, eventually yielding over 15,000",
  "<LI><B> Title </B> by Author (\nURL: www.sff.net/locus/c\nThe three templates were then used to retrieve 4047 more (author, title) examples. The exam-\nples were then used to generate more templates, and so on, eventually yielding over 15,000\ntitles. Given a good set of templates, the system can collect a good set of examples. Given a\ngood set of examples, the system can build a good set of templates.\nThe biggest weakness in this approach is the sensitivity to noise. If one of the ﬁrst\nfew templates is incorrect, errors can propagate quickly. One way to limit this problem is to\nnot accept a new example unless it is veriﬁed by multiple templates, and not accept a new\ntemplate unless it discovers multiple examples that are also found by other templates.\n22.4.6\nMachine reading\nAutomated template construction is a big step up from handcrafted template construction, but\nit still requires a handful of labeled examples of each relation to get started. To build a large\nontology with many thousands of relations, even that amount of work would be onerous; we\nwould like to have an extraction system with no human input of any kind—a system that could\nread on its own and build up its own database. Such a system would be relation-independent;\nwould work for any relation. In practice, these systems work on all relations in parallel,\nbecause of the I/O demands of large corpora. They behave less like a traditional information-\nextraction system that is targeted at a few relations and more like a human reader who learns\nfrom the text itself; because of this the ﬁeld has been called machine reading.\nMACHINE READING\nA representative machine-reading system is TEXTRUNNER (Banko and Etzioni, 2008).\nTEXTRUNNER uses cotraining to boost its performance, but it needs something to bootstrap\nfrom. In the case of Hearst (1992), speciﬁc patterns (e.g., such as) provided the bootstrap, and\nfor Brin (1998), it was a set of ﬁve author–title pairs. For TEXTRUNNER, the original inspi-\nration was a taxonomy of eight very general syntactic templates, as shown in Figure 22.3. It\nwas felt that a small number of templates like this could cover most of the ways that relation-\nships are expressed in English. The actual bootsrapping starts from a set of labelled examples\nthat are extracted from the Penn Treebank, a corpus of parsed sentences. For example, from\nthe parse of the sentence “Einstein received the Nobel Prize in 1921,” TEXTRUNNER is able 882\nChapter\n22.\nNatural Language Processing",
  "that are extracted from the Penn Treebank, a corpus of parsed sentences. For example, from\nthe parse of the sentence “Einstein received the Nobel Prize in 1921,” TEXTRUNNER is able 882\nChapter\n22.\nNatural Language Processing\nto extract the relation (“Einstein,” “received,” “Nobel Prize”).\nGiven a set of labeled examples of this type, TEXTRUNNER trains a linear-chain CRF\nto extract further examples from unlabeled text. The features in the CRF include function\nwords like “to” and “of” and “the,” but not nouns and verbs (and not noun phrases or verb\nphrases). Because TEXTRUNNER is domain-independent, it cannot rely on predeﬁned lists\nof nouns and verbs.\nType\nTemplate\nExample\nFrequency\nVerb\nNP1 Verb NP2\nX established Y\n38%\nNoun–Prep\nNP1 NP Prep NP 2\nX settlement with Y\n23%\nVerb–Prep\nNP1 Verb Prep NP2\nX moved to Y\n16%\nInﬁnitive\nNP1 to Verb NP2\nX plans to acquire Y\n9%\nModiﬁer\nNP1 Verb NP2 Noun\nX is Y winner\n5%\nNoun-Coordinate\nNP1 (, | and | - | :) NP2 NP\nX-Y deal\n2%\nVerb-Coordinate\nNP1 (,| and) NP 2 Verb\nX, Y merge\n1%\nAppositive\nNP1 NP (:| ,)? NP2\nX hometown : Y\n1%\nFigure 22.3\nEight general templates that cover about 95% of the ways that relations are\nexpressed in English.\nTEXTRUNNER achieves a precision of 88% and recall of 45% (F1 of 60%) on a large\nWeb corpus. TEXTRUNNER has extracted hundreds of millions of facts from a corpus of a\nhalf-billion Web pages. For example, even though it has no predeﬁned medical knowledge,\nit has extracted over 2000 answers to the query [what kills bacteria]; correct answers include\nantibiotics, ozone, chlorine, Cipro, and broccoli sprouts. Questionable answers include “wa-\nter,” which came from the sentence “Boiling water for at least 10 minutes will kill bacteria.”\nIt would be better to attribute this to “boiling water” rather than just “water.”\nWith the techniques outlined in this chapter and continual new inventions, we are start-\ning to get closer to the goal of machine reading.\n22.5\nSUMMARY\nThe main points of this chapter are as follows:\n• Probabilistic language models based on n-grams recover a surprising amount of infor-\nmation about a language. They can perform well on such diverse tasks as language\nidentiﬁcation, spelling correction, genre classiﬁcation, and named-entity recognition.\n• These language models can have millions of features, so feature selection and prepro-\ncessing of the data to reduce noise is important.\n• Text classiﬁcation can be done with naive Bayes n-gram models or with any of the",
  "• These language models can have millions of features, so feature selection and prepro-\ncessing of the data to reduce noise is important.\n• Text classiﬁcation can be done with naive Bayes n-gram models or with any of the\nclassiﬁcation algorithms we have previously discussed. Classiﬁcation can also be seen\nas a problem in data compression. Bibliographical and Historical Notes\n883\n• Information retrieval systems use a very simple language model based on bags of\nwords, yet still manage to perform well in terms of recall and precision on very large\ncorpora of text. On Web corpora, link-analysis algorithms improve performance.\n• Question answering can be handled by an approach based on information retrieval, for\nquestions that have multiple answers in the corpus. When more answers are available\nin the corpus, we can use techniques that emphasize precision rather than recall.\n• Information-extraction systems use a more complex model that includes limited no-\ntions of syntax and semantics in the form of templates. They can be built from ﬁnite-\nstate automata, HMMs, or conditional random ﬁelds, and can be learned from examples.\n• In building a statistical language system, it is best to devise a model that can make good\nuse of available data, even if the model seems overly simplistic.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nN-gram letter models for language modeling were proposed by Markov (1913). Claude\nShannon (Shannon and Weaver, 1949) was the ﬁrst to generate n-gram word models of En-\nglish. Chomsky (1956, 1957) pointed out the limitations of ﬁnite-state models compared with\ncontext-free models, concluding, “Probabilistic models give no particular insight into some\nof the basic problems of syntactic structure.” This is true, but probabilistic models do provide\ninsight into some other basic problems—problems that context-free models ignore. Chom-\nsky’s remarks had the unfortunate effect of scaring many people away from statistical models\nfor two decades, until these models reemerged for use in speech recognition (Jelinek, 1976).\nKessler et al. (1997) show how to apply character n-gram models to genre classiﬁcation,\nand Klein et al. (2003) describe named-entity recognition with character models. Franz and\nBrants (2006) describe the Google n-gram corpus of 13 million unique words from a trillion\nwords of Web text; it is now publicly available. The bag of words model gets its name from",
  "Brants (2006) describe the Google n-gram corpus of 13 million unique words from a trillion\nwords of Web text; it is now publicly available. The bag of words model gets its name from\na passage from linguist Zellig Harris (1954), “language is not merely a bag of words but\na tool with particular properties.” Norvig (2009) gives some examples of tasks that can be\naccomplished with n-gram models.\nAdd-one smoothing, ﬁrst suggested by Pierre-Simon Laplace (1816), was formalized by\nJeffreys (1948), and interpolation smoothing is due to Jelinek and Mercer (1980), who used\nit for speech recognition. Other techniques include Witten–Bell smoothing (1991), Good–\nTuring smoothing (Church and Gale, 1991) and Kneser–Ney smoothing (1995). Chen and\nGoodman (1996) and Goodman (2001) survey smoothing techniques.\nSimple n-gram letter and word models are not the only possible probabilistic models.\nBlei et al. (2001) describe a probabilistic text model called latent Dirichlet allocation that\nviews a document as a mixture of topics, each with its own distribution of words. This model\ncan be seen as an extension and rationalization of the latent semantic indexing model of\n(Deerwester et al., 1990) (see also Papadimitriou et al. (1998)) and is also related to the\nmultiple-cause mixture model of (Sahami et al., 1996). 884\nChapter\n22.\nNatural Language Processing\nManning and Sch¨utze (1999) and Sebastiani (2002) survey text-classiﬁcation techniques.\nJoachims (2001) uses statistical learning theory and support vector machines to give a theo-\nretical analysis of when classiﬁcation will be successful. Apt´e et al. (1994) report an accuracy\nof 96% in classifying Reuters news articles into the “Earnings” category. Koller and Sahami\n(1997) report accuracy up to 95% with a naive Bayes classiﬁer, and up to 98.6% with a Bayes\nclassiﬁer that accounts for some dependencies among features. Lewis (1998) surveys forty\nyears of application of naive Bayes techniques to text classiﬁcation and retrieval. Schapire\nand Singer (2000) show that simple linear classiﬁers can often achieve accuracy almost as\ngood as more complex models and are more efﬁcient to evaluate. Nigam et al. (2000) show\nhow to use the EM algorithm to label unlabeled documents, thus learning a better classiﬁ-\ncation model. Witten et al. (1999) describe compression algorithms for classiﬁcation, and\nshow the deep connection between the LZW compression algorithm and maximum-entropy\nlanguage models.",
  "cation model. Witten et al. (1999) describe compression algorithms for classiﬁcation, and\nshow the deep connection between the LZW compression algorithm and maximum-entropy\nlanguage models.\nMany of the n-gram model techniques are also used in bioinformatics problems. Bio-\nstatistics and probabilistic NLP are coming closer together, as each deals with long, structured\nsequences chosen from an alphabet of constituents.\nThe ﬁeld of information retrieval is experiencing a regrowth in interest, sparked by\nthe wide usage of Internet searching. Robertson (1977) gives an early overview and intro-\nduces the probability ranking principle. Croft et al. (2009) and Manning et al. (2008) are\nthe ﬁrst textbooks to cover Web-based search as well as traditional IR. Hearst (2009) covers\nuser interfaces for Web search. The TREC conference, organized by the U.S. government’s\nNational Institute of Standards and Technology (NIST), hosts an annual competition for IR\nsystems and publishes proceedings with results. In the ﬁrst seven years of the competition,\nperformance roughly doubled.\nThe most popular model for IR is the vector space model (Salton et al., 1975). Salton’s\nwork dominated the early years of the ﬁeld. There are two alternative probabilistic models,\none due to Ponte and Croft (1998) and one by Maron and Kuhns (1960) and Robertson and\nSparck Jones (1976). Lafferty and Zhai (2001) show that the models are based on the same\njoint probability distribution, but that the choice of model has implications for training the\nparameters. Craswell et al. (2005) describe the BM25 scoring function and Svore and Burges\n(2009) describe how BM25 can be improved with a machine learning approach that incorpo-\nrates click data—examples of past search queies and the results that were clicked on.\nBrin and Page (1998) describe the PageRank algorithm and the implementation of a\nWeb search engine. Kleinberg (1999) describes the HITS algorithm. Silverstein et al. (1998)\ninvestigate a log of a billion Web searches. The journal Information Retrieval and the pro-\nceedings of the annual SIGIR conference cover recent developments in the ﬁeld.\nEarly information extraction programs include GUS (Bobrow et al., 1977) and FRUMP\n(DeJong, 1982). Recent information extraction has been pushed forward by the annual Mes-\nsage Understand Conferences (MUC), sponsored by the U.S. government. The FASTUS\nﬁnite-state system was done by Hobbs et al. (1997). It was based in part on the idea from",
  "sage Understand Conferences (MUC), sponsored by the U.S. government. The FASTUS\nﬁnite-state system was done by Hobbs et al. (1997). It was based in part on the idea from\nPereira and Wright (1991) of using FSAs as approximations to phrase-structure grammars.\nSurveys of template-based systems are given by Roche and Schabes (1997), Appelt (1999), Exercises\n885\nand Muslea (1999). Large databases of facts were extracted by Craven et al. (2000), Pasca\net al. (2006), Mitchell (2007), and Durme and Pasca (2008).\nFreitag and McCallum (2000) discuss HMMs for Information Extraction. CRFs were\nintroduced by Lafferty et al. (2001); an example of their use for information extraction is\ndescribed in (McCallum, 2003) and a tutorial with practical guidance is given by (Sutton and\nMcCallum, 2007). Sarawagi (2007) gives a comprehensive survey.\nBanko et al. (2002) present the ASKMSR question-answering system; a similar sys-\ntem is due to Kwok et al. (2001). Pasca and Harabagiu (2001) discuss a contest-winning\nquestion-answering system. Two early inﬂuential approaches to automated knowledge engi-\nneering were by Riloff (1993), who showed that an automatically constructed dictionary per-\nformed almost as well as a carefully handcrafted domain-speciﬁc dictionary, and by Yarowsky\n(1995), who showed that the task of word sense classiﬁcation (see page 756) could be accom-\nplished through unsupervised training on a corpus of unlabeled text with accuracy as good as\nsupervised methods.\nThe idea of simultaneously extracting templates and examples from a handful of labeled\nexamples was developed independently and simultaneously by Blum and Mitchell (1998),\nwho called it cotraining and by Brin (1998), who called it DIPRE (Dual Iterative Pattern\nRelation Extraction). You can see why the term cotraining has stuck. Similar early work,\nunder the name of bootstrapping, was done by Jones et al. (1999). The method was advanced\nby the QXTRACT (Agichtein and Gravano, 2003) and KNOWITALL (Etzioni et al., 2005)\nsystems. Machine reading was introduced by Mitchell (2005) and Etzioni et al. (2006) and is\nthe focus of the TEXTRUNNER project (Banko et al., 2007; Banko and Etzioni, 2008).\nThis chapter has focused on natural language text, but it is also possible to do informa-\ntion extraction based on the physical structure or layout of text rather than on the linguistic\nstructure. HTML lists and tables in both HTML and relational databases are home to data",
  "tion extraction based on the physical structure or layout of text rather than on the linguistic\nstructure. HTML lists and tables in both HTML and relational databases are home to data\nthat can be extracted and consolidated (Hurst, 2000; Pinto et al., 2003; Cafarella et al., 2008).\nThe Association for Computational Linguistics (ACL) holds regular conferences and\npublishes the journal Computational Linguistics. There is also an International Conference\non Computational Linguistics (COLING). The textbook by Manning and Sch¨utze (1999) cov-\ners statistical language processing, while Jurafsky and Martin (2008) give a comprehensive\nintroduction to speech and natural language processing.\nEXERCISES\n22.1\nThis exercise explores the quality of the n-gram model of language. Find or create a\nmonolingual corpus of 100,000 words or more. Segment it into words, and compute the fre-\nquency of each word. How many distinct words are there? Also count frequencies of bigrams\n(two consecutive words) and trigrams (three consecutive words). Now use those frequencies\nto generate language: from the unigram, bigram, and trigram models, in turn, generate a 100-\nword text by making random choices according to the frequency counts. Compare the three\ngenerated texts with actual language. Finally, calculate the perplexity of each model. 886\nChapter\n22.\nNatural Language Processing\n22.2\nWrite a program to do segmentation of words without spaces. Given a string, such\nas the URL “thelongestlistofthelongeststuffatthelongestdomainnameatlonglast.com,” return a\nlist of component words: [“the,” “longest,” “list,” . . .]. This task is useful for parsing URLs,\nfor spelling correction when words runtogether, and for languages such as Chinese that do\nnot have spaces between words. It can be solved with a unigram or bigram word model and\na dynamic programming algorithm similar to the Viterbi algorithm.\n22.3\n(Adapted from Jurafsky and Martin (2000).) In this exercise you will develop a classi-\nﬁer for authorship: given a text, the classiﬁer predicts which of two candidate authors wrote\nthe text. Obtain samples of text from two different authors. Separate them into training and\ntest sets. Now train a language model on the training set. You can choose what features to\nuse; n-grams of words or letters are the easiest, but you can add additional features that you\nthink may help. Then compute the probability of the text under each language model and",
  "use; n-grams of words or letters are the easiest, but you can add additional features that you\nthink may help. Then compute the probability of the text under each language model and\nchose the most probable model. Assess the accuracy of this technique. How does accuracy\nchange as you alter the set of features? This subﬁeld of linguistics is called stylometry; its\nSTYLOMETRY\nsuccesses include the identiﬁcation of the author of the disputed Federalist Papers (Mosteller\nand Wallace, 1964) and some disputed works of Shakespeare (Hope, 1994). Khmelev and\nTweedie (2001) produce good results with a simple letter bigram model.\n22.4\nThis exercise concerns the classiﬁcation of spam email. Create a corpus of spam email\nand one of non-spam mail. Examine each corpus and decide what features appear to be useful\nfor classiﬁcation: unigram words? bigrams? message length, sender, time of arrival? Then\ntrain a classiﬁcation algorithm (decision tree, naive Bayes, SVM, logistic regression, or some\nother algorithm of your choosing) on a training set and report its accuracy on a test set.\n22.5\nCreate a test set of ten queries, and pose them to three major Web search engines.\nEvaluate each one for precision at 1, 3, and 10 documents. Can you explain the differences\nbetween engines?\n22.6\nTry to ascertain which of the search engines from the previous exercise are using case\nfolding, stemming, synonyms, and spelling correction.\n22.7\nWrite a regular expression or a short program to extract company names. Test it on a\ncorpus of business news articles. Report your recall and precision.\n22.8\nConsider the problem of trying to evaluate the quality of an IR system that returns a\nranked list of answers (like most Web search engines). The appropriate measure of quality\ndepends on the presumed model of what the searcher is trying to achieve, and what strategy\nshe employs. For each of the following models, propose a corresponding numeric measure.\na. The searcher will look at the ﬁrst twenty answers returned, with the objective of getting\nas much relevant information as possible.\nb. The searcher needs only one relevant document, and will go down the list until she ﬁnds\nthe ﬁrst one.\nc. The searcher has a fairly narrow query and is able to examine all the answers retrieved.\nShe wants to be sure that she has seen everything in the document collection that is Exercises\n887\nrelevant to her query. (E.g., a lawyer wants to be sure that she has found all relevant",
  "She wants to be sure that she has seen everything in the document collection that is Exercises\n887\nrelevant to her query. (E.g., a lawyer wants to be sure that she has found all relevant\nprecedents, and is willing to spend considerable resources on that.)\nd. The searcher needs just one document relevant to the query, and can afford to pay a\nresearch assistant for an hour’s work looking through the results. The assistant can look\nthrough 100 retrieved documents in an hour. The assistant will charge the searcher for\nthe full hour regardless of whether he ﬁnds it immediately or at the end of the hour.\ne. The searcher will look through all the answers. Examining a document has cost $A;\nﬁnding a relevant document has value $B; failing to ﬁnd a relevant document has cost\n$C for each relevant document not found.\nf. The searcher wants to collect as many relevant documents as possible, but needs steady\nencouragement. She looks through the documents in order. If the documents she has\nlooked at so far are mostly good, she will continue; otherwise, she will stop. 23\nNATURAL LANGUAGE\nFOR COMMUNICATION\nIn which we see how humans communicate with one another in natural language,\nand how computer agents might join in the conversation.\nCommunication is the intentional exchange of information brought about by the production\nCOMMUNICATION\nand perception of signs drawn from a shared system of conventional signs. Most animals use\nSIGN\nsigns to represent important messages: food here, predator nearby, approach, withdraw, let’s\nmate. In a partially observable world, communication can help agents be successful because\nthey can learn information that is observed or inferred by others. Humans are the most chatty\nof all species, and if computer agents are to be helpful, they’ll need to learn to speak the\nlanguage. In this chapter we look at language models for communication. Models aimed at\ndeep understanding of a conversation necessarily need to be more complex than the simple\nmodels aimed at, say, spam classiﬁcation. We start with grammatical models of the phrase\nstructure of sentences, add semantics to the model, and then apply it to machine translation\nand speech recognition.\n23.1\nPHRASE STRUCTURE GRAMMARS\nThe n-gram language models of Chapter 22 were based on sequences of words. The big\nissue for these models is data sparsity—with a vocabulary of, say, 105 words, there are 1015\ntrigram probabilities to estimate, and so a corpus of even a trillion words will not be able to",
  "issue for these models is data sparsity—with a vocabulary of, say, 105 words, there are 1015\ntrigram probabilities to estimate, and so a corpus of even a trillion words will not be able to\nsupply reliable estimates for all of them. We can address the problem of sparsity through\ngeneralization. From the fact that “black dog” is more frequent than “dog black” and similar\nobservations, we can form the generalization that adjectives tend to come before nouns in\nEnglish (whereas they tend to follow nouns in French: “chien noir” is more frequent). Of\ncourse there are always exceptions; “galore” is an adjective that follows the noun it modiﬁes.\nDespite the exceptions, the notion of a lexical category (also known as a part of speech) such\nLEXICAL CATEGORY\nas noun or adjective is a useful generalization—useful in its own right, but more so when we\nstring together lexical categories to form syntactic categories such as noun phrase or verb\nSYNTACTIC\nCATEGORIES\nphrase, and combine these syntactic categories into trees representing the phrase structure\nPHRASE STRUCTURE\nof sentences: nested phrases, each marked with a category.\n888 Section 23.1.\nPhrase Structure Grammars\n889\nGENERATIVE CAPACITY\nGrammatical formalisms can be classiﬁed by their generative capacity: the set of\nlanguages they can represent. Chomsky (1957) describes four classes of grammat-\nical formalisms that differ only in the form of the rewrite rules. The classes can\nbe arranged in a hierarchy, where each class can be used to describe all the lan-\nguages that can be described by a less powerful class, as well as some additional\nlanguages. Here we list the hierarchy, most powerful class ﬁrst:\nRecursively enumerable grammars use unrestricted rules: both sides of the\nrewrite rules can have any number of terminal and nonterminal symbols, as in the\nrule A B C →D E. These grammars are equivalent to Turing machines in their\nexpressive power.\nContext-sensitive grammars are restricted only in that the right-hand side\nmust contain at least as many symbols as the left-hand side. The name “context-\nsensitive” comes from the fact that a rule such as A X B\n→A Y B says that\nan X can be rewritten as a Y in the context of a preceding A and a following B.\nContext-sensitive grammars can represent languages such as anbncn (a sequence\nof n copies of a followed by the same number of bs and then cs).\nIn context-free grammars (or CFGs), the left-hand side consists of a sin-",
  "Context-sensitive grammars can represent languages such as anbncn (a sequence\nof n copies of a followed by the same number of bs and then cs).\nIn context-free grammars (or CFGs), the left-hand side consists of a sin-\ngle nonterminal symbol. Thus, each rule licenses rewriting the nonterminal as\nthe right-hand side in any context. CFGs are popular for natural-language and\nprogramming-language grammars, although it is now widely accepted that at least\nsome natural languages have constructions that are not context-free (Pullum, 1991).\nContext-free grammars can represent anbn, but not anbncn.\nRegular grammars are the most restricted class. Every rule has a single non-\nterminal on the left-hand side and a terminal symbol optionally followed by a non-\nterminal on the right-hand side. Regular grammars are equivalent in power to ﬁnite-\nstate machines. They are poorly suited for programming languages, because they\ncannot represent constructs such as balanced opening and closing parentheses (a\nvariation of the anbn language). The closest they can come is representing a∗b∗, a\nsequence of any number of as followed by any number of bs.\nThe grammars higher up in the hierarchy have more expressive power, but\nthe algorithms for dealing with them are less efﬁcient. Up to the 1980s, linguists\nfocused on context-free and context-sensitive languages. Since then, there has been\nrenewed interest in regular grammars, brought about by the need to process and\nlearn from gigabytes or terabytes of online text very quickly, even at the cost of\na less complete analysis. As Fernando Pereira put it, “The older I get, the further\ndown the Chomsky hierarchy I go.” To see what he means, compare Pereira and\nWarren (1980) with Mohri, Pereira, and Riley (2002) (and note that these three\nauthors all now work on large text corpora at Google). 890\nChapter\n23.\nNatural Language for Communication\nThere have been many competing language models based on the idea of phrase struc-\nture; we will describe a popular model called the probabilistic context-free grammar, or\nPROBABILISTIC\nCONTEXT-FREE\nGRAMMAR\nPCFG.1 A grammar is a collection of rules that deﬁnes a language as a set of allowable\nGRAMMAR\nLANGUAGE\nstrings of words. “Context-free” is described in the sidebar on page 889, and “probabilistic”\nmeans that the grammar assigns a probability to every string. Here is a PCFG rule:\nVP\n→\nVerb [0.70]\n|\nVP NP [0.30] .\nHere VP (verb phrase) and NP (noun phrase) are non-terminal symbols. The grammar",
  "means that the grammar assigns a probability to every string. Here is a PCFG rule:\nVP\n→\nVerb [0.70]\n|\nVP NP [0.30] .\nHere VP (verb phrase) and NP (noun phrase) are non-terminal symbols. The grammar\nNON-TERMINAL\nSYMBOLS\nalso refers to actual words, which are called terminal symbols. This rule is saying that with\nTERMINAL SYMBOL\nprobability 0.70 a verb phrase consists solely of a verb, and with probability 0.30 it is a VP\nfollowed by an NP. Appendix B describes non-probabilistic context-free grammars.\nWe now deﬁne a grammar for a tiny fragment of English that is suitable for communi-\ncation between agents exploring the wumpus world. We call this language E0. Later sections\nimprove on E0 to make it slightly closer to real English. We are unlikely ever to devise a\ncomplete grammar for English, if only because no two persons would agree entirely on what\nconstitutes valid English.\n23.1.1\nThe lexicon of E0\nFirst we deﬁne the lexicon, or list of allowable words. The words are grouped into the lexical\nLEXICON\ncategories familiar to dictionary users: nouns, pronouns, and names to denote things; verbs\nto denote events; adjectives to modify nouns; adverbs to modify verbs; and function words:\narticles (such as the), prepositions (in), and conjunctions (and). Figure 23.1 shows a small\nlexicon for the language E0.\nEach of the categories ends in . . . to indicate that there are other words in the category.\nFor nouns, names, verbs, adjectives, and adverbs, it is infeasible even in principle to list all\nthe words. Not only are there tens of thousands of members in each class, but new ones–\nlike iPod or biodiesel—are being added constantly. These ﬁve categories are called open\nclasses. For the categories of pronoun, relative pronoun, article, preposition, and conjunction\nOPEN CLASS\nwe could have listed all the words with a little more work. These are called closed classes;\nCLOSED CLASS\nthey have a small number of words (a dozen or so). Closed classes change over the course\nof centuries, not months. For example, “thee” and “thou” were commonly used pronouns in\nthe 17th century, were on the decline in the 19th, and are seen today only in poetry and some\nregional dialects.\n23.1.2\nThe Grammar of E0\nThe next step is to combine the words into phrases. Figure 23.2 shows a grammar for E0,\nwith rules for each of the six syntactic categories and an example for each rewrite rule.2\nFigure 23.3 shows a parse tree for the sentence “Every wumpus smells.” The parse tree\nPARSE TREE",
  "with rules for each of the six syntactic categories and an example for each rewrite rule.2\nFigure 23.3 shows a parse tree for the sentence “Every wumpus smells.” The parse tree\nPARSE TREE\n1 PCFGs are also known as stochastic context-free grammars, or SCFGs.\n2 A relative clause follows and modiﬁes a noun phrase. It consists of a relative pronoun (such as “who” or\n“that”) followed by a verb phrase. An example of a relative clause is that stinks in “The wumpus that stinks is in\n2 2.” Another kind of relative clause has no relative pronoun, e.g., I know in “the man I know.” Section 23.1.\nPhrase Structure Grammars\n891\nNoun\n→\nstench [0.05] | breeze [0.10] | wumpus [0.15] | pits [0.05] | . . .\nVerb\n→\nis [0.10] | feel [0.10] | smells [0.10] | stinks [0.05] | . . .\nAdjective\n→\nright [0.10] | dead [0.05] | smelly [0.02] | breezy [0.02] . . .\nAdverb\n→\nhere [0.05] | ahead [0.05] | nearby [0.02] | . . .\nPronoun\n→\nme [0.10] | you [0.03] | I [0.10] | it [0.10] | . . .\nRelPro\n→\nthat [0.40] | which [0.15] | who [0.20] | whom [0.02] ∨. . .\nName\n→\nJohn [0.01] | Mary [0.01] | Boston [0.01] | . . .\nArticle\n→\nthe [0.40] | a [0.30] | an [0.10] | every [0.05] | . . .\nPrep\n→\nto [0.20] | in [0.10] | on [0.05] | near [0.10] | . . .\nConj\n→\nand [0.50] | or [0.10] | but [0.20] | yet [0.02] ∨. . .\nDigit\n→\n0 [0.20] | 1 [0.20] | 2 [0.20] | 3 [0.20] | 4 [0.20] | . . .\nFigure 23.1\nThe lexicon for E0. RelPro is short for relative pronoun, Prep for preposition,\nand Conj for conjunction. The sum of the probabilities for each category is 1.\nE0 :\nS\n→\nNP VP\n[0.90] I + feel a breeze\n|\nS Conj S\n[0.10] I feel a breeze + and + It stinks\nNP\n→\nPronoun\n[0.30] I\n|\nName\n[0.10] John\n|\nNoun\n[0.10] pits\n|\nArticle Noun\n[0.25] the + wumpus\n|\nArticle Adjs Noun [0.05] the + smelly dead + wumpus\n|\nDigit Digit\n[0.05] 3 4\n|\nNP PP\n[0.10] the wumpus + in 1 3\n|\nNP RelClause\n[0.05] the wumpus + that is smelly\nVP\n→\nVerb\n[0.40] stinks\n|\nVP NP\n[0.35] feel + a breeze\n|\nVP Adjective\n[0.05] smells + dead\n|\nVP PP\n[0.10] is + in 1 3\n|\nVP Adverb\n[0.10] go + ahead\nAdjs\n→\nAdjective\n[0.80] smelly\n|\nAdjective Adjs\n[0.20] smelly + dead\nPP\n→\nPrep NP\n[1.00] to + the east\nRelClause\n→\nRelPro VP\n[1.00] that + is smelly\nFigure 23.2\nThe grammar for E0, with example phrases for each rule. The syntactic cat-\negories are sentence (S), noun phrase (NP), verb phrase (VP), list of adjectives (Adjs),\nprepositional phrase (PP), and relative clause (RelClause). 892\nChapter\n23.\nNatural Language for Communication\nArticle\nNoun\nwumpus\nVerb\nNP\nVP\nS",
  "egories are sentence (S), noun phrase (NP), verb phrase (VP), list of adjectives (Adjs),\nprepositional phrase (PP), and relative clause (RelClause). 892\nChapter\n23.\nNatural Language for Communication\nArticle\nNoun\nwumpus\nVerb\nNP\nVP\nS\nEvery\nsmells\n0.25\n0.90\n 0.05\n 0.15\n 0.10\n 0.40\nFigure 23.3\nParse tree for the sentence “Every wumpus smells” according to the grammar\nE0. Each interior node of the tree is labeled with its probability. The probability of the tree\nas a whole is 0.9 × 0.25 × 0.05 × 0.15 × 0.40 × 0.10 = 0.0000675. Since this tree is the only\nparse of the sentence, that number is also the probability of the sentence. The tree can also\nbe written in linear form as [S [NP [Article every] [Noun wumpus]][VP [Verb smells]]].\ngives a constructive proof that the string of words is indeed a sentence according to the rules\nof E0. The E0 grammar generates a wide range of English sentences such as the following:\nJohn is in the pit\nThe wumpus that stinks is in 2 2\nMary is in Boston and the wumpus is near 3 2\nUnfortunately, the grammar overgenerates: that is, it generates sentences that are not gram-\nOVERGENERATION\nmatical, such as “Me go Boston” and “I smell pits wumpus John.” It also undergenerates:\nUNDERGENERATION\nthere are many sentences of English that it rejects, such as “I think the wumpus is smelly.”\nWe will see how to learn a better grammar later; for now we concentrate on what we can do\nwith the grammar we have.\n23.2\nSYNTACTIC ANALYSIS (PARSING)\nParsing is the process of analyzing a string of words to uncover its phrase structure, according\nPARSING\nto the rules of a grammar. Figure 23.4 shows that we can start with the S symbol and search\ntop down for a tree that has the words as its leaves, or we can start with the words and search\nbottom up for a tree that culminates in an S. Both top-down and bottom-up parsing can be\ninefﬁcient, however, because they can end up repeating effort in areas of the search space that\nlead to dead ends. Consider the following two sentences:\nHave the students in section 2 of Computer Science 101 take the exam.\nHave the students in section 2 of Computer Science 101 taken the exam?\nEven though they share the ﬁrst 10 words, these sentences have very different parses, because\nthe ﬁrst is a command and the second is a question. A left-to-right parsing algorithm would\nhave to guess whether the ﬁrst word is part of a command or a question and will not be able",
  "the ﬁrst is a command and the second is a question. A left-to-right parsing algorithm would\nhave to guess whether the ﬁrst word is part of a command or a question and will not be able\nto tell if the guess is correct until at least the eleventh word, take or taken. If the algorithm\nguesses wrong, it will have to backtrack all the way to the ﬁrst word and reanalyze the whole\nsentence under the other interpretation. Section 23.2.\nSyntactic Analysis (Parsing)\n893\nList of items\nRule\nS\nNP VP\nS →NP VP\nNP VP Adjective\nVP →VP Adjective\nNP Verb Adjective\nVP →Verb\nNP Verb dead\nAdjective →dead\nNP is dead\nVerb →is\nArticle Noun is dead\nNP →Article Noun\nArticle wumpus is dead\nNoun →wumpus\nthe wumpus is dead\nArticle →the\nFigure 23.4\nTrace of the process of ﬁnding a parse for the string “The wumpus is dead”\nas a sentence, according to the grammar E0. Viewed as a top-down parse, we start with the\nlist of items being S and, on each step, match an item X with a rule of the form (X →...)\nand replace X in the list of items with (...). Viewed as a bottom-up parse, we start with the\nlist of items being the words of the sentence, and, on each step, match a string of tokens (...)\nin the list against a rule of the form (X →...) and replace (...) with X .\nTo avoid this source of inefﬁciency we can use dynamic programming: every time we\nanalyze a substring, store the results so we won’t have to reanalyze it later. For example,\nonce we discover that “the students in section 2 of Computer Science 101” is an NP, we can\nrecord that result in a data structure known as a chart. Algorithms that do this are called chart\nCHART\nparsers. Because we are dealing with context-free grammars, any phrase that was found in\nthe context of one branch of the search space can work just as well in any other branch of the\nsearch space. There are many types of chart parsers; we describe a bottom-up version called\nthe CYK algorithm, after its inventors, John Cocke, Daniel Younger, and Tadeo Kasami.\nCYK ALGORITHM\nThe CYK algorithm is shown in Figure 23.5. Note that it requires a grammar with all\nrules in one of two very speciﬁc formats: lexical rules of the form X →word, and syntactic\nrules of the form X →Y Z. This grammar format, called Chomsky Normal Form, may\nCHOMSKY NORMAL\nFORM\nseem restrictive, but it is not: any context-free grammar can be automatically transformed\ninto Chomsky Normal Form. Exercise 23.8 leads you through the process.",
  "CHOMSKY NORMAL\nFORM\nseem restrictive, but it is not: any context-free grammar can be automatically transformed\ninto Chomsky Normal Form. Exercise 23.8 leads you through the process.\nThe CYK algorithm uses space of O(n2m) for the P table, where n is the number of\nwords in the sentence, and m is the number of nonterminal symbols in the grammar, and takes\ntime O(n3m). (Since m is constant for a particular grammar, this is commonly described as\nO(n3).) No algorithm can do better for general context-free grammars, although there are\nfaster algorithms on more restricted grammars. In fact, it is quite a trick for the algorithm to\ncomplete in O(n3) time, given that it is possible for a sentence to have an exponential number\nof parse trees. Consider the sentence\nFall leaves fall and spring leaves spring.\nIt is ambiguous because each word (except “and”) can be either a noun or a verb, and “fall”\nand “spring” can be adjectives as well. (For example, one meaning of “Fall leaves fall” is 894\nChapter\n23.\nNatural Language for Communication\nfunction CYK-PARSE(words,grammar) returns P, a table of probabilities\nN ←LENGTH(words)\nM ←the number of nonterminal symbols in grammar\nP ←an array of size [M , N , N ], initially all 0\n/* Insert lexical rules for each word */\nfor i = 1 to N do\nfor each rule of form (X →wordsi [p]) do\nP[X , i, 1] ←p\n/* Combine ﬁrst and second parts of right-hand sides of rules, from short to long */\nfor length = 2 to N do\nfor start = 1 to N −length + 1 do\nfor len1 = 1 to N −1 do\nlen2 ←length −len1\nfor each rule of the form (X →Y Z [p]) do\nP[X , start, length] ←MAX(P[X , start, length],\nP[Y , start, len1] × P[Z, start + len1, len2] × p)\nreturn P\nFigure 23.5\nThe CYK algorithm for parsing. Given a sequence of words, it ﬁnds the\nmost probable derivation for the whole sequence and for each subsequence. It returns the\nwhole table, P, in which an entry P[X , start, len] is the probability of the most probable\nX of length len starting at position start. If there is no X of that size at that location, the\nprobability is 0.\nequivalent to “Autumn abandons autumn.) With E0 the sentence has four parses:\n[S [S [NP Fall leaves] fall] and [S [NP spring leaves] spring]\n[S [S [NP Fall leaves] fall] and [S spring [VP leaves spring]]\n[S [S Fall [VP leaves fall]] and [S [NP spring leaves] spring]\n[S [S Fall [VP leaves fall]] and [S spring [VP leaves spring]] .\nIf we had c two-ways-ambiguous conjoined subsentences, we would have 2c ways of choos-",
  "[S [S Fall [VP leaves fall]] and [S [NP spring leaves] spring]\n[S [S Fall [VP leaves fall]] and [S spring [VP leaves spring]] .\nIf we had c two-ways-ambiguous conjoined subsentences, we would have 2c ways of choos-\ning parses for the subsentences.3 How does the CYK algorithm process these 2c parse trees\nin O(c3) time? The answer is that it doesn’t examine all the parse trees; all it has to do is\ncompute the probability of the most probable tree. The subtrees are all represented in the P\ntable, and with a little work we could enumerate them all (in exponential time), but the beauty\nof the CYK algorithm is that we don’t have to enumerate them unless we want to.\nIn practice we are usually not interested in all parses; just the best one or best few. Think\nof the CYK algorithm as deﬁning the complete state space deﬁned by the “apply grammar\nrule” operator. It is possible to search just part of this space using A∗search. Each state\nin this space is a list of items (words or categories), as shown in the bottom-up parse table\n(Figure 23.4). The start state is a list of words, and a goal state is the single item S. The\n3 There also would be O(c!) ambiguity in the way the components conjoin—for example, (X and (Y and Z))\nversus ((X and Y ) and Z). But that is another story, one told well by Church and Patil (1982). Section 23.2.\nSyntactic Analysis (Parsing)\n895\n[ [S [NP-SBJ-2 Her eyes]\n[VP were\n[VP glazed\n[NP *-2]\n[SBAR-ADV as if\n[S [NP-SBJ she]\n[VP did n’t\n[VP [VP hear [NP *-1]]\nor\n[VP [ADVP even] see [NP *-1]]\n[NP-1 him]]]]]]]]\n.]\nFigure 23.6\nAnnotated tree for the sentence “Her eyes were glazed as if she didn’t hear\nor even see him.” from the Penn Treebank. Note that in this grammar there is a distinction\nbetween an object noun phrase (NP) and a subject noun phrase (NP-SBJ). Note also a gram-\nmatical phenomenon we have not covered yet: the movement of a phrase from one part of\nthe tree to another. This tree analyzes the phrase “hear or even see him” as consisting of two\nconstituent VPs, [VP hear [NP *-1]] and [VP [ADVP even] see [NP *-1]], both of which\nhave a missing object, denoted *-1, which refers to the NP labeled elsewhere in the tree as\n[NP-1 him].\ncost of a state is the inverse of its probability as deﬁned by the rules applied so far, and there\nare various heuristics to estimate the remaining distance to the goal; the best heuristics come\nfrom machine learning applied to a corpus of sentences. With the A∗algorithm we don’t have",
  "are various heuristics to estimate the remaining distance to the goal; the best heuristics come\nfrom machine learning applied to a corpus of sentences. With the A∗algorithm we don’t have\nto search the entire state space, and we are guaranteed that the ﬁrst parse found will be the\nmost probable.\n23.2.1\nLearning probabilities for PCFGs\nA PCFG has many rules, with a probability for each rule. This suggests that learning the\ngrammar from data might be better than a knowledge engineering approach. Learning is eas-\niest if we are given a corpus of correctly parsed sentences, commonly called a treebank. The\nTREEBANK\nPenn Treebank (Marcus et al., 1993) is the best known; it consists of 3 million words which\nhave been annotated with part of speech and parse-tree structure, using human labor assisted\nby some automated tools. Figure 23.6 shows an annotated tree from the Penn Treebank.\nGiven a corpus of trees, we can create a PCFG just by counting (and smoothing). In the\nexample above, there are two nodes of the form [S[NP . . .][VP . . .]]. We would count these,\nand all the other subtrees with root S in the corpus. If there are 100,000 S nodes of which\n60,000 are of this form, then we create the rule:\nS →NP VP [0.60] .\nWhat if a treebank is not available, but we have a corpus of raw unlabeled sentences? It is\nstill possible to learn a grammar from such a corpus, but it is more difﬁcult. First of all,\nwe actually have two problems: learning the structure of the grammar rules and learning the 896\nChapter\n23.\nNatural Language for Communication\nprobabilities associated with each rule. (We have the same distinction in learning Bayes nets.)\nWe’ll assume that we’re given the lexical and syntactic category names. (If not, we can just\nassume categories X1, . . . Xn and use cross-validation to pick the best value of n.) We can\nthen assume that the grammar includes every possible (X\n→Y Z) or (X\n→word) rule,\nalthough many of these rules will have probability 0 or close to 0.\nWe can then use an expectation–maximization (EM) approach, just as we did in learning\nHMMs. The parameters we are trying to learn are the rule probabilities; we start them off at\nrandom or uniform values. The hidden variables are the parse trees: we don’t know whether\na string of words wi . . . wj is or is not generated by a rule (X →. . .). The E step estimates\nthe probability that each subsequence is generated by each rule. The M step then estimates",
  "a string of words wi . . . wj is or is not generated by a rule (X →. . .). The E step estimates\nthe probability that each subsequence is generated by each rule. The M step then estimates\nthe probability of each rule. The whole computation can be done in a dynamic-programming\nfashion with an algorithm called the inside–outside algorithm in analogy to the forward–\nINSIDE–OUTSIDE\nALGORITHM\nbackward algorithm for HMMs.\nThe inside–outside algorithm seems magical in that it induces a grammar from unparsed\ntext. But it has several drawbacks. First, the parses that are assigned by the induced grammars\nare often difﬁcult to understand and unsatisfying to linguists. This makes it hard to combine\nhandcrafted knowledge with automated induction. Second, it is slow: O(n3m3), where n is\nthe number of words in a sentence and m is the number of grammar categories. Third, the\nspace of probability assignments is very large, and empirically it seems that getting stuck in\nlocal maxima is a severe problem. Alternatives such as simulated annealing can get closer to\nthe global maximum, at a cost of even more computation. Lari and Young (1990) conclude\nthat inside–outside is “computationally intractable for realistic problems.”\nHowever, progress can be made if we are willing to step outside the bounds of learning\nsolely from unparsed text. One approach is to learn from prototypes: to seed the process with\na dozen or two rules, similar to the rules in E1. From there, more complex rules can be learned\nmore easily, and the resulting grammar parses English with an overall recall and precision for\nsentences of about 80% (Haghighi and Klein, 2006). Another approach is to use treebanks,\nbut in addition to learning PCFG rules directly from the bracketings, also learning distinctions\nthat are not in the treebank. For example, not that the tree in Figure 23.6 makes the distinction\nbetween NP and NP −SBJ. The latter is used for the pronoun “she,” the former for the\npronoun “her.” We will explore this issue in Section 23.6; for now let us just say that there\nare many ways in which it would be useful to split a category like NP—grammar induction\nsystems that use treebanks but automatically split categories do better than those that stick\nwith the original category set (Petrov and Klein, 2007c). The error rates for automatically\nlearned grammars are still about 50% higher than for hand-constructed grammar, but the gap\nis decreasing.\n23.2.2\nComparing context-free and Markov models",
  "with the original category set (Petrov and Klein, 2007c). The error rates for automatically\nlearned grammars are still about 50% higher than for hand-constructed grammar, but the gap\nis decreasing.\n23.2.2\nComparing context-free and Markov models\nThe problem with PCFGs is that they are context-free. That means that the difference between\nP(“eat a banana”) and P(“eat a bandanna”) depends only on P(Noun →“banana”) versus\nP(Noun →“bandanna”) and not on the relation between “eat” and the respective objects.\nA Markov model of order two or more, given a sufﬁciently large corpus, will know that “eat Section 23.3.\nAugmented Grammars and Semantic Interpretation\n897\na banana” is more probable. We can combine a PCFG and Markov model to get the best of\nboth. The simplest approach is to estimate the probability of a sentence with the geometric\nmean of the probabilities computed by both models. Then we would know that “eat a banana”\nis probable from both the grammatical and lexical point of view. But it still wouldn’t pick up\nthe relation between “eat” and “banana” in “eat a slightly aging but still palatable banana”\nbecause here the relation is more than two words away. Increasing the order of the Markov\nmodel won’t get at the relation precisely; to do that we can use a lexicalized PCFG, as\ndescribed in the next section.\nAnother problem with PCFGs is that they tend to have too strong a preference for shorter\nsentences. In a corpus such as the Wall Street Journal, the average length of a sentence\nis about 25 words. But a PCFG will usually assign fairly high probability to many short\nsentences, such as “He slept,” whereas in the Journal we’re more likely to see something like\n“It has been reported by a reliable source that the allegation that he slept is credible.” It seems\nthat the phrases in the Journal really are not context-free; instead the writers have an idea of\nthe expected sentence length and use that length as a soft global constraint on their sentences.\nThis is hard to reﬂect in a PCFG.\n23.3\nAUGMENTED GRAMMARS AND SEMANTIC INTERPRETATION\nIn this section we see how to extend context-free grammars—to say that, for example, not\nevery NP is independent of context, but rather, certain NPs are more likely to appear in one\ncontext, and others in another context.\n23.3.1\nLexicalized PCFGs\nTo get at the relationship between the verb “eat” and the nouns “banana” versus “bandanna,”",
  "every NP is independent of context, but rather, certain NPs are more likely to appear in one\ncontext, and others in another context.\n23.3.1\nLexicalized PCFGs\nTo get at the relationship between the verb “eat” and the nouns “banana” versus “bandanna,”\nwe can use a lexicalized PCFG, in which the probabilities for a rule depend on the relation-\nLEXICALIZED PCFG\nship between words in the parse tree, not just on the adjacency of words in a sentence. Of\ncourse, we can’t have the probability depend on every word in the tree, because we won’t\nhave enough training data to estimate all those probabilities. It is useful to introduce the no-\ntion of the head of a phrase—the most important word. Thus, “eat” is the head of the VP\nHEAD\n“eat a banana” and “banana” is the head of the NP “a banana.” We use the notation VP(v)\nto denote a phrase with category VP whose head word is v. We say that the category VP\nis augmented with the head variable v. Here is an augmented grammar that describes the\nAUGMENTED\nGRAMMAR\nverb–object relation:\nVP(v) →Verb(v) NP(n)\n[P1(v, n)]\nVP(v) →Verb(v)\n[P2(v)]\nNP(n) →Article(a) Adjs(j) Noun(n)\n[P3(n, a)]\nNoun(banana) →banana\n[pn]\n. . .\n. . .\nHere the probability P1(v, n) depends on the head words v and n. We would set this proba-\nbility to be relatively high when v is “eat” and n is “banana,” and low when n is “bandanna.” 898\nChapter\n23.\nNatural Language for Communication\nNote that since we are considering only heads, the distinction between “eat a banana” and\n“eat a rancid banana” will not be caught by these probabilities. Another issue with this ap-\nproach is that, in a vocabulary with, say, 20,000 nouns and 5,000 verbs, P1 needs 100 million\nprobability estimates. Only a few percent of these can come from a corpus; the rest will have\nto come from smoothing (see Section 22.1.2). For example, we can estimateP1(v, n) for a\n(v, n) pair that we have not seen often (or at all) by backing off to a model that depends\nonly on v. These objectless probabilities are still very useful; they can capture the distinction\nbetween a transitive verb like “eat”—which will have a high value for P1 and a low value for\nP2—and an intransitive verb like “sleep,” which will have the reverse. It is quite feasible to\nlearn these probabilities from a treebank.\n23.3.2\nFormal deﬁnition of augmented grammar rules\nAugmented rules are complicated, so we will give them a formal deﬁnition by showing how",
  "learn these probabilities from a treebank.\n23.3.2\nFormal deﬁnition of augmented grammar rules\nAugmented rules are complicated, so we will give them a formal deﬁnition by showing how\nan augmented rule can be translated into a logical sentence. The sentence will have the form\nof a deﬁnite clause (see page 256), so the result is called a deﬁnite clause grammar, or DCG.\nDEFINITE CLAUSE\nGRAMMAR\nWe’ll use as an example a version of a rule from the lexicalized grammar for NP with one\nnew piece of notation:\nNP(n) →Article(a) Adjs(j) Noun(n) {Compatible(j, n)} .\nThe new aspect here is the notation {constraint} to denote a logical constraint on some of the\nvariables; the rule only holds when the constraint is true. Here the predicate Compatible(j, n)\nis meant to test whether adjective j and noun n are compatible; it would be deﬁned by a series\nof assertions such as Compatible(black, dog). We can convert this grammar rule into a def-\ninite clause by (1) reversing the order of right- and left-hand sides, (2) making a conjunction\nof all the constituents and constraints, (3) adding a variable si to the list of arguments for each\nconstituent to represent the sequence of words spanned by the constituent, (4) adding a term\nfor the concatenation of words, Append(s1, . . .), to the list of arguments for the root of the\ntree. That gives us\nArticle(a, s1) ∧Adjs(j, s2) ∧Noun(n, s3) ∧Compatible(j, n)\n⇒NP(n, Append(s1, s2, s3)) .\nThis deﬁnite clause says that if the predicate Article is true of a head word a and a string s1,\nand Adjs is similarly true of a head word j and a string s2, and Noun is true of a head word\nn and a string s3, and if j and n are compatible, then the predicate NP is true of the head\nword n and the result of appending strings s1, s2, and s3.\nThe DCG translation left out the probabilities, but we could put them back in: just aug-\nment each constituent with one more variable representing the probability of the constituent,\nand augment the root with a variable that is the product of the constituent probabilities times\nthe rule probability.\nThe translation from grammar rule to deﬁnite clause allows us to talk about parsing\nas logical inference. This makes it possible to reason about languages and strings in many\ndifferent ways. For example, it means we can do bottom-up parsing using forward chaining or\ntop-down parsing using backward chaining. In fact, parsing natural language with DCGs was Section 23.3.\nAugmented Grammars and Semantic Interpretation\n899",
  "different ways. For example, it means we can do bottom-up parsing using forward chaining or\ntop-down parsing using backward chaining. In fact, parsing natural language with DCGs was Section 23.3.\nAugmented Grammars and Semantic Interpretation\n899\none of the ﬁrst applications of (and motivations for) the Prolog logic programming language.\nIt is sometimes possible to run the process backward and do language generation as well as\nLANGUAGE\nGENERATION\nparsing. For example, skipping ahead to Figure 23.10 (page 903), a logic program could be\ngiven the semantic form Loves(John, Mary) and apply the deﬁnite-clause rules to deduce\nS(Loves(John, Mary), [John, loves, Mary]) .\nThis works for toy examples, but serious language-generation systems need more control over\nthe process than is afforded by the DCG rules alone.\nE1 :\nS\n→\nNPS VP |\n. . .\nNPS\n→\nPronounS | Name | Noun | . . .\nNPO\n→\nPronounO | Name | Noun | . . .\nVP\n→\nVP NPO |\n. . .\nPP\n→\nPrep NPO\nPronounS\n→\nI | you | he | she | it | . . .\nPronounO\n→\nme | you | him | her | it | . . .\n. . .\nE2 :\nS(head)\n→\nNP(Sbj, pn, h) VP(pn, head) | . . .\nNP(c, pn, head)\n→\nPronoun(c, pn, head) | Noun(c, pn, head) | . . .\nVP(pn, head)\n→\nVP(pn, head) NP(Obj, p, h) | . . .\nPP(head)\n→\nPrep(head) NP(Obj, pn, h)\nPronoun(Sbj, 1S, I)\n→\nI\nPronoun(Sbj , 1P, we)\n→\nwe\nPronoun(Obj, 1S, me)\n→\nme\nPronoun(Obj, 3P, them)\n→\nthem\n. . .\nFigure 23.7\nTop: part of a grammar for the language E1, which handles subjective and\nobjective cases in noun phrases and thus does not overgenerate quite as badly as E0. The\nportions that are identical to E0 have been omitted. Bottom: part of an augmented grammar\nfor E2, with three augmentations: case agreement, subject–verb agreement, and head word.\nSbj, Obj, 1S, 1P and 3P are constants, and lowercase names are variables.\n23.3.3\nCase agreement and subject–verb agreement\nWe saw in Section 23.1 that the simple grammar for E0 overgenerates, producing nonsen-\ntences such as “Me smell a stench.” To avoid this problem, our grammar would have to know\nthat “me” is not a valid NP when it is the subject of a sentence. Linguists say that the pronoun\n“I” is in the subjective case, and “me” is in the objective case.4 We can account for this by\n4 The subjective case is also sometimes called the nominative case and the objective case is sometimes called\nthe accusative case. Many languages also have a dative case for words in the indirect object position. 900\nChapter\n23.\nNatural Language for Communication",
  "the accusative case. Many languages also have a dative case for words in the indirect object position. 900\nChapter\n23.\nNatural Language for Communication\nsplitting NP into two categories, NPS and NPO, to stand for noun phrases in the subjective\nand objective case, respectively. We would also need to split the category Pronoun into the\ntwo categories PronounS (which includes “I”) and PronounO (which includes “me”). The\ntop part of Figure 23.7 shows the grammar for case agreement; we call the resulting language\nCASE AGREEMENT\nE1. Notice that all the NP rules must be duplicated, once for NPS and once for NPO.\nUnfortunately, E1 still overgenerates. English requires subject–verb agreement for\nSUBJECT–VERB\nAGREEMENT\nperson and number of the subject and main verb of a sentence. For example, if “I” is the\nsubject, then “I smell” is grammatical, but “I smells” is not. If “it” is the subject, we get the\nreverse. In English, the agreement distinctions are minimal: most verbs have one form for\nthird-person singular subjects (he, she, or it), and a second form for all other combinations\nof person and number. There is one exception: the verb “to be” has three forms, “I am / you\nare / he is.” So one distinction (case) splits NP two ways, another distinction (person and\nnumber) splits NP three ways, and as we uncover other distinctions we would end up with an\nexponential number of subscripted NP forms if we took the approach of E1. Augmentations\nare a better approach: they can represent an exponential number of forms as a single rule.\nIn the bottom of Figure 23.7 we see (part of) an augmented grammar for the language\nE2, which handles case agreement, subject–verb agreement, and head words. We have just\none NP category, but NP(c, pn, head) has three augmentations: c is a parameter for case,\npn is a parameter for person and number, and head is a parameter for the head word of\nthe phrase. The other categories also are augmented with heads and other arguments. Let’s\nconsider one rule in detail:\nS(head) →NP(Sbj , pn, h) VP(pn, head) .\nThis rule is easiest to understand right-to-left: when an NP and a VP are conjoined they form\nan S, but only if the NP has the subjective (Sbj) case and the person and number (pn) of the\nNP and VP are identical. If that holds, then we have an S whose head is the same as the\nhead of the VP. Note the head of the NP, denoted by the dummy variable h, is not part of the",
  "NP and VP are identical. If that holds, then we have an S whose head is the same as the\nhead of the VP. Note the head of the NP, denoted by the dummy variable h, is not part of the\naugmentation of the S. The lexical rules for E2 ﬁll in the values of the parameters and are also\nbest read right-to-left. For example, the rule\nPronoun(Sbj, 1S, I) →I\nsays that “I” can be interpreted as a Pronoun in the subjective case, ﬁrst-person singular, with\nhead “I.” For simplicity we have omitted the probabilities for these rules, but augmentation\ndoes work with probabilities. Augmentation can also work with automated learning mecha-\nnisms. Petrov and Klein (2007c) show how a learning algorithm can automatically split the\nNP category into NPS and NPO.\n23.3.4\nSemantic interpretation\nTo show how to add semantics to a grammar, we start with an example that is simpler than\nEnglish: the semantics of arithmetic expressions. Figure 23.8 shows a grammar for arithmetic\nexpressions, where each rule is augmented with a variable indicating the semantic interpreta-\ntion of the phrase. The semantics of a digit such as “3” is the digit itself. The semantics of an\nexpression such as “3 + 4” is the operator “+” applied to the semantics of the phrase “3” and Section 23.3.\nAugmented Grammars and Semantic Interpretation\n901\nExp(x) →Exp(x1) Operator(op) Exp(x2) {x = Apply(op, x1, x2)}\nExp(x) →( Exp(x) )\nExp(x) →Number(x)\nNumber(x) →Digit(x)\nNumber(x) →Number(x1) Digit(x2) {x = 10 × x1 + x2}\nDigit(x) →x {0 ≤x ≤9}\nOperator(x) →x {x ∈{+, −, ÷, ×}}\nFigure 23.8\nA grammar for arithmetic expressions, augmented with semantics. Each vari-\nable xi represents the semantics of a constituent. Note the use of the {test} notation to deﬁne\nlogical predicates that must be satisﬁed, but that are not constituents.\nOperator(÷)\n3\n(\n)\n4\n2\n+\nNumber(2)\nDigit(2)\nNumber(4)\nDigit(4)\nOperator(+)\nDigit(3)\nNumber(3)\nExp(5)\nExp(2)\nExp(2)\nExp(4)\nExp(2)\nExp(3)\n÷\nFigure 23.9\nParse tree with semantic interpretations for the string “3 + (4 ÷ 2)”.\nthe phrase “4.” The rules obey the principle of compositional semantics—the semantics of\nCOMPOSITIONAL\nSEMANTICS\na phrase is a function of the semantics of the subphrases. Figure 23.9 shows the parse tree for\n3 + (4 ÷ 2) according to this grammar. The root of the parse tree is Exp(5), an expression\nwhose semantic interpretation is 5.\nNow let’s move on to the semantics of English, or at least of E0. We start by determin-",
  "3 + (4 ÷ 2) according to this grammar. The root of the parse tree is Exp(5), an expression\nwhose semantic interpretation is 5.\nNow let’s move on to the semantics of English, or at least of E0. We start by determin-\ning what semantic representations we want to associate with what phrases. We use the simple\nexample sentence “John loves Mary.” The NP “John” should have as its semantic interpreta-\ntion the logical term John, and the sentence as a whole should have as its interpretation the\nlogical sentence Loves(John, Mary). That much seems clear. The complicated part is the\nVP “loves Mary.” The semantic interpretation of this phrase is neither a logical term nor a\ncomplete logical sentence. Intuitively, “loves Mary” is a description that might or might not 902\nChapter\n23.\nNatural Language for Communication\napply to a particular person. (In this case, it applies to John.) This means that “loves Mary”\nis a predicate that, when combined with a term that represents a person (the person doing\nthe loving), yields a complete logical sentence. Using the λ-notation (see page 294), we can\nrepresent “loves Mary” as the predicate\nλx Loves(x, Mary) .\nNow we need a rule that says “an NP with semantics obj followed by a VP with semantics\npred yields a sentence whose semantics is the result of applying pred to obj:”\nS(pred(obj)) →NP(obj) VP(pred) .\nThe rule tells us that the semantic interpretation of “John loves Mary” is\n(λx Loves(x, Mary))(John) ,\nwhich is equivalent to Loves(John, Mary).\nThe rest of the semantics follows in a straightforward way from the choices we have\nmade so far. Because VPs are represented as predicates, it is a good idea to be consistent and\nrepresent verbs as predicates as well. The verb “loves” is represented as λy λx Loves(x, y),\nthe predicate that, when given the argument Mary, returns the predicate λx Loves(x, Mary).\nWe end up with the grammar shown in Figure 23.10 and the parse tree shown in Figure 23.11.\nWe could just as easily have added semantics to E2; we chose to work with E0 so that the\nreader can focus on one type of augmentation at a time.\nAdding semantic augmentations to a grammar by hand is laborious and error prone.\nTherefore, there have been several projects to learn semantic augmentations from examples.\nCHILL (Zelle and Mooney, 1996) is an inductive logic programming (ILP) program that\nlearns a grammar and a specialized parser for that grammar from examples. The target domain",
  "CHILL (Zelle and Mooney, 1996) is an inductive logic programming (ILP) program that\nlearns a grammar and a specialized parser for that grammar from examples. The target domain\nis natural language database queries. The training examples consist of pairs of word strings\nand corresponding semantic forms—for example;\nWhat is the capital of the state with the largest population?\nAnswer(c, Capital(s, c) ∧Largest(p, State(s) ∧Population(s, p)))\nCHILL’s task is to learn a predicate Parse(words, semantics) that is consistent with the ex-\namples and, hopefully, generalizes well to other examples. Applying ILP directly to learn\nthis predicate results in poor performance: the induced parser has only about 20% accuracy.\nFortunately, ILP learners can improve by adding knowledge. In this case, most of the Parse\npredicate was deﬁned as a logic program, and CHILL’s task was reduced to inducing the\ncontrol rules that guide the parser to select one parse over another. With this additional back-\nground knowledge, CHILL can learn to achieve 70% to 85% accuracy on various database\nquery tasks.\n23.3.5\nComplications\nThe grammar of real English is endlessly complex. We will brieﬂy mention some examples.\nTime and tense: Suppose we want to represent the difference between “John loves\nTIME AND TENSE\nMary” and “John loved Mary.” English uses verb tenses (past, present, and future) to indicate Section 23.3.\nAugmented Grammars and Semantic Interpretation\n903\nS(pred(obj)) →NP(obj) VP(pred)\nVP(pred(obj )) →Verb(pred) NP(obj)\nNP(obj ) →Name(obj)\nName(John) →John\nName(Mary) →Mary\nVerb(λy λx Loves(x, y)) →loves\nFigure 23.10\nA grammar that can derive a parse tree and semantic interpretation for “John\nloves Mary” (and three other sentences). Each category is augmented with a single argument\nrepresenting the semantics.\nJohn\nloves\nMary\nName(John)\nName(Mary)\nNP(Mary)\nNP(John)\nS(Loves(John,Mary))\nVerb(λy λx Loves(x,y))\nVP(λx Loves(x,Mary))\nFigure 23.11\nA parse tree with semantic interpretations for the string “John loves Mary”.\nthe relative time of an event. One good choice to represent the time of events is the event\ncalculus notation of Section 12.3. In event calculus we have\nJohn loves mary: E1 ∈Loves(John, Mary) ∧During(Now, Extent(E1))\nJohn loved mary: E2 ∈Loves(John, Mary) ∧After(Now, Extent(E2)) .\nThis suggests that our two lexical rules for the words “loves” and “loved” should be these:\nVerb(λy λx e ∈Loves(x, y) ∧During(Now, e)) →loves\nVerb(λy λx e ∈Loves(x, y) ∧After(Now, e)) →loved .",
  "This suggests that our two lexical rules for the words “loves” and “loved” should be these:\nVerb(λy λx e ∈Loves(x, y) ∧During(Now, e)) →loves\nVerb(λy λx e ∈Loves(x, y) ∧After(Now, e)) →loved .\nOther than this change, everything else about the grammar remains the same, which is en-\ncouraging news; it suggests we are on the right track if we can so easily add a complication\nlike the tense of verbs (although we have just scratched the surface of a complete grammar\nfor time and tense). It is also encouraging that the distinction between processes and discrete\nevents that we made in our discussion of knowledge representation in Section 12.3.1 is actu-\nally reﬂected in language use. We can say “John slept a lot last night,” where Sleeping is a\nprocess category, but it is odd to say “John found a unicorn a lot last night,” where Finding\nis a discrete event category. A grammar would reﬂect that fact by having a low probability\nfor adding the adverbial phrase “a lot” to discrete events.\nQuantiﬁcation: Consider the sentence “Every agent feels a breeze.” The sentence has\nQUANTIFICATION\nonly one syntactic parse under E0, but it is actually semantically ambiguous; the preferred 904\nChapter\n23.\nNatural Language for Communication\nmeaning is “For every agent there exists a breeze that the agent feels,” but an acceptable\nalternative meaning is “There exists a breeze that every agent feels.” 5 The two interpretations\ncan be represented as\n∀a a ∈Agents ⇒\n∃b b ∈Breezes ∧∃e e ∈Feel(a, b) ∧During(Now, e) ;\n∃b b ∈Breezes ∀a a ∈Agents ⇒\n∃e e ∈Feel(a, b) ∧During(Now, e) .\nThe standard approach to quantiﬁcation is for the grammar to deﬁne not an actual logical\nsemantic sentence, but rather a quasi-logical form that is then turned into a logical sentence\nQUASI-LOGICAL\nFORM\nby algorithms outside of the parsing process. Those algorithms can have preference rules for\npreferring one quantiﬁer scope over another—preferences that need not be reﬂected directly\nin the grammar.\nPragmatics: We have shown how an agent can perceive a string of words and use a\nPRAGMATICS\ngrammar to derive a set of possible semantic interpretations. Now we address the problem\nof completing the interpretation by adding context-dependent information about the current\nsituation. The most obvious need for pragmatic information is in resolving the meaning of\nindexicals, which are phrases that refer directly to the current situation. For example, in the\nINDEXICAL",
  "situation. The most obvious need for pragmatic information is in resolving the meaning of\nindexicals, which are phrases that refer directly to the current situation. For example, in the\nINDEXICAL\nsentence “I am in Boston today,” both “I” and “today” are indexicals. The word “I” would be\nrepresented by the ﬂuent Speaker, and it would be up to the hearer to resolve the meaning of\nthe ﬂuent—that is not considered part of the grammar but rather an issue of pragmatics; of\nusing the context of the current situation to interpret ﬂuents.\nAnother part of pragmatics is interpreting the speaker’s intent. The speaker’s action is\nconsidered a speech act, and it is up to the hearer to decipher what type of action it is—a\nSPEECH ACT\nquestion, a statement, a promise, a warning, a command, and so on. A command such as\n“go to 2 2” implicitly refers to the hearer. So far, our grammar for S covers only declarative\nsentences. We can easily extend it to cover commands. A command can be formed from\na VP, where the subject is implicitly the hearer. We need to distinguish commands from\nstatements, so we alter the rules for S to include the type of speech act:\nS(Statement(Speaker, pred(obj))) →NP(obj ) VP(pred)\nS(Command(Speaker, pred(Hearer))) →VP(pred) .\nLong-distance dependencies: Questions introduce a new grammatical complexity. In\nLONG-DISTANCE\nDEPENDENCIES\n“Who did the agent tell you to give the gold to?” the ﬁnal word “to” should be parsed as\n[PP to ], where the “ ” denotes a gap or trace where an NP is missing; the missing NP\nTRACE\nis licensed by the ﬁrst word of the sentence, “who.” A complex system of augmentations is\nused to make sure that the missing NPs match up with the licensing words in just the right\nway, and prohibit gaps in the wrong places. For example, you can’t have a gap in one branch\nof an NP conjunction: “What did he play [NP Dungeons and ]?” is ungrammatical. But\nyou can have the same gap in both branches of a VP conjunction: “What did you [VP [VP\nsmell ] and [VP shoot an arrow at ]]?”\nAmbiguity: In some cases, hearers are consciously aware of ambiguity in an utterance.\nAMBIGUITY\nHere are some examples taken from newspaper headlines:\n5 If this interpretation seems unlikely, consider “Every Protestant believes in a just God.” Section 23.3.\nAugmented Grammars and Semantic Interpretation\n905\nSquad helps dog bite victim.\nPolice begin campaign to run down jaywalkers.\nHelicopter powered by human ﬂies.\nOnce-sagging cloth diaper industry saved by full dumps.",
  "Augmented Grammars and Semantic Interpretation\n905\nSquad helps dog bite victim.\nPolice begin campaign to run down jaywalkers.\nHelicopter powered by human ﬂies.\nOnce-sagging cloth diaper industry saved by full dumps.\nPortable toilet bombed; police have nothing to go on.\nTeacher strikes idle kids.\nInclude your children when baking cookies.\nHospitals are sued by 7 foot doctors.\nMilk drinkers are turning to powder.\nSafety experts say school bus passengers should be belted.\nBut most of the time the language we hear seems unambiguous. Thus, when researchers ﬁrst\nbegan to use computers to analyze language in the 1960s, they were quite surprised to learn\nthat almost every utterance is highly ambiguous, even though the alternative interpretations\nmight not be apparent to a native speaker. A system with a large grammar and lexicon might\nﬁnd thousands of interpretations for a perfectly ordinary sentence. Lexical ambiguity, in\nLEXICAL AMBIGUITY\nwhich a word has more than one meaning, is quite common; “back” can be an adverb (go\nback), an adjective (back door), a noun (the back of the room) or a verb (back up your ﬁles).\n“Jack” can be a name, a noun (a playing card, a six-pointed metal game piece, a nautical ﬂag,\na ﬁsh, a socket, or a device for raising heavy objects), or a verb (to jack up a car, to hunt with\na light, or to hit a baseball hard). Syntactic ambiguity refers to a phrase that has multiple\nSYNTACTIC\nAMBIGUITY\nparses: “I smelled a wumpus in 2,2” has two parses: one where the prepositional phrase “in\n2,2” modiﬁes the noun and one where it modiﬁes the verb. The syntactic ambiguity leads to a\nsemantic ambiguity, because one parse means that the wumpus is in 2,2 and the other means\nSEMANTIC\nAMBIGUITY\nthat a stench is in 2,2. In this case, getting the wrong interpretation could be a deadly mistake\nfor the agent.\nFinally, there can be ambiguity between literal and ﬁgurative meanings. Figures of\nspeech are important in poetry, but are surprisingly common in everyday speech as well. A\nmetonymy is a ﬁgure of speech in which one object is used to stand for another. When\nMETONYMY\nwe hear “Chrysler announced a new model,” we do not interpret it as saying that compa-\nnies can talk; rather we understand that a spokesperson representing the company made the\nannouncement. Metonymy is common and is often interpreted unconsciously by human hear-\ners. Unfortunately, our grammar as it is written is not so facile. To handle the semantics of",
  "announcement. Metonymy is common and is often interpreted unconsciously by human hear-\ners. Unfortunately, our grammar as it is written is not so facile. To handle the semantics of\nmetonymy properly, we need to introduce a whole new level of ambiguity. We do this by pro-\nviding two objects for the semantic interpretation of every phrase in the sentence: one for the\nobject that the phrase literally refers to (Chrysler) and one for the metonymic reference (the\nspokesperson). We then have to say that there is a relation between the two. In our current\ngrammar, “Chrysler announced” gets interpreted as\nx = Chrysler ∧e ∈Announce(x) ∧After(Now, Extent(e)) .\nWe need to change that to\nx = Chrysler ∧e ∈Announce(m) ∧After(Now, Extent(e))\n∧Metonymy(m, x) . 906\nChapter\n23.\nNatural Language for Communication\nThis says that there is one entity x that is equal to Chrysler, and another entity m that did\nthe announcing, and that the two are in a metonymy relation. The next step is to deﬁne what\nkinds of metonymy relations can occur. The simplest case is when there is no metonymy at\nall—the literal object x and the metonymic object m are identical:\n∀m, x (m = x) ⇒Metonymy(m, x) .\nFor the Chrysler example, a reasonable generalization is that an organization can be used to\nstand for a spokesperson of that organization:\n∀m, x x ∈Organizations ∧Spokesperson(m, x) ⇒Metonymy(m, x) .\nOther metonymies include the author for the works (I read Shakespeare) or more generally\nthe producer for the product (I drive a Honda) and the part for the whole (The Red Sox need\na strong arm). Some examples of metonymy, such as “The ham sandwich on Table 4 wants\nanother beer,” are more novel and are interpreted with respect to a situation.\nA metaphor is another ﬁgure of speech, in which a phrase with one literal meaning is\nMETAPHOR\nused to suggest a different meaning by way of an analogy. Thus, metaphor can be seen as a\nkind of metonymy where the relation is one of similarity.\nDisambiguation is the process of recovering the most probable intended meaning of\nDISAMBIGUATION\nan utterance. In one sense we already have a framework for solving this problem: each rule\nhas a probability associated with it, so the probability of an interpretation is the product of\nthe probabilities of the rules that led to the interpretation. Unfortunately, the probabilities\nreﬂect how common the phrases are in the corpus from which the grammar was learned,",
  "the probabilities of the rules that led to the interpretation. Unfortunately, the probabilities\nreﬂect how common the phrases are in the corpus from which the grammar was learned,\nand thus reﬂect general knowledge, not speciﬁc knowledge of the current situation. To do\ndisambiguation properly, we need to combine four models:\n1. The world model: the likelihood that a proposition occurs in the world. Given what we\nknow about the world, it is more likely that a speaker who says “I’m dead” means “I\nam in big trouble” rather than “My life ended, and yet I can still talk.”\n2. The mental model: the likelihood that the speaker forms the intention of communicat-\ning a certain fact to the hearer. This approach combines models of what the speaker\nbelieves, what the speaker believes the hearer believes, and so on. For example, when\na politician says, “I am not a crook,” the world model might assign a probability of\nonly 50% to the proposition that the politician is not a criminal, and 99.999% to the\nproposition that he is not a hooked shepherd’s staff. Nevertheless, we select the former\ninterpretation because it is a more likely thing to say.\n3. The language model: the likelihood that a certain string of words will be chosen, given\nthat the speaker has the intention of communicating a certain fact.\n4. The acoustic model: for spoken communication, the likelihood that a particular se-\nquence of sounds will be generated, given that the speaker has chosen a given string of\nwords. Section 23.5 covers speech recognition. Section 23.4.\nMachine Translation\n907\n23.4\nMACHINE TRANSLATION\nMachine translation is the automatic translation of text from one natural language (the source)\nto another (the target). It was one of the ﬁrst application areas envisioned for computers\n(Weaver, 1949), but it is only in the past decade that the technology has seen widespread\nusage. Here is a passage from page 1 of this book:\nAI is one of the newest ﬁelds in science and engineering. Work started in earnest soon\nafter World War II, and the name itself was coined in 1956. Along with molecular biol-\nogy, AI is regularly cited as the “ﬁeld I would most like to be in” by scientists in other\ndisciplines.\nAnd here it is translated from English to Danish by an online tool, Google Translate:\nAI er en af de nyeste omr˚ader inden for videnskab og teknik. Arbejde startede for alvor\nlige efter Anden Verdenskrig, og navnet i sig selv var opfundet i 1956. Sammen med",
  "AI er en af de nyeste omr˚ader inden for videnskab og teknik. Arbejde startede for alvor\nlige efter Anden Verdenskrig, og navnet i sig selv var opfundet i 1956. Sammen med\nmolekylær biologi, er AI jævnligt nævnt som “feltet Jeg ville de ﬂeste gerne være i” af\nforskere i andre discipliner.\nFor those who don’t read Danish, here is the Danish translated back to English. The words\nthat came out different are in italics:\nAI is one of the newest ﬁelds of science and engineering. Work began in earnest just after\nthe Second World War, and the name itself was invented in 1956. Together with molecular\nbiology, AI is frequently mentioned as\n“ﬁeld I would most like to be in” by researchers\nin other disciplines.\nThe differences are all reasonable paraphrases, such as frequently mentioned for regularly\ncited. The only real error is the omission of the article the, denoted by the\nsymbol. This is\ntypical accuracy: of the two sentences, one has an error that would not be made by a native\nspeaker, yet the meaning is clearly conveyed.\nHistorically, there have been three main applications of machine translation. Rough\ntranslation, as provided by free online services, gives the “gist” of a foreign sentence or\ndocument, but contains errors. Pre-edited translation is used by companies to publish their\ndocumentation and sales materials in multiple languages. The original source text is written\nin a constrained language that is easier to translate automatically, and the results are usually\nedited by a human to correct any errors. Restricted-source translation works fully automati-\ncally, but only on highly stereotypical language, such as a weather report.\nTranslation is difﬁcult because, in the fully general case, it requires in-depth understand-\ning of the text. This is true even for very simple texts—even “texts” of one word. Consider\nthe word “Open” on the door of a store.6 It communicates the idea that the store is accepting\ncustomers at the moment. Now consider the same word “Open” on a large banner outside a\nnewly constructed store. It means that the store is now in daily operation, but readers of this\nsign would not feel misled if the store closed at night without removing the banner. The two\nsigns use the identical word to convey different meanings. In German the sign on the door\nwould be “Offen” while the banner would read “Neu Er¨offnet.”\n6 This example is due to Martin Kay. 908\nChapter\n23.\nNatural Language for Communication",
  "signs use the identical word to convey different meanings. In German the sign on the door\nwould be “Offen” while the banner would read “Neu Er¨offnet.”\n6 This example is due to Martin Kay. 908\nChapter\n23.\nNatural Language for Communication\nThe problem is that different languages categorize the world differently. For example,\nthe French word “doux” covers a wide range of meanings corresponding approximately to\nthe English words “soft,” “sweet,” and “gentle.” Similarly, the English word “hard” covers\nvirtually all uses of the German word “hart” (physically recalcitrant, cruel) and some uses\nof the word “schwierig” (difﬁcult). Therefore, representing the meaning of a sentence is\nmore difﬁcult for translation than it is for single-language understanding. An English parsing\nsystem could use predicates like Open(x), but for translation, the representation language\nwould have to make more distinctions, perhaps with Open1(x) representing the “Offen” sense\nand Open2(x) representing the “Neu Er¨offnet” sense. A representation language that makes\nall the distinctions necessary for a set of languages is called an interlingua.\nINTERLINGUA\nA translator (human or machine) often needs to understand the actual situation de-\nscribed in the source, not just the individual words. For example, to translate the English\nword “him,” into Korean, a choice must be made between the humble and honoriﬁc form, a\nchoice that depends on the social relationship between the speaker and the referent of “him.”\nIn Japanese, the honoriﬁcs are relative, so the choice depends on the social relationships be-\ntween the speaker, the referent, and the listener. Translators (both machine and human) some-\ntimes ﬁnd it difﬁcult to make this choice. As another example, to translate “The baseball hit\nthe window. It broke.” into French, we must choose the feminine “elle” or the masculine\n“il” for “it,” so we must decide whether “it” refers to the baseball or the window. To get the\ntranslation right, one must understand physics as well as language.\nSometimes there is no choice that can yield a completely satisfactory translation. For\nexample, an Italian love poem that uses the masculine “il sole” (sun) and feminine “la luna”\n(moon) to symbolize two lovers will necessarily be altered when translated into German,\nwhere the genders are reversed, and further altered when translated into a language where the\ngenders are the same.7\n23.4.1\nMachine translation systems",
  "(moon) to symbolize two lovers will necessarily be altered when translated into German,\nwhere the genders are reversed, and further altered when translated into a language where the\ngenders are the same.7\n23.4.1\nMachine translation systems\nAll translation systems must model the source and target languages, but systems vary in the\ntype of models they use. Some systems attempt to analyze the source language text all the way\ninto an interlingua knowledge representation and then generate sentences in the target lan-\nguage from that representation. This is difﬁcult because it involves three unsolved problems:\ncreating a complete knowledge representation of everything; parsing into that representation;\nand generating sentences from that representation.\nOther systems are based on a transfer model. They keep a database of translation rules\nTRANSFER MODEL\n(or examples), and whenever the rule (or example) matches, they translate directly. Transfer\ncan occur at the lexical, syntactic, or semantic level. For example, a strictly syntactic rule\nmaps English [Adjective Noun] to French [Noun Adjective]. A mixed syntactic and lexical\nrule maps French [S1 “et puis” S2] to English [S1 “and then” S2]. Figure 23.12 diagrams the\nvarious transfer points.\n7 Warren Weaver (1949) reports that Max Zeldner points out that the great Hebrew poet H. N. Bialik once said\nthat translation “is like kissing the bride through a veil.” Section 23.4.\nMachine Translation\n909\nInterlingua Semantics\nAttraction(NamedJohn, NamedMary, High)\nEnglish Words\nJohn loves Mary\nFrench Words\nJean aime Marie\nEnglish Syntax\nS(NP(John), VP(loves, NP(Mary)))\nS(NP(Jean), VP(aime, NP(Marie)))\nFrench Syntax\nEnglish Semantics\nLoves(John, Mary)\nAime(Jean, Marie)\nFrench Semantics\nFigure 23.12\nThe Vauquois triangle: schematic diagram of the choices for a machine\ntranslation system (Vauquois, 1968). We start with English text at the top. An interlingua-\nbased system follows the solid lines, parsing English ﬁrst into a syntactic form, then into\na semantic representation and an interlingua representation, and then through generation to\na semantic, syntactic, and lexical form in French. A transfer-based system uses the dashed\nlines as a shortcut. Different systems make the transfer at different points; some make it at\nmultiple points.\n23.4.2\nStatistical machine translation\nNow that we have seen how complex the translation task can be, it should come as no sur-",
  "lines as a shortcut. Different systems make the transfer at different points; some make it at\nmultiple points.\n23.4.2\nStatistical machine translation\nNow that we have seen how complex the translation task can be, it should come as no sur-\nprise that the most successful machine translation systems are built by training a probabilistic\nmodel using statistics gathered from a large corpus of text. This approach does not need\na complex ontology of interlingua concepts, nor does it need handcrafted grammars of the\nsource and target languages, nor a hand-labeled treebank. All it needs is data—sample trans-\nlations from which a translation model can be learned. To translate a sentence in, say, English\n(e) into French (f), we ﬁnd the string of words f ∗that maximizes\nf ∗= argmax\nf\nP(f | e) = argmax P(e | f) P(f) .\nHere the factor P(f) is the target language model for French; it says how probable a given\nLANGUAGE MODEL\nsentence is in French. P(e|f) is the translation model; it says how probable an English\nTRANSLATION\nMODEL\nsentence is as a translation for a given French sentence. Similarly, P(f | e) is a translation\nmodel from English to French.\nShould we work directly on P(f | e), or apply Bayes’ rule and work on P(e | f) P(f)?\nIn diagnostic applications like medicine, it is easier to model the domain in the causal di-\nrection: P(symptoms | disease) rather than P(disease | symptoms). But in translation both\ndirections are equally easy. The earliest work in statistical machine translation did apply\nBayes’ rule—in part because the researchers had a good language model, P(f), and wanted\nto make use of it, and in part because they came from a background in speech recognition,\nwhich is a diagnostic problem. We follow their lead in this chapter, but we note that re-\ncent work in statistical machine translation often optimizes P(f | e) directly, using a more\nsophisticated model that takes into account many of the features from the language model. 910\nChapter\n23.\nNatural Language for Communication\nThe language model, P(f), could address any level(s) on the right-hand side of Fig-\nure 23.12, but the easiest and most common approach is to build an n-gram model from a\nFrench corpus, as we have seen before. This captures only a partial, local idea of French\nsentences; however, that is often sufﬁcient for rough translation.8\nThe translation model is learned from a bilingual corpus—a collection of parallel texts,\nBILINGUAL CORPUS",
  "sentences; however, that is often sufﬁcient for rough translation.8\nThe translation model is learned from a bilingual corpus—a collection of parallel texts,\nBILINGUAL CORPUS\neach an English/French pair. Now, if we had an inﬁnitely large corpus, then translating a\nsentence would just be a lookup task: we would have seen the English sentence before in the\ncorpus, so we could just return the paired French sentence. But of course our resources are\nﬁnite, and most of the sentences we will be asked to translate will be novel. However, they\nwill be composed of phrases that we have seen before (even if some phrases are as short as\none word). For example, in this book, common phrases include “in this exercise we will,”\n“size of the state space,” “as a function of the” and “notes at the end of the chapter.” If asked\nto translate the novel sentence “In this exercise we will compute the size of the state space as a\nfunction of the number of actions.” into French, we should be able to break the sentence into\nphrases, ﬁnd the phrases in the English corpus (this book), ﬁnd the corresponding French\nphrases (from the French translation of the book), and then reassemble the French phrases\ninto an order that makes sense in French. In other words, given a source English sentence, e,\nﬁnding a French translation f is a matter of three steps:\n1. Break the English sentence into phrases e1, . . . , en.\n2. For each phrase ei, choose a corresponding French phrase fi. We use the notation\nP(fi | ei) for the phrasal probability that fi is a translation of ei.\n3. Choose a permutation of the phrases f1, . . . , fn. We will specify this permutation in a\nway that seems a little complicated, but is designed to have a simple probability dis-\ntribution: For each fi, we choose a distortion di, which is the number of words that\nDISTORTION\nphrase fi has moved with respect to fi−1; positive for moving to the right, negative for\nmoving to the left, and zero if fi immediately follows fi−1.\nFigure 23.13 shows an example of the process. At the top, the sentence “There is a smelly\nwumpus sleeping in 2 2” is broken into ﬁve phrases, e1, . . . , e5. Each of them is translated\ninto a corresponding phrase fi, and then these are permuted into the order f1, f3, f4, f2, f5.\nWe specify the permutation in terms of the distortions di of each French phrase, deﬁned as\ndi = START(fi) −END(fi−1) −1 ,\nwhere START(fi) is the ordinal number of the ﬁrst word of phrase fi in the French sentence,",
  "We specify the permutation in terms of the distortions di of each French phrase, deﬁned as\ndi = START(fi) −END(fi−1) −1 ,\nwhere START(fi) is the ordinal number of the ﬁrst word of phrase fi in the French sentence,\nand END(fi−1) is the ordinal number of the last word of phrase fi−1. In Figure 23.13 we see\nthat f5, “`a 2 2,” immediately follows f4, “qui dort,” and thus d5 = 0. Phrase f2, however, has\nmoved one words to the right of f1, so d2 = 1. As a special case we have d1 = 0, because f1\nstarts at position 1 and END(f0) is deﬁned to be 0 (even though f0 does not exist).\nNow that we have deﬁned the distortion, di, we can deﬁne the probability distribution\nfor distortion, P(di). Note that for sentences bounded by length n we have |di| ≤n , and\n8 For the ﬁner points of translation, n-grams are clearly not enough. Marcel Proust’s 4000-page novel A la\nr´echerche du temps perdu begins and ends with the same word (longtemps), so some translators have decided to\ndo the same, thus basing the translation of the ﬁnal word on one that appeared roughly 2 million words earlier. Section 23.4.\nMachine Translation\n911\nso the full probability distribution P(di) has only 2n + 1 elements, far fewer numbers to\nlearn than the number of permutations, n!. That is why we deﬁned the permutation in this\ncircuitous way. Of course, this is a rather impoverished model of distortion. It doesn’t say\nthat adjectives are usually distorted to appear after the noun when we are translating from\nEnglish to French—that fact is represented in the French language model, P(f). The distor-\ntion probability is completely independent of the words in the phrases—it depends only on\nthe integer value di. The probability distribution provides a summary of the volatility of the\npermutations; how likely a distortion of P(d = 2) is, compared to P(d = 0), for example.\nWe’re ready now to put it all together: we can deﬁne P(f, d | e), the probability that\nthe sequence of phrases f with distortions d is a translation of the sequence of phrases e. We\nmake the assumption that each phrase translation and each distortion is independent of the\nothers, and thus we can factor the expression as\nP(f, d | e) =\n\u0019\ni\nP(fi | ei) P(di)\nThere is a\nsmelly\nwumpus\nsleeping\nin 2 2\nIl y a un\nwumpus\nqui dort\nmalodorant\nà 2 2\ne1\ne2\ne3\ne4\ne5\nd1 = 0\nd3 = -2\nd2 = +1\nd4 = +1\nd5 = 0\nf1\nf3\nf2\nf4\nf5\nFigure 23.13\nCandidate French phrases for each phrase of an English sentence, with dis-\ntortion (d) values for each French phrase.",
  "in 2 2\nIl y a un\nwumpus\nqui dort\nmalodorant\nà 2 2\ne1\ne2\ne3\ne4\ne5\nd1 = 0\nd3 = -2\nd2 = +1\nd4 = +1\nd5 = 0\nf1\nf3\nf2\nf4\nf5\nFigure 23.13\nCandidate French phrases for each phrase of an English sentence, with dis-\ntortion (d) values for each French phrase.\nThat gives us a way to compute the probability P(f, d | e) for a candidate translation f\nand distortion d. But to ﬁnd the best f and d we can’t just enumerate sentences; with maybe\n100 French phrases for each English phrase in the corpus, there are 1005 different 5-phrase\ntranslations, and 5! reorderings for each of those. We will have to search for a good solution.\nA local beam search (see page 125) with a heuristic that estimates probability has proven\neffective at ﬁnding a nearly-most-probable translation.\nAll that remains is to learn the phrasal and distortion probabilities. We sketch the pro-\ncedure; see the notes at the end of the chapter for details.\n1. Find parallel texts: First, gather a parallel bilingual corpus. For example, a Hansard9\nHANSARD\nis a record of parliamentary debate. Canada, Hong Kong, and other countries pro-\nduce bilingual Hansards, the European Union publishes its ofﬁcial documents in 11\nlanguages, and the United Nations publishes multilingual documents. Bilingual text is\nalso available online; some Web sites publish parallel content with parallel URLs, for\n9 Named after William Hansard, who ﬁrst published the British parliamentary debates in 1811. 912\nChapter\n23.\nNatural Language for Communication\nexample, /en/ for the English page and /fr/ for the corresponding French page. The\nleading statistical translation systems train on hundreds of millions of words of parallel\ntext and billions of words of monolingual text.\n2. Segment into sentences: The unit of translation is a sentence, so we will have to break\nthe corpus into sentences. Periods are strong indicators of the end of a sentence, but\nconsider “Dr. J. R. Smith of Rodeo Dr. paid $29.99 on 9.9.09.”; only the ﬁnal period\nends a sentence. One way to decide if a period ends a sentence is to train a model\nthat takes as features the surrounding words and their parts of speech. This approach\nachieves about 98% accuracy.\n3. Align sentences: For each sentence in the English version, determine what sentence(s)\nit corresponds to in the French version. Usually, the next sentence of English corre-\nsponds to the next sentence of French in a 1:1 match, but sometimes there is variation:",
  "it corresponds to in the French version. Usually, the next sentence of English corre-\nsponds to the next sentence of French in a 1:1 match, but sometimes there is variation:\none sentence in one language will be split into a 2:1 match, or the order of two sentences\nwill be swapped, resulting in a 2:2 match. By looking at the sentence lengths alone (i.e.\nshort sentences should align with short sentences), it is possible to align them (1:1, 1:2,\nor 2:2, etc.) with accuracy in the 90% to 99% range using a variation on the Viterbi\nalgorithm. Even better alignment can be achieved by using landmarks that are common\nto both languages, such as numbers, dates, proper names, or words that we know from\na bilingual dictionary have an unambiguous translation. For example, if the 3rd English\nand 4th French sentences contain the string “1989” and neighboring sentences do not,\nthat is good evidence that the sentences should be aligned together.\n4. Align phrases: Within a sentence, phrases can be aligned by a process that is similar to\nthat used for sentence alignment, but requiring iterative improvement. When we start,\nwe have no way of knowing that “qui dort” aligns with “sleeping,” but we can arrive at\nthat alignment by a process of aggregation of evidence. Over all the example sentences\nwe have seen, we notice that “qui dort” and “sleeping” co-occur with high frequency,\nand that in the pair of aligned sentences, no phrase other than “qui dort” co-occurs so\nfrequently in other sentences with “sleeping.” A complete phrase alignment over our\ncorpus gives us the phrasal probabilities (after appropriate smoothing).\n5. Extract distortions: Once we have an alignment of phrases we can deﬁne distortion\nprobabilities. Simply count how often distortion occurs in the corpus for each distance\nd = 0, ±1, ±2, . . ., and apply smoothing.\n6. Improve estimates with EM: Use expectation–maximization to improve the estimates\nof P(f | e) and P(d) values. We compute the best alignments with the current values\nof these parameters in the E step, then update the estimates in the M step and iterate the\nprocess until convergence.\n23.5\nSPEECH RECOGNITION\nSpeech recognition is the task of identifying a sequence of words uttered by a speaker, given\nSPEECH\nRECOGNITION\nthe acoustic signal. It has become one of the mainstream applications of AI—millions of Section 23.5.\nSpeech Recognition\n913\npeople interact with speech recognition systems every day to navigate voice mail systems,",
  "SPEECH\nRECOGNITION\nthe acoustic signal. It has become one of the mainstream applications of AI—millions of Section 23.5.\nSpeech Recognition\n913\npeople interact with speech recognition systems every day to navigate voice mail systems,\nsearch the Web from mobile phones, and other applications. Speech is an attractive option\nwhen hands-free operation is necessary, as when operating machinery.\nSpeech recognition is difﬁcult because the sounds made by a speaker are ambiguous\nand, well, noisy. As a well-known example, the phrase “recognize speech” sounds almost\nthe same as “wreck a nice beach” when spoken quickly. Even this short example shows\nseveral of the issues that make speech problematic. First, segmentation: written words in\nSEGMENTATION\nEnglish have spaces between them, but in fast speech there are no pauses in “wreck a nice”\nthat would distinguish it as a multiword phrase as opposed to the single word “recognize.”\nSecond, coarticulation: when speaking quickly the “s” sound at the end of “nice” merges\nCOARTICULATION\nwith the “b” sound at the beginning of “beach,” yielding something that is close to a “sp.”\nAnother problem that does not show up in this example is homophones—words like “to,”\nHOMOPHONES\n“too,” and “two” that sound the same but differ in meaning.\nWe can view speech recognition as a problem in most-likely-sequence explanation. As\nwe saw in Section 15.2, this is the problem of computing the most likely sequence of state\nvariables, x1:t, given a sequence of observations e1:t. In this case the state variables are the\nwords, and the observations are sounds. More precisely, an observation is a vector of features\nextracted from the audio signal. As usual, the most likely sequence can be computed with the\nhelp of Bayes’ rule to be:\nargmax\nword1:t\nP(word 1:t | sound1:t) = argmax\nword1:t\nP(sound 1:t | word 1:t)P(word 1:t) .\nHere P(sound 1:t|word 1:t) is the acoustic model. It describes the sounds of words—that\nACOUSTIC MODEL\n“ceiling” begins with a soft “c” and sounds the same as “sealing.” P(word 1:t) is known as\nthe language model. It speciﬁes the prior probability of each utterance—for example, that\nLANGUAGE MODEL\n“ceiling fan” is about 500 times more likely as a word sequence than “sealing fan.”\nThis approach was named the noisy channel model by Claude Shannon (1948). He\nNOISY CHANNEL\nMODEL\ndescribed a situation in which an original message (the words in our example) is transmitted",
  "This approach was named the noisy channel model by Claude Shannon (1948). He\nNOISY CHANNEL\nMODEL\ndescribed a situation in which an original message (the words in our example) is transmitted\nover a noisy channel (such as a telephone line) such that a corrupted message (the sounds\nin our example) are received at the other end. Shannon showed that no matter how noisy\nthe channel, it is possible to recover the original message with arbitrarily small error, if we\nencode the original message in a redundant enough way. The noisy channel approach has\nbeen applied to speech recognition, machine translation, spelling correction, and other tasks.\nOnce we deﬁne the acoustic and language models, we can solve for the most likely\nsequence of words using the Viterbi algorithm (Section 15.2.3 on page 576). Most speech\nrecognition systems use a language model that makes the Markov assumption—that the cur-\nrent state Wordt depends only on a ﬁxed number n of previous states—and represent Word t\nas a single random variable taking on a ﬁnite set of values, which makes it a Hidden Markov\nModel (HMM). Thus, speech recognition becomes a simple application of the HMM method-\nology, as described in Section 15.3—simple that is, once we deﬁne the acoustic and language\nmodels. We cover them next. 914\nChapter\n23.\nNatural Language for Communication\nVowels\nConsonants B–N\nConsonants P–Z\nPhone\nExample\nPhone\nExample\nPhone\nExample\n[iy]\nbeat\n[b]\nbet\n[p]\npet\n[ih]\nbit\n[ch]\nChet\n[r]\nrat\n[eh]\nbet\n[d]\ndebt\n[s]\nset\n[æ]\nbat\n[f]\nfat\n[sh]\nshoe\n[ah]\nbut\n[g]\nget\n[t]\nten\n[ao]\nbought\n[hh]\nhat\n[th]\nthick\n[ow]\nboat\n[hv]\nhigh\n[dh]\nthat\n[uh]\nbook\n[jh]\njet\n[dx]\nbutter\n[ey]\nbait\n[k]\nkick\n[v]\nvet\n[er]\nBert\n[l]\nlet\n[w]\nwet\n[ay]\nbuy\n[el]\nbottle\n[wh]\nwhich\n[oy]\nboy\n[m]\nmet\n[y]\nyet\n[axr]\ndiner\n[em]\nbottom\n[z]\nzoo\n[aw]\ndown\n[n]\nnet\n[zh]\nmeasure\n[ax]\nabout\n[en]\nbutton\n[ix]\nroses\n[ng]\nsing\n[aa]\ncot\n[eng]\nwashing\n[-]\nsilence\nFigure 23.14\nThe ARPA phonetic alphabet, or ARPAbet, listing all the phones used in\nAmerican English. There are several alternative notations, including an International Pho-\nnetic Alphabet (IPA), which contains the phones in all known languages.\n23.5.1\nAcoustic model\nSound waves are periodic changes in pressure that propagate through the air. When these\nwaves strike the diaphragm of a microphone, the back-and-forth movement generates an\nelectric current. An analog-to-digital converter measures the size of the current—which ap-",
  "waves strike the diaphragm of a microphone, the back-and-forth movement generates an\nelectric current. An analog-to-digital converter measures the size of the current—which ap-\nproximates the amplitude of the sound wave—at discrete intervals called the sampling rate.\nSAMPLING RATE\nSpeech sounds, which are mostly in the range of 100 Hz (100 cycles per second) to 1000 Hz,\nare typically sampled at a rate of 8 kHz. (CDs and mp3 ﬁles are sampled at 44.1 kHz.) The\nprecision of each measurement is determined by the quantization factor; speech recognizers\nQUANTIZATION\nFACTOR\ntypically keep 8 to 12 bits. That means that a low-end system, sampling at 8 kHz with 8-bit\nquantization, would require nearly half a megabyte per minute of speech.\nSince we only want to know what words were spoken, not exactly what they sounded\nlike, we don’t need to keep all that information. We only need to distinguish between differ-\nent speech sounds. Linguists have identiﬁed about 100 speech sounds, or phones, that can be\nPHONE\ncomposed to form all the words in all known human languages. Roughly speaking, a phone\nis the sound that corresponds to a single vowel or consonant, but there are some complica-\ntions: combinations of letters, such as “th” and “ng” produce single phones, and some letters\nproduce different phones in different contexts (e.g., the “a” in rat and rate. Figure 23.14 lists Section 23.5.\nSpeech Recognition\n915\nall the phones that are used in English, with an example of each. A phoneme is the smallest\nPHONEME\nunit of sound that has a distinct meaning to speakers of a particular language. For example,\nthe “t” in “stick” sounds similar enough to the “t” in “tick” that speakers of English consider\nthem the same phoneme. But the difference is signiﬁcant in the Thai language, so there they\nare two phonemes. To represent spoken English we want a representation that can distinguish\nbetween different phonemes, but one that need not distinguish the nonphonemic variations in\nsound: loud or soft, fast or slow, male or female voice, etc.\nFirst, we observe that although the sound frequencies in speech may be several kHz,\nthe changes in the content of the signal occur much less often, perhaps at no more than 100\nHz. Therefore, speech systems summarize the properties of the signal over time slices called\nframes. A frame length of about 10 milliseconds (i.e., 80 samples at 8 kHz) is short enough\nFRAME",
  "Hz. Therefore, speech systems summarize the properties of the signal over time slices called\nframes. A frame length of about 10 milliseconds (i.e., 80 samples at 8 kHz) is short enough\nFRAME\nto ensure that few short-duration phenomena will be missed. Overlapping frames are used to\nmake sure that we don’t miss a signal because it happens to fall on a frame boundary.\nEach frame is summarized by a vector of features. Picking out features from a speech\nFEATURE\nsignal is like listening to an orchestra and saying “here the French horns are playing loudly\nand the violins are playing softly.” We’ll give a brief overview of the features in a typical\nsystem. First, a Fourier transform is used to determine the amount of acoustic energy at\nabout a dozen frequencies. Then we compute a measure called the mel frequency cepstral\ncoefﬁcient (MFCC) or MFCC for each frequency. We also compute the total energy in\nMEL FREQUENCY\nCEPSTRAL\nCOEFFICIENT (MFCC)\nthe frame. That gives thirteen features; for each one we compute the difference between\nthis frame and the previous frame, and the difference between differences, for a total of 39\nfeatures. These are continuous-valued; the easiest way to ﬁt them into the HMM framework\nis to discretize the values. (It is also possible to extend the HMM model to handle continuous\nmixtures of Gaussians.) Figure 23.15 shows the sequence of transformations from the raw\nsound to a sequence of frames with discrete features.\nWe have seen how to go from the raw acoustic signal to a series of observations, et.\nNow we have to describe the (unobservable) states of the HMM and deﬁne the transition\nmodel, P(Xt | Xt−1), and the sensor model, P(Et | Xt). The transition model can be broken\ninto two levels: word and phone. We’ll start from the bottom: the phone model describes\nPHONE MODEL\nAnalog acoustic signal:\nSampled, quantized \ndigital signal:\nFrames with features:\n10  15  38\n52  47  82\n22  63  24\n89  94  11\n10  12  73\nFigure 23.15\nTranslating the acoustic signal into a sequence of frames. In this diagram\neach frame is described by the discretized values of three acoustic features; a real system\nwould have dozens of features. 916\nChapter\n23.\nNatural Language for Communication\nPhone HMM for [m]:\n0.1\n0.9\n0.3\n0.6\n0.4\nC1: 0.5\nC2: 0.2\nC3: 0.3\nC3: 0.2\nC4: 0.7\nC5: 0.1\nC4: 0.1\nC6: 0.5\nC7: 0.4\nOutput probabilities for the phone HMM:\nOnset:\nMid:\nEnd:\nFINAL\n0.7\nMid\nEnd\nOnset\nFigure 23.16\nAn HMM for the three-state phone [m]. Each state has several possible",
  "0.3\n0.6\n0.4\nC1: 0.5\nC2: 0.2\nC3: 0.3\nC3: 0.2\nC4: 0.7\nC5: 0.1\nC4: 0.1\nC6: 0.5\nC7: 0.4\nOutput probabilities for the phone HMM:\nOnset:\nMid:\nEnd:\nFINAL\n0.7\nMid\nEnd\nOnset\nFigure 23.16\nAn HMM for the three-state phone [m]. Each state has several possible\noutputs, each with its own probability. The MFCC feature labels C1 through C7 are arbitrary,\nstanding for some combination of feature values.\n0.5\n0.5\n[t]\n[ow]\n[m]\n[ey]\n[ow]\n[aa]\n[t]\n0.5\n0.5\n0.2\n0.8\n[m]\n[ey]\n[ow]\n[t]\n[aa]\n[t]\n[ah]\n[ow]\n(a) Word model with dialect variation:\n(b) Word model with coarticulation and dialect variations\n:\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nFigure 23.17\nTwo pronunciation models of the word “tomato.” Each model is shown as\na transition diagram with states as circles and arrows showing allowed transitions with their\nassociated probabilities. (a) A model allowing for dialect differences. The 0.5 numbers are\nestimates based on the two authors’ preferred pronunciations. (b) A model with a coarticula-\ntion effect on the ﬁrst vowel, allowing either the [ow] or the [ah] phone. Section 23.5.\nSpeech Recognition\n917\na phone as three states, the onset, middle, and end. For example, the [t] phone has a silent\nbeginning, a small explosive burst of sound in the middle, and (usually) a hissing at the end.\nFigure 23.16 shows an example for the phone [m]. Note that in normal speech, an average\nphone has a duration of 50–100 milliseconds, or 5–10 frames. The self-loops in each state\nallows for variation in this duration. By taking many self-loops (especially in the mid state),\nwe can represent a long “mmmmmmmmmmm” sound. Bypassing the self-loops yields a\nshort “m” sound.\nIn Figure 23.17 the phone models are strung together to form a pronunciation model\nPRONUNCIATION\nMODEL\nfor a word. According to Gershwin (1937), you say [t ow m ey t ow] and I say [t ow m aa t\now]. Figure 23.17(a) shows a transition model that provides for this dialect variation. Each\nof the circles in this diagram represents a phone model like the one in Figure 23.16.\nIn addition to dialect variation, words can have coarticulation variation. For example,\nthe [t] phone is produced with the tongue at the top of the mouth, whereas the [ow] has the\ntongue near the bottom. When speaking quickly, the tongue doesn’t have time to get into\nposition for the [ow], and we end up with [t ah] rather than [t ow]. Figure 23.17(b) gives\na model for “tomato” that takes this coarticulation effect into account. More sophisticated",
  "position for the [ow], and we end up with [t ah] rather than [t ow]. Figure 23.17(b) gives\na model for “tomato” that takes this coarticulation effect into account. More sophisticated\nphone models take into account the context of the surrounding phones.\nThere can be substantial variation in pronunciation for a word. The most common\npronunciation of “because” is [b iy k ah z], but that only accounts for about a quarter of\nuses. Another quarter (approximately) substitutes [ix], [ih] or [ax] for the ﬁrst vowel, and the\nremainder substitute [ax] or [aa] for the second vowel, [zh] or [s] for the ﬁnal [z], or drop\n“be” entirely, leaving “cuz.”\n23.5.2\nLanguage model\nFor general-purpose speech recognition, the language model can be an n-gram model of\ntext learned from a corpus of written sentences. However, spoken language has different\ncharacteristics than written language, so it is better to get a corpus of transcripts of spoken\nlanguage. For task-speciﬁc speech recognition, the corpus should be task-speciﬁc: to build\nyour airline reservation system, get transcripts of prior calls. It also helps to have task-speciﬁc\nvocabulary, such as a list of all the airports and cities served, and all the ﬂight numbers.\nPart of the design of a voice user interface is to coerce the user into saying things from a\nlimited set of options, so that the speech recognizer will have a tighter probability distribution\nto deal with. For example, asking “What city do you want to go to?” elicits a response with\na highly constrained language model, while asking “How can I help you?” does not.\n23.5.3\nBuilding a speech recognizer\nThe quality of a speech recognition system depends on the quality of all of its components—\nthe language model, the word-pronunciation models, the phone models, and the signal-\nprocessing algorithms used to extract spectral features from the acoustic signal. We have\ndiscussed how the language model can be constructed from a corpus of written text, and we\nleave the details of signal processing to other textbooks. We are left with the pronunciation\nand phone models. The structure of the pronunciation models—such as the tomato models in 918\nChapter\n23.\nNatural Language for Communication\nFigure 23.17—is usually developed by hand. Large pronunciation dictionaries are now avail-\nable for English and other languages, although their accuracy varies greatly. The structure\nof the three-state phone models is the same for all phones, as shown in Figure 23.16. That",
  "able for English and other languages, although their accuracy varies greatly. The structure\nof the three-state phone models is the same for all phones, as shown in Figure 23.16. That\nleaves the probabilities themselves.\nAs usual, we will acquire the probabilities from a corpus, this time a corpus of speech.\nThe most common type of corpus to obtain is one that includes the speech signal for each\nsentence paired with a transcript of the words. Building a model from this corpus is more\ndifﬁcult than building an n-gram model of text, because we have to build a hidden Markov\nmodel—the phone sequence for each word and the phone state for each time frame are hidden\nvariables. In the early days of speech recognition, the hidden variables were provided by\nlaborious hand-labeling of spectrograms. Recent systems use expectation–maximization to\nautomatically supply the missing data. The idea is simple: given an HMM and an observation\nsequence, we can use the smoothing algorithms from Sections 15.2 and 15.3 to compute the\nprobability of each state at each time step and, by a simple extension, the probability of each\nstate–state pair at consecutive time steps. These probabilities can be viewed as uncertain\nlabels. From the uncertain labels, we can estimate new transition and sensor probabilities,\nand the EM procedure repeats. The method is guaranteed to increase the ﬁt between model\nand data on each iteration, and it generally converges to a much better set of parameter values\nthan those provided by the initial, hand-labeled estimates.\nThe systems with the highest accuracy work by training a different model for each\nspeaker, thereby capturing differences in dialect as well as male/female and other variations.\nThis training can require several hours of interaction with the speaker, so the systems with\nthe most widespread adoption do not create speaker-speciﬁc models.\nThe accuracy of a system depends on a number of factors. First, the quality of the signal\nmatters: a high-quality directional microphone aimed at a stationary mouth in a padded room\nwill do much better than a cheap microphone transmitting a signal over phone lines from a\ncar in trafﬁc with the radio playing. The vocabulary size matters: when recognizing digit\nstrings with a vocabulary of 11 words (1-9 plus “oh” and “zero”), the word error rate will be\nbelow 0.5%, whereas it rises to about 10% on news stories with a 20,000-word vocabulary,",
  "strings with a vocabulary of 11 words (1-9 plus “oh” and “zero”), the word error rate will be\nbelow 0.5%, whereas it rises to about 10% on news stories with a 20,000-word vocabulary,\nand 20% on a corpus with a 64,000-word vocabulary. The task matters too: when the system\nis trying to accomplish a speciﬁc task—book a ﬂight or give directions to a restaurant—the\ntask can often be accomplished perfectly even with a word error rate of 10% or more.\n23.6\nSUMMARY\nNatural language understanding is one of the most important subﬁelds of AI. Unlike most\nother areas of AI, natural language understanding requires an empirical investigation of actual\nhuman behavior—which turns out to be complex and interesting.\n• Formal language theory and phrase structure grammars (and in particular, context-\nfree grammar) are useful tools for dealing with some aspects of natural language. The\nprobabilistic context-free grammar (PCFG) formalism is widely used. Bibliographical and Historical Notes\n919\n• Sentences in a context-free language can be parsed in O(n3) time by a chart parser\nsuch as the CYK algorithm, which requires grammar rules to be in Chomsky Normal\nForm.\n• A treebank can be used to learn a grammar. It is also possible to learn a grammar from\nan unparsed corpus of sentences, but this is less successful.\n• A lexicalized PCFG allows us to represent that some relationships between words are\nmore common than others.\n• It is convenient to augment a grammar to handle such problems as subject–verb agree-\nment and pronoun case. Deﬁnite clause grammar (DCG) is a formalism that allows for\naugmentations. With DCG, parsing and semantic interpretation (and even generation)\ncan be done using logical inference.\n• Semantic interpretation can also be handled by an augmented grammar.\n• Ambiguity is a very important problem in natural language understanding; most sen-\ntences have many possible interpretations, but usually only one is appropriate. Disam-\nbiguation relies on knowledge about the world, about the current situation, and about\nlanguage use.\n• Machine translation systems have been implemented using a range of techniques,\nfrom full syntactic and semantic analysis to statistical techniques based on phrase fre-\nquencies. Currently the statistical models are most popular and most successful.\n• Speech recognition systems are also primarily based on statistical principles. Speech\nsystems are popular and useful, albeit imperfect.",
  "quencies. Currently the statistical models are most popular and most successful.\n• Speech recognition systems are also primarily based on statistical principles. Speech\nsystems are popular and useful, albeit imperfect.\n• Together, machine translation and speech recognition are two of the big successes of\nnatural language technology. One reason that the models perform well is that large\ncorpora are available—both translation and speech are tasks that are performed “in the\nwild” by people every day. In contrast, tasks like parsing sentences have been less\nsuccessful, in part because no large corpora of parsed sentences are available “in the\nwild” and in part because parsing is not useful in and of itself.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nLike semantic networks, context-free grammars (also known as phrase structure grammars)\nare a reinvention of a technique ﬁrst used by ancient Indian grammarians (especially Panini,\nca. 350 B.C.) studying Shastric Sanskrit (Ingerman, 1967). They were reinvented by Noam\nChomsky (1956) for the analysis of English syntax and independently by John Backus for\nthe analysis of Algol-58 syntax. Peter Naur extended Backus’s notation and is now credited\n(Backus, 1996) with the “N” in BNF, which originally stood for “Backus Normal Form.”\nKnuth (1968) deﬁned a kind of augmented grammar called attribute grammar that is use-\nATTRIBUTE\nGRAMMAR\nful for programming languages.\nDeﬁnite clause grammars were introduced by Colmer-\nauer (1975) and developed and popularized by Pereira and Shieber (1987).\nProbabilistic context-free grammars were investigated by Booth (1969) and Salo-\nmaa (1969). Other algorithms for PCFGs are presented in the excellent short monograph by 920\nChapter\n23.\nNatural Language for Communication\nCharniak (1993) and the excellent long textbooks by Manning and Sch¨utze (1999) and Juraf-\nsky and Martin (2008). Baker (1979) introduces the inside–outside algorithm for learning a\nPCFG, and Lari and Young (1990) describe its uses and limitations. Stolcke and Omohundro\n(1994) show how to learn grammar rules with Bayesian model merging; Haghighi and Klein\n(2006) describe a learning system based on prototypes.\nLexicalized PCFGs (Charniak, 1997; Hwa, 1998) combine the best aspects of PCFGs\nand n-gram models. Collins (1999) describes PCFG parsing that is lexicalized with head\nfeatures. Petrov and Klein (2007a) show how to get the advantages of lexicalization without",
  "and n-gram models. Collins (1999) describes PCFG parsing that is lexicalized with head\nfeatures. Petrov and Klein (2007a) show how to get the advantages of lexicalization without\nactual lexical augmentations by learning speciﬁc syntactic categories from a treebank that has\ngeneral categories; for example, the treebank has the category NP, from which more speciﬁc\ncategories such as NPO and NPS can be learned.\nThere have been many attempts to write formal grammars of natural languages, both\nin “pure” linguistics and in computational linguistics. There are several comprehensive but\ninformal grammars of English (Quirk et al., 1985; McCawley, 1988; Huddleston and Pullum,\n2002). Since the mid-1980s, there has been a trend toward putting more information in the\nlexicon and less in the grammar. Lexical-functional grammar, or LFG (Bresnan, 1982) was\nthe ﬁrst major grammar formalism to be highly lexicalized. If we carry lexicalization to an\nextreme, we end up with categorial grammar (Clark and Curran, 2004), in which there can\nbe as few as two grammar rules, or with dependency grammar (Smith and Eisner, 2008;\nK¨ubler et al., 2009) in which there are no syntactic categories, only relations between words.\nSleator and Temperley (1993) describe a dependency parser. Paskin (2001) shows that a\nversion of dependency grammar is easier to learn than PCFGs.\nThe ﬁrst computerized parsing algorithms were demonstrated by Yngve (1955). Ef-\nﬁcient algorithms were developed in the late 1960s, with a few twists since then (Kasami,\n1965; Younger, 1967; Earley, 1970; Graham et al., 1980). Maxwell and Kaplan (1993) show\nhow chart parsing with augmentations can be made efﬁcient in the average case. Church\nand Patil (1982) address the resolution of syntactic ambiguity. Klein and Manning (2003)\ndescribe A∗parsing, and Pauls and Klein (2009) extend that to K-best A∗parsing, in which\nthe result is not a single parse but the K best.\nLeading parsers today include those by Petrov and Klein (2007b), which achieved\n90.6% accuracy on the Wall Street Journal corpus, Charniak and Johnson (2005), which\nachieved 92.0%, and Koo et al. (2008), which achieved 93.2% on the Penn treebank. These\nnumbers are not directly comparable, and there is some criticism of the ﬁeld that it is focusing\ntoo narrowly on a few select corpora, and perhaps overﬁtting on them.\nFormal semantic interpretation of natural languages originates within philosophy and",
  "too narrowly on a few select corpora, and perhaps overﬁtting on them.\nFormal semantic interpretation of natural languages originates within philosophy and\nformal logic, particularly Alfred Tarski’s (1935) work on the semantics of formal languages.\nBar-Hillel (1954) was the ﬁrst to consider the problems of pragmatics and propose that they\ncould be handled by formal logic. For example, he introduced C. S. Peirce’s (1902) term\nindexical into linguistics. Richard Montague’s essay “English as a formal language” (1970)\nis a kind of manifesto for the logical analysis of language, but the books by Dowty et al.\n(1991) and Portner and Partee (2002) are more readable.\nThe ﬁrst NLP system to solve an actual task was probably the BASEBALL question\nanswering system (Green et al., 1961), which handled questions about a database of baseball Bibliographical and Historical Notes\n921\nstatistics. Close after that was Woods’s (1973) LUNAR, which answered questions about the\nrocks brought back from the moon by the Apollo program. Roger Schank and his students\nbuilt a series of programs (Schank and Abelson, 1977; Schank and Riesbeck, 1981) that\nall had the task of understanding language. Modern approaches to semantic interpretation\nusually assume that the mapping from syntax to semantics will be learned from examples\n(Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005).\nHobbs et al. (1993) describes a quantitative nonprobabilistic framework for interpreta-\ntion. More recent work follows an explicitly probabilistic framework (Charniak and Gold-\nman, 1992; Wu, 1993; Franz, 1996). In linguistics, optimality theory (Kager, 1999) is based\non the idea of building soft constraints into the grammar, giving a natural ranking to inter-\npretations (similar to a probability distribution), rather than having the grammar generate all\npossibilities with equal rank. Norvig (1988) discusses the problems of considering multiple\nsimultaneous interpretations, rather than settling for a single maximum-likelihood interpre-\ntation. Literary critics (Empson, 1953; Hobbs, 1990) have been ambiguous about whether\nambiguity is something to be resolved or cherished.\nNunberg (1979) outlines a formal model of metonymy. Lakoff and Johnson (1980) give\nan engaging analysis and catalog of common metaphors in English. Martin (1990) and Gibbs\n(2006) offer computational models of metaphor interpretation.\nThe ﬁrst important result on grammar induction was a negative one: Gold (1967)",
  "an engaging analysis and catalog of common metaphors in English. Martin (1990) and Gibbs\n(2006) offer computational models of metaphor interpretation.\nThe ﬁrst important result on grammar induction was a negative one: Gold (1967)\nshowed that it is not possible to reliably learn a correct context-free grammar, given a set of\nstrings from that grammar. Prominent linguists, such as Chomsky (1957) and Pinker (2003),\nhave used Gold’s result to argue that there must be an innate universal grammar that all\nUNIVERSAL\nGRAMMAR\nchildren have from birth. The so-called Poverty of the Stimulus argument says that children\naren’t given enough input to learn a CFG, so they must already “know” the grammar and be\nmerely tuning some of its parameters. While this argument continues to hold sway throughout\nmuch of Chomskyan linguistics, it has been dismissed by some other linguists (Pullum, 1996;\nElman et al., 1997) and most computer scientists. As early as 1969, Horning showed that it\nis possible to learn, in the sense of PAC learning, a probabilistic context-free grammar. Since\nthen, there have been many convincing empirical demonstrations of learning from positive\nexamples alone, such as the ILP work of Mooney (1999) and Muggleton and De Raedt (1994),\nthe sequence learning of Nevill-Manning and Witten (1997), and the remarkable Ph.D. theses\nof Sch¨utze (1995) and de Marcken (1996). There is an annual International Conference on\nGrammatical Inference (ICGI). It is possible to learn other grammar formalisms, such as\nregular languages (Denis, 2001) and ﬁnite state automata (Parekh and Honavar, 2001). Abney\n(2007) is a textbook introduction to semi-supervised learning for language models.\nWordnet (Fellbaum, 2001) is a publicly available dictionary of about 100,000 words and\nphrases, categorized into parts of speech and linked by semantic relations such as synonym,\nantonym, and part-of. The Penn Treebank (Marcus et al., 1993) provides parse trees for a\n3-million-word corpus of English. Charniak (1996) and Klein and Manning (2001) discuss\nparsing with treebank grammars. The British National Corpus (Leech et al., 2001) contains\n100 million words, and the World Wide Web contains several trillion words; (Brants et al.,\n2007) describe n-gram models over a 2-trillion-word Web corpus. 922\nChapter\n23.\nNatural Language for Communication\nIn the 1930s Petr Troyanskii applied for a patent for a “translating machine,” but there",
  "2007) describe n-gram models over a 2-trillion-word Web corpus. 922\nChapter\n23.\nNatural Language for Communication\nIn the 1930s Petr Troyanskii applied for a patent for a “translating machine,” but there\nwere no computers available to implement his ideas. In March 1947, the Rockefeller Founda-\ntion’s Warren Weaver wrote to Norbert Wiener, suggesting that machine translation might be\npossible. Drawing on work in cryptography and information theory, Weaver wrote, “When I\nlook at an article in Russian, I say: ‘This is really written in English, but it has been coded in\nstrange symbols. I will now proceed to decode.”’ For the next decade, the community tried\nto decode in this way. IBM exhibited a rudimentary system in 1954. Bar-Hillel (1960) de-\nscribes the enthusiasm of this period. However, the U.S. government subsequently reported\n(ALPAC, 1966) that “there is no immediate or predictable prospect of useful machine trans-\nlation.” However, limited work continued, and starting in the 1980s, computer power had\nincreased to the point where the ALPAC ﬁndings were no longer correct.\nThe basic statistical approach we describe in the chapter is based on early work by the\nIBM group (Brown et al., 1988, 1993) and the recent work by the ISI and Google research\ngroups (Och and Ney, 2004; Zollmann et al., 2008). A textbook introduction on statistical\nmachine translation is given by Koehn (2009), and a short tutorial by Kevin Knight (1999) has\nbeen inﬂuential. Early work on sentence segmentation was done by Palmer and Hearst (1994).\nOch and Ney (2003) and Moore (2005) cover bilingual sentence alignment.\nThe prehistory of speech recognition began in the 1920s with Radio Rex, a voice-\nactivated toy dog. Rex jumped out of his doghouse in response to the word “Rex!” (or\nactually almost any sufﬁciently loud word). Somewhat more serious work began after World\nWar II. At AT&T Bell Labs, a system was built for recognizing isolated digits (Davis et al.,\n1952) by means of simple pattern matching of acoustic features. Starting in 1971, the Defense\nAdvanced Research Projects Agency (DARPA) of the United States Department of Defense\nfunded four competing ﬁve-year projects to develop high-performance speech recognition\nsystems. The winner, and the only system to meet the goal of 90% accuracy with a 1000-word\nvocabulary, was the HARPY system at CMU (Lowerre and Reddy, 1980). The ﬁnal version\nof HARPY was derived from a system called DRAGON built by CMU graduate student James",
  "vocabulary, was the HARPY system at CMU (Lowerre and Reddy, 1980). The ﬁnal version\nof HARPY was derived from a system called DRAGON built by CMU graduate student James\nBaker (1975); DRAGON was the ﬁrst to use HMMs for speech. Almost simultaneously, Je-\nlinek (1976) at IBM had developed another HMM-based system. Recent years have been\ncharacterized by steady incremental progress, larger data sets and models, and more rigor-\nous competitions on more realistic speech tasks. In 1997, Bill Gates predicted, “The PC ﬁve\nyears from now—you won’t recognize it, because speech will come into the interface.” That\ndidn’t quite happen, but in 2008 he predicted “In ﬁve years, Microsoft expects more Internet\nsearches to be done through speech than through typing on a keyboard.” History will tell if\nhe is right this time around.\nSeveral good textbooks on speech recognition are available (Rabiner and Juang, 1993;\nJelinek, 1997; Gold and Morgan, 2000; Huang et al., 2001). The presentation in this chapter\ndrew on the survey by Kay, Gawron, and Norvig (1994) and on the textbook by Jurafsky and\nMartin (2008). Speech recognition research is published in Computer Speech and Language,\nSpeech Communications, and the IEEE Transactions on Acoustics, Speech, and Signal Pro-\ncessing and at the DARPA Workshops on Speech and Natural Language Processing and the\nEurospeech, ICSLP, and ASRU conferences. Exercises\n923\nKen Church (2004) shows that natural language research has cycled between concen-\ntrating on the data (empiricism) and concentrating on theories (rationalism). The linguist\nJohn Firth (1957) proclaimed “You shall know a word by the company it keeps,” and linguis-\ntics of the 1940s and early 1950s was based largely on word frequencies, although without\nthe computational power we have available today. Then Noam (Chomsky, 1956) showed\nthe limitations of ﬁnite-state models, and sparked an interest in theoretical studies of syntax,\ndisregarding frequency counts. This approach dominated for twenty years, until empiricism\nmade a comeback based on the success of work in statistical speech recognition (Jelinek,\n1976). Today, most work accepts the statistical framework, but there is great interest in build-\ning statistical models that consider higher-level models, such as syntactic trees and semantic\nrelations, not just sequences of words.\nWork on applications of language processing is presented at the biennial Applied Natu-",
  "ing statistical models that consider higher-level models, such as syntactic trees and semantic\nrelations, not just sequences of words.\nWork on applications of language processing is presented at the biennial Applied Natu-\nral Language Processing conference (ANLP), the conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), and the journal Natural Language Engineering. A broad\nrange of NLP work appears in the journal Computational Linguistics and its conference, ACL,\nand in the Computational Linguistics (COLING) conference.\nEXERCISES\n23.1\nRead the following text once for understanding, and remember as much of it as you\ncan. There will be a test later.\nThe procedure is actually quite simple. First you arrange things into different groups. Of\ncourse, one pile may be sufﬁcient depending on how much there is to do. If you have to go\nsomewhere else due to lack of facilities that is the next step, otherwise you are pretty well\nset. It is important not to overdo things. That is, it is better to do too few things at once\nthan too many. In the short run this may not seem important but complications can easily\narise. A mistake is expensive as well. At ﬁrst the whole procedure will seem complicated.\nSoon, however, it will become just another facet of life. It is difﬁcult to foresee any end\nto the necessity for this task in the immediate future, but then one can never tell. After the\nprocedure is completed one arranges the material into different groups again. Then they\ncan be put into their appropriate places. Eventually they will be used once more and the\nwhole cycle will have to be repeated. However, this is part of life.\n23.2\nAn HMM grammar is essentially a standard HMM whose state variable is N (nonter-\nminal, with values such as Det, Adjective, Noun and so on) and whose evidence variable is\nW (word, with values such as is, duck, and so on). The HMM model includes a prior P(N0),\na transition model P(Nt+1|Nt), and a sensor model P(Wt|Nt). Show that every HMM gram-\nmar can be written as a PCFG. [Hint: start by thinking about how the HMM prior can be\nrepresented by PCFG rules for the sentence symbol. You may ﬁnd it helpful to illustrate for\nthe particular HMM with values A, B for N and values x, y for W.] 924\nChapter\n23.\nNatural Language for Communication\n23.3\nConsider the following PCFG for simple verb phrases:\n0.1 : V P →V erb\n0.2 : V P →Copula Adjective\n0.5 : V P →V erb the Noun\n0.2 : V P →V P Adverb\n0.5 : V erb →is\n0.5 : V erb →shoots",
  "Chapter\n23.\nNatural Language for Communication\n23.3\nConsider the following PCFG for simple verb phrases:\n0.1 : V P →V erb\n0.2 : V P →Copula Adjective\n0.5 : V P →V erb the Noun\n0.2 : V P →V P Adverb\n0.5 : V erb →is\n0.5 : V erb →shoots\n0.8 : Copula →is\n0.2 : Copula →seems\n0.5 : Adjective →unwell\n0.5 : Adjective →well\n0.5 : Adverb →well\n0.5 : Adverb →badly\n0.6 : Noun →duck\n0.4 : Noun →well\na. Which of the following have a nonzero probability as a VP? (i) shoots the duck well\nwell well\n(ii) seems the well well\n(iii) shoots the unwell well badly\nb. What is the probability of generating “is well well”?\nc. What types of ambiguity are exhibited by the phrase in (b)?\nd. Given any PCFG, is it possible to calculate the probability that the PCFG generates a\nstring of exactly 10 words?\n23.4\nOutline the major differences between Java (or any other computer language with\nwhich you are familiar) and English, commenting on the “understanding” problem in each\ncase. Think about such things as grammar, syntax, semantics, pragmatics, compositional-\nity, context-dependence, lexical ambiguity, syntactic ambiguity, reference ﬁnding (including\npronouns), background knowledge, and what it means to “understand” in the ﬁrst place.\n23.5\nThis exercise concerns grammars for very simple languages.\na. Write a context-free grammar for the language anbn.\nb. Write a context-free grammar for the palindrome language: the set of all strings whose\nsecond half is the reverse of the ﬁrst half.\nc. Write a context-sensitive grammar for the duplicate language: the set of all strings\nwhose second half is the same as the ﬁrst half.\n23.6\nConsider the sentence “Someone walked slowly to the supermarket” and a lexicon\nconsisting of the following words:\nPronoun →someone\nVerb →walked\nAdv →slowly\nPrep →to\nArticle →the\nNoun →supermarket\nWhich of the following three grammars, combined with the lexicon, generates the given sen-\ntence? Show the corresponding parse tree(s). Exercises\n925\n(A):\n(B):\n(C):\nS →NP VP\nS →NP VP\nS →NP VP\nNP →Pronoun\nNP →Pronoun\nNP →Pronoun\nNP →Article Noun\nNP →Noun\nNP →Article NP\nVP →VP PP\nNP →Article NP\nVP →Verb Adv\nVP →VP Adv Adv\nVP →Verb Vmod\nAdv →Adv Adv\nVP →Verb\nVmod →Adv Vmod\nAdv →PP\nPP →Prep NP\nVmod →Adv\nPP →Prep NP\nNP →Noun\nAdv →PP\nNP →Noun\nPP →Prep NP\nFor each of the preceding three grammars, write down three sentences of English and three\nsentences of non-English generated by the grammar. Each sentence should be signiﬁcantly",
  "PP →Prep NP\nVmod →Adv\nPP →Prep NP\nNP →Noun\nAdv →PP\nNP →Noun\nPP →Prep NP\nFor each of the preceding three grammars, write down three sentences of English and three\nsentences of non-English generated by the grammar. Each sentence should be signiﬁcantly\ndifferent, should be at least six words long, and should include some new lexical entries\n(which you should deﬁne). Suggest ways to improve each grammar to avoid generating the\nnon-English sentences.\n23.7\nCollect some examples of time expressions, such as “two o’clock,” “midnight,” and\n“12:46.” Also think up some examples that are ungrammatical, such as “thirteen o’clock” or\n“half past two ﬁfteen.” Write a grammar for the time language.\n23.8\nIn this exercise you will transform E0 into Chomsky Normal Form (CNF). There are\nﬁve steps: (a) Add a new start symbol, (b) Eliminate ϵ rules, (c) Eliminate multiple words\non right-hand sides, (d) Eliminate rules of the form (X →Y ), (e) Convert long right-hand\nsides into binary rules.\na. The start symbol, S, can occur only on the left-hand side in CNF. Add a new rule of the\nform S ′ →S, using a new symbol S ′.\nb. The empty string, ϵ cannot appear on the right-hand side in CNF. E0 does not have any\nrules with ϵ, so this is not an issue.\nc. A word can appear on the right-hand side in a rule only of the form (X\n→word).\nReplace each rule of the form (X\n→. . . word . . . ) with (X\n→. . . W ′ . . . ) and (W ′\n→word), using a new symbol W ′.\nd. A rule (X\n→Y ) is not allowed in CNF; it must be (X\n→Y Z) or (X\n→word).\nReplace each rule of the form (X\n→Y ) with a set of rules of the form (X\n→. . . ),\none for each rule (Y →. . . ), where (. . . ) indicates one or more symbols.\ne. Replace each rule of the form (X →Y Z . . . ) with two rules, (X →Y Z ′) and (Z ′\n→Z . . . ), where Z ′ is a new symbol.\nShow each step of the process and the ﬁnal set of rules.\n23.9\nUsing DCG notation, write a grammar for a language that is just like E1, except that\nit enforces agreement between the subject and verb of a sentence and thus does not generate\nungrammatical sentences such as “I smells the wumpus.” 926\nChapter\n23.\nNatural Language for Communication\n23.10\nConsider the following PCFG:\nS →NP VP [1.0]\nNP →Noun [0.6] | Pronoun [0.4]\nVP →Verb NP [0.8] | Modal Verb [0.2]\nNoun →can [0.1] | ﬁsh [0.3] | ...\nPronoun →I [0.4] | ...\nVerb →can [0.01] | ﬁsh [0.1] | ...\nModal →can [0.3] | ...\nThe sentence “I can ﬁsh” has two parse trees with this grammar. Show the two trees, their",
  "VP →Verb NP [0.8] | Modal Verb [0.2]\nNoun →can [0.1] | ﬁsh [0.3] | ...\nPronoun →I [0.4] | ...\nVerb →can [0.01] | ﬁsh [0.1] | ...\nModal →can [0.3] | ...\nThe sentence “I can ﬁsh” has two parse trees with this grammar. Show the two trees, their\nprior probabilities, and their conditional probabilities, given the sentence.\n23.11\nAn augmented context-free grammar can represent languages that a regular context-\nfree grammar cannot. Show an augmented context-free grammar for the language anbncn.\nThe allowable values for augmentation variables are 1 and SUCCESSOR(n), where n is a\nvalue. The rule for a sentence in this language is\nS(n) →A(n) B(n) C(n) .\nShow the rule(s) for each of A, B, and C.\n23.12\nAugment the E1 grammar so that it handles article–noun agreement. That is, make\nsure that “agents” and “an agent” are NPs, but “agent” and “an agents” are not.\n23.13\nConsider the following sentence (from The New York Times, July 28, 2008):\nBanks struggling to recover from multibillion-dollar loans on real estate are cur-\ntailing loans to American businesses, depriving even healthy companies of money\nfor expansion and hiring.\na. Which of the words in this sentence are lexically ambiguous?\nb. Find two cases of syntactic ambiguity in this sentence (there are more than two.)\nc. Give an instance of metaphor in this sentence.\nd. Can you ﬁnd semantic ambiguity?\n23.14\nWithout looking back at Exercise 23.1, answer the following questions:\na. What are the four steps that are mentioned?\nb. What step is left out?\nc. What is “the material” that is mentioned in the text?\nd. What kind of mistake would be expensive?\ne. Is it better to do too few things or too many? Why?\n23.15\nSelect ﬁve sentences and submit them to an online translation service. Translate\nthem from English to another language and back to English. Rate the resulting sentences for\ngrammaticality and preservation of meaning. Repeat the process; does the second round of Exercises\n927\niteration give worse results or the same results? Does the choice of intermediate language\nmake a difference to the quality of the results? If you know a foreign language, look at the\ntranslation of one paragraph into that language. Count and describe the errors made, and\nconjecture why these errors were made.\n23.16\nThe Di values for the sentence in Figure 23.13 sum to 0. Will that be true of every\ntranslation pair? Prove it or give a counterexample.\n23.17\n(Adapted from Knight (1999).) Our translation model assumes that, after the phrase",
  "23.16\nThe Di values for the sentence in Figure 23.13 sum to 0. Will that be true of every\ntranslation pair? Prove it or give a counterexample.\n23.17\n(Adapted from Knight (1999).) Our translation model assumes that, after the phrase\ntranslation model selects phrases and the distortion model permutes them, the language model\ncan unscramble the permutation. This exercise investigates how sensible that assumption is.\nTry to unscramble these proposed lists of phrases into the correct order:\na. have, programming, a, seen, never, I, language, better\nb. loves, john, mary\nc. is the, communication, exchange of, intentional, information brought, by, about, the\nproduction, perception of, and signs, from, drawn, a, of, system, signs, conventional,\nshared\nd. created, that, we hold these, to be, all men, truths, are, equal, self-evident\nWhich ones could you do? What type of knowledge did you draw upon? Train a bigram\nmodel from a training corpus, and use it to ﬁnd the highest-probability permutation of some\nsentences from a test corpus. Report on the accuracy of this model.\n23.18\nCalculate the most probable path through the HMM in Figure 23.16 for the output\nsequence [C1, C2, C3, C4, C4, C6, C7]. Also give its probability.\n23.19\nWe forgot to mention that the text in Exercise 23.1 is entitled “Washing Clothes.”\nReread the text and answer the questions in Exercise 23.14. Did you do better this time?\nBransford and Johnson (1973) used this text in a controlled experiment and found that the title\nhelped signiﬁcantly. What does this tell you about how language and memory works? 24\nPERCEPTION\nIn which we connect the computer to the raw, unwashed world.\nPerception provides agents with information about the world they inhabit by interpreting the\nPERCEPTION\nresponse of sensors. A sensor measures some aspect of the environment in a form that can\nSENSOR\nbe used as input by an agent program. The sensor could be as simple as a switch, which gives\none bit telling whether it is on or off, or as complex as the eye. A variety of sensory modalities\nare available to artiﬁcial agents. Those they share with humans include vision, hearing, and\ntouch. Modalities that are not available to the unaided human include radio, infrared, GPS,\nand wireless signals. Some robots do active sensing, meaning they send out a signal, such as\nradar or ultrasound, and sense the reﬂection of this signal off of the environment. Rather than",
  "and wireless signals. Some robots do active sensing, meaning they send out a signal, such as\nradar or ultrasound, and sense the reﬂection of this signal off of the environment. Rather than\ntrying to cover all of these, this chapter will cover one modality in depth: vision.\nWe saw in our description of POMDPs (Section 17.4, page 658) that a model-based\ndecision-theoretic agent in a partially observable environment has a sensor model—a prob-\nability distribution P(E | S) over the evidence that its sensors provide, given a state of the\nworld. Bayes’ rule can then be used to update the estimation of the state.\nFor vision, the sensor model can be broken into two components: An object model\nOBJECT MODEL\ndescribes the objects that inhabit the visual world—people, buildings, trees, cars, etc. The\nobject model could include a precise 3D geometric model taken from a computer-aided design\n(CAD) system, or it could be vague constraints, such as the fact that human eyes are usually 5\nto 7 cm apart. A rendering model describes the physical, geometric, and statistical processes\nRENDERING MODEL\nthat produce the stimulus from the world. Rendering models are quite accurate, but they are\nambiguous. For example, a white object under low light may appear as the same color as a\nblack object under intense light. A small nearby object may look the same as a large distant\nobject. Without additional evidence, we cannot tell if the image that ﬁlls the frame is a toy\nGodzilla or a real monster.\nAmbiguity can be managed with prior knowledge—we know Godzilla is not real, so\nthe image must be a toy—or by selectively choosing to ignore the ambiguity. For example,\nthe vision system for an autonomous car may not be able to interpret objects that are far in\nthe distance, but the agent can choose to ignore the problem, because it is unlikely to crash\ninto an object that is miles away.\n928 Section 24.1.\nImage Formation\n929\nA decision-theoretic agent is not the only architecture that can make use of vision sen-\nsors. For example, fruit ﬂies (Drosophila) are in part reﬂex agents: they have cervical giant\nﬁbers that form a direct pathway from their visual system to the wing muscles that initiate an\nescape response—an immediate reaction, without deliberation. Flies and many other ﬂying\nanimals make used of a closed-loop control architecture to land on an object. The visual\nsystem extracts an estimate of the distance to the object, and the control system adjusts the",
  "animals make used of a closed-loop control architecture to land on an object. The visual\nsystem extracts an estimate of the distance to the object, and the control system adjusts the\nwing muscles accordingly, allowing very fast changes of direction, with no need for a detailed\nmodel of the object.\nCompared to the data from other sensors (such as the single bit that tells the vacuum\nrobot that it has bumped into a wall), visual observations are extraordinarily rich, both in\nthe detail they can reveal and in the sheer amount of data they produce. A video camera\nfor robotic applications might produce a million 24-bit pixels at 60 Hz; a rate of 10 GB per\nminute. The problem for a vision-capable agent then is: Which aspects of the rich visual\nstimulus should be considered to help the agent make good action choices, and which aspects\nshould be ignored?\nVision—and all perception—serves to further the agent’s goals, not as\nan end to itself.\nWe can characterize three broad approaches to the problem. The feature extraction\nFEATURE\nEXTRACTION\napproach, as exhibited by Drosophila, emphasizes simple computations applied directly to\nthe sensor observations. In the recognition approach an agent draws distinctions among the\nRECOGNITION\nobjects it encounters based on visual and other information. Recognition could mean labeling\neach image with a yes or no as to whether it contains food that we should forage, or contains\nGrandma’s face. Finally, in the reconstruction approach an agent builds a geometric model\nRECONSTRUCTION\nof the world from an image or a set of images.\nThe last thirty years of research have produced powerful tools and methods for ad-\ndressing these approaches. Understanding these methods requires an understanding of the\nprocesses by which images are formed. Therefore, we now cover the physical and statistical\nphenomena that occur in the production of an image.\n24.1\nIMAGE FORMATION\nImaging distorts the appearance of objects. For example, a picture taken looking down a\nlong straight set of railway tracks will suggest that the rails converge and meet. As another\nexample, if you hold your hand in front of your eye, you can block out the moon, which is\nnot smaller than your hand. As you move your hand back and forth or tilt it, your hand will\nseem to shrink and grow in the image, but it is not doing so in reality (Figure 24.1). Models\nof these effects are essential for both recognition and reconstruction.\n24.1.1\nImages without lenses: The pinhole camera",
  "seem to shrink and grow in the image, but it is not doing so in reality (Figure 24.1). Models\nof these effects are essential for both recognition and reconstruction.\n24.1.1\nImages without lenses: The pinhole camera\nImage sensors gather light scattered from objects in a scene and create a two-dimensional\nSCENE\nimage. In the eye, the image is formed on the retina, which consists of two types of cells:\nIMAGE\nabout 100 million rods, which are sensitive to light at a wide range of wavelengths, and 5 930\nChapter\n24.\nPerception\nFigure 24.1\nImaging distorts geometry. Parallel lines appear to meet in the distance, as\nin the image of the railway tracks on the left. In the center, a small hand blocks out most of\na large moon. On the right is a foreshortening effect: the hand is tilted away from the eye,\nmaking it appear shorter than in the center ﬁgure.\nmillion cones. Cones, which are essential for color vision, are of three main types, each of\nwhich is sensitive to a different set of wavelengths. In cameras, the image is formed on an\nimage plane, which can be a piece of ﬁlm coated with silver halides or a rectangular grid\nof a few million photosensitive pixels, each a complementary metal-oxide semiconductor\nPIXEL\n(CMOS) or charge-coupled device (CCD). Each photon arriving at the sensor produces an\neffect, whose strength depends on the wavelength of the photon. The output of the sensor\nis the sum of all effects due to photons observed in some time window, meaning that image\nsensors report a weighted average of the intensity of light arriving at the sensor.\nTo see a focused image, we must ensure that all the photons from approximately the\nsame spot in the scene arrive at approximately the same point in the image plane. The simplest\nway to form a focused image is to view stationary objects with a pinhole camera, which\nPINHOLE CAMERA\nconsists of a pinhole opening, O, at the front of a box, and an image plane at the back of the\nbox (Figure 24.2). Photons from the scene must pass through the pinhole, so if it is small\nenough then nearby photons in the scene will be nearby in the image plane, and the image\nwill be in focus.\nThe geometry of scene and image is easiest to understand with the pinhole camera. We\nuse a three-dimensional coordinate system with the origin at the pinhole, and consider a point\nP in the scene, with coordinates (X, Y, Z). P gets projected to the point P ′ in the image",
  "use a three-dimensional coordinate system with the origin at the pinhole, and consider a point\nP in the scene, with coordinates (X, Y, Z). P gets projected to the point P ′ in the image\nplane with coordinates (x, y, z). If f is the distance from the pinhole to the image plane, then\nby similar triangles, we can derive the following equations:\n−x\nf\n= X\nZ , −y\nf\n= Y\nZ\n⇒\nx = −fX\nZ\n, y = −fY\nZ\n.\nThese equations deﬁne an image-formation process known as perspective projection. Note\nPERSPECTIVE\nPROJECTION\nthat the Z in the denominator means that the farther away an object is, the smaller its image Section 24.1.\nImage Formation\n931\nf\nImage \nplane\nP′\nY\nX\nZ\nP\nPinhole\nFigure 24.2\nEach light-sensitive element in the image plane at the back of a pinhole cam-\nera receives light from a the small range of directions that passes through the pinhole. If the\npinhole is small enough, the result is a focused image at the back of the pinhole. The process\nof projection means that large, distant objects look the same as smaller, nearby objects. Note\nthat the image is projected upside down.\nwill be. Also, note that the minus signs mean that the image is inverted, both left–right and\nup–down, compared with the scene.\nUnder perspective projection, distant objects look small. This is what allows you to\ncover the moon with your hand (Figure 24.1). An important result of this effect is that parallel\nlines converge to a point on the horizon. (Think of railway tracks, Figure 24.1.) A line in the\nscene in the direction (U, V, W) and passing through the point (X0, Y0, Z0) can be described\nas the set of points (X0 + λU, Y0 + λV, Z0 + λW), with λ varying between −∞and +∞.\nDifferent choices of (X0, Y0, Z0) yield different lines parallel to one another. The projection\nof a point Pλ from this line onto the image plane is given by\n\r\nf X0 + λU\nZ0 + λW , f Y0 + λV\nZ0 + λW\n\u000e\n.\nAs λ →∞or λ →−∞, this becomes p∞= (fU/W, fV/W) if W ̸= 0. This means that\ntwo parallel lines leaving different points in space will converge in the image—for large λ,\nthe image points are nearly the same, whatever the value of (X0, Y0, Z0) (again, think railway\ntracks, Figure 24.1). We call p∞the vanishing point associated with the family of straight\nVANISHING POINT\nlines with direction (U, V, W). Lines with the same direction share the same vanishing point.\n24.1.2\nLens systems\nThe drawback of the pinhole camera is that we need a small pinhole to keep the image in",
  "VANISHING POINT\nlines with direction (U, V, W). Lines with the same direction share the same vanishing point.\n24.1.2\nLens systems\nThe drawback of the pinhole camera is that we need a small pinhole to keep the image in\nfocus. But the smaller the pinhole, the fewer photons get through, meaning the image will be\ndark. We can gather more photons by keeping the pinhole open longer, but then we will get\nmotion blur—objects in the scene that move will appear blurred because they send photons\nMOTION BLUR\nto multiple locations on the image plane. If we can’t keep the pinhole open longer, we can\ntry to make it bigger. More light will enter, but light from a small patch of object in the scene\nwill now be spread over a patch on the image plane, causing a blurred image. 932\nChapter\n24.\nPerception\nIris\nCornea\nFovea\nVisual Axis\nOptical Axis\nLens\nRetina\nOptic Nerve\nLens\nSystem\nImage plane\nLight Source\nFigure 24.3\nLenses collect the light leaving a scene point in a range of directions, and steer\nit all to arrive at a single point on the image plane. Focusing works for points lying close to\na focal plane in space; other points will not be focused properly. In cameras, elements of\nthe lens system move to change the focal plane, whereas in the eye, the shape of the lens is\nchanged by specialized muscles.\nVertebrate eyes and modern cameras use a lens system to gather sufﬁcient light while\nLENS\nkeeping the image in focus. A large opening is covered with a lens that focuses light from\nnearby object locations down to nearby locations in the image plane. However, lens systems\nhave a limited depth of ﬁeld: they can focus light only from points that lie within a range\nDEPTH OF FIELD\nof depths (centered around a focal plane). Objects outside this range will be out of focus in\nFOCAL PLANE\nthe image. To move the focal plane, the lens in the eye can change shape (Figure 24.3); in a\ncamera, the lenses move back and forth.\n24.1.3\nScaled orthographic projection\nPerspective effects aren’t always pronounced. For example, spots on a distant leopard may\nlook small because the leopard is far away, but two spots that are next to each other will have\nabout the same size. This is because the difference in distance to the spots is small compared\nto the distance to them, and so we can simplify the projection model. The appropriate model\nis scaled orthographic projection. The idea is as follows: If the depth Z of points on the\nSCALED\nORTHOGRAPHIC\nPROJECTION",
  "to the distance to them, and so we can simplify the projection model. The appropriate model\nis scaled orthographic projection. The idea is as follows: If the depth Z of points on the\nSCALED\nORTHOGRAPHIC\nPROJECTION\nobject varies within some range Z0 ± ΔZ, with ΔZ ≪Z0, then the perspective scaling\nfactor f/Z can be approximated by a constant s = f/Z0. The equations for projection from\nthe scene coordinates (X, Y, Z) to the image plane become x = sX and y = sY . Scaled\northographic projection is an approximation that is valid only for those parts of the scene with\nnot much internal depth variation. For example, scaled orthographic projection can be a good\nmodel for the features on the front of a distant building.\n24.1.4\nLight and shading\nThe brightness of a pixel in the image is a function of the brightness of the surface patch in\nthe scene that projects to the pixel. We will assume a linear model (current cameras have non-\nlinearities at the extremes of light and dark, but are linear in the middle). Image brightness is Section 24.1.\nImage Formation\n933\nSpecularities\nCast shadow\nDiffuse reflection, bright\nDiffuse reflection, dark\nFigure 24.4\nA variety of illumination effects. There are specularities on the metal spoon\nand on the milk. The bright diffuse surface is bright because it faces the light direction. The\ndark diffuse surface is dark because it is tangential to the illumination direction. The shadows\nappear at surface points that cannot see the light source. Photo by Mike Linksvayer (mlinksva\non ﬂickr).\na strong, if ambiguous, cue to the shape of an object, and from there to its identity. People are\nusually able to distinguish the three main causes of varying brightness and reverse-engineer\nthe object’s properties. The ﬁrst cause is overall intensity of the light. Even though a white\nOVERALL INTENSITY\nobject in shadow may be less bright than a black object in direct sunlight, the eye can distin-\nguish relative brightness well, and perceive the white object as white. Second, different points\nin the scene may reﬂect more or less of the light. Usually, the result is that people perceive\nREFLECT\nthese points as lighter or darker, and so see texture or markings on the object. Third, surface\npatches facing the light are brighter than surface patches tilted away from the light, an effect\nknown as shading. Typically, people can tell that this shading comes from the geometry of\nSHADING",
  "patches facing the light are brighter than surface patches tilted away from the light, an effect\nknown as shading. Typically, people can tell that this shading comes from the geometry of\nSHADING\nthe object, but sometimes get shading and markings mixed up. For example, a streak of dark\nmakeup under a cheekbone will often look like a shading effect, making the face look thinner.\nMost surfaces reﬂect light by a process of diffuse reﬂection. Diffuse reﬂection scat-\nDIFFUSE\nREFLECTION\nters light evenly across the directions leaving a surface, so the brightness of a diffuse surface\ndoesn’t depend on the viewing direction. Most cloth, paints, rough wooden surfaces, vegeta-\ntion, and rough stone are diffuse. Mirrors are not diffuse, because what you see depends on\nthe direction in which you look at the mirror. The behavior of a perfect mirror is known as\nspecular reﬂection. Some surfaces—such as brushed metal, plastic, or a wet ﬂoor—display\nSPECULAR\nREFLECTION\nsmall patches where specular reﬂection has occurred, called specularities. These are easy to\nSPECULARITIES\nidentify, because they are small and bright (Figure 24.4). For almost all purposes, it is enough\nto model all surfaces as being diffuse with specularities. 934\nChapter\n24.\nPerception\nA\nB\nθ\nθ\nFigure 24.5\nTwo surface patches are illuminated by a distant point source, whose rays are\nshown as gray arrowheads. Patch A is tilted away from the source (θ is close to 900) and\ncollects less energy, because it cuts fewer light rays per unit surface area. Patch B, facing the\nsource (θ is close to 00), collects more energy.\nThe main source of illumination outside is the sun, whose rays all travel parallel to one\nanother. We model this behavior as a distant point light source. This is the most important\nDISTANT POINT\nLIGHT SOURCE\nmodel of lighting, and is quite effective for indoor scenes as well as outdoor scenes. The\namount of light collected by a surface patch in this model depends on the angle θ between the\nillumination direction and the normal to the surface.\nA diffuse surface patch illuminated by a distant point light source will reﬂect some\nfraction of the light it collects; this fraction is called the diffuse albedo. White paper and\nDIFFUSE ALBEDO\nsnow have a high albedo, about 0.90, whereas ﬂat black velvet and charcoal have a low albedo\nof about 0.05 (which means that 95% of the incoming light is absorbed within the ﬁbers of",
  "DIFFUSE ALBEDO\nsnow have a high albedo, about 0.90, whereas ﬂat black velvet and charcoal have a low albedo\nof about 0.05 (which means that 95% of the incoming light is absorbed within the ﬁbers of\nthe velvet or the pores of the charcoal). Lambert’s cosine law states that the brightness of a\nLAMBERT’S COSINE\nLAW\ndiffuse patch is given by\nI = ρI0 cos θ ,\nwhere ρ is the diffuse albedo, I0 is the intensity of the light source and θ is the angle between\nthe light source direction and the surface normal (see Figure 24.5). Lampert’s law predicts\nbright image pixels come from surface patches that face the light directly and dark pixels\ncome from patches that see the light only tangentially, so that the shading on a surface pro-\nvides some shape information. We explore this cue in Section 24.4.5. If the surface is not\nreached by the light source, then it is in shadow. Shadows are very seldom a uniform black,\nSHADOW\nbecause the shadowed surface receives some light from other sources. Outdoors, the most\nimportant such source is the sky, which is quite bright. Indoors, light reﬂected from other\nsurfaces illuminates shadowed patches. These interreﬂections can have a signiﬁcant effect\nINTERREFLECTIONS\non the brightness of other surfaces, too. These effects are sometimes modeled by adding a\nconstant ambient illumination term to the predicted intensity.\nAMBIENT\nILLUMINATION Section 24.2.\nEarly Image-Processing Operations\n935\n24.1.5\nColor\nFruit is a bribe that a tree offers to animals to carry its seeds around. Trees have evolved to\nhave fruit that turns red or yellow when ripe, and animals have evolved to detect these color\nchanges. Light arriving at the eye has different amounts of energy at different wavelengths;\nthis can be represented by a spectral energy density function. Human eyes respond to light in\nthe 380–750nm wavelength region, with three different types of color receptor cells, which\nhave peak receptiveness at 420mm (blue), 540nm (green), and 570nm (red). The human eye\ncan capture only a small fraction of the full spectral energy density function—but it is enough\nto tell when the fruit is ripe.\nThe principle of trichromacy states that for any spectral energy density, no matter how\nPRINCIPLE OF\nTRICHROMACY\ncomplicated, it is possible to construct another spectral energy density consisting of a mixture\nof just three colors—usually red, green, and blue—such that a human can’t tell the difference",
  "PRINCIPLE OF\nTRICHROMACY\ncomplicated, it is possible to construct another spectral energy density consisting of a mixture\nof just three colors—usually red, green, and blue—such that a human can’t tell the difference\nbetween the two. That means that our TVs and computer displays can get by with just the\nthree red/green/blue (or R/G/B) color elements. It makes our computer vision algorithms\neasier, too. Each surface can be modeled with three different albedos for R/G/B. Similarly,\neach light source can be modeled with three R/G/B intensities. We then apply Lambert’s\ncosine law to each to get three R/G/B pixel values. This model predicts, correctly, that the\nsame surface will produce different colored image patches under different-colored lights. In\nfact, human observers are quite good at ignoring the effects of different colored lights and are\nable to estimate the color of the surface under white light, an effect known as color constancy.\nCOLOR CONSTANCY\nQuite accurate color constancy algorithms are now available; simple versions show up in the\n“auto white balance” function of your camera. Note that if we wanted to build a camera for\nmantis shrimp, we would need 12 different pixel colors, corresponding to the 12 types of\ncolor receptors of the crustacean.\n24.2\nEARLY IMAGE-PROCESSING OPERATIONS\nWe have seen how light reﬂects off objects in the scene to form an image consisting of, say,\nﬁve million 3-byte pixels. With all sensors there will be noise in the image, and in any case\nthere is a lot of data to deal with. So how do we get started on analyzing this data?\nIn this section we will study three useful image-processing operations: edge detection,\ntexture analysis, and computation of optical ﬂow. These are called “early” or “low-level”\noperations because they are the ﬁrst in a pipeline of operations. Early vision operations are\ncharacterized by their local nature (they can be carried out in one part of the image without\nregard for anything more than a few pixels away) and by their lack of knowledge: we can\nperform these operations without consideration of the objects that might be present in the\nscene. This makes the low-level operations good candidates for implementation in parallel\nhardware—either in a graphics processor unit (GPU) or an eye. We will then look at one\nmid-level operation: segmenting the image into regions. 936\nChapter\n24.\nPerception\nA\nB\n1\n2\n4\n2\n1\n1\n3\nFigure 24.6\nDifferent kinds of edges: (1) depth discontinuities; (2) surface orientation",
  "mid-level operation: segmenting the image into regions. 936\nChapter\n24.\nPerception\nA\nB\n1\n2\n4\n2\n1\n1\n3\nFigure 24.6\nDifferent kinds of edges: (1) depth discontinuities; (2) surface orientation\ndiscontinuities; (3) reﬂectance discontinuities; (4) illumination discontinuities (shadows).\n24.2.1\nEdge detection\nEdges are straight lines or curves in the image plane across which there is a “signiﬁcant”\nEDGE\nchange in image brightness. The goal of edge detection is to abstract away from the messy,\nmultimegabyte image and toward a more compact, abstract representation, as in Figure 24.6.\nThe motivation is that edge contours in the image correspond to important scene contours.\nIn the ﬁgure we have three examples of depth discontinuity, labeled 1; two surface-normal\ndiscontinuities, labeled 2; a reﬂectance discontinuity, labeled 3; and an illumination discon-\ntinuity (shadow), labeled 4. Edge detection is concerned only with the image, and thus does\nnot distinguish between these different types of scene discontinuities; later processing will.\nFigure 24.7(a) shows an image of a scene containing a stapler resting on a desk, and\n(b) shows the output of an edge-detection algorithm on this image. As you can see, there\nis a difference between the output and an ideal line drawing. There are gaps where no edge\nappears, and there are “noise” edges that do not correspond to anything of signiﬁcance in the\nscene. Later stages of processing will have to correct for these errors.\nHow do we detect edges in an image? Consider the proﬁle of image brightness along a\none-dimensional cross-section perpendicular to an edge—for example, the one between the\nleft edge of the desk and the wall. It looks something like what is shown in Figure 24.8 (top).\nEdges correspond to locations in images where the brightness undergoes a sharp change,\nso a naive idea would be to differentiate the image and look for places where the magnitude\nof the derivative I′(x) is large. That almost works. In Figure 24.8 (middle), we see that there\nis indeed a peak at x = 50, but there are also subsidiary peaks at other locations (e.g., x = 75).\nThese arise because of the presence of noise in the image. If we smooth the image ﬁrst, the\nspurious peaks are diminished, as we see in the bottom of the ﬁgure. Section 24.2.\nEarly Image-Processing Operations\n937\n(a)\n(b)\nFigure 24.7\n(a) Photograph of a stapler. (b) Edges computed from (a).\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n−1\n0\n1\n2\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n−1\n0\n1\n0\n10\n20",
  "Early Image-Processing Operations\n937\n(a)\n(b)\nFigure 24.7\n(a) Photograph of a stapler. (b) Edges computed from (a).\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n−1\n0\n1\n2\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n−1\n0\n1\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n−1\n0\n1\nFigure 24.8\nTop: Intensity proﬁle I(x) along a one-dimensional section across an edge at\nx = 50. Middle: The derivative of intensity, I′(x). Large values of this function correspond\nto edges, but the function is noisy. Bottom: The derivative of a smoothed version of the\nintensity, (I ∗Gσ)′, which can be computed in one step as the convolution I ∗G′\nσ. The noisy\ncandidate edge at x = 75 has disappeared.\nThe measurement of brightness at a pixel in a CCD camera is based on a physical\nprocess involving the absorption of photons and the release of electrons; inevitably there\nwill be statistical ﬂuctuations of the measurement—noise. The noise can be modeled with 938\nChapter\n24.\nPerception\na Gaussian probability distribution, with each pixel independent of the others. One way to\nsmooth an image is to assign to each pixel the average of its neighbors. This tends to cancel\nout extreme values. But how many neighbors should we consider—one pixel away, or two, or\nmore? One good answer is a weighted average that weights the nearest pixels the most, then\ngradually decreases the weight for more distant pixels. The Gaussian ﬁlter does just that.\nGAUSSIAN FILTER\n(Users of Photoshop recognize this as the Gaussian blur operation.) Recall that the Gaussian\nfunction with standard deviation σ and mean 0 is\nNσ(x) =\n1\n√\n2πσe−x2/2σ2\nin one dimension, or\nNσ(x, y) =\n1\n2πσ2 e−(x2+y2)/2σ2\nin two dimensions.\nThe application of the Gaussian ﬁlter replaces the intensity I(x0, y0) with the sum, over all\n(x, y) pixels, of I(x, y) Nσ(d), where d is the distance from (x0, y0) to (x, y). This kind of\nweighted sum is so common that there is a special name and notation for it. We say that the\nfunction h is the convolution of two functions f and g (denoted f ∗g) if we have\nCONVOLUTION\nh(x) = (f ∗g)(x) =\n+∞\n\f\nu=−∞\nf(u) g(x −u)\nin one dimension, or\nh(x, y) = (f ∗g)(x, y) =\n+∞\n\f\nu=−∞\n+∞\n\f\nv=−∞\nf(u, v) g(x −u, y −v)\nin two.\nSo the smoothing function is achieved by convolving the image with the Gaussian, I ∗Nσ. A\nσ of 1 pixel is enough to smooth over a small amount of noise, whereas 2 pixels will smooth a\nlarger amount, but at the loss of some detail. Because the Gaussian’s inﬂuence fades quickly\nat a distance, we can replace the ±∞in the sums with ±3σ.",
  "σ of 1 pixel is enough to smooth over a small amount of noise, whereas 2 pixels will smooth a\nlarger amount, but at the loss of some detail. Because the Gaussian’s inﬂuence fades quickly\nat a distance, we can replace the ±∞in the sums with ±3σ.\nWe can optimize the computation by combining smoothing and edge ﬁnding into a sin-\ngle operation. It is a theorem that for any functions f and g, the derivative of the convolution,\n(f ∗g)′, is equal to the convolution with the derivative, f ∗(g′). So rather than smoothing\nthe image and then differentiating, we can just convolve the image with the derivative of the\nsmoothing function, N ′\nσ. We then mark as edges those peaks in the response that are above\nsome threshold.\nThere is a natural generalization of this algorithm from one-dimensional cross sections\nto general two-dimensional images. In two dimensions edges may be at any angle θ. Consid-\nering the image brightness as a scalar function of the variables x, y, its gradient is a vector\n∇I =\n\u001f ∂I\n∂x\n∂I\n∂y\n \n=\n\r Ix\nIy\n\u000e\n.\nEdges correspond to locations in images where the brightness undergoes a sharp change, and\nso the magnitude of the gradient, ∥∇I∥, should be large at an edge point. Of independent\ninterest is the direction of the gradient\n∇I\n∥∇I∥=\n\r cos θ\nsin θ\n\u000e\n.\nThis gives us a θ = θ(x, y) at every pixel, which deﬁnes the edge orientation at that pixel.\nORIENTATION Section 24.2.\nEarly Image-Processing Operations\n939\nAs in one dimension, to form the gradient we don’t compute ∇I, but rather ∇(I ∗Nσ),\nthe gradient after smoothing the image by convolving it with a Gaussian. And again, the\nshortcut is that this is equivalent to convolving the image with the partial derivatives of a\nGaussian. Once we have computed the gradient, we can obtain edges by ﬁnding edge points\nand linking them together. To tell whether a point is an edge point, we must look at other\npoints a small distance forward and back along the direction of the gradient. If the gradient\nmagnitude at one of these points is larger, then we could get a better edge point by shifting\nthe edge curve very slightly. Furthermore, if the gradient magnitude is too small, the point\ncannot be an edge point. So at an edge point, the gradient magnitude is a local maximum\nalong the direction of the gradient, and the gradient magnitude is above a suitable threshold.\nOnce we have marked edge pixels by this algorithm, the next stage is to link those pixels",
  "along the direction of the gradient, and the gradient magnitude is above a suitable threshold.\nOnce we have marked edge pixels by this algorithm, the next stage is to link those pixels\nthat belong to the same edge curves. This can be done by assuming that any two neighboring\nedge pixels with consistent orientations must belong to the same edge curve.\n24.2.2\nTexture\nIn everyday language, texture is the visual feel of a surface—what you see evokes what\nTEXTURE\nthe surface might feel like if you touched it (“texture” has the same root as “textile”). In\ncomputational vision, texture refers to a spatially repeating pattern on a surface that can be\nsensed visually. Examples include the pattern of windows on a building, stitches on a sweater,\nspots on a leopard, blades of grass on a lawn, pebbles on a beach, and people in a stadium.\nSometimes the arrangement is quite periodic, as in the stitches on a sweater; in other cases,\nsuch as pebbles on a beach, the regularity is only statistical.\nWhereas brightness is a property of individual pixels, the concept of texture makes sense\nonly for a multipixel patch. Given such a patch, we could compute the orientation at each\npixel, and then characterize the patch by a histogram of orientations. The texture of bricks in\na wall would have two peaks in the histogram (one vertical and one horizontal), whereas the\ntexture of spots on a leopard’s skin would have a more uniform distribution of orientations.\nFigure 24.9 shows that orientations are largely invariant to changes in illumination. This\nmakes texture an important clue for object recognition, because other clues, such as edges,\ncan yield different results in different lighting conditions.\nIn images of textured objects, edge detection does not work as well as it does for smooth\nobjects. This is because the most important edges can be lost among the texture elements.\nQuite literally, we may miss the tiger for the stripes. The solution is to look for differences in\ntexture properties, just the way we look for differences in brightness. A patch on a tiger and\na patch on the grassy background will have very different orientation histograms, allowing us\nto ﬁnd the boundary curve between them.\n24.2.3\nOptical ﬂow\nNext, let us consider what happens when we have a video sequence, instead of just a single\nstatic image. When an object in the video is moving, or when the camera is moving relative",
  "to ﬁnd the boundary curve between them.\n24.2.3\nOptical ﬂow\nNext, let us consider what happens when we have a video sequence, instead of just a single\nstatic image. When an object in the video is moving, or when the camera is moving relative\nto an object, the resulting apparent motion in the image is called optical ﬂow. Optical ﬂow\nOPTICAL FLOW\ndescribes the direction and speed of motion of features in the image—the optical ﬂow of a 940\nChapter\n24.\nPerception\n(a)\n(b)\nFigure 24.9\nTwo images of the same texture of crumpled rice paper, with different illumi-\nnation levels. The gradient vector ﬁeld (at every eighth pixel) is plotted on top of each one.\nNotice that, as the light gets darker, all the gradient vectors get shorter. The vectors do not\nrotate, so the gradient orientations do not change.\nvideo of a race car would be measured in pixels per second, not miles per hour. The optical\nﬂow encodes useful information about scene structure. For example, in a video of scenery\ntaken from a moving train, distant objects have slower apparent motion than close objects;\nthus, the rate of apparent motion can tell us something about distance. Optical ﬂow also\nenables us to recognize actions. In Figure 24.10(a) and (b), we show two frames from a video\nof a tennis player. In (c) we display the optical ﬂow vectors computed from these images,\nshowing that the racket and front leg are moving fastest.\nThe optical ﬂow vector ﬁeld can be represented at any point (x, y) by its components\nvx(x, y) in the x direction and vy(x, y) in the y direction. To measure optical ﬂow we need to\nﬁnd corresponding points between one time frame and the next. A simple-minded technique\nis based on the fact that image patches around corresponding points have similar intensity\npatterns. Consider a block of pixels centered at pixel p, (x0, y0), at time t0. This block\nof pixels is to be compared with pixel blocks centered at various candidate pixels at (x0 +\nDx, y0 + Dy) at time t0 + Dt. One possible measure of similarity is the sum of squared\ndifferences (SSD):\nSUM OF SQUARED\nDIFFERENCES\nSSD(Dx, Dy) =\n\f\n(x,y)\n(I(x, y, t) −I(x + Dx, y + Dy, t + Dt))2 .\nHere, (x, y) ranges over pixels in the block centered at (x0, y0). We ﬁnd the (Dx, Dy) that\nminimizes the SSD. The optical ﬂow at (x0, y0) is then (vx, vy) = (Dx/Dt, Dy/Dt). Note\nthat for this to work, there needs to be some texture or variation in the scene. If one is looking",
  "minimizes the SSD. The optical ﬂow at (x0, y0) is then (vx, vy) = (Dx/Dt, Dy/Dt). Note\nthat for this to work, there needs to be some texture or variation in the scene. If one is looking\nat a uniform white wall, then the SSD is going to be nearly the same for the different can- Section 24.2.\nEarly Image-Processing Operations\n941\nFigure 24.10\nTwo frames of a video sequence. On the right is the optical ﬂow ﬁeld cor-\nresponding to the displacement from one frame to the other. Note how the movement of\nthe tennis racket and the front leg is captured by the directions of the arrows. (Courtesy of\nThomas Brox.)\ndidate matches, and the algorithm is reduced to making a blind guess. The best-performing\nalgorithms for measuring optical ﬂow rely on a variety of additional constraints when the\nscene is only partially textured.\n24.2.4\nSegmentation of images\nSegmentation is the process of breaking an image into regions of similar pixels. Each image\nSEGMENTATION\nREGIONS\npixel can be associated with certain visual properties, such as brightness, color, and texture.\nWithin an object, or a single part of an object, these attributes vary relatively little, whereas\nacross an inter-object boundary there is typically a large change in one or more of these at-\ntributes. There are two approaches to segmentation, one focusing on detecting the boundaries\nof these regions, and the other on detecting the regions themselves (Figure 24.11).\nA boundary curve passing through a pixel (x, y) will have an orientation θ, so one way\nto formalize the problem of detecting boundary curves is as a machine learning classiﬁcation\nproblem. Based on features from a local neighborhood, we want to compute the probability\nPb(x, y, θ) that indeed there is a boundary curve at that pixel along that orientation. Consider\na circular disk centered at (x, y), subdivided into two half disks by a diameter oriented at θ.\nIf there is a boundary at (x, y, θ) the two half disks might be expected to differ signiﬁcantly\nin their brightness, color, and texture. Martin, Fowlkes, and Malik (2004) used features based\non differences in histograms of brightness, color, and texture values measured in these two\nhalf disks, and then trained a classiﬁer. For this they used a data set of natural images where\nhumans had marked the “ground truth” boundaries, and the goal of the classiﬁer was to mark\nexactly those boundaries marked by humans and no others.",
  "half disks, and then trained a classiﬁer. For this they used a data set of natural images where\nhumans had marked the “ground truth” boundaries, and the goal of the classiﬁer was to mark\nexactly those boundaries marked by humans and no others.\nBoundaries detected by this technique turn out to be signiﬁcantly better than those found\nusing the simple edge-detection technique described previously. But still there are two limita-\ntions. (1) The boundary pixels formed by thresholding Pb(x, y, θ) are not guaranteed to form\nclosed curves, so this approach doesn’t deliver regions, and (2) the decision making exploits\nonly local context and does not use global consistency constraints. 942\nChapter\n24.\nPerception\n(a)\n(b)\n(c)\n(d)\nFigure 24.11\n(a) Original image. (b) Boundary contours, where the higher the Pb value,\nthe darker the contour. (c) Segmentation into regions, corresponding to a ﬁne partition of\nthe image. Regions are rendered in their mean colors. (d) Segmentation into regions, corre-\nsponding to a coarser partition of the image, resulting in fewer regions. (Courtesy of Pablo\nArbelaez, Michael Maire, Charles Fowlkes, and Jitendra Malik)\nThe alternative approach is based on trying to “cluster” the pixels into regions based on\ntheir brightness, color, and texture. Shi and Malik (2000) set this up as a graph partitioning\nproblem. The nodes of the graph correspond to pixels, and edges to connections between\npixels. The weight Wij on the edge connecting a pair of pixels i and j is based on how similar\nthe two pixels are in brightness, color, texture, etc. Partitions that minimize a normalized cut\ncriterion are then found. Roughly speaking, the criterion for partitioning the graph is to\nminimize the sum of weights of connections across the groups of pixels and maximize the\nsum of weights of connections within the groups.\nSegmentation based purely on low-level, local attributes such as brightness and color\ncannot be expected to deliver the ﬁnal correct boundaries of all the objects in the scene. To\nreliably ﬁnd object boundaries we need high-level knowledge of the likely kinds of objects\nin the scene. Representing this knowledge is a topic of active research. A popular strategy is\nto produce an over-segmentation of an image, containing hundreds of homogeneous regions\nknown as superpixels. From there, knowledge-based algorithms can take over; they will\nSUPERPIXELS\nﬁnd it easier to deal with hundreds of superpixels rather than millions of raw pixels. How to",
  "known as superpixels. From there, knowledge-based algorithms can take over; they will\nSUPERPIXELS\nﬁnd it easier to deal with hundreds of superpixels rather than millions of raw pixels. How to\nexploit high-level knowledge of objects is the subject of the next section.\n24.3\nOBJECT RECOGNITION BY APPEARANCE\nAppearance is shorthand for what an object tends to look like. Some object categories—for\nAPPEARANCE\nexample, baseballs—vary rather little in appearance; all of the objects in the category look\nabout the same under most circumstances. In this case, we can compute a set of features\ndescribing each class of images likely to contain the object, then test it with a classiﬁer. Section 24.3.\nObject Recognition by Appearance\n943\nOther object categories—for example, houses or ballet dancers—vary greatly. A house\ncan have different size, color, and shape and can look different from different angles. A dancer\nlooks different in each pose, or when the stage lights change colors. A useful abstraction is to\nsay that some objects are made up of local patterns which tend to move around with respect to\none another. We can then ﬁnd the object by looking at local histograms of detector responses,\nwhich expose whether some part is present but suppress the details of where it is.\nTesting each class of images with a learned classiﬁer is an important general recipe.\nIt works extremely well for faces looking directly at the camera, because at low resolution\nand under reasonable lighting, all such faces look quite similar. The face is round, and quite\nbright compared to the eye sockets; these are dark, because they are sunken, and the mouth is\na dark slash, as are the eyebrows. Major changes of illumination can cause some variations in\nthis pattern, but the range of variation is quite manageable. That makes it possible to detect\nface positions in an image that contains faces. Once a computational challenge, this feature\nis now commonplace in even inexpensive digital cameras.\nFor the moment, we will consider only faces where the nose is oriented vertically; we\nwill deal with rotated faces below. We sweep a round window of ﬁxed size over the image,\ncompute features for it, and present the features to a classiﬁer. This strategy is sometimes\ncalled the sliding window. Features need to be robust to shadows and to changes in brightness\nSLIDING WINDOW\ncaused by illumination changes. One strategy is to build features out of gradient orientations.",
  "called the sliding window. Features need to be robust to shadows and to changes in brightness\nSLIDING WINDOW\ncaused by illumination changes. One strategy is to build features out of gradient orientations.\nAnother is to estimate and correct the illumination in each image window. To ﬁnd faces of\ndifferent sizes, repeat the sweep over larger or smaller versions of the image. Finally, we\npostprocess the responses across scales and locations to produce the ﬁnal set of detections.\nPostprocessing is important, because it is unlikely that we have chosen a window size\nthat is exactly the right size for a face (even if we use multiple sizes). Thus, we will likely\nhave several overlapping windows that each report a match for a face. However, if we use\na classiﬁer that can report strength of response (for example, logistic regression or a support\nvector machine) we can combine these partial overlapping matches at nearby locations to\nyield a single high-quality match. That gives us a face detector that can search over locations\nand scales. To search rotations as well, we use two steps. We train a regression procedure\nto estimate the best orientation of any face present in a window. Now, for each window, we\nestimate the orientation, reorient the window, then test whether a vertical face is present with\nour classiﬁer. All this yields a system whose architecture is sketched in Figure 24.12.\nTraining data is quite easily obtained. There are several data sets of marked-up face\nimages, and rotated face windows are easy to build (just rotate a window from a training\ndata set). One trick that is widely used is to take each example window, then produce new\nexamples by changing the orientation of the window, the center of the window, or the scale\nvery slightly. This is an easy way of getting a bigger data set that reﬂects real images fairly\nwell; the trick usually improves performance signiﬁcantly. Face detectors built along these\nlines now perform very well for frontal faces (side views are harder). 944\nChapter\n24.\nPerception\nImage\nResponses\nDetections\nNon-maximal \nsuppresion\nCorrect\nillumination\nEstimate\norientation\nRotate\nwindow\nFeatures\nClassifier\nFigure 24.12\nFace ﬁnding systems vary, but most follow the architecture illustrated in\ntwo parts here. On the top, we go from images to responses, then apply non-maximum\nsuppression to ﬁnd the strongest local response. The responses are obtained by the process",
  "Face ﬁnding systems vary, but most follow the architecture illustrated in\ntwo parts here. On the top, we go from images to responses, then apply non-maximum\nsuppression to ﬁnd the strongest local response. The responses are obtained by the process\nillustrated on the bottom. We sweep a window of ﬁxed size over larger and smaller versions\nof the image, so as to ﬁnd smaller or larger faces, respectively. The illumination in the\nwindow is corrected, and then a regression engine (quite often, a neural net) predicts the\norientation of the face. The window is corrected to this orientation and then presented to a\nclassiﬁer. Classiﬁer outputs are then postprocessed to ensure that only one face is placed at\neach location in the image.\n24.3.1\nComplex appearance and pattern elements\nMany objects produce much more complex patterns than faces do. This is because several\neffects can move features around in an image of the object. Effects include (Figure 24.13)\n• Foreshortening, which causes a pattern viewed at a slant to be signiﬁcantly distorted.\n• Aspect, which causes objects to look different when seen from different directions.\nEven as simple an object as a doughnut has several aspects; seen from the side, it looks\nlike a ﬂattened oval, but from above it is an annulus.\n• Occlusion, where some parts are hidden from some viewing directions. Objects can\nocclude one another, or parts of an object can occlude other parts, an effect known as\nself-occlusion.\n• Deformation, where internal degrees of freedom of the object change its appearance.\nFor example, people can move their arms and legs around, generating a very wide range\nof different body conﬁgurations.\nHowever, our recipe of searching across location and scale can still work. This is because\nsome structure will be present in the images produced by the object. For example, a picture\nof a car is likely to show some of headlights, doors, wheels, windows, and hubcaps, though\nthey may be in somewhat different arrangements in different pictures. This suggests modeling\nobjects with pattern elements—collections of parts. These pattern elements may move around Section 24.3.\nObject Recognition by Appearance\n945\nForeshortening\nAspect\nOcclusion\nDeformation\nFigure 24.13\nSources of appearance variation. First, elements can foreshorten, like the\ncircular patch on the top left. This patch is viewed at a slant, and so is elliptical in the\nimage. Second, objects viewed from different directions can change shape quite dramatically,",
  "circular patch on the top left. This patch is viewed at a slant, and so is elliptical in the\nimage. Second, objects viewed from different directions can change shape quite dramatically,\na phenomenon known as aspect. On the top right are three different aspects of a doughnut.\nOcclusion causes the handle of the mug on the bottom left to disappear when the mug is\nrotated. In this case, because the body and handle belong to the same mug, we have self-\nocclusion. Finally, on the bottom right, some objects can deform dramatically.\nwith respect to one another, but if most of the pattern elements are present in about the right\nplace, then the object is present. An object recognizer is then a collection of features that can\ntell whether the pattern elements are present, and whether they are in about the right place.\nThe most obvious approach is to represent the image window with a histogram of the\npattern elements that appear there. This approach does not work particularly well, because\ntoo many patterns get confused with one another. For example, if the pattern elements are\ncolor pixels, the French, UK, and Netherlands ﬂags will get confused because they have\napproximately the same color histograms, though the colors are arranged in very different\nways. Quite simple modiﬁcations of histograms yield very useful features. The trick is to\npreserve some spatial detail in the representation; for example, headlights tend to be at the\nfront of a car and wheels tend to be at the bottom. Histogram-based features have been\nsuccessful in a wide variety of recognition applications; we will survey pedestrian detection.\n24.3.2\nPedestrian detection with HOG features\nThe World Bank estimates that each year car accidents kill about 1.2 million people, of whom\nabout two thirds are pedestrians. This means that detecting pedestrians is an important appli-\ncation problem, because cars that can automatically detect and avoid pedestrians might save\nmany lives. Pedestrians wear many different kinds of clothing and appear in many different\nconﬁgurations, but, at relatively low resolution, pedestrians can have a fairly characteristic\nappearance. The most usual cases are lateral or frontal views of a walk. In these cases, 946\nChapter\n24.\nPerception\nImage\nOrientation\nhistograms\nPositive\ncomponents\nNegative\ncomponents\nFigure 24.14\nLocal orientation histograms are a powerful feature for recognizing even",
  "Chapter\n24.\nPerception\nImage\nOrientation\nhistograms\nPositive\ncomponents\nNegative\ncomponents\nFigure 24.14\nLocal orientation histograms are a powerful feature for recognizing even\nquite complex objects. On the left, an image of a pedestrian. On the center left, local orien-\ntation histograms for patches. We then apply a classiﬁer such as a support vector machine\nto ﬁnd the weights for each histogram that best separate the positive examples of pedestrians\nfrom non-pedestrians. We see that the positively weighted components look like the outline\nof a person. The negative components are less clear; they represent all the patterns that are\nnot pedestrians. Figure from Dalal and Triggs (2005) c⃝IEEE.\nwe see either a “lollipop” shape — the torso is wider than the legs, which are together in\nthe stance phase of the walk — or a “scissor” shape — where the legs are swinging in the\nwalk. We expect to see some evidence of arms and legs, and the curve around the shoulders\nand head also tends to visible and quite distinctive. This means that, with a careful feature\nconstruction, we can build a useful moving-window pedestrian detector.\nThere isn’t always a strong contrast between the pedestrian and the background, so it\nis better to use orientations than edges to represent the image window. Pedestrians can move\ntheir arms and legs around, so we should use a histogram to suppress some spatial detail in\nthe feature. We break up the window into cells, which could overlap, and build an orientation\nhistogram in each cell. Doing so will produce a feature that can tell whether the head-and-\nshoulders curve is at the top of the window or at the bottom, but will not change if the head\nmoves slightly.\nOne further trick is required to make a good feature. Because orientation features are\nnot affected by illumination brightness, we cannot treat high-contrast edges specially. This\nmeans that the distinctive curves on the boundary of a pedestrian are treated in the same way\nas ﬁne texture detail in clothing or in the background, and so the signal may be submerged\nin noise. We can recover contrast information by counting gradient orientations with weights\nthat reﬂect how signiﬁcant a gradient is compared to other gradients in the same cell. We\nwill write || ∇Ix || for the gradient magnitude at point x in the image, write C for the cell\nwhose histogram we wish to compute, and write wx,C for the weight that we will use for the Section 24.4.\nReconstructing the 3D World\n947\nFigure 24.15",
  "will write || ∇Ix || for the gradient magnitude at point x in the image, write C for the cell\nwhose histogram we wish to compute, and write wx,C for the weight that we will use for the Section 24.4.\nReconstructing the 3D World\n947\nFigure 24.15\nAnother example of object recognition, this one using the SIFT feature\n(Scale Invariant Feature Transform), an earlier version of the HOG feature. On the left, im-\nages of a shoe and a telephone that serve as object models. In the center, a test image. On the\nright, the shoe and the telephone have been detected by: ﬁnding points in the image whose\nSIFT feature descriptions match a model; computing an estimate of pose of the model; and\nverifying that estimate. A strong match is usually veriﬁed with rare false positives. Images\nfrom Lowe (1999) c⃝IEEE.\norientation at x for this cell. A natural choice of weight is\nwx,C =\n||∇Ix ||\n\u0002\nu∈C ||∇Iu || .\nThis compares the gradient magnitude to others in the cell, so gradients that are large com-\npared to their neighbors get a large weight. The resulting feature is usually called a HOG\nfeature (for Histogram Of Gradient orientations).\nHOG FEATURE\nThis feature construction is the main way in which pedestrian detection differs from\nface detection. Otherwise, building a pedestrian detector is very like building a face detector.\nThe detector sweeps a window across the image, computes features for that window, then\npresents it to a classiﬁer. Non-maximum suppression needs to be applied to the output. In\nmost applications, the scale and orientation of typical pedestrians is known. For example, in\ndriving applications in which a camera is ﬁxed to the car, we expect to view mainly vertical\npedestrians, and we are interested only in nearby pedestrians. Several pedestrian data sets\nhave been published, and these can be used for training the classiﬁer.\nPedestrians are not the only type of object we can detect. In Figure 24.15 we see that\nsimilar techniques can be used to ﬁnd a variety of objects in different contexts.\n24.4\nRECONSTRUCTING THE 3D WORLD\nIn this section we show how to go from the two-dimensional image to a three-dimensional\nrepresentation of the scene. The fundamental question is this: Given that all points in the\nscene that fall along a ray to the pinhole are projected to the same point in the image, how do\nwe recover three-dimensional information? Two ideas come to our rescue: 948\nChapter\n24.\nPerception",
  "scene that fall along a ray to the pinhole are projected to the same point in the image, how do\nwe recover three-dimensional information? Two ideas come to our rescue: 948\nChapter\n24.\nPerception\n• If we have two (or more) images from different camera positions, then we can triangu-\nlate to ﬁnd the position of a point in the scene.\n• We can exploit background knowledge about the physical scene that gave rise to the\nimage. Given an object model P(Scene) and a rendering model P(Image | Scene), we\ncan compute a posterior distribution P(Scene | Image).\nThere is as yet no single uniﬁed theory for scene reconstruction. We survey eight commonly\nused visual cues: motion, binocular stereopsis, multiple views, texture, shading, contour,\nand familiar objects.\n24.4.1\nMotion parallax\nIf the camera moves relative to the three-dimensional scene, the resulting apparent motion in\nthe image, optical ﬂow, can be a source of information for both the movement of the camera\nand depth in the scene. To understand this, we state (without proof) an equation that relates\nthe optical ﬂow to the viewer’s translational velocity T and the depth in the scene.\nThe components of the optical ﬂow ﬁeld are\nvx(x, y) = −Tx + xTz\nZ(x, y)\n,\nvy(x, y) = −Ty + yTz\nZ(x, y)\n,\nwhere Z(x, y) is the z-coordinate of the point in the scene corresponding to the point in the\nimage at (x, y).\nNote that both components of the optical ﬂow, vx(x, y) and vy(x, y), are zero at the\npoint x = Tx/Tz, y = Ty/Tz. This point is called the focus of expansion of the ﬂow\nFOCUS OF\nEXPANSION\nﬁeld. Suppose we change the origin in the x–y plane to lie at the focus of expansion; then\nthe expressions for optical ﬂow take on a particularly simple form. Let (x′, y′) be the new\ncoordinates deﬁned by x′ = x −Tx/Tz, y′ = y −Ty/Tz. Then\nvx(x′, y′) =\nx′Tz\nZ(x′, y′),\nvy(x′, y′) =\ny′Tz\nZ(x′, y′) .\nNote that there is a scale-factor ambiguity here. If the camera was moving twice as fast, and\nevery object in the scene was twice as big and at twice the distance to the camera, the optical\nﬂow ﬁeld would be exactly the same. But we can still extract quite useful information.\n1. Suppose you are a ﬂy trying to land on a wall and you want to know the time-to-\ncontact at the current velocity. This time is given by Z/Tz. Note that although the\ninstantaneous optical ﬂow ﬁeld cannot provide either the distance Z or the velocity\ncomponent Tz, it can provide the ratio of the two and can therefore be used to control",
  "instantaneous optical ﬂow ﬁeld cannot provide either the distance Z or the velocity\ncomponent Tz, it can provide the ratio of the two and can therefore be used to control\nthe landing approach. There is considerable experimental evidence that many different\nanimal species exploit this cue.\n2. Consider two points at depths Z1, Z2, respectively. We may not know the absolute\nvalue of either of these, but by considering the inverse of the ratio of the optical ﬂow\nmagnitudes at these points, we can determine the depth ratio Z1/Z2. This is the cue of\nmotion parallax, one we use when we look out of the side window of a moving car or\ntrain and infer that the slower moving parts of the landscape are farther away. Section 24.4.\nReconstructing the 3D World\n949\nPerceived object\nRight image\n(a)\n(b)\nLeft image\nDisparity\nLeft\nRight\nFigure 24.16\nTranslating a camera parallel to the image plane causes image features to\nmove in the camera plane. The disparity in positions that results is a cue to depth. If we\nsuperimpose left and right image, as in (b), we see the disparity.\n24.4.2\nBinocular stereopsis\nMost vertebrates have two eyes. This is useful for redundancy in case of a lost eye, but it\nhelps in other ways too. Most prey have eyes on the side of the head to enable a wider ﬁeld\nof vision. Predators have the eyes in the front, enabling them to use binocular stereopsis.\nBINOCULAR\nSTEREOPSIS\nThe idea is similar to motion parallax, except that instead of using images over time, we use\ntwo (or more) images separated in space. Because a given feature in the scene will be in a\ndifferent place relative to the z-axis of each image plane, if we superpose the two images,\nthere will be a disparity in the location of the image feature in the two images. You can see\nDISPARITY\nthis in Figure 24.16, where the nearest point of the pyramid is shifted to the left in the right\nimage and to the right in the left image.\nNote that to measure disparity we need to solve the correspondence problem, that is,\ndetermine for a point in the left image, the point in the right image that results from the\nprojection of the same scene point. This is analogous to what one has to do in measuring\noptical ﬂow, and the most simple-minded approaches are somewhat similar and based on\ncomparing blocks of pixels around corresponding points using the sum of squared differences.\nIn practice, we use much more sophisticated algorithms, which exploit additional constraints.",
  "comparing blocks of pixels around corresponding points using the sum of squared differences.\nIn practice, we use much more sophisticated algorithms, which exploit additional constraints.\nAssuming that we can measure disparity, how does this yield information about depth\nin the scene? We will need to work out the geometrical relationship between disparity and\ndepth. First, we will consider the case when both the eyes (or cameras) are looking forward\nwith their optical axes parallel. The relationship of the right camera to the left camera is then\njust a displacement along the x-axis by an amount b, the baseline. We can use the optical\nﬂow equations from the previous section, if we think of this as resulting from a translation 950\nChapter\n24.\nPerception\nb\nδθ/2\nδZ\nP\nP0\nPR\nPL\nLeft\neye\nZ\nRight\neye\nθ\nFigure 24.17\nThe relation between disparity and depth in stereopsis. The centers of pro-\njection of the two eyes are b apart, and the optical axes intersect at the ﬁxation point P0. The\npoint P in the scene projects to points PL and PR in the two eyes. In angular terms, the\ndisparity between these is δθ. See text.\nvector T acting for time δt, with Tx = b/δt and Ty = Tz = 0. The horizontal and vertical\ndisparity are given by the optical ﬂow components, multiplied by the time step δt, H = vx δt,\nV = vy δt. Carrying out the substitutions, we get the result that H = b/Z, V = 0. In words,\nthe horizontal disparity is equal to the ratio of the baseline to the depth, and the vertical\ndisparity is zero. Given that we know b, we can measure H and recover the depth Z.\nUnder normal viewing conditions, humans ﬁxate; that is, there is some point in the\nFIXATE\nscene at which the optical axes of the two eyes intersect. Figure 24.17 shows two eyes ﬁxated\nat a point P0, which is at a distance Z from the midpoint of the eyes. For convenience,\nwe will compute the angular disparity, measured in radians. The disparity at the point of\nﬁxation P0 is zero. For some other point P in the scene that is δZ farther away, we can\ncompute the angular displacements of the left and right images of P, which we will call PL\nand PR, respectively. If each of these is displaced by an angle δθ/2 relative to P0, then the\ndisplacement between PL and PR, which is the disparity of P, is just δθ. From Figure 24.17,\ntan θ = b/2\nZ and tan(θ −δθ/2) =\nb/2\nZ+δZ , but for small angles, tan θ ≈θ, so\nδθ/2 = b/2\nZ −\nb/2\nZ + δZ ≈bδZ\n2Z2\nand, since the actual disparity is δθ, we have\ndisparity = bδZ\nZ2 .",
  "tan θ = b/2\nZ and tan(θ −δθ/2) =\nb/2\nZ+δZ , but for small angles, tan θ ≈θ, so\nδθ/2 = b/2\nZ −\nb/2\nZ + δZ ≈bδZ\n2Z2\nand, since the actual disparity is δθ, we have\ndisparity = bδZ\nZ2 .\nIn humans, b (the baseline distance between the eyes) is about 6 cm. Suppose that Z is about\nBASELINE\n100 cm. If the smallest detectable δθ (corresponding to the pixel size) is about 5 seconds\nof arc, this gives a δZ of 0.4 mm. For Z = 30 cm, we get the impressively small value\nδZ = 0.036 mm. That is, at a distance of 30 cm, humans can discriminate depths that differ\nby as little as 0.036 mm, enabling us to thread needles and the like. Section 24.4.\nReconstructing the 3D World\n951\nFigure 24.18\n(a) Four frames from a video sequence in which the camera is moved and\nrotated relative to the object. (b) The ﬁrst frame of the sequence, annotated with small boxes\nhighlighting the features found by the feature detector. (Courtesy of Carlo Tomasi.)\n24.4.3\nMultiple views\nShape from optical ﬂow or binocular disparity are two instances of a more general framework,\nthat of exploiting multiple views for recovering depth. In computer vision, there is no reason\nfor us to be restricted to differential motion or to only use two cameras converging at a ﬁxation\npoint. Therefore, techniques have been developed that exploit the information available in\nmultiple views, even from hundreds or thousands of cameras. Algorithmically, there are\nthree subproblems that need to be solved:\n• The correspondence problem, i.e., identifying features in the different images that are\nprojections of the same feature in the three-dimensional world.\n• The relative orientation problem, i.e., determining the transformation (rotation and\ntranslation) between the coordinate systems ﬁxed to the different cameras.\n• The depth estimation problem, i.e., determining the depths of various points in the world\nfor which image plane projections were available in at least two views\nThe development of robust matching procedures for the correspondence problem, accompa-\nnied by numerically stable algorithms for solving for relative orientations and scene depth, is\none of the success stories of computer vision. Results from one such approach due to Tomasi\nand Kanade (1992) are shown in Figures 24.18 and 24.19.\n24.4.4\nTexture\nEarlier we saw how texture was used for segmenting objects. It can also be used to estimate\ndistances. In Figure 24.20 we see that a homogeneous texture in the scene results in varying",
  "24.4.4\nTexture\nEarlier we saw how texture was used for segmenting objects. It can also be used to estimate\ndistances. In Figure 24.20 we see that a homogeneous texture in the scene results in varying\ntexture elements, or texels, in the image. All the paving tiles in (a) are identical in the scene.\nTEXEL\nThey appear different in the image for two reasons: 952\nChapter\n24.\nPerception\n(a)\n(b)\nFigure 24.19\n(a) Three-dimensional reconstruction of the locations of the image features\nin Figure 24.18, shown from above. (b) The real house, taken from the same position.\n1. Differences in the distances of the texels from the camera. Distant objects appear smaller\nby a scaling factor of 1/Z.\n2. Differences in the foreshortening of the texels. If all the texels are in the ground plane\nthen distance ones are viewed at an angle that is farther off the perpendicular, and so\nare more foreshortened. The magnitude of the foreshortening effect is proportional to\ncos σ, where σ is the slant, the angle between the Z-axis and n, the surface normal to\nthe texel.\nResearchers have developed various algorithms that try to exploit the variation in the\nappearance of the projected texels as a basis for determining surface normals. However, the\naccuracy and applicability of these algorithms is not anywhere as general as those based on\nusing multiple views.\n24.4.5\nShading\nShading—variation in the intensity of light received from different portions of a surface in a\nscene—is determined by the geometry of the scene and by the reﬂectance properties of the\nsurfaces. In computer graphics, the objective is to compute the image brightness I(x, y),\ngiven the scene geometry and reﬂectance properties of the objects in the scene. Computer\nvision aims to invert the process—that is, to recover the geometry and reﬂectance properties,\ngiven the image brightness I(x, y). This has proved to be difﬁcult to do in anything but the\nsimplest cases.\nFrom the physical model of section 24.1.4, we know that if a surface normal points\ntoward the light source, the surface is brighter, and if it points away, the surface is darker.\nWe cannot conclude that a dark patch has its normal pointing away from the light; instead,\nit could have low albedo. Generally, albedo changes quite quickly in images, and shading Section 24.4.\nReconstructing the 3D World\n953\n(a)\n(b)\nFigure 24.20\n(a) A textured scene. Assuming that the real texture is uniform allows recov-",
  "it could have low albedo. Generally, albedo changes quite quickly in images, and shading Section 24.4.\nReconstructing the 3D World\n953\n(a)\n(b)\nFigure 24.20\n(a) A textured scene. Assuming that the real texture is uniform allows recov-\nery of the surface orientation. The computed surface orientation is indicated by overlaying a\nblack circle and pointer, transformed as if the circle were painted on the surface at that point.\n(b) Recovery of shape from texture for a curved surface (white circle and pointer this time).\nImages courtesy of Jitendra Malik and Ruth Rosenholtz (1994).\nchanges rather slowly, and humans seem to be quite good at using this observation to tell\nwhether low illumination, surface orientation, or albedo caused a surface patch to be dark.\nTo simplify the problem, let us assume that the albedo is known at every surface point. It\nis still difﬁcult to recover the normal, because the image brightness is one measurement but\nthe normal has two unknown parameters, so we cannot simply solve for the normal. The key\nto this situation seems to be that nearby normals will be similar, because most surfaces are\nsmooth—they do not have sharp changes.\nThe real difﬁculty comes in dealing with interreﬂections. If we consider a typical indoor\nscene, such as the objects inside an ofﬁce, surfaces are illuminated not only by the light\nsources, but also by the light reﬂected from other surfaces in the scene that effectively serve\nas secondary light sources. These mutual illumination effects are quite signiﬁcant and make\nit quite difﬁcult to predict the relationship between the normal and the image brightness. Two\nsurface patches with the same normal might have quite different brightnesses, because one\nreceives light reﬂected from a large white wall and the other faces only a dark bookcase.\nDespite these difﬁculties, the problem is important. Humans seem to be able to ignore the\neffects of interreﬂections and get a useful perception of shape from shading, but we know\nfrustratingly little about algorithms to do this.\n24.4.6\nContour\nWhen we look at a line drawing, such as Figure 24.21, we get a vivid perception of three-\ndimensional shape and layout. How? It is a combination of recognition of familiar objects in\nthe scene and the application of generic constraints such as the following:\n• Occluding contours, such as the outlines of the hills. One side of the contour is nearer",
  "the scene and the application of generic constraints such as the following:\n• Occluding contours, such as the outlines of the hills. One side of the contour is nearer\nto the viewer, the other side is farther away. Features such as local convexity and sym- 954\nChapter\n24.\nPerception\nFigure 24.21\nAn evocative line drawing. (Courtesy of Isha Malik.)\nmetry provide cues to solving the ﬁgure-ground problem—assigning which side of the\nFIGURE-GROUND\ncontour is ﬁgure (nearer), and which is ground (farther). At an occluding contour, the\nline of sight is tangential to the surface in the scene.\n• T-junctions. When one object occludes another, the contour of the farther object is\ninterrupted, assuming that the nearer object is opaque. A T-junction results in the image.\n• Position on the ground plane. Humans, like many other terrestrial animals, are very\noften in a scene that contains a ground plane, with various objects at different locations\nGROUND PLANE\non this plane. Because of gravity, typical objects don’t ﬂoat in air but are supported by\nthis ground plane, and we can exploit the very special geometry of this viewing scenario.\nLet us work out the projection of objects of different heights and at different loca-\ntions on the ground plane. Suppose that the eye, or camera, is at a height hc above\nthe ground plane. Consider an object of height δY resting on the ground plane, whose\nbottom is at (X, −hc, Z) and top is at (X, δY −hc, Z). The bottom projects to the\nimage point (fX/Z, −fhc/Z) and the top to (fX/Z, f(δY −hc)/Z). The bottoms of\nnearer objects (small Z) project to points lower in the image plane; farther objects have\nbottoms closer to the horizon.\n24.4.7\nObjects and the geometric structure of scenes\nA typical adult human head is about 9 inches long. This means that for someone who is 43\nfeet away, the angle subtended by the head at the camera is 1 degree. If we see a person whose\nhead appears to subtend just half a degree, Bayesian inference suggests we are looking at a\nnormal person who is 86 feet away, rather than someone with a half-size head. This line of\nreasoning supplies us with a method to check the results of a pedestrian detector, as well as a\nmethod to estimate the distance to an object. For example, all pedestrians are about the same\nheight, and they tend to stand on a ground plane. If we know where the horizon is in an image,\nwe can rank pedestrians by distance to the camera. This works because we know where their Section 24.4.",
  "height, and they tend to stand on a ground plane. If we know where the horizon is in an image,\nwe can rank pedestrians by distance to the camera. This works because we know where their Section 24.4.\nReconstructing the 3D World\n955\nImage plane\nHorizon\nGround plane\nA\nB\nC\nA\nC\nB\nFigure 24.22\nIn an image of people standing on a ground plane, the people whose feet\nare closer to the horizon in the image must be farther away (top drawing). This means they\nmust look smaller in the image (left lower drawing). This means that the size and location of\nreal pedestrians in an image depend upon one another and on the location of the horizon. To\nexploit this, we need to identify the ground plane, which is done using shape-from-texture\nmethods. From this information, and from some likely pedestrians, we can recover a horizon\nas shown in the center image. On the right, acceptable pedestrian boxes given this geometric\ncontext. Notice that pedestrians who are higher in the scene must be smaller. If they are not,\nthen they are false positives. Images from Hoiem et al. (2008) c⃝IEEE.\nfeet are, and pedestrians whose feet are closer to the horizon in the image are farther away\nfrom the camera (Figure 24.22). Pedestrians who are farther away from the camera must also\nbe smaller in the image. This means we can rule out some detector responses — if a detector\nﬁnds a pedestrian who is large in the image and whose feet are close to the horizon, it has\nfound an enormous pedestrian; these don’t exist, so the detector is wrong. In fact, many or\nmost image windows are not acceptable pedestrian windows, and need not even be presented\nto the detector.\nThere are several strategies for ﬁnding the horizon, including searching for a roughly\nhorizontal line with a lot of blue above it, and using surface orientation estimates obtained\nfrom texture deformation. A more elegant strategy exploits the reverse of our geometric\nconstraints. A reasonably reliable pedestrian detector is capable of producing estimates of the\nhorizon, if there are several pedestrians in the scene at different distances from the camera.\nThis is because the relative scaling of the pedestrians is a cue to where the horizon is. So we\ncan extract a horizon estimate from the detector, then use this estimate to prune the pedestrian\ndetector’s mistakes. 956\nChapter\n24.\nPerception\nIf the object is familiar, we can estimate more than just the distance to it, because what it",
  "can extract a horizon estimate from the detector, then use this estimate to prune the pedestrian\ndetector’s mistakes. 956\nChapter\n24.\nPerception\nIf the object is familiar, we can estimate more than just the distance to it, because what it\nlooks like in the image depends very strongly on its pose, i.e., its position and orientation with\nrespect to the viewer. This has many applications. For instance, in an industrial manipulation\ntask, the robot arm cannot pick up an object until the pose is known. In the case of rigid\nobjects, whether three-dimensional or two-dimensional, this problem has a simple and well-\ndeﬁned solution based on the alignment method, which we now develop.\nALIGNMENT METHOD\nThe object is represented by M features or distinguished points m1, m2, . . . , mM in\nthree-dimensional space—perhaps the vertices of a polyhedral object. These are measured\nin some coordinate system that is natural for the object. The points are then subjected to\nan unknown three-dimensional rotation R, followed by translation by an unknown amount t\nand then projection to give rise to image feature points p1, p2, . . . , pN on the image plane.\nIn general, N ̸= M, because some model points may be occluded, and the feature detector\ncould miss some features (or invent false ones due to noise). We can express this as\npi = Π(Rmi + t) = Q(mi)\nfor a three-dimensional model point mi and the corresponding image point pi. Here, R\nis a rotation matrix, t is a translation, and Π denotes perspective projection or one of its\napproximations, such as scaled orthographic projection. The net result is a transformation Q\nthat will bring the model point mi into alignment with the image point pi. Although we do\nnot know Q initially, we do know (for rigid objects) that Q must be the same for all the model\npoints.\nWe can solve for Q, given the three-dimensional coordinates of three model points and\ntheir two-dimensional projections. The intuition is as follows: we can write down equations\nrelating the coordinates of pi to those of mi. In these equations, the unknown quantities\ncorrespond to the parameters of the rotation matrix R and the translation vector t. If we have\nenough equations, we ought to be able to solve for Q. We will not give a proof here; we\nmerely state the following result:\nGiven three noncollinear points m1, m2, and m3 in the model, and their scaled\northographic projections p1, p2, and p3 on the image plane, there exist exactly",
  "merely state the following result:\nGiven three noncollinear points m1, m2, and m3 in the model, and their scaled\northographic projections p1, p2, and p3 on the image plane, there exist exactly\ntwo transformations from the three-dimensional model coordinate frame to a two-\ndimensional image coordinate frame.\nThese transformations are related by a reﬂection around the image plane and can be computed\nby a simple closed-form solution. If we could identify the corresponding model features for\nthree features in the image, we could compute Q, the pose of the object.\nLet us specify position and orientation in mathematical terms. The position of a point P\nin the scene is characterized by three numbers, the (X, Y, Z) coordinates of P in a coordinate\nframe with its origin at the pinhole and the Z-axis along the optical axis (Figure 24.2 on\npage 931). What we have available is the perspective projection (x, y) of the point in the\nimage. This speciﬁes the ray from the pinhole along which P lies; what we do not know is\nthe distance. The term “orientation” could be used in two senses:\n1. The orientation of the object as a whole. This can be speciﬁed in terms of a three-\ndimensional rotation relating its coordinate frame to that of the camera. Section 24.5.\nObject Recognition from Structural Information\n957\n2. The orientation of the surface of the object at P. This can be speciﬁed by a normal\nvector, n—which is a vector specifying the direction that is perpendicular to the surface.\nOften we express the surface orientation using the variables slant and tilt. Slant is the\nSLANT\nTILT\nangle between the Z-axis and n. Tilt is the angle between the X-axis and the projection\nof n on the image plane.\nWhen the camera moves relative to an object, both the object’s distance and its orientation\nchange. What is preserved is the shape of the object. If the object is a cube, that fact is\nSHAPE\nnot changed when the object moves. Geometers have been attempting to formalize shape for\ncenturies, the basic concept being that shape is what remains unchanged under some group of\ntransformations—for example, combinations of rotations and translations. The difﬁculty lies\nin ﬁnding a representation of global shape that is general enough to deal with the wide variety\nof objects in the real world—not just simple forms like cylinders, cones, and spheres—and yet\ncan be recovered easily from the visual input. The problem of characterizing the local shape",
  "of objects in the real world—not just simple forms like cylinders, cones, and spheres—and yet\ncan be recovered easily from the visual input. The problem of characterizing the local shape\nof a surface is much better understood. Essentially, this can be done in terms of curvature:\nhow does the surface normal change as one moves in different directions on the surface? For\na plane, there is no change at all. For a cylinder, if one moves parallel to the axis, there is\nno change, but in the perpendicular direction, the surface normal rotates at a rate inversely\nproportional to the radius of the cylinder, and so on. All this is studied in the subject called\ndifferential geometry.\nThe shape of an object is relevant for some manipulation tasks (e.g., deciding where to\ngrasp an object), but its most signiﬁcant role is in object recognition, where geometric shape\nalong with color and texture provide the most signiﬁcant cues to enable us to identify objects,\nclassify what is in the image as an example of some class one has seen before, and so on.\n24.5\nOBJECT RECOGNITION FROM STRUCTURAL INFORMATION\nPutting a box around pedestrians in an image may well be enough to avoid driving into them.\nWe have seen that we can ﬁnd a box by pooling the evidence provided by orientations, using\nhistogram methods to suppress potentially confusing spatial detail. If we want to know more\nabout what someone is doing, we will need to know where their arms, legs, body, and head lie\nin the picture. Individual body parts are quite difﬁcult to detect on their own using a moving\nwindow method, because their color and texture can vary widely and because they are usually\nsmall in images. Often, forearms and shins are as small as two to three pixels wide. Body\nparts do not usually appear on their own, and representing what is connected to what could\nbe quite powerful, because parts that are easy to ﬁnd might tell us where to look for parts that\nare small and hard to detect.\nInferring the layout of human bodies in pictures is an important task in vision, because\nthe layout of the body often reveals what people are doing. A model called a deformable\ntemplate can tell us which conﬁgurations are acceptable: the elbow can bend but the head is\nDEFORMABLE\nTEMPLATE\nnever joined to the foot. The simplest deformable template model of a person connects lower\narms to upper arms, upper arms to the torso, and so on. There are richer models: for example, 958\nChapter\n24.\nPerception",
  "DEFORMABLE\nTEMPLATE\nnever joined to the foot. The simplest deformable template model of a person connects lower\narms to upper arms, upper arms to the torso, and so on. There are richer models: for example, 958\nChapter\n24.\nPerception\nwe could represent the fact that left and right upper arms tend to have the same color and\ntexture, as do left and right legs. These richer models remain difﬁcult to work with, however.\n24.5.1\nThe geometry of bodies: Finding arms and legs\nFor the moment, we assume that we know what the person’s body parts look like (e.g., we\nknow the color and texture of the person’s clothing). We can model the geometry of the\nbody as a tree of eleven segments (upper and lower left and right arms and legs respectively,\na torso, a face, and hair on top of the face) each of which is rectangular. We assume that\nthe position and orientation (pose) of the left lower arm is independent of all other segments\nPOSE\ngiven the pose of the left upper arm; that the pose of the left upper arm is independent of\nall segments given the pose of the torso; and extend these assumptions in the obvious way\nto include the right arm and the legs, the face, and the hair. Such models are often called\n“cardboard people” models. The model forms a tree, which is usually rooted at the torso. We\nwill search the image for the best match to this cardboard person using inference methods for\na tree-structured Bayes net (see Chapter 14).\nThere are two criteria for evaluating a conﬁguration. First, an image rectangle should\nlook like its segment. For the moment, we will remain vague about precisely what that means,\nbut we assume we have a function φi that scores how well an image rectangle matches a body\nsegment. For each pair of related segments, we have another function ψ that scores how\nwell relations between a pair of image rectangles match those to be expected from the body\nsegments. The dependencies between segments form a tree, so each segment has only one\nparent, and we could write ψi,pa(i). All the functions will be larger if the match is better,\nso we can think of them as being like a log probability. The cost of a particular match that\nallocates image rectangle mi to body segment i is then\n\f\ni∈segments\nφi(mi) +\n\f\ni∈segments\nψi,pa(i)(mi, mpa(i)) .\nDynamic programming can ﬁnd the best match, because the relational model is a tree.\nIt is inconvenient to search a continuous space, and we will discretize the space of image",
  "i∈segments\nφi(mi) +\n\f\ni∈segments\nψi,pa(i)(mi, mpa(i)) .\nDynamic programming can ﬁnd the best match, because the relational model is a tree.\nIt is inconvenient to search a continuous space, and we will discretize the space of image\nrectangles. We do so by discretizing the location and orientation of rectangles of ﬁxed size\n(the sizes may be different for different segments). Because ankles and knees are different,\nwe need to distinguish between a rectangle and the same rectangle rotated by 180 ◦. One\ncould visualize the result as a set of very large stacks of small rectangles of image, cut out at\ndifferent locations and orientations. There is one stack per segment. We must now ﬁnd the\nbest allocation of rectangles to segments. This will be slow, because there are many image\nrectangles and, for the model we have given, choosing the right torso will be O(M6) if there\nare M image rectangles. However, various speedups are available for an appropriate choice\nof ψ, and the method is practical (Figure 24.23). The model is usually known as a pictorial\nstructure model.\nPICTORIAL\nSTRUCTURE MODEL\nRecall our assumption that we know what we need to know about what the person looks\nlike. If we are matching a person in a single image, the most useful feature for scoring seg-\nment matches turns out to be color. Texture features don’t work well in most cases, because\nfolds on loose clothing produce strong shading patterns that overlay the image texture. These Section 24.5.\nObject Recognition from Structural Information\n959\nFigure 24.23\nA pictorial structure model evaluates a match between a set of image rect-\nangles and a cardboard person (shown on the left) by scoring the similarity in appearance\nbetween body segments and image segments and the spatial relations between the image seg-\nments. Generally, a match is better if the image segments have about the right appearance and\nare in about the right place with respect to one another. The appearance model uses average\ncolors for hair, head, torso, and upper and lower arms and legs. The relevant relations are\nshown as arrows. On the right, the best match for a particular image, obtained using dynamic\nprogramming. The match is a fair estimate of the conﬁguration of the body. Figure from\nFelzenszwalb and Huttenlocher (2000) c⃝IEEE.\npatterns are strong enough to disrupt the true texture of the cloth. In current work, ψ typically\nreﬂects the need for the ends of the segments to be reasonably close together, but there are",
  "Felzenszwalb and Huttenlocher (2000) c⃝IEEE.\npatterns are strong enough to disrupt the true texture of the cloth. In current work, ψ typically\nreﬂects the need for the ends of the segments to be reasonably close together, but there are\nusually no constraints on the angles. Generally, we don’t know what a person looks like,\nand must build a model of segment appearances. We call the description of what a person\nlooks like the appearance model. If we must report the conﬁguration of a person in a single\nAPPEARANCE\nMODEL\nimage, we can start with a poorly tuned appearance model, estimate conﬁguration with this,\nthen re-estimate appearance, and so on. In video, we have many frames of the same person,\nand this will reveal their appearance.\n24.5.2\nCoherent appearance: Tracking people in video\nTracking people in video is an important practical problem. If we could reliably report the\nlocation of arms, legs, torso, and head in video sequences, we could build much improved\ngame interfaces and surveillance systems. Filtering methods have not had much success\nwith this problem, because people can produce large accelerations and move quite fast. This\nmeans that for 30 Hz video, the conﬁguration of the body in frame i doesn’t constrain the\nconﬁguration of the body in frame i+1 all that strongly. Currently, the most effective methods\nexploit the fact that appearance changes very slowly from frame to frame. If we can infer an\nappearance model of an individual from the video, then we can use this information in a\npictorial structure model to detect that person in each frame of the video. We can then link\nthese locations across time to make a track. 960\nChapter\n24.\nPerception\nLateral walking \ndetector\nAppearance\nmodel\nBody part \nmaps\nDetected figure\ntorso\narm\nmotion blur\n& interlacing\nFigure 24.24\nWe can track moving people with a pictorial structure model by ﬁrst ob-\ntaining an appearance model, then applying it. To obtain the appearance model, we scan the\nimage to ﬁnd a lateral walking pose. The detector does not need to be very accurate, but\nshould produce few false positives. From the detector response, we can read off pixels that\nlie on each body segment, and others that do not lie on that segment. This makes it possible to\nbuild a discriminative model of the appearance of each body part, and these are tied together\ninto a pictorial structure model of the person being tracked. Finally, we can reliably track by",
  "build a discriminative model of the appearance of each body part, and these are tied together\ninto a pictorial structure model of the person being tracked. Finally, we can reliably track by\ndetecting this model in each frame. As the frames in the lower part of the image suggest, this\nprocedure can track complicated, fast-changing body conﬁgurations, despite degradation of\nthe video signal due to motion blur. Figure from Ramanan et al. (2007) c⃝IEEE.\nThere are several ways to infer a good appearance model. We regard the video as a\nlarge stack of pictures of the person we wish to track. We can exploit this stack by looking\nfor appearance models that explain many of the pictures. This would work by detecting\nbody segments in each frame, using the fact that segments have roughly parallel edges. Such\ndetectors are not particularly reliable, but the segments we want to ﬁnd are special. They\nwill appear at least once in most of the frames of video; such segments can be found by\nclustering the detector responses. It is best to start with the torso, because it is big and\nbecause torso detectors tend to be reliable. Once we have a torso appearance model, upper\nleg segments should appear near the torso, and so on. This reasoning yields an appearance\nmodel, but it can be unreliable if people appear against a near-ﬁxed background where the\nsegment detector generates lots of false positives. An alternative is to estimate appearance\nfor many of the frames of video by repeatedly reestimating conﬁguration and appearance; we\nthen see if one appearance model explains many frames. Another alternative, which is quite Section 24.6.\nUsing Vision\n961\nFigure 24.25\nSome complex human actions produce consistent patterns of appearance\nand motion. For example, drinking involves movements of the hand in front of the face. The\nﬁrst three images are correct detections of drinking; the fourth is a false-positive (the cook is\nlooking into the coffee pot, but not drinking from it). Figure from Laptev and Perez (2007)\nc⃝IEEE.\nreliable in practice, is to apply a detector for a ﬁxed body conﬁguration to all of the frames. A\ngood choice of conﬁguration is one that is easy to detect reliably, and where there is a strong\nchance the person will appear in that conﬁguration even in a short sequence (lateral walking\nis a good choice). We tune the detector to have a low false positive rate, so we know when it\nresponds that we have found a real person; and because we have localized their torso, arms,",
  "is a good choice). We tune the detector to have a low false positive rate, so we know when it\nresponds that we have found a real person; and because we have localized their torso, arms,\nlegs, and head, we know what these segments look like.\n24.6\nUSING VISION\nIf vision systems could analyze video and understood what people are doing, we would be\nable to: design buildings and public places better by collecting and using data about what\npeople do in public; build more accurate, more secure, and less intrusive surveillance systems;\nbuild computer sports commentators; and build human-computer interfaces that watch people\nand react to their behavior. Applications for reactive interfaces range from computer games\nthat make a player get up and move around to systems that save energy by managing heat and\nlight in a building to match where the occupants are and what they are doing.\nSome problems are well understood. If people are relatively small in the video frame,\nand the background is stable, it is easy to detect the people by subtracting a background image\nfrom the current frame. If the absolute value of the difference is large, this background\nsubtraction declares the pixel to be a foreground pixel; by linking foreground blobs over\nBACKGROUND\nSUBTRACTION\ntime, we obtain a track.\nStructured behaviors like ballet, gymnastics, or tai chi have speciﬁc vocabularies of ac-\ntions. When performed against a simple background, videos of these actions are easy to deal\nwith. Background subtraction identiﬁes the major moving regions, and we can build HOG\nfeatures (keeping track of ﬂow rather than orientation) to present to a classiﬁer. We can detect\nconsistent patterns of action with a variant of our pedestrian detector, where the orientation\nfeatures are collected into histogram buckets over time as well as space (Figure 24.25).\nMore general problems remain open. The big research question is to link observations\nof the body and the objects nearby to the goals and intentions of the moving people. One\nsource of difﬁculty is that we lack a simple vocabulary of human behavior. Behavior is a lot 962\nChapter\n24.\nPerception\nlike color, in that people tend to think they know a lot of behavior names but can’t produce\nlong lists of such words on demand. There is quite a lot of evidence that behaviors combine—\nyou can, for example, drink a milkshake while visiting an ATM—but we don’t yet know\nwhat the pieces are, how the composition works, or how many composites there might be.",
  "you can, for example, drink a milkshake while visiting an ATM—but we don’t yet know\nwhat the pieces are, how the composition works, or how many composites there might be.\nA second source of difﬁculty is that we don’t know what features expose what is happening.\nFor example, knowing someone is close to an ATM may be enough to tell that they’re visiting\nthe ATM. A third difﬁculty is that the usual reasoning about the relationship between training\nand test data is untrustworthy. For example, we cannot argue that a pedestrian detector is\nsafe simply because it performs well on a large data set, because that data set may well omit\nimportant, but rare, phenomena (for example, people mounting bicycles). We wouldn’t want\nour automated driver to run over a pedestrian who happened to do something unusual.\n24.6.1\nWords and pictures\nMany Web sites offer collections of images for viewing. How can we ﬁnd the images we\nwant? Let’s suppose the user enters a text query, such as “bicycle race.” Some of the images\nwill have keywords or captions attached, or will come from Web pages that contain text near\nthe image. For these, image retrieval can be like text retrieval: ignore the images and match\nthe image’s text against the query (see Section 22.3 on page 867).\nHowever, keywords are usually incomplete. For example, a picture of a cat playing in\nthe street might be tagged with words like “cat” and “street,” but it is easy to forget to mention\nthe “garbage can” or the “ﬁsh bones.” Thus an interesting task is to annotate an image (which\nmay already have a few keywords) with additional appropriate keywords.\nIn the most straightforward version of this task, we have a set of correctly tagged ex-\nample images, and we wish to tag some test images. This problem is sometimes known as\nauto-annotation. The most accurate solutions are obtained using nearest-neighbors methods.\nOne ﬁnds the training images that are closest to the test image in a feature space metric that\nis trained using examples, then reports their tags.\nAnother version of the problem involves predicting which tags to attach to which re-\ngions in a test image. Here we do not know which regions produced which tags for the train-\ning data. We can use a version of expectation maximization to guess an initial correspondence\nbetween text and regions, and from that estimate a better decomposition into regions, and so\non.\n24.6.2\nReconstruction from many views",
  "ing data. We can use a version of expectation maximization to guess an initial correspondence\nbetween text and regions, and from that estimate a better decomposition into regions, and so\non.\n24.6.2\nReconstruction from many views\nBinocular stereopsis works because for each point we have four measurements constraining\nthree unknown degrees of freedom. The four measurements are the (x, y) positions of the\npoint in each view, and the unknown degrees of freedom are the (x, y, z) coordinate values of\nthe point in the scene. This rather crude argument suggests, correctly, that there are geometric\nconstraints that prevent most pairs of points from being acceptable matches. Many images of\na set of points should reveal their positions unambiguously.\nWe don’t always need a second picture to get a second view of a set of points. If we\nbelieve the original set of points comes from a familiar rigid 3D object, then we might have Section 24.6.\nUsing Vision\n963\nan object model available as a source of information. If this object model consists of a set of\n3D points or of a set of pictures of the object, and if we can establish point correspondences,\nwe can determine the parameters of the camera that produced the points in the original image.\nThis is very powerful information. We could use it to evaluate our original hypothesis that\nthe points come from an object model. We do this by using some points to determine the\nparameters of the camera, then projecting model points in this camera and checking to see\nwhether there are image points nearby.\nWe have sketched here a technology that is now very highly developed. The technology\ncan be generalized to deal with views that are not orthographic; to deal with points that are\nobserved in only some views; to deal with unknown camera properties like focal length; to\nexploit various sophisticated searches for appropriate correspondences; and to do reconstruc-\ntion from very large numbers of points and of views. If the locations of points in the images\nare known with some accuracy and the viewing directions are reasonable, very high accuracy\ncamera and point information can be obtained. Some applications are\n• Model-building: For example, one might build a modeling system that takes a video\nsequence depicting an object and produces a very detailed three-dimensional mesh of\ntextured polygons for use in computer graphics and virtual reality applications. Models",
  "sequence depicting an object and produces a very detailed three-dimensional mesh of\ntextured polygons for use in computer graphics and virtual reality applications. Models\nlike this can now be built from apparently quite unpromising sets of pictures. For ex-\nample, Figure 24.26 shows a model of the Statue of Liberty built from pictures found\non the Internet.\n• Matching moves: To place computer graphics characters into real video, we need to\nknow how the camera moved for the real video, so that we can render the character\ncorrectly.\n• Path reconstruction: Mobile robots need to know where they have been. If they are\nmoving in a world of rigid objects, then performing a reconstruction and keeping the\ncamera information is one way to obtain a path.\n24.6.3\nUsing vision for controlling movement\nOne of the principal uses of vision is to provide information both for manipulating objects—\npicking them up, grasping them, twirling them, and so on—and for navigating while avoiding\nobstacles. The ability to use vision for these purposes is present in the most primitive of\nanimal visual systems. In many cases, the visual system is minimal, in the sense that it\nextracts from the available light ﬁeld just the information the animal needs to inform its\nbehavior. Quite probably, modern vision systems evolved from early, primitive organisms\nthat used a photosensitive spot at one end to orient themselves toward (or away from) the\nlight. We saw in Section 24.4 that ﬂies use a very simple optical ﬂow detection system to\nland on walls. A classic study, What the Frog’s Eye Tells the Frog’s Brain (Lettvin et al.,\n1959), observes of a frog that, “He will starve to death surrounded by food if it is not moving.\nHis choice of food is determined only by size and movement.”\nLet us consider a vision system for an automated vehicle driving on a freeway. The\ntasks faced by the driver include the following: 964\nChapter\n24.\nPerception\na\nb\nc\n(a)\n(b)\n(c)\nFigure 24.26\nThe state of the art in multiple-view reconstruction is now highly advanced.\nThis ﬁgure outlines a system built by Michael Goesele and colleagues from the University\nof Washington, TU Darmstadt, and Microsoft Research. From a collection of pictures of a\nmonument taken by a large community of users and posted on the Internet (a), their system\ncan determine the viewing directions for those pictures, shown by the small black pyramids\nin (b) and a comprehensive 3D reconstruction shown in (c).",
  "monument taken by a large community of users and posted on the Internet (a), their system\ncan determine the viewing directions for those pictures, shown by the small black pyramids\nin (b) and a comprehensive 3D reconstruction shown in (c).\n1. Lateral control—ensure that the vehicle remains securely within its lane or changes\nlanes smoothly when required.\n2. Longitudinal control—ensure that there is a safe distance to the vehicle in front.\n3. Obstacle avoidance—monitor vehicles in neighboring lanes and be prepared for evasive\nmaneuvers if one of them decides to change lanes.\nThe problem for the driver is to generate appropriate steering, acceleration, and braking ac-\ntions to best accomplish these tasks.\nFor lateral control, one needs to maintain a representation of the position and orientation\nof the car relative to the lane. We can use edge-detection algorithms to ﬁnd edges correspond-\ning to the lane-marker segments. We can then ﬁt smooth curves to these edge elements. The\nparameters of these curves carry information about the lateral position of the car, the direc-\ntion it is pointing relative to the lane, and the curvature of the lane. This information, along\nwith information about the dynamics of the car, is all that is needed by the steering-control\nsystem. If we have good detailed maps of the road, then the vision system serves to conﬁrm\nour position (and to watch for obstacles that are not on the map).\nFor longitudinal control, one needs to know distances to the vehicles in front. This can\nbe accomplished with binocular stereopsis or optical ﬂow. Using these techniques, vision-\ncontrolled cars can now drive reliably at highway speeds.\nThe more general case of mobile robots navigating in various indoor and outdoor envi-\nronments has been studied, too. One particular problem, localizing the robot in its environ-\nment, now has pretty good solutions. A group at Sarnoff has developed a system based on\ntwo cameras looking forward that track feature points in 3D and use that to reconstruct the Section 24.7.\nSummary\n965\nposition of the robot relative to the environment. In fact, they have two stereoscopic camera\nsystems, one looking front and one looking back—this gives greater robustness in case the\nrobot has to go through a featureless patch due to dark shadows, blank walls, and the like. It is\nunlikely that there are no features either in the front or in the back. Now of course, that could",
  "robot has to go through a featureless patch due to dark shadows, blank walls, and the like. It is\nunlikely that there are no features either in the front or in the back. Now of course, that could\nhappen, so a backup is provided by using an inertial motion unit (IMU) somewhat akin to the\nmechanisms for sensing acceleration that we humans have in our inner ears. By integrating\nthe sensed acceleration twice, one can keep track of the change in position. Combining the\ndata from vision and the IMU is a problem of probabilistic evidence fusion and can be tackled\nusing techniques, such as Kalman ﬁltering, we have studied elsewhere in the book.\nIn the use of visual odometry (estimation of change in position), as in other problems\nof odometry, there is the problem of “drift,” positional errors accumulating over time. The\nsolution for this is to use landmarks to provide absolute position ﬁxes: as soon as the robot\npasses a location in its internal map, it can adjust its estimate of its position appropriately.\nAccuracies on the order of centimeters have been demonstrated with the these techniques.\nThe driving example makes one point very clear: for a speciﬁc task, one does not need\nto recover all the information that, in principle, can be recovered from an image. One does\nnot need to recover the exact shape of every vehicle, solve for shape-from-texture on the grass\nsurface adjacent to the freeway, and so on. Instead, a vision system should compute just what\nis needed to accomplish the task.\n24.7\nSUMMARY\nAlthough perception appears to be an effortless activity for humans, it requires a signiﬁcant\namount of sophisticated computation. The goal of vision is to extract information needed for\ntasks such as manipulation, navigation, and object recognition.\n• The process of image formation is well understood in its geometric and physical as-\npects. Given a description of a three-dimensional scene, we can easily produce a picture\nof it from some arbitrary camera position (the graphics problem). Inverting the process\nby going from an image to a description of the scene is more difﬁcult.\n• To extract the visual information necessary for the tasks of manipulation, navigation,\nand recognition, intermediate representations have to be constructed.\nEarly vision\nimage-processing algorithms extract primitive features from the image, such as edges\nand regions.\n• There are various cues in the image that enable one to obtain three-dimensional in-",
  "Early vision\nimage-processing algorithms extract primitive features from the image, such as edges\nand regions.\n• There are various cues in the image that enable one to obtain three-dimensional in-\nformation about the scene: motion, stereopsis, texture, shading, and contour analysis.\nEach of these cues relies on background assumptions about physical scenes to provide\nnearly unambiguous interpretations.\n• Object recognition in its full generality is a very hard problem. We discussed brightness-\nbased and feature-based approaches. We also presented a simple algorithm for pose\nestimation. Other possibilities exist. 966\nChapter\n24.\nPerception\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe eye developed in the Cambrian explosion (530 million years ago), apparently in a com-\nmon ancestor. Since then, endless variations have developed in different creatures, but the\nsame gene, Pax-6, regulates the development of the eye in animals as diverse as humans,\nmice, and Drosophila.\nSystematic attempts to understand human vision can be traced back to ancient times.\nEuclid (ca. 300 B.C.) wrote about natural perspective—the mapping that associates, with\neach point P in the three-dimensional world, the direction of the ray OP joining the center of\nprojection O to the point P. He was well aware of the notion of motion parallax. The use of\nperspective in art was developed in ancient Roman culture, as evidenced by art found in the\nruins of Pompeii (A.D. 79), but was then largely lost for 1300 years. The mathematical under-\nstanding of perspective projection, this time in the context of projection onto planar surfaces,\nhad its next signiﬁcant advance in the 15th-century in Renaissance Italy. Brunelleschi (1413)\nis usually credited with creating the ﬁrst paintings based on geometrically correct projection\nof a three-dimensional scene. In 1435, Alberti codiﬁed the rules and inspired generations of\nartists whose artistic achievements amaze us to this day. Particularly notable in their develop-\nment of the science of perspective, as it was called in those days, were Leonardo da Vinci and\nAlbrecht D¨urer. Leonardo’s late 15th century descriptions of the interplay of light and shade\n(chiaroscuro), umbra and penumbra regions of shadows, and aerial perspective are still worth\nreading in translation (Kemp, 1989). Stork (2004) analyzes the creation of various pieces of\nRenaissance art using computer vision techniques.",
  "(chiaroscuro), umbra and penumbra regions of shadows, and aerial perspective are still worth\nreading in translation (Kemp, 1989). Stork (2004) analyzes the creation of various pieces of\nRenaissance art using computer vision techniques.\nAlthough perspective was known to the ancient Greeks, they were curiously confused\nby the role of the eyes in vision. Aristotle thought of the eyes as devices emitting rays, rather\nin the manner of modern laser range ﬁnders. This mistaken view was laid to rest by the work\nof Arab scientists, such as Abu Ali Alhazen, in the 10th century. Alhazen also developed the\ncamera obscura, a room (camera is Latin for “room” or “chamber”) with a pinhole that casts\nan image on the opposite wall. Of course the image was inverted, which caused no end of\nconfusion. If the eye was to be thought of as such an imaging device, how do we see right-\nside up? This enigma exercised the greatest minds of the era (including Leonardo). Kepler\nﬁrst proposed that the lens of the eye focuses an image on the retina, and Descartes surgically\nremoved an ox eye and demonstrated that Kepler was right. There was still puzzlement as to\nwhy we do not see everything upside down; today we realize it is just a question of accessing\nthe retinal data structure in the right way.\nIn the ﬁrst half of the 20th century, the most signiﬁcant research results in vision were\nobtained by the Gestalt school of psychology, led by Max Wertheimer. They pointed out the\nimportance of perceptual organization: for a human observer, the image is not a collection\nof pointillist photoreceptor outputs (pixels in computer vision terminology); rather it is or-\nganized into coherent groups. One could trace the motivation in computer vision of ﬁnding\nregions and curves back to this insight. The Gestaltists also drew attention to the “ﬁgure–\nground” phenomenon—a contour separating two image regions that, in the world, are at\ndifferent depths, appears to belong only to the nearer region, the “ﬁgure,” and not the farther Bibliographical and Historical Notes\n967\nregion, the “ground.” The computer vision problem of classifying image curves according to\ntheir signiﬁcance in the scene can be thought of as a generalization of this insight.\nThe period after World War II was marked by renewed activity. Most signiﬁcant was\nthe work of J. J. Gibson (1950, 1979), who pointed out the importance of optical ﬂow, as well",
  "The period after World War II was marked by renewed activity. Most signiﬁcant was\nthe work of J. J. Gibson (1950, 1979), who pointed out the importance of optical ﬂow, as well\nas texture gradients in the estimation of environmental variables such as surface slant and tilt.\nHe reemphasized the importance of the stimulus and how rich it was. Gibson emphasized the\nrole of the active observer whose self-directed movement facilitates the pickup of information\nabout the external environment.\nComputer vision was founded in the 1960s. Roberts’s (1963) thesis at MIT was one\nof the earliest publications in the ﬁeld, introducing key ideas such as edge detection and\nmodel-based matching. There is an urban legend that Marvin Minsky assigned the problem\nof “solving” computer vision to a graduate student as a summer project. According to Minsky\nthe legend is untrue—it was actually an undergraduate student. But it was an exceptional\nundergraduate, Gerald Jay Sussman (who is now a professor at MIT) and the task was not to\n“solve” vision, but to investigate some aspects of it.\nIn the 1960s and 1970s, progress was slow, hampered considerably by the lack of com-\nputational and storage resources. Low-level visual processing received a lot of attention. The\nwidely used Canny edge-detection technique was introduced in Canny (1986). Techniques\nfor ﬁnding texture boundaries based on multiscale, multiorientation ﬁltering of images date to\nwork such as Malik and Perona (1990). Combining multiple clues—brightness, texture and\ncolor—for ﬁnding boundary curves in a learning framework was shown by Martin, Fowlkes\nand Malik (2004) to considerably improve performance.\nThe closely related problem of ﬁnding regions of coherent brightness, color, and tex-\nture, naturally lends itself to formulations in which ﬁnding the best partition becomes an\noptimization problem. Three leading examples are the Markov Random Fields approach of\nGeman and Geman (1984), the variational formulation of Mumford and Shah (1989), and\nnormalized cuts by Shi and Malik (2000).\nThrough much of the 1960s, 1970s and 1980s, there were two distinct paradigms in\nwhich visual recognition was pursued, dictated by different perspectives on what was per-\nceived to be the primary problem. Computer vision research on object recognition largely fo-\ncused on issues arising from the projection of three-dimensional objects onto two-dimensional",
  "ceived to be the primary problem. Computer vision research on object recognition largely fo-\ncused on issues arising from the projection of three-dimensional objects onto two-dimensional\nimages. The idea of alignment, also ﬁrst introduced by Roberts, resurfaced in the 1980s in the\nwork of Lowe (1987) and Huttenlocher and Ullman (1990). Also popular was an approach\nbased on describing shapes in terms of volumetric primitives, with generalized cylinders,\nGENERALIZED\nCYLINDER\nintroduced by Tom Binford (1971), proving particularly popular.\nIn contrast, the pattern recognition community viewed the 3D-to-2D aspects of the prob-\nlem as not signiﬁcant. Their motivating examples were in domains such as optical character\nrecognition and handwritten zip code recognition where the primary concern is that of learn-\ning the typical variations characteristic of a class of objects and separating them from other\nclasses. See LeCun et al. (1995) for a comparison of approaches.\nIn the late 1990s, these two paradigms started to converge, as both sides adopted the\nprobabilistic modeling and learning techniques that were becoming popular throughout AI.\nTwo lines of work contributed signiﬁcantly. One was research on face detection, such as that 968\nChapter\n24.\nPerception\nof Rowley, Baluja and Kanade (1996), and of Viola and Jones (2002b) which demonstrated\nthe power of pattern recognition techniques on clearly important and useful tasks. The other\nwas the development of point descriptors, which enable one to construct feature vectors from\nparts of objects. This was pioneered by Schmid and Mohr (1996). Lowe’s (2004) SIFT\ndescriptor is widely used. The HOG descriptor is due to Dalal and Triggs (2005).\nUllman (1979) and Longuet-Higgins (1981) are inﬂuential early works in reconstruc-\ntion from multiple images. Concerns about the stability of structure from motion were sig-\nniﬁcantly allayed by the work of Tomasi and Kanade (1992) who showed that with the use of\nmultiple frames shape could be recovered quite accurately. In the 1990s, with great increase\nin computer speed and storage, motion analysis found many new applications. Building geo-\nmetrical models of real-world scenes for rendering by computer graphics techniques proved\nparticularly popular, led by reconstruction algorithms such as the one developed by Debevec,\nTaylor, and Malik (1996). The books by Hartley and Zisserman (2000) and Faugeras et al.\n(2001) provide a comprehensive treatment of the geometry of multiple views.",
  "Taylor, and Malik (1996). The books by Hartley and Zisserman (2000) and Faugeras et al.\n(2001) provide a comprehensive treatment of the geometry of multiple views.\nFor single images, inferring shape from shading was ﬁrst studied by Horn (1970), and\nHorn and Brooks (1989) present an extensive survey of the main papers from a period when\nthis was a much-studied problem. Gibson (1950) was the ﬁrst to propose texture gradients\nas a cue to shape, though a comprehensive analysis for curved surfaces ﬁrst appears in Gard-\ning (1992) and Malik and Rosenholtz (1997). The mathematics of occluding contours, and\nmore generally understanding the visual events in the projection of smooth curved objects,\nowes much to the work of Koenderink and van Doorn, which ﬁnds an extensive treatment in\nKoenderink’s (1990) Solid Shape. In recent years, attention has turned to treating the problem\nof shape and surface recovery from a single image as a probabilistic inference problem, where\ngeometrical cues are not modeled explicitly, but used implicitly in a learning framework. A\ngood representative is the work of Hoiem, Efros, and Hebert (2008).\nFor the reader interested in human vision, Palmer (1999) provides the best comprehen-\nsive treatment; Bruce et al. (2003) is a shorter textbook. The books by Hubel (1988) and\nRock (1984) are friendly introductions centered on neurophysiology and perception respec-\ntively. David Marr’s book Vision (Marr, 1982) played a historical role in connecting computer\nvision to psychophysics and neurobiology. While many of his speciﬁc models haven’t stood\nthe test of time, the theoretical perspective from which each task is analyzed at an informa-\ntional, computational, and implementation level is still illuminating.\nFor computer vision, the most comprehensive textbook is Forsyth and Ponce (2002).\nTrucco and Verri (1998) is a shorter account. Horn (1986) and Faugeras (1993) are two older\nand still useful textbooks.\nThe main journals for computer vision are IEEE Transactions on Pattern Analysis and\nMachine Intelligence and International Journal of Computer Vision. Computer vision con-\nferences include ICCV (International Conference on Computer Vision), CVPR (Computer\nVision and Pattern Recognition), and ECCV (European Conference on Computer Vision).\nResearch with a machine learning component is also published in the NIPS (Neural Informa-\ntion Processing Systems) conference, and work on the interface with computer graphics often",
  "Research with a machine learning component is also published in the NIPS (Neural Informa-\ntion Processing Systems) conference, and work on the interface with computer graphics often\nappears at the ACM SIGGRAPH (Special Interest Group in Graphics) conference. Exercises\n969\nEXERCISES\n24.1\nIn the shadow of a tree with a dense, leafy canopy, one sees a number of light spots.\nSurprisingly, they all appear to be circular. Why? After all, the gaps between the leaves\nthrough which the sun shines are not likely to be circular.\n24.2\nConsider a picture of a white sphere ﬂoating in front of a black backdrop. The im-\nage curve separating white pixels from black pixels is sometimes called the “outline” of the\nsphere. Show that the outline of a sphere, viewed in a perspective camera, can be an ellipse.\nWhy do spheres not look like ellipses to you?\n24.3\nConsider an inﬁnitely long cylinder of radius r oriented with its axis along the y-axis.\nThe cylinder has a Lambertian surface and is viewed by a camera along the positive z-axis.\nWhat will you expect to see in the image if the cylinder is illuminated by a point source\nat inﬁnity located on the positive x-axis? Draw the contours of constant brightness in the\nprojected image. Are the contours of equal brightness uniformly spaced?\n24.4\nEdges in an image can correspond to a variety of events in a scene. Consider Fig-\nure 24.4 (page 933), and assume that it is a picture of a real three-dimensional scene. Identify\nten different brightness edges in the image, and for each, state whether it corresponds to a\ndiscontinuity in (a) depth, (b) surface orientation, (c) reﬂectance, or (d) illumination.\n24.5\nA stereoscopic system is being contemplated for terrain mapping. It will consist of two\nCCD cameras, each having 512 × 512 pixels on a 10 cm × 10 cm square sensor. The lenses\nto be used have a focal length of 16 cm, with the focus ﬁxed at inﬁnity. For corresponding\npoints (u1, v1) in the left image and (u2, v2) in the right image, v1 = v2 because the x-axes\nin the two image planes are parallel to the epipolar lines—the lines from the object to the\ncamera. The optical axes of the two cameras are parallel. The baseline between the cameras\nis 1 meter.\na. If the nearest distance to be measured is 16 meters, what is the largest disparity that will\noccur (in pixels)?\nb. What is the distance resolution at 16 meters, due to the pixel spacing?\nc. What distance corresponds to a disparity of one pixel?\n24.6",
  "occur (in pixels)?\nb. What is the distance resolution at 16 meters, due to the pixel spacing?\nc. What distance corresponds to a disparity of one pixel?\n24.6\nWhich of the following are true, and which are false?\na. Finding corresponding points in stereo images is the easiest phase of the stereo depth-\nﬁnding process.\nb. Shape-from-texture can be done by projecting a grid of light-stripes onto the scene.\nc. Lines with equal lengths in the scene always project to equal lengths in the image.\nd. Straight lines in the image necessarily correspond to straight lines in the scene. 970\nChapter\n24.\nPerception\nE\nA\nB\nC\nX\nY\nD\nFigure 24.27\nTop view of a two-camera vision system observing a bottle with a wall\nbehind it.\n24.7\n(Courtesy of Pietro Perona.) Figure 24.27 shows two cameras at X and Y observing a\nscene. Draw the image seen at each camera, assuming that all named points are in the same\nhorizontal plane. What can be concluded from these two images about the relative distances\nof points A, B, C, D, and E from the camera baseline, and on what basis? 25\nROBOTICS\nIn which agents are endowed with physical effectors with which to do mischief.\n25.1\nINTRODUCTION\nRobots are physical agents that perform tasks by manipulating the physical world. To do so,\nROBOT\nthey are equipped with effectors such as legs, wheels, joints, and grippers. Effectors have\nEFFECTOR\na single purpose: to assert physical forces on the environment.1 Robots are also equipped\nwith sensors, which allow them to perceive their environment. Present day robotics em-\nSENSOR\nploys a diverse set of sensors, including cameras and lasers to measure the environment, and\ngyroscopes and accelerometers to measure the robot’s own motion.\nMost of today’s robots fall into one of three primary categories. Manipulators, or robot\nMANIPULATOR\narms (Figure 25.1(a)), are physically anchored to their workplace, for example in a factory\nassembly line or on the International Space Station. Manipulator motion usually involves\na chain of controllable joints, enabling such robots to place their effectors in any position\nwithin the workplace. Manipulators are by far the most common type of industrial robots,\nwith approximately one million units installed worldwide. Some mobile manipulators are\nused in hospitals to assist surgeons. Few car manufacturers could survive without robotic\nmanipulators, and some manipulators have even been used to generate original artwork.",
  "used in hospitals to assist surgeons. Few car manufacturers could survive without robotic\nmanipulators, and some manipulators have even been used to generate original artwork.\nThe second category is the mobile robot. Mobile robots move about their environment\nMOBILE ROBOT\nusing wheels, legs, or similar mechanisms. They have been put to use delivering food in\nhospitals, moving containers at loading docks, and similar tasks. Unmanned ground vehi-\ncles, or UGVs, drive autonomously on streets, highways, and off-road. The planetary rover\nUGV\nPLANETARY ROVER\nshown in Figure 25.2(b) explored Mars for a period of 3 months in 1997. Subsequent NASA\nrobots include the twin Mars Exploration Rovers (one is depicted on the cover of this book),\nwhich landed in 2003 and were still operating six years later. Other types of mobile robots\ninclude unmanned air vehicles (UAVs), commonly used for surveillance, crop-spraying, and\nUAV\n1 In Chapter 2 we talked about actuators, not effectors. Here we distinguish the effector (the physical device)\nfrom the actuator (the control line that communicates a command to the effector).\n971 972\nChapter\n25.\nRobotics\n(a)\n(b)\nFigure 25.1\n(a) An industrial robotic manipulator for stacking bags on a pallet. Image\ncourtesy of Nachi Robotic Systems. (b) Honda’s P3 and Asimo humanoid robots.\n(a)\n(b)\nFigure 25.2\n(a) Predator, an unmanned aerial vehicle (UAV) used by the U.S. Military.\nImage courtesy of General Atomics Aeronautical Systems. (b) NASA’s Sojourner, a mobile\nrobot that explored the surface of Mars in July 1997.\nmilitary operations. Figure 25.2(a) shows a UAV commonly used by the U.S. military. Au-\ntonomous underwater vehicles (AUVs) are used in deep sea exploration. Mobile robots\nAUV\ndeliver packages in the workplace and vacuum the ﬂoors at home.\nThe third type of robot combines mobility with manipulation, and is often called a\nmobile manipulator. Humanoid robots mimic the human torso. Figure 25.1(b) shows two\nMOBILE\nMANIPULATOR\nHUMANOID ROBOT\nearly humanoid robots, both manufactured by Honda Corp. in Japan. Mobile manipulators Section 25.2.\nRobot Hardware\n973\ncan apply their effectors further aﬁeld than anchored manipulators can, but their task is made\nharder because they don’t have the rigidity that the anchor provides.\nThe ﬁeld of robotics also includes prosthetic devices (artiﬁcial limbs, ears, and eyes\nfor humans), intelligent environments (such as an entire house that is equipped with sensors",
  "harder because they don’t have the rigidity that the anchor provides.\nThe ﬁeld of robotics also includes prosthetic devices (artiﬁcial limbs, ears, and eyes\nfor humans), intelligent environments (such as an entire house that is equipped with sensors\nand effectors), and multibody systems, wherein robotic action is achieved through swarms of\nsmall cooperating robots.\nReal robots must cope with environments that are partially observable, stochastic, dy-\nnamic, and continuous. Many robot environments are sequential and multiagent as well.\nPartial observability and stochasticity are the result of dealing with a large, complex world.\nRobot cameras cannot see around corners, and motion commands are subject to uncertainty\ndue to gears slipping, friction, etc. Also, the real world stubbornly refuses to operate faster\nthan real time. In a simulated environment, it is possible to use simple algorithms (such as the\nQ-learning algorithm described in Chapter 21) to learn in a few CPU hours from millions of\ntrials. In a real environment, it might take years to run these trials. Furthermore, real crashes\nreally hurt, unlike simulated ones. Practical robotic systems need to embody prior knowledge\nabout the robot, its physical environment, and the tasks that the robot will perform so that the\nrobot can learn quickly and perform safely.\nRobotics brings together many of the concepts we have seen earlier in the book, in-\ncluding probabilistic state estimation, perception, planning, unsupervised learning, and re-\ninforcement learning. For some of these concepts robotics serves as a challenging example\napplication. For other concepts this chapter breaks new ground in introducing the continuous\nversion of techniques that we previously saw only in the discrete case.\n25.2\nROBOT HARDWARE\nSo far in this book, we have taken the agent architecture—sensors, effectors, and processors—\nas given, and we have concentrated on the agent program. The success of real robots depends\nat least as much on the design of sensors and effectors that are appropriate for the task.\n25.2.1\nSensors\nSensors are the perceptual interface between robot and environment. Passive sensors, such\nPASSIVE SENSOR\nas cameras, are true observers of the environment: they capture signals that are generated by\nother sources in the environment. Active sensors, such as sonar, send energy into the envi-\nACTIVE SENSOR\nronment. They rely on the fact that this energy is reﬂected back to the sensor. Active sensors",
  "other sources in the environment. Active sensors, such as sonar, send energy into the envi-\nACTIVE SENSOR\nronment. They rely on the fact that this energy is reﬂected back to the sensor. Active sensors\ntend to provide more information than passive sensors, but at the expense of increased power\nconsumption and with a danger of interference when multiple active sensors are used at the\nsame time. Whether active or passive, sensors can be divided into three types, depending on\nwhether they sense the environment, the robot’s location, or the robot’s internal conﬁguration.\nRange ﬁnders are sensors that measure the distance to nearby objects. In the early\nRANGE FINDER\ndays of robotics, robots were commonly equipped with sonar sensors. Sonar sensors emit\nSONAR SENSORS\ndirectional sound waves, which are reﬂected by objects, with some of the sound making it 974\nChapter\n25.\nRobotics\n(a)\n(b)\nFigure 25.3\n(a) Time of ﬂight camera; image courtesy of Mesa Imaging GmbH. (b) 3D\nrange image obtained with this camera. The range image makes it possible to detect obstacles\nand objects in a robot’s vicinity.\nback into the sensor. The time and intensity of the returning signal indicates the distance\nto nearby objects. Sonar is the technology of choice for autonomous underwater vehicles.\nStereo vision (see Section 24.4.2) relies on multiple cameras to image the environment from\nSTEREO VISION\nslightly different viewpoints, analyzing the resulting parallax in these images to compute the\nrange of surrounding objects. For mobile ground robots, sonar and stereo vision are now\nrarely used, because they are not reliably accurate.\nMost ground robots are now equipped with optical range ﬁnders. Just like sonar sensors,\noptical range sensors emit active signals (light) and measure the time until a reﬂection of this\nsignal arrives back at the sensor. Figure 25.3(a) shows a time of ﬂight camera. This camera\nTIME OF FLIGHT\nCAMERA\nacquires range images like the one shown in Figure 25.3(b) at up to 60 frames per second.\nOther range sensors use laser beams and special 1-pixel cameras that can be directed using\ncomplex arrangements of mirrors or rotating elements. These sensors are called scanning\nlidars (short for light detection and ranging). Scanning lidars tend to provide longer ranges\nSCANNING LIDARS\nthan time of ﬂight cameras, and tend to perform better in bright daylight.\nOther common range sensors include radar, which is often the sensor of choice for",
  "SCANNING LIDARS\nthan time of ﬂight cameras, and tend to perform better in bright daylight.\nOther common range sensors include radar, which is often the sensor of choice for\nUAVs. Radar sensors can measure distances of multiple kilometers. On the other extreme\nend of range sensing are tactile sensors such as whiskers, bump panels, and touch-sensitive\nTACTILE SENSORS\nskin. These sensors measure range based on physical contact, and can be deployed only for\nsensing objects very close to the robot.\nA second important class of sensors is location sensors. Most location sensors use\nLOCATION SENSORS\nrange sensing as a primary component to determine location. Outdoors, the Global Position-\ning System (GPS) is the most common solution to the localization problem. GPS measures\nGLOBAL\nPOSITIONING\nSYSTEM\nthe distance to satellites that emit pulsed signals. At present, there are 31 satellites in orbit,\ntransmitting signals on multiple frequencies. GPS receivers can recover the distance to these\nsatellites by analyzing phase shifts. By triangulating signals from multiple satellites, GPS Section 25.2.\nRobot Hardware\n975\nreceivers can determine their absolute location on Earth to within a few meters. Differential\nGPS involves a second ground receiver with known location, providing millimeter accuracy\nDIFFERENTIAL GPS\nunder ideal conditions. Unfortunately, GPS does not work indoors or underwater. Indoors,\nlocalization is often achieved by attaching beacons in the environment at known locations.\nMany indoor environments are full of wireless base stations, which can help robots localize\nthrough the analysis of the wireless signal. Underwater, active sonar beacons can provide a\nsense of location, using sound to inform AUVs of their relative distances to those beacons.\nThe third important class is proprioceptive sensors, which inform the robot of its own\nPROPRIOCEPTIVE\nSENSOR\nmotion. To measure the exact conﬁguration of a robotic joint, motors are often equipped\nwith shaft decoders that count the revolution of motors in small increments. On robot arms,\nSHAFT DECODER\nshaft decoders can provide accurate information over any period of time. On mobile robots,\nshaft decoders that report wheel revolutions can be used for odometry—the measurement of\nODOMETRY\ndistance traveled. Unfortunately, wheels tend to drift and slip, so odometry is accurate only\nover short distances. External forces, such as the current for AUVs and the wind for UAVs,",
  "ODOMETRY\ndistance traveled. Unfortunately, wheels tend to drift and slip, so odometry is accurate only\nover short distances. External forces, such as the current for AUVs and the wind for UAVs,\nincrease positional uncertainty. Inertial sensors, such as gyroscopes, rely on the resistance\nINERTIAL SENSOR\nof mass to the change of velocity. They can help reduce uncertainty.\nOther important aspects of robot state are measured by force sensors and torque sen-\nFORCE SENSOR\nsors. These are indispensable when robots handle fragile objects or objects whose exact shape\nTORQUE SENSOR\nand location is unknown. Imagine a one-ton robotic manipulator screwing in a light bulb. It\nwould be all too easy to apply too much force and break the bulb. Force sensors allow the\nrobot to sense how hard it is gripping the bulb, and torque sensors allow it to sense how hard\nit is turning. Good sensors can measure forces in all three translational and three rotational\ndirections. They do this at a frequency of several hundred times a second, so that a robot can\nquickly detect unexpected forces and correct its actions before it breaks a light bulb.\n25.2.2\nEffectors\nEffectors are the means by which robots move and change the shape of their bodies. To\nunderstand the design of effectors, it will help to talk about motion and shape in the abstract,\nusing the concept of a degree of freedom (DOF) We count one degree of freedom for each\nDEGREE OF\nFREEDOM\nindependent direction in which a robot, or one of its effectors, can move. For example, a rigid\nmobile robot such as an AUV has six degrees of freedom, three for its (x, y, z) location in\nspace and three for its angular orientation, known as yaw, roll, and pitch. These six degrees\ndeﬁne the kinematic state2 or pose of the robot. The dynamic state of a robot includes these\nKINEMATIC STATE\nPOSE\nDYNAMIC STATE\nsix plus an additional six dimensions for the rate of change of each kinematic dimension, that\nis, their velocities.\nFor nonrigid bodies, there are additional degrees of freedom within the robot itself. For\nexample, the elbow of a human arm possesses two degree of freedom. It can ﬂex the upper\narm towards or away, and can rotate right or left. The wrist has three degrees of freedom. It\ncan move up and down, side to side, and can also rotate. Robot joints also have one, two,\nor three degrees of freedom each. Six degrees of freedom are required to place an object,",
  "can move up and down, side to side, and can also rotate. Robot joints also have one, two,\nor three degrees of freedom each. Six degrees of freedom are required to place an object,\nsuch as a hand, at a particular point in a particular orientation. The arm in Figure 25.4(a)\n2 “Kinematic” is from the Greek word for motion, as is “cinema.” 976\nChapter\n25.\nRobotics\nhas exactly six degrees of freedom, created by ﬁve revolute joints that generate rotational\nREVOLUTE JOINT\nmotion and one prismatic joint that generates sliding motion. You can verify that the human\nPRISMATIC JOINT\narm as a whole has more than six degrees of freedom by a simple experiment: put your hand\non the table and notice that you still have the freedom to rotate your elbow without changing\nthe conﬁguration of your hand. Manipulators that have extra degrees of freedom are easier to\ncontrol than robots with only the minimum number of DOFs. Many industrial manipulators\ntherefore have seven DOFs, not six.\nR\nR\nR\nP\nR\nR\nθ\n(x, y)\n(a)\n(b)\nFigure 25.4\n(a) The Stanford Manipulator, an early robot arm with ﬁve revolute joints (R)\nand one prismatic joint (P), for a total of six degrees of freedom. (b) Motion of a nonholo-\nnomic four-wheeled vehicle with front-wheel steering.\nFor mobile robots, the DOFs are not necessarily the same as the number of actuated ele-\nments. Consider, for example, your average car: it can move forward or backward, and it can\nturn, giving it two DOFs. In contrast, a car’s kinematic conﬁguration is three-dimensional:\non an open ﬂat surface, one can easily maneuver a car to any (x, y) point, in any orientation.\n(See Figure 25.4(b).) Thus, the car has three effective degrees of freedom but two control-\nEFFECTIVE DOF\nlable degrees of freedom. We say a robot is nonholonomic if it has more effective DOFs\nCONTROLLABLE DOF\nNONHOLONOMIC\nthan controllable DOFs and holonomic if the two numbers are the same. Holonomic robots\nare easier to control—it would be much easier to park a car that could move sideways as well\nas forward and backward—but holonomic robots are also mechanically more complex. Most\nrobot arms are holonomic, and most mobile robots are nonholonomic.\nMobile robots have a range of mechanisms for locomotion, including wheels, tracks,\nand legs. Differential drive robots possess two independently actuated wheels (or tracks),\nDIFFERENTIAL DRIVE\none on each side, as on a military tank. If both wheels move at the same velocity, the robot",
  "and legs. Differential drive robots possess two independently actuated wheels (or tracks),\nDIFFERENTIAL DRIVE\none on each side, as on a military tank. If both wheels move at the same velocity, the robot\nmoves on a straight line. If they move in opposite directions, the robot turns on the spot. An\nalternative is the synchro drive, in which each wheel can move and turn around its own axis.\nSYNCHRO DRIVE\nTo avoid chaos, the wheels are tightly coordinated. When moving straight, for example, all\nwheels point in the same direction and move at the same speed. Both differential and synchro\ndrives are nonholonomic. Some more expensive robots use holonomic drives, which have\nthree or more wheels that can be oriented and moved independently.\nSome mobile robots possess arms. Figure 25.5(a) displays a two-armed robot. This\nrobot’s arms use springs to compensate for gravity, and they provide minimal resistance to Section 25.2.\nRobot Hardware\n977\n(a)\n(b)\nFigure 25.5\n(a) Mobile manipulator plugging its charge cable into a wall outlet. Image\ncourtesy of Willow Garage, c⃝2009. (b) One of Marc Raibert’s legged robots in motion.\nexternal forces. Such a design minimizes the physical danger to people who might stumble\ninto such a robot. This is a key consideration in deploying robots in domestic environments.\nLegs, unlike wheels, can handle rough terrain. However, legs are notoriously slow on\nﬂat surfaces, and they are mechanically difﬁcult to build. Robotics researchers have tried de-\nsigns ranging from one leg up to dozens of legs. Legged robots have been made to walk, run,\nand even hop—as we see with the legged robot in Figure 25.5(b). This robot is dynamically\nstable, meaning that it can remain upright while hopping around. A robot that can remain\nDYNAMICALLY\nSTABLE\nupright without moving its legs is called statically stable. A robot is statically stable if its\nSTATICALLY STABLE\ncenter of gravity is above the polygon spanned by its legs. The quadruped (four-legged) robot\nshown in Figure 25.6(a) may appear statically stable. However, it walks by lifting multiple\nlegs at the same time, which renders it dynamically stable. The robot can walk on snow and\nice, and it will not fall over even if you kick it (as demonstrated in videos available online).\nTwo-legged robots such as those in Figure 25.6(b) are dynamically stable.\nOther methods of movement are possible: air vehicles use propellers or turbines; un-",
  "Two-legged robots such as those in Figure 25.6(b) are dynamically stable.\nOther methods of movement are possible: air vehicles use propellers or turbines; un-\nderwater vehicles use propellers or thrusters, similar to those used on submarines. Robotic\nblimps rely on thermal effects to keep themselves aloft.\nSensors and effectors alone do not make a robot. A complete robot also needs a source\nof power to drive its effectors. The electric motor is the most popular mechanism for both\nELECTRIC MOTOR\nmanipulator actuation and locomotion, but pneumatic actuation using compressed gas and\nPNEUMATIC\nACTUATION\nhydraulic actuation using pressurized ﬂuids also have their application niches.\nHYDRAULIC\nACTUATION 978\nChapter\n25.\nRobotics\n(a)\n(b)\nFigure 25.6\n(a) Four-legged dynamically-stable robot “Big Dog.” Image courtesy Boston\nDynamics, c⃝2009. (b) 2009 RoboCup Standard Platform League competition, showing the\nwinning team, B-Human, from the DFKI center at the University of Bremen. Throughout the\nmatch, B-Human outscored their opponents 64:1. Their success was built on probabilistic\nstate estimation using particle ﬁlters and Kalman ﬁlters; on machine-learning models for gait\noptimization; and on dynamic kicking moves. Image courtesy DFKI, c⃝2009.\n25.3\nROBOTIC PERCEPTION\nPerception is the process by which robots map sensor measurements into internal representa-\ntions of the environment. Perception is difﬁcult because sensors are noisy, and the environ-\nment is partially observable, unpredictable, and often dynamic. In other words, robots have\nall the problems of state estimation (or ﬁltering) that we discussed in Section 15.2. As a\nrule of thumb, good internal representations for robots have three properties: they contain\nenough information for the robot to make good decisions, they are structured so that they can\nbe updated efﬁciently, and they are natural in the sense that internal variables correspond to\nnatural state variables in the physical world.\nIn Chapter 15, we saw that Kalman ﬁlters, HMMs, and dynamic Bayes nets can repre-\nsent the transition and sensor models of a partially observable environment, and we described\nboth exact and approximate algorithms for updating the belief state—the posterior probabil-\nity distribution over the environment state variables. Several dynamic Bayes net models for\nthis process were shown in Chapter 15. For robotics problems, we include the robot’s own",
  "ity distribution over the environment state variables. Several dynamic Bayes net models for\nthis process were shown in Chapter 15. For robotics problems, we include the robot’s own\npast actions as observed variables in the model. Figure 25.7 shows the notation used in this\nchapter: Xt is the state of the environment (including the robot) at time t, Zt is the observation\nreceived at time t, and At is the action taken after the observation is received. Section 25.3.\nRobotic Perception\n979\nXt+1\nXt\nAt−2\nAt−1\nAt\nZt−1\nXt−1\nZt\nZt+1\nFigure 25.7\nRobot perception can be viewed as temporal inference from sequences of\nactions and measurements, as illustrated by this dynamic Bayes network.\nWe would like to compute the new belief state, P(Xt+1 | z1:t+1, a1:t), from the current\nbelief state P(Xt | z1:t, a1:t−1) and the new observation zt+1. We did this in Section 15.2,\nbut here there are two differences: we condition explicitly on the actions as well as the ob-\nservations, and we deal with continuous rather than discrete variables. Thus, we modify the\nrecursive ﬁltering equation (15.5 on page 572) to use integration rather than summation:\nP(Xt+1 | z1:t+1, a1:t)\n= αP(zt+1 | Xt+1)\n\u001a\nP(Xt+1 | xt, at) P(xt | z1:t, a1:t−1) dxt .\n(25.1)\nThis equation states that the posterior over the state variables X at time t + 1 is calculated\nrecursively from the corresponding estimate one time step earlier. This calculation involves\nthe previous action at and the current sensor measurement zt+1. For example, if our goal\nis to develop a soccer-playing robot, Xt+1 might be the location of the soccer ball relative\nto the robot. The posterior P(Xt | z1:t, a1:t−1) is a probability distribution over all states that\ncaptures what we know from past sensor measurements and controls. Equation (25.1) tells us\nhow to recursively estimate this location, by incrementally folding in sensor measurements\n(e.g., camera images) and robot motion commands. The probability P(Xt+1 | xt, at) is called\nthe transition model or motion model, and P(zt+1 | Xt+1) is the sensor model.\nMOTION MODEL\n25.3.1\nLocalization and mapping\nLocalization is the problem of ﬁnding out where things are—including the robot itself.\nLOCALIZATION\nKnowledge about where things are is at the core of any successful physical interaction with\nthe environment. For example, robot manipulators must know the location of objects they\nseek to manipulate; navigating robots must know where they are to ﬁnd their way around.",
  "the environment. For example, robot manipulators must know the location of objects they\nseek to manipulate; navigating robots must know where they are to ﬁnd their way around.\nTo keep things simple, let us consider a mobile robot that moves slowly in a ﬂat 2D\nworld. Let us also assume the robot is given an exact map of the environment. (An example\nof such a map appears in Figure 25.10.) The pose of such a mobile robot is deﬁned by its\ntwo Cartesian coordinates with values x and y and its heading with value θ, as illustrated in\nFigure 25.8(a). If we arrange those three values in a vector, then any particular state is given\nby Xt = (xt, yt, θt)⊤. So far so good. 980\nChapter\n25.\nRobotics\nxi, yi\nvt Δt\nxt+1\nh(xt)\nxt\nθt\nt+1\nθ\nt Δt\nω\nZ1\nZ2\nZ3\nZ4\n(a)\n(b)\nFigure 25.8\n(a) A simpliﬁed kinematic model of a mobile robot. The robot is shown as a\ncircle with an interior line marking the forward direction. The state xt consists of the (xt, yt)\nposition (shown implicitly) and the orientation θt. The new state xt+1 is obtained by an\nupdate in position of vtΔt and in orientation of ωtΔt. Also shown is a landmark at (xi, yi)\nobserved at time t. (b) The range-scan sensor model. Two possible robot poses are shown for\na given range scan (z1, z2, z3, z4). It is much more likely that the pose on the left generated\nthe range scan than the pose on the right.\nIn the kinematic approximation, each action consists of the “instantaneous” speciﬁca-\ntion of two velocities—a translational velocity vt and a rotational velocity ωt. For small time\nintervals Δt, a crude deterministic model of the motion of such robots is given by\nˆXt+1 = f(Xt, vt, ωt\n\u001b \u001c\u001d \u001e\nat\n) = Xt +\n⎛\n⎝\nvtΔt cos θt\nvtΔt sin θt\nωtΔt\n⎞\n⎠.\nThe notation ˆX refers to a deterministic state prediction.\nOf course, physical robots are\nsomewhat unpredictable. This is commonly modeled by a Gaussian distribution with mean\nf(Xt, vt, ωt) and covariance Σx. (See Appendix A for a mathematical deﬁnition.)\nP(Xt+1 | Xt, vt, ωt) = N(ˆXt+1, Σx) .\nThis probability distribution is the robot’s motion model. It models the effects of the motion\nat on the location of the robot.\nNext, we need a sensor model. We will consider two kinds of sensor model. The\nﬁrst assumes that the sensors detect stable, recognizable features of the environment called\nlandmarks. For each landmark, the range and bearing are reported. Suppose the robot’s state\nLANDMARK\nis xt = (xt, yt, θt)⊤and it senses a landmark whose location is known to be (xi, yi)⊤. Without",
  "landmarks. For each landmark, the range and bearing are reported. Suppose the robot’s state\nLANDMARK\nis xt = (xt, yt, θt)⊤and it senses a landmark whose location is known to be (xi, yi)⊤. Without\nnoise, the range and bearing can be calculated by simple geometry. (See Figure 25.8(a).) The\nexact prediction of the observed range and bearing would be\nˆzt = h(xt) =\n\r \n(xt −xi)2 + (yt −yi)2\narctan yi−yt\nxi−xt −θt\n\u000e\n. Section 25.3.\nRobotic Perception\n981\nAgain, noise distorts our measurements. To keep things simple, one might assume Gaussian\nnoise with covariance Σz, giving us the sensor model\nP(zt | xt) = N(ˆzt, Σz) .\nA somewhat different sensor model is used for an array of range sensors, each of which\nhas a ﬁxed bearing relative to the robot. Such sensors produce a vector of range values\nzt = (z1, . . . , zM)⊤. Given a pose xt, let ˆzj be the exact range along the jth beam direction\nfrom xt to the nearest obstacle. As before, this will be corrupted by Gaussian noise. Typically,\nwe assume that the errors for the different beam directions are independent and identically\ndistributed, so we have\nP(zt | xt) = α\nM\n\u0019\nj = 1\ne−(zj−ˆzj)/2σ2.\nFigure 25.8(b) shows an example of a four-beam range scan and two possible robot poses,\none of which is reasonably likely to have produced the observed scan and one of which is not.\nComparing the range-scan model to the landmark model, we see that the range-scan model\nhas the advantage that there is no need to identify a landmark before the range scan can be\ninterpreted; indeed, in Figure 25.8(b), the robot faces a featureless wall. On the other hand,\nif there are visible, identiﬁable landmarks, they may provide instant localization.\nChapter 15 described the Kalman ﬁlter, which represents the belief state as a single\nmultivariate Gaussian, and the particle ﬁlter, which represents the belief state by a collection\nof particles that correspond to states. Most modern localization algorithms use one of two\nrepresentations of the robot’s belief P(Xt | z1:t, a1:t−1).\nLocalization using particle ﬁltering is called Monte Carlo localization, or MCL. The\nMONTE CARLO\nLOCALIZATION\nMCL alfgorithm is an instance of the particle-ﬁltering algorithm of Figure 15.17 (page 598).\nAll we need to do is supply the appropriate motion model and sensor model. Figure 25.9\nshows one version using the range-scan model. The operation of the algorithm is illustrated in\nFigure 25.10 as the robot ﬁnds out where it is inside an ofﬁce building. In the ﬁrst image, the",
  "shows one version using the range-scan model. The operation of the algorithm is illustrated in\nFigure 25.10 as the robot ﬁnds out where it is inside an ofﬁce building. In the ﬁrst image, the\nparticles are uniformly distributed based on the prior, indicating global uncertainty about the\nrobot’s position. In the second image, the ﬁrst set of measurements arrives and the particles\nform clusters in the areas of high posterior belief. In the third, enough measurements are\navailable to push all the particles to a single location.\nThe Kalman ﬁlter is the other major way to localize. A Kalman ﬁlter represents the\nposterior P(Xt | z1:t, a1:t−1) by a Gaussian. The mean of this Gaussian will be denoted μt and\nits covariance Σt. The main problem with Gaussian beliefs is that they are only closed under\nlinear motion models f and linear measurement models h. For nonlinear f or h, the result of\nupdating a ﬁlter is in general not Gaussian. Thus, localization algorithms using the Kalman\nﬁlter linearize the motion and sensor models. Linearization is a local approximation of a\nLINEARIZATION\nnonlinear function by a linear function. Figure 25.11 illustrates the concept of linearization\nfor a (one-dimensional) robot motion model. On the left, it depicts a nonlinear motion model\nf(xt, at) (the control at is omitted in this graph since it plays no role in the linearization).\nOn the right, this function is approximated by a linear function ˜f(xt, at). This linear function\nis tangent to f at the point μt, the mean of our state estimate at time t. Such a linearization 982\nChapter\n25.\nRobotics\nfunction MONTE-CARLO-LOCALIZATION(a, z, N , P(X′|X, v, ω), P(z|z∗), m) returns\na set of samples for the next time step\ninputs: a, robot velocities v and ω\nz, range scan z1, . . . , zM\nP(X′|X, v, ω), motion model\nP(z|z∗), range sensor noise model\nm, 2D map of the environment\npersistent: S, a vector of samples of size N\nlocal variables: W, a vector of weights of size N\nS′, a temporary vector of particles of size N\nW ′, a vector of weights of size N\nif S is empty then\n/* initialization phase */\nfor i = 1 to N do\nS[i] ←sample from P(X0)\nfor i = 1 to N do\n/* update cycle */\nS′[i] ←sample from P(X′|X = S[i], v, ω)\nW ′[i] ←1\nfor j = 1 to M do\nz∗←RAYCAST(j, X = S′[i], m)\nW ′[i] ←W ′[i] · P(zj| z∗)\nS ←WEIGHTED-SAMPLE-WITH-REPLACEMENT(N ,S ′,W ′)\nreturn S\nFigure 25.9\nA Monte Carlo localization algorithm using a range-scan sensor model with\nindependent noise.",
  "W ′[i] ←1\nfor j = 1 to M do\nz∗←RAYCAST(j, X = S′[i], m)\nW ′[i] ←W ′[i] · P(zj| z∗)\nS ←WEIGHTED-SAMPLE-WITH-REPLACEMENT(N ,S ′,W ′)\nreturn S\nFigure 25.9\nA Monte Carlo localization algorithm using a range-scan sensor model with\nindependent noise.\nis called (ﬁrst degree) Taylor expansion. A Kalman ﬁlter that linearizes f and h via Taylor\nTAYLOR EXPANSION\nexpansion is called an extended Kalman ﬁlter (or EKF). Figure 25.12 shows a sequence\nof estimates of a robot running an extended Kalman ﬁlter localization algorithm. As the\nrobot moves, the uncertainty in its location estimate increases, as shown by the error ellipses.\nIts error decreases as it senses the range and bearing to a landmark with known location\nand increases again as the robot loses sight of the landmark. EKF algorithms work well if\nlandmarks are easily identiﬁed. Otherwise, the posterior distribution may be multimodal, as\nin Figure 25.10(b). The problem of needing to know the identity of landmarks is an instance\nof the data association problem discussed in Figure 15.6.\nIn some situations, no map of the environment is available. Then the robot will have to\nacquire a map. This is a bit of a chicken-and-egg problem: the navigating robot will have to\ndetermine its location relative to a map it doesn’t quite know, at the same time building this\nmap while it doesn’t quite know its actual location. This problem is important for many robot\napplications, and it has been studied extensively under the name simultaneous localization\nand mapping, abbreviated as SLAM.\nSIMULTANEOUS\nLOCALIZATION AND\nMAPPING\nSLAM problems are solved using many different probabilistic techniques, including\nthe extended Kalman ﬁlter discussed above. Using the EKF is straightforward: just augment Section 25.3.\nRobotic Perception\n983\nRobot position\n(a)\nRobot position\n(b)\nRobot position\n(c)\nFigure 25.10\nMonte Carlo localization, a particle ﬁltering algorithm for mobile robot lo-\ncalization. (a) Initial, global uncertainty. (b) Approximately bimodal uncertainty after navi-\ngating in the (symmetric) corridor. (c) Unimodal uncertainty after entering a room and ﬁnding\nit to be distinctive. 984\nChapter\n25.\nRobotics\nXt+1\nXt\nμt\nΣt\nf(Xt, at)\nf(μt, at)\nΣt+1\nXt+1\nXt\nμt\nΣt\nf(Xt, at)\nf(μt, at)\nΣt+1\nΣt+1\n~\nf(Xt, at) = f(μt, at) + Ft(Xt − μt)\n~\n(a)\n(b)\nFigure 25.11\nOne-dimensional illustration of a linearized motion model: (a) The function\nf, and the projection of a mean μt and a covariance interval (based on Σt) into time t + 1.",
  "f(μt, at)\nΣt+1\nΣt+1\n~\nf(Xt, at) = f(μt, at) + Ft(Xt − μt)\n~\n(a)\n(b)\nFigure 25.11\nOne-dimensional illustration of a linearized motion model: (a) The function\nf, and the projection of a mean μt and a covariance interval (based on Σt) into time t + 1.\n(b) The linearized version is the tangent of f at μt. The projection of the mean μt is correct.\nHowever, the projected covariance ˜Σt+1 differs from Σt+1.\nrobot\nlandmark\nFigure 25.12\nExample of localization using the extended Kalman ﬁlter. The robot moves\non a straight line. As it progresses, its uncertainty increases gradually, as illustrated by the\nerror ellipses. When it observes a landmark with known position, the uncertainty is reduced.\nthe state vector to include the locations of the landmarks in the environment. Luckily, the\nEKF update scales quadratically, so for small maps (e.g., a few hundred landmarks) the com-\nputation is quite feasible. Richer maps are often obtained using graph relaxation methods,\nsimilar to the Bayesian network inference techniques discussed in Chapter 14. Expectation–\nmaximization is also used for SLAM.\n25.3.2\nOther types of perception\nNot all of robot perception is about localization or mapping. Robots also perceive the tem-\nperature, odors, acoustic signals, and so on. Many of these quantities can be estimated using\nvariants of dynamic Bayes networks. All that is required for such estimators are conditional\nprobability distributions that characterize the evolution of state variables over time, and sen-\nsor models that describe the relation of measurements to state variables.\nIt is also possible to program a robot as a reactive agent, without explicitly reasoning\nabout probability distributions over states. We cover that approach in Section 25.6.3.\nThe trend in robotics is clearly towards representations with well-deﬁned semantics. Section 25.3.\nRobotic Perception\n985\n(a)\n(b)\n(c)\nFigure 25.13\nSequence of “drivable surface” classiﬁer results using adaptive vision. In\n(a) only the road is classiﬁed as drivable (striped area). The V-shaped dark line shows where\nthe vehicle is heading. In (b) the vehicle is commanded to drive off the road, onto a grassy\nsurface, and the classiﬁer is beginning to classify some of the grass as drivable. In (c) the\nvehicle has updated its model of drivable surface to correspond to grass as well as road.\nProbabilistic techniques outperform other approaches in many hard perceptual problems such",
  "vehicle has updated its model of drivable surface to correspond to grass as well as road.\nProbabilistic techniques outperform other approaches in many hard perceptual problems such\nas localization and mapping. However, statistical techniques are sometimes too cumbersome,\nand simpler solutions may be just as effective in practice. To help decide which approach to\ntake, experience working with real physical robots is your best teacher.\n25.3.3\nMachine learning in robot perception\nMachine learning plays an important role in robot perception. This is particularly the case\nwhen the best internal representation is not known. One common approach is to map high-\ndimensional sensor streams into lower-dimensional spaces using unsupervised machine learn-\ning methods (see Chapter 18). Such an approach is called low-dimensional embedding.\nLOW-DIMENSIONAL\nEMBEDDING\nMachine learning makes it possible to learn sensor and motion models from data, while si-\nmultaneously discovering a suitable internal representations.\nAnother machine learning technique enables robots to continuously adapt to broad\nchanges in sensor measurements. Picture yourself walking from a sun-lit space into a dark\nneon-lit room. Clearly things are darker inside. But the change of light source also affects all\nthe colors: Neon light has a stronger component of green light than sunlight. Yet somehow\nwe seem not to notice the change. If we walk together with people into a neon-lit room, we\ndon’t think that suddenly their faces turned green. Our perception quickly adapts to the new\nlighting conditions, and our brain ignores the differences.\nAdaptive perception techniques enable robots to adjust to such changes. One example\nis shown in Figure 25.13, taken from the autonomous driving domain. Here an unmanned\nground vehicle adapts its classiﬁer of the concept “drivable surface.” How does this work?\nThe robot uses a laser to provide classiﬁcation for a small area right in front of the robot.\nWhen this area is found to be ﬂat in the laser range scan, it is used as a positive training\nexample for the concept “drivable surface.” A mixture-of-Gaussians technique similar to the\nEM algorithm discussed in Chapter 20 is then trained to recognize the speciﬁc color and\ntexture coefﬁcients of the small sample patch. The images in Figure 25.13 are the result of\napplying this classiﬁer to the full image. 986\nChapter\n25.\nRobotics\nMethods that make robots collect their own training data (with labels!) are called self-",
  "texture coefﬁcients of the small sample patch. The images in Figure 25.13 are the result of\napplying this classiﬁer to the full image. 986\nChapter\n25.\nRobotics\nMethods that make robots collect their own training data (with labels!) are called self-\nsupervised. In this instance, the robot uses machine learning to leverage a short-range sensor\nSELF-SUPERVISED\nLEARNING\nthat works well for terrain classiﬁcation into a sensor that can see much farther. That allows\nthe robot to drive faster, slowing down only when the sensor model says there is a change in\nthe terrain that needs to be examined more carefully by the short-range sensors.\n25.4\nPLANNING TO MOVE\nAll of a robot’s deliberations ultimately come down to deciding how to move effectors. The\npoint-to-point motion problem is to deliver the robot or its end effector to a designated target\nPOINT-TO-POINT\nMOTION\nlocation. A greater challenge is the compliant motion problem, in which a robot moves\nCOMPLIANT MOTION\nwhile being in physical contact with an obstacle. An example of compliant motion is a robot\nmanipulator that screws in a light bulb, or a robot that pushes a box across a table top.\nWe begin by ﬁnding a suitable representation in which motion-planning problems can\nbe described and solved. It turns out that the conﬁguration space—the space of robot states\ndeﬁned by location, orientation, and joint angles—is a better place to work than the original\n3D space. The path planning problem is to ﬁnd a path from one conﬁguration to another in\nPATH PLANNING\nconﬁguration space. We have already encountered various versions of the path-planning prob-\nlem throughout this book; the complication added by robotics is that path planning involves\ncontinuous spaces. There are two main approaches: cell decomposition and skeletonization.\nEach reduces the continuous path-planning problem to a discrete graph-search problem. In\nthis section, we assume that motion is deterministic and that localization of the robot is exact.\nSubsequent sections will relax these assumptions.\n25.4.1\nConﬁguration space\nWe will start with a simple representation for a simple robot motion problem. Consider the\nrobot arm shown in Figure 25.14(a). It has two joints that move independently. Moving\nthe joints alters the (x, y) coordinates of the elbow and the gripper. (The arm cannot move\nin the z direction.) This suggests that the robot’s conﬁguration can be described by a four-",
  "the joints alters the (x, y) coordinates of the elbow and the gripper. (The arm cannot move\nin the z direction.) This suggests that the robot’s conﬁguration can be described by a four-\ndimensional coordinate: (xe, ye) for the location of the elbow relative to the environment and\n(xg, yg) for the location of the gripper. Clearly, these four coordinates characterize the full\nstate of the robot. They constitute what is known as workspace representation, since the\nWORKSPACE\nREPRESENTATION\ncoordinates of the robot are speciﬁed in the same coordinate system as the objects it seeks to\nmanipulate (or to avoid). Workspace representations are well-suited for collision checking,\nespecially if the robot and all objects are represented by simple polygonal models.\nThe problem with the workspace representation is that not all workspace coordinates\nare actually attainable, even in the absence of obstacles. This is because of the linkage con-\nstraints on the space of attainable workspace coordinates. For example, the elbow position\nLINKAGE\nCONSTRAINTS\n(xe, ye) and the gripper position (xg, yg) are always a ﬁxed distance apart, because they are\njoined by a rigid forearm. A robot motion planner deﬁned over workspace coordinates faces\nthe challenge of generating paths that adhere to these constraints. This is particularly tricky Section 25.4.\nPlanning to Move\n987\nshou\nelb\ns\ne\ne\ntable\ntable\nleft wall\nvertical\nobstacle\ns\ne\ns\n(a)\n(b)\nFigure 25.14\n(a) Workspace representation of a robot arm with 2 DOFs. The workspace\nis a box with a ﬂat obstacle hanging from the ceiling. (b) Conﬁguration space of the same\nrobot. Only white regions in the space are conﬁgurations that are free of collisions. The dot\nin this diagram corresponds to the conﬁguration of the robot shown on the left.\nbecause the state space is continuous and the constraints are nonlinear. It turns out to be eas-\nier to plan with a conﬁguration space representation. Instead of representing the state of the\nCONFIGURATION\nSPACE\nrobot by the Cartesian coordinates of its elements, we represent the state by a conﬁguration\nof the robot’s joints. Our example robot possesses two joints. Hence, we can represent its\nstate with the two angles ϕs and ϕe for the shoulder joint and elbow joint, respectively. In\nthe absence of any obstacles, a robot could freely take on any value in conﬁguration space. In\nparticular, when planning a path one could simply connect the present conﬁguration and the",
  "the absence of any obstacles, a robot could freely take on any value in conﬁguration space. In\nparticular, when planning a path one could simply connect the present conﬁguration and the\ntarget conﬁguration by a straight line. In following this path, the robot would then move its\njoints at a constant velocity, until a target location is reached.\nUnfortunately, conﬁguration spaces have their own problems. The task of a robot is usu-\nally expressed in workspace coordinates, not in conﬁguration space coordinates. This raises\nthe question of how to map between workspace coordinates and conﬁguration space. Trans-\nforming conﬁguration space coordinates into workspace coordinates is simple: it involves\na series of straightforward coordinate transformations. These transformations are linear for\nprismatic joints and trigonometric for revolute joints. This chain of coordinate transformation\nis known as kinematics.\nKINEMATICS\nThe inverse problem of calculating the conﬁguration of a robot whose effector location\nis speciﬁed in workspace coordinates is known as inverse kinematics. Calculating the inverse\nINVERSE\nKINEMATICS\nkinematics is hard, especially for robots with many DOFs. In particular, the solution is seldom\nunique. Figure 25.14(a) shows one of two possible conﬁgurations that put the gripper in the\nsame location. (The other conﬁguration would has the elbow below the shoulder.) 988\nChapter\n25.\nRobotics\nconf-3\nconf-1\nconf-2\nconf-3\nconf-2\nconf-1\ne\ns\ne\n(a)\n(b)\nFigure 25.15\nThree robot conﬁgurations, shown in workspace and conﬁguration space.\nIn general, this two-link robot arm has between zero and two inverse kinematic solu-\ntions for any set of workspace coordinates. Most industrial robots have sufﬁcient degrees\nof freedom to ﬁnd inﬁnitely many solutions to motion problems. To see how this is possi-\nble, simply imagine that we added a third revolute joint to our example robot, one whose\nrotational axis is parallel to the ones of the existing joints. In such a case, we can keep the\nlocation (but not the orientation!) of the gripper ﬁxed and still freely rotate its internal joints,\nfor most conﬁgurations of the robot. With a few more joints (how many?) we can achieve the\nsame effect while keeping the orientation of the gripper constant as well. We have already\nseen an example of this in the “experiment” of placing your hand on the desk and moving\nyour elbow. The kinematic constraint of your hand position is insufﬁcient to determine the",
  "seen an example of this in the “experiment” of placing your hand on the desk and moving\nyour elbow. The kinematic constraint of your hand position is insufﬁcient to determine the\nconﬁguration of your elbow. In other words, the inverse kinematics of your shoulder–arm\nassembly possesses an inﬁnite number of solutions.\nThe second problem with conﬁguration space representations arises from the obsta-\ncles that may exist in the robot’s workspace. Our example in Figure 25.14(a) shows several\nsuch obstacles, including a free-hanging obstacle that protrudes into the center of the robot’s\nworkspace. In workspace, such obstacles take on simple geometric forms—especially in\nmost robotics textbooks, which tend to focus on polygonal obstacles. But how do they look\nin conﬁguration space?\nFigure 25.14(b) shows the conﬁguration space for our example robot, under the speciﬁc\nobstacle conﬁguration shown in Figure 25.14(a). The conﬁguration space can be decomposed\ninto two subspaces: the space of all conﬁgurations that a robot may attain, commonly called\nfree space, and the space of unattainable conﬁgurations, called occupied space. The white\nFREE SPACE\nOCCUPIED SPACE\narea in Figure 25.14(b) corresponds to the free space. All other regions correspond to occu- Section 25.4.\nPlanning to Move\n989\npied space. The different shadings of the occupied space corresponds to the different objects\nin the robot’s workspace; the black region surrounding the entire free space corresponds to\nconﬁgurations in which the robot collides with itself. It is easy to see that extreme values of\nthe shoulder or elbow angles cause such a violation. The two oval-shaped regions on both\nsides of the robot correspond to the table on which the robot is mounted. The third oval region\ncorresponds to the left wall. Finally, the most interesting object in conﬁguration space is the\nvertical obstacle that hangs from the ceiling and impedes the robot’s motions. This object has\na funny shape in conﬁguration space: it is highly nonlinear and at places even concave. With\na little bit of imagination the reader will recognize the shape of the gripper at the upper left\nend. We encourage the reader to pause for a moment and study this diagram. The shape of\nthis obstacle is not at all obvious! The dot inside Figure 25.14(b) marks the conﬁguration of\nthe robot, as shown in Figure 25.14(a). Figure 25.15 depicts three additional conﬁgurations,",
  "this obstacle is not at all obvious! The dot inside Figure 25.14(b) marks the conﬁguration of\nthe robot, as shown in Figure 25.14(a). Figure 25.15 depicts three additional conﬁgurations,\nboth in workspace and in conﬁguration space. In conﬁguration conf-1, the gripper encloses\nthe vertical obstacle.\nEven if the robot’s workspace is represented by ﬂat polygons, the shape of the free space\ncan be very complicated. In practice, therefore, one usually probes a conﬁguration space\ninstead of constructing it explicitly. A planner may generate a conﬁguration and then test to\nsee if it is in free space by applying the robot kinematics and then checking for collisions in\nworkspace coordinates.\n25.4.2\nCell decomposition methods\nThe ﬁrst approach to path planning uses cell decomposition—that is, it decomposes the\nCELL\nDECOMPOSITION\nfree space into a ﬁnite number of contiguous regions, called cells. These regions have the\nimportant property that the path-planning problem within a single region can be solved by\nsimple means (e.g., moving along a straight line). The path-planning problem then becomes\na discrete graph-search problem, very much like the search problems introduced in Chapter 3.\nThe simplest cell decomposition consists of a regularly spaced grid. Figure 25.16(a)\nshows a square grid decomposition of the space and a solution path that is optimal for this\ngrid size. Grayscale shading indicates the value of each free-space grid cell—i.e., the cost of\nthe shortest path from that cell to the goal. (These values can be computed by a deterministic\nform of the VALUE-ITERATION algorithm given in Figure 17.4 on page 653.) Figure 25.16(b)\nshows the corresponding workspace trajectory for the arm. Of course, we can also use the A∗\nalgorithm to ﬁnd a shortest path.\nSuch a decomposition has the advantage that it is extremely simple to implement, but\nit also suffers from three limitations. First, it is workable only for low-dimensional conﬁgu-\nration spaces, because the number of grid cells increases exponentially with d, the number of\ndimensions. Sounds familiar? This is the curse!dimensionality@of dimensionality. Second,\nthere is the problem of what to do with cells that are “mixed”—that is, neither entirely within\nfree space nor entirely within occupied space. A solution path that includes such a cell may\nnot be a real solution, because there may be no way to cross the cell in the desired direction",
  "free space nor entirely within occupied space. A solution path that includes such a cell may\nnot be a real solution, because there may be no way to cross the cell in the desired direction\nin a straight line. This would make the path planner unsound. On the other hand, if we insist\nthat only completely free cells may be used, the planner will be incomplete, because it might 990\nChapter\n25.\nRobotics\nstart\ngoal\nstart\ngoal\n(a)\n(b)\nFigure 25.16\n(a) Value function and path found for a discrete grid cell approximation of\nthe conﬁguration space. (b) The same path visualized in workspace coordinates. Notice how\nthe robot bends its elbow to avoid a collision with the vertical obstacle.\nbe the case that the only paths to the goal go through mixed cells—especially if the cell size\nis comparable to that of the passageways and clearances in the space. And third, any path\nthrough a discretized state space will not be smooth. It is generally difﬁcult to guarantee that\na smooth solution exists near the discrete path. So a robot may not be able to execute the\nsolution found through this decomposition.\nCell decomposition methods can be improved in a number of ways, to alleviate some\nof these problems. The ﬁrst approach allows further subdivision of the mixed cells—perhaps\nusing cells of half the original size. This can be continued recursively until a path is found\nthat lies entirely within free cells. (Of course, the method only works if there is a way to\ndecide if a given cell is a mixed cell, which is easy only if the conﬁguration space boundaries\nhave relatively simple mathematical descriptions.) This method is complete provided there is\na bound on the smallest passageway through which a solution must pass. Although it focuses\nmost of the computational effort on the tricky areas within the conﬁguration space, it still\nfails to scale well to high-dimensional problems because each recursive splitting of a cell\ncreates 2d smaller cells. A second way to obtain a complete algorithm is to insist on an exact\ncell decomposition of the free space. This method must allow cells to be irregularly shaped\nEXACT CELL\nDECOMPOSITION\nwhere they meet the boundaries of free space, but the shapes must still be “simple” in the\nsense that it should be easy to compute a traversal of any free cell. This technique requires\nsome quite advanced geometric ideas, so we shall not pursue it further here.\nExamining the solution path shown in Figure 25.16(a), we can see an additional difﬁ-",
  "some quite advanced geometric ideas, so we shall not pursue it further here.\nExamining the solution path shown in Figure 25.16(a), we can see an additional difﬁ-\nculty that will have to be resolved. The path contains arbitrarily sharp corners; a robot moving\nat any ﬁnite speed could not execute such a path. This problem is solved by storing certain\ncontinuous values for each grid cell. Consider an algorithm which stores, for each grid cell, Section 25.4.\nPlanning to Move\n991\nthe exact, continuous state that was attained with the cell was ﬁrst expanded in the search.\nAssume further, that when propagating information to nearby grid cells, we use this continu-\nous state as a basis, and apply the continuous robot motion model for jumping to nearby cells.\nIn doing so, we can now guarantee that the resulting trajectory is smooth and can indeed be\nexecuted by the robot. One algorithm that implements this is hybrid A*.\nHYBRID A*\n25.4.3\nModiﬁed cost functions\nNotice that in Figure 25.16, the path goes very close to the obstacle. Anyone who has driven\na car knows that a parking space with one millimeter of clearance on either side is not really a\nparking space at all; for the same reason, we would prefer solution paths that are robust with\nrespect to small motion errors.\nThis problem can be solved by introducing a potential ﬁeld. A potential ﬁeld is a\nPOTENTIAL FIELD\nfunction deﬁned over state space, whose value grows with the distance to the closest obstacle.\nFigure 25.17(a) shows such a potential ﬁeld—the darker a conﬁguration state, the closer it is\nto an obstacle.\nThe potential ﬁeld can be used as an additional cost term in the shortest-path calculation.\nThis induces an interesting tradeoff. On the one hand, the robot seeks to minimize path length\nto the goal. On the other hand, it tries to stay away from obstacles by virtue of minimizing the\npotential function. With the appropriate weight balancing the two objectives, a resulting path\nmay look like the one shown in Figure 25.17(b). This ﬁgure also displays the value function\nderived from the combined cost function, again calculated by value iteration. Clearly, the\nresulting path is longer, but it is also safer.\nThere exist many other ways to modify the cost function. For example, it may be\ndesirable to smooth the control parameters over time. For example, when driving a car, a\nsmooth path is better than a jerky one. In general, such higher-order constraints are not easy",
  "desirable to smooth the control parameters over time. For example, when driving a car, a\nsmooth path is better than a jerky one. In general, such higher-order constraints are not easy\nto accommodate in the planning process, unless we make the most recent steering command\na part of the state. However, it is often easy to smooth the resulting trajectory after planning,\nusing conjugate gradient methods. Such post-planning smoothing is essential in many real-\nworld applications.\n25.4.4\nSkeletonization methods\nThe second major family of path-planning algorithms is based on the idea of skeletonization.\nSKELETONIZATION\nThese algorithms reduce the robot’s free space to a one-dimensional representation, for which\nthe planning problem is easier. This lower-dimensional representation is called a skeleton of\nthe conﬁguration space.\nFigure 25.18 shows an example skeletonization: it is a Voronoi graph of the free\nVORONOI GRAPH\nspace—the set of all points that are equidistant to two or more obstacles. To do path plan-\nning with a Voronoi graph, the robot ﬁrst changes its present conﬁguration to a point on the\nVoronoi graph. It is easy to show that this can always be achieved by a straight-line motion\nin conﬁguration space. Second, the robot follows the Voronoi graph until it reaches the point\nnearest to the target conﬁguration. Finally, the robot leaves the Voronoi graph and moves to\nthe target. Again, this ﬁnal step involves straight-line motion in conﬁguration space. 992\nChapter\n25.\nRobotics\nstart\ngoal\n(a)\n(b)\nFigure 25.17\n(a) A repelling potential ﬁeld pushes the robot away from obstacles. (b)\nPath found by simultaneously minimizing path length and the potential.\n(a)\n(b)\nFigure 25.18\n(a) The Voronoi graph is the set of points equidistant to two or more obsta-\ncles in conﬁguration space. (b) A probabilistic roadmap, composed of 400 randomly chosen\npoints in free space.\nIn this way, the original path-planning problem is reduced to ﬁnding a path on the\nVoronoi graph, which is generally one-dimensional (except in certain nongeneric cases) and\nhas ﬁnitely many points where three or more one-dimensional curves intersect. Thus, ﬁnding Section 25.5.\nPlanning Uncertain Movements\n993\nthe shortest path along the Voronoi graph is a discrete graph-search problem of the kind\ndiscussed in Chapters 3 and 4. Following the Voronoi graph may not give us the shortest\npath, but the resulting paths tend to maximize clearance. Disadvantages of Voronoi graph",
  "discussed in Chapters 3 and 4. Following the Voronoi graph may not give us the shortest\npath, but the resulting paths tend to maximize clearance. Disadvantages of Voronoi graph\ntechniques are that they are difﬁcult to apply to higher-dimensional conﬁguration spaces, and\nthat they tend to induce unnecessarily large detours when the conﬁguration space is wide\nopen. Furthermore, computing the Voronoi graph can be difﬁcult, especially in conﬁguration\nspace, where the shapes of obstacles can be complex.\nAn alternative to the Voronoi graphs is the probabilistic roadmap, a skeletonization\nPROBABILISTIC\nROADMAP\napproach that offers more possible routes, and thus deals better with wide-open spaces. Fig-\nure 25.18(b) shows an example of a probabilistic roadmap. The graph is created by randomly\ngenerating a large number of conﬁgurations, and discarding those that do not fall into free\nspace. Two nodes are joined by an arc if it is “easy” to reach one node from the other–for\nexample, by a straight line in free space. The result of all this is a randomized graph in the\nrobot’s free space. If we add the robot’s start and goal conﬁgurations to this graph, path\nplanning amounts to a discrete graph search. Theoretically, this approach is incomplete, be-\ncause a bad choice of random points may leave us without any paths from start to goal. It\nis possible to bound the probability of failure in terms of the number of points generated\nand certain geometric properties of the conﬁguration space. It is also possible to direct the\ngeneration of sample points towards the areas where a partial search suggests that a good\npath may be found, working bidirectionally from both the start and the goal positions. With\nthese improvements, probabilistic roadmap planning tends to scale better to high-dimensional\nconﬁguration spaces than most alternative path-planning techniques.\n25.5\nPLANNING UNCERTAIN MOVEMENTS\nNone of the robot motion-planning algorithms discussed thus far addresses a key characteris-\ntic of robotics problems: uncertainty. In robotics, uncertainty arises from partial observability\nof the environment and from the stochastic (or unmodeled) effects of the robot’s actions. Er-\nrors can also arise from the use of approximation algorithms such as particle ﬁltering, which\ndoes not provide the robot with an exact belief state even if the stochastic nature of the envi-\nronment is modeled perfectly.\nMost of today’s robots use deterministic algorithms for decision making, such as the",
  "does not provide the robot with an exact belief state even if the stochastic nature of the envi-\nronment is modeled perfectly.\nMost of today’s robots use deterministic algorithms for decision making, such as the\npath-planning algorithms of the previous section. To do so, it is common practice to extract\nthe most likely state from the probability distribution produced by the state estimation al-\nMOST LIKELY STATE\ngorithm. The advantage of this approach is purely computational. Planning paths through\nconﬁguration space is already a challenging problem; it would be worse if we had to work\nwith a full probability distribution over states. Ignoring uncertainty in this way works when\nthe uncertainty is small. In fact, when the environment model changes over time as the result\nof incorporating sensor measurements, many robots plan paths online during plan execution.\nThis is the online replanning technique of Section 11.3.3.\nONLINE REPLANNING 994\nChapter\n25.\nRobotics\nUnfortunately, ignoring the uncertainty does not always work. In some problems the\nrobot’s uncertainty is simply too massive: How can we use a deterministic path planner to\ncontrol a mobile robot that has no clue where it is? In general, if the robot’s true state is not\nthe one identiﬁed by the maximum likelihood rule, the resulting control will be suboptimal.\nDepending on the magnitude of the error this can lead to all sorts of unwanted effects, such\nas collisions with obstacles.\nThe ﬁeld of robotics has adopted a range of techniques for accommodating uncertainty.\nSome are derived from the algorithms given in Chapter 17 for decision making under uncer-\ntainty. If the robot faces uncertainty only in its state transition, but its state is fully observable,\nthe problem is best modeled as a Markov decision process (MDP). The solution of an MDP is\nan optimal policy, which tells the robot what to do in every possible state. In this way, it can\nhandle all sorts of motion errors, whereas a single-path solution from a deterministic planner\nwould be much less robust. In robotics, policies are called navigation functions. The value\nNAVIGATION\nFUNCTION\nfunction shown in Figure 25.16(a) can be converted into such a navigation function simply\nby following the gradient.\nJust as in Chapter 17, partial observability makes the problem much harder. The result-\ning robot control problem is a partially observable MDP, or POMDP. In such situations, the",
  "by following the gradient.\nJust as in Chapter 17, partial observability makes the problem much harder. The result-\ning robot control problem is a partially observable MDP, or POMDP. In such situations, the\nrobot maintains an internal belief state, like the ones discussed in Section 25.3. The solution\nto a POMDP is a policy deﬁned over the robot’s belief state. Put differently, the input to\nthe policy is an entire probability distribution. This enables the robot to base its decision not\nonly on what it knows, but also on what it does not know. For example, if it is uncertain\nabout a critical state variable, it can rationally invoke an information gathering action. This\nINFORMATION\nGATHERING ACTION\nis impossible in the MDP framework, since MDPs assume full observability. Unfortunately,\ntechniques that solve POMDPs exactly are inapplicable to robotics—there are no known tech-\nniques for high-dimensional continuous spaces. Discretization produces POMDPs that are far\ntoo large to handle. One remedy is to make the minimization of uncertainty a control objec-\ntive. For example, the coastal navigation heuristic requires the robot to stay near known\nCOASTAL\nNAVIGATION\nlandmarks to decrease its uncertainty. Another approach applies variants of the probabilis-\ntic roadmap planning method to the belief space representation. Such methods tend to scale\nbetter to large discrete POMDPs.\n25.5.1\nRobust methods\nUncertainty can also be handled using so-called robust control methods (see page 836) rather\nROBUST CONTROL\nthan probabilistic methods. A robust method is one that assumes a bounded amount of un-\ncertainty in each aspect of a problem, but does not assign probabilities to values within the\nallowed interval. A robust solution is one that works no matter what actual values occur,\nprovided they are within the assumed interval. An extreme form of robust method is the con-\nformant planning approach given in Chapter 11—it produces plans that work with no state\ninformation at all.\nHere, we look at a robust method that is used for ﬁne-motion planning (or FMP) in\nFINE-MOTION\nPLANNING\nrobotic assembly tasks. Fine-motion planning involves moving a robot arm in very close\nproximity to a static environment object. The main difﬁculty with ﬁne-motion planning is Section 25.5.\nPlanning Uncertain Movements\n995\nv\nCv\nmotion\nenvelope\ninitial\nconfiguration\nFigure 25.19\nA two-dimensional environment, velocity uncertainty cone, and envelope of",
  "Planning Uncertain Movements\n995\nv\nCv\nmotion\nenvelope\ninitial\nconfiguration\nFigure 25.19\nA two-dimensional environment, velocity uncertainty cone, and envelope of\npossible robot motions. The intended velocity is v, but with uncertainty the actual velocity\ncould be anywhere in Cv, resulting in a ﬁnal conﬁguration somewhere in the motion envelope,\nwhich means we wouldn’t know if we hit the hole or not.\nv\nCv\nmotion\nenvelope\ninitial\nconfiguration\nFigure 25.20\nThe ﬁrst motion command and the resulting envelope of possible robot mo-\ntions. No matter what the error, we know the ﬁnal conﬁguration will be to the left of the\nhole.\nthat the required motions and the relevant features of the environment are very small. At such\nsmall scales, the robot is unable to measure or control its position accurately and may also be\nuncertain of the shape of the environment itself; we will assume that these uncertainties are\nall bounded. The solutions to FMP problems will typically be conditional plans or policies\nthat make use of sensor feedback during execution and are guaranteed to work in all situations\nconsistent with the assumed uncertainty bounds.\nA ﬁne-motion plan consists of a series of guarded motions. Each guarded motion\nGUARDED MOTION\nconsists of (1) a motion command and (2) a termination condition, which is a predicate on the\nrobot’s sensor values, and returns true to indicate the end of the guarded move. The motion\ncommands are typically compliant motions that allow the effector to slide if the motion\nCOMPLIANT MOTION\ncommand would cause collision with an obstacle. As an example, Figure 25.19 shows a two-\ndimensional conﬁguration space with a narrow vertical hole. It could be the conﬁguration\nspace for insertion of a rectangular peg into a hole or a car key into the ignition. The motion\ncommands are constant velocities. The termination conditions are contact with a surface. To\nmodel uncertainty in control, we assume that instead of moving in the commanded direction,\nthe robot’s actual motion lies in the cone Cv about it. The ﬁgure shows what would happen 996\nChapter\n25.\nRobotics\nv\nCv\nmotion\nenvelope\nFigure 25.21\nThe second motion command and the envelope of possible motions. Even\nwith error, we will eventually get into the hole.\nif we commanded a velocity straight down from the initial conﬁguration. Because of the\nuncertainty in velocity, the robot could move anywhere in the conical envelope, possibly",
  "with error, we will eventually get into the hole.\nif we commanded a velocity straight down from the initial conﬁguration. Because of the\nuncertainty in velocity, the robot could move anywhere in the conical envelope, possibly\ngoing into the hole, but more likely landing to one side of it. Because the robot would not\nthen know which side of the hole it was on, it would not know which way to move.\nA more sensible strategy is shown in Figures 25.20 and 25.21. In Figure 25.20, the\nrobot deliberately moves to one side of the hole. The motion command is shown in the ﬁgure,\nand the termination test is contact with any surface. In Figure 25.21, a motion command is\ngiven that causes the robot to slide along the surface and into the hole. Because all possible\nvelocities in the motion envelope are to the right, the robot will slide to the right whenever it\nis in contact with a horizontal surface. It will slide down the right-hand vertical edge of the\nhole when it touches it, because all possible velocities are down relative to a vertical surface.\nIt will keep moving until it reaches the bottom of the hole, because that is its termination\ncondition. In spite of the control uncertainty, all possible trajectories of the robot terminate\nin contact with the bottom of the hole—that is, unless surface irregularities cause the robot to\nstick in one place.\nAs one might imagine, the problem of constructing ﬁne-motion plans is not trivial; in\nfact, it is a good deal harder than planning with exact motions. One can either choose a\nﬁxed number of discrete values for each motion or use the environment geometry to choose\ndirections that give qualitatively different behavior. A ﬁne-motion planner takes as input the\nconﬁguration-space description, the angle of the velocity uncertainty cone, and a speciﬁcation\nof what sensing is possible for termination (surface contact in this case). It should produce a\nmultistep conditional plan or policy that is guaranteed to succeed, if such a plan exists.\nOur example assumes that the planner has an exact model of the environment, but it is\npossible to allow for bounded error in this model as follows. If the error can be described in\nterms of parameters, those parameters can be added as degrees of freedom to the conﬁguration\nspace. In the last example, if the depth and width of the hole were uncertain, we could add\nthem as two degrees of freedom to the conﬁguration space. It is impossible to move the",
  "space. In the last example, if the depth and width of the hole were uncertain, we could add\nthem as two degrees of freedom to the conﬁguration space. It is impossible to move the\nrobot in these directions in the conﬁguration space or to sense its position directly. But\nboth those restrictions can be incorporated when describing this problem as an FMP problem\nby appropriately specifying control and sensor uncertainties. This gives a complex, four-\ndimensional planning problem, but exactly the same planning techniques can be applied. Section 25.6.\nMoving\n997\nNotice that unlike the decision-theoretic methods in Chapter 17, this kind of robust approach\nresults in plans designed for the worst-case outcome, rather than maximizing the expected\nquality of the plan. Worst-case plans are optimal in the decision-theoretic sense only if failure\nduring execution is much worse than any of the other costs involved in execution.\n25.6\nMOVING\nSo far, we have talked about how to plan motions, but not about how to move. Our plans—\nparticularly those produced by deterministic path planners—assume that the robot can simply\nfollow any path that the algorithm produces. In the real world, of course, this is not the case.\nRobots have inertia and cannot execute arbitrary paths except at arbitrarily slow speeds. In\nmost cases, the robot gets to exert forces rather than specify positions. This section discusses\nmethods for calculating these forces.\n25.6.1\nDynamics and control\nSection 25.2 introduced the notion of dynamic state, which extends the kinematic state of a\nrobot by its velocity. For example, in addition to the angle of a robot joint, the dynamic state\nalso captures the rate of change of the angle, and possibly even its momentary acceleration.\nThe transition model for a dynamic state representation includes the effect of forces on this\nrate of change. Such models are typically expressed via differential equations, which are\nDIFFERENTIAL\nEQUATION\nequations that relate a quantity (e.g., a kinematic state) to the change of the quantity over\ntime (e.g., velocity). In principle, we could have chosen to plan robot motion using dynamic\nmodels, instead of our kinematic models. Such a methodology would lead to superior robot\nperformance, if we could generate the plans. However, the dynamic state has higher dimen-\nsion than the kinematic space, and the curse of dimensionality would render many motion",
  "performance, if we could generate the plans. However, the dynamic state has higher dimen-\nsion than the kinematic space, and the curse of dimensionality would render many motion\nplanning algorithms inapplicable for all but the most simple robots. For this reason, practical\nrobot system often rely on simpler kinematic path planners.\nA common technique to compensate for the limitations of kinematic plans is to use a\nseparate mechanism, a controller, for keeping the robot on track. Controllers are techniques\nCONTROLLER\nfor generating robot controls in real time using feedback from the environment, so as to\nachieve a control objective. If the objective is to keep the robot on a preplanned path, it is\noften referred to as a reference controller and the path is called a reference path. Controllers\nREFERENCE\nCONTROLLER\nREFERENCE PATH\nthat optimize a global cost function are known as optimal controllers. Optimal policies for\nOPTIMAL\nCONTROLLERS\ncontinuous MDPs are, in effect, optimal controllers.\nOn the surface, the problem of keeping a robot on a prespeciﬁed path appears to be\nrelatively straightforward. In practice, however, even this seemingly simple problem has its\npitfalls. Figure 25.22(a) illustrates what can go wrong; it shows the path of a robot that\nattempts to follow a kinematic path. Whenever a deviation occurs—whether due to noise or\nto constraints on the forces the robot can apply—the robot provides an opposing force whose\nmagnitude is proportional to this deviation. Intuitively, this might appear plausible, since\ndeviations should be compensated by a counterforce to keep the robot on track. However, 998\nChapter\n25.\nRobotics\n(a)\n(b)\n(c)\nFigure 25.22\nRobot arm control using (a) proportional control with gain factor 1.0, (b)\nproportional control with gain factor 0.1, and (c) PD (proportional derivative) control with\ngain factors 0.3 for the proportional component and 0.8 for the differential component. In all\ncases the robot arm tries to follow the path shown in gray.\nas Figure 25.22(a) illustrates, our controller causes the robot to vibrate rather violently. The\nvibration is the result of a natural inertia of the robot arm: once driven back to its reference\nposition the robot then overshoots, which induces a symmetric error with opposite sign. Such\novershooting may continue along an entire trajectory, and the resulting robot motion is far\nfrom desirable.\nBefore we can deﬁne a better controller, let us formally describe what went wrong.",
  "overshooting may continue along an entire trajectory, and the resulting robot motion is far\nfrom desirable.\nBefore we can deﬁne a better controller, let us formally describe what went wrong.\nControllers that provide force in negative proportion to the observed error are known as P\ncontrollers. The letter ‘P’ stands for proportional, indicating that the actual control is pro-\nP CONTROLLER\nportional to the error of the robot manipulator. More formally, let y(t) be the reference path,\nparameterized by time index t. The control at generated by a P controller has the form:\nat = KP (y(t) −xt) .\nHere xt is the state of the robot at time t and KP is a constant known as the gain parameter of\nGAIN PARAMETER\nthe controller and its value is called the gain factor); Kp regulates how strongly the controller\ncorrects for deviations between the actual state xt and the desired one y(t). In our example,\nKP = 1. At ﬁrst glance, one might think that choosing a smaller value for KP would\nremedy the problem. Unfortunately, this is not the case. Figure 25.22(b) shows a trajectory\nfor KP = .1, still exhibiting oscillatory behavior. Lower values of the gain parameter may\nsimply slow down the oscillation, but do not solve the problem. In fact, in the absence of\nfriction, the P controller is essentially a spring law; so it will oscillate indeﬁnitely around a\nﬁxed target location.\nTraditionally, problems of this type fall into the realm of control theory, a ﬁeld of\nincreasing importance to researchers in AI. Decades of research in this ﬁeld have led to a large\nnumber of controllers that are superior to the simple control law given above. In particular, a\nreference controller is said to be stable if small perturbations lead to a bounded error between\nSTABLE\nthe robot and the reference signal. It is said to be strictly stable if it is able to return to and\nSTRICTLY STABLE Section 25.6.\nMoving\n999\nthen stay on its reference path upon such perturbations. Our P controller appears to be stable\nbut not strictly stable, since it fails to stay anywhere near its reference trajectory.\nThe simplest controller that achieves strict stability in our domain is a PD controller.\nPD CONTROLLER\nThe letter ‘P’ stands again for proportional, and ‘D’ stands for derivative. PD controllers are\ndescribed by the following equation:\nat = KP (y(t) −xt) + KD\n∂(y(t) −xt)\n∂t\n.\n(25.2)\nAs this equation suggests, PD controllers extend P controllers by a differential component,",
  "described by the following equation:\nat = KP (y(t) −xt) + KD\n∂(y(t) −xt)\n∂t\n.\n(25.2)\nAs this equation suggests, PD controllers extend P controllers by a differential component,\nwhich adds to the value of at a term that is proportional to the ﬁrst derivative of the error\ny(t) −xt over time. What is the effect of such a term? In general, a derivative term dampens\nthe system that is being controlled. To see this, consider a situation where the error (y(t)−xt)\nis changing rapidly over time, as is the case for our P controller above. The derivative of this\nerror will then counteract the proportional term, which will reduce the overall response to\nthe perturbation. However, if the same error persists and does not change, the derivative will\nvanish and the proportional term dominates the choice of control.\nFigure 25.22(c) shows the result of applying this PD controller to our robot arm, using\nas gain parameters KP = .3 and KD = .8. Clearly, the resulting path is much smoother, and\ndoes not exhibit any obvious oscillations.\nPD controllers do have failure modes, however. In particular, PD controllers may fail\nto regulate an error down to zero, even in the absence of external perturbations. Often such\na situation is the result of a systematic external force that is not part of the model. An au-\ntonomous car driving on a banked surface, for example, may ﬁnd itself systematically pulled\nto one side. Wear and tear in robot arms cause similar systematic errors. In such situations,\nan over-proportional feedback is required to drive the error closer to zero. The solution to this\nproblem lies in adding a third term to the control law, based on the integrated error over time:\nat = KP (y(t) −xt) + KI\n\u001a\n(y(t) −xt)dt + KD\n∂(y(t) −xt)\n∂t\n.\n(25.3)\nHere KI is yet another gain parameter. The term\n+\n(y(t) −xt)dt calculates the integral of the\nerror over time. The effect of this term is that long-lasting deviations between the reference\nsignal and the actual state are corrected. If, for example, xt is smaller than y(t) for a long\nperiod of time, this integral will grow until the resulting control at forces this error to shrink.\nIntegral terms, then, ensure that a controller does not exhibit systematic error, at the expense\nof increased danger of oscillatory behavior. A controller with all three terms is called a PID\ncontroller (for proportional integral derivative). PID controllers are widely used in industry,\nPID CONTROLLER\nfor a variety of control problems.\n25.6.2",
  "of increased danger of oscillatory behavior. A controller with all three terms is called a PID\ncontroller (for proportional integral derivative). PID controllers are widely used in industry,\nPID CONTROLLER\nfor a variety of control problems.\n25.6.2\nPotential-ﬁeld control\nWe introduced potential ﬁelds as an additional cost function in robot motion planning, but\nthey can also be used for generating robot motion directly, dispensing with the path planning\nphase altogether. To achieve this, we have to deﬁne an attractive force that pulls the robot\ntowards its goal conﬁguration and a repellent potential ﬁeld that pushes the robot away from\nobstacles. Such a potential ﬁeld is shown in Figure 25.23. Its single global minimum is 1000\nChapter\n25.\nRobotics\nstart\ngoal\nstart\ngoal\n(a)\n(b)\nFigure 25.23\nPotential ﬁeld control. The robot ascends a potential ﬁeld composed of\nrepelling forces asserted from the obstacles and an attracting force that corresponds to the\ngoal conﬁguration. (a) Successful path. (b) Local optimum.\nthe goal conﬁguration, and the value is the sum of the distance to this goal conﬁguration\nand the proximity to obstacles. No planning was involved in generating the potential ﬁeld\nshown in the ﬁgure. Because of this, potential ﬁelds are well suited to real-time control.\nFigure 25.23(a) shows a trajectory of a robot that performs hill climbing in the potential\nﬁeld. In many applications, the potential ﬁeld can be calculated efﬁciently for any given\nconﬁguration. Moreover, optimizing the potential amounts to calculating the gradient of the\npotential for the present robot conﬁguration. These calculations can be extremely efﬁcient,\nespecially when compared to path-planning algorithms, all of which are exponential in the\ndimensionality of the conﬁguration space (the DOFs) in the worst case.\nThe fact that the potential ﬁeld approach manages to ﬁnd a path to the goal in such\nan efﬁcient manner, even over long distances in conﬁguration space, raises the question as\nto whether there is a need for planning in robotics at all. Are potential ﬁeld techniques\nsufﬁcient, or were we just lucky in our example? The answer is that we were indeed lucky.\nPotential ﬁelds have many local minima that can trap the robot. In Figure 25.23(b), the robot\napproaches the obstacle by simply rotating its shoulder joint, until it gets stuck on the wrong\nside of the obstacle. The potential ﬁeld is not rich enough to make the robot bend its elbow",
  "approaches the obstacle by simply rotating its shoulder joint, until it gets stuck on the wrong\nside of the obstacle. The potential ﬁeld is not rich enough to make the robot bend its elbow\nso that the arm ﬁts under the obstacle. In other words, potential ﬁeld control is great for local\nrobot motion but sometimes we still need global planning. Another important drawback with\npotential ﬁelds is that the forces they generate depend only on the obstacle and robot positions,\nnot on the robot’s velocity. Thus, potential ﬁeld control is really a kinematic method and may\nfail if the robot is moving quickly. Section 25.6.\nMoving\n1001\nS1\nS2\nS4\nS3\npush backward\nlift up\nset down\nretract, lift higher\nmove\nforward\nno\nyes\nstuck?\n(a)\n(b)\nFigure 25.24\n(a) Genghis, a hexapod robot.\n(b) An augmented ﬁnite state machine\n(AFSM) for the control of a single leg. Notice that this AFSM reacts to sensor feedback:\nif a leg is stuck during the forward swinging phase, it will be lifted increasingly higher.\n25.6.3\nReactive control\nSo far we have considered control decisions that require some model of the environment for\nconstructing either a reference path or a potential ﬁeld. There are some difﬁculties with this\napproach. First, models that are sufﬁciently accurate are often difﬁcult to obtain, especially\nin complex or remote environments, such as the surface of Mars, or for robots that have\nfew sensors. Second, even in cases where we can devise a model with sufﬁcient accuracy,\ncomputational difﬁculties and localization error might render these techniques impractical.\nIn some cases, a reﬂex agent architecture using reactive control is more appropriate.\nREACTIVE CONTROL\nFor example, picture a legged robot that attempts to lift a leg over an obstacle. We could\ngive this robot a rule that says lift the leg a small height h and move it forward, and if the leg\nencounters an obstacle, move it back and start again at a higher height. You could say that h\nis modeling an aspect of the world, but we can also think of h as an auxiliary variable of the\nrobot controller, devoid of direct physical meaning.\nOne such example is the six-legged (hexapod) robot, shown in Figure 25.24(a), de-\nsigned for walking through rough terrain. The robot’s sensors are inadequate to obtain mod-\nels of the terrain for path planning. Moreover, even if we added sufﬁciently accurate sensors,\nthe twelve degrees of freedom (two for each leg) would render the resulting path planning\nproblem computationally intractable.",
  "els of the terrain for path planning. Moreover, even if we added sufﬁciently accurate sensors,\nthe twelve degrees of freedom (two for each leg) would render the resulting path planning\nproblem computationally intractable.\nIt is possible, nonetheless, to specify a controller directly without an explicit environ-\nmental model. (We have already seen this with the PD controller, which was able to keep a\ncomplex robot arm on target without an explicit model of the robot dynamics; it did, however,\nrequire a reference path generated from a kinematic model.) For the hexapod robot we ﬁrst\nchoose a gait, or pattern of movement of the limbs. One statically stable gait is to ﬁrst move\nGAIT\nthe right front, right rear, and left center legs forward (keeping the other three ﬁxed), and\nthen move the other three. This gait works well on ﬂat terrain. On rugged terrain, obstacles\nmay prevent a leg from swinging forward. This problem can be overcome by a remarkably\nsimple control rule: when a leg’s forward motion is blocked, simply retract it, lift it higher, 1002\nChapter\n25.\nRobotics\nFigure 25.25\nMultiple exposures of an RC helicopter executing a ﬂip based on a policy\nlearned with reinforcement learning. Images courtesy of Andrew Ng, Stanford University.\nand try again. The resulting controller is shown in Figure 25.24(b) as a ﬁnite state machine;\nit constitutes a reﬂex agent with state, where the internal state is represented by the index of\nthe current machine state (s1 through s4).\nVariants of this simple feedback-driven controller have been found to generate remark-\nably robust walking patterns, capable of maneuvering the robot over rugged terrain. Clearly,\nsuch a controller is model-free, and it does not deliberate or use search for generating con-\ntrols. Environmental feedback plays a crucial role in the controller’s execution. The software\nalone does not specify what will actually happen when the robot is placed in an environment.\nBehavior that emerges through the interplay of a (simple) controller and a (complex) envi-\nronment is often referred to as emergent behavior. Strictly speaking, all robots discussed\nEMERGENT\nBEHAVIOR\nin this chapter exhibit emergent behavior, due to the fact that no model is perfect. Histori-\ncally, however, the term has been reserved for control techniques that do not utilize explicit\nenvironmental models. Emergent behavior is also characteristic of biological organisms.\n25.6.4\nReinforcement learning control",
  "cally, however, the term has been reserved for control techniques that do not utilize explicit\nenvironmental models. Emergent behavior is also characteristic of biological organisms.\n25.6.4\nReinforcement learning control\nOne particularly exciting form of control is based on the policy search form of reinforcement\nlearning (see Section 21.5). This work has been enormously inﬂuential in recent years, at\nis has solved challenging robotics problems for which previously no solution existed. An\nexample is acrobatic autonomous helicopter ﬂight. Figure 25.25 shows an autonomous ﬂip\nof a small RC (radio-controlled) helicopter. This maneuver is challenging due to the highly\nnonlinear nature of the aerodynamics involved. Only the most experienced of human pilots\nare able to perform it. Yet a policy search method (as described in Chapter 21), using only a\nfew minutes of computation, learned a policy that can safely execute a ﬂip every time.\nPolicy search needs an accurate model of the domain before it can ﬁnd a policy. The\ninput to this model is the state of the helicopter at time t, the controls at time t, and the\nresulting state at time t+Δt. The state of a helicopter can be described by the 3D coordinates\nof the vehicle, its yaw, pitch, and roll angles, and the rate of change of these six variables.\nThe controls are the manual controls of of the helicopter: throttle, pitch, elevator, aileron,\nand rudder. All that remains is the resulting state—how are we going to deﬁne a model that\naccurately says how the helicopter responds to each control? The answer is simple: Let an\nexpert human pilot ﬂy the helicopter, and record the controls that the expert transmits over\nthe radio and the state variables of the helicopter. About four minutes of human-controlled\nﬂight sufﬁces to build a predictive model that is sufﬁciently accurate to simulate the vehicle. Section 25.7.\nRobotic Software Architectures\n1003\nWhat is remarkable about this example is the ease with which this learning approach\nsolves a challenging robotics problem. This is one of the many successes of machine learning\nin scientiﬁc ﬁelds previously dominated by careful mathematical analysis and modeling.\n25.7\nROBOTIC SOFTWARE ARCHITECTURES\nA methodology for structuring algorithms is called a software architecture. An architecture\nSOFTWARE\nARCHITECTURE\nincludes languages and tools for writing programs, as well as an overall philosophy for how\nprograms can be brought together.",
  "A methodology for structuring algorithms is called a software architecture. An architecture\nSOFTWARE\nARCHITECTURE\nincludes languages and tools for writing programs, as well as an overall philosophy for how\nprograms can be brought together.\nModern-day software architectures for robotics must decide how to combine reactive\ncontrol and model-based deliberative planning. In many ways, reactive and deliberate tech-\nniques have orthogonal strengths and weaknesses. Reactive control is sensor-driven and ap-\npropriate for making low-level decisions in real time. However, it rarely yields a plausible\nsolution at the global level, because global control decisions depend on information that can-\nnot be sensed at the time of decision making. For such problems, deliberate planning is a\nmore appropriate choice.\nConsequently, most robot architectures use reactive techniques at the lower levels of\ncontrol and deliberative techniques at the higher levels. We encountered such a combination\nin our discussion of PD controllers, where we combined a (reactive) PD controller with a\n(deliberate) path planner. Architectures that combine reactive and deliberate techniques are\ncalled hybrid architectures.\nHYBRID\nARCHITECTURE\n25.7.1\nSubsumption architecture\nThe subsumption architecture (Brooks, 1986) is a framework for assembling reactive con-\nSUBSUMPTION\nARCHITECTURE\ntrollers out of ﬁnite state machines. Nodes in these machines may contain tests for certain\nsensor variables, in which case the execution trace of a ﬁnite state machine is conditioned on\nthe outcome of such a test. Arcs can be tagged with messages that will be generated when\ntraversing them, and that are sent to the robot’s motors or to other ﬁnite state machines. Addi-\ntionally, ﬁnite state machines possess internal timers (clocks) that control the time it takes to\ntraverse an arc. The resulting machines are refereed to as augmented ﬁnite state machines,\nAUGMENTED FINITE\nSTATE MACHINE\nor AFSMs, where the augmentation refers to the use of clocks.\nAn example of a simple AFSM is the four-state machine shown in Figure 25.24(b),\nwhich generates cyclic leg motion for a hexapod walker. This AFSM implements a cyclic\ncontroller, whose execution mostly does not rely on environmental feedback. The forward\nswing phase, however, does rely on sensor feedback. If the leg is stuck, meaning that it has\nfailed to execute the forward swing, the robot retracts the leg, lifts it up a little higher, and",
  "swing phase, however, does rely on sensor feedback. If the leg is stuck, meaning that it has\nfailed to execute the forward swing, the robot retracts the leg, lifts it up a little higher, and\nattempts to execute the forward swing once again. Thus, the controller is able to react to\ncontingencies arising from the interplay of the robot and its environment.\nThe subsumption architecture offers additional primitives for synchronizing AFSMs,\nand for combining output values of multiple, possibly conﬂicting AFSMs. In this way, it\nenables the programmer to compose increasingly complex controllers in a bottom-up fashion. 1004\nChapter\n25.\nRobotics\nIn our example, we might begin with AFSMs for individual legs, followed by an AFSM for\ncoordinating multiple legs. On top of this, we might implement higher-level behaviors such\nas collision avoidance, which might involve backing up and turning.\nThe idea of composing robot controllers from AFSMs is quite intriguing. Imagine\nhow difﬁcult it would be to generate the same behavior with any of the conﬁguration-space\npath-planning algorithms described in the previous section. First, we would need an accu-\nrate model of the terrain. The conﬁguration space of a robot with six legs, each of which\nis driven by two independent motors, totals eighteen dimensions (twelve dimensions for the\nconﬁguration of the legs, and six for the location and orientation of the robot relative to its\nenvironment). Even if our computers were fast enough to ﬁnd paths in such high-dimensional\nspaces, we would have to worry about nasty effects such as the robot sliding down a slope.\nBecause of such stochastic effects, a single path through conﬁguration space would almost\ncertainly be too brittle, and even a PID controller might not be able to cope with such con-\ntingencies. In other words, generating motion behavior deliberately is simply too complex a\nproblem for present-day robot motion planning algorithms.\nUnfortunately, the subsumption architecture has its own problems. First, the AFSMs\nare driven by raw sensor input, an arrangement that works if the sensor data is reliable and\ncontains all necessary information for decision making, but fails if sensor data has to be inte-\ngrated in nontrivial ways over time. Subsumption-style controllers have therefore mostly been\napplied to simple tasks, such as following a wall or moving towards visible light sources. Sec-",
  "grated in nontrivial ways over time. Subsumption-style controllers have therefore mostly been\napplied to simple tasks, such as following a wall or moving towards visible light sources. Sec-\nond, the lack of deliberation makes it difﬁcult to change the task of the robot. A subsumption-\nstyle robot usually does just one task, and it has no notion of how to modify its controls to\naccommodate different goals (just like the dung beetle on page 39). Finally, subsumption-\nstyle controllers tend to be difﬁcult to understand. In practice, the intricate interplay between\ndozens of interacting AFSMs (and the environment) is beyond what most human program-\nmers can comprehend. For all these reasons, the subsumption architecture is rarely used in\nrobotics, despite its great historical importance. However, it has had an inﬂuence on other\narchitectures, and on individual components of some architectures.\n25.7.2\nThree-layer architecture\nHybrid architectures combine reaction with deliberation. The most popular hybrid architec-\nture is the three-layer architecture, which consists of a reactive layer, an executive layer,\nTHREE-LAYER\nARCHITECTURE\nand a deliberative layer.\nThe reactive layer provides low-level control to the robot. It is characterized by a tight\nREACTIVE LAYER\nsensor–action loop. Its decision cycle is often on the order of milliseconds.\nThe executive layer (or sequencing layer) serves as the glue between the reactive layer\nEXECUTIVE LAYER\nand the deliberative layer. It accepts directives by the deliberative layer, and sequences them\nfor the reactive layer. For example, the executive layer might handle a set of via-points\ngenerated by a deliberative path planner, and make decisions as to which reactive behavior\nto invoke. Decision cycles at the executive layer are usually in the order of a second. The\nexecutive layer is also responsible for integrating sensor information into an internal state\nrepresentation. For example, it may host the robot’s localization and online mapping routines. Section 25.7.\nRobotic Software Architectures\n1005\nTouareg interface \nLaser mapper \nWireless E-Stop \nTop level control \nLaser 2 interface \nLaser 3 interface \nLaser 4 interface \nLaser 1 interface \nLaser 5 interface \nCamera interface \nRadar interface \nRadar mapper \nVision mapper \nUKF Pose estimation \nWheel velocity \nGPS position \nGPS compass \nIMU interface \nSurface assessment \nHealth monitor \nRoad finder \nTouch screen UI \nThrottle/brake control \nSteering control \nPath planner",
  "Radar interface \nRadar mapper \nVision mapper \nUKF Pose estimation \nWheel velocity \nGPS position \nGPS compass \nIMU interface \nSurface assessment \nHealth monitor \nRoad finder \nTouch screen UI \nThrottle/brake control \nSteering control \nPath planner \nlaser map \nvehicle state (pose, velocity) \nvelocity limit \nmap \nvision map \nvehicle \nstate \nobstacle list \ntrajectory \nroad center \nRDDF database \ndriving mode \npause/disable command \nPower server interface \nclocks \nemergency stop \npower on/off \nLinux processes start/stop \nheart beats \ncorridor \n    SENSOR INTERFACE                PERCEPTION                  PLANNING&CONTROL        USER INTERFACE \nVEHICLE \nINTERFACE \nRDDF corridor (smoothed and original) \nProcess controller \nGLOBAL \nSERVICES \nhealth status \ndata \nData logger \nFile system \nCommunication requests \nvehicle state (pose, velocity) \nBrake/steering \nCommunication channels \nInter-process communication (IPC) server \nTime server \nFigure 25.26\nSoftware architecture of a robot car.\nThis software implements a data\npipeline, in which all modules process data simultaneously.\nThe deliberative layer generates global solutions to complex tasks using planning.\nDELIBERATIVE LAYER\nBecause of the computational complexity involved in generating such solutions, its decision\ncycle is often in the order of minutes. The deliberative layer (or planning layer) uses models\nfor decision making. Those models might be either learned from data or supplied and may\nutilize state information gathered at the executive layer.\nVariants of the three-layer architecture can be found in most modern-day robot software\nsystems. The decomposition into three layers is not very strict. Some robot software systems\npossess additional layers, such as user interface layers that control the interaction with people,\nor a multiagent level for coordinating a robot’s actions with that of other robots operating in\nthe same environment.\n25.7.3\nPipeline architecture\nAnother architecture for robots is known as the pipeline architecture. Just like the subsump-\nPIPELINE\nARCHITECTURE\ntion architecture, the pipeline architecture executes multiple process in parallel. However, the\nspeciﬁc modules in this architecture resemble those in the three-layer architecture.\nFigure 25.26 shows an example pipeline architecture, which is used to control an au-\ntonomous car. Data enters this pipeline at the sensor interface layer. The perception layer\nSENSOR INTERFACE\nLAYER\nPERCEPTION LAYER 1006\nChapter\n25.\nRobotics\n(a)\n(b)",
  "tonomous car. Data enters this pipeline at the sensor interface layer. The perception layer\nSENSOR INTERFACE\nLAYER\nPERCEPTION LAYER 1006\nChapter\n25.\nRobotics\n(a)\n(b)\nFigure 25.27\n(a) The Helpmate robot transports food and other medical items in dozens\nof hospitals worldwide. (b) Kiva robots are part of a material-handling system for moving\nshelves in fulﬁllment centers. Image courtesy of Kiva Systems.\nthen updates the robot’s internal models of the environment based on this data. Next, these\nmodels are handed to the planning and control layer, which adjusts the robot’s internal\nPLANNING AND\nCONTROL LAYER\nplans turns them into actual controls for the robot. Those are then communicated back to the\nvehicle through the vehicle interface layer.\nVEHICLE INTERFACE\nLAYER\nThe key to the pipeline architecture is that this all happens in parallel. While the per-\nception layer processes the most recent sensor data, the control layer bases its choices on\nslightly older data. In this way, the pipeline architecture is similar to the human brain. We\ndon’t switch off our motion controllers when we digest new sensor data. Instead, we perceive,\nplan, and act all at the same time. Processes in the pipeline architecture run asynchronously,\nand all computation is data-driven. The resulting system is robust, and it is fast.\nThe architecture in Figure 25.26 also contains other, cross-cutting modules, responsible\nfor establishing communication between the different elements of the pipeline.\n25.8\nAPPLICATION DOMAINS\nHere are some of the prime application domains for robotic technology.\nIndustry and Agriculture. Traditionally, robots have been ﬁelded in areas that require\ndifﬁcult human labor, yet are structured enough to be amenable to robotic automation. The\nbest example is the assembly line, where manipulators routinely perform tasks such as as-\nsembly, part placement, material handling, welding, and painting. In many of these tasks,\nrobots have become more cost-effective than human workers. Outdoors, many of the heavy\nmachines that we use to harvest, mine, or excavate earth have been turned into robots. For Section 25.8.\nApplication Domains\n1007\n(a)\n(b)\nFigure 25.28\n(a) Robotic car BOSS, which won the DARPA Urban Challenge. Courtesy\nof Carnegie Mellon University. (b) Surgical robots in the operating room. Image courtesy of\nda Vinci Surgical Systems.\nexample, a project at Carnegie Mellon University has demonstrated that robots can strip paint",
  "of Carnegie Mellon University. (b) Surgical robots in the operating room. Image courtesy of\nda Vinci Surgical Systems.\nexample, a project at Carnegie Mellon University has demonstrated that robots can strip paint\noff large ships about 50 times faster than people can, and with a much reduced environmental\nimpact. Prototypes of autonomous mining robots have been found to be faster and more pre-\ncise than people in transporting ore in underground mines. Robots have been used to generate\nhigh-precision maps of abandoned mines and sewer systems. While many of these systems\nare still in their prototype stages, it is only a matter of time until robots will take over much\nof the semimechanical work that is presently performed by people.\nTransportation. Robotic transportation has many facets: from autonomous helicopters\nthat deliver payloads to hard-to-reach locations, to automatic wheelchairs that transport peo-\nple who are unable to control wheelchairs by themselves, to autonomous straddle carriers that\noutperform skilled human drivers when transporting containers from ships to trucks on load-\ning docks. A prime example of indoor transportation robots, or gofers, is the Helpmate robot\nshown in Figure 25.27(a). This robot has been deployed in dozens of hospitals to transport\nfood and other items. In factory settings, autonomous vehicles are now routinely deployed\nto transport goods in warehouses and between production lines. The Kiva system, shown in\nFigure 25.27(b), helps workers at fulﬁllment centers package goods into shipping containers.\nMany of these robots require environmental modiﬁcations for their operation. The most\ncommon modiﬁcations are localization aids such as inductive loops in the ﬂoor, active bea-\ncons, or barcode tags. An open challenge in robotics is the design of robots that can use\nnatural cues, instead of artiﬁcial devices, to navigate, particularly in environments such as the\ndeep ocean where GPS is unavailable.\nRobotic cars. Most of use cars every day. Many of us make cell phone calls while\ndriving. Some of us even text. The sad result: more than a million people die every year in\ntrafﬁc accidents. Robotic cars like BOSS and STANLEY offer hope: Not only will they make\ndriving much safer, but they will also free us from the need to pay attention to the road during\nour daily commute.\nProgress in robotic cars was stimulated by the DARPA Grand Challenge, a race over",
  "driving much safer, but they will also free us from the need to pay attention to the road during\nour daily commute.\nProgress in robotic cars was stimulated by the DARPA Grand Challenge, a race over\n100 miles of unrehearsed desert terrain, which represented a much more challenging task than 1008\nChapter\n25.\nRobotics\n(a)\n(b)\nFigure 25.29\n(a) A robot mapping an abandoned coal mine. (b) A 3D map of the mine\nacquired by the robot.\nhad ever been accomplished before. Stanford’s STANLEY vehicle completed the course in less\nthan seven hours in 2005, winning a $2 million prize and a place in the National Museum of\nAmerican History. Figure 25.28(a) depicts BOSS, which in 2007 won the DARPA Urban\nChallenge, a complicated road race on city streets where robots faced other robots and had to\nobey trafﬁc rules.\nHealth care. Robots are increasingly used to assist surgeons with instrument placement\nwhen operating on organs as intricate as brains, eyes, and hearts. Figure 25.28(b) shows such\na system. Robots have become indispensable tools in a range of surgical procedures, such as\nhip replacements, thanks to their high precision. In pilot studies, robotic devices have been\nfound to reduce the danger of lesions when performing colonoscopy. Outside the operating\nroom, researchers have begun to develop robotic aides for elderly and handicapped people,\nsuch as intelligent robotic walkers and intelligent toys that provide reminders to take medica-\ntion and provide comfort. Researchers are also working on robotic devices for rehabilitation\nthat aid people in performing certain exercises.\nHazardous environments. Robots have assisted people in cleaning up nuclear waste,\nmost notably in Chernobyl and Three Mile Island. Robots were present after the collapse\nof the World Trade Center, where they entered structures deemed too dangerous for human\nsearch and rescue crews.\nSome countries have used robots to transport ammunition and to defuse bombs—a no-\ntoriously dangerous task. A number of research projects are presently developing prototype\nrobots for clearing mineﬁelds, on land and at sea. Most existing robots for these tasks are\nteleoperated—a human operates them by remote control. Providing such robots with auton-\nomy is an important next step.\nExploration. Robots have gone where no one has gone before, including the surface\nof Mars (see Figure 25.2(b) and the cover). Robotic arms assist astronauts in deploying",
  "omy is an important next step.\nExploration. Robots have gone where no one has gone before, including the surface\nof Mars (see Figure 25.2(b) and the cover). Robotic arms assist astronauts in deploying\nand retrieving satellites and in building the International Space Station. Robots also help\nexplore under the sea. They are routinely used to acquire maps of sunken ships. Figure 25.29\nshows a robot mapping an abandoned coal mine, along with a 3D model of the mine acquired Section 25.8.\nApplication Domains\n1009\n(a)\n(b)\nFigure 25.30\n(a) Roomba, the world’s best-selling mobile robot, vacuums ﬂoors. Image\ncourtesy of iRobot, c⃝2009. (b) Robotic hand modeled after human hand. Image courtesy\nof University of Washington and Carnegie Mellon University.\nusing range sensors. In 1996, a team of researches released a legged robot into the crater\nof an active volcano to acquire data for climate research. Unmanned air vehicles known as\ndrones are used in military operations. Robots are becoming very effective tools for gathering\nDRONE\ninformation in domains that are difﬁcult (or dangerous) for people to access.\nPersonal Services. Service is an up-and-coming application domain of robotics. Ser-\nvice robots assist individuals in performing daily tasks. Commercially available domestic\nservice robots include autonomous vacuum cleaners, lawn mowers, and golf caddies. The\nworld’s most popular mobile robot is a personal service robot: the robotic vacuum cleaner\nRoomba, shown in Figure 25.30(a).\nMore than three million Roombas have been sold.\nROOMBA\nRoomba can navigate autonomously and perform its tasks without human help.\nOther service robots operate in public places, such as robotic information kiosks that\nhave been deployed in shopping malls and trade fairs, or in museums as tour guides. Ser-\nvice tasks require human interaction, and the ability to cope robustly with unpredictable and\ndynamic environments.\nEntertainment. Robots have begun to conquer the entertainment and toy industry.\nIn Figure 25.6(b) we see robotic soccer, a competitive game very much like human soc-\nROBOTIC SOCCER\ncer, but played with autonomous mobile robots. Robot soccer provides great opportunities\nfor research in AI, since it raises a range of problems relevant to many other, more serious\nrobot applications. Annual robotic soccer competitions have attracted large numbers of AI\nresearchers and added a lot of excitement to the ﬁeld of robotics.",
  "for research in AI, since it raises a range of problems relevant to many other, more serious\nrobot applications. Annual robotic soccer competitions have attracted large numbers of AI\nresearchers and added a lot of excitement to the ﬁeld of robotics.\nHuman augmentation. A ﬁnal application domain of robotic technology is that of\nhuman augmentation. Researchers have developed legged walking machines that can carry\npeople around, very much like a wheelchair. Several research efforts presently focus on the\ndevelopment of devices that make it easier for people to walk or move their arms by providing\nadditional forces through extraskeletal attachments. If such devices are attached permanently, 1010\nChapter\n25.\nRobotics\nthey can be thought of as artiﬁcial robotic limbs. Figure 25.30(b) shows a robotic hand that\nmay serve as a prosthetic device in the future.\nRobotic teleoperation, or telepresence, is another form of human augmentation. Tele-\noperation involves carrying out tasks over long distances with the aid of robotic devices.\nA popular conﬁguration for robotic teleoperation is the master–slave conﬁguration, where\na robot manipulator emulates the motion of a remote human operator, measured through a\nhaptic interface. Underwater vehicles are often teleoperated; the vehicles can go to a depth\nthat would be dangerous for humans but can still be guided by the human operator. All these\nsystems augment people’s ability to interact with their environments. Some projects go as far\nas replicating humans, at least at a very superﬁcial level. Humanoid robots are now available\ncommercially through several companies in Japan.\n25.9\nSUMMARY\nRobotics concerns itself with intelligent agents that manipulate the physical world. In this\nchapter, we have learned the following basics of robot hardware and software.\n• Robots are equipped with sensors for perceiving their environment and effectors with\nwhich they can assert physical forces on their environment.\nMost robots are either\nmanipulators anchored at ﬁxed locations or mobile robots that can move.\n• Robotic perception concerns itself with estimating decision-relevant quantities from\nsensor data. To do so, we need an internal representation and a method for updating\nthis internal representation over time. Common examples of hard perceptual problems\ninclude localization, mapping, and object recognition.\n• Probabilistic ﬁltering algorithms such as Kalman ﬁlters and particle ﬁlters are useful",
  "this internal representation over time. Common examples of hard perceptual problems\ninclude localization, mapping, and object recognition.\n• Probabilistic ﬁltering algorithms such as Kalman ﬁlters and particle ﬁlters are useful\nfor robot perception. These techniques maintain the belief state, a posterior distribution\nover state variables.\n• The planning of robot motion is usually done in conﬁguration space, where each point\nspeciﬁes the location and orientation of the robot and its joint angles.\n• Conﬁguration space search algorithms include cell decomposition techniques, which\ndecompose the space of all conﬁgurations into ﬁnitely many cells, and skeletonization\ntechniques, which project conﬁguration spaces onto lower-dimensional manifolds. The\nmotion planning problem is then solved using search in these simpler structures.\n• A path found by a search algorithm can be executed by using the path as the reference\ntrajectory for a PID controller. Controllers are necessary in robotics to accommodate\nsmall perturbations; path planning alone is usually insufﬁcient.\n• Potential ﬁeld techniques navigate robots by potential functions, deﬁned over the dis-\ntance to obstacles and the goal location. Potential ﬁeld techniques may get stuck in\nlocal minima, but they can generate motion directly without the need for path planning.\n• Sometimes it is easier to specify a robot controller directly, rather than deriving a path\nfrom an explicit model of the environment. Such controllers can often be written as\nsimple ﬁnite state machines. Bibliographical and Historical Notes\n1011\n• There exist different architectures for software design. The subsumption architec-\nture enables programmers to compose robot controllers from interconnected ﬁnite state\nmachines. Three-layer architectures are common frameworks for developing robot\nsoftware that integrate deliberation, sequencing of subgoals, and control. The related\npipeline architecture processes data in parallel through a sequence of modules, corre-\nsponding to perception, modeling, planning, control, and robot interfaces.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe word robot was popularized by Czech playwright Karel Capek in his 1921 play R.U.R.\n(Rossum’s Universal Robots). The robots, which were grown chemically rather than con-\nstructed mechanically, end up resenting their masters and decide to take over. It appears\n(Glanc, 1978) it was Capek’s brother, Josef, who ﬁrst combined the Czech words “robota”",
  "structed mechanically, end up resenting their masters and decide to take over. It appears\n(Glanc, 1978) it was Capek’s brother, Josef, who ﬁrst combined the Czech words “robota”\n(obligatory work) and “robotnik” (serf) to yield “robot” in his 1917 short story Opilec.\nThe term robotics was ﬁrst used by Asimov (1950). Robotics (under other names) has\na much longer history, however. In ancient Greek mythology, a mechanical man named Talos\nwas supposedly designed and built by Hephaistos, the Greek god of metallurgy. Wonderful\nautomata were built in the 18th century—Jacques Vaucanson’s mechanical duck from 1738\nbeing one early example—but the complex behaviors they exhibited were entirely ﬁxed in\nadvance. Possibly the earliest example of a programmable robot-like device was the Jacquard\nloom (1805), described on page 14.\nThe ﬁrst commercial robot was a robot arm called Unimate, short for universal automa-\nUNIMATE\ntion, developed by Joseph Engelberger and George Devol. In 1961, the ﬁrst Unimate robot\nwas sold to General Motors, where it was used for manufacturing TV picture tubes. 1961\nwas also the year when Devol obtained the ﬁrst U.S. patent on a robot. Eleven years later, in\n1972, Nissan Corp. was among the ﬁrst to automate an entire assembly line with robots, de-\nveloped by Kawasaki with robots supplied by Engelberger and Devol’s company Unimation.\nThis development initiated a major revolution that took place mostly in Japan and the U.S.,\nand that is still ongoing. Unimation followed up in 1978 with the development of the PUMA\nPUMA\nrobot, short for Programmable Universal Machine for Assembly. The PUMA robot, initially\ndeveloped for General Motors, was the de facto standard for robotic manipulation for the two\ndecades that followed. At present, the number of operating robots is estimated at one million\nworldwide, more than half of which are installed in Japan.\nThe literature on robotics research can be divided roughly into two parts: mobile robots\nand stationary manipulators. Grey Walter’s “turtle,” built in 1948, could be considered the\nﬁrst autonomous mobile robot, although its control system was not programmable. The “Hop-\nkins Beast,” built in the early 1960s at Johns Hopkins University, was much more sophisti-\ncated; it had pattern-recognition hardware and could recognize the cover plate of a standard\nAC power outlet. It was capable of searching for outlets, plugging itself in, and then recharg-",
  "cated; it had pattern-recognition hardware and could recognize the cover plate of a standard\nAC power outlet. It was capable of searching for outlets, plugging itself in, and then recharg-\ning its batteries! Still, the Beast had a limited repertoire of skills. The ﬁrst general-purpose\nmobile robot was “Shakey,” developed at what was then the Stanford Research Institute (now 1012\nChapter\n25.\nRobotics\nSRI) in the late 1960s (Fikes and Nilsson, 1971; Nilsson, 1984). Shakey was the ﬁrst robot\nto integrate perception, planning, and execution, and much subsequent research in AI was\ninﬂuenced by this remarkable achievement. Shakey appears on the cover of this book with\nproject leader Charlie Rosen (1917–2002). Other inﬂuential projects include the Stanford\nCart and the CMU Rover (Moravec, 1983). Cox and Wilfong (1990) describes classic work\non autonomous vehicles.\nThe ﬁeld of robotic mapping has evolved from two distinct origins. The ﬁrst thread\nbegan with work by Smith and Cheeseman (1986), who applied Kalman ﬁlters to the si-\nmultaneous localization and mapping problem. This algorithm was ﬁrst implemented by\nMoutarlier and Chatila (1989), and later extended by Leonard and Durrant-Whyte (1992);\nsee Dissanayake et al. (2001) for an overview of early Kalman ﬁlter variations. The second\nthread began with the development of the occupancy grid representation for probabilistic\nOCCUPANCY GRID\nmapping, which speciﬁes the probability that each (x, y) location is occupied by an obsta-\ncle (Moravec and Elfes, 1985). Kuipers and Levitt (1988) were among the ﬁrst to propose\ntopological rather than metric mapping, motivated by models of human spatial cognition. A\nseminal paper by Lu and Milios (1997) recognized the sparseness of the simultaneous local-\nization and mapping problem, which gave rise to the development of nonlinear optimization\ntechniques by Konolige (2004) and Montemerlo and Thrun (2004), as well as hierarchical\nmethods by Bosse et al. (2004). Shatkay and Kaelbling (1997) and Thrun et al. (1998) intro-\nduced the EM algorithm into the ﬁeld of robotic mapping for data association. An overview\nof probabilistic mapping methods can be found in (Thrun et al., 2005).\nEarly mobile robot localization techniques are surveyed by Borenstein et al. (1996).\nAlthough Kalman ﬁltering was well known as a localization method in control theory for\ndecades, the general probabilistic formulation of the localization problem did not appear",
  "Although Kalman ﬁltering was well known as a localization method in control theory for\ndecades, the general probabilistic formulation of the localization problem did not appear\nin the AI literature until much later, through the work of Tom Dean and colleagues (Dean\net al., 1990, 1990) and of Simmons and Koenig (1995). The latter work introduced the term\nMarkov localization. The ﬁrst real-world application of this technique was by Burgard et al.\nMARKOV\nLOCALIZATION\n(1999), through a series of robots that were deployed in museums. Monte Carlo localiza-\ntion based on particle ﬁlters was developed by Fox et al. (1999) and is now widely used.\nThe Rao-Blackwellized particle ﬁlter combines particle ﬁltering for robot localization with\nRAO-\nBLACKWELLIZED\nPARTICLE FILTER\nexact ﬁltering for map building (Murphy and Russell, 2001; Montemerlo et al., 2002).\nThe study of manipulator robots, originally called hand–eye machines, has evolved\nHAND–EYE\nMACHINES\nalong quite different lines. The ﬁrst major effort at creating a hand–eye machine was Hein-\nrich Ernst’s MH-1, described in his MIT Ph.D. thesis (Ernst, 1961). The Machine Intelligence\nproject at Edinburgh also demonstrated an impressive early system for vision-based assem-\nbly called FREDDY (Michie, 1972). After these pioneering efforts, a great deal of work fo-\ncused on geometric algorithms for deterministic and fully observable motion planning prob-\nlems. The PSPACE-hardness of robot motion planning was shown in a seminal paper by\nReif (1979). The conﬁguration space representation is due to Lozano-Perez (1983). A series\nof papers by Schwartz and Sharir on what they called piano movers problems (Schwartz\nPIANO MOVERS\net al., 1987) was highly inﬂuential.\nRecursive cell decomposition for conﬁguration space planning was originated by Brooks\nand Lozano-Perez (1985) and improved signiﬁcantly by Zhu and Latombe (1991). The ear- Bibliographical and Historical Notes\n1013\nliest skeletonization algorithms were based on Voronoi diagrams (Rowat, 1979) and visi-\nbility graphs (Wesley and Lozano-Perez, 1979). Guibas et al. (1992) developed efﬁcient\nVISIBILITY GRAPH\ntechniques for calculating Voronoi diagrams incrementally, and Choset (1996) generalized\nVoronoi diagrams to broader motion-planning problems. John Canny (1988) established the\nﬁrst singly exponential algorithm for motion planning. The seminal text by Latombe (1991)\ncovers a variety of approaches to motion-planning, as do the texts by Choset et al. (2004) and",
  "ﬁrst singly exponential algorithm for motion planning. The seminal text by Latombe (1991)\ncovers a variety of approaches to motion-planning, as do the texts by Choset et al. (2004) and\nLaValle (2006). Kavraki et al. (1996) developed probabilistic roadmaps, which are currently\none of the most effective methods. Fine-motion planning with limited sensing was investi-\ngated by Lozano-Perez et al. (1984) and Canny and Reif (1987). Landmark-based naviga-\ntion (Lazanas and Latombe, 1992) uses many of the same ideas in the mobile robot arena.\nKey work applying POMDP methods (Section 17.4) to motion planning under uncertainty in\nrobotics is due to Pineau et al. (2003) and Roy et al. (2005).\nThe control of robots as dynamical systems—whether for manipulation or navigation—\nhas generated a huge literature that is barely touched on by this chapter. Important works\ninclude a trilogy on impedance control by Hogan (1985) and a general study of robot dy-\nnamics by Featherstone (1987). Dean and Wellman (1991) were among the ﬁrst to try to tie\ntogether control theory and AI planning systems. Three classic textbooks on the mathematics\nof robot manipulation are due to Paul (1981), Craig (1989), and Yoshikawa (1990). The area\nof grasping is also important in robotics—the problem of determining a stable grasp is quite\nGRASPING\ndifﬁcult (Mason and Salisbury, 1985). Competent grasping requires touch sensing, or haptic\nfeedback, to determine contact forces and detect slip (Fearing and Hollerbach, 1985).\nHAPTIC FEEDBACK\nPotential-ﬁeld control, which attempts to solve the motion planning and control prob-\nlems simultaneously, was introduced into the robotics literature by Khatib (1986). In mobile\nrobotics, this idea was viewed as a practical solution to the collision avoidance problem, and\nwas later extended into an algorithm called vector ﬁeld histograms by Borenstein (1991).\nVECTOR FIELD\nHISTOGRAMS\nNavigation functions, the robotics version of a control policy for deterministic MDPs, were\nintroduced by Koditschek (1987). Reinforcement learning in robotics took off with the semi-\nnal work by Bagnell and Schneider (2001) and Ng et al. (2004), who developed the paradigm\nin the context of autonomous helicopter control.\nThe topic of software architectures for robots engenders much religious debate. The\ngood old-fashioned AI candidate—the three-layer architecture—dates back to the design of\nShakey and is reviewed by Gat (1998). The subsumption architecture is due to Brooks (1986),",
  "good old-fashioned AI candidate—the three-layer architecture—dates back to the design of\nShakey and is reviewed by Gat (1998). The subsumption architecture is due to Brooks (1986),\nalthough similar ideas were developed independently by Braitenberg (1984), whose book,\nVehicles, describes a series of simple robots based on the behavioral approach. The suc-\ncess of Brooks’s six-legged walking robot was followed by many other projects. Connell,\nin his Ph.D. thesis (1989), developed a mobile robot capable of retrieving objects that was\nentirely reactive. Extensions of the behavior-based paradigm to multirobot systems can be\nfound in (Mataric, 1997) and (Parker, 1996). GRL (Horswill, 2000) and COLBERT (Kono-\nlige, 1997) abstract the ideas of concurrent behavior-based robotics into general robot control\nlanguages. Arkin (1998) surveys some of the most popular approaches in this ﬁeld.\nResearch on mobile robotics has been stimulated over the last decade by several impor-\ntant competitions. The earliest competition, AAAI’s annual mobile robot competition, began\nin 1992. The ﬁrst competition winner was CARMEL (Congdon et al., 1992). Progress has 1014\nChapter\n25.\nRobotics\nbeen steady and impressive: in more recent competitions robots entered the conference com-\nplex, found their way to the registration desk, registered for the conference, and even gave a\nshort talk. The Robocup competition, launched in 1995 by Kitano and colleagues (1997a),\nROBOCUP\naims to “develop a team of fully autonomous humanoid robots that can win against the hu-\nman world champion team in soccer” by 2050. Play occurs in leagues for simulated robots,\nwheeled robots of different sizes, and humanoid robots. In 2009 teams from 43 countries\nparticipated and the event was broadcast to millions of viewers. Visser and Burkhard (2007)\ntrack the improvements that have been made in perception, team coordination, and low-level\nskills over the past decade.\nThe DARPA Grand Challenge, organized by DARPA in 2004 and 2005, required\nDARPA GRAND\nCHALLENGE\nautonomous robots to travel more than 100 miles through unrehearsed desert terrain in less\nthan 10 hours (Buehler et al., 2006). In the original event in 2004, no robot traveled more\nthan 8 miles, leading many to believe the prize would never be claimed. In 2005, Stanford’s\nrobot STANLEY won the competition in just under 7 hours of travel (Thrun, 2006). DARPA\nthen organized the Urban Challenge, a competition in which robots had to navigate 60 miles",
  "robot STANLEY won the competition in just under 7 hours of travel (Thrun, 2006). DARPA\nthen organized the Urban Challenge, a competition in which robots had to navigate 60 miles\nURBAN CHALLENGE\nin an urban environment with other trafﬁc. Carnegie Mellon University’s robot BOSS took\nﬁrst place and claimed the $2 million prize (Urmson and Whittaker, 2008). Early pioneers in\nthe development of robotic cars included Dickmanns and Zapp (1987) and Pomerleau (1993).\nTwo early textbooks, by Dudek and Jenkin (2000) and Murphy (2000), cover robotics\ngenerally. A more recent overview is due to Bekey (2008). An excellent book on robot\nmanipulation addresses advanced topics such as compliant motion (Mason, 2001). Robot\nmotion planning is covered in Choset et al. (2004) and LaValle (2006). Thrun et al. (2005)\nprovide an introduction into probabilistic robotics. The premiere conference for robotics is\nRobotics: Science and Systems Conference, followed by the IEEE International Conference\non Robotics and Automation. Leading robotics journals include IEEE Robotics and Automa-\ntion, the International Journal of Robotics Research, and Robotics and Autonomous Systems.\nEXERCISES\n25.1\nMonte Carlo localization is biased for any ﬁnite sample size—i.e., the expected value\nof the location computed by the algorithm differs from the true expected value—because of\nthe way particle ﬁltering works. In this question, you are asked to quantify this bias.\nTo simplify, consider a world with four possible robot locations: X = {x1, x2, x3, x4}.\nInitially, we draw N ≥1 samples uniformly from among those locations. As usual, it is\nperfectly acceptable if more than one sample is generated for any of the locations X. Let Z\nbe a Boolean sensor variable characterized by the following conditional probabilities:\nP(z | x1) = 0.8\nP(¬z | x1)\n=\n0.2\nP(z | x2) = 0.4\nP(¬z | x2)\n=\n0.6\nP(z | x3) = 0.1\nP(¬z | x3)\n=\n0.9\nP(z | x4) = 0.1\nP(¬z | x4)\n=\n0.9 . Exercises\n1015\nB\nA\nA\nB\nStarting configuration <−0.5, 7>\nEnding configuration <−0.5, −7> \nFigure 25.31\nA Robot manipulator in two of its possible conﬁgurations.\nMCL uses these probabilities to generate particle weights, which are subsequently normalized\nand used in the resampling process. For simplicity, let us assume we generate only one new\nsample in the resampling process, regardless of N. This sample might correspond to any of\nthe four locations in X. Thus, the sampling process deﬁnes a probability distribution over X.",
  "sample in the resampling process, regardless of N. This sample might correspond to any of\nthe four locations in X. Thus, the sampling process deﬁnes a probability distribution over X.\na. What is the resulting probability distribution over X for this new sample? Answer this\nquestion separately for N = 1, . . . , 10, and for N = ∞.\nb. The difference between two probability distributions P and Q can be measured by the\nKL divergence, which is deﬁned as\nKL(P, Q) =\n\f\ni\nP(xi) log P(xi)\nQ(xi) .\nWhat are the KL divergences between the distributions in (a) and the true posterior?\nc. What modiﬁcation of the problem formulation (not the algorithm!) would guarantee\nthat the speciﬁc estimator above is unbiased even for ﬁnite values of N? Provide at\nleast two such modiﬁcations (each of which should be sufﬁcient).\n25.2\nImplement Monte Carlo localization for a simulated robot with range sensors. A grid\nmap and range data are available from the code repository at aima.cs.berkeley.edu.\nYou should demonstrate successful global localization of the robot.\n25.3\nConsider a robot with two simple manipulators, as shown in ﬁgure 25.31. Manipulator\nA is a square block of side 2 which can slide back and on a rod that runs along the x-axis\nfrom x=−10 to x=10. Manipulator B is a square block of side 2 which can slide back and\non a rod that runs along the y-axis from y=−10 to y=10. The rods lie outside the plane of 1016\nChapter\n25.\nRobotics\nmanipulation, so the rods do not interfere with the movement of the blocks. A conﬁguration\nis then a pair ⟨x, y⟩where x is the x-coordinate of the center of manipulator A and where y is\nthe y-coordinate of the center of manipulator B. Draw the conﬁguration space for this robot,\nindicating the permitted and excluded zones.\n25.4\nSuppose that you are working with the robot in Exercise 25.3 and you are given the\nproblem of ﬁnding a path from the starting conﬁguration of ﬁgure 25.31 to the ending con-\nﬁguration. Consider a potential function\nD(A, Goal)2 + D(B, Goal)2 +\n1\nD(A, B)2\nwhere D(A, B) is the distance between the closest points of A and B.\na. Show that hill climbing in this potential ﬁeld will get stuck in a local minimum.\nb. Describe a potential ﬁeld where hill climbing will solve this particular problem. You\nneed not work out the exact numerical coefﬁcients needed, just the general form of the\nsolution. (Hint: Add a term that “rewards” the hill climber for moving A out of B’s",
  "need not work out the exact numerical coefﬁcients needed, just the general form of the\nsolution. (Hint: Add a term that “rewards” the hill climber for moving A out of B’s\nway, even in a case like this where this does not reduce the distance from A to B in the\nabove sense.)\n25.5\nConsider the robot arm shown in Figure 25.14. Assume that the robot’s base element\nis 60cm long and that its upper arm and forearm are each 40cm long. As argued on page 987,\nthe inverse kinematics of a robot is often not unique. State an explicit closed-form solution of\nthe inverse kinematics for this arm. Under what exact conditions is the solution unique?\n25.6\nImplement an algorithm for calculating the Voronoi diagram of an arbitrary 2D en-\nvironment, described by an n × n Boolean array. Illustrate your algorithm by plotting the\nVoronoi diagram for 10 interesting maps. What is the complexity of your algorithm?\n25.7\nThis exercise explores the relationship between workspace and conﬁguration space\nusing the examples shown in Figure 25.32.\na. Consider the robot conﬁgurations shown in Figure 25.32(a) through (c), ignoring the\nobstacle shown in each of the diagrams. Draw the corresponding arm conﬁgurations in\nconﬁguration space. (Hint: Each arm conﬁguration maps to a single point in conﬁgura-\ntion space, as illustrated in Figure 25.14(b).)\nb. Draw the conﬁguration space for each of the workspace diagrams in Figure 25.32(a)–\n(c). (Hint: The conﬁguration spaces share with the one shown in Figure 25.32(a) the\nregion that corresponds to self-collision, but differences arise from the lack of enclosing\nobstacles and the different locations of the obstacles in these individual ﬁgures.)\nc. For each of the black dots in Figure 25.32(e)–(f), draw the corresponding conﬁgurations\nof the robot arm in workspace. Please ignore the shaded regions in this exercise.\nd. The conﬁguration spaces shown in Figure 25.32(e)–(f) have all been generated by a\nsingle workspace obstacle (dark shading), plus the constraints arising from the self-\ncollision constraint (light shading). Draw, for each diagram, the workspace obstacle\nthat corresponds to the darkly shaded area. Exercises\n1017\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 25.32\nDiagrams for Exercise 25.7.\ne. Figure 25.32(d) illustrates that a single planar obstacle can decompose the workspace\ninto two disconnected regions.\nWhat is the maximum number of disconnected re-\ngions that can be created by inserting a planar obstacle into an obstacle-free, connected",
  "into two disconnected regions.\nWhat is the maximum number of disconnected re-\ngions that can be created by inserting a planar obstacle into an obstacle-free, connected\nworkspace, for a 2DOF robot? Give an example, and argue why no larger number of\ndisconnected regions can be created. How about a non-planar obstacle?\n25.8\nConsider a mobile robot moving on a horizontal surface. Suppose that the robot can\nexecute two kinds of motions:\n• Rolling forward a speciﬁed distance.\n• Rotating in place through a speciﬁed angle.\nThe state of such a robot can be characterized in terms of three parameters ⟨x, y, φ, the x-\ncoordinate and y-coordinate of the robot (more precisely, of its center of rotation) and the\nrobot’s orientation expressed as the angle from the positive x direction. The action “Roll(D)”\nhas the effect of changing state ⟨x, y, φ to ⟨x + D cos(φ), y + D sin(φ), φ⟩, and the action\nRotate(θ) has the effect of changing state ⟨x, y, φ⟩to ⟨x, y, φ + θ⟩.\na. Suppose that the robot is initially at ⟨0, 0, 0⟩and then executes the actions Rotate(60◦),\nRoll(1), Rotate(25◦), Roll(2). What is the ﬁnal state of the robot? 1018\nChapter\n25.\nRobotics\nrobot\nsensor\nrange\ngoal\nFigure 25.33\nSimpliﬁed robot in a maze. See Exercise 25.9.\nb. Now suppose that the robot has imperfect control of its own rotation, and that, if it\nattempts to rotate by θ, it may actually rotate by any angle between θ−10◦and θ+10◦.\nIn that case, if the robot attempts to carry out the sequence of actions in (A), there is\na range of possible ending states. What are the minimal and maximal values of the\nx-coordinate, the y-coordinate and the orientation in the ﬁnal state?\nc. Let us modify the model in (B) to a probabilistic model in which, when the robot\nattempts to rotate by θ, its actual angle of rotation follows a Gaussian distribution\nwith mean θ and standard deviation 10◦. Suppose that the robot executes the actions\nRotate(90◦), Roll(1). Give a simple argument that (a) the expected value of the loca-\ntion at the end is not equal to the result of rotating exactly 90◦and then rolling forward\n1 unit, and (b) that the distribution of locations at the end does not follow a Gaussian.\n(Do not attempt to calculate the true mean or the true distribution.)\nThe point of this exercise is that rotational uncertainty quickly gives rise to a lot of\npositional uncertainty and that dealing with rotational uncertainty is painful, whether",
  "(Do not attempt to calculate the true mean or the true distribution.)\nThe point of this exercise is that rotational uncertainty quickly gives rise to a lot of\npositional uncertainty and that dealing with rotational uncertainty is painful, whether\nuncertainty is treated in terms of hard intervals or probabilistically, due to the fact that\nthe relation between orientation and position is both non-linear and non-monotonic.\n25.9\nConsider the simpliﬁed robot shown in Figure 25.33. Suppose the robot’s Cartesian\ncoordinates are known at all times, as are those of its goal location. However, the locations\nof the obstacles are unknown. The robot can sense obstacles in its immediate proximity, as\nillustrated in this ﬁgure. For simplicity, let us assume the robot’s motion is noise-free, and\nthe state space is discrete. Figure 25.33 is only one example; in this exercise you are required\nto address all possible grid worlds with a valid path from the start to the goal location.\na. Design a deliberate controller that guarantees that the robot always reaches its goal\nlocation if at all possible. The deliberate controller can memorize measurements in the\nform of a map that is being acquired as the robot moves. Between individual moves, it\nmay spend arbitrary time deliberating. Exercises\n1019\nb. Now design a reactive controller for the same task. This controller may not memorize\npast sensor measurements. (It may not build a map!) Instead, it has to make all decisions\nbased on the current measurement, which includes knowledge of its own location and\nthat of the goal. The time to make a decision must be independent of the environment\nsize or the number of past time steps. What is the maximum number of steps that it may\ntake for your robot to arrive at the goal?\nc. How will your controllers from (a) and (b) perform if any of the following six conditions\napply: continuous state space, noise in perception, noise in motion, noise in both per-\nception and motion, unknown location of the goal (the goal can be detected only when\nwithin sensor range), or moving obstacles. For each condition and each controller, give\nan example of a situation where the robot fails (or explain why it cannot fail).\n25.10\nIn Figure 25.24(b) on page 1001, we encountered an augmented ﬁnite state machine\nfor the control of a single leg of a hexapod robot. In this exercise, the aim is to design an\nAFSM that, when combined with six copies of the individual leg controllers, results in efﬁ-",
  "for the control of a single leg of a hexapod robot. In this exercise, the aim is to design an\nAFSM that, when combined with six copies of the individual leg controllers, results in efﬁ-\ncient, stable locomotion. For this purpose, you have to augment the individual leg controller\nto pass messages to your new AFSM and to wait until other messages arrive. Argue why your\ncontroller is efﬁcient, in that it does not unnecessarily waste energy (e.g., by sliding legs),\nand in that it propels the robot at reasonably high speeds. Prove that your controller satisﬁes\nthe dynamic stability condition given on page 977.\n25.11\n(This exercise was ﬁrst devised by Michael Genesereth and Nils Nilsson. It works\nfor ﬁrst graders through graduate students.) Humans are so adept at basic household tasks\nthat they often forget how complex these tasks are. In this exercise you will discover the\ncomplexity and recapitulate the last 30 years of developments in robotics. Consider the task\nof building an arch out of three blocks. Simulate a robot with four humans as follows:\nBrain. The Brain direct the hands in the execution of a plan to achieve the goal. The\nBrain receives input from the Eyes, but cannot see the scene directly. The brain is the only\none who knows what the goal is.\nEyes. The Eyes report a brief description of the scene to the Brain: “There is a red box\nstanding on top of a green box, which is on its side” Eyes can also answer questions from the\nBrain such as, “Is there a gap between the Left Hand and the red box?” If you have a video\ncamera, point it at the scene and allow the eyes to look at the viewﬁnder of the video camera,\nbut not directly at the scene.\nLeft hand and right hand. One person plays each Hand. The two Hands stand next to\neach other, each wearing an oven mitt on one hand, Hands execute only simple commands\nfrom the Brain—for example, “Left Hand, move two inches forward.” They cannot execute\ncommands other than motions; for example, they cannot be commanded to “Pick up the box.”\nThe Hands must be blindfolded. The only sensory capability they have is the ability to tell\nwhen their path is blocked by an immovable obstacle such as a table or the other Hand. In\nsuch cases, they can beep to inform the Brain of the difﬁculty. 26\nPHILOSOPHICAL\nFOUNDATIONS\nIn which we consider what it means to think and whether artifacts could and\nshould ever do so.\nPhilosophers have been around far longer than computers and have been trying to resolve",
  "PHILOSOPHICAL\nFOUNDATIONS\nIn which we consider what it means to think and whether artifacts could and\nshould ever do so.\nPhilosophers have been around far longer than computers and have been trying to resolve\nsome questions that relate to AI: How do minds work? Is it possible for machines to act\nintelligently in the way that people do, and if they did, would they have real, conscious\nminds? What are the ethical implications of intelligent machines?\nFirst, some terminology: the assertion that machines could act as if they were intelligent\nis called the weak AI hypothesis by philosophers, and the assertion that machines that do so\nWEAK AI\nare actually thinking (not just simulating thinking) is called the strong AI hypothesis.\nSTRONG AI\nMost AI researchers take the weak AI hypothesis for granted, and don’t care about the\nstrong AI hypothesis—as long as their program works, they don’t care whether you call it a\nsimulation of intelligence or real intelligence. All AI researchers should be concerned with\nthe ethical implications of their work.\n26.1\nWEAK AI: CAN MACHINES ACT INTELLIGENTLY?\nThe proposal for the 1956 summer workshop that deﬁned the ﬁeld of Artiﬁcial Intelligence\n(McCarthy et al., 1955) made the assertion that “Every aspect of learning or any other feature\nof intelligence can be so precisely described that a machine can be made to simulate it.” Thus,\nAI was founded on the assumption that weak AI is possible. Others have asserted that weak\nAI is impossible: “Artiﬁcial intelligence pursued within the cult of computationalism stands\nnot even a ghost of a chance of producing durable results” (Sayre, 1993).\nClearly, whether AI is impossible depends on how it is deﬁned. In Section 1.1, we de-\nﬁned AI as the quest for the best agent program on a given architecture. With this formulation,\nAI is by deﬁnition possible: for any digital architecture with k bits of program storage there\nare exactly 2k agent programs, and all we have to do to ﬁnd the best one is enumerate and test\nthem all. This might not be feasible for large k, but philosophers deal with the theoretical,\nnot the practical.\n1020 Section 26.1.\nWeak AI: Can Machines Act Intelligently?\n1021\nOur deﬁnition of AI works well for the engineering problem of ﬁnding a good agent,\ngiven an architecture. Therefore, we’re tempted to end this section right now, answering the\ntitle question in the afﬁrmative. But philosophers are interested in the problem of compar-",
  "given an architecture. Therefore, we’re tempted to end this section right now, answering the\ntitle question in the afﬁrmative. But philosophers are interested in the problem of compar-\ning two architectures—human and machine. Furthermore, they have traditionally posed the\nquestion not in terms of maximizing expected utility but rather as, “Can machines think?”\nCAN MACHINES\nTHINK?\nThe computer scientist Edsger Dijkstra (1984) said that “The question of whether Ma-\nchines Can Think . . . is about as relevant as the question of whether Submarines Can Swim.”\nCAN SUBMARINES\nSWIM?\nThe American Heritage Dictionary’s ﬁrst deﬁnition of swim is “To move through water by\nmeans of the limbs, ﬁns, or tail,” and most people agree that submarines, being limbless,\ncannot swim. The dictionary also deﬁnes ﬂy as “To move through the air by means of wings\nor winglike parts,” and most people agree that airplanes, having winglike parts, can ﬂy. How-\never, neither the questions nor the answers have any relevance to the design or capabilities of\nairplanes and submarines; rather they are about the usage of words in English. (The fact that\nships do swim in Russian only ampliﬁes this point.). The practical possibility of “thinking\nmachines” has been with us for only 50 years or so, not long enough for speakers of English to\nsettle on a meaning for the word “think”—does it require “a brain” or just “brain-like parts.”\nAlan Turing, in his famous paper “Computing Machinery and Intelligence” (1950), sug-\ngested that instead of asking whether machines can think, we should ask whether machines\ncan pass a behavioral intelligence test, which has come to be called the Turing Test. The test\nTURING TEST\nis for a program to have a conversation (via online typed messages) with an interrogator for\nﬁve minutes. The interrogator then has to guess if the conversation is with a program or a\nperson; the program passes the test if it fools the interrogator 30% of the time. Turing con-\njectured that, by the year 2000, a computer with a storage of 109 units could be programmed\nwell enough to pass the test. He was wrong—programs have yet to fool a sophisticated judge.\nOn the other hand, many people have been fooled when they didn’t know they might\nbe chatting with a computer. The ELIZA program and Internet chatbots such as MGONZ\n(Humphrys, 2008) and NATACHATA have fooled their correspondents repeatedly, and the\nchatbot CYBERLOVER has attracted the attention of law enforcement because of its penchant",
  "(Humphrys, 2008) and NATACHATA have fooled their correspondents repeatedly, and the\nchatbot CYBERLOVER has attracted the attention of law enforcement because of its penchant\nfor tricking fellow chatters into divulging enough personal information that their identity can\nbe stolen. The Loebner Prize competition, held annually since 1991, is the longest-running\nTuring Test-like contest. The competitions have led to better models of human typing errors.\nTuring himself examined a wide variety of possible objections to the possibility of in-\ntelligent machines, including virtually all of those that have been raised in the half-century\nsince his paper appeared. We will look at some of them.\n26.1.1\nThe argument from disability\nThe “argument from disability” makes the claim that “a machine can never do X.” As exam-\nples of X, Turing lists the following:\nBe kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right\nfrom wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone\nfall in love with it, learn from experience, use words properly, be the subject of its own\nthought, have as much diversity of behavior as man, do something really new. 1022\nChapter\n26.\nPhilosophical Foundations\nIn retrospect, some of these are rather easy—we’re all familiar with computers that “make\nmistakes.” We are also familiar with a century-old technology that has had a proven ability\nto “make someone fall in love with it”—the teddy bear. Computer chess expert David Levy\npredicts that by 2050 people will routinely fall in love with humanoid robots (Levy, 2007).\nAs for a robot falling in love, that is a common theme in ﬁction,1 but there has been only lim-\nited speculation about whether it is in fact likely (Kim et al., 2007). Programs do play chess,\ncheckers and other games; inspect parts on assembly lines, steer cars and helicopters; diag-\nnose diseases; and do hundreds of other tasks as well as or better than humans. Computers\nhave made small but signiﬁcant discoveries in astronomy, mathematics, chemistry, mineral-\nogy, biology, computer science, and other ﬁelds. Each of these required performance at the\nlevel of a human expert.\nGiven what we now know about computers, it is not surprising that they do well at\ncombinatorial problems such as playing chess. But algorithms also perform at human levels\non tasks that seemingly involve human judgment, or as Turing put it, “learning from experi-",
  "combinatorial problems such as playing chess. But algorithms also perform at human levels\non tasks that seemingly involve human judgment, or as Turing put it, “learning from experi-\nence” and the ability to “tell right from wrong.” As far back as 1955, Paul Meehl (see also\nGrove and Meehl, 1996) studied the decision-making processes of trained experts at subjec-\ntive tasks such as predicting the success of a student in a training program or the recidivism\nof a criminal. In 19 out of the 20 studies he looked at, Meehl found that simple statistical\nlearning algorithms (such as linear regression or naive Bayes) predict better than the experts.\nThe Educational Testing Service has used an automated program to grade millions of essay\nquestions on the GMAT exam since 1999. The program agrees with human graders 97% of\nthe time, about the same level that two human graders agree (Burstein et al., 2001).\nIt is clear that computers can do many things as well as or better than humans, including\nthings that people believe require great human insight and understanding. This does not mean,\nof course, that computers use insight and understanding in performing these tasks—those are\nnot part of behavior, and we address such questions elsewhere—but the point is that one’s\nﬁrst guess about the mental processes required to produce a given behavior is often wrong. It\nis also true, of course, that there are many tasks at which computers do not yet excel (to put\nit mildly), including Turing’s task of carrying on an open-ended conversation.\n26.1.2\nThe mathematical objection\nIt is well known, through the work of Turing (1936) and G¨odel (1931), that certain math-\nematical questions are in principle unanswerable by particular formal systems. G¨odel’s in-\ncompleteness theorem (see Section 9.5) is the most famous example of this. Brieﬂy, for any\nformal axiomatic system F powerful enough to do arithmetic, it is possible to construct a\nso-called G¨odel sentence G(F) with the following properties:\n• G(F) is a sentence of F, but cannot be proved within F.\n• If F is consistent, then G(F) is true.\n1 For example, the opera Copp´elia (1870), the novel Do Androids Dream of Electric Sheep? (1968), the movies\nAI (2001) and Wall-E (2008), and in song, Noel Coward’s 1955 version of Let’s Do It: Let’s Fall in Love predicted\n“probably we’ll live to see machines do it.” He didn’t. Section 26.1.\nWeak AI: Can Machines Act Intelligently?\n1023",
  "AI (2001) and Wall-E (2008), and in song, Noel Coward’s 1955 version of Let’s Do It: Let’s Fall in Love predicted\n“probably we’ll live to see machines do it.” He didn’t. Section 26.1.\nWeak AI: Can Machines Act Intelligently?\n1023\nPhilosophers such as J. R. Lucas (1961) have claimed that this theorem shows that machines\nare mentally inferior to humans, because machines are formal systems that are limited by the\nincompleteness theorem—they cannot establish the truth of their own G¨odel sentence—while\nhumans have no such limitation. This claim has caused decades of controversy, spawning a\nvast literature, including two books by the mathematician Sir Roger Penrose (1989, 1994)\nthat repeat the claim with some fresh twists (such as the hypothesis that humans are different\nbecause their brains operate by quantum gravity). We will examine only three of the problems\nwith the claim.\nFirst, G¨odel’s incompleteness theorem applies only to formal systems that are powerful\nenough to do arithmetic. This includes Turing machines, and Lucas’s claim is in part based\non the assertion that computers are Turing machines. This is a good approximation, but is not\nquite true. Turing machines are inﬁnite, whereas computers are ﬁnite, and any computer can\ntherefore be described as a (very large) system in propositional logic, which is not subject to\nG¨odel’s incompleteness theorem. Second, an agent should not be too ashamed that it cannot\nestablish the truth of some sentence while other agents can. Consider the sentence\nJ. R. Lucas cannot consistently assert that this sentence is true.\nIf Lucas asserted this sentence, then he would be contradicting himself, so therefore Lucas\ncannot consistently assert it, and hence it must be true. We have thus demonstrated that there\nis a sentence that Lucas cannot consistently assert while other people (and machines) can. But\nthat does not make us think less of Lucas. To take another example, no human could compute\nthe sum of a billion 10 digit numbers in his or her lifetime, but a computer could do it in\nseconds. Still, we do not see this as a fundamental limitation in the human’s ability to think.\nHumans were behaving intelligently for thousands of years before they invented mathematics,\nso it is unlikely that formal mathematical reasoning plays more than a peripheral role in what\nit means to be intelligent.\nThird, and most important, even if we grant that computers have limitations on what",
  "so it is unlikely that formal mathematical reasoning plays more than a peripheral role in what\nit means to be intelligent.\nThird, and most important, even if we grant that computers have limitations on what\nthey can prove, there is no evidence that humans are immune from those limitations. It is\nall too easy to show rigorously that a formal system cannot do X, and then claim that hu-\nmans can do X using their own informal method, without giving any evidence for this claim.\nIndeed, it is impossible to prove that humans are not subject to G¨odel’s incompleteness theo-\nrem, because any rigorous proof would require a formalization of the claimed unformalizable\nhuman talent, and hence refute itself. So we are left with an appeal to intuition that humans\ncan somehow perform superhuman feats of mathematical insight. This appeal is expressed\nwith arguments such as “we must assume our own consistency, if thought is to be possible at\nall” (Lucas, 1976). But if anything, humans are known to be inconsistent. This is certainly\ntrue for everyday reasoning, but it is also true for careful mathematical thought. A famous\nexample is the four-color map problem. Alfred Kempe published a proof in 1879 that was\nwidely accepted and contributed to his election as a Fellow of the Royal Society. In 1890,\nhowever, Percy Heawood pointed out a ﬂaw and the theorem remained unproved until 1977. 1024\nChapter\n26.\nPhilosophical Foundations\n26.1.3\nThe argument from informality\nOne of the most inﬂuential and persistent criticisms of AI as an enterprise was raised by Tur-\ning as the “argument from informality of behavior.” Essentially, this is the claim that human\nbehavior is far too complex to be captured by any simple set of rules and that because com-\nputers can do no more than follow a set of rules, they cannot generate behavior as intelligent\nas that of humans. The inability to capture everything in a set of logical rules is called the\nqualiﬁcation problem in AI.\nQUALIFICATION\nPROBLEM\nThe principal proponent of this view has been the philosopher Hubert Dreyfus, who\nhas produced a series of inﬂuential critiques of artiﬁcial intelligence: What Computers Can’t\nDo (1972), the sequel What Computers Still Can’t Do (1992), and, with his brother Stuart,\nMind Over Machine (1986).\nThe position they criticize came to be called “Good Old-Fashioned AI,” or GOFAI, a\nterm coined by philosopher John Haugeland (1985).\nGOFAI is supposed to claim that all",
  "Mind Over Machine (1986).\nThe position they criticize came to be called “Good Old-Fashioned AI,” or GOFAI, a\nterm coined by philosopher John Haugeland (1985).\nGOFAI is supposed to claim that all\nintelligent behavior can be captured by a system that reasons logically from a set of facts and\nrules describing the domain. It therefore corresponds to the simplest logical agent described\nin Chapter 7. Dreyfus is correct in saying that logical agents are vulnerable to the qualiﬁcation\nproblem. As we saw in Chapter 13, probabilistic reasoning systems are more appropriate for\nopen-ended domains. The Dreyfus critique therefore is not addressed against computers per\nse, but rather against one particular way of programming them. It is reasonable to suppose,\nhowever, that a book called What First-Order Logical Rule-Based Systems Without Learning\nCan’t Do might have had less impact.\nUnder Dreyfus’s view, human expertise does include knowledge of some rules, but only\nas a “holistic context” or “background” within which humans operate. He gives the example\nof appropriate social behavior in giving and receiving gifts: “Normally one simply responds\nin the appropriate circumstances by giving an appropriate gift.” One apparently has “a direct\nsense of how things are done and what to expect.” The same claim is made in the context of\nchess playing: “A mere chess master might need to ﬁgure out what to do, but a grandmaster\njust sees the board as demanding a certain move . . . the right response just pops into his or her\nhead.” It is certainly true that much of the thought processes of a present-giver or grandmaster\nis done at a level that is not open to introspection by the conscious mind. But that does not\nmean that the thought processes do not exist. The important question that Dreyfus does not\nanswer is how the right move gets into the grandmaster’s head. One is reminded of Daniel\nDennett’s (1984) comment,\nIt is rather as if philosophers were to proclaim themselves expert explainers of the meth-\nods of stage magicians, and then, when we ask how the magician does the sawing-the-\nlady-in-half trick, they explain that it is really quite obvious: the magician doesn’t really\nsaw her in half; he simply makes it appear that he does. “But how does he do that?” we\nask. “Not our department,” say the philosophers.\nDreyfus and Dreyfus (1986) propose a ﬁve-stage process of acquiring expertise, beginning",
  "saw her in half; he simply makes it appear that he does. “But how does he do that?” we\nask. “Not our department,” say the philosophers.\nDreyfus and Dreyfus (1986) propose a ﬁve-stage process of acquiring expertise, beginning\nwith rule-based processing (of the sort proposed in GOFAI) and ending with the ability to\nselect correct responses instantaneously. In making this proposal, Dreyfus and Dreyfus in\neffect move from being AI critics to AI theorists—they propose a neural network architecture Section 26.1.\nWeak AI: Can Machines Act Intelligently?\n1025\norganized into a vast “case library,” but point out several problems. Fortunately, all of their\nproblems have been addressed, some with partial success and some with total success. Their\nproblems include the following:\n1. Good generalization from examples cannot be achieved without background knowl-\nedge. They claim no one has any idea how to incorporate background knowledge into\nthe neural network learning process. In fact, we saw in Chapters 19 and 20 that there\nare techniques for using prior knowledge in learning algorithms. Those techniques,\nhowever, rely on the availability of knowledge in explicit form, something that Dreyfus\nand Dreyfus strenuously deny. In our view, this is a good reason for a serious redesign\nof current models of neural processing so that they can take advantage of previously\nlearned knowledge in the way that other learning algorithms do.\n2. Neural network learning is a form of supervised learning (see Chapter 18), requiring\nthe prior identiﬁcation of relevant inputs and correct outputs. Therefore, they claim,\nit cannot operate autonomously without the help of a human trainer. In fact, learning\nwithout a teacher can be accomplished by unsupervised learning (Chapter 20) and\nreinforcement learning (Chapter 21).\n3. Learning algorithms do not perform well with many features, and if we pick a subset\nof features, “there is no known way of adding new features should the current set prove\ninadequate to account for the learned facts.” In fact, new methods such as support\nvector machines handle large feature sets very well. With the introduction of large\nWeb-based data sets, many applications in areas such as language processing (Sha and\nPereira, 2003) and computer vision (Viola and Jones, 2002a) routinely handle millions\nof features. We saw in Chapter 19 that there are also principled ways to generate new\nfeatures, although much more work is needed.",
  "Pereira, 2003) and computer vision (Viola and Jones, 2002a) routinely handle millions\nof features. We saw in Chapter 19 that there are also principled ways to generate new\nfeatures, although much more work is needed.\n4. The brain is able to direct its sensors to seek relevant information and to process it\nto extract aspects relevant to the current situation. But, Dreyfus and Dreyfus claim,\n“Currently, no details of this mechanism are understood or even hypothesized in a way\nthat could guide AI research.” In fact, the ﬁeld of active vision, underpinned by the\ntheory of information value (Chapter 16), is concerned with exactly the problem of\ndirecting sensors, and already some robots have incorporated the theoretical results\nobtained. STANLEY’s 132-mile trip through the desert (page 28) was made possible in\nlarge part by an active sensing system of this kind.\nIn sum, many of the issues Dreyfus has focused on—background commonsense knowledge,\nthe qualiﬁcation problem, uncertainty, learning, compiled forms of decision making—are\nindeed important issues, and have by now been incorporated into standard intelligent agent\ndesign. In our view, this is evidence of AI’s progress, not of its impossibility.\nOne of Dreyfus’ strongest arguments is for situated agents rather than disembodied\nlogical inference engines. An agent whose understanding of “dog” comes only from a limited\nset of logical sentences such as “Dog(x) ⇒Mammal(x)” is at a disadvantage compared\nto an agent that has watched dogs run, has played fetch with them, and has been licked by\none. As philosopher Andy Clark (1998) says, “Biological brains are ﬁrst and foremost the\ncontrol systems for biological bodies. Biological bodies move and act in rich real-world 1026\nChapter\n26.\nPhilosophical Foundations\nsurroundings.” To understand how human (or other animal) agents work, we have to consider\nthe whole agent, not just the agent program. Indeed, the embodied cognition approach claims\nEMBODIED\nCOGNITION\nthat it makes no sense to consider the brain separately: cognition takes place within a body,\nwhich is embedded in an environment. We need to study the system as a whole; the brain\naugments its reasoning by referring to the environment, as the reader does in perceiving (and\ncreating) marks on paper to transfer knowledge. Under the embodied cognition program,\nrobotics, vision, and other sensors become central, not peripheral.\n26.2\nSTRONG AI: CAN MACHINES REALLY THINK?",
  "creating) marks on paper to transfer knowledge. Under the embodied cognition program,\nrobotics, vision, and other sensors become central, not peripheral.\n26.2\nSTRONG AI: CAN MACHINES REALLY THINK?\nMany philosophers have claimed that a machine that passes the Turing Test would still not\nbe actually thinking, but would be only a simulation of thinking. Again, the objection was\nforeseen by Turing. He cites a speech by Professor Geoffrey Jefferson (1949):\nNot until a machine could write a sonnet or compose a concerto because of thoughts and\nemotions felt, and not by the chance fall of symbols, could we agree that machine equals\nbrain—that is, not only write it but know that it had written it.\nTuring calls this the argument from consciousness—the machine has to be aware of its own\nmental states and actions. While consciousness is an important subject, Jefferson’s key point\nactually relates to phenomenology, or the study of direct experience: the machine has to\nactually feel emotions. Others focus on intentionality—that is, the question of whether the\nmachine’s purported beliefs, desires, and other representations are actually “about” some-\nthing in the real world.\nTuring’s response to the objection is interesting. He could have presented reasons that\nmachines can in fact be conscious (or have phenomenology, or have intentions). Instead, he\nmaintains that the question is just as ill-deﬁned as asking, “Can machines think?” Besides,\nwhy should we insist on a higher standard for machines than we do for humans? After all,\nin ordinary life we never have any direct evidence about the internal mental states of other\nhumans. Nevertheless, Turing says, “Instead of arguing continually over this point, it is usual\nto have the polite convention that everyone thinks.”\nTuring argues that Jefferson would be willing to extend the polite convention to ma-\nchines if only he had experience with ones that act intelligently. He cites the following dialog,\nwhich has become such a part of AI’s oral tradition that we simply have to include it:\nHUMAN: In the ﬁrst line of your sonnet which reads “shall I compare thee to a summer’s\nday,” would not a “spring day” do as well or better?\nMACHINE: It wouldn’t scan.\nHUMAN: How about “a winter’s day.” That would scan all right.\nMACHINE: Yes, but nobody wants to be compared to a winter’s day.\nHUMAN: Would you say Mr. Pickwick reminded you of Christmas?\nMACHINE: In a way.",
  "MACHINE: It wouldn’t scan.\nHUMAN: How about “a winter’s day.” That would scan all right.\nMACHINE: Yes, but nobody wants to be compared to a winter’s day.\nHUMAN: Would you say Mr. Pickwick reminded you of Christmas?\nMACHINE: In a way.\nHUMAN: Yet Christmas is a winter’s day, and I do not think Mr. Pickwick would mind\nthe comparison. Section 26.2.\nStrong AI: Can Machines Really Think?\n1027\nMACHINE: I don’t think you’re serious. By a winter’s day one means a typical winter’s\nday, rather than a special one like Christmas.\nOne can easily imagine some future time in which such conversations with machines are\ncommonplace, and it becomes customary to make no linguistic distinction between “real”\nand “artiﬁcial” thinking. A similar transition occurred in the years after 1848, when artiﬁcial\nurea was synthesized for the ﬁrst time by Frederick W¨ohler. Prior to this event, organic and\ninorganic chemistry were essentially disjoint enterprises and many thought that no process\ncould exist that would convert inorganic chemicals into organic material. Once the synthesis\nwas accomplished, chemists agreed that artiﬁcial urea was urea, because it had all the right\nphysical properties. Those who had posited an intrinsic property possessed by organic ma-\nterial that inorganic material could never have were faced with the impossibility of devising\nany test that could reveal the supposed deﬁciency of artiﬁcial urea.\nFor thinking, we have not yet reached our 1848 and there are those who believe that\nartiﬁcial thinking, no matter how impressive, will never be real. For example, the philosopher\nJohn Searle (1980) argues as follows:\nNo one supposes that a computer simulation of a storm will leave us all wet . . . Why on\nearth would anyone in his right mind suppose a computer simulation of mental processes\nactually had mental processes? (pp. 37–38)\nWhile it is easy to agree that computer simulations of storms do not make us wet, it is not\nclear how to carry this analogy over to computer simulations of mental processes. After\nall, a Hollywood simulation of a storm using sprinklers and wind machines does make the\nactors wet, and a video game simulation of a storm does make the simulated characters wet.\nMost people are comfortable saying that a computer simulation of addition is addition, and\nof chess is chess. In fact, we typically speak of an implementation of addition or chess, not a\nsimulation. Are mental processes more like storms, or more like addition?",
  "of chess is chess. In fact, we typically speak of an implementation of addition or chess, not a\nsimulation. Are mental processes more like storms, or more like addition?\nTuring’s answer—the polite convention—suggests that the issue will eventually go\naway by itself once machines reach a certain level of sophistication. This would have the\neffect of dissolving the difference between weak and strong AI. Against this, one may insist\nthat there is a factual issue at stake: humans do have real minds, and machines might or\nmight not. To address this factual issue, we need to understand how it is that humans have\nreal minds, not just bodies that generate neurophysiological processes. Philosophical efforts\nto solve this mind–body problem are directly relevant to the question of whether machines\nMIND–BODY\nPROBLEM\ncould have real minds.\nThe mind–body problem was considered by the ancient Greek philosophers and by var-\nious schools of Hindu thought, but was ﬁrst analyzed in depth by the 17th-century French\nphilosopher and mathematician Ren´e Descartes. His Meditations on First Philosophy (1641)\nconsidered the mind’s activity of thinking (a process with no spatial extent or material prop-\nerties) and the physical processes of the body, concluding that the two must exist in separate\nrealms—what we would now call a dualist theory. The mind–body problem faced by du-\nDUALISM\nalists is the question of how the mind can control the body if the two are really separate.\nDescartes speculated that the two might interact through the pineal gland, which simply begs\nthe question of how the mind controls the pineal gland. 1028\nChapter\n26.\nPhilosophical Foundations\nThe monist theory of mind, often called physicalism, avoids this problem by asserting\nMONISM\nPHYSICALISM\nthe mind is not separate from the body—that mental states are physical states. Most modern\nphilosophers of mind are physicalists of one form or another, and physicalism allows, at least\nin principle, for the possibility of strong AI. The problem for physicalists is to explain how\nphysical states—in particular, the molecular conﬁgurations and electrochemical processes of\nthe brain—can simultaneously be mental states, such as being in pain, enjoying a hamburger,\nMENTAL STATES\nknowing that one is riding a horse, or believing that Vienna is the capital of Austria.\n26.2.1\nMental states and the brain in a vat\nPhysicalist philosophers have attempted to explicate what it means to say that a person—and,",
  "MENTAL STATES\nknowing that one is riding a horse, or believing that Vienna is the capital of Austria.\n26.2.1\nMental states and the brain in a vat\nPhysicalist philosophers have attempted to explicate what it means to say that a person—and,\nby extension, a computer—is in a particular mental state. They have focused in particular on\nintentional states. These are states, such as believing, knowing, desiring, fearing, and so on,\nINTENTIONAL STATE\nthat refer to some aspect of the external world. For example, the knowledge that one is eating\na hamburger is a belief about the hamburger and what is happening to it.\nIf physicalism is correct, it must be the case that the proper description of a person’s\nmental state is determined by that person’s brain state. Thus, if I am currently focused on\neating a hamburger in a mindful way, my instantaneous brain state is an instance of the class of\nmental states “knowing that one is eating a hamburger.” Of course, the speciﬁc conﬁgurations\nof all the atoms of my brain are not essential: there are many conﬁgurations of my brain, or\nof other people’s brain, that would belong to the same class of mental states. The key point is\nthat the same brain state could not correspond to a fundamentally distinct mental state, such\nas the knowledge that one is eating a banana.\nThe simplicity of this view is challenged by some simple thought experiments. Imag-\nine, if you will, that your brain was removed from your body at birth and placed in a mar-\nvelously engineered vat. The vat sustains your brain, allowing it to grow and develop. At the\nsame time, electronic signals are fed to your brain from a computer simulation of an entirely\nﬁctitious world, and motor signals from your brain are intercepted and used to modify the\nsimulation as appropriate.2 In fact, the simulated life you live replicates exactly the life you\nwould have lived, had your brain not been placed in the vat, including simulated eating of\nsimulated hamburgers. Thus, you could have a brain state identical to that of someone who is\nreally eating a real hamburger, but it would be literally false to say that you have the mental\nstate “knowing that one is eating a hamburger.” You aren’t eating a hamburger, you have\nnever even experienced a hamburger, and you could not, therefore, have such a mental state.\nThis example seems to contradict the view that brain states determine mental states. One",
  "never even experienced a hamburger, and you could not, therefore, have such a mental state.\nThis example seems to contradict the view that brain states determine mental states. One\nway to resolve the dilemma is to say that the content of mental states can be interpreted from\ntwo different points of view. The “wide content” view interprets it from the point of view\nWIDE CONTENT\nof an omniscient outside observer with access to the whole situation, who can distinguish\ndifferences in the world. Under this view, the content of mental states involves both the brain\nstate and the environment history. Narrow content, on the other hand, considers only the\nNARROW CONTENT\nbrain state. The narrow content of the brain states of a real hamburger-eater and a brain-in-a-\nvat “hamburger”-“eater” is the same in both cases.\n2 This situation may be familiar to those who have seen the 1999 ﬁlm The Matrix. Section 26.2.\nStrong AI: Can Machines Really Think?\n1029\nWide content is entirely appropriate if one’s goals are to ascribe mental states to others\nwho share one’s world, to predict their likely behavior and its effects, and so on. This is the\nsetting in which our ordinary language about mental content has evolved. On the other hand,\nif one is concerned with the question of whether AI systems are really thinking and really\ndo have mental states, then narrow content is appropriate; it simply doesn’t make sense to\nsay that whether or not an AI system is really thinking depends on conditions outside that\nsystem. Narrow content is also relevant if we are thinking about designing AI systems or\nunderstanding their operation, because it is the narrow content of a brain state that determines\nwhat will be the (narrow content of the) next brain state. This leads naturally to the idea that\nwhat matters about a brain state—what makes it have one kind of mental content and not\nanother—is its functional role within the mental operation of the entity involved.\n26.2.2\nFunctionalism and the brain replacement experiment\nThe theory of functionalism says that a mental state is any intermediate causal condition\nFUNCTIONALISM\nbetween input and output. Under functionalist theory, any two systems with isomorphic\ncausal processes would have the same mental states. Therefore, a computer program could\nhave the same mental states as a person. Of course, we have not yet said what “isomorphic”\nreally means, but the assumption is that there is some level of abstraction below which the",
  "have the same mental states as a person. Of course, we have not yet said what “isomorphic”\nreally means, but the assumption is that there is some level of abstraction below which the\nspeciﬁc implementation does not matter.\nThe claims of functionalism are illustrated most clearly by the brain replacement ex-\nperiment. This thought experiment was introduced by the philosopher Clark Glymour and\nwas touched on by John Searle (1980), but is most commonly associated with roboticist Hans\nMoravec (1988). It goes like this: Suppose neurophysiology has developed to the point where\nthe input–output behavior and connectivity of all the neurons in the human brain are perfectly\nunderstood. Suppose further that we can build microscopic electronic devices that mimic this\nbehavior and can be smoothly interfaced to neural tissue. Lastly, suppose that some mirac-\nulous surgical technique can replace individual neurons with the corresponding electronic\ndevices without interrupting the operation of the brain as a whole. The experiment consists\nof gradually replacing all the neurons in someone’s head with electronic devices.\nWe are concerned with both the external behavior and the internal experience of the\nsubject, during and after the operation. By the deﬁnition of the experiment, the subject’s\nexternal behavior must remain unchanged compared with what would be observed if the\noperation were not carried out.3 Now although the presence or absence of consciousness\ncannot easily be ascertained by a third party, the subject of the experiment ought at least to\nbe able to record any changes in his or her own conscious experience. Apparently, there is\na direct clash of intuitions as to what would happen. Moravec, a robotics researcher and\nfunctionalist, is convinced his consciousness would remain unaffected. Searle, a philosopher\nand biological naturalist, is equally convinced his consciousness would vanish:\nYou ﬁnd, to your total amazement, that you are indeed losing control of your external\nbehavior. You ﬁnd, for example, that when doctors test your vision, you hear them say\n“We are holding up a red object in front of you; please tell us what you see.” You want\n3 One can imagine using an identical “control” subject who is given a placebo operation, for comparison. 1030\nChapter\n26.\nPhilosophical Foundations\nto cry out “I can’t see anything. I’m going totally blind.” But you hear your voice saying",
  "3 One can imagine using an identical “control” subject who is given a placebo operation, for comparison. 1030\nChapter\n26.\nPhilosophical Foundations\nto cry out “I can’t see anything. I’m going totally blind.” But you hear your voice saying\nin a way that is completely out of your control, “I see a red object in front of me.” . . .\nyour conscious experience slowly shrinks to nothing, while your externally observable\nbehavior remains the same. (Searle, 1992)\nOne can do more than argue from intuition. First, note that, for the external behavior to re-\nmain the same while the subject gradually becomes unconscious, it must be the case that the\nsubject’s volition is removed instantaneously and totally; otherwise the shrinking of aware-\nness would be reﬂected in external behavior—“Help, I’m shrinking!” or words to that effect.\nThis instantaneous removal of volition as a result of gradual neuron-at-a-time replacement\nseems an unlikely claim to have to make.\nSecond, consider what happens if we do ask the subject questions concerning his or\nher conscious experience during the period when no real neurons remain. By the conditions\nof the experiment, we will get responses such as “I feel ﬁne. I must say I’m a bit surprised\nbecause I believed Searle’s argument.” Or we might poke the subject with a pointed stick and\nobserve the response, “Ouch, that hurt.” Now, in the normal course of affairs, the skeptic can\ndismiss such outputs from AI programs as mere contrivances. Certainly, it is easy enough to\nuse a rule such as “If sensor 12 reads ‘High’ then output ‘Ouch.’ ” But the point here is that,\nbecause we have replicated the functional properties of a normal human brain, we assume\nthat the electronic brain contains no such contrivances. Then we must have an explanation of\nthe manifestations of consciousness produced by the electronic brain that appeals only to the\nfunctional properties of the neurons. And this explanation must also apply to the real brain,\nwhich has the same functional properties. There are three possible conclusions:\n1. The causal mechanisms of consciousness that generate these kinds of outputs in normal\nbrains are still operating in the electronic version, which is therefore conscious.\n2. The conscious mental events in the normal brain have no causal connection to behavior,\nand are missing from the electronic brain, which is therefore not conscious.\n3. The experiment is impossible, and therefore speculation about it is meaningless.",
  "and are missing from the electronic brain, which is therefore not conscious.\n3. The experiment is impossible, and therefore speculation about it is meaningless.\nAlthough we cannot rule out the second possibility, it reduces consciousness to what philoso-\nphers call an epiphenomenal role—something that happens, but casts no shadow, as it were,\nEPIPHENOMENON\non the observable world. Furthermore, if consciousness is indeed epiphenomenal, then it\ncannot be the case that the subject says “Ouch” because it hurts—that is, because of the con-\nscious experience of pain. Instead, the brain must contain a second, unconscious mechanism\nthat is responsible for the “Ouch.”\nPatricia Churchland (1986) points out that the functionalist arguments that operate at\nthe level of the neuron can also operate at the level of any larger functional unit—a clump\nof neurons, a mental module, a lobe, a hemisphere, or the whole brain. That means that if\nyou accept the notion that the brain replacement experiment shows that the replacement brain\nis conscious, then you should also believe that consciousness is maintained when the entire\nbrain is replaced by a circuit that updates its state and maps from inputs to outputs via a huge\nlookup table. This is disconcerting to many people (including Turing himself), who have\nthe intuition that lookup tables are not conscious—or at least, that the conscious experiences\ngenerated during table lookup are not the same as those generated during the operation of a Section 26.2.\nStrong AI: Can Machines Really Think?\n1031\nsystem that might be described (even in a simple-minded, computational sense) as accessing\nand generating beliefs, introspections, goals, and so on.\n26.2.3\nBiological naturalism and the Chinese Room\nA strong challenge to functionalism has been mounted by John Searle’s (1980) biological\nnaturalism, according to which mental states are high-level emergent features that are caused\nBIOLOGICAL\nNATURALISM\nby low-level physical processes in the neurons, and it is the (unspeciﬁed) properties of the\nneurons that matter. Thus, mental states cannot be duplicated just on the basis of some pro-\ngram having the same functional structure with the same input–output behavior; we would\nrequire that the program be running on an architecture with the same causal power as neurons.\nTo support his view, Searle describes a hypothetical system that is clearly running a program",
  "require that the program be running on an architecture with the same causal power as neurons.\nTo support his view, Searle describes a hypothetical system that is clearly running a program\nand passes the Turing Test, but that equally clearly (according to Searle) does not understand\nanything of its inputs and outputs. His conclusion is that running the appropriate program\n(i.e., having the right outputs) is not a sufﬁcient condition for being a mind.\nThe system consists of a human, who understands only English, equipped with a rule\nbook, written in English, and various stacks of paper, some blank, some with indecipherable\ninscriptions. (The human therefore plays the role of the CPU, the rule book is the program,\nand the stacks of paper are the storage device.) The system is inside a room with a small\nopening to the outside. Through the opening appear slips of paper with indecipherable sym-\nbols. The human ﬁnds matching symbols in the rule book, and follows the instructions. The\ninstructions may include writing symbols on new slips of paper, ﬁnding symbols in the stacks,\nrearranging the stacks, and so on. Eventually, the instructions will cause one or more symbols\nto be transcribed onto a piece of paper that is passed back to the outside world.\nSo far, so good. But from the outside, we see a system that is taking input in the form\nof Chinese sentences and generating answers in Chinese that are as “intelligent” as those\nin the conversation imagined by Turing.4 Searle then argues: the person in the room does\nnot understand Chinese (given). The rule book and the stacks of paper, being just pieces of\npaper, do not understand Chinese. Therefore, there is no understanding of Chinese. Hence,\naccording to Searle, running the right program does not necessarily generate understanding.\nLike Turing, Searle considered and attempted to rebuff a number of replies to his ar-\ngument. Several commentators, including John McCarthy and Robert Wilensky, proposed\nwhat Searle calls the systems reply. The objection is that asking if the human in the room\nunderstands Chinese is analogous to asking if the CPU can take cube roots. In both cases,\nthe answer is no, and in both cases, according to the systems reply, the entire system does\nhave the capacity in question. Certainly, if one asks the Chinese Room whether it understands\nChinese, the answer would be afﬁrmative (in ﬂuent Chinese). By Turing’s polite convention,",
  "have the capacity in question. Certainly, if one asks the Chinese Room whether it understands\nChinese, the answer would be afﬁrmative (in ﬂuent Chinese). By Turing’s polite convention,\nthis should be enough. Searle’s response is to reiterate the point that the understanding is not\nin the human and cannot be in the paper, so there cannot be any understanding. He seems to\nbe relying on the argument that a property of the whole must reside in one of the parts. Yet\n4 The fact that the stacks of paper might contain trillions of pages and the generation of answers would take\nmillions of years has no bearing on the logical structure of the argument. One aim of philosophical training is to\ndevelop a ﬁnely honed sense of which objections are germane and which are not. 1032\nChapter\n26.\nPhilosophical Foundations\nwater is wet, even though neither H nor O2 is. The real claim made by Searle rests upon the\nfollowing four axioms (Searle, 1990):\n1. Computer programs are formal (syntactic).\n2. Human minds have mental contents (semantics).\n3. Syntax by itself is neither constitutive of nor sufﬁcient for semantics.\n4. Brains cause minds.\nFrom the ﬁrst three axioms Searle concludes that programs are not sufﬁcient for minds. In\nother words, an agent running a program might be a mind, but it is not necessarily a mind just\nby virtue of running the program. From the fourth axiom he concludes “Any other system\ncapable of causing minds would have to have causal powers (at least) equivalent to those\nof brains.” From there he infers that any artiﬁcial brain would have to duplicate the causal\npowers of brains, not just run a particular program, and that human brains do not produce\nmental phenomena solely by virtue of running a program.\nThe axioms are controversial. For example, axioms 1 and 2 rely on an unspeciﬁed\ndistinction between syntax and semantics that seems to be closely related to the distinction\nbetween narrow and wide content. On the one hand, we can view computers as manipulating\nsyntactic symbols; on the other, we can view them as manipulating electric current, which\nhappens to be what brains mostly do (according to our current understanding). So it seems\nwe could equally say that brains are syntactic.\nAssuming we are generous in interpreting the axioms, then the conclusion—that pro-\ngrams are not sufﬁcient for minds—does follow. But the conclusion is unsatisfactory—all\nSearle has shown is that if you explicitly deny functionalism (that is what his axiom 3 does),",
  "grams are not sufﬁcient for minds—does follow. But the conclusion is unsatisfactory—all\nSearle has shown is that if you explicitly deny functionalism (that is what his axiom 3 does),\nthen you can’t necessarily conclude that non-brains are minds. This is reasonable enough—\nalmost tautological—so the whole argument comes down to whether axiom 3 can be ac-\ncepted. According to Searle, the point of the Chinese Room argument is to provide intuitions\nfor axiom 3. The public reaction shows that the argument is acting as what Daniel Dennett\n(1991) calls an intuition pump: it ampliﬁes one’s prior intuitions, so biological naturalists\nINTUITION PUMP\nare more convinced of their positions, and functionalists are convinced only that axiom 3 is\nunsupported, or that in general Searle’s argument is unconvincing. The argument stirs up\ncombatants, but has done little to change anyone’s opinion. Searle remains undeterred, and\nhas recently started calling the Chinese Room a “refutation” of strong AI rather than just an\n“argument” (Snell, 2008).\nEven those who accept axiom 3, and thus accept Searle’s argument, have only their in-\ntuitions to fall back on when deciding what entities are minds. The argument purports to show\nthat the Chinese Room is not a mind by virtue of running the program, but the argument says\nnothing about how to decide whether the room (or a computer, some other type of machine,\nor an alien) is a mind by virtue of some other reason. Searle himself says that some machines\ndo have minds: humans are biological machines with minds. According to Searle, human\nbrains may or may not be running something like an AI program, but if they are, that is not\nthe reason they are minds. It takes more to make a mind—according to Searle, something\nequivalent to the causal powers of individual neurons. What these powers are is left unspec-\niﬁed. It should be noted, however, that neurons evolved to fulﬁll functional roles—creatures Section 26.2.\nStrong AI: Can Machines Really Think?\n1033\nwith neurons were learning and deciding long before consciousness appeared on the scene. It\nwould be a remarkable coincidence if such neurons just happened to generate consciousness\nbecause of some causal powers that are irrelevant to their functional capabilities; after all, it\nis the functional capabilities that dictate survival of the organism.\nIn the case of the Chinese Room, Searle relies on intuition, not proof: just look at the",
  "is the functional capabilities that dictate survival of the organism.\nIn the case of the Chinese Room, Searle relies on intuition, not proof: just look at the\nroom; what’s there to be a mind? But one could make the same argument about the brain:\njust look at this collection of cells (or of atoms), blindly operating according to the laws of\nbiochemistry (or of physics)—what’s there to be a mind? Why can a hunk of brain be a mind\nwhile a hunk of liver cannot? That remains the great mystery.\n26.2.4\nConsciousness, qualia, and the explanatory gap\nRunning through all the debates about strong AI—the elephant in the debating room, so\nto speak—is the issue of consciousness. Consciousness is often broken down into aspects\nCONSCIOUSNESS\nsuch as understanding and self-awareness. The aspect we will focus on is that of subjective\nexperience: why it is that it feels like something to have certain brain states (e.g., while eating\na hamburger), whereas it presumably does not feel like anything to have other physical states\n(e.g., while being a rock). The technical term for the intrinsic nature of experiences is qualia\nQUALIA\n(from the Latin word meaning, roughly, “such things”).\nQualia present a challenge for functionalist accounts of the mind because different\nqualia could be involved in what are otherwise isomorphic causal processes. Consider, for\nexample, the inverted spectrum thought experiment, which the subjective experience of per-\nINVERTED\nSPECTRUM\nson X when seeing red objects is the same experience that the rest of us experience when\nseeing green objects, and vice versa. X still calls red objects “red,” stops for red trafﬁc lights,\nand agrees that the redness of red trafﬁc lights is a more intense red than the redness of the\nsetting sun. Yet, X’s subjective experience is just different.\nQualia are challenging not just for functionalism but for all of science. Suppose, for the\nsake of argument, that we have completed the process of scientiﬁc research on the brain—we\nhave found that neural process P12 in neuron N177 transforms molecule A into molecule B,\nand so on, and on. There is simply no currently accepted form of reasoning that would lead\nfrom such ﬁndings to the conclusion that the entity owning those neurons has any particular\nsubjective experience. This explanatory gap has led some philosophers to conclude that\nEXPLANATORY GAP\nhumans are simply incapable of forming a proper understanding of their own consciousness.",
  "subjective experience. This explanatory gap has led some philosophers to conclude that\nEXPLANATORY GAP\nhumans are simply incapable of forming a proper understanding of their own consciousness.\nOthers, notably Daniel Dennett (1991), avoid the gap by denying the existence of qualia,\nattributing them to a philosophical confusion.\nTuring himself concedes that the question of consciousness is a difﬁcult one, but denies\nthat it has much relevance to the practice of AI: “I do not wish to give the impression that I\nthink there is no mystery about consciousness . . . But I do not think these mysteries neces-\nsarily need to be solved before we can answer the question with which we are concerned in\nthis paper.” We agree with Turing—we are interested in creating programs that behave intel-\nligently. The additional project of making them conscious is not one that we are equipped to\ntake on, nor one whose success we would be able to determine. 1034\nChapter\n26.\nPhilosophical Foundations\n26.3\nTHE ETHICS AND RISKS OF DEVELOPING ARTIFICIAL INTELLIGENCE\nSo far, we have concentrated on whether we can develop AI, but we must also consider\nwhether we should. If the effects of AI technology are more likely to be negative than positive,\nthen it would be the moral responsibility of workers in the ﬁeld to redirect their research.\nMany new technologies have had unintended negative side effects: nuclear ﬁssion brought\nChernobyl and the threat of global destruction; the internal combustion engine brought air\npollution, global warming, and the paving-over of paradise. In a sense, automobiles are\nrobots that have conquered the world by making themselves indispensable.\nAll scientists and engineers face ethical considerations of how they should act on the\njob, what projects should or should not be done, and how they should be handled. See the\nhandbook on the Ethics of Computing (Berleur and Brunnstein, 2001). AI, however, seems\nto pose some fresh problems beyond that of, say, building bridges that don’t fall down:\n• People might lose their jobs to automation.\n• People might have too much (or too little) leisure time.\n• People might lose their sense of being unique.\n• AI systems might be used toward undesirable ends.\n• The use of AI systems might result in a loss of accountability.\n• The success of AI might mean the end of the human race.\nWe will look at each issue in turn.\nPeople might lose their jobs to automation. The modern industrial economy has be-",
  "• The use of AI systems might result in a loss of accountability.\n• The success of AI might mean the end of the human race.\nWe will look at each issue in turn.\nPeople might lose their jobs to automation. The modern industrial economy has be-\ncome dependent on computers in general, and select AI programs in particular. For example,\nmuch of the economy, especially in the United States, depends on the availability of con-\nsumer credit. Credit card applications, charge approvals, and fraud detection are now done\nby AI programs. One could say that thousands of workers have been displaced by these AI\nprograms, but in fact if you took away the AI programs these jobs would not exist, because\nhuman labor would add an unacceptable cost to the transactions. So far, automation through\ninformation technology in general and AI in particular has created more jobs than it has\neliminated, and has created more interesting, higher-paying jobs. Now that the canonical AI\nprogram is an “intelligent agent” designed to assist a human, loss of jobs is less of a concern\nthan it was when AI focused on “expert systems” designed to replace humans. But some\nresearchers think that doing the complete job is the right goal for AI. In reﬂecting on the 25th\nAnniversary of the AAAI, Nils Nilsson (2005) set as a challenge the creation of human-level\nAI that could pass the employment test rather than the Turing Test—a robot that could learn\nto do any one of a range of jobs. We may end up in a future where unemployment is high, but\neven the unemployed serve as managers of their own cadre of robot workers.\nPeople might have too much (or too little) leisure time. Alvin Tofﬂer wrote in Future\nShock (1970), “The work week has been cut by 50 percent since the turn of the century. It\nis not out of the way to predict that it will be slashed in half again by 2000.” Arthur C.\nClarke (1968b) wrote that people in 2001 might be “faced with a future of utter boredom,\nwhere the main problem in life is deciding which of several hundred TV channels to select.” Section 26.3.\nThe Ethics and Risks of Developing Artiﬁcial Intelligence\n1035\nThe only one of these predictions that has come close to panning out is the number of TV\nchannels. Instead, people working in knowledge-intensive industries have found themselves\npart of an integrated computerized system that operates 24 hours a day; to keep up, they have\nbeen forced to work longer hours. In an industrial economy, rewards are roughly proportional",
  "part of an integrated computerized system that operates 24 hours a day; to keep up, they have\nbeen forced to work longer hours. In an industrial economy, rewards are roughly proportional\nto the time invested; working 10% more would tend to mean a 10% increase in income. In\nan information economy marked by high-bandwidth communication and easy replication of\nintellectual property (what Frank and Cook (1996) call the “Winner-Take-All Society”), there\nis a large reward for being slightly better than the competition; working 10% more could mean\na 100% increase in income. So there is increasing pressure on everyone to work harder. AI\nincreases the pace of technological innovation and thus contributes to this overall trend, but\nAI also holds the promise of allowing us to take some time off and let our automated agents\nhandle things for a while. Tim Ferriss (2007) recommends using automation and outsourcing\nto achieve a four-hour work week.\nPeople might lose their sense of being unique. In Computer Power and Human Rea-\nson, Weizenbaum (1976), the author of the ELIZA program, points out some of the potential\nthreats that AI poses to society. One of Weizenbaum’s principal arguments is that AI research\nmakes possible the idea that humans are automata—an idea that results in a loss of autonomy\nor even of humanity. We note that the idea has been around much longer than AI, going back\nat least to L’Homme Machine (La Mettrie, 1748). Humanity has survived other setbacks to\nour sense of uniqueness: De Revolutionibus Orbium Coelestium (Copernicus, 1543) moved\nthe Earth away from the center of the solar system, and Descent of Man (Darwin, 1871) put\nHomo sapiens at the same level as other species. AI, if widely successful, may be at least as\nthreatening to the moral assumptions of 21st-century society as Darwin’s theory of evolution\nwas to those of the 19th century.\nAI systems might be used toward undesirable ends. Advanced technologies have\noften been used by the powerful to suppress their rivals. As the number theorist G. H. Hardy\nwrote (Hardy, 1940), “A science is said to be useful if its development tends to accentuate the\nexisting inequalities in the distribution of wealth, or more directly promotes the destruction\nof human life.” This holds for all sciences, AI being no exception. Autonomous AI systems\nare now commonplace on the battleﬁeld; the U.S. military deployed over 5,000 autonomous",
  "of human life.” This holds for all sciences, AI being no exception. Autonomous AI systems\nare now commonplace on the battleﬁeld; the U.S. military deployed over 5,000 autonomous\naircraft and 12,000 autonomous ground vehicles in Iraq (Singer, 2009). One moral theory\nholds that military robots are like medieval armor taken to its logical extreme: no one would\nhave moral objections to a soldier wanting to wear a helmet when being attacked by large,\nangry, axe-wielding enemies, and a teleoperated robot is like a very safe form of armor. On\nthe other hand, robotic weapons pose additional risks. To the extent that human decision\nmaking is taken out of the ﬁring loop, robots may end up making decisions that lead to the\nkilling of innocent civilians. At a larger scale, the possession of powerful robots (like the\npossession of sturdy helmets) may give a nation overconﬁdence, causing it to go to war more\nrecklessly than necessary. In most wars, at least one party is overconﬁdent in its military\nabilities—otherwise the conﬂict would have been resolved peacefully.\nWeizenbaum (1976) also pointed out that speech recognition technology could lead to\nwidespread wiretapping, and hence to a loss of civil liberties. He didn’t foresee a world with\nterrorist threats that would change the balance of how much surveillance people are willing to 1036\nChapter\n26.\nPhilosophical Foundations\naccept, but he did correctly recognize that AI has the potential to mass-produce surveillance.\nHis prediction has in part come true: the U.K. now has an extensive network of surveillance\ncameras, and other countries routinely monitor Web trafﬁc and telephone calls. Some accept\nthat computerization leads to a loss of privacy—Sun Microsystems CEO Scott McNealy has\nsaid “You have zero privacy anyway. Get over it.” David Brin (1998) argues that loss of\nprivacy is inevitable, and the way to combat the asymmetry of power of the state over the\nindividual is to make the surveillance accessible to all citizens. Etzioni (2004) argues for a\nbalancing of privacy and security; individual rights and community.\nThe use of AI systems might result in a loss of accountability. In the litigious atmo-\nsphere that prevails in the United States, legal liability becomes an important issue. When a\nphysician relies on the judgment of a medical expert system for a diagnosis, who is at fault if\nthe diagnosis is wrong? Fortunately, due in part to the growing inﬂuence of decision-theoretic",
  "physician relies on the judgment of a medical expert system for a diagnosis, who is at fault if\nthe diagnosis is wrong? Fortunately, due in part to the growing inﬂuence of decision-theoretic\nmethods in medicine, it is now accepted that negligence cannot be shown if the physician\nperforms medical procedures that have high expected utility, even if the actual result is catas-\ntrophic for the patient. The question should therefore be “Who is at fault if the diagnosis is\nunreasonable?” So far, courts have held that medical expert systems play the same role as\nmedical textbooks and reference books; physicians are responsible for understanding the rea-\nsoning behind any decision and for using their own judgment in deciding whether to accept\nthe system’s recommendations. In designing medical expert systems as agents, therefore,\nthe actions should be thought of not as directly affecting the patient but as inﬂuencing the\nphysician’s behavior. If expert systems become reliably more accurate than human diagnosti-\ncians, doctors might become legally liable if they don’t use the recommendations of an expert\nsystem. Atul Gawande (2002) explores this premise.\nSimilar issues are beginning to arise regarding the use of intelligent agents on the Inter-\nnet. Some progress has been made in incorporating constraints into intelligent agents so that\nthey cannot, for example, damage the ﬁles of other users (Weld and Etzioni, 1994). The prob-\nlem is magniﬁed when money changes hands. If monetary transactions are made “on one’s\nbehalf” by an intelligent agent, is one liable for the debts incurred? Would it be possible for\nan intelligent agent to have assets itself and to perform electronic trades on its own behalf?\nSo far, these questions do not seem to be well understood. To our knowledge, no program\nhas been granted legal status as an individual for the purposes of ﬁnancial transactions; at\npresent, it seems unreasonable to do so. Programs are also not considered to be “drivers”\nfor the purposes of enforcing trafﬁc regulations on real highways. In California law, at least,\nthere do not seem to be any legal sanctions to prevent an automated vehicle from exceeding\nthe speed limits, although the designer of the vehicle’s control mechanism would be liable in\nthe case of an accident. As with human reproductive technology, the law has yet to catch up\nwith the new developments.\nThe success of AI might mean the end of the human race. Almost any technology",
  "the case of an accident. As with human reproductive technology, the law has yet to catch up\nwith the new developments.\nThe success of AI might mean the end of the human race. Almost any technology\nhas the potential to cause harm in the wrong hands, but with AI and robotics, we have the new\nproblem that the wrong hands might belong to the technology itself. Countless science ﬁction\nstories have warned about robots or robot–human cyborgs running amok. Early examples Section 26.3.\nThe Ethics and Risks of Developing Artiﬁcial Intelligence\n1037\ninclude Mary Shelley’s Frankenstein, or the Modern Prometheus (1818)5 and Karel Capek’s\nplay R.U.R. (1921), in which robots conquer the world. In movies, we have The Terminator\n(1984), which combines the cliches of robots-conquer-the-world with time travel, and The\nMatrix (1999), which combines robots-conquer-the-world with brain-in-a-vat.\nIt seems that robots are the protagonists of so many conquer-the-world stories because\nthey represent the unknown, just like the witches and ghosts of tales from earlier eras, or the\nMartians from The War of the Worlds (Wells, 1898). The question is whether an AI system\nposes a bigger risk than traditional software. We will look at three sources of risk.\nFirst, the AI system’s state estimation may be incorrect, causing it to do the wrong\nthing. For example, an autonomous car might incorrectly estimate the position of a car in the\nadjacent lane, leading to an accident that might kill the occupants. More seriously, a missile\ndefense system might erroneously detect an attack and launch a counterattack, leading to\nthe death of billions. These risks are not really risks of AI systems—in both cases the same\nmistake could just as easily be made by a human as by a computer. The correct way to mitigate\nthese risks is to design a system with checks and balances so that a single state-estimation\nerror does not propagate through the system unchecked.\nSecond, specifying the right utility function for an AI system to maximize is not so\neasy. For example, we might propose a utility function designed to minimize human suffering,\nexpressed as an additive reward function over time as in Chapter 17. Given the way humans\nare, however, we’ll always ﬁnd a way to suffer even in paradise; so the optimal decision for\nthe AI system is to terminate the human race as soon as possible—no humans, no suffering.\nWith AI systems, then, we need to be very careful what we ask for, whereas humans would",
  "the AI system is to terminate the human race as soon as possible—no humans, no suffering.\nWith AI systems, then, we need to be very careful what we ask for, whereas humans would\nhave no trouble realizing that the proposed utility function cannot be taken literally. On the\nother hand, computers need not be tainted by the irrational behaviors described in Chapter 16.\nHumans sometimes use their intelligence in aggressive ways because humans have some\ninnately aggressive tendencies, due to natural selection. The machines we build need not be\ninnately aggressive, unless we decide to build them that way (or unless they emerge as the\nend product of a mechanism design that encourages aggressive behavior). Fortunately, there\nare techniques, such as apprenticeship learning, that allows us to specify a utility function by\nexample. One can hope that a robot that is smart enough to ﬁgure out how to terminate the\nhuman race is also smart enough to ﬁgure out that that was not the intended utility function.\nThird, the AI system’s learning function may cause it to evolve into a system with\nunintended behavior. This scenario is the most serious, and is unique to AI systems, so we\nwill cover it in more depth. I. J. Good wrote (1965),\nLet an ultraintelligent machine be deﬁned as a machine that can far surpass all the\nULTRAINTELLIGENT\nMACHINE\nintellectual activities of any man however clever. Since the design of machines is one of\nthese intellectual activities, an ultraintelligent machine could design even better machines;\nthere would then unquestionably be an “intelligence explosion,” and the intelligence of\nman would be left far behind. Thus the ﬁrst ultraintelligent machine is the last invention\nthat man need ever make, provided that the machine is docile enough to tell us how to\nkeep it under control.\n5 As a young man, Charles Babbage was inﬂuenced by reading Frankenstein. 1038\nChapter\n26.\nPhilosophical Foundations\nThe “intelligence explosion” has also been called the technological singularity by mathe-\nTECHNOLOGICAL\nSINGULARITY\nmatics professor and science ﬁction author Vernor Vinge, who writes (1993), “Within thirty\nyears, we will have the technological means to create superhuman intelligence. Shortly after,\nthe human era will be ended.” Good and Vinge (and many others) correctly note that the curve\nof technological progress (on many measures) is growing exponentially at present (consider",
  "the human era will be ended.” Good and Vinge (and many others) correctly note that the curve\nof technological progress (on many measures) is growing exponentially at present (consider\nMoore’s Law). However, it is a leap to extrapolate that the curve will continue to a singularity\nof near-inﬁnite growth. So far, every other technology has followed an S-shaped curve, where\nthe exponential growth eventually tapers off. Sometimes new technologies step in when the\nold ones plateau; sometimes we hit hard limits. With less than a century of high-technology\nhistory to go on, it is difﬁcult to extrapolate hundreds of years ahead.\nNote that the concept of ultraintelligent machines assumes that intelligence is an es-\npecially important attribute, and if you have enough of it, all problems can be solved. But\nwe know there are limits on computability and computational complexity. If the problem\nof deﬁning ultraintelligent machines (or even approximations to them) happens to fall in the\nclass of, say, NEXPTIME-complete problems, and if there are no heuristic shortcuts, then\neven exponential progress in technology won’t help—the speed of light puts a strict upper\nbound on how much computing can be done; problems beyond that limit will not be solved.\nWe still don’t know where those upper bounds are.\nVinge is concerned about the coming singularity, but some computer scientists and\nfuturists relish it. Hans Moravec (2000) encourages us to give every advantage to our “mind\nchildren,” the robots we create, which may surpass us in intelligence. There is even a new\nword—transhumanism—for the active social movement that looks forward to this future in\nTRANSHUMANISM\nwhich humans are merged with—or replaced by—robotic and biotech inventions. Sufﬁce it\nto say that such issues present a challenge for most moral theorists, who take the preservation\nof human life and the human species to be a good thing. Ray Kurzweil is currently the most\nvisible advocate for the singularity view, writing in The Singularity is Near (2005):\nThe Singularity will allow us to transcend these limitations of our biological bodies and\nbrain. We will gain power over our fates. Our mortality will be in our own hands. We\nwill be able to live as long as we want (a subtly different statement from saying we will\nlive forever). We will fully understand human thinking and will vastly extend and expand\nits reach. By the end of this century, the nonbiological portion of our intelligence will be",
  "live forever). We will fully understand human thinking and will vastly extend and expand\nits reach. By the end of this century, the nonbiological portion of our intelligence will be\ntrillions of trillions of times more powerful than unaided human intelligence.\nKurzweil also notes the potential dangers, writing “But the Singularity will also amplify the\nability to act on our destructive inclinations, so its full story has not yet been written.”\nIf ultraintelligent machines are a possibility, we humans would do well to make sure\nthat we design their predecessors in such a way that they design themselves to treat us well.\nScience ﬁction writer Isaac Asimov (1942) was the ﬁrst to address this issue, with his three\nlaws of robotics:\n1. A robot may not injure a human being or, through inaction, allow a human being to\ncome to harm.\n2. A robot must obey orders given to it by human beings, except where such orders would\nconﬂict with the First Law. Section 26.3.\nThe Ethics and Risks of Developing Artiﬁcial Intelligence\n1039\n3. A robot must protect its own existence as long as such protection does not conﬂict with\nthe First or Second Law.\nThese laws seem reasonable, at least to us humans.6 But the trick is how to implement these\nlaws. In the Asimov story Roundabout a robot is sent to fetch some selenium. Later the\nrobot is found wandering in a circle around the selenium source. Every time it heads toward\nthe source, it senses a danger, and the third law causes it to veer away. But every time it\nveers away, the danger recedes, and the power of the second law takes over, causing it to\nveer back towards the selenium. The set of points that deﬁne the balancing point between\nthe two laws deﬁnes a circle. This suggests that the laws are not logical absolutes, but rather\nare weighed against each other, with a higher weighting for the earlier laws. Asimov was\nprobably thinking of an architecture based on control theory—perhaps a linear combination\nof factors—while today the most likely architecture would be a probabilistic reasoning agent\nthat reasons over probability distributions of outcomes, and maximizes utility as deﬁned by\nthe three laws. But presumably we don’t want our robots to prevent a human from crossing\nthe street because of the nonzero chance of harm. That means that the negative utility for\nharm to a human must be much greater than for disobeying, but that each of the utilities is\nﬁnite, not inﬁnite.",
  "the street because of the nonzero chance of harm. That means that the negative utility for\nharm to a human must be much greater than for disobeying, but that each of the utilities is\nﬁnite, not inﬁnite.\nYudkowsky (2008) goes into more detail about how to design a Friendly AI. He asserts\nFRIENDLY AI\nthat friendliness (a desire not to harm humans) should be designed in from the start, but that\nthe designers should recognize both that their own designs may be ﬂawed, and that the robot\nwill learn and evolve over time. Thus the challenge is one of mechanism design—to deﬁne a\nmechanism for evolving AI systems under a system of checks and balances, and to give the\nsystems utility functions that will remain friendly in the face of such changes.\nWe can’t just give a program a static utility function, because circumstances, and our de-\nsired responses to circumstances, change over time. For example, if technology had allowed\nus to design a super-powerful AI agent in 1800 and endow it with the prevailing morals of\nthe time, it would be ﬁghting today to reestablish slavery and abolish women’s right to vote.\nOn the other hand, if we build an AI agent today and tell it to evolve its utility function, how\ncan we assure that it won’t reason that “Humans think it is moral to kill annoying insects, in\npart because insect brains are so primitive. But human brains are primitive compared to my\npowers, so it must be moral for me to kill humans.”\nOmohundro (2008) hypothesizes that even an innocuous chess program could pose a\nrisk to society. Similarly, Marvin Minsky once suggested that an AI program designed to\nsolve the Riemann Hypothesis might end up taking over all the resources of Earth to build\nmore powerful supercomputers to help achieve its goal. The moral is that even if you only\nwant your program to play chess or prove theorems, if you give it the capability to learn\nand alter itself, you need safeguards. Omohundro concludes that “Social structures which\ncause individuals to bear the cost of their negative externalities would go a long way toward\nensuring a stable and positive future,” This seems to be an excellent idea for society in general,\nregardless of the possibility of ultraintelligent machines.\n6 A robot might notice the inequity that a human is allowed to kill another in self-defense, but a robot is required\nto sacriﬁce its own life to save a human. 1040\nChapter\n26.\nPhilosophical Foundations",
  "6 A robot might notice the inequity that a human is allowed to kill another in self-defense, but a robot is required\nto sacriﬁce its own life to save a human. 1040\nChapter\n26.\nPhilosophical Foundations\nWe should note that the idea of safeguards against change in utility function is not a\nnew one. In the Odyssey, Homer (ca. 700 B.C.) described Ulysses’ encounter with the sirens,\nwhose song was so alluring it compelled sailors to cast themselves into the sea. Knowing it\nwould have that effect on him, Ulysses ordered his crew to bind him to the mast so that he\ncould not perform the self-destructive act. It is interesting to think how similar safeguards\ncould be built into AI systems.\nFinally, let us consider the robot’s point of view. If robots become conscious, then to\ntreat them as mere “machines” (e.g., to take them apart) might be immoral. Science ﬁction\nwriters have addressed the issue of robot rights. The movie A.I. (Spielberg, 2001) was based\non a story by Brian Aldiss about an intelligent robot who was programmed to believe that\nhe was human and fails to understand his eventual abandonment by his owner–mother. The\nstory (and the movie) argue for the need for a civil rights movement for robots.\n26.4\nSUMMARY\nThis chapter has addressed the following issues:\n• Philosophers use the term weak AI for the hypothesis that machines could possibly\nbehave intelligently, and strong AI for the hypothesis that such machines would count\nas having actual minds (as opposed to simulated minds).\n• Alan Turing rejected the question “Can machines think?” and replaced it with a be-\nhavioral test. He anticipated many objections to the possibility of thinking machines.\nFew AI researchers pay attention to the Turing Test, preferring to concentrate on their\nsystems’ performance on practical tasks, rather than the ability to imitate humans.\n• There is general agreement in modern times that mental states are brain states.\n• Arguments for and against strong AI are inconclusive. Few mainstream AI researchers\nbelieve that anything signiﬁcant hinges on the outcome of the debate.\n• Consciousness remains a mystery.\n• We identiﬁed six potential threats to society posed by AI and related technology. We\nconcluded that some of the threats are either unlikely or differ little from threats posed\nby “unintelligent” technologies. One threat in particular is worthy of further consider-\nation: that ultraintelligent machines might lead to a future that is very different from",
  "by “unintelligent” technologies. One threat in particular is worthy of further consider-\nation: that ultraintelligent machines might lead to a future that is very different from\ntoday—we may not like it, and at that point we may not have a choice. Such consid-\nerations lead inevitably to the conclusion that we must weigh carefully, and soon, the\npossible consequences of AI research.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nSources for the various responses to Turing’s 1950 paper and for the main critics of weak\nAI were given in the chapter. Although it became fashionable in the post-neural-network era Bibliographical and Historical Notes\n1041\nto deride symbolic approaches, not all philosophers are critical of GOFAI. Some are, in fact,\nardent advocates and even practitioners. Zenon Pylyshyn (1984) has argued that cognition\ncan best be understood through a computational model, not only in principle but also as a\nway of conducting research at present, and has speciﬁcally rebutted Dreyfus’s criticisms of\nthe computational model of human cognition (Pylyshyn, 1974). Gilbert Harman (1983), in\nanalyzing belief revision, makes connections with AI research on truth maintenance systems.\nMichael Bratman has applied his “belief-desire-intention” model of human psychology (Brat-\nman, 1987) to AI research on planning (Bratman, 1992). At the extreme end of strong AI,\nAaron Sloman (1978, p. xiii) has even described as “racialist” the claim by Joseph Weizen-\nbaum (1976) that intelligent machines can never be regarded as persons.\nProponents of the importance of embodiment in cognition include the philosophers\nMerleau-Ponty, whose Phenomenology of Perception (1945) stressed the importance of the\nbody and the subjective interpretation of reality afforded by our senses, and Heidegger, whose\nBeing and Time (1927) asked what it means to actually be an agent, and criticized all of the\nhistory of philosophy for taking this notion for granted. In the computer age, Alva Noe (2009)\nand Andy Clark (1998, 2008) propose that our brains form a rather minimal representation\nof the world, use the world itself in a just-in-time basis to maintain the illusion of a detailed\ninternal model, use props in the world (such as paper and pencil as well as computers) to\nincrease the capabilities of the mind. Pfeifer et al. (2006) and Lakoff and Johnson (1999)\npresent arguments for how the body helps shape cognition.\nThe nature of the mind has been a standard topic of philosophical theorizing from an-",
  "increase the capabilities of the mind. Pfeifer et al. (2006) and Lakoff and Johnson (1999)\npresent arguments for how the body helps shape cognition.\nThe nature of the mind has been a standard topic of philosophical theorizing from an-\ncient times to the present. In the Phaedo, Plato speciﬁcally considered and rejected the idea\nthat the mind could be an “attunement” or pattern of organization of the parts of the body, a\nviewpoint that approximates the functionalist viewpoint in modern philosophy of mind. He\ndecided instead that the mind had to be an immortal, immaterial soul, separable from the\nbody and different in substance—the viewpoint of dualism. Aristotle distinguished a variety\nof souls (Greek ψυχη) in living things, some of which, at least, he described in a functionalist\nmanner. (See Nussbaum (1978) for more on Aristotle’s functionalism.)\nDescartes is notorious for his dualistic view of the human mind, but ironically his histor-\nical inﬂuence was toward mechanism and physicalism. He explicitly conceived of animals as\nautomata, and he anticipated the Turing Test, writing “it is not conceivable [that a machine]\nshould produce different arrangements of words so as to give an appropriately meaningful\nanswer to whatever is said in its presence, as even the dullest of men can do” (Descartes,\n1637). Descartes’s spirited defense of the animals-as-automata viewpoint actually had the\neffect of making it easier to conceive of humans as automata as well, even though he himself\ndid not take this step. The book L’Homme Machine (La Mettrie, 1748) did explicitly argue\nthat humans are automata.\nModern analytic philosophy has typically accepted physicalism, but the variety of views\non the content of mental states is bewildering. The identiﬁcation of mental states with brain\nstates is usually attributed to Place (1956) and Smart (1959). The debate between narrow-\ncontent and wide-content views of mental states was triggered by Hilary Putnam (1975), who\nintroduced so-called twin earths (rather than brain-in-a-vat, as we did in the chapter) as a\nTWIN EARTHS\ndevice to generate identical brain states with different (wide) content. 1042\nChapter\n26.\nPhilosophical Foundations\nFunctionalism is the philosophy of mind most naturally suggested by AI. The idea that\nmental states correspond to classes of brain states deﬁned functionally is due to Putnam\n(1960, 1967) and Lewis (1966, 1980). Perhaps the most forceful proponent of functional-",
  "mental states correspond to classes of brain states deﬁned functionally is due to Putnam\n(1960, 1967) and Lewis (1966, 1980). Perhaps the most forceful proponent of functional-\nism is Daniel Dennett, whose ambitiously titled work Consciousness Explained (Dennett,\n1991) has attracted many attempted rebuttals. Metzinger (2009) argues there is no such thing\nas an objective self, that consciousness is the subjective appearance of a world. The inverted\nspectrum argument concerning qualia was introduced by John Locke (1690). Frank Jack-\nson (1982) designed an inﬂuential thought experiment involving Mary, a color scientist who\nhas been brought up in an entirely black-and-white world. There’s Something About Mary\n(Ludlow et al., 2004) collects several papers on this topic.\nFunctionalism has come under attack from authors who claim that they do not account\nfor the qualia or “what it’s like” aspect of mental states (Nagel, 1974). Searle has focused\ninstead on the alleged inability of functionalism to account for intentionality (Searle, 1980,\n1984, 1992). Churchland and Churchland (1982) rebut both these types of criticism. The\nChinese Room has been debated endlessly (Searle, 1980, 1990; Preston and Bishop, 2002).\nWe’ll just mention here a related work: Terry Bisson’s (1990) science ﬁction story They’re\nMade out of Meat, in which alien robotic explorers who visit earth are incredulous to ﬁnd\nthinking human beings whose minds are made of meat. Presumably, the robotic alien equiv-\nalent of Searle believes that he can think due to the special causal powers of robotic circuits;\ncausal powers that mere meat-brains do not possess.\nEthical issues in AI predate the existence of the ﬁeld itself. I. J. Good’s (1965) ul-\ntraintelligent machine idea was foreseen a hundred years earlier by Samuel Butler (1863).\nWritten four years after the publication of Darwin’s On the Origins of Species and at a time\nwhen the most sophisticated machines were steam engines, Butler’s article on Darwin Among\nthe Machines envisioned “the ultimate development of mechanical consciousness” by natural\nselection. The theme was reiterated by George Dyson (1998) in a book of the same title.\nThe philosophical literature on minds, brains, and related topics is large and difﬁcult to\nread without training in the terminology and methods of argument employed. The Encyclo-\npedia of Philosophy (Edwards, 1967) is an impressively authoritative and very useful aid in",
  "read without training in the terminology and methods of argument employed. The Encyclo-\npedia of Philosophy (Edwards, 1967) is an impressively authoritative and very useful aid in\nthis process. The Cambridge Dictionary of Philosophy (Audi, 1999) is a shorter and more\naccessible work, and the online Stanford Encyclopedia of Philosophy offers many excellent\narticles and up-to-date references. The MIT Encyclopedia of Cognitive Science (Wilson and\nKeil, 1999) covers the philosophy of mind as well as the biology and psychology of mind.\nThere are several general introductions to the philosophical “AI question” (Boden, 1990;\nHaugeland, 1985; Copeland, 1993; McCorduck, 2004; Minsky, 2007). The Behavioral and\nBrain Sciences, abbreviated BBS, is a major journal devoted to philosophical and scientiﬁc\ndebates about AI and neuroscience. Topics of ethics and responsibility in AI are covered in\nthe journals AI and Society and Journal of Artiﬁcial Intelligence and Law. Exercises\n1043\nEXERCISES\n26.1\nGo through Turing’s list of alleged “disabilities” of machines, identifying which have\nbeen achieved, which are achievable in principle by a program, and which are still problem-\natic because they require conscious mental states.\n26.2\nFind and analyze an account in the popular media of one or more of the arguments to\nthe effect that AI is impossible.\n26.3\nIn the brain replacement argument, it is important to be able to restore the subject’s\nbrain to normal, such that its external behavior is as it would have been if the operation had\nnot taken place. Can the skeptic reasonably object that this would require updating those\nneurophysiological properties of the neurons relating to conscious experience, as distinct\nfrom those involved in the functional behavior of the neurons?\n26.4\nSuppose that a Prolog program containing many clauses about the rules of British\ncitizenship is compiled and run on an ordinary computer. Analyze the “brain states” of the\ncomputer under wide and narrow content.\n26.5\nAlan Perlis (1982) wrote, “A year spent in artiﬁcial intelligence is enough to make one\nbelieve in God”. He also wrote, in a letter to Philip Davis, that one of the central dreams of\ncomputer science is that “through the performance of computers and their programs we will\nremove all doubt that there is only a chemical distinction between the living and nonliving\nworld.” To what extent does the progress made so far in artiﬁcial intelligence shed light on",
  "remove all doubt that there is only a chemical distinction between the living and nonliving\nworld.” To what extent does the progress made so far in artiﬁcial intelligence shed light on\nthese issues? Suppose that at some future date, the AI endeavor has been completely success-\nful; that is, we have build intelligent agents capable of carrying out any human cognitive task\nat human levels of ability. To what extent would that shed light on these issues?\n26.6\nCompare the social impact of artiﬁcial intelligence in the last ﬁfty years with the social\nimpact of the introduction of electric appliances and the internal combustion engine in the\nﬁfty years between 1890 and 1940.\n26.7\nI. J. Good claims that intelligence is the most important quality, and that building\nultraintelligent machines will change everything. A sentient cheetah counters that “Actually\nspeed is more important; if we could build ultrafast machines, that would change everything,”\nand a sentient elephant claims “You’re both wrong; what we need is ultrastrong machines.”\nWhat do you think of these arguments?\n26.8\nAnalyze the potential threats from AI technology to society. What threats are most se-\nrious, and how might they be combated? How do they compare to the potential beneﬁts?\n26.9\nHow do the potential threats from AI technology compare with those from other com-\nputer science technologies, and to bio-, nano-, and nuclear technologies?\n26.10\nSome critics object that AI is impossible, while others object that it is too possible\nand that ultraintelligent machines pose a threat. Which of these objections do you think is\nmore likely? Would it be a contradiction for someone to hold both positions? 27\nAI: THE PRESENT AND\nFUTURE\nIn which we take stock of where we are and where we are going, this being a good\nthing to do before continuing.\nIn Chapter 2, we suggested that it would be helpful to view the AI task as that of designing\nrational agents—that is, agents whose actions maximize their expected utility given their\npercept histories. We showed that the design problem depends on the percepts and actions\navailable to the agent, the utility function that the agent’s behavior should satisfy, and the\nnature of the environment. A variety of different agent designs are possible, ranging from\nreﬂex agents to fully deliberative, knowledge-based, decision-theoretic agents. Moreover,\nthe components of these designs can have a number of different instantiations—for example,",
  "reﬂex agents to fully deliberative, knowledge-based, decision-theoretic agents. Moreover,\nthe components of these designs can have a number of different instantiations—for example,\nlogical or probabilistic reasoning, and atomic, factored, or structured representations of states.\nThe intervening chapters presented the principles by which these components operate.\nFor all the agent designs and components, there has been tremendous progress both in\nour scientiﬁc understanding and in our technological capabilities. In this chapter, we stand\nback from the details and ask, “Will all this progress lead to a general-purpose intelligent\nagent that can perform well in a wide variety of environments?” Section 27.1 looks at the\ncomponents of an intelligent agent to assess what’s known and what’s missing. Section 27.2\ndoes the same for the overall agent architecture. Section 27.3 asks whether designing rational\nagents is the right goal in the ﬁrst place. (The answer is, “Not really, but it’s OK for now.”)\nFinally, Section 27.4 examines the consequences of success in our endeavors.\n27.1\nAGENT COMPONENTS\nChapter 2 presented several agent designs and their components. To focus our discussion\nhere, we will look at the utility-based agent, which we show again in Figure 27.1. When en-\ndowed with a learning component (Figure 2.15), this is the most general of our agent designs.\nLet’s see where the state of the art stands for each of the components.\nInteraction with the environment through sensors and actuators: For much of the\nhistory of AI, this has been a glaring weak point. With a few honorable exceptions, AI sys-\ntems were built in such a way that humans had to supply the inputs and interpret the outputs,\n1044 Section 27.1.\nAgent Components\n1045\nAgent\nEnvironment\nSensors\nHow happy I will be\nin such a state\nState\nHow the world evolves\nWhat my actions do\nUtility\nActuators\nWhat action I\nshould do now\nWhat it will be like\nif I do action A\nWhat the world\nis like now\nFigure 27.1\nA model-based, utility-based agent, as ﬁrst presented in Figure 2.14.\nwhile robotic systems focused on low-level tasks in which high-level reasoning and plan-\nning were largely absent. This was due in part to the great expense and engineering effort\nrequired to get real robots to work at all. The situation has changed rapidly in recent years\nwith the availability of ready-made programmable robots. These, in turn, have beneﬁted",
  "required to get real robots to work at all. The situation has changed rapidly in recent years\nwith the availability of ready-made programmable robots. These, in turn, have beneﬁted\nfrom small, cheap, high-resolution CCD cameras and compact, reliable motor drives. MEMS\n(micro-electromechanical systems) technology has supplied miniaturized accelerometers, gy-\nroscopes, and actuators for an artiﬁcial ﬂying insect (Floreano et al., 2009). It may also be\npossible to combine millions of MEMS devices to produce powerful macroscopic actuators.\nThus, we see that AI systems are at the cusp of moving from primarily software-only\nsystems to embedded robotic systems. The state of robotics today is roughly comparable to\nthe state of personal computers in about 1980: at that time researchers and hobbyists could\nexperiment with PCs, but it would take another decade before they became commonplace.\nKeeping track of the state of the world: This is one of the core capabilities required\nfor an intelligent agent. It requires both perception and updating of internal representations.\nChapter 4 showed how to keep track of atomic state representations; Chapter 7 described\nhow to do it for factored (propositional) state representations; Chapter 12 extended this to\nﬁrst-order logic; and Chapter 15 described ﬁltering algorithms for probabilistic reasoning in\nuncertain environments. Current ﬁltering and perception algorithms can be combined to do a\nreasonable job of reporting low-level predicates such as “the cup is on the table.” Detecting\nhigher-level actions, such as “Dr. Russell is having a cup of tea with Dr. Norvig while dis-\ncussing plans for next week,” is more difﬁcult. Currently it can be done (see Figure 24.25 on\npage 961) only with the help of annotated examples.\nAnother problem is that, although the approximate ﬁltering algorithms from Chapter 15\ncan handle quite large environments, they are still dealing with a factored representation—\nthey have random variables, but do not represent objects and relations explicitly. Section 14.6\nexplained how probability and ﬁrst-order logic can be combined to solve this problem, and 1046\nChapter\n27.\nAI: The Present and Future\nSection 14.6.3 showed how we can handle uncertainty about the identity of objects. We expect\nthat the application of these ideas for tracking complex environments will yield huge beneﬁts.\nHowever, we are still faced with a daunting task of deﬁning general, reusable representation",
  "that the application of these ideas for tracking complex environments will yield huge beneﬁts.\nHowever, we are still faced with a daunting task of deﬁning general, reusable representation\nschemes for complex domains. As discussed in Chapter 12, we don’t yet know how to do that\nin general; only for isolated, simple domains. It is possible that a new focus on probabilistic\nrather than logical representation coupled with aggressive machine learning (rather than hand-\nencoding of knowledge) will allow for progress.\nProjecting, evaluating, and selecting future courses of action: The basic knowledge-\nrepresentation requirements here are the same as for keeping track of the world; the primary\ndifﬁculty is coping with courses of action—such as having a conversation or a cup of tea—\nthat consist eventually of thousands or millions of primitive steps for a real agent. It is only\nby imposing hierarchical structure on behavior that we humans cope at all. We saw in\nSection 11.2 how to use hierarchical representations to handle problems of this scale; fur-\nthermore, work in hierarchical reinforcement learning has succeeded in combining some\nof these ideas with the techniques for decision making under uncertainty described in Chap-\nter 17. As yet, algorithms for the partially observable case (POMDPs) are using the same\natomic state representation we used for the search algorithms of Chapter 3. There is clearly a\ngreat deal of work to do here, but the technical foundations are largely in place. Section 27.2\ndiscusses the question of how the search for effective long-range plans might be controlled.\nUtility as an expression of preferences: In principle, basing rational decisions on the\nmaximization of expected utility is completely general and avoids many of the problems of\npurely goal-based approaches, such as conﬂicting goals and uncertain attainment. As yet,\nhowever, there has been very little work on constructing realistic utility functions—imagine,\nfor example, the complex web of interacting preferences that must be understood by an agent\noperating as an ofﬁce assistant for a human being. It has proven very difﬁcult to decompose\npreferences over complex states in the same way that Bayes nets decompose beliefs over\ncomplex states. One reason may be that preferences over states are really compiled from\npreferences over state histories, which are described by reward functions (see Chapter 17).",
  "complex states. One reason may be that preferences over states are really compiled from\npreferences over state histories, which are described by reward functions (see Chapter 17).\nEven if the reward function is simple, the corresponding utility function may be very complex.\nThis suggests that we take seriously the task of knowledge engineering for reward functions\nas a way of conveying to our agents what it is that we want them to do.\nLearning: Chapters 18 to 21 described how learning in an agent can be formulated as\ninductive learning (supervised, unsupervised, or reinforcement-based) of the functions that\nconstitute the various components of the agent. Very powerful logical and statistical tech-\nniques have been developed that can cope with quite large problems, reaching or exceeding\nhuman capabilities in many tasks—as long as we are dealing with a predeﬁned vocabulary\nof features and concepts. On the other hand, machine learning has made very little progress\non the important problem of constructing new representations at levels of abstraction higher\nthan the input vocabulary. In computer vision, for example, learning complex concepts such\nas Classroom and Cafeteria would be made unnecessarily difﬁcult if the agent were forced\nto work from pixels as the input representation; instead, the agent needs to be able to form\nintermediate concepts ﬁrst, such as Desk and Tray, without explicit human supervision.\nSimilar considerations apply to learning behavior: HavingACupOfTea is a very important Section 27.2.\nAgent Architectures\n1047\nhigh-level step in many plans, but how does it get into an action library that initially contains\nmuch simpler actions such as RaiseArm and Swallow? Perhaps this will incorporate some\nof the ideas of deep belief networks—Bayesian networks that have multiple layers of hidden\nDEEP BELIEF\nNETWORKS\nvariables, as in the work of Hinton et al. (2006), Hawkins and Blakeslee (2004), and Bengio\nand LeCun (2007).\nThe vast majority of machine learning research today assumes a factored representa-\ntion, learning a function h : Rn →R for regression and h : Rn →{0, 1} for classiﬁcation.\nLearning researchers will need to adapt their very successful techniques for factored repre-\nsentations to structured representations, particularly hierarchical representations. The work\non inductive logic programming in Chapter 19 is a ﬁrst step in this direction; the logical next\nstep is to combine these ideas with the probabilistic languages of Section 14.6.",
  "on inductive logic programming in Chapter 19 is a ﬁrst step in this direction; the logical next\nstep is to combine these ideas with the probabilistic languages of Section 14.6.\nUnless we understand such issues, we are faced with the daunting task of constructing\nlarge commonsense knowledge bases by hand, an approach that has not fared well to date.\nThere is great promise in using the Web as a source of natural language text, images, and\nvideos to serve as a comprehensive knowledge base, but so far machine learning algorithms\nare limited in the amount of organized knowledge they can extract from these sources.\n27.2\nAGENT ARCHITECTURES\nIt is natural to ask, “Which of the agent architectures in Chapter 2 should an agent use?”\nThe answer is, “All of them!” We have seen that reﬂex responses are needed for situations\nin which time is of the essence, whereas knowledge-based deliberation allows the agent to\nplan ahead. A complete agent must be able to do both, using a hybrid architecture. One\nHYBRID\nARCHITECTURE\nimportant property of hybrid architectures is that the boundaries between different decision\ncomponents are not ﬁxed. For example, compilation continually converts declarative in-\nformation at the deliberative level into more efﬁcient representations, eventually reaching the\nreﬂex level—see Figure 27.2. (This is the purpose of explanation-based learning, as discussed\nin Chapter 19.) Agent architectures such as SOAR (Laird et al., 1987) and THEO (Mitchell,\n1990) have exactly this structure. Every time they solve a problem by explicit deliberation,\nthey save away a generalized version of the solution for use by the reﬂex component. A\nless studied problem is the reversal of this process: when the environment changes, learned\nreﬂexes may no longer be appropriate and the agent must return to the deliberative level to\nproduce new behaviors.\nAgents also need ways to control their own deliberations. They must be able to cease\ndeliberating when action is demanded, and they must be able to use the time available for\ndeliberation to execute the most proﬁtable computations. For example, a taxi-driving agent\nthat sees an accident ahead must decide in a split second either to brake or to take evasive\naction. It should also spend that split second thinking about the most important questions,\nsuch as whether the lanes to the left and right are clear and whether there is a large truck",
  "action. It should also spend that split second thinking about the most important questions,\nsuch as whether the lanes to the left and right are clear and whether there is a large truck\nclose behind, rather than worrying about wear and tear on the tires or where to pick up the\nnext passenger. These issues are usually studied under the heading of real-time AI. As AI\nREAL-TIME AI 1048\nChapter\n27.\nAI: The Present and Future\nPercepts\nCompilation\nKnowledge-based\ndeliberation\nReflex system\nActions\nFigure 27.2\nCompilation serves to convert deliberative decision making into more efﬁ-\ncient, reﬂexive mechanisms.\nsystems move into more complex domains, all problems will become real-time, because the\nagent will never have long enough to solve the decision problem exactly.\nClearly, there is a pressing need for general methods of controlling deliberation, rather\nthan speciﬁc recipes for what to think about in each situation. The ﬁrst useful idea is to em-\nploy anytime algorithms (Dean and Boddy, 1988; Horvitz, 1987). An anytime algorithm is\nANYTIME\nALGORITHM\nan algorithm whose output quality improves gradually over time, so that it has a reasonable\ndecision ready whenever it is interrupted. Such algorithms are controlled by a metalevel de-\ncision procedure that assesses whether further computation is worthwhile. (See Section 3.5.4\nfor a brief description of metalevel decision making.) Example of an anytime algorithms\ninclude iterative deepening in game-tree search and MCMC in Bayesian networks.\nThe second technique for controlling deliberation is decision-theoretic metareasoning\nDECISION-\nTHEORETIC\nMETAREASONING\n(Russell and Wefald, 1989, 1991; Horvitz, 1989; Horvitz and Breese, 1996). This method\napplies the theory of information value (Chapter 16) to the selection of individual computa-\ntions. The value of a computation depends on both its cost (in terms of delaying action) and\nits beneﬁts (in terms of improved decision quality). Metareasoning techniques can be used to\ndesign better search algorithms and to guarantee that the algorithms have the anytime prop-\nerty. Metareasoning is expensive, of course, and compilation methods can be applied so that\nthe overhead is small compared to the costs of the computations being controlled. Metalevel\nreinforcement learning may provide another way to acquire effective policies for controlling\ndeliberation: in essence, computations that lead to better decisions are reinforced, while those",
  "reinforcement learning may provide another way to acquire effective policies for controlling\ndeliberation: in essence, computations that lead to better decisions are reinforced, while those\nthat turn out to have no effect are penalized. This approach avoids the myopia problems of\nthe simple value-of-information calculation.\nMetareasoning is one speciﬁc example of a reﬂective architecture—that is, an archi-\nREFLECTIVE\nARCHITECTURE\ntecture that enables deliberation about the computational entities and actions occurring within\nthe architecture itself. A theoretical foundation for reﬂective architectures can be built by\ndeﬁning a joint state space composed from the environment state and the computational state\nof the agent itself. Decision-making and learning algorithms can be designed that operate\nover this joint state space and thereby serve to implement and improve the agent’s compu-\ntational activities. Eventually, we expect task-speciﬁc algorithms such as alpha–beta search\nand backward chaining to disappear from AI systems, to be replaced by general methods that\ndirect the agent’s computations toward the efﬁcient generation of high-quality decisions. Section 27.3.\nAre We Going in the Right Direction?\n1049\n27.3\nARE WE GOING IN THE RIGHT DIRECTION?\nThe preceding section listed many advances and many opportunities for further progress. But\nwhere is this all leading? Dreyfus (1992) gives the analogy of trying to get to the moon by\nclimbing a tree; one can report steady progress, all the way to the top of the tree. In this\nsection, we consider whether AI’s current path is more like a tree climb or a rocket trip.\nIn Chapter 1, we said that our goal was to build agents that act rationally. However, we\nalso said that\n. . . achieving perfect rationality—always doing the right thing—is not feasible in compli-\ncated environments. The computational demands are just too high. For most of the book,\nhowever, we will adopt the working hypothesis that perfect rationality is a good starting\npoint for analysis.\nNow it is time to consider again what exactly the goal of AI is. We want to build agents, but\nwith what speciﬁcation in mind? Here are four possibilities:\nPerfect rationality. A perfectly rational agent acts at every instant in such a way as to\nPERFECT\nRATIONALITY\nmaximize its expected utility, given the information it has acquired from the environment. We\nhave seen that the calculations necessary to achieve perfect rationality in most environments",
  "PERFECT\nRATIONALITY\nmaximize its expected utility, given the information it has acquired from the environment. We\nhave seen that the calculations necessary to achieve perfect rationality in most environments\nare too time consuming, so perfect rationality is not a realistic goal.\nCalculative rationality. This is the notion of rationality that we have used implicitly in de-\nCALCULATIVE\nRATIONALITY\nsigning logical and decision-theoretic agents, and most of theoretical AI research has focused\non this property. A calculatively rational agent eventually returns what would have been the\nrational choice at the beginning of its deliberation. This is an interesting property for a system\nto exhibit, but in most environments, the right answer at the wrong time is of no value. In\npractice, AI system designers are forced to compromise on decision quality to obtain reason-\nable overall performance; unfortunately, the theoretical basis of calculative rationality does\nnot provide a well-founded way to make such compromises.\nBounded rationality. Herbert Simon (1957) rejected the notion of perfect (or even approx-\nBOUNDED\nRATIONALITY\nimately perfect) rationality and replaced it with bounded rationality, a descriptive theory of\ndecision making by real agents. He wrote,\nThe capacity of the human mind for formulating and solving complex problems is very\nsmall compared with the size of the problems whose solution is required for objectively\nrational behavior in the real world—or even for a reasonable approximation to such ob-\njective rationality.\nHe suggested that bounded rationality works primarily by satisﬁcing—that is, deliberating\nonly long enough to come up with an answer that is “good enough.” Simon won the Nobel\nPrize in economics for this work and has written about it in depth (Simon, 1982). It appears\nto be a useful model of human behaviors in many cases. It is not a formal speciﬁcation\nfor intelligent agents, however, because the deﬁnition of “good enough” is not given by the\ntheory. Furthermore, satisﬁcing seems to be just one of a large range of methods used to cope\nwith bounded resources. 1050\nChapter\n27.\nAI: The Present and Future\nBounded optimality (BO). A bounded optimal agent behaves as well as possible, given its\nBOUNDED\nOPTIMALITY\ncomputational resources. That is, the expected utility of the agent program for a bounded\noptimal agent is at least as high as the expected utility of any other agent program running on\nthe same machine.",
  "BOUNDED\nOPTIMALITY\ncomputational resources. That is, the expected utility of the agent program for a bounded\noptimal agent is at least as high as the expected utility of any other agent program running on\nthe same machine.\nOf these four possibilities, bounded optimality seems to offer the best hope for a strong\ntheoretical foundation for AI. It has the advantage of being possible to achieve: there is always\nat least one best program—something that perfect rationality lacks. Bounded optimal agents\nare actually useful in the real world, whereas calculatively rational agents usually are not, and\nsatisﬁcing agents might or might not be, depending on how ambitious they are.\nThe traditional approach in AI has been to start with calculative rationality and then\nmake compromises to meet resource constraints. If the problems imposed by the constraints\nare minor, one would expect the ﬁnal design to be similar to a BO agent design. But as the\nresource constraints become more critical—for example, as the environment becomes more\ncomplex—one would expect the two designs to diverge. In the theory of bounded optimality,\nthese constraints can be handled in a principled fashion.\nAs yet, little is known about bounded optimality. It is possible to construct bounded\noptimal programs for very simple machines and for somewhat restricted kinds of environ-\nments (Etzioni, 1989; Russell et al., 1993), but as yet we have no idea what BO programs\nare like for large, general-purpose computers in complex environments. If there is to be a\nconstructive theory of bounded optimality, we have to hope that the design of bounded op-\ntimal programs does not depend too strongly on the details of the computer being used. It\nwould make scientiﬁc research very difﬁcult if adding a few kilobytes of memory to a giga-\nbyte machine made a signiﬁcant difference to the design of the BO program. One way to\nmake sure this cannot happen is to be slightly more relaxed about the criteria for bounded\noptimality. By analogy with the notion of asymptotic complexity (Appendix A), we can de-\nﬁne asymptotic bounded optimality (ABO) as follows (Russell and Subramanian, 1995).\nASYMPTOTIC\nBOUNDED\nOPTIMALITY\nSuppose a program P is bounded optimal for a machine M in a class of environments E,\nwhere the complexity of environments in E is unbounded. Then program P ′ is ABO for M\nin E if it can outperform P by running on a machine kM that is k times faster (or larger)",
  "where the complexity of environments in E is unbounded. Then program P ′ is ABO for M\nin E if it can outperform P by running on a machine kM that is k times faster (or larger)\nthan M. Unless k were enormous, we would be happy with a program that was ABO for\na nontrivial environment on a nontrivial architecture. There would be little point in putting\nenormous effort into ﬁnding BO rather than ABO programs, because the size and speed of\navailable machines tends to increase by a constant factor in a ﬁxed amount of time anyway.\nWe can hazard a guess that BO or ABO programs for powerful computers in complex\nenvironments will not necessarily have a simple, elegant structure. We have already seen that\ngeneral-purpose intelligence requires some reﬂex capability and some deliberative capability;\na variety of forms of knowledge and decision making; learning and compilation mechanisms\nfor all of those forms; methods for controlling reasoning; and a large store of domain-speciﬁc\nknowledge. A bounded optimal agent must adapt to the environment in which it ﬁnds itself,\nso that eventually its internal organization will reﬂect optimizations that are speciﬁc to the\nparticular environment. This is only to be expected, and it is similar to the way in which\nracing cars restricted by engine capacity have evolved into extremely complex designs. We Section 27.4.\nWhat If AI Does Succeed?\n1051\nsuspect that a science of artiﬁcial intelligence based on bounded optimality will involve a\ngood deal of study of the processes that allow an agent program to converge to bounded\noptimality and perhaps less concentration on the details of the messy programs that result.\nIn sum, the concept of bounded optimality is proposed as a formal task for AI research\nthat is both well deﬁned and feasible. Bounded optimality speciﬁes optimal programs rather\nthan optimal actions. Actions are, after all, generated by programs, and it is over programs\nthat designers have control.\n27.4\nWHAT IF AI DOES SUCCEED?\nIn David Lodge’s Small World (1984), a novel about the academic world of literary criticism,\nthe protagonist causes consternation by asking a panel of eminent but contradictory literary\ntheorists the following question: “What if you were right?” None of the theorists seems to\nhave considered this question before, perhaps because debating unfalsiﬁable theories is an end\nin itself. Similar confusion can be evoked by asking AI researchers, “What if you succeed?”",
  "have considered this question before, perhaps because debating unfalsiﬁable theories is an end\nin itself. Similar confusion can be evoked by asking AI researchers, “What if you succeed?”\nAs Section 26.3 relates, there are ethical issues to consider. Intelligent computers are\nmore powerful than dumb ones, but will that power be used for good or ill? Those who strive\nto develop AI have a responsibility to see that the impact of their work is a positive one. The\nscope of the impact will depend on the degree of success of AI. Even modest successes in AI\nhave already changed the ways in which computer science is taught (Stein, 2002) and software\ndevelopment is practiced. AI has made possible new applications such as speech recognition\nsystems, inventory control systems, surveillance systems, robots, and search engines.\nWe can expect that medium-level successes in AI would affect all kinds of people in\ntheir daily lives. So far, computerized communication networks, such as cell phones and the\nInternet, have had this kind of pervasive effect on society, but AI has not. AI has been at work\nbehind the scenes—for example, in automatically approving or denying credit card transac-\ntions for every purchase made on the Web—but has not been visible to the average consumer.\nWe can imagine that truly useful personal assistants for the ofﬁce or the home would have a\nlarge positive impact on people’s lives, although they might cause some economic disloca-\ntion in the short term. Automated assistants for driving could prevent accidents, saving tens\nof thousands of lives per year. A technological capability at this level might also be applied\nto the development of autonomous weapons, which many view as undesirable. Some of the\nbiggest societal problems we face today—such as the harnessing of genomic information for\ntreating disease, the efﬁcient management of energy resources, and the veriﬁcation of treaties\nconcerning nuclear weapons—are being addressed with the help of AI technologies.\nFinally, it seems likely that a large-scale success in AI—the creation of human-level in-\ntelligence and beyond—would change the lives of a majority of humankind. The very nature\nof our work and play would be altered, as would our view of intelligence, consciousness, and\nthe future destiny of the human race. AI systems at this level of capability could threaten hu-\nman autonomy, freedom, and even survival. For these reasons, we cannot divorce AI research",
  "the future destiny of the human race. AI systems at this level of capability could threaten hu-\nman autonomy, freedom, and even survival. For these reasons, we cannot divorce AI research\nfrom its ethical consequences (see Section 26.3). 1052\nChapter\n27.\nAI: The Present and Future\nWhich way will the future go? Science ﬁction authors seem to favor dystopian futures\nover utopian ones, probably because they make for more interesting plots. But so far, AI\nseems to ﬁt in with other revolutionary technologies (printing, plumbing, air travel, telephony)\nwhose negative repercussions are outweighed by their positive aspects.\nIn conclusion, we see that AI has made great progress in its short history, but the ﬁnal\nsentence of Alan Turing’s (1950) essay on Computing Machinery and Intelligence is still\nvalid today:\nWe can see only a short distance ahead,\nbut we can see that much remains to be done. A\nMATHEMATICAL\nBACKGROUND\nA.1\nCOMPLEXITY ANALYSIS AND O() NOTATION\nComputer scientists are often faced with the task of comparing algorithms to see how fast\nthey run or how much memory they require. There are two approaches to this task. The ﬁrst\nis benchmarking—running the algorithms on a computer and measuring speed in seconds\nBENCHMARKING\nand memory consumption in bytes. Ultimately, this is what really matters, but a benchmark\ncan be unsatisfactory because it is so speciﬁc: it measures the performance of a particular\nprogram written in a particular language, running on a particular computer, with a particular\ncompiler and particular input data. From the single result that the benchmark provides, it\ncan be difﬁcult to predict how well the algorithm would do on a different compiler, com-\nputer, or data set. The second approach relies on a mathematical analysis of algorithms,\nANALYSIS OF\nALGORITHMS\nindependently of the particular implementation and input, as discussed below.\nA.1.1\nAsymptotic analysis\nWe will consider algorithm analysis through the following example, a program to compute\nthe sum of a sequence of numbers:\nfunction SUMMATION(sequence) returns a number\nsum ←0\nfor i = 1 to LENGTH(sequence) do\nsum ←sum + sequence[i]\nreturn sum\nThe ﬁrst step in the analysis is to abstract over the input, in order to ﬁnd some parameter or\nparameters that characterize the size of the input. In this example, the input can be charac-\nterized by the length of the sequence, which we will call n. The second step is to abstract",
  "parameters that characterize the size of the input. In this example, the input can be charac-\nterized by the length of the sequence, which we will call n. The second step is to abstract\nover the implementation, to ﬁnd some measure that reﬂects the running time of the algorithm\nbut is not tied to a particular compiler or computer. For the SUMMATION program, this could\nbe just the number of lines of code executed, or it could be more detailed, measuring the\nnumber of additions, assignments, array references, and branches executed by the algorithm.\n1053 1054\nAppendix\nA.\nMathematical background\nEither way gives us a characterization of the total number of steps taken by the algorithm as\na function of the size of the input. We will call this characterization T(n). If we count lines\nof code, we have T(n) = 2n + 2 for our example.\nIf all programs were as simple as SUMMATION, the analysis of algorithms would be a\ntrivial ﬁeld. But two problems make it more complicated. First, it is rare to ﬁnd a parameter\nlike n that completely characterizes the number of steps taken by an algorithm. Instead, the\nbest we can usually do is compute the worst case Tworst(n) or the average case Tavg(n).\nComputing an average means that the analyst must assume some distribution of inputs.\nThe second problem is that algorithms tend to resist exact analysis. In that case, it is\nnecessary to fall back on an approximation. We say that the SUMMATION algorithm is O(n),\nmeaning that its measure is at most a constant times n, with the possible exception of a few\nsmall values of n. More formally,\nT(n) is O(f(n)) if T(n) ≤kf(n) for some k, for all n > n0 .\nThe O() notation gives us what is called an asymptotic analysis. We can say without ques-\nASYMPTOTIC\nANALYSIS\ntion that, as n asymptotically approaches inﬁnity, an O(n) algorithm is better than an O(n2)\nalgorithm. A single benchmark ﬁgure could not substantiate such a claim.\nThe O() notation abstracts over constant factors, which makes it easier to use, but less\nprecise, than the T() notation. For example, an O(n2) algorithm will always be worse than\nan O(n) in the long run, but if the two algorithms are T(n2 + 1) and T(100n + 1000), then\nthe O(n2) algorithm is actually better for n < 110.\nDespite this drawback, asymptotic analysis is the most widely used tool for analyzing\nalgorithms. It is precisely because the analysis abstracts over both the exact number of oper-",
  "the O(n2) algorithm is actually better for n < 110.\nDespite this drawback, asymptotic analysis is the most widely used tool for analyzing\nalgorithms. It is precisely because the analysis abstracts over both the exact number of oper-\nations (by ignoring the constant factor k) and the exact content of the input (by considering\nonly its size n) that the analysis becomes mathematically feasible. The O() notation is a good\ncompromise between precision and ease of analysis.\nA.1.2\nNP and inherently hard problems\nThe analysis of algorithms and the O() notation allow us to talk about the efﬁciency of a\nparticular algorithm. However, they have nothing to say about whether there could be a better\nalgorithm for the problem at hand. The ﬁeld of complexity analysis analyzes problems rather\nCOMPLEXITY\nANALYSIS\nthan algorithms. The ﬁrst gross division is between problems that can be solved in polynomial\ntime and problems that cannot be solved in polynomial time, no matter what algorithm is\nused. The class of polynomial problems—those which can be solved in time O(nk) for some\nk—is called P. These are sometimes called “easy” problems, because the class contains those\nproblems with running times like O(log n) and O(n). But it also contains those with time\nO(n1000), so the name “easy” should not be taken too literally.\nAnother important class of problems is NP, the class of nondeterministic polynomial\nproblems. A problem is in this class if there is some algorithm that can guess a solution and\nthen verify whether the guess is correct in polynomial time. The idea is that if you have an\narbitrarily large number of processors, so that you can try all the guesses at once, or you are\nvery lucky and always guess right the ﬁrst time, then the NP problems become P problems.\nOne of the biggest open questions in computer science is whether the class NP is equivalent Section A.2.\nVectors, Matrices, and Linear Algebra\n1055\nto the class P when one does not have the luxury of an inﬁnite number of processors or\nomniscient guessing. Most computer scientists are convinced that P ̸= NP; that NP problems\nare inherently hard and have no polynomial-time algorithms. But this has never been proven.\nThose who are interested in deciding whether P = NP look at a subclass of NP called the\nNP-complete problems. The word “complete” is used here in the sense of “most extreme”\nNP-COMPLETE\nand thus refers to the hardest problems in the class NP. It has been proven that either all",
  "NP-complete problems. The word “complete” is used here in the sense of “most extreme”\nNP-COMPLETE\nand thus refers to the hardest problems in the class NP. It has been proven that either all\nthe NP-complete problems are in P or none of them is. This makes the class theoretically\ninteresting, but the class is also of practical interest because many important problems are\nknown to be NP-complete. An example is the satisﬁability problem: given a sentence of\npropositional logic, is there an assignment of truth values to the proposition symbols of the\nsentence that makes it true? Unless a miracle occurs and P = NP, there can be no algorithm\nthat solves all satisﬁability problems in polynomial time. However, AI is more interested in\nwhether there are algorithms that perform efﬁciently on typical problems drawn from a pre-\ndetermined distribution; as we saw in Chapter 7, there are algorithms such as WALKSAT that\ndo quite well on many problems.\nThe class co-NP is the complement of NP, in the sense that, for every decision problem\nCO-NP\nin NP, there is a corresponding problem in co-NP with the “yes” and “no” answers reversed.\nWe know that P is a subset of both NP and co-NP, and it is believed that there are problems\nin co-NP that are not in P. The co-NP-complete problems are the hardest problems in co-NP.\nCO-NP-COMPLETE\nThe class #P (pronounced “sharp P”) is the set of counting problems corresponding to\nthe decision problems in NP. Decision problems have a yes-or-no answer: is there a solution\nto this 3-SAT formula? Counting problems have an integer answer: how many solutions are\nthere to this 3-SAT formula? In some cases, the counting problem is much harder than the\ndecision problem. For example, deciding whether a bipartite graph has a perfect matching\ncan be done in time O(V E) (where the graph has V vertices and E edges), but the counting\nproblem “how many perfect matches does this bipartite graph have” is #P-complete, meaning\nthat it is hard as any problem in #P and thus at least as hard as any NP problem.\nAnother class is the class of PSPACE problems—those that require a polynomial amount\nof space, even on a nondeterministic machine. It is believed that PSPACE-hard problems are\nworse than NP-complete problems, although it could turn out that NP = PSPACE, just as it\ncould turn out that P = NP.\nA.2\nVECTORS, MATRICES, AND LINEAR ALGEBRA\nMathematicians deﬁne a vector as a member of a vector space, but we will use a more con-\nVECTOR",
  "could turn out that P = NP.\nA.2\nVECTORS, MATRICES, AND LINEAR ALGEBRA\nMathematicians deﬁne a vector as a member of a vector space, but we will use a more con-\nVECTOR\ncrete deﬁnition: a vector is an ordered sequence of values. For example, in two-dimensional\nspace, we have vectors such as x = ⟨3, 4⟩and y = ⟨0, 2⟩. We follow the convention of bold-\nface characters for vector names, although some authors use arrows or bars over the names:\n⃗x or ¯y. The elements of a vector can be accessed using subscripts: z = ⟨z1, z2, . . . , zn⟩. One\nconfusing point: this book is synthesizing work from many subﬁelds, which variously call\ntheir sequences vectors, lists, or tuples, and variously use the notations ⟨1, 2⟩, [1, 2], or (1, 2). 1056\nAppendix\nA.\nMathematical background\nThe two fundamental operations on vectors are vector addition and scalar multiplica-\ntion. The vector addition x+y is the elementwise sum: x+y = ⟨3+0, 4+2⟩= ⟨3, 6⟩. Scalar\nmultiplication multiplies each element by a constant: 5x = ⟨5 × 3, 5 × 4⟩= ⟨15, 20⟩.\nThe length of a vector is denoted |x| and is computed by taking the square root of the\nsum of the squares of the elements: |x| =",
  "(32 + 42) = 5. The dot product x · y (also called\nscalar product) of two vectors is the sum of the products of corresponding elements, that is,\nx · y = \u0002\ni xiyi, or in our particular case, x · y = 3 × 0 + 4 × 2 = 8.\nVectors are often interpreted as directed line segments (arrows) in an n-dimensional\nEuclidean space. Vector addition is then equivalent to placing the tail of one vector at the\nhead of the other, and the dot product x · y is equal to |x| |y| cos θ, where θ is the angle\nbetween x and y.\nA matrix is a rectangular array of values arranged into rows and columns. Here is a\nMATRIX\nmatrix A of size 3 × 4:\n⎛\n⎝\nA1,1 A1,2 A1,3 A1,4\nA2,1 A2,2 A2,3 A2,4\nA3,1 A3,2 A3,3 A3,4\n⎞\n⎠\nThe ﬁrst index of Ai,j speciﬁes the row and the second the column. In programming lan-\nguages, Ai,j is often written A[i,j] or A[i][j].\nThe sum of two matrices is deﬁned by adding their corresponding elements; for example\n(A + B)i,j = Ai,j + Bi,j. (The sum is undeﬁned if A and B have different sizes.) We can also\ndeﬁne the multiplication of a matrix by a scalar: (cA)i,j = cAi,j. Matrix multiplication (the\nproduct of two matrices) is more complicated. The product AB is deﬁned only if A is of size\na × b and B is of size b × c (i.e., the second matrix has the same number of rows as the ﬁrst\nhas columns); the result is a matrix of size a × c. If the matrices are of appropriate size, then\nthe result is\n(AB)i,k =\n\f\nj\nAi,jBj,k .\nMatrix multiplication is not commutative, even for square matrices: AB ̸= BA in general.\nIt is, however, associative: (AB)C = A(BC). Note that the dot product can be expressed in\nterms of a transpose and a matrix multiplication:x · y = x⊤y.\nThe identity matrix I has elements Ii,j equal to 1 when i = j and equal to 0 otherwise.\nIDENTITY MATRIX\nIt has the property that AI = A for all A. The transpose of A, written A⊤is formed by\nTRANSPOSE\nturning rows into columns and vice versa, or, more formally, by A⊤i,j = Aj,i. The inverse of\nINVERSE\na square matrix A is another square matrix A−1 such that A−1A = I. For a singular matrix,\nSINGULAR\nthe inverse does not exist. For a nonsingular matrix, it can be computed in O(n3) time.\nMatrices are used to solve systems of linear equations in O(n3) time; the time is domi-\nnated by inverting a matrix of coefﬁcients. Consider the following set of equations, for which\nwe want a solution in x, y, and z:\n+2x + y −z = 8\n−3x −y + 2z = −11\n−2x + y + 2z = −3 . Section A.3.\nProbability Distributions\n1057",
  "nated by inverting a matrix of coefﬁcients. Consider the following set of equations, for which\nwe want a solution in x, y, and z:\n+2x + y −z = 8\n−3x −y + 2z = −11\n−2x + y + 2z = −3 . Section A.3.\nProbability Distributions\n1057\nWe can represent this system as the matrix equation A x = b, where\nA =\n⎛\n⎝\n2\n1 −1\n−3 −1\n2\n−2\n1\n2\n⎞\n⎠,\nx =\n⎛\n⎝\nx\ny\nz\n⎞\n⎠,\nb =\n⎛\n⎝\n8\n−11\n−3\n⎞\n⎠.\nTo solve A x = b we multiply both sides by A−1, yielding A−1Ax = A−1b, which simpliﬁes\nto x = A−1b. After inverting A and multiplying by b, we get the answer\nx =\n⎛\n⎝\nx\ny\nz\n⎞\n⎠=\n⎛\n⎝\n2\n3\n−1\n⎞\n⎠.\nA.3\nPROBABILITY DISTRIBUTIONS\nA probability is a measure over a set of events that satisﬁes three axioms:\n1. The measure of each event is between 0 and 1. We write this as 0 ≤P(X = xi) ≤1,\nwhere X is a random variable representing an event and xi are the possible values of\nX. In general, random variables are denoted by uppercase letters and their values by\nlowercase letters.\n2. The measure of the whole set is 1; that is, \u0002n\ni = 1 P(X = xi) = 1.\n3. The probability of a union of disjoint events is the sum of the probabilities of the indi-\nvidual events; that is, P(X = x1 ∨X = x2) = P(X = x1) + P(X = x2), where x1 and\nx2 are disjoint.\nA probabilistic model consists of a sample space of mutually exclusive possible outcomes,\ntogether with a probability measure for each outcome. For example, in a model of the weather\ntomorrow, the outcomes might be sunny, cloudy, rainy, and snowy. A subset of these out-\ncomes constitutes an event. For example, the event of precipitation is the subset consisting of\n{rainy, snowy}.\nWe use P(X) to denote the vector of values ⟨P(X = x1), . . . , P(X = xn)⟩. We also\nuse P(xi) as an abbreviation for P(X = xi) and \u0002\nx P(x) for \u0002n\ni = 1 P(X = xi).\nThe conditional probability P(B|A) is deﬁned as P(B∩A)/P(A). A and B are condi-\ntionally independent if P(B|A) = P(B) (or equivalently, P(A|B) = P(A)). For continuous\nvariables, there are an inﬁnite number of values, and unless there are point spikes, the proba-\nbility of any one value is 0. Therefore, we deﬁne a probability density function, which we\nPROBABILITY\nDENSITY FUNCTION\nalso denote as P(·), but which has a slightly different meaning from the discrete probability\nfunction. The density function P(x) for a random variable X, which might be thought of as\nP(X = x), is intuitively deﬁned as the ratio of the probability that X falls into an interval\naround x, divided by the width of the interval, as the interval width goes to zero:",
  "P(X = x), is intuitively deﬁned as the ratio of the probability that X falls into an interval\naround x, divided by the width of the interval, as the interval width goes to zero:\nP(x) = lim\ndx→0 P(x ≤X ≤x + dx)/dx . 1058\nAppendix\nA.\nMathematical background\nThe density function must be nonnegative for all x and must have\n\u001a ∞\n−∞\nP(x) dx = 1 .\nWe can also deﬁne a cumulative probability density function FX(x), which is the proba-\nCUMULATIVE\nPROBABILITY\nDENSITY FUNCTION\nbility of a random variable being less than x:\nFX(x) = P(X ≤x) =\n\u001a x\n−∞\nP(u) du .\nNote that the probability density function has units, whereas the discrete probability function\nis unitless. For example, if values of X are measured in seconds, then the density is measured\nin Hz (i.e., 1/sec). If values of X are points in three-dimensional space measured in meters,\nthen density is measured in 1/m3.\nOne of the most important probability distributions is the Gaussian distribution, also\nGAUSSIAN\nDISTRIBUTION\nknown as the normal distribution. A Gaussian distribution with mean μ and standard devi-\nation σ (and therefore variance σ2) is deﬁned as\nP(x) =\n1\nσ\n√\n2πe−(x−μ)2/(2σ2) ,\nwhere x is a continuous variable ranging from −∞to +∞. With mean μ = 0 and variance\nσ2 = 1, we get the special case of the standard normal distribution. For a distribution over\nSTANDARD NORMAL\nDISTRIBUTION\na vector x in n dimensions, there is the multivariate Gaussian distribution:\nMULTIVARIATE\nGAUSSIAN\nP(x) =\n1",
  "(2π)n|Σ|\ne−1\n2\n“\n(x−μ)⊤Σ\n−1(x−μ)\n”\n,\nwhere μ is the mean vector and Σ is the covariance matrix (see below).\nIn one dimension, we can deﬁne the cumulative distribution function F(x) as the\nCUMULATIVE\nDISTRIBUTION\nprobability that a random variable will be less than x. For the normal distribution, this is\nF(x) =\nx\n\u001a\n−∞\nP(z)dz = 1\n2(1 + erf(z −μ\nσ\n√\n2 )) ,\nwhere erf(x) is the so-called error function, which has no closed-form representation.\nThe central limit theorem states that the distribution formed by sampling n indepen-\nCENTRAL LIMIT\nTHEOREM\ndent random variables and taking their mean tends to a normal distribution as n tends to\ninﬁnity. This holds for almost any collection of random variables, even if they are not strictly\nindependent, unless the variance of any ﬁnite subset of variables dominates the others.\nThe expectation of a random variable, E(X), is the mean or average value, weighted\nEXPECTATION\nby the probability of each value. For a discrete variable it is:\nE(X) =\n\f\ni\nxi P(X = xi) .\nFor a continuous variable, replace the summation with an integral over the probability density\nfunction, P(x):\nE(X) =\n∞\n\u001a\n−∞\nxP(x) dx , Bibliographical and Historical Notes\n1059\nThe root mean square, RMS, of a set of values (often samples of a random variable) is\nROOT MEAN SQUARE\nthe square root of the mean of the squares of the values,\nRMS(x1, . . . , xn) =\n\b\nx2\n1 + . . . + x2n\nn\n.\nThe covariance of two random variables is the expectation of the product of their differences\nCOVARIANCE\nfrom their means:\ncov(X, Y ) = E((X −μX)(Y −μY )) .\nThe covariance matrix, often denoted Σ, is a matrix of covariances between elements of a\nCOVARIANCE MATRIX\nvector of random variables. Given X = ⟨X1, . . . Xn⟩⊤, the entries of the covariance matrix\nare as follows:\nΣi,j = cov(Xi, Xj) = E((Xi −μi)(Xj −μj)) .\nA few more miscellaneous points: we use log(x) for the natural logarithm, loge(x). We use\nargmaxx f(x) for the value of x for which f(x) is maximal.\nBIBLIOGRAPHICAL AND HISTORICAL NOTES\nThe O() notation so widely used in computer science today was ﬁrst introduced in the context\nof number theory by the German mathematician P. G. H. Bachmann (1894). The concept of\nNP-completeness was invented by Cook (1971), and the modern method for establishing a\nreduction from one problem to another is due to Karp (1972). Cook and Karp have both won\nthe Turing award, the highest honor in computer science, for their work.",
  "NP-completeness was invented by Cook (1971), and the modern method for establishing a\nreduction from one problem to another is due to Karp (1972). Cook and Karp have both won\nthe Turing award, the highest honor in computer science, for their work.\nClassic works on the analysis and design of algorithms include those by Knuth (1973)\nand Aho, Hopcroft, and Ullman (1974); more recent contributions are by Tarjan (1983) and\nCormen, Leiserson, and Rivest (1990). These books place an emphasis on designing and\nanalyzing algorithms to solve tractable problems. For the theory of NP-completeness and\nother forms of intractability, see Garey and Johnson (1979) or Papadimitriou (1994). Good\ntexts on probability include Chung (1979), Ross (1988), and Bertsekas and Tsitsiklis (2008). B\nNOTES ON LANGUAGES\nAND ALGORITHMS\nB.1\nDEFINING LANGUAGES WITH BACKUS–NAUR FORM (BNF)\nIn this book, we deﬁne several languages, including the languages of propositional logic\n(page 243), ﬁrst-order logic (page 293), and a subset of English (page 899). A formal lan-\nguage is deﬁned as a set of strings where each string is a sequence of symbols. The languages\nwe are interested in consist of an inﬁnite set of strings, so we need a concise way to charac-\nterize the set. We do that with a grammar. The particular type of grammar we use is called a\ncontext-free grammar, because each expression has the same form in any context. We write\nCONTEXT-FREE\nGRAMMAR\nour grammars in a formalism called Backus–Naur form (BNF). There are four components\nBACKUS–NAUR\nFORM (BNF)\nto a BNF grammar:\n• A set of terminal symbols. These are the symbols or words that make up the strings of\nTERMINAL SYMBOL\nthe language. They could be letters (A, B, C, . . .) or words (a, aardvark, abacus, . . .),\nor whatever symbols are appropriate for the domain.\n• A set of nonterminal symbols that categorize subphrases of the language. For exam-\nNONTERMINAL\nSYMBOL\nple, the nonterminal symbol NounPhrase in English denotes an inﬁnite set of strings\nincluding “you” and “the big slobbery dog.”\n• A start symbol, which is the nonterminal symbol that denotes the complete set of\nSTART SYMBOL\nstrings of the language. In English, this is Sentence; for arithmetic, it might be Expr,\nand for programming languages it is Program.\n• A set of rewrite rules, of the form LHS\n→\nRHS, where LHS is a nonterminal\nsymbol and RHS is a sequence of zero or more symbols. These can be either terminal",
  "and for programming languages it is Program.\n• A set of rewrite rules, of the form LHS\n→\nRHS, where LHS is a nonterminal\nsymbol and RHS is a sequence of zero or more symbols. These can be either terminal\nor nonterminal symbols, or the symbol ϵ, which is used to denote the empty string.\nA rewrite rule of the form\nSentence →NounPhrase VerbPhrase\nmeans that whenever we have two strings categorized as a NounPhrase and a VerbPhrase,\nwe can append them together and categorize the result as a Sentence. As an abbreviation,\nthe two rules (S →A) and (S →B) can be written (S →A | B).\n1060 Section B.2.\nDescribing Algorithms with Pseudocode\n1061\nHere is a BNF grammar for simple arithmetic expressions:\nExpr\n→\nExpr Operator Expr | ( Expr ) | Number\nNumber\n→\nDigit | Number Digit\nDigit\n→\n0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\nOperator\n→\n+ |\n−|\n÷ |\n×\nWe cover languages and grammars in more detail in Chapter 22. Be aware that other books\nuse slightly different notations for BNF; for example, you might see ⟨Digit⟩instead of Digit\nfor a nonterminal, ‘word’ instead of word for a terminal, or ::= instead of →in a rule.\nB.2\nDESCRIBING ALGORITHMS WITH PSEUDOCODE\nThe algorithms in this book are described in pseudocode. Most of the pseudocode should be\nfamiliar to users of languages like Java, C++, or Lisp. In some places we use mathematical\nformulas or ordinary English to describe parts that would otherwise be more cumbersome. A\nfew idiosyncrasies should be noted.\n• Persistent variables: We use the keyword persistent to say that a variable is given an\ninitial value the ﬁrst time a function is called and retains that value (or the value given to\nit by a subsequent assignment statement) on all subsequent calls to the function. Thus,\npersistent variables are like global variables in that they outlive a single call to their\nfunction, but they are accessible only within the function. The agent programs in the\nbook use persistent variables for memory. Programs with persistent variables can be\nimplemented as objects in object-oriented languages such as C++, Java, Python, and\nSmalltalk. In functional languages, they can be implemented by functional closures\nover an environment containing the required variables.\n• Functions as values: Functions and procedures have capitalized names, and variables\nhave lowercase italic names. So most of the time, a function call looks like FN(x).\nHowever, we allow the value of a variable to be a function; for example, if the value of",
  "have lowercase italic names. So most of the time, a function call looks like FN(x).\nHowever, we allow the value of a variable to be a function; for example, if the value of\nthe variable f is the square root function, then f (9) returns 3.\n• for each: The notation “for each x in c do” means that the loop is executed with the\nvariable x bound to successive elements of the collection c.\n• Indentation is signiﬁcant: Indentation is used to mark the scope of a loop or condi-\ntional, as in the language Python, and unlike Java and C++ (which use braces) or Pascal\nand Visual Basic (which use end).\n• Destructuring assignment: The notation “x, y ←pair” means that the right-hand side\nmust evaluate to a two-element tuple, and the ﬁrst element is assigned to x and the\nsecond to y. The same idea is used in “for each x, y in pairs do” and can be used to\nswap two variables: “x, y ←y, x”\n• Generators and yield: the notation “generator G(x) yields numbers” deﬁnes G as a\ngenerator function. This is best understood by an example. The code fragment shown in 1062\nAppendix\nB.\nNotes on Languages and Algorithms\ngenerator POWERS-OF-2() yields ints\ni ←1\nwhile true do\nyield i\ni ←2 × i\nfor p in POWERS-OF-2() do\nPRINT(p)\nFigure B.1\nExample of a generator function and its invocation within a loop.\nFigure B.1 prints the numbers 1, 2, 4, . . . , and never stops. The call to POWERS-OF-2\nreturns a generator, which in turn yields one value each time the loop code asks for the\nnext element of the collection. Even though the collection is inﬁnite, it is enumerated\none element at a time.\n• Lists: [x, y, z] denotes a list of three elements. [ﬁrst|rest] denotes a list formed by\nadding ﬁrst to the list rest. In Lisp, this is the cons function.\n• Sets: {x, y, z} denotes a set of three elements. {x : p(x)} denotes the set of all elements\nx for which p(x) is true.\n• Arrays start at 1: Unless stated otherwise, the ﬁrst index of an array is 1 as in usual\nmathematical notation, not 0, as in Java and C.\nB.3\nONLINE HELP\nMost of the algorithms in the book have been implemented in Java, Lisp, and Python at our\nonline code repository:\naima.cs.berkeley.edu\nThe same Web site includes instructions for sending comments, corrections, or suggestions\nfor improving the book, and for joining discussion lists. Bibliography\nThe following abbreviations are used for frequently cited conferences and journals:\nAAAI\nProceedings of the AAAI Conference on Artiﬁcial Intelligence\nAAMAS",
  "for improving the book, and for joining discussion lists. Bibliography\nThe following abbreviations are used for frequently cited conferences and journals:\nAAAI\nProceedings of the AAAI Conference on Artiﬁcial Intelligence\nAAMAS\nProceedings of the International Conference on Autonomous Agents and Multi-agent Systems\nACL\nProceedings of the Annual Meeting of the Association for Computational Linguistics\nAIJ\nArtiﬁcial Intelligence\nAIMag\nAI Magazine\nAIPS\nProceedings of the International Conference on AI Planning Systems\nBBS\nBehavioral and Brain Sciences\nCACM\nCommunications of the Association for Computing Machinery\nCOGSCI\nProceedings of the Annual Conference of the Cognitive Science Society\nCOLING\nProceedings of the International Conference on Computational Linguistics\nCOLT\nProceedings of the Annual ACM Workshop on Computational Learning Theory\nCP\nProceedings of the International Conference on Principles and Practice of Constraint Programming\nCVPR\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\nEC\nProceedings of the ACM Conference on Electronic Commerce\nECAI\nProceedings of the European Conference on Artiﬁcial Intelligence\nECCV\nProceedings of the European Conference on Computer Vision\nECML\nProceedings of the The European Conference on Machine Learning\nECP\nProceedings of the European Conference on Planning\nFGCS\nProceedings of the International Conference on Fifth Generation Computer Systems\nFOCS\nProceedings of the Annual Symposium on Foundations of Computer Science\nICAPS\nProceedings of the International Conference on Automated Planning and Scheduling\nICASSP\nProceedings of the International Conference on Acoustics, Speech, and Signal Processing\nICCV\nProceedings of the International Conference on Computer Vision\nICLP\nProceedings of the International Conference on Logic Programming\nICML\nProceedings of the International Conference on Machine Learning\nICPR\nProceedings of the International Conference on Pattern Recognition\nICRA\nProceedings of the IEEE International Conference on Robotics and Automation\nICSLP\nProceedings of the International Conference on Speech and Language Processing\nIJAR\nInternational Journal of Approximate Reasoning\nIJCAI\nProceedings of the International Joint Conference on Artiﬁcial Intelligence\nIJCNN\nProceedings of the International Joint Conference on Neural Networks\nIJCV\nInternational Journal of Computer Vision\nILP\nProceedings of the International Workshop on Inductive Logic Programming\nISMIS",
  "IJCNN\nProceedings of the International Joint Conference on Neural Networks\nIJCV\nInternational Journal of Computer Vision\nILP\nProceedings of the International Workshop on Inductive Logic Programming\nISMIS\nProceedings of the International Symposium on Methodologies for Intelligent Systems\nISRR\nProceedings of the International Symposium on Robotics Research\nJACM\nJournal of the Association for Computing Machinery\nJAIR\nJournal of Artiﬁcial Intelligence Research\nJAR\nJournal of Automated Reasoning\nJASA\nJournal of the American Statistical Association\nJMLR\nJournal of Machine Learning Research\nJSL\nJournal of Symbolic Logic\nKDD\nProceedings of the International Conference on Knowledge Discovery and Data Mining\nKR\nProceedings of the International Conference on Principles of Knowledge Representation and Reasoning\nLICS\nProceedings of the IEEE Symposium on Logic in Computer Science\nNIPS\nAdvances in Neural Information Processing Systems\nPAMI\nIEEE Transactions on Pattern Analysis and Machine Intelligence\nPNAS\nProceedings of the National Academy of Sciences of the United States of America\nPODS\nProceedings of the ACM International Symposium on Principles of Database Systems\nSIGIR\nProceedings of the Special Interest Group on Information Retrieval\nSIGMOD Proceedings of the ACM SIGMOD International Conference on Management of Data\nSODA\nProceedings of the Annual ACM–SIAM Symposium on Discrete Algorithms\nSTOC\nProceedings of the Annual ACM Symposium on Theory of Computing\nTARK\nProceedings of the Conference on Theoretical Aspects of Reasoning about Knowledge\nUAI\nProceedings of the Conference on Uncertainty in Artiﬁcial Intelligence\n1063 1064\nBibliography\nAarup, M., Arentoft, M. M., Parrod, Y., Stader,\nJ.,\nand Stokes,\nI. (1994).\nOPTIMUM-AIV:\nA knowledge-based planning and scheduling system\nfor spacecraft AIV.\nIn Fox, M. and Zweben, M.\n(Eds.), Knowledge Based Scheduling. Morgan Kauf-\nmann.\nAbney, S. (2007).\nSemisupervised Learning for\nComputational Linguistics. CRC Press.\nAbramson, B. and Yung, M. (1989).\nDivide and\nconquer under global constraints: A solution to the\nN-queens problem. J. Parallel and Distributed Com-\nputing, 6(3), 649–662.\nAchlioptas, D. (2009).\nRandom satisﬁability.\nIn\nBiere, A., Heule, M., van Maaren, H., and Walsh, T.\n(Eds.), Handbook of Satisﬁability. IOS Press.\nAchlioptas, D., Beame, P., and Molloy, M. (2004).\nExponential bounds for DPLL below the satisﬁabil-\nity threshold. In SODA-04.\nAchlioptas, D., Naor, A., and Peres, Y. (2007).",
  "(Eds.), Handbook of Satisﬁability. IOS Press.\nAchlioptas, D., Beame, P., and Molloy, M. (2004).\nExponential bounds for DPLL below the satisﬁabil-\nity threshold. In SODA-04.\nAchlioptas, D., Naor, A., and Peres, Y. (2007).\nOn the maximum satisﬁability of random formulas.\nJACM, 54(2).\nAchlioptas, D. and Peres, Y. (2004). The threshold\nfor random k-SAT is 2k log 2 −o(k). J. American\nMathematical Society, 17(4), 947–973.\nAckley, D. H. and Littman, M. L. (1991).\nInter-\nactions between learning and evolution.\nIn Lang-\nton, C., Taylor, C., Farmer, J. D., and Ramussen,\nS. (Eds.), Artiﬁcial Life II, pp. 487–509. Addison-\nWesley.\nAdelson-Velsky, G. M., Arlazarov, V. L., Bitman,\nA. R., Zhivotovsky, A. A., and Uskov, A. V. (1970).\nProgramming a computer to play chess.\nRussian\nMathematical Surveys, 25, 221–262.\nAdida, B. and Birbeck, M. (2008). RDFa primer.\nTech. rep., W3C.\nAgerbeck, C. and Hansen, M. O. (2008). A multi-\nagent approach to solving NP-complete problems.\nMaster’s thesis, Technical Univ. of Denmark.\nAggarwal, G., Goel, A., and Motwani, R. (2006).\nTruthful auctions for pricing search keywords.\nIn\nEC-06, pp. 1–7.\nAgichtein, E. and Gravano, L. (2003).\nQuerying\ntext databases for efﬁcient information extraction. In\nProc. IEEE Conference on Data Engineering.\nAgmon, S. (1954). The relaxation method for lin-\near inequalities. Canadian Journal of Mathematics,\n6(3), 382–392.\nAgre, P. E. and Chapman, D. (1987). Pengi: an im-\nplementation of a theory of activity. In IJCAI-87, pp.\n268–272.\nAho, A. V., Hopcroft, J., and Ullman, J. D. (1974).\nThe Design and Analysis of Computer Algorithms.\nAddison-Wesley.\nAizerman, M., Braverman, E., and Rozonoer, L.\n(1964).\nTheoretical foundations of the potential\nfunction method in pattern recognition learning. Au-\ntomation and Remote Control, 25, 821–837.\nAl-Chang, M., Bresina, J., Charest, L., Chase, A.,\nHsu, J., Jonsson, A., Kanefsky, B., Morris, P., Rajan,\nK., Yglesias, J., Chaﬁn, B., Dias, W., and Maldague,\nP. (2004). MAPGEN: Mixed-Initiative planning and\nscheduling for the Mars Exploration Rover mission.\nIEEE Intelligent Systems, 19(1), 8–12.\nAlbus, J. S. (1975). A new approach to manipulator\ncontrol: The cerebellar model articulation controller\n(CMAC). J. Dynamic Systems, Measurement, and\nControl, 97, 270–277.\nAldous, D. and Vazirani, U. (1994). “Go with the\nwinners” algorithms. In FOCS-94, pp. 492–501.\nAlekhnovich, M., Hirsch, E. A., and Itsykson, D.\n(2005). Exponential lower bounds for the running",
  "Control, 97, 270–277.\nAldous, D. and Vazirani, U. (1994). “Go with the\nwinners” algorithms. In FOCS-94, pp. 492–501.\nAlekhnovich, M., Hirsch, E. A., and Itsykson, D.\n(2005). Exponential lower bounds for the running\ntime of DPLL algorithms on satisﬁable formulas.\nJAR, 35(1–3), 51–72.\nAllais, M. (1953).\nLe comportment de l’homme\nrationnel devant la risque: critique des postulats et\naxiomes de l’´ecole Am´ericaine. Econometrica, 21,\n503–546.\nAllen, J. F. (1983). Maintaining knowledge about\ntemporal intervals. CACM, 26(11), 832–843.\nAllen, J. F. (1984). Towards a general theory of ac-\ntion and time. AIJ, 23, 123–154.\nAllen, J. F. (1991). Time and time again: The many\nways to represent time. Int. J. Intelligent Systems, 6,\n341–355.\nAllen, J. F., Hendler, J., and Tate, A. (Eds.). (1990).\nReadings in Planning. Morgan Kaufmann.\nAllis, L. (1988).\nA knowledge-based approach to\nconnect four. The game is solved: White wins. Mas-\nter’s thesis, Vrije Univ., Amsterdam.\nAlmuallim, H. and Dietterich, T. (1991). Learning\nwith many irrelevant features. In AAAI-91, Vol. 2,\npp. 547–552.\nALPAC (1966).\nLanguage and machines: Com-\nputers in translation and linguistics.\nTech. rep.\n1416, The Automatic Language Processing Advi-\nsory Committee of the National Academy of Sci-\nences.\nAlterman, R. (1988). Adaptive planning. Cognitive\nScience, 12, 393–422.\nAmarel, S. (1967).\nAn approach to heuristic\nproblem-solving and theorem proving in the propo-\nsitional calculus.\nIn Hart, J. and Takasu, S.\n(Eds.), Systems and Computer Science. University of\nToronto Press.\nAmarel, S. (1968).\nOn representations of prob-\nlems of reasoning about actions.\nIn Michie, D.\n(Ed.), Machine Intelligence 3, Vol. 3, pp. 131–171.\nElsevier/North-Holland.\nAmir, E. and Russell, S. J. (2003). Logical ﬁltering.\nIn IJCAI-03.\nAmit, D., Gutfreund, H., and Sompolinsky, H.\n(1985). Spin-glass models of neural networks. Phys-\nical Review, A 32, 1007–1018.\nAndersen, S. K., Olesen, K. G., Jensen, F. V., and\nJensen, F. (1989).\nHUGIN—A shell for building\nBayesian belief universes for expert systems.\nIn\nIJCAI-89, Vol. 2, pp. 1080–1085.\nAnderson, J. R. (1980). Cognitive Psychology and\nIts Implications. W. H. Freeman.\nAnderson, J. R. (1983). The Architecture of Cogni-\ntion. Harvard University Press.\nAndoni, A. and Indyk, P. (2006). Near-optimal hash-\ning algorithms for approximate nearest neighbor in\nhigh dimensions. In FOCS-06.\nAndre, D. and Russell, S. J. (2002). State abstraction\nfor programmable reinforcement learning agents. In",
  "Andoni, A. and Indyk, P. (2006). Near-optimal hash-\ning algorithms for approximate nearest neighbor in\nhigh dimensions. In FOCS-06.\nAndre, D. and Russell, S. J. (2002). State abstraction\nfor programmable reinforcement learning agents. In\nAAAI-02, pp. 119–125.\nAnthony, M. and Bartlett, P. (1999). Neural Net-\nwork Learning: Theoretical Foundations.\nCam-\nbridge University Press.\nAoki, M. (1965). Optimal control of partially ob-\nservable Markov systems.\nJ. Franklin Institute,\n280(5), 367–386.\nAppel, K. and Haken, W. (1977). Every planar map\nis four colorable: Part I: Discharging.\nIllinois J.\nMath., 21, 429–490.\nAppelt, D. (1999). Introduction to information ex-\ntraction. CACM, 12(3), 161–172.\nApt, K. R. (1999). The essence of constraint prop-\nagation. Theoretical Computer Science, 221(1–2),\n179–210.\nApt, K. R. (2003).\nPrinciples of Constraint Pro-\ngramming. Cambridge University Press.\nApt´e, C., Damerau, F., and Weiss, S. (1994). Auto-\nmated learning of decision rules for text categoriza-\ntion. ACM Transactions on Information Systems, 12,\n233–251.\nArbuthnot, J. (1692).\nOf the Laws of Chance.\nMotte, London. Translation into English, with ad-\nditions, of Huygens (1657).\nArchibald,\nC.,\nAltman,\nA.,\nand Shoham,\nY.\n(2009).\nAnalysis of a winning computational bil-\nliards player. In IJCAI-09.\nAriely, D. (2009). Predictably Irrational (Revised\nedition). Harper.\nArkin, R. (1998). Behavior-Based Robotics. MIT\nPress.\nArmando, A., Carbone, R., Compagna, L., Cuel-\nlar, J., and Tobarra, L. (2008). Formal analysis of\nSAML 2.0 web browser single sign-on: Breaking\nthe SAML-based single sign-on for google apps. In\nFMSE ’08: Proc. 6th ACM workshop on Formal\nmethods in security engineering, pp. 1–10.\nArnauld, A. (1662). La logique, ou l’art de penser.\nChez Charles Savreux, au pied de la Tour de Nostre\nDame, Paris.\nArora, S. (1998). Polynomial time approximation\nschemes for Euclidean traveling salesman and other\ngeometric problems. JACM, 45(5), 753–782.\nArunachalam, R. and Sadeh, N. M. (2005). The\nsupply chain trading agent competition. Electronic\nCommerce Research and Applications, Spring, 66–\n84.\nAshby, W. R. (1940). Adaptiveness and equilibrium.\nJ. Mental Science, 86, 478–483.\nAshby, W. R. (1948). Design for a brain. Electronic\nEngineering, December, 379–383.\nAshby, W. R. (1952). Design for a Brain. Wiley.\nAsimov, I. (1942). Runaround. Astounding Science\nFiction, March.\nAsimov, I. (1950). I, Robot. Doubleday.\nAstrom, K. J. (1965). Optimal control of Markov",
  "Engineering, December, 379–383.\nAshby, W. R. (1952). Design for a Brain. Wiley.\nAsimov, I. (1942). Runaround. Astounding Science\nFiction, March.\nAsimov, I. (1950). I, Robot. Doubleday.\nAstrom, K. J. (1965). Optimal control of Markov\ndecision processes with incomplete state estimation.\nJ. Math. Anal. Applic., 10, 174–205.\nAudi, R. (Ed.). (1999). The Cambridge Dictionary\nof Philosophy. Cambridge University Press.\nAxelrod, R. (1985). The Evolution of Cooperation.\nBasic Books.\nBaader, F., Calvanese, D., McGuinness, D., Nardi,\nD., and Patel-Schneider, P. (2007). The Description\nLogic Handbook (2nd edition). Cambridge Univer-\nsity Press.\nBaader, F. and Snyder, W. (2001). Uniﬁcation the-\nory. In Robinson, J. and Voronkov, A. (Eds.), Hand-\nbook of Automated Reasoning, pp. 447–533. Else-\nvier.\nBacchus, F. (1990).\nRepresenting and Reasoning\nwith Probabilistic Knowledge. MIT Press.\nBacchus, F. and Grove, A. (1995). Graphical models\nfor preference and utility. In UAI-95, pp. 3–10.\nBacchus, F. and Grove, A. (1996). Utility indepen-\ndence in a qualitative decision theory. In KR-96, pp.\n542–552. Bibliography\n1065\nBacchus, F., Grove, A., Halpern, J. Y., and Koller,\nD. (1992). From statistics to beliefs. In AAAI-92,\npp. 602–608.\nBacchus, F. and van Beek, P. (1998). On the conver-\nsion between non-binary and binary constraint satis-\nfaction problems. In AAAI-98, pp. 311–318.\nBacchus, F. and van Run, P. (1995). Dynamic vari-\nable ordering in CSPs. In CP-95, pp. 258–275.\nBachmann, P. G. H. (1894). Die analytische Zahlen-\ntheorie. B. G. Teubner, Leipzig.\nBackus, J. W. (1996). Transcript of question and an-\nswer session. In Wexelblat, R. L. (Ed.), History of\nProgramming Languages, p. 162. Academic Press.\nBagnell, J. A. and Schneider, J. (2001). Autonomous\nhelicopter control using reinforcement learning pol-\nicy search methods. In ICRA-01.\nBaker,\nJ. (1975).\nThe Dragon system—An\noverview. IEEE Transactions on Acoustics; Speech;\nand Signal Processing, 23, 24–29.\nBaker, J. (1979).\nTrainable grammars for speech\nrecognition. In Speech Communication Papers for\nthe 97th Meeting of the Acoustical Society of Amer-\nica, pp. 547–550.\nBaldi, P., Chauvin, Y., Hunkapiller, T., and Mc-\nClure, M. (1994). Hidden Markov models of bio-\nlogical primary sequence information. PNAS, 91(3),\n1059–1063.\nBaldwin, J. M. (1896). A new factor in evolution.\nAmerican Naturalist, 30, 441–451.\nContinued on\npages 536–553.\nBallard, B. W. (1983). The *-minimax search pro-",
  "logical primary sequence information. PNAS, 91(3),\n1059–1063.\nBaldwin, J. M. (1896). A new factor in evolution.\nAmerican Naturalist, 30, 441–451.\nContinued on\npages 536–553.\nBallard, B. W. (1983). The *-minimax search pro-\ncedure for trees containing chance nodes. AIJ, 21(3),\n327–350.\nBaluja, S. (1997). Genetic algorithms and explicit\nsearch statistics. In Mozer, M. C., Jordan, M. I., and\nPetsche, T. (Eds.), NIPS 9, pp. 319–325. MIT Press.\nBancilhon, F., Maier, D., Sagiv, Y., and Ullman,\nJ. D. (1986). Magic sets and other strange ways to\nimplement logic programs. In PODS-86, pp. 1–16.\nBanko, M. and Brill, E. (2001). Scaling to very very\nlarge corpora for natural language disambiguation.\nIn ACL-01, pp. 26–33.\nBanko, M., Brill, E., Dumais, S. T., and Lin, J.\n(2002).\nAskmsr:\nQuestion answering using the\nworldwide web. In Proc. AAAI Spring Symposium on\nMining Answers from Texts and Knowledge Bases,\npp. 7–9.\nBanko, M., Cafarella, M. J., Soderland, S., Broad-\nhead, M., and Etzioni, O. (2007). Open information\nextraction from the web. In IJCAI-07.\nBanko, M. and Etzioni, O. (2008).\nThe tradeoffs\nbetween open and traditional relation extraction. In\nACL-08, pp. 28–36.\nBar-Hillel, Y. (1954). Indexical expressions. Mind,\n63, 359–379.\nBar-Hillel, Y. (1960). The present status of auto-\nmatic translation of languages. In Alt, F. L. (Ed.),\nAdvances in Computers, Vol. 1, pp. 91–163. Aca-\ndemic Press.\nBar-Shalom,\nY. (Ed.). (1992).\nMultitarget-\nmultisensor\ntracking:\nAdvanced\napplications.\nArtech House.\nBar-Shalom, Y. and Fortmann, T. E. (1988). Track-\ning and Data Association. Academic Press.\nBartak, R. (2001). Theory and practice of constraint\npropagation. In Proc. Third Workshop on Constraint\nProgramming for Decision and Control (CPDC-01),\npp. 7–14.\nBarto, A. G., Bradtke, S. J., and Singh, S. P. (1995).\nLearning to act using real-time dynamic program-\nming. AIJ, 73(1), 81–138.\nBarto, A. G., Sutton, R. S., and Anderson, C. W.\n(1983). Neuron-like adaptive elements that can solve\ndifﬁcult learning control problems. IEEE Transac-\ntions on Systems, Man and Cybernetics, 13, 834–\n846.\nBarto, A. G., Sutton, R. S., and Brouwer, P. S.\n(1981).\nAssociative search network: A reinforce-\nment learning associative memory. Biological Cy-\nbernetics, 40(3), 201–211.\nBarwise, J. and Etchemendy, J. (1993). The Lan-\nguage of First-Order Logic: Including the Macin-\ntosh Program Tarski’s World 4.0 (Third Revised and\nExpanded edition). Center for the Study of Language\nand Information (CSLI).",
  "Barwise, J. and Etchemendy, J. (1993). The Lan-\nguage of First-Order Logic: Including the Macin-\ntosh Program Tarski’s World 4.0 (Third Revised and\nExpanded edition). Center for the Study of Language\nand Information (CSLI).\nBarwise, J. and Etchemendy, J. (2002). Language,\nProof and Logic. CSLI (Univ. of Chicago Press).\nBaum, E., Boneh, D., and Garrett, C. (1995). On\ngenetic algorithms. In COLT-95, pp. 230–239.\nBaum, E. and Haussler, D. (1989). What size net\ngives valid generalization?\nNeural Computation,\n1(1), 151–160.\nBaum, E. and Smith, W. D. (1997). A Bayesian ap-\nproach to relevance in game playing. AIJ, 97(1–2),\n195–242.\nBaum, E. and Wilczek, F. (1988). Supervised learn-\ning of probability distributions by neural networks.\nIn Anderson, D. Z. (Ed.), Neural Information Pro-\ncessing Systems, pp. 52–61. American Institute of\nPhysics.\nBaum, L. E. and Petrie, T. (1966).\nStatistical\ninference for probabilistic functions of ﬁnite state\nMarkov chains. Annals of Mathematical Statistics,\n41.\nBaxter, J. and Bartlett, P. (2000).\nReinforcement\nlearning in POMDP’s via direct gradient ascent. In\nICML-00, pp. 41–48.\nBayardo, R. J. and Miranker, D. P. (1994).\nAn\noptimal backtrack algorithm for tree-structured con-\nstraint satisfaction problems. AIJ, 71(1), 159–181.\nBayardo, R. J. and Schrag, R. C. (1997).\nUsing\nCSP look-back techniques to solve real-world SAT\ninstances. In AAAI-97, pp. 203–208.\nBayes, T. (1763). An essay towards solving a prob-\nlem in the doctrine of chances. Philosophical Trans-\nactions of the Royal Society of London, 53, 370–418.\nBeal, D. F. (1980).\nAn analysis of minimax.\nIn\nClarke, M. R. B. (Ed.), Advances in Computer\nChess 2, pp. 103–109. Edinburgh University Press.\nBeal, J. and Winston, P. H. (2009). The new frontier\nof human-level artiﬁcial intelligence. IEEE Intelli-\ngent Systems, 24(4), 21–23.\nBeckert, B. and Posegga, J. (1995). Leantap: Lean,\ntableau-based deduction. JAR, 15(3), 339–358.\nBeeri, C., Fagin, R., Maier, D., and Yannakakis,\nM. (1983). On the desirability of acyclic database\nschemes. JACM, 30(3), 479–513.\nBekey, G. (2008). Robotics: State Of The Art And\nFuture Challenges. Imperial College Press.\nBell, C. and Tate, A. (1985). Using temporal con-\nstraints to restrict search in a planner. In Proc. Third\nAlvey IKBS SIG Workshop.\nBell, J. L. and Machover, M. (1977). A Course in\nMathematical Logic. Elsevier/North-Holland.\nBellman, R. E. (1952). On the theory of dynamic\nprogramming. PNAS, 38, 716–719.",
  "Alvey IKBS SIG Workshop.\nBell, J. L. and Machover, M. (1977). A Course in\nMathematical Logic. Elsevier/North-Holland.\nBellman, R. E. (1952). On the theory of dynamic\nprogramming. PNAS, 38, 716–719.\nBellman, R. E. (1961). Adaptive Control Processes:\nA Guided Tour. Princeton University Press.\nBellman, R. E. (1965). On the application of dy-\nnamic programming to the determination of optimal\nplay in chess and checkers. PNAS, 53, 244–246.\nBellman, R. E. (1978). An Introduction to Artiﬁcial\nIntelligence: Can Computers Think? Boyd & Fraser\nPublishing Company.\nBellman, R. E. (1984). Eye of the Hurricane. World\nScientiﬁc.\nBellman, R. E. and Dreyfus, S. E. (1962). Applied\nDynamic Programming. Princeton University Press.\nBellman, R. E. (1957).\nDynamic Programming.\nPrinceton University Press.\nBelongie, S., Malik, J., and Puzicha, J. (2002).\nShape matching and object recognition using shape\ncontexts. PAMI, 24(4), 509–522.\nBen-Tal, A. and Nemirovski, A. (2001). Lectures on\nModern Convex Optimization: Analysis, Algorithms,\nand Engineering Applications. SIAM (Society for\nIndustrial and Applied Mathematics).\nBengio, Y. and LeCun, Y. (2007).\nScaling learn-\ning algorithms towards AI. In Bottou, L., Chapelle,\nO., DeCoste, D., and Weston, J. (Eds.), Large-Scale\nKernel Machines. MIT Press.\nBentham, J. (1823). Principles of Morals and Legis-\nlation. Oxford University Press, Oxford, UK. Orig-\ninal work published in 1789.\nBerger, J. O. (1985).\nStatistical Decision Theory\nand Bayesian Analysis. Springer Verlag.\nBerkson, J. (1944). Application of the logistic func-\ntion to bio-assay. JASA, 39, 357–365.\nBerlekamp, E. R., Conway, J. H., and Guy, R. K.\n(1982).\nWinning Ways, For Your Mathematical\nPlays. Academic Press.\nBerlekamp, E. R. and Wolfe, D. (1994). Mathemat-\nical Go: Chilling Gets the Last Point. A.K. Peters.\nBerleur, J. and Brunnstein, K. (2001).\nEthics of\nComputing: Codes, Spaces for Discussion and Law.\nChapman and Hall.\nBerliner, H. J. (1979). The B* tree search algorithm:\nA best-ﬁrst proof procedure. AIJ, 12(1), 23–40.\nBerliner, H. J. (1980a).\nBackgammon computer\nprogram beats world champion. AIJ, 14, 205–220.\nBerliner, H. J. (1980b).\nComputer backgammon.\nScientiﬁc American, 249(6), 64–72.\nBernardo, J. M. and Smith, A. F. M. (1994).\nBayesian Theory. Wiley.\nBerners-Lee, T., Hendler, J., and Lassila, O. (2001).\nThe semantic web. Scientiﬁc American, 284(5), 34–\n43.\nBernoulli, D. (1738).\nSpecimen theoriae novae\nde mensura sortis.\nProc. St. Petersburg Imperial",
  "Bayesian Theory. Wiley.\nBerners-Lee, T., Hendler, J., and Lassila, O. (2001).\nThe semantic web. Scientiﬁc American, 284(5), 34–\n43.\nBernoulli, D. (1738).\nSpecimen theoriae novae\nde mensura sortis.\nProc. St. Petersburg Imperial\nAcademy of Sciences, 5, 175–192.\nBernstein, A. and Roberts, M. (1958). Computer\nvs. chess player. Scientiﬁc American, 198(6), 96–\n105.\nBernstein, P. L. (1996). Against the Odds: The Re-\nmarkable Story of Risk. Wiley.\nBerrou, C., Glavieux, A., and Thitimajshima, P.\n(1993). Near Shannon limit error control-correcting\ncoding and decoding: Turbo-codes. 1. In Proc. IEEE\nInternational Conference on Communications, pp.\n1064–1070.\nBerry, D. A. and Fristedt, B. (1985). Bandit Prob-\nlems: Sequential Allocation of Experiments. Chap-\nman and Hall. 1066\nBibliography\nBertele, U. and Brioschi, F. (1972). Nonserial dy-\nnamic programming. Academic Press.\nBertoli, P., Cimatti, A., and Roveri, M. (2001a).\nHeuristic search + symbolic model checking = ef-\nﬁcient conformant planning. In IJCAI-01, pp. 467–\n472.\nBertoli, P., Cimatti, A., Roveri, M., and Traverso, P.\n(2001b). Planning in nondeterministic domains un-\nder partial observability via symbolic model check-\ning. In IJCAI-01, pp. 473–478.\nBertot, Y., Casteran, P., Huet, G., and Paulin-\nMohring, C. (2004). Interactive Theorem Proving\nand Program Development. Springer.\nBertsekas, D. (1987). Dynamic Programming: De-\nterministic and Stochastic Models. Prentice-Hall.\nBertsekas, D. and Tsitsiklis, J. N. (1996). Neuro-\ndynamic programming. Athena Scientiﬁc.\nBertsekas, D. and Tsitsiklis, J. N. (2008). Introduc-\ntion to Probability (2nd edition). Athena Scientiﬁc.\nBertsekas, D. and Shreve, S. E. (2007). Stochastic\nOptimal Control: The Discrete-Time Case. Athena\nScientiﬁc.\nBessi`ere, C. (2006).\nConstraint propagation.\nIn\nRossi, F., van Beek, P., and Walsh, T. (Eds.), Hand-\nbook of Constraint Programming. Elsevier.\nBhar, R. and Hamori, S. (2004). Hidden Markov\nModels:\nApplications to Financial Economics.\nSpringer.\nBibel, W. (1993).\nDeduction: Automated Logic.\nAcademic Press.\nBiere, A., Heule, M., van Maaren, H., and Walsh,\nT. (Eds.). (2009). Handbook of Satisﬁability. IOS\nPress.\nBillings, D., Burch, N., Davidson, A., Holte, R.,\nSchaeffer, J., Schauenberg, T., and Szafron, D.\n(2003).\nApproximating game-theoretic optimal\nstrategies for full-scale poker. In IJCAI-03.\nBinder, J., Koller, D., Russell, S. J., and Kanazawa,\nK. (1997a).\nAdaptive probabilistic networks with",
  "Schaeffer, J., Schauenberg, T., and Szafron, D.\n(2003).\nApproximating game-theoretic optimal\nstrategies for full-scale poker. In IJCAI-03.\nBinder, J., Koller, D., Russell, S. J., and Kanazawa,\nK. (1997a).\nAdaptive probabilistic networks with\nhidden variables. Machine Learning, 29, 213–244.\nBinder, J., Murphy, K., and Russell, S. J. (1997b).\nSpace-efﬁcient inference in dynamic probabilistic\nnetworks. In IJCAI-97, pp. 1292–1296.\nBinford, T. O. (1971). Visual perception by com-\nputer. Invited paper presented at the IEEE Systems\nScience and Cybernetics Conference, Miami.\nBinmore, K. (1982).\nEssays on Foundations of\nGame Theory. Pitman.\nBishop, C. M. (1995). Neural Networks for Pattern\nRecognition. Oxford University Press.\nBishop, C. M. (2007). Pattern Recognition and Ma-\nchine Learning. Springer-Verlag.\nBisson, T. (1990). They’re made out of meat. Omni\nMagazine.\nBistarelli, S., Montanari, U., and Rossi, F. (1997).\nSemiring-based constraint satisfaction and optimiza-\ntion. JACM, 44(2), 201–236.\nBitner, J. R. and Reingold, E. M. (1975). Backtrack\nprogramming techniques. CACM, 18(11), 651–656.\nBizer, C., Auer, S., Kobilarov, G., Lehmann, J., and\nCyganiak, R. (2007). DBPedia – querying wikipedia\nlike a database. In Developers Track Presentation\nat the 16th International Conference on World Wide\nWeb.\nBlazewicz, J., Ecker, K., Pesch, E., Schmidt, G.,\nand Weglarz, J. (2007).\nHandbook on Schedul-\ning: Models and Methods for Advanced Planning\n(International Handbooks on Information Systems).\nSpringer-Verlag New York, Inc.\nBlei, D. M., Ng, A. Y., and Jordan, M. I. (2001).\nLatent Dirichlet Allocation. In Neural Information\nProcessing Systems, Vol. 14.\nBlinder, A. S. (1983).\nIssues in the coordination\nof monetary and ﬁscal policies. In Monetary Policy\nIssues in the 1980s. Federal Reserve Bank, Kansas\nCity, Missouri.\nBliss, C. I. (1934). The method of probits. Science,\n79(2037), 38–39.\nBlock, H. D., Knight, B., and Rosenblatt, F. (1962).\nAnalysis of a four-layer series-coupled perceptron.\nRev. Modern Physics, 34(1), 275–282.\nBlum, A. L. and Furst, M. (1995). Fast planning\nthrough planning graph analysis. In IJCAI-95, pp.\n1636–1642.\nBlum, A. L. and Furst, M. (1997). Fast planning\nthrough planning graph analysis. AIJ, 90(1–2), 281–\n300.\nBlum, A. L. (1996). On-line algorithms in machine\nlearning. In Proc. Workshop on On-Line Algorithms,\nDagstuhl, pp. 306–325.\nBlum, A. L. and Mitchell, T. M. (1998). Combin-\ning labeled and unlabeled data with co-training. In",
  "300.\nBlum, A. L. (1996). On-line algorithms in machine\nlearning. In Proc. Workshop on On-Line Algorithms,\nDagstuhl, pp. 306–325.\nBlum, A. L. and Mitchell, T. M. (1998). Combin-\ning labeled and unlabeled data with co-training. In\nCOLT-98, pp. 92–100.\nBlumer, A., Ehrenfeucht, A., Haussler, D., and War-\nmuth, M. (1989).\nLearnability and the Vapnik-\nChervonenkis dimension. JACM, 36(4), 929–965.\nBobrow, D. G. (1967). Natural language input for a\ncomputer problem solving system. In Minsky, M. L.\n(Ed.), Semantic Information Processing, pp. 133–\n215. MIT Press.\nBobrow, D. G., Kaplan, R., Kay, M., Norman, D. A.,\nThompson, H., and Winograd, T. (1977). GUS, a\nframe driven dialog system. AIJ, 8, 155–173.\nBoden, M. A. (1977).\nArtiﬁcial Intelligence and\nNatural Man. Basic Books.\nBoden, M. A. (Ed.). (1990). The Philosophy of Ar-\ntiﬁcial Intelligence. Oxford University Press.\nBolognesi, A. and Ciancarini, P. (2003). Computer\nprogramming of kriegspiel endings: The case of KR\nvs. k. In Advances in Computer Games 10.\nBonet, B. (2002).\nAn epsilon-optimal grid-based\nalgorithm for partially observable Markov decision\nprocesses. In ICML-02, pp. 51–58.\nBonet, B. and Geffner, H. (1999).\nPlanning as\nheuristic search: New results. In ECP-99, pp. 360–\n372.\nBonet, B. and Geffner, H. (2000).\nPlanning with\nincomplete information as heuristic search in belief\nspace. In ICAPS-00, pp. 52–61.\nBonet, B. and Geffner, H. (2005). An algorithm bet-\nter than AO∗? In AAAI-05.\nBoole, G. (1847).\nThe Mathematical Analysis of\nLogic: Being an Essay towards a Calculus of Deduc-\ntive Reasoning. Macmillan, Barclay, and Macmillan,\nCambridge.\nBooth, T. L. (1969). Probabilistic representation of\nformal languages. In IEEE Conference Record of\nthe 1969 Tenth Annual Symposium on Switching and\nAutomata Theory, pp. 74–81.\nBorel, E. (1921). La th´eorie du jeu et les ´equations\nint´egrales `a noyau sym´etrique.\nComptes Rendus\nHebdomadaires des S´eances de l’Acad´emie des Sci-\nences, 173, 1304–1308.\nBorenstein, J., Everett, B., and Feng, L. (1996).\nNavigating Mobile Robots: Systems and Techniques.\nA. K. Peters, Ltd.\nBorenstein, J. and Koren., Y. (1991).\nThe vector\nﬁeld histogram—Fast obstacle avoidance for mobile\nrobots. IEEE Transactions on Robotics and Automa-\ntion, 7(3), 278–288.\nBorgida, A., Brachman, R. J., McGuinness, D., and\nAlperin Resnick, L. (1989). CLASSIC: A structural\ndata model for objects. SIGMOD Record, 18(2), 58–\n67.\nBoroditsky, L. (2003).\nLinguistic relativity.\nIn",
  "tion, 7(3), 278–288.\nBorgida, A., Brachman, R. J., McGuinness, D., and\nAlperin Resnick, L. (1989). CLASSIC: A structural\ndata model for objects. SIGMOD Record, 18(2), 58–\n67.\nBoroditsky, L. (2003).\nLinguistic relativity.\nIn\nNadel, L. (Ed.), Encyclopedia of Cognitive Science,\npp. 917–921. Macmillan.\nBoser, B., Guyon, I., and Vapnik, V. N. (1992). A\ntraining algorithm for optimal margin classiﬁers. In\nCOLT-92.\nBosse, M., Newman, P., Leonard, J., Soika, M.,\nFeiten, W., and Teller, S. (2004).\nSimultaneous\nlocalization and map building in large-scale cyclic\nenvironments using the atlas framework.\nInt. J.\nRobotics Research, 23(12), 1113–1139.\nBourzutschky,\nM.\n(2006).\n7-man\nendgames\nwith\npawns.\nCCRL\nDiscussion\nBoard,\nkirill-kryukov.com/chess/\ndiscussion-board/viewtopic.php?t=\n805.\nBoutilier, C. and Brafman, R. I. (2001).\nPartial-\norder planning with concurrent interacting actions.\nJAIR, 14, 105–136.\nBoutilier, C., Dearden, R., and Goldszmidt, M.\n(2000). Stochastic dynamic programming with fac-\ntored representations. AIJ, 121, 49–107.\nBoutilier, C., Reiter, R., and Price, B. (2001). Sym-\nbolic dynamic programming for ﬁrst-order MDPs. In\nIJCAI-01, pp. 467–472.\nBoutilier, C., Friedman, N., Goldszmidt, M., and\nKoller, D. (1996). Context-speciﬁc independence in\nBayesian networks. In UAI-96, pp. 115–123.\nBouzy, B. and Cazenave, T. (2001). Computer go:\nAn AI oriented survey. AIJ, 132(1), 39–103.\nBowerman, M. and Levinson, S. (2001). Language\nacquisition and conceptual development. Cambridge\nUniversity Press.\nBowling, M., Johanson, M., Burch, N., and Szafron,\nD. (2008). Strategy evaluation in extensive games\nwith importance sampling. In ICML-08.\nBox, G. E. P. (1957).\nEvolutionary operation: A\nmethod of increasing industrial productivity. Applied\nStatistics, 6, 81–101.\nBox, G. E. P., Jenkins, G., and Reinsel, G. (1994).\nTime Series Analysis: Forecasting and Control (3rd\nedition). Prentice Hall.\nBoyan, J. A. (2002).\nTechnical update:\nLeast-\nsquares temporal difference learning.\nMachine\nLearning, 49(2–3), 233–246.\nBoyan, J. A. and Moore, A. W. (1998).\nLearn-\ning evaluation functions for global optimization and\nBoolean satisﬁability. In AAAI-98.\nBoyd, S. and Vandenberghe, L. (2004). Convex Op-\ntimization. Cambridge University Press.\nBoyen, X., Friedman, N., and Koller, D. (1999). Dis-\ncovering the hidden structure of complex dynamic\nsystems. In UAI-99.\nBoyer, R. S. and Moore, J. S. (1979). A Computa-\ntional Logic. Academic Press.",
  "timization. Cambridge University Press.\nBoyen, X., Friedman, N., and Koller, D. (1999). Dis-\ncovering the hidden structure of complex dynamic\nsystems. In UAI-99.\nBoyer, R. S. and Moore, J. S. (1979). A Computa-\ntional Logic. Academic Press.\nBoyer, R. S. and Moore, J. S. (1984). Proof checking\nthe RSA public key encryption algorithm. American\nMathematical Monthly, 91(3), 181–189. Bibliography\n1067\nBrachman, R. J. (1979).\nOn the epistemologi-\ncal status of semantic networks. In Findler, N. V.\n(Ed.), Associative Networks:\nRepresentation and\nUse of Knowledge by Computers, pp. 3–50. Aca-\ndemic Press.\nBrachman, R. J., Fikes, R. E., and Levesque, H. J.\n(1983). Krypton: A functional approach to knowl-\nedge representation. Computer, 16(10), 67–73.\nBrachman, R. J. and Levesque, H. J. (Eds.). (1985).\nReadings in Knowledge Representation.\nMorgan\nKaufmann.\nBradtke, S. J. and Barto, A. G. (1996). Linear least-\nsquares algorithms for temporal difference learning.\nMachine Learning, 22, 33–57.\nBrafman, O. and Brafman, R. (2009). Sway: The\nIrresistible Pull of Irrational Behavior. Broadway\nBusiness.\nBrafman, R. I. and Domshlak, C. (2008). From one\nto many: Planning for loosely coupled multi-agent\nsystems. In ICAPS-08, pp. 28–35.\nBrafman, R. I. and Tennenholtz, M. (2000). A near\noptimal polynomial time algorithm for learning in\ncertain classes of stochastic games. AIJ, 121, 31–47.\nBraitenberg, V. (1984). Vehicles: Experiments in\nSynthetic Psychology. MIT Press.\nBransford, J. and Johnson, M. (1973). Considera-\ntion of some problems in comprehension. In Chase,\nW. G. (Ed.), Visual Information Processing. Aca-\ndemic Press.\nBrants, T., Popat, A. C., Xu, P., Och, F. J., and Dean,\nJ. (2007). Large language models in machine trans-\nlation. In EMNLP-CoNLL-2007: Proc. 2007 Joint\nConference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Lan-\nguage Learning, pp. 858–867.\nBratko, I. (1986). Prolog Programming for Artiﬁ-\ncial Intelligence (1st edition). Addison-Wesley.\nBratko, I. (2001). Prolog Programming for Artiﬁ-\ncial Intelligence (Third edition). Addison-Wesley.\nBratman, M. E. (1987). Intention, Plans, and Prac-\ntical Reason. Harvard University Press.\nBratman, M. E. (1992). Planning and the stability\nof intention. Minds and Machines, 2(1), 1–16.\nBreese, J. S. (1992). Construction of belief and de-\ncision networks. Computational Intelligence, 8(4),\n624–647.\nBreese, J. S. and Heckerman, D. (1996). Decision-",
  "of intention. Minds and Machines, 2(1), 1–16.\nBreese, J. S. (1992). Construction of belief and de-\ncision networks. Computational Intelligence, 8(4),\n624–647.\nBreese, J. S. and Heckerman, D. (1996). Decision-\ntheoretic troubleshooting: A framework for repair\nand experiment. In UAI-96, pp. 124–132.\nBreiman, L. (1996). Bagging predictors. Machine\nLearning, 24(2), 123–140.\nBreiman, L., Friedman, J., Olshen, R. A., and Stone,\nC. J. (1984). Classiﬁcation and Regression Trees.\nWadsworth International Group.\nBrelaz, D. (1979). New methods to color the vertices\nof a graph. CACM, 22(4), 251–256.\nBrent, R. P. (1973).\nAlgorithms for minimization\nwithout derivatives. Prentice-Hall.\nBresnan, J. (1982). The Mental Representation of\nGrammatical Relations. MIT Press.\nBrewka, G., Dix, J., and Konolige, K. (1997).\nNononotonic Reasoning: An Overview. CSLI Publi-\ncations.\nBrickley, D. and Guha, R. V. (2004). RDF vocab-\nulary description language 1.0: RDF schema. Tech.\nrep., W3C.\nBridle, J. S. (1990). Probabilistic interpretation of\nfeedforward classiﬁcation network outputs, with re-\nlationships to statistical pattern recognition. In Fo-\ngelman Souli´e, F. and H´erault, J. (Eds.), Neurocom-\nputing: Algorithms, Architectures and Applications.\nSpringer-Verlag.\nBriggs, R. (1985).\nKnowledge representation in\nSanskrit and artiﬁcial intelligence. AIMag, 6(1), 32–\n39.\nBrin, D. (1998). The Transparent Society. Perseus.\nBrin, S. (1999).\nExtracting patterns and relations\nfrom the world wide web. Technical report 1999-65,\nStanford InfoLab.\nBrin, S. and Page, L. (1998).\nThe anatomy of a\nlarge-scale hypertextual web search engine. In Proc.\nSeventh World Wide Web Conference.\nBringsjord, S. (2008). If I were judge. In Epstein,\nR., Roberts, G., and Beber, G. (Eds.), Parsing the\nTuring Test. Springer.\nBroadbent, D. E. (1958). Perception and Commu-\nnication. Pergamon.\nBrooks, R. A. (1986). A robust layered control sys-\ntem for a mobile robot. IEEE Journal of Robotics\nand Automation, 2, 14–23.\nBrooks, R. A. (1989).\nEngineering approach to\nbuilding complete, intelligent beings. Proc. SPIE—\nthe International Society for Optical Engineering,\n1002, 618–625.\nBrooks, R. A. (1991). Intelligence without represen-\ntation. AIJ, 47(1–3), 139–159.\nBrooks, R. A. and Lozano-Perez, T. (1985). A sub-\ndivision algorithm in conﬁguration space for ﬁnd-\npath with rotation. IEEE Transactions on Systems,\nMan and Cybernetics, 15(2), 224–233.\nBrown, C., Finkelstein, L., and Purdom, P. (1988).",
  "Brooks, R. A. and Lozano-Perez, T. (1985). A sub-\ndivision algorithm in conﬁguration space for ﬁnd-\npath with rotation. IEEE Transactions on Systems,\nMan and Cybernetics, 15(2), 224–233.\nBrown, C., Finkelstein, L., and Purdom, P. (1988).\nBacktrack searching in the presence of symmetry.\nIn Mora, T. (Ed.), Applied Algebra, Algebraic Al-\ngorithms and Error-Correcting Codes, pp. 99–110.\nSpringer-Verlag.\nBrown, K. C. (1974). A note on the apparent bias of\nnet revenue estimates. J. Finance, 29, 1215–1216.\nBrown, P. F., Cocke, J., Della Pietra, S. A.,\nDella Pietra, V. J., Jelinek, F., Mercer, R. L., and\nRoossin, P. (1988).\nA statistical approach to lan-\nguage translation. In COLING-88, pp. 71–76.\nBrown, P. F., Della Pietra, S. A., Della Pietra, V. J.,\nand Mercer, R. L. (1993). The mathematics of sta-\ntistical machine translation: Parameter estimation.\nComputational Linguistics, 19(2), 263–311.\nBrownston, L., Farrell, R., Kant, E., and Martin, N.\n(1985). Programming expert systems in OPS5: An\nintroduction to rule-based programming. Addison-\nWesley.\nBruce, V., Georgeson, M., and Green, P. (2003). Vi-\nsual Perception: Physiology, Psychology and Ecol-\nogy. Psychology Press.\nBruner, J. S., Goodnow, J. J., and Austin, G. A.\n(1957). A Study of Thinking. Wiley.\nBryant, B. D. and Miikkulainen, R. (2007). Acquir-\ning visibly intelligent behavior with example-guided\nneuroevolution. In AAAI-07.\nBryce, D. and Kambhampati, S. (2007).\nA tuto-\nrial on planning graph-based reachability heuristics.\nAIMag, Spring, 47–83.\nBryce, D., Kambhampati, S., and Smith, D. E.\n(2006). Planning graph heuristics for belief space\nsearch. JAIR, 26, 35–99.\nBryson, A. E. and Ho, Y.-C. (1969). Applied Opti-\nmal Control. Blaisdell.\nBuchanan, B. G. and Mitchell, T. M. (1978).\nModel-directed learning of production rules. In Wa-\nterman, D. A. and Hayes-Roth, F. (Eds.), Pattern-\nDirected Inference Systems, pp. 297–312. Academic\nPress.\nBuchanan, B. G., Mitchell, T. M., Smith, R. G., and\nJohnson, C. R. (1978). Models of learning systems.\nIn Encyclopedia of Computer Science and Technol-\nogy, Vol. 11. Dekker.\nBuchanan, B. G. and Shortliffe, E. H. (Eds.).\n(1984).\nRule-Based Expert Systems: The MYCIN\nExperiments of the Stanford Heuristic Programming\nProject. Addison-Wesley.\nBuchanan, B. G., Sutherland, G. L., and Feigen-\nbaum, E. A. (1969). Heuristic DENDRAL: A pro-\ngram for generating explanatory hypotheses in or-\nganic chemistry.\nIn Meltzer, B., Michie, D., and",
  "Project. Addison-Wesley.\nBuchanan, B. G., Sutherland, G. L., and Feigen-\nbaum, E. A. (1969). Heuristic DENDRAL: A pro-\ngram for generating explanatory hypotheses in or-\nganic chemistry.\nIn Meltzer, B., Michie, D., and\nSwann, M. (Eds.), Machine Intelligence 4, pp. 209–\n254. Edinburgh University Press.\nBuehler, M., Iagnemma, K., and Singh, S. (Eds.).\n(2006). The 2005 DARPA Grand Challenge: The\nGreat Robot Race. Springer-Verlag.\nBunt, H. C. (1985). The formal representation of\n(quasi-) continuous concepts. In Hobbs, J. R. and\nMoore, R. C. (Eds.), Formal Theories of the Com-\nmonsense World, chap. 2, pp. 37–70. Ablex.\nBurgard, W., Cremers, A. B., Fox, D., H¨ahnel, D.,\nLakemeyer, G., Schulz, D., Steiner, W., and Thrun,\nS. (1999). Experiences with an interactive museum\ntour-guide robot. AIJ, 114(1–2), 3–55.\nBuro, M. (1995). ProbCut: An effective selective\nextension of the alpha-beta algorithm.\nJ. Interna-\ntional Computer Chess Association, 18(2), 71–76.\nBuro, M. (2002).\nImproving heuristic mini-max\nsearch by supervised learning. AIJ, 134(1–2), 85–\n99.\nBurstein, J., Leacock, C., and Swartz, R. (2001).\nAutomated evaluation of essays and short answers.\nIn Fifth International Computer Assisted Assessment\n(CAA) Conference.\nBurton, R. (2009). On Being Certain: Believing You\nAre Right Even When You’re Not. St. Martin’s Grif-\nﬁn.\nBuss, D. M. (2005). Handbook of evolutionary psy-\nchology. Wiley.\nButler, S. (1863). Darwin among the machines. The\nPress (Christchurch, New Zealand), June 13.\nBylander, T. (1992). Complexity results for serial\ndecomposability. In AAAI-92, pp. 729–734.\nBylander, T. (1994). The computational complexity\nof propositional STRIPS planning.\nAIJ, 69, 165–\n204.\nByrd, R. H., Lu, P., Nocedal, J., and Zhu, C. (1995).\nA limited memory algorithm for bound constrained\noptimization. SIAM Journal on Scientiﬁc and Statis-\ntical Computing, 16(5), 1190–1208.\nCabeza, R. and Nyberg, L. (2001). Imaging cogni-\ntion II: An empirical review of 275 PET and fMRI\nstudies. J. Cognitive Neuroscience, 12, 1–47.\nCafarella, M. J., Halevy, A., Zhang, Y., Wang, D. Z.,\nand Wu, E. (2008). Webtables: Exploring the power\nof tables on the web. In VLDB-2008.\nCalvanese, D., Lenzerini, M., and Nardi, D.\n(1999).\nUnifying class-based representation for-\nmalisms. JAIR, 11, 199–240.\nCampbell, M. S., Hoane, A. J., and Hsu, F.-H.\n(2002). Deep Blue. AIJ, 134(1–2), 57–83. 1068\nBibliography\nCanny, J. and Reif, J. (1987).\nNew lower bound\ntechniques for robot motion planning problems. In",
  "malisms. JAIR, 11, 199–240.\nCampbell, M. S., Hoane, A. J., and Hsu, F.-H.\n(2002). Deep Blue. AIJ, 134(1–2), 57–83. 1068\nBibliography\nCanny, J. and Reif, J. (1987).\nNew lower bound\ntechniques for robot motion planning problems. In\nFOCS-87, pp. 39–48.\nCanny, J. (1986). A computational approach to edge\ndetection. PAMI, 8, 679–698.\nCanny, J. (1988). The Complexity of Robot Motion\nPlanning. MIT Press.\nCapen, E., Clapp, R., and Campbell, W. (1971).\nCompetitive bidding in high-risk situations.\nJ.\nPetroleum Technology, 23, 641–653.\nCaprara, A., Fischetti, M., and Toth, P. (1995). A\nheuristic method for the set covering problem. Op-\nerations Research, 47, 730–743.\nCarbonell, J. G. (1983). Derivational analogy and\nits role in problem solving. In AAAI-83, pp. 64–69.\nCarbonell, J. G., Knoblock, C. A., and Minton, S.\n(1989). PRODIGY: An integrated architecture for\nplanning and learning. Technical report CMU-CS-\n89-189, Computer Science Department, Carnegie-\nMellon University.\nCarbonell, J. R. and Collins, A. M. (1973). Natural\nsemantics in artiﬁcial intelligence. In IJCAI-73, pp.\n344–351.\nCardano, G. (1663). Liber de ludo aleae. Lyons.\nCarnap, R. (1928). Der logische Aufbau der Welt.\nWeltkreis-verlag. Translated into English as (Car-\nnap, 1967).\nCarnap, R. (1948). On the application of inductive\nlogic. Philosophy and Phenomenological Research,\n8, 133–148.\nCarnap, R. (1950). Logical Foundations of Proba-\nbility. University of Chicago Press.\nCarroll, S. (2007). The Making of the Fittest: DNA\nand the Ultimate Forensic Record of Evolution. Nor-\nton.\nCasati, R. and Varzi, A. (1999). Parts and places:\nthe structures of spatial representation. MIT Press.\nCassandra, A. R., Kaelbling, L. P., and Littman,\nM. L. (1994). Acting optimally in partially observ-\nable stochastic domains.\nIn AAAI-94, pp. 1023–\n1028.\nCassandras, C. G. and Lygeros, J. (2006). Stochas-\ntic Hybrid Systems. CRC Press.\nCastro, R., Coates, M., Liang, G., Nowak, R., and\nYu, B. (2004). Network tomography: Recent devel-\nopments. Statistical Science, 19(3), 499–517.\nCesa-Bianchi, N. and Lugosi, G. (2006). Prediction,\nlearning, and Games. Cambridge University Press.\nCesta, A., Cortellessa, G., Denis, M., Donati, A.,\nFratini, S., Oddi, A., Policella, N., Rabenau, E., and\nSchulster, J. (2007). MEXAR2: AI solves mission\nplanner problems. IEEE Intelligent Systems, 22(4),\n12–19.\nChakrabarti, P. P., Ghose, S., Acharya, A., and\nde Sarkar, S. C. (1989). Heuristic search in restricted\nmemory. AIJ, 41(2), 197–222.",
  "Schulster, J. (2007). MEXAR2: AI solves mission\nplanner problems. IEEE Intelligent Systems, 22(4),\n12–19.\nChakrabarti, P. P., Ghose, S., Acharya, A., and\nde Sarkar, S. C. (1989). Heuristic search in restricted\nmemory. AIJ, 41(2), 197–222.\nChandra, A. K. and Harel, D. (1980). Computable\nqueries for relational data bases. J. Computer and\nSystem Sciences, 21(2), 156–178.\nChang, C.-L. and Lee, R. C.-T. (1973). Symbolic\nLogic and Mechanical Theorem Proving. Academic\nPress.\nChapman, D. (1987).\nPlanning for conjunctive\ngoals. AIJ, 32(3), 333–377.\nCharniak, E. (1993). Statistical Language Learn-\ning. MIT Press.\nCharniak, E. (1996).\nTree-bank grammars.\nIn\nAAAI-96, pp. 1031–1036.\nCharniak, E. (1997).\nStatistical parsing with a\ncontext-free grammar and word statistics. In AAAI-\n97, pp. 598–603.\nCharniak, E. and Goldman, R. (1992). A Bayesian\nmodel of plan recognition. AIJ, 64(1), 53–79.\nCharniak, E. and McDermott, D. (1985). Introduc-\ntion to Artiﬁcial Intelligence. Addison-Wesley.\nCharniak, E., Riesbeck, C., McDermott, D., and\nMeehan, J. (1987). Artiﬁcial Intelligence Program-\nming (2nd edition). Lawrence Erlbaum Associates.\nCharniak, E. (1991).\nBayesian networks without\ntears. AIMag, 12(4), 50–63.\nCharniak, E. and Johnson, M. (2005).\nCoarse-\nto-ﬁne n-best parsing and maxent discriminative\nreranking. In ACL-05.\nChater, N. and Oaksford, M. (Eds.). (2008). The\nprobabilistic mind: Prospects for Bayesian cognitive\nscience. Oxford University Press.\nChatﬁeld, C. (1989). The Analysis of Time Series:\nAn Introduction (4th edition). Chapman and Hall.\nCheeseman, P. (1985). In defense of probability. In\nIJCAI-85, pp. 1002–1009.\nCheeseman, P. (1988). An inquiry into computer un-\nderstanding. Computational Intelligence, 4(1), 58–\n66.\nCheeseman, P., Kanefsky, B., and Taylor, W.\n(1991).\nWhere the really hard problems are.\nIn\nIJCAI-91, pp. 331–337.\nCheeseman, P., Self, M., Kelly, J., and Stutz, J.\n(1988). Bayesian classiﬁcation. In AAAI-88, Vol. 2,\npp. 607–611.\nCheeseman, P. and Stutz, J. (1996).\nBayesian\nclassiﬁcation (AutoClass): Theory and results. In\nFayyad, U., Piatesky-Shapiro, G., Smyth, P., and\nUthurusamy, R. (Eds.), Advances in Knowledge Dis-\ncovery and Data Mining. AAAI Press/MIT Press.\nChen, S. F. and Goodman, J. (1996). An empirical\nstudy of smoothing techniques for language model-\ning. In ACL-96, pp. 310–318.\nCheng, J. and Druzdzel, M. J. (2000). AIS-BN: An\nadaptive importance sampling algorithm for eviden-\ntial reasoning in large Bayesian networks. JAIR, 13,\n155–188.",
  "study of smoothing techniques for language model-\ning. In ACL-96, pp. 310–318.\nCheng, J. and Druzdzel, M. J. (2000). AIS-BN: An\nadaptive importance sampling algorithm for eviden-\ntial reasoning in large Bayesian networks. JAIR, 13,\n155–188.\nCheng, J., Greiner, R., Kelly, J., Bell, D. A., and\nLiu, W. (2002). Learning Bayesian networks from\ndata: An information-theory based approach. AIJ,\n137, 43–90.\nChklovski, T. and Gil, Y. (2005).\nImproving the\ndesign of intelligent acquisition interfaces for col-\nlecting world knowledge from web contributors. In\nProc. Third International Conference on Knowledge\nCapture (K-CAP).\nChomsky, N. (1956). Three models for the descrip-\ntion of language. IRE Transactions on Information\nTheory, 2(3), 113–124.\nChomsky, N. (1957). Syntactic Structures. Mouton.\nChoset, H. (1996). Sensor Based Motion Planning:\nThe Hierarchical Generalized Voronoi Graph. Ph.D.\nthesis, California Institute of Technology.\nChoset, H., Lynch, K., Hutchinson, S., Kantor, G.,\nBurgard, W., Kavraki, L., and Thrun, S. (2004).\nPrinciples of Robotic Motion: Theory, Algorithms,\nand Implementation. MIT Press.\nChung, K. L. (1979).\nElementary Probability\nTheory with Stochastic Processes (3rd edition).\nSpringer-Verlag.\nChurch, A. (1936).\nA note on the Entschei-\ndungsproblem. JSL, 1, 40–41 and 101–102.\nChurch, A. (1956). Introduction to Mathematical\nLogic. Princeton University Press.\nChurch, K. and Patil, R. (1982). Coping with syn-\ntactic ambiguity or how to put the block in the box on\nthe table. Computational Linguistics, 8(3–4), 139–\n149.\nChurch, K. (2004). Speech and language process-\ning: Can we use the past to predict the future. In\nProc. Conference on Text, Speech, and Dialogue.\nChurch, K. and Gale, W. A. (1991). A comparison\nof the enhanced Good–Turing and deleted estima-\ntion methods for estimating probabilities of English\nbigrams. Computer Speech and Language, 5, 19–54.\nChurchland, P. M. and Churchland, P. S. (1982).\nFunctionalism, qualia, and intentionality. In Biro,\nJ. I. and Shahan, R. W. (Eds.), Mind, Brain and\nFunction: Essays in the Philosophy of Mind, pp.\n121–145. University of Oklahoma Press.\nChurchland, P. S. (1986).\nNeurophilosophy:\nToward a Uniﬁed Science of the Mind–Brain.\nMIT Press.\nCiancarini, P. and Wooldridge, M. (2001). Agent-\nOriented Software Engineering. Springer-Verlag.\nCimatti, A., Roveri, M., and Traverso, P. (1998).\nAutomatic OBDD-based generation of universal\nplans in non-deterministic domains. In AAAI-98, pp.\n875–881.",
  "Oriented Software Engineering. Springer-Verlag.\nCimatti, A., Roveri, M., and Traverso, P. (1998).\nAutomatic OBDD-based generation of universal\nplans in non-deterministic domains. In AAAI-98, pp.\n875–881.\nClark, A. (1998). Being There: Putting Brain, Body,\nand World Together Again. MIT Press.\nClark, A. (2008). Supersizing the Mind: Embodi-\nment, Action, and Cognitive Extension. Oxford Uni-\nversity Press.\nClark, K. L. (1978). Negation as failure. In Gallaire,\nH. and Minker, J. (Eds.), Logic and Data Bases, pp.\n293–322. Plenum.\nClark, P. and Niblett, T. (1989). The CN2 induction\nalgorithm. Machine Learning, 3, 261–283.\nClark, S. and Curran, J. R. (2004). Parsing the WSJ\nusing CCG and log-linear models. In ACL-04, pp.\n104–111.\nClarke, A. C. (1968a).\n2001: A Space Odyssey.\nSignet.\nClarke, A. C. (1968b). The world of 2001. Vogue.\nClarke, E. and Grumberg, O. (1987). Research on\nautomatic veriﬁcation of ﬁnite-state concurrent sys-\ntems. Annual Review of Computer Science, 2, 269–\n290.\nClarke, M. R. B. (Ed.). (1977). Advances in Com-\nputer Chess 1. Edinburgh University Press.\nClearwater, S. H. (Ed.). (1996). Market-Based Con-\ntrol. World Scientiﬁc.\nClocksin, W. F. and Mellish, C. S. (2003). Program-\nming in Prolog (5th edition). Springer-Verlag.\nClocksin, W. F. (2003).\nClause and Effect: Pro-\nlog Programming for the Working Programmer.\nSpringer.\nCoarfa, C., Demopoulos, D., Aguirre, A., Subrama-\nnian, D., and Yardi, M. (2003). Random 3-SAT: The\nplot thickens. Constraints, 8(3), 243–261.\nCoates, A., Abbeel, P., and Ng, A. Y. (2009). Ap-\nprenticeship learning for helicopter control. JACM,\n52(7), 97–105.\nCobham, A. (1964).\nThe intrinsic computational\ndifﬁculty of functions. In Proc. 1964 International\nCongress for Logic, Methodology, and Philosophy of\nScience, pp. 24–30. Bibliography\n1069\nCohen, P. R. (1995). Empirical methods for artiﬁ-\ncial intelligence. MIT Press.\nCohen, P. R. and Levesque, H. J. (1990). Intention\nis choice with commitment. AIJ, 42(2–3), 213–261.\nCohen, P. R., Morgan, J., and Pollack, M. E. (1990).\nIntentions in Communication. MIT Press.\nCohen, W. W. and Page, C. D. (1995). Learnabil-\nity in inductive logic programming: Methods and\nresults. New Generation Computing, 13(3–4), 369–\n409.\nCohn, A. G., Bennett, B., Gooday, J. M., and Gotts,\nN. (1997). RCC: A calculus for region based qualita-\ntive spatial reasoning. GeoInformatica, 1, 275–316.\nCollin, Z., Dechter, R., and Katz, S. (1999).\nSelf-stabilizing distributed constraint satisfaction.",
  "N. (1997). RCC: A calculus for region based qualita-\ntive spatial reasoning. GeoInformatica, 1, 275–316.\nCollin, Z., Dechter, R., and Katz, S. (1999).\nSelf-stabilizing distributed constraint satisfaction.\nChicago Journal of Theoretical Computer Science,\n1999(115).\nCollins, F. S., Morgan, M., and Patrinos, A. (2003).\nThe human genome project: Lessons from large-\nscale biology. Science, 300(5617), 286–290.\nCollins, M. (1999). Head-driven Statistical Models\nfor Natural Language Processing. Ph.D. thesis, Uni-\nversity of Pennsylvania.\nCollins, M. and Duffy, K. (2002). New ranking algo-\nrithms for parsing and tagging: Kernels over discrete\nstructures, and the voted perceptron. In ACL-02.\nColmerauer, A. and Roussel, P. (1993). The birth\nof Prolog. SIGPLAN Notices, 28(3), 37–52.\nColmerauer, A. (1975). Les grammaires de meta-\nmorphose. Tech. rep., Groupe d’Intelligence Artiﬁ-\ncielle, Universit´e de Marseille-Luminy.\nColmerauer, A., Kanoui, H., Pasero, R., and\nRoussel, P. (1973).\nUn syst´eme de communi-\ncation homme–machine en Franc¸ais.\nRapport,\nGroupe d’Intelligence Artiﬁcielle, Universit´e d’Aix-\nMarseille II.\nCondon, J. H. and Thompson, K. (1982).\nBelle\nchess hardware. In Clarke, M. R. B. (Ed.), Advances\nin Computer Chess 3, pp. 45–54. Pergamon.\nCongdon, C. B., Huber, M., Kortenkamp, D., Bid-\nlack, C., Cohen, C., Huffman, S., Koss, F., Raschke,\nU., and Weymouth, T. (1992).\nCARMEL versus\nFlakey: A comparison of two robots. Tech. rep. Pa-\npers from the AAAI Robot Competition, RC-92-01,\nAmerican Association for Artiﬁcial Intelligence.\nConlisk, J. (1989). Three variants on the Allais ex-\nample. American Economic Review, 79(3), 392–407.\nConnell, J. (1989). A Colony Architecture for an Ar-\ntiﬁcial Creature. Ph.D. thesis, Artiﬁcial Intelligence\nLaboratory, MIT. Also available as AI Technical Re-\nport 1151.\nConsortium, T. G. O. (2008). The gene ontology\nproject in 2008. Nucleic Acids Research, 36.\nCook, S. A. (1971).\nThe complexity of theorem-\nproving procedures. In STOC-71, pp. 151–158.\nCook, S. A. and Mitchell, D. (1997). Finding hard\ninstances of the satisﬁability problem: A survey. In\nDu, D., Gu, J., and Pardalos, P. (Eds.), Satisﬁabil-\nity problems: Theory and applications. American\nMathematical Society.\nCooper, G. (1990). The computational complexity\nof probabilistic inference using Bayesian belief net-\nworks. AIJ, 42, 393–405.\nCooper, G. and Herskovits, E. (1992). A Bayesian\nmethod for the induction of probabilistic networks",
  "Mathematical Society.\nCooper, G. (1990). The computational complexity\nof probabilistic inference using Bayesian belief net-\nworks. AIJ, 42, 393–405.\nCooper, G. and Herskovits, E. (1992). A Bayesian\nmethod for the induction of probabilistic networks\nfrom data. Machine Learning, 9, 309–347.\nCopeland,\nJ. (1993).\nArtiﬁcial Intelligence:\nA Philosophical Introduction. Blackwell.\nCopernicus (1543).\nDe Revolutionibus Orbium\nCoelestium. Apud Ioh. Petreium, Nuremberg.\nCormen, T. H., Leiserson, C. E., and Rivest, R.\n(1990). Introduction to Algorithms. MIT Press.\nCortes, C. and Vapnik, V. N. (1995). Support vector\nnetworks. Machine Learning, 20, 273–297.\nCournot, A. (Ed.). (1838).\nRecherches sur les\nprincipes math´ematiques de la th´eorie des richesses.\nL. Hachette, Paris.\nCover, T. and Thomas, J. (2006). Elements of Infor-\nmation Theory (2nd edition). Wiley.\nCowan, J. D. and Sharp, D. H. (1988a). Neural nets.\nQuarterly Reviews of Biophysics, 21, 365–427.\nCowan, J. D. and Sharp, D. H. (1988b). Neural nets\nand artiﬁcial intelligence. Daedalus, 117, 85–121.\nCowell, R., Dawid, A. P., Lauritzen, S., and Spiegel-\nhalter, D. J. (2002). Probabilistic Networks and Ex-\npert Systems. Springer.\nCox, I. (1993). A review of statistical data associ-\nation techniques for motion correspondence. IJCV,\n10, 53–66.\nCox, I. and Hingorani, S. L. (1994). An efﬁcient im-\nplementation and evaluation of Reid’s multiple hy-\npothesis tracking algorithm for visual tracking. In\nICPR-94, Vol. 1, pp. 437–442.\nCox, I. and Wilfong, G. T. (Eds.). (1990).\nAu-\ntonomous Robot Vehicles. Springer Verlag.\nCox, R. T. (1946). Probability, frequency, and rea-\nsonable expectation. American Journal of Physics,\n14(1), 1–13.\nCraig, J. (1989). Introduction to Robotics: Mechan-\nics and Control (2nd edition). Addison-Wesley Pub-\nlishing, Inc.\nCraik, K. J. (1943).\nThe Nature of Explanation.\nCambridge University Press.\nCraswell, N., Zaragoza, H., and Robertson, S. E.\n(2005). Microsoft cambridge at trec-14: Enterprise\ntrack.\nIn Proc. Fourteenth Text REtrieval Confer-\nence.\nCrauser, A., Mehlhorn, K., Meyer, U., and Sanders,\nP. (1998).\nA parallelization of Dijkstra’s shortest\npath algorithm.\nIn Proc. 23rd International Sym-\nposium on Mathematical Foundations of Computer\nScience,, pp. 722–731.\nCraven, M., DiPasquo, D., Freitag, D., McCallum,\nA., Mitchell, T. M., Nigam, K., and Slattery, S.\n(2000). Learning to construct knowledge bases from\nthe World Wide Web. AIJ, 118(1/2), 69–113.",
  "Science,, pp. 722–731.\nCraven, M., DiPasquo, D., Freitag, D., McCallum,\nA., Mitchell, T. M., Nigam, K., and Slattery, S.\n(2000). Learning to construct knowledge bases from\nthe World Wide Web. AIJ, 118(1/2), 69–113.\nCrawford, J. M. and Auton, L. D. (1993). Experi-\nmental results on the crossover point in satisﬁability\nproblems. In AAAI-93, pp. 21–27.\nCristianini, N. and Hahn, M. (2007). Introduction\nto Computational Genomics: A Case Studies Ap-\nproach. Cambridge University Press.\nCristianini, N. and Sch¨olkopf, B. (2002). Support\nvector machines and kernel methods: The new gen-\neration of learning machines. AIMag, 23(3), 31–41.\nCristianini, N. and Shawe-Taylor, J. (2000).\nAn\nintroduction to support vector machines and other\nkernel-based learning methods. Cambridge Univer-\nsity Press.\nCrockett, L. (1994). The Turing Test and the Frame\nProblem: AI’s Mistaken Understanding of Intelli-\ngence. Ablex.\nCroft, B., Metzler, D., and Stroham, T. (2009).\nSearch Engines: Information retrieval in Practice.\nAddison Wesley.\nCross, S. E. and Walker, E. (1994). DART: Apply-\ning knowledge based planning and scheduling to cri-\nsis action planning. In Zweben, M. and Fox, M. S.\n(Eds.), Intelligent Scheduling, pp. 711–729. Morgan\nKaufmann.\nCruse, D. A. (1986). Lexical Semantics. Cambridge\nUniversity Press.\nCulberson, J. and Schaeffer, J. (1996). Searching\nwith pattern databases.\nIn Advances in Artiﬁcial\nIntelligence (Lecture Notes in Artiﬁcial Intelligence\n1081), pp. 402–416. Springer-Verlag.\nCulberson, J. and Schaeffer, J. (1998).\nPattern\ndatabases. Computational Intelligence, 14(4), 318–\n334.\nCullingford, R. E. (1981).\nIntegrating knowl-\nedge sources for computer “understanding” tasks.\nIEEE Transactions on Systems, Man and Cybernet-\nics (SMC), 11.\nCummins, D. and Allen, C. (1998). The Evolution\nof Mind. Oxford University Press.\nCushing, W., Kambhampati, S., Mausam, and Weld,\nD. S. (2007). When is temporal planning really tem-\nporal? In IJCAI-07.\nCybenko, G. (1988). Continuous valued neural net-\nworks with two hidden layers are sufﬁcient. Techni-\ncal report, Department of Computer Science, Tufts\nUniversity.\nCybenko, G. (1989). Approximation by superposi-\ntions of a sigmoidal function. Mathematics of Con-\ntrols, Signals, and Systems, 2, 303–314.\nDaganzo, C. (1979). Multinomial probit: The theory\nand its application to demand forecasting. Academic\nPress.\nDagum, P. and Luby, M. (1993).\nApproximating\nprobabilistic inference in Bayesian belief networks",
  "trols, Signals, and Systems, 2, 303–314.\nDaganzo, C. (1979). Multinomial probit: The theory\nand its application to demand forecasting. Academic\nPress.\nDagum, P. and Luby, M. (1993).\nApproximating\nprobabilistic inference in Bayesian belief networks\nis NP-hard. AIJ, 60(1), 141–153.\nDalal, N. and Triggs, B. (2005). Histograms of ori-\nented gradients for human detection. In CVPR, pp.\n886–893.\nDantzig, G. B. (1949). Programming of interdepen-\ndent activities: II. Mathematical model. Economet-\nrica, 17, 200–211.\nDarwiche, A. (2001). Recursive conditioning. AIJ,\n126, 5–41.\nDarwiche, A. and Ginsberg, M. L. (1992). A sym-\nbolic generalization of probability theory. In AAAI-\n92, pp. 622–627.\nDarwiche, A. (2009). Modeling and reasoning with\nBayesian networks. Cambridge University Press.\nDarwin, C. (1859). On The Origin of Species by\nMeans of Natural Selection. J. Murray, London.\nDarwin, C. (1871). Descent of Man. J. Murray.\nDasgupta, P., Chakrabarti, P. P., and de Sarkar, S. C.\n(1994). Agent searching in a tree and the optimality\nof iterative deepening. AIJ, 71, 195–208.\nDavidson, D. (1980). Essays on Actions and Events.\nOxford University Press.\nDavies, T. R. (1985). Analogy. Informal note IN-\nCSLI-85-4, Center for the Study of Language and\nInformation (CSLI).\nDavies, T. R. and Russell, S. J. (1987). A logical ap-\nproach to reasoning by analogy. In IJCAI-87, Vol. 1,\npp. 264–270.\nDavis, E. (1986). Representing and Acquiring Geo-\ngraphic Knowledge. Pitman and Morgan Kaufmann.\nDavis, E. (1990). Representations of Commonsense\nKnowledge. Morgan Kaufmann. 1070\nBibliography\nDavis, E. (2005). Knowledge and communication:\nA ﬁrst-order theory. AIJ, 166, 81–140.\nDavis, E. (2006). The expressivity of quantifying\nover regions. J. Logic and Computation, 16, 891–\n916.\nDavis, E. (2007). Physical reasoning. In van Harme-\nlan, F., Lifschitz, V., and Porter, B. (Eds.), The Hand-\nbook of Knowledge Representation, pp. 597–620. El-\nsevier.\nDavis, E. (2008). Pouring liquids: A study in com-\nmonsense physical reasoning. AIJ, 172(1540–1578).\nDavis, E. and Morgenstern, L. (2004). Introduction:\nProgress in formal commonsense reasoning.\nAIJ,\n153, 1–12.\nDavis, E. and Morgenstern, L. (2005). A ﬁrst-order\ntheory of communication and multi-agent plans. J.\nLogic and Computation, 15(5), 701–749.\nDavis, K. H., Biddulph, R., and Balashek, S. (1952).\nAutomatic recognition of spoken digits. J. Acousti-\ncal Society of America, 24(6), 637–642.\nDavis, M. (1957). A computer program for Pres-",
  "Logic and Computation, 15(5), 701–749.\nDavis, K. H., Biddulph, R., and Balashek, S. (1952).\nAutomatic recognition of spoken digits. J. Acousti-\ncal Society of America, 24(6), 637–642.\nDavis, M. (1957). A computer program for Pres-\nburger’s algorithm. In Proving Theorems (as Done\nby Man, Logician, or Machine), pp. 215–233. Proc.\nSummer Institute for Symbolic Logic. Second edi-\ntion; publication date is 1960.\nDavis, M., Logemann, G., and Loveland, D. (1962).\nA machine program for theorem-proving. CACM, 5,\n394–397.\nDavis, M. and Putnam, H. (1960). A computing pro-\ncedure for quantiﬁcation theory. JACM, 7(3), 201–\n215.\nDavis, R. and Lenat, D. B. (1982).\nKnowledge-\nBased Systems in Artiﬁcial Intelligence. McGraw-\nHill.\nDayan, P. (1992). The convergence of TD(λ) for\ngeneral λ. Machine Learning, 8(3–4), 341–362.\nDayan, P. and Abbott, L. F. (2001). Theoretical Neu-\nroscience: Computational and Mathematical Mod-\neling of Neural Systems. MIT Press.\nDayan, P. and Niv, Y. (2008). Reinforcement learn-\ning and the brain: The good, the bad and the ugly.\nCurrent Opinion in Neurobiology, 18(2), 185–196.\nde Dombal, F. T., Leaper, D. J., Horrocks, J. C.,\nand Staniland, J. R. (1974). Human and computer-\naided diagnosis of abdominal pain: Further report\nwith emphasis on performance of clinicians. British\nMedical Journal, 1, 376–380.\nde Dombal, F. T., Staniland, J. R., and Clamp, S. E.\n(1981). Geographical variation in disease presenta-\ntion. Medical Decision Making, 1, 59–69.\nde Finetti, B. (1937).\nLe pr´evision:\nses lois\nlogiques,\nses sources subjectives.\nAnn. Inst.\nPoincar´e, 7, 1–68.\nde Finetti, B. (1993). On the subjective meaning\nof probability. In Monari, P. and Cocchi, D. (Eds.),\nProbabilita e Induzione, pp. 291–321. Clueb.\nde Freitas, J. F. G., Niranjan, M., and Gee, A. H.\n(2000).\nSequential Monte Carlo methods to train\nneural network models. Neural Computation, 12(4),\n933–953.\nde Kleer, J. (1975).\nQualitative and quantitative\nknowledge in classical mechanics.\nTech. rep. AI-\nTR-352, MIT Artiﬁcial Intelligence Laboratory.\nde Kleer, J. (1989). A comparison of ATMS and\nCSP techniques. In IJCAI-89, Vol. 1, pp. 290–296.\nde Kleer, J. and Brown, J. S. (1985). A qualitative\nphysics based on conﬂuences. In Hobbs, J. R. and\nMoore, R. C. (Eds.), Formal Theories of the Com-\nmonsense World, chap. 4, pp. 109–183. Ablex.\nde Marcken, C. (1996).\nUnsupervised Language\nAcquisition. Ph.D. thesis, MIT.\nDe Morgan, A. (1864). On the syllogism, No. IV,\nand on the logic of relations.",
  "Moore, R. C. (Eds.), Formal Theories of the Com-\nmonsense World, chap. 4, pp. 109–183. Ablex.\nde Marcken, C. (1996).\nUnsupervised Language\nAcquisition. Ph.D. thesis, MIT.\nDe Morgan, A. (1864). On the syllogism, No. IV,\nand on the logic of relations.\nTransaction of the\nCambridge Philosophical Society, X, 331–358.\nDe Raedt, L. (1992). Interactive Theory Revision:\nAn Inductive Logic Programming Approach. Aca-\ndemic Press.\nde Salvo Braz, R., Amir, E., and Roth, D. (2007).\nLifted ﬁrst-order probabilistic inference. In Getoor,\nL. and Taskar, B. (Eds.), Introduction to Statistical\nRelational Learning. MIT Press.\nDeacon, T. W. (1997). The symbolic species: The\nco-evolution of language and the brain. W. W. Nor-\nton.\nDeale, M., Yvanovich, M., Schnitzius, D., Kautz,\nD., Carpenter, M., Zweben, M., Davis, G., and Daun,\nB. (1994).\nThe space shuttle ground processing\nscheduling system.\nIn Zweben, M. and Fox, M.\n(Eds.), Intelligent Scheduling, pp. 423–449. Morgan\nKaufmann.\nDean, T., Basye, K., Chekaluk, R., and Hyun, S.\n(1990). Coping with uncertainty in a control system\nfor navigation and exploration. In AAAI-90, Vol. 2,\npp. 1010–1015.\nDean, T. and Boddy, M. (1988). An analysis of time-\ndependent planning. In AAAI-88, pp. 49–54.\nDean, T., Firby, R. J., and Miller, D. (1990). Hierar-\nchical planning involving deadlines, travel time, and\nresources. Computational Intelligence, 6(1), 381–\n398.\nDean, T., Kaelbling, L. P., Kirman, J., and Nichol-\nson, A. (1993). Planning with deadlines in stochastic\ndomains. In AAAI-93, pp. 574–579.\nDean, T. and Kanazawa, K. (1989a). A model for\nprojection and action. In IJCAI-89, pp. 985–990.\nDean, T. and Kanazawa, K. (1989b). A model for\nreasoning about persistence and causation. Compu-\ntational Intelligence, 5(3), 142–150.\nDean, T., Kanazawa, K., and Shewchuk, J. (1990).\nPrediction, observation and estimation in planning\nand control. In 5th IEEE International Symposium\non Intelligent Control, Vol. 2, pp. 645–650.\nDean, T. and Wellman, M. P. (1991). Planning and\nControl. Morgan Kaufmann.\nDearden, R., Friedman, N., and Andre, D. (1999).\nModel-based Bayesian exploration. In UAI-99.\nDearden, R., Friedman, N., and Russell, S. J.\n(1998). Bayesian q-learning. In AAAI-98.\nDebevec, P., Taylor, C., and Malik, J. (1996). Mod-\neling and rendering architecture from photographs:\nA hybrid geometry- and image-based approach. In\nProc. 23rd Annual Conference on Computer Graph-\nics (SIGGRAPH), pp. 11–20.\nDebreu, G. (1960). Topological methods in cardinal",
  "eling and rendering architecture from photographs:\nA hybrid geometry- and image-based approach. In\nProc. 23rd Annual Conference on Computer Graph-\nics (SIGGRAPH), pp. 11–20.\nDebreu, G. (1960). Topological methods in cardinal\nutility theory. In Arrow, K. J., Karlin, S., and Sup-\npes, P. (Eds.), Mathematical Methods in the Social\nSciences, 1959. Stanford University Press.\nDechter, R. (1990a). Enhancement schemes for con-\nstraint processing: Backjumping, learning and cutset\ndecomposition. AIJ, 41, 273–312.\nDechter, R. (1990b). On the expressiveness of net-\nworks with hidden variables. In AAAI-90, pp. 379–\n385.\nDechter, R. (1992).\nConstraint networks.\nIn\nShapiro, S. (Ed.), Encyclopedia of Artiﬁcial Intelli-\ngence (2nd edition)., pp. 276–285. Wiley and Sons.\nDechter, R. (1999). Bucket elimination: A unifying\nframework for reasoning. AIJ, 113, 41–85.\nDechter, R. and Pearl, J. (1985).\nGeneralized\nbest-ﬁrst search strategies and the optimality of A*.\nJACM, 32(3), 505–536.\nDechter, R. and Pearl, J. (1987).\nNetwork-based\nheuristics for constraint-satisfaction problems. AIJ,\n34(1), 1–38.\nDechter, R. and Pearl, J. (1989). Tree clustering for\nconstraint networks. AIJ, 38(3), 353–366.\nDechter, R. (2003). Constraint Processing. Morgan\nKaufmann.\nDechter, R. and Frost, D. (2002). Backjump-based\nbacktracking for constraint satisfaction problems.\nAIJ, 136(2), 147–188.\nDechter, R. and Mateescu, R. (2007).\nAND/OR\nsearch spaces for graphical models. AIJ, 171(2–3),\n73–106.\nDeCoste, D. and Sch¨olkopf, B. (2002). Training in-\nvariant support vector machines. Machine Learning,\n46(1), 161–190.\nDedekind, R. (1888). Was sind und was sollen die\nZahlen. Braunschweig, Germany.\nDeerwester, S. C., Dumais, S. T., Landauer, T. K.,\nFurnas, G. W., and Harshman, R. A. (1990). Index-\ning by latent semantic analysis. J. American Society\nfor Information Science, 41(6), 391–407.\nDeGroot, M. H. (1970). Optimal Statistical Deci-\nsions. McGraw-Hill.\nDeGroot, M. H. and Schervish, M. J. (2001). Prob-\nability and Statistics (3rd edition). Addison Wesley.\nDeJong, G. (1981). Generalizations based on expla-\nnations. In IJCAI-81, pp. 67–69.\nDeJong, G. (1982).\nAn overview of the FRUMP\nsystem. In Lehnert, W. and Ringle, M. (Eds.), Strate-\ngies for Natural Language Processing, pp. 149–176.\nLawrence Erlbaum.\nDeJong, G. and Mooney, R. (1986). Explanation-\nbased learning: An alternative view. Machine Learn-\ning, 1, 145–176.\nDel Moral, P., Doucet, A., and Jasra, A. (2006). Se-",
  "gies for Natural Language Processing, pp. 149–176.\nLawrence Erlbaum.\nDeJong, G. and Mooney, R. (1986). Explanation-\nbased learning: An alternative view. Machine Learn-\ning, 1, 145–176.\nDel Moral, P., Doucet, A., and Jasra, A. (2006). Se-\nquential Monte Carlo samplers. J. Royal Statistical\nSociety, Series B, 68(3), 411–436.\nDel Moral, P. (2004). Feynman–Kac Formulae, Ge-\nnealogical and Interacting Particle Systems with Ap-\nplications. Springer-Verlag.\nDelgrande, J. and Schaub, T. (2003). On the relation\nbetween Reiter’s default logic and its (major) vari-\nants. In Seventh European Conference on Symbolic\nand Quantitative Approaches to Reasoning with Un-\ncertainty, pp. 452–463.\nDempster, A. P. (1968).\nA generalization of\nBayesian inference.\nJ. Royal Statistical Society,\n30 (Series B), 205–247.\nDempster, A. P., Laird, N., and Rubin, D. (1977).\nMaximum likelihood from incomplete data via the\nEM algorithm. J. Royal Statistical Society, 39 (Se-\nries B), 1–38.\nDeng, X. and Papadimitriou, C. H. (1990). Explor-\ning an unknown graph. In FOCS-90, pp. 355–361.\nDenis, F. (2001).\nLearning regular languages\nfrom simple positive examples. Machine Learning,\n44(1/2), 37–66.\nDennett, D. C. (1984). Cognitive wheels: the frame\nproblem of AI. In Hookway, C. (Ed.), Minds, Ma-\nchines, and Evolution: Philosophical Studies, pp.\n129–151. Cambridge University Press.\nDennett, D. C. (1991). Consciousness Explained.\nPenguin Press. Bibliography\n1071\nDenney, E., Fischer, B., and Schumann, J. (2006).\nAn empirical evaluation of automated theorem\nprovers in software certiﬁcation.\nInt. J. AI Tools,\n15(1), 81–107.\nDescartes, R. (1637). Discourse on method. In Cot-\ntingham, J., Stoothoff, R., and Murdoch, D. (Eds.),\nThe Philosophical Writings of Descartes, Vol. I.\nCambridge University Press, Cambridge, UK.\nDescartes, R. (1641). Meditations on ﬁrst philoso-\nphy. In Cottingham, J., Stoothoff, R., and Murdoch,\nD. (Eds.), The Philosophical Writings of Descartes,\nVol. II. Cambridge University Press, Cambridge,\nUK.\nDescotte, Y. and Latombe, J.-C. (1985).\nMak-\ning compromises among antagonist constraints in a\nplanner. AIJ, 27, 183–217.\nDetwarasiti, A. and Shachter, R. D. (2005). Inﬂu-\nence diagrams for team decision analysis. Decision\nAnalysis, 2(4), 207–228.\nDevroye, L. (1987). A course in density estimation.\nBirkhauser.\nDickmanns, E. D. and Zapp, A. (1987).\nAu-\ntonomous high speed road vehicle guidance by\ncomputer vision.\nIn Automatic Control—World\nCongress, 1987: Selected Papers from the 10th Tri-",
  "Devroye, L. (1987). A course in density estimation.\nBirkhauser.\nDickmanns, E. D. and Zapp, A. (1987).\nAu-\ntonomous high speed road vehicle guidance by\ncomputer vision.\nIn Automatic Control—World\nCongress, 1987: Selected Papers from the 10th Tri-\nennial World Congress of the International Federa-\ntion of Automatic Control, pp. 221–226.\nDietterich, T. (1990). Machine learning.\nAnnual\nReview of Computer Science, 4, 255–306.\nDietterich, T. (2000).\nHierarchical reinforcement\nlearning with the MAXQ value function decompo-\nsition. JAIR, 13, 227–303.\nDijkstra, E. W. (1959). A note on two problems in\nconnexion with graphs. Numerische Mathematik, 1,\n269–271.\nDijkstra, E. W. (1984). The threats to computing\nscience. In ACM South Central Regional Confer-\nence.\nDillenburg, J. F. and Nelson, P. C. (1994). Perimeter\nsearch. AIJ, 65(1), 165–178.\nDinh, H., Russell, A., and Su, Y. (2007). On the\nvalue of good advice: The complexity of A* with\naccurate heuristics. In AAAI-07.\nDissanayake, G., Newman, P., Clark, S., Durrant-\nWhyte, H., and Csorba, M. (2001). A solution to the\nsimultaneous localisation and map building (SLAM)\nproblem. IEEE Transactions on Robotics and Au-\ntomation, 17(3), 229–241.\nDo, M. B. and Kambhampati, S. (2001). Sapa: A\ndomain-independent heuristic metric temporal plan-\nner. In ECP-01.\nDo, M. B. and Kambhampati, S. (2003). Planning as\nconstraint satisfaction: solving the planning graph\nby compiling it into CSP. AIJ, 132(2), 151–182.\nDoctorow, C. (2001). Metacrap: Putting the torch to\nseven straw-men of the meta-utopia. www.well.\ncom/˜doctorow/metacrap.htm.\nDomingos, P. and Pazzani, M. (1997). On the opti-\nmality of the simple Bayesian classiﬁer under zero–\none loss. Machine Learning, 29, 103–30.\nDomingos, P. and Richardson, M. (2004). Markov\nlogic: A unifying framework for statistical relational\nlearning. In Proc. ICML-04 Workshop on Statistical\nRelational Learning.\nDonninger, C. and Lorenz, U. (2004). The chess\nmonster hydra.\nIn Proc. 14th International Con-\nference on Field-Programmable Logic and Applica-\ntions, pp. 927–932.\nDoorenbos, R. (1994). Combining left and right un-\nlinking for matching a large number of learned rules.\nIn AAAI-94.\nDoran, J. and Michie, D. (1966). Experiments with\nthe graph traverser program. Proc. Royal Society of\nLondon, 294, Series A, 235–259.\nDorf, R. C. and Bishop, R. H. (2004). Modern Con-\ntrol Systems (10th edition). Prentice-Hall.\nDoucet, A. (1997).\nMonte Carlo methods for",
  "the graph traverser program. Proc. Royal Society of\nLondon, 294, Series A, 235–259.\nDorf, R. C. and Bishop, R. H. (2004). Modern Con-\ntrol Systems (10th edition). Prentice-Hall.\nDoucet, A. (1997).\nMonte Carlo methods for\nBayesian estimation of hidden Markov models: Ap-\nplication to radiation signals. Ph.D. thesis, Univer-\nsit´e de Paris-Sud.\nDoucet,\nA.,\nde Freitas,\nN.,\nand Gordon,\nN.\n(2001). Sequential Monte Carlo Methods in Prac-\ntice. Springer-Verlag.\nDoucet, A., de Freitas, N., Murphy, K., and Russell,\nS. J. (2000). Rao-blackwellised particle ﬁltering for\ndynamic bayesian networks. In UAI-00.\nDowling, W. F. and Gallier, J. H. (1984). Linear-\ntime algorithms for testing the satisﬁability of propo-\nsitional Horn formulas. J. Logic Programming, 1,\n267–284.\nDowty, D., Wall, R., and Peters, S. (1991). Intro-\nduction to Montague Semantics. D. Reidel.\nDoyle, J. (1979). A truth maintenance system. AIJ,\n12(3), 231–272.\nDoyle, J. (1983). What is rational psychology? To-\nward a modern mental philosophy. AIMag, 4(3), 50–\n53.\nDoyle, J. and Patil, R. (1991). Two theses of knowl-\nedge representation:\nLanguage restrictions, taxo-\nnomic classiﬁcation, and the utility of representation\nservices. AIJ, 48(3), 261–297.\nDrabble, B. (1990). Mission scheduling for space-\ncraft: Diaries of T-SCHED. In Expert Planning Sys-\ntems, pp. 76–81. Institute of Electrical Engineers.\nDredze, M., Crammer, K., and Pereira, F. (2008).\nConﬁdence-weighted linear classiﬁcation. In ICML-\n08, pp. 264–271.\nDreyfus, H. L. (1972). What Computers Can’t Do:\nA Critique of Artiﬁcial Reason. Harper and Row.\nDreyfus, H. L. (1992). What Computers Still Can’t\nDo: A Critique of Artiﬁcial Reason. MIT Press.\nDreyfus, H. L. and Dreyfus, S. E. (1986). Mind over\nMachine: The Power of Human Intuition and Exper-\ntise in the Era of the Computer. Blackwell.\nDreyfus, S. E. (1969).\nAn appraisal of some\nshortest-paths algorithms. Operations Research, 17,\n395–412.\nDubois, D. and Prade, H. (1994). A survey of belief\nrevision and updating rules in various uncertainty\nmodels. Int. J. Intelligent Systems, 9(1), 61–100.\nDuda, R. O., Gaschnig, J., and Hart, P. E. (1979).\nModel design in the Prospector consultant system\nfor mineral exploration.\nIn Michie, D. (Ed.), Ex-\npert Systems in the Microelectronic Age, pp. 153–\n167. Edinburgh University Press.\nDuda, R. O. and Hart, P. E. (1973). Pattern classiﬁ-\ncation and scene analysis. Wiley.\nDuda, R. O., Hart, P. E., and Stork, D. G. (2001).",
  "In Michie, D. (Ed.), Ex-\npert Systems in the Microelectronic Age, pp. 153–\n167. Edinburgh University Press.\nDuda, R. O. and Hart, P. E. (1973). Pattern classiﬁ-\ncation and scene analysis. Wiley.\nDuda, R. O., Hart, P. E., and Stork, D. G. (2001).\nPattern Classiﬁcation (2nd edition). Wiley.\nDudek, G. and Jenkin, M. (2000). Computational\nPrinciples of Mobile Robotics. Cambridge Univer-\nsity Press.\nDuffy, D. (1991). Principles of Automated Theorem\nProving. John Wiley & Sons.\nDunn, H. L. (1946). Record linkage”. Am. J. Public\nHealth, 36(12), 1412–1416.\nDurfee, E. H. and Lesser, V. R. (1989). Negotiat-\ning task decomposition and allocation using partial\nglobal planning. In Huhns, M. and Gasser, L. (Eds.),\nDistributed AI, Vol. 2. Morgan Kaufmann.\nDurme, B. V. and Pasca, M. (2008). Finding cars,\ngoddesses and enzymes: Parametrizable acquisition\nof labeled instances for open-domain information\nextraction. In AAAI-08, pp. 1243–1248.\nDyer,\nM. (1983).\nIn-Depth\nUnderstanding.\nMIT Press.\nDyson, G. (1998). Darwin among the machines : the\nevolution of global intelligence. Perseus Books.\nDuzeroski, S., Muggleton, S. H., and Russell, S. J.\n(1992). PAC-learnability of determinate logic pro-\ngrams. In COLT-92, pp. 128–135.\nEarley, J. (1970). An efﬁcient context-free parsing\nalgorithm. CACM, 13(2), 94–102.\nEdelkamp, S. (2009). Scaling search with symbolic\npattern databases. In Model Checking and Artiﬁcial\nIntelligence (MOCHART), pp. 49–65.\nEdmonds, J. (1965).\nPaths, trees, and ﬂowers.\nCanadian Journal of Mathematics, 17, 449–467.\nEdwards, P. (Ed.). (1967). The Encyclopedia of Phi-\nlosophy. Macmillan.\nEen, N. and S¨orensson, N. (2003).\nAn extensi-\nble SAT-solver. In Giunchiglia, E. and Tacchella,\nA. (Eds.), Theory and Applications of Satisﬁability\nTesting: 6th International Conference (SAT 2003).\nSpringer-Verlag.\nEiter, T., Leone, N., Mateis, C., Pfeifer, G., and\nScarcello, F. (1998). The KR system dlv: Progress\nreport, comparisons and benchmarks. In KR-98, pp.\n406–417.\nElio, R. (Ed.). (2002). Common Sense, Reasoning,\nand Rationality. Oxford University Press.\nElkan, C. (1993). The paradoxical success of fuzzy\nlogic. In AAAI-93, pp. 698–703.\nElkan, C. (1997).\nBoosting and naive Bayesian\nlearning. Tech. rep., Department of Computer Sci-\nence and Engineering, University of California, San\nDiego.\nEllsberg, D. (1962). Risk, Ambiguity, and Decision.\nPh.D. thesis, Harvard University.\nElman, J., Bates, E., Johnson, M., Karmiloff-Smith,",
  "learning. Tech. rep., Department of Computer Sci-\nence and Engineering, University of California, San\nDiego.\nEllsberg, D. (1962). Risk, Ambiguity, and Decision.\nPh.D. thesis, Harvard University.\nElman, J., Bates, E., Johnson, M., Karmiloff-Smith,\nA., Parisi, D., and Plunkett, K. (1997). Rethinking\nInnateness. MIT Press.\nEmpson, W. (1953). Seven Types of Ambiguity. New\nDirections.\nEnderton, H. B. (1972). A Mathematical Introduc-\ntion to Logic. Academic Press.\nEpstein, R., Roberts, G., and Beber, G. (Eds.).\n(2008). Parsing the Turing Test. Springer.\nErdmann, M. A. and Mason, M. (1988). An explo-\nration of sensorless manipulation. IEEE Journal of\nRobotics and Automation, 4(4), 369–379.\nErnst, H. A. (1961). MH-1, a Computer-Operated\nMechanical Hand. Ph.D. thesis, Massachusetts In-\nstitute of Technology.\nErnst, M., Millstein, T., and Weld, D. S. (1997). Au-\ntomatic SAT-compilation of planning problems. In\nIJCAI-97, pp. 1169–1176.\nErol, K., Hendler, J., and Nau, D. S. (1994). HTN\nplanning: Complexity and expressivity. In AAAI-94,\npp. 1123–1128. 1072\nBibliography\nErol, K., Hendler, J., and Nau, D. S. (1996). Com-\nplexity results for HTN planning. AIJ, 18(1), 69–93.\nEtzioni, A. (2004). From Empire to Community: A\nNew Approach to International Relation. Palgrave\nMacmillan.\nEtzioni, O. (1989). Tractable decision-analytic con-\ntrol.\nIn Proc. First International Conference on\nKnowledge Representation and Reasoning, pp. 114–\n125.\nEtzioni, O., Banko, M., Soderland, S., and Weld,\nD. S. (2008). Open information extraction from the\nweb. CACM, 51(12).\nEtzioni, O., Hanks, S., Weld, D. S., Draper, D.,\nLesh, N., and Williamson, M. (1992). An approach\nto planning with incomplete information. In KR-92.\nEtzioni, O. and Weld, D. S. (1994). A softbot-based\ninterface to the Internet. CACM, 37(7), 72–76.\nEtzioni, O., Banko, M., and Cafarella, M. J. (2006).\nMachine reading. In AAAI-06.\nEtzioni, O., Cafarella, M. J., Downey, D., Popescu,\nA.-M., Shaked, T., Soderland, S., Weld, D. S., and\nYates, A. (2005).\nUnsupervised named-entity ex-\ntraction from the web: An experimental study. AIJ,\n165(1), 91–134.\nEvans, T. G. (1968). A program for the solution of\na class of geometric-analogy intelligence-test ques-\ntions. In Minsky, M. L. (Ed.), Semantic Information\nProcessing, pp. 271–353. MIT Press.\nFagin, R., Halpern, J. Y., Moses, Y., and Vardi, M. Y.\n(1995). Reasoning about Knowledge. MIT Press.\nFahlman, S. E. (1974). A planning system for robot\nconstruction tasks. AIJ, 5(1), 1–49.",
  "Processing, pp. 271–353. MIT Press.\nFagin, R., Halpern, J. Y., Moses, Y., and Vardi, M. Y.\n(1995). Reasoning about Knowledge. MIT Press.\nFahlman, S. E. (1974). A planning system for robot\nconstruction tasks. AIJ, 5(1), 1–49.\nFaugeras, O. (1993). Three-Dimensional Computer\nVision: A Geometric Viewpoint. MIT Press.\nFaugeras, O., Luong, Q.-T., and Papadopoulo, T.\n(2001).\nThe Geometry of Multiple Images.\nMIT\nPress.\nFearing, R. S. and Hollerbach, J. M. (1985). Basic\nsolid mechanics for tactile sensing. Int. J. Robotics\nResearch, 4(3), 40–54.\nFeatherstone, R. (1987).\nRobot Dynamics Algo-\nrithms. Kluwer Academic Publishers.\nFeigenbaum, E. A. (1961). The simulation of ver-\nbal learning behavior. Proc. Western Joint Computer\nConference, 19, 121–131.\nFeigenbaum, E. A., Buchanan, B. G., and Leder-\nberg, J. (1971).\nOn generality and problem solv-\ning: A case study using the DENDRAL program.\nIn Meltzer, B. and Michie, D. (Eds.), Machine Intel-\nligence 6, pp. 165–190. Edinburgh University Press.\nFeldman, J. and Sproull, R. F. (1977). Decision the-\nory and artiﬁcial intelligence II: The hungry mon-\nkey.\nTechnical report, Computer Science Depart-\nment, University of Rochester.\nFeldman, J. and Yakimovsky, Y. (1974). Decision\ntheory and artiﬁcial intelligence I: Semantics-based\nregion analyzer. AIJ, 5(4), 349–371.\nFellbaum, C. (2001). Wordnet: An Electronic Lexi-\ncal Database. MIT Press.\nFellegi, I. and Sunter, A. (1969). A theory for record\nlinkage”. JASA, 64, 1183–1210.\nFelner, A., Korf, R. E., and Hanan, S. (2004). Addi-\ntive pattern database heuristics. JAIR, 22, 279–318.\nFelner, A., Korf, R. E., Meshulam, R., and Holte,\nR. (2007). Compressed pattern databases. JAIR, 30,\n213–247.\nFelzenszwalb, P. and Huttenlocher, D. (2000). Efﬁ-\ncient matching of pictorial structures. In CVPR.\nFelzenszwalb, P. and McAllester, D. A. (2007). The\ngeneralized A* architecture. JAIR.\nFerguson, T. (1992). Mate with knight and bishop\nin kriegspiel. Theoretical Computer Science, 96(2),\n389–403.\nFerguson, T. (1995). Mate with the two bishops in\nkriegspiel. www.math.ucla.edu/˜tom/papers.\nFerguson, T. (1973).\nBayesian analysis of some\nnonparametric problems. Annals of Statistics, 1(2),\n209–230.\nFerraris, P. and Giunchiglia, E. (2000). Planning as\nsatisability in nondeterministic domains. In AAAI-\n00, pp. 748–753.\nFerriss, T. (2007). The 4-Hour Workweek. Crown.\nFikes, R. E., Hart, P. E., and Nilsson, N. J. (1972).\nLearning and executing generalized robot plans. AIJ,\n3(4), 251–288.",
  "satisability in nondeterministic domains. In AAAI-\n00, pp. 748–753.\nFerriss, T. (2007). The 4-Hour Workweek. Crown.\nFikes, R. E., Hart, P. E., and Nilsson, N. J. (1972).\nLearning and executing generalized robot plans. AIJ,\n3(4), 251–288.\nFikes, R. E. and Nilsson, N. J. (1971). STRIPS: A\nnew approach to the application of theorem proving\nto problem solving. AIJ, 2(3–4), 189–208.\nFikes, R. E. and Nilsson, N. J. (1993). STRIPS, a\nretrospective. AIJ, 59(1–2), 227–232.\nFine, S., Singer, Y., and Tishby, N. (1998). The hier-\narchical hidden markov model: Analysis and appli-\ncations. Machine Learning, 32(41–62).\nFinney, D. J. (1947). Probit analysis: A statistical\ntreatment of the sigmoid response curve. Cambridge\nUniversity Press.\nFirth, J. (1957). Papers in Linguistics. Oxford Uni-\nversity Press.\nFisher, R. A. (1922). On the mathematical founda-\ntions of theoretical statistics. Philosophical Transac-\ntions of the Royal Society of London, Series A 222,\n309–368.\nFix, E. and Hodges, J. L. (1951).\nDiscrimina-\ntory analysis—Nonparametric discrimination: Con-\nsistency properties.\nTech. rep. 21-49-004, USAF\nSchool of Aviation Medicine.\nFloreano, D., Zufferey, J. C., Srinivasan, M. V., and\nEllington, C. (2009).\nFlying Insects and Robots.\nSpringer.\nFogel, D. B. (2000).\nEvolutionary Computation:\nToward a New Philosophy of Machine Intelligence.\nIEEE Press.\nFogel, L. J., Owens, A. J., and Walsh, M. J. (1966).\nArtiﬁcial Intelligence through Simulated Evolution.\nWiley.\nFoo, N. (2001).\nWhy engineering models do not\nhave a frame problem. In Discrete event modeling\nand simulation technologies: a tapestry of systems\nand AI-based theories and methodologies. Springer.\nForbes, J. (2002). Learning Optimal Control for Au-\ntonomous Vehicles. Ph.D. thesis, University of Cali-\nfornia.\nForbus, K. D. (1985). Qualitative process theory.\nIn Bobrow, D. (Ed.), Qualitative Reasoning About\nPhysical Systems, pp. 85–186. MIT Press.\nForbus, K. D. and de Kleer, J. (1993).\nBuilding\nProblem Solvers. MIT Press.\nFord, K. M. and Hayes, P. J. (1995). Turing Test\nconsidered harmful. In IJCAI-95, pp. 972–977.\nForestier, J.-P. and Varaiya, P. (1978). Multilayer\ncontrol of large Markov chains. IEEE Transactions\non Automatic Control, 23(2), 298–304.\nForgy, C. (1981). OPS5 user’s manual. Technical\nreport CMU-CS-81-135, Computer Science Depart-\nment, Carnegie-Mellon University.\nForgy, C. (1982).\nA fast algorithm for the many\npatterns/many objects match problem.\nAIJ, 19(1),\n17–37.",
  "Forgy, C. (1981). OPS5 user’s manual. Technical\nreport CMU-CS-81-135, Computer Science Depart-\nment, Carnegie-Mellon University.\nForgy, C. (1982).\nA fast algorithm for the many\npatterns/many objects match problem.\nAIJ, 19(1),\n17–37.\nForsyth, D. and Ponce, J. (2002). Computer Vision:\nA Modern Approach. Prentice Hall.\nFourier, J. (1827).\nAnalyse des travaux de\nl’Acad´emie Royale des Sciences, pendant l’ann´ee\n1824; partie math´ematique. Histoire de l’Acad´emie\nRoyale des Sciences de France, 7, xlvii–lv.\nFox, C. and Tversky, A. (1995). Ambiguity aver-\nsion and comparative ignorance. Quarterly Journal\nof Economics, 110(3), 585–603.\nFox, D., Burgard, W., Dellaert, F., and Thrun, S.\n(1999). Monte carlo localization: Efﬁcient position\nestimation for mobile robots. In AAAI-99.\nFox, M. S. (1990). Constraint-guided scheduling:\nA short history of research at CMU. Computers in\nIndustry, 14(1–3), 79–88.\nFox, M. S., Allen, B., and Strohm, G. (1982). Job\nshop scheduling:\nAn investigation in constraint-\ndirected reasoning. In AAAI-82, pp. 155–158.\nFox, M. S. and Long, D. (1998). The automatic in-\nference of state invariants in TIM. JAIR, 9, 367–421.\nFranco, J. and Paull, M. (1983). Probabilistic anal-\nysis of the Davis Putnam procedure for solving the\nsatisﬁability problem. Discrete Applied Mathemat-\nics, 5, 77–87.\nFrank, I., Basin, D. A., and Matsubara, H. (1998).\nFinding optimal strategies for imperfect information\ngames. In AAAI-98, pp. 500–507.\nFrank, R. H. and Cook, P. J. (1996). The Winner-\nTake-All Society. Penguin.\nFranz, A. (1996). Automatic Ambiguity resolution\nin Natural Language Processing: An Empirical Ap-\nproach. Springer.\nFranz, A. and Brants, T. (2006). All our n-gram are\nbelong to you. Blog posting.\nFrege, G. (1879).\nBegriffsschrift, eine der arith-\nmetischen nachgebildete Formelsprache des reinen\nDenkens. Halle, Berlin. English translation appears\nin van Heijenoort (1967).\nFreitag, D. and McCallum, A. (2000). Information\nextraction with hmm structures learned by stochastic\noptimization. In AAAI-00.\nFreuder, E. C. (1978). Synthesizing constraint ex-\npressions. CACM, 21(11), 958–966.\nFreuder, E. C. (1982). A sufﬁcient condition for\nbacktrack-free search. JACM, 29(1), 24–32.\nFreuder, E. C. (1985). A sufﬁcient condition for\nbacktrack-bounded search. JACM, 32(4), 755–761.\nFreuder, E. C. and Mackworth, A. K. (Eds.). (1994).\nConstraint-based reasoning. MIT Press.\nFreund, Y. and Schapire, R. E. (1996). Experiments\nwith a new boosting algorithm. In ICML-96.",
  "backtrack-bounded search. JACM, 32(4), 755–761.\nFreuder, E. C. and Mackworth, A. K. (Eds.). (1994).\nConstraint-based reasoning. MIT Press.\nFreund, Y. and Schapire, R. E. (1996). Experiments\nwith a new boosting algorithm. In ICML-96.\nFreund, Y. and Schapire, R. E. (1999). Large margin\nclassiﬁcation using the perceptron algorithm. Ma-\nchine Learning, 37(3), 277–296.\nFriedberg, R. M. (1958).\nA learning machine:\nPart I. IBM Journal of Research and Development,\n2, 2–13.\nFriedberg, R. M., Dunham, B., and North, T.\n(1959). A learning machine: Part II. IBM Journal of\nResearch and Development, 3(3), 282–287. Bibliography\n1073\nFriedgut, E. (1999). Necessary and sufﬁcient con-\nditions for sharp thresholds of graph properties, and\nthe k-SAT problem. J. American Mathematical So-\nciety, 12, 1017–1054.\nFriedman, G. J. (1959). Digital simulation of an\nevolutionary process. General Systems Yearbook, 4,\n171–184.\nFriedman, J., Hastie, T., and Tibshirani, R. (2000).\nAdditive logistic regression: A statistical view of\nboosting. Annals of Statistics, 28(2), 337–374.\nFriedman, N. (1998). The Bayesian structural EM\nalgorithm. In UAI-98.\nFriedman, N. and Goldszmidt, M. (1996). Learning\nBayesian networks with local structure. In UAI-96,\npp. 252–262.\nFriedman,\nN. and Koller,\nD. (2003).\nBe-\ning Bayesian about Bayesian network structure:\nA Bayesian approach to structure discovery in\nBayesian networks. Machine Learning, 50, 95–125.\nFriedman, N., Murphy, K., and Russell, S. J.\n(1998).\nLearning the structure of dynamic proba-\nbilistic networks. In UAI-98.\nFriedman, N. (2004). Inferring cellular networks\nusing probabilistic graphical models.\nScience,\n303(5659), 799–805.\nFruhwirth, T. and Abdennadher, S. (2003). Essen-\ntials of constraint programming. Cambridge Univer-\nsity Press.\nFuchs, J. J., Gasquet, A., Olalainty, B., and Currie,\nK. W. (1990). PlanERS-1: An expert planning sys-\ntem for generating spacecraft mission plans. In First\nInternational Conference on Expert Planning Sys-\ntems, pp. 70–75. Institute of Electrical Engineers.\nFudenberg, D. and Tirole, J. (1991). Game theory.\nMIT Press.\nFukunaga, A. S., Rabideau, G., Chien, S., and Yan,\nD. (1997).\nASPEN: A framework for automated\nplanning and scheduling of spacecraft control and\noperations. In Proc. International Symposium on AI,\nRobotics and Automation in Space, pp. 181–187.\nFung, R. and Chang, K. C. (1989).\nWeighting\nand integrating evidence for stochastic simulation in\nBayesian networks. In UAI-98, pp. 209–220.",
  "operations. In Proc. International Symposium on AI,\nRobotics and Automation in Space, pp. 181–187.\nFung, R. and Chang, K. C. (1989).\nWeighting\nand integrating evidence for stochastic simulation in\nBayesian networks. In UAI-98, pp. 209–220.\nGaddum, J. H. (1933). Reports on biological stan-\ndard III: Methods of biological assay depending on a\nquantal response. Special report series of the medi-\ncal research council 183, Medical Research Council.\nGaifman, H. (1964). Concerning measures in ﬁrst\norder calculi. Israel Journal of Mathematics, 2, 1–\n18.\nGallaire, H. and Minker, J. (Eds.). (1978). Logic\nand Databases. Plenum.\nGallier, J. H. (1986). Logic for Computer Science:\nFoundations of Automatic Theorem Proving. Harper\nand Row.\nGamba, A., Gamberini, L., Palmieri, G., and Sanna,\nR. (1961). Further experiments with PAPA. Nuovo\nCimento Supplemento, 20(2), 221–231.\nGarding, J. (1992). Shape from texture for smooth\ncurved surfaces in perspective projection. J. Mathe-\nmatical Imaging and Vision, 2(4), 327–350.\nGardner, M. (1968).\nLogic Machines, Diagrams\nand Boolean Algebra. Dover.\nGarey, M. R. and Johnson, D. S. (1979). Computers\nand Intractability. W. H. Freeman.\nGaschnig, J. (1977). A general backtrack algorithm\nthat eliminates most redundant tests. In IJCAI-77, p.\n457.\nGaschnig, J. (1979). Performance measurement and\nanalysis of certain search algorithms. Technical re-\nport CMU-CS-79-124, Computer Science Depart-\nment, Carnegie-Mellon University.\nGasser, R. (1995). Efﬁciently harnessing computa-\ntional resources for exhaustive search. Ph.D. thesis,\nETH Z¨urich.\nGasser, R. (1998). Solving nine men’s morris. In\nNowakowski, R. (Ed.), Games of No Chance. Cam-\nbridge University Press.\nGat, E. (1998). Three-layered architectures. In Ko-\nrtenkamp, D., Bonasso, R. P., and Murphy, R. (Eds.),\nAI-based Mobile Robots: Case Studies of Successful\nRobot Systems, pp. 195–210. MIT Press.\nGauss, C. F. (1809).\nTheoria Motus Corporum\nCoelestium in Sectionibus Conicis Solem Ambien-\ntium. Sumtibus F. Perthes et I. H. Besser, Hamburg.\nGauss, C. F. (1829).\nBeitr¨age zur theorie der\nalgebraischen gleichungen.\nCollected in Werke,\nVol. 3, pages 71–102. K. Gesellschaft Wissenschaft,\nG¨ottingen, Germany, 1876.\nGawande, A. (2002). Complications: A Surgeon’s\nNotes on an Imperfect Science. Metropolitan Books.\nGeiger, D., Verma, T., and Pearl, J. (1990). Identify-\ning independence in Bayesian networks. Networks,\n20(5), 507–534.\nGeisel, T. (1955).\nOn Beyond Zebra.\nRandom\nHouse.\nGelb, A. (1974).",
  "Notes on an Imperfect Science. Metropolitan Books.\nGeiger, D., Verma, T., and Pearl, J. (1990). Identify-\ning independence in Bayesian networks. Networks,\n20(5), 507–534.\nGeisel, T. (1955).\nOn Beyond Zebra.\nRandom\nHouse.\nGelb, A. (1974).\nApplied Optimal Estimation.\nMIT Press.\nGelernter, H. (1959). Realization of a geometry-\ntheorem proving machine.\nIn Proc. an Interna-\ntional Conference on Information Processing, pp.\n273–282. UNESCO House.\nGelfond, M. and Lifschitz, V. (1988). Compiling cir-\ncumscriptive theories into logic programs. In Non-\nMonotonic Reasoning: 2nd International Workshop\nProceedings, pp. 74–99.\nGelfond, M. (2008). Answer sets. In van Harmelan,\nF., Lifschitz, V., and Porter, B. (Eds.), Handbook of\nKnowledge Representation, pp. 285–316. Elsevier.\nGelly, S. and Silver, D. (2008). Achieving master\nlevel play in 9 x 9 computer go. In AAAI-08, pp.\n1537–1540.\nGelman, A., Carlin, J. B., Stern, H. S., and Rubin,\nD. (1995). Bayesian Data Analysis. Chapman &\nHall.\nGeman, S. and Geman, D. (1984). Stochastic relax-\nation, Gibbs distributions, and Bayesian restoration\nof images. PAMI, 6(6), 721–741.\nGenesereth, M. R. (1984). The use of design de-\nscriptions in automated diagnosis.\nAIJ, 24(1–3),\n411–436.\nGenesereth, M. R. and Nilsson, N. J. (1987). Log-\nical Foundations of Artiﬁcial Intelligence. Morgan\nKaufmann.\nGenesereth, M. R. and Nourbakhsh, I. (1993).\nTime-saving tips for problem solving with incom-\nplete information. In AAAI-93, pp. 724–730.\nGenesereth, M. R. and Smith, D. E. (1981). Meta-\nlevel architecture. Memo HPP-81-6, Computer Sci-\nence Department, Stanford University.\nGent, I., Petrie, K., and Puget, J.-F. (2006). Sym-\nmetry in constraint programming. In Rossi, F., van\nBeek, P., and Walsh, T. (Eds.), Handbook of Con-\nstraint Programming. Elsevier.\nGentner, D. (1983). Structure mapping: A theoret-\nical framework for analogy. Cognitive Science, 7,\n155–170.\nGentner, D. and Goldin-Meadow, S. (Eds.). (2003).\nLanguage in mind: Advances in the study of lan-\nguage and though. MIT Press.\nGerevini, A. and Long, D. (2005). Plan constraints\nand preferences in PDDL3. Tech. rep., Dept. of Elec-\ntronics for Automation, University of Brescia, Italy.\nGerevini, A. and Serina, I. (2002). LPG: A plan-\nner based on planning graphs with action costs. In\nICAPS-02, pp. 281–290.\nGerevini, A. and Serina, I. (2003).\nPlanning as\npropositional CSP: from walksat to local search for\naction graphs. Constraints, 8, 389–413.",
  "ner based on planning graphs with action costs. In\nICAPS-02, pp. 281–290.\nGerevini, A. and Serina, I. (2003).\nPlanning as\npropositional CSP: from walksat to local search for\naction graphs. Constraints, 8, 389–413.\nGershwin, G. (1937). Let’s call the whole thing off.\nSong.\nGetoor, L. and Taskar, B. (Eds.). (2007). Introduc-\ntion to Statistical Relational Learning. MIT Press.\nGhahramani, Z. and Jordan, M. I. (1997). Facto-\nrial hidden Markov models. Machine Learning, 29,\n245–274.\nGhahramani,\nZ. (1998).\nLearning dynamic\nbayesian networks. In Adaptive Processing of Se-\nquences and Data Structures, pp. 168–197.\nGhahramani, Z. (2005). Tutorial on nonparametric\nBayesian methods. Tutorial presentation at the UAI\nConference.\nGhallab, M., Howe, A., Knoblock, C. A., and Mc-\nDermott, D. (1998). PDDL—The planning domain\ndeﬁnition language. Tech. rep. DCS TR-1165, Yale\nCenter for Computational Vision and Control.\nGhallab, M. and Laruelle, H. (1994). Representa-\ntion and control in IxTeT, a temporal planner.\nIn\nAIPS-94, pp. 61–67.\nGhallab, M., Nau, D. S., and Traverso, P. (2004).\nAutomated Planning: Theory and practice. Morgan\nKaufmann.\nGibbs, R. W. (2006). Metaphor interpretation as em-\nbodied simulation. Mind, 21(3), 434–458.\nGibson, J. J. (1950). The Perception of the Visual\nWorld. Houghton Mifﬂin.\nGibson, J. J. (1979). The Ecological Approach to\nVisual Perception. Houghton Mifﬂin.\nGilks, W. R., Richardson, S., and Spiegelhalter, D. J.\n(Eds.). (1996). Markov chain Monte Carlo in prac-\ntice. Chapman and Hall.\nGilks, W. R., Thomas, A., and Spiegelhalter, D. J.\n(1994).\nA language and program for complex\nBayesian modelling. The Statistician, 43, 169–178.\nGilmore, P. C. (1960). A proof method for quantiﬁ-\ncation theory: Its justiﬁcation and realization. IBM\nJournal of Research and Development, 4, 28–35.\nGinsberg, M. L. (1993). Essentials of Artiﬁcial In-\ntelligence. Morgan Kaufmann.\nGinsberg, M. L. (1999).\nGIB: Steps toward an\nexpert-level bridge-playing program. In IJCAI-99,\npp. 584–589.\nGinsberg, M. L., Frank, M., Halpin, M. P., and Tor-\nrance, M. C. (1990). Search lessons learned from\ncrossword puzzles. In AAAI-90, Vol. 1, pp. 210–215.\nGinsberg, M. L. (2001). GIB: Imperfect infoorma-\ntion in a computationally challenging game. JAIR,\n14, 303–358.\nGionis, A., Indyk, P., and Motwani, R. (1999). Simi-\nlarity search in high dimensions vis hashing. In Proc.\n25th Very Large Database (VLDB) Conference. 1074\nBibliography\nGittins, J. C. (1989). Multi-Armed Bandit Allocation",
  "14, 303–358.\nGionis, A., Indyk, P., and Motwani, R. (1999). Simi-\nlarity search in high dimensions vis hashing. In Proc.\n25th Very Large Database (VLDB) Conference. 1074\nBibliography\nGittins, J. C. (1989). Multi-Armed Bandit Allocation\nIndices. Wiley.\nGlanc, A. (1978). On the etymology of the word\n“robot”. SIGART Newsletter, 67, 12.\nGlover, F. and Laguna, M. (Eds.). (1997).\nTabu\nsearch. Kluwer.\nG¨odel, K. (1930).\n¨Uber die Vollst¨andigkeit des\nLogikkalk¨uls. Ph.D. thesis, University of Vienna.\nG¨odel, K. (1931).\n¨Uber formal unentscheidbare\nS¨atze der Principia mathematica und verwandter\nSysteme I. Monatshefte f¨ur Mathematik und Physik,\n38, 173–198.\nGoebel, J., Volk, K., Walker, H., and Gerbault, F.\n(1989). Automatic classiﬁcation of spectra from the\ninfrared astronomical satellite (IRAS).\nAstronomy\nand Astrophysics, 222, L5–L8.\nGoertzel, B. and Pennachin, C. (2007).\nArtiﬁcial\nGeneral Intelligence. Springer.\nGold, B. and Morgan, N. (2000). Speech and Audio\nSignal Processing. Wiley.\nGold, E. M. (1967). Language identiﬁcation in the\nlimit. Information and Control, 10, 447–474.\nGoldberg, A. V., Kaplan, H., and Werneck, R. F.\n(2006). Reach for a*: Efﬁcient point-to-point short-\nest path algorithms. In Workshop on algorithm engi-\nneering and experiments, pp. 129–143.\nGoldman, R. and Boddy, M. (1996).\nExpressive\nplanning and explicit knowledge. In AIPS-96, pp.\n110–117.\nGoldszmidt, M. and Pearl, J. (1996).\nQualitative\nprobabilities for default reasoning, belief revision,\nand causal modeling. AIJ, 84(1–2), 57–112.\nGolomb, S. and Baumert, L. (1965). Backtrack pro-\nramming. JACM, 14, 516–524.\nGolub, G., Heath, M., and Wahba, G. (1979). Gen-\neralized cross-validation as a method for choosing a\ngood ridge parameter. Technometrics, 21(2).\nGomes, C., Selman, B., Crato, N., and Kautz, H.\n(2000). Heavy-tailed phenomena in satisﬁability and\nconstrain processing. JAR, 24, 67–100.\nGomes, C., Kautz, H., Sabharwal, A., and Selman,\nB. (2008). Satisﬁability solvers. In van Harmelen,\nF., Lifschitz, V., and Porter, B. (Eds.), Handbook of\nKnowledge Representation. Elsevier.\nGomes, C. and Selman, B. (2001). Algorithm port-\nfolios. AIJ, 126, 43–62.\nGomes, C., Selman, B., and Kautz, H. (1998).\nBoosting combinatorial search through randomiza-\ntion. In AAAI-98, pp. 431–437.\nGonthier, G. (2008). Formal proof–The four-color\ntheorem. Notices of the AMS, 55(11), 1382–1393.\nGood, I. J. (1961). A causal calculus. British Jour-\nnal of the Philosophy of Science, 11, 305–318.",
  "tion. In AAAI-98, pp. 431–437.\nGonthier, G. (2008). Formal proof–The four-color\ntheorem. Notices of the AMS, 55(11), 1382–1393.\nGood, I. J. (1961). A causal calculus. British Jour-\nnal of the Philosophy of Science, 11, 305–318.\nGood, I. J. (1965). Speculations concerning the ﬁrst\nultraintelligent machine. In Alt, F. L. and Rubinoff,\nM. (Eds.), Advances in Computers, Vol. 6, pp. 31–\n88. Academic Press.\nGood, I. J. (1983). Good Thinking: The Founda-\ntions of Probability and Its Applications. University\nof Minnesota Press.\nGoodman, D. and Keene, R. (1997). Man versus\nMachine: Kasparov versus Deep Blue. H3 Publica-\ntions.\nGoodman, J. (2001). A bit of progress in language\nmodeling. Tech. rep. MSR-TR-2001-72, Microsoft\nResearch.\nGoodman, J. and Heckerman, D. (2004). Fighting\nspam with statistics. Signiﬁcance, the Magazine of\nthe Royal Statistical Society, 1, 69–72.\nGoodman, N. (1954). Fact, Fiction and Forecast.\nUniversity of London Press.\nGoodman, N. (1977). The Structure of Appearance\n(3rd edition). D. Reidel.\nGopnik, A. and Glymour, C. (2002). Causal maps\nand bayes nets: A cognitive and computational ac-\ncount of theory-formation. In Caruthers, P., Stich,\nS., and Siegal, M. (Eds.), The Cognitive Basis of Sci-\nence. Cambridge University Press.\nGordon, D. M. (2000). Ants at Work. Norton.\nGordon, D. M. (2007). Control without hierarchy.\nNature, 446(8), 143.\nGordon, M. J., Milner, A. J., and Wadsworth, C. P.\n(1979). Edinburgh LCF. Springer-Verlag.\nGordon, N. (1994). Bayesian methods for tracking.\nPh.D. thesis, Imperial College.\nGordon, N., Salmond, D. J., and Smith, A. F. M.\n(1993). Novel approach to nonlinear/non-Gaussian\nBayesian state estimation.\nIEE Proceedings F\n(Radar and Signal Processing), 140(2), 107–113.\nGorry, G. A. (1968). Strategies for computer-aided\ndiagnosis. Mathematical Biosciences, 2(3–4), 293–\n318.\nGorry, G. A., Kassirer, J. P., Essig, A., and\nSchwartz, W. B. (1973).\nDecision analysis as the\nbasis for computer-aided management of acute renal\nfailure. American Journal of Medicine, 55, 473–484.\nGottlob, G., Leone, N., and Scarcello, F. (1999a). A\ncomparison of structural CSP decomposition meth-\nods. In IJCAI-99, pp. 394–399.\nGottlob, G., Leone, N., and Scarcello, F. (1999b).\nHypertree decompositions and tractable queries. In\nPODS-99, pp. 21–32.\nGraham, S. L., Harrison, M. A., and Ruzzo, W. L.\n(1980). An improved context-free recognizer. ACM\nTransactions on Programming Languages and Sys-\ntems, 2(3), 415–462.",
  "Hypertree decompositions and tractable queries. In\nPODS-99, pp. 21–32.\nGraham, S. L., Harrison, M. A., and Ruzzo, W. L.\n(1980). An improved context-free recognizer. ACM\nTransactions on Programming Languages and Sys-\ntems, 2(3), 415–462.\nGrama, A. and Kumar, V. (1995). A survey of paral-\nlel search algorithms for discrete optimization prob-\nlems. ORSA Journal of Computing, 7(4), 365–385.\nGrassmann, H. (1861). Lehrbuch der Arithmetik.\nTh. Chr. Fr. Enslin, Berlin.\nGrayson, C. J. (1960).\nDecisions under uncer-\ntainty: Drilling decisions by oil and gas operators.\nTech. rep., Division of Research, Harvard Business\nSchool.\nGreen, B., Wolf, A., Chomsky, C., and Laugherty,\nK. (1961). BASEBALL: An automatic question an-\nswerer.\nIn Proc. Western Joint Computer Confer-\nence, pp. 219–224.\nGreen, C. (1969a). Application of theorem proving\nto problem solving. In IJCAI-69, pp. 219–239.\nGreen, C. (1969b).\nTheorem-proving by resolu-\ntion as a basis for question-answering systems. In\nMeltzer, B., Michie, D., and Swann, M. (Eds.), Ma-\nchine Intelligence 4, pp. 183–205. Edinburgh Uni-\nversity Press.\nGreen, C. and Raphael, B. (1968).\nThe use of\ntheorem-proving techniques in question-answering\nsystems. In Proc. 23rd ACM National Conference.\nGreenblatt, R. D., Eastlake, D. E., and Crocker,\nS. D. (1967). The Greenblatt chess program. In Proc.\nFall Joint Computer Conference, pp. 801–810.\nGreiner, R. (1989). Towards a formal analysis of\nEBL. In ICML-89, pp. 450–453.\nGrinstead, C. and Snell, J. (1997). Introduction to\nProbability. AMS.\nGrove, W. and Meehl, P. (1996). Comparative efﬁ-\nciency of informal (subjective, impressionistic) and\nformal (mechanical, algorithmic) prediction proce-\ndures: The clinical statistical controversy. Psychol-\nogy, Public Policy, and Law, 2, 293–323.\nGruber, T. (2004). Interview of Tom Gruber. AIS\nSIGSEMIS Bulletin, 1(3).\nGu, J. (1989). Parallel Algorithms and Architectures\nfor Very Fast AI Search. Ph.D. thesis, University of\nUtah.\nGuard, J., Oglesby, F., Bennett, J., and Settle, L.\n(1969). Semi-automated mathematics. JACM, 16,\n49–62.\nGuestrin, C., Koller, D., Gearhart, C., and Kanodia,\nN. (2003a). Generalizing plans to new environments\nin relational MDPs. In IJCAI-03.\nGuestrin, C., Koller, D., Parr, R., and Venkatara-\nman, S. (2003b). Efﬁcient solution algorithms for\nfactored MDPs. JAIR, 19, 399–468.\nGuestrin, C., Lagoudakis, M. G., and Parr, R.\n(2002).\nCoordinated reinforcement learning.\nIn\nICML-02, pp. 227–234.",
  "Guestrin, C., Koller, D., Parr, R., and Venkatara-\nman, S. (2003b). Efﬁcient solution algorithms for\nfactored MDPs. JAIR, 19, 399–468.\nGuestrin, C., Lagoudakis, M. G., and Parr, R.\n(2002).\nCoordinated reinforcement learning.\nIn\nICML-02, pp. 227–234.\nGuibas, L. J., Knuth, D. E., and Sharir, M. (1992).\nRandomized incremental construction of Delaunay\nand Voronoi diagrams. Algorithmica, 7, 381–413.\nSee also 17th Int. Coll. on Automata, Languages and\nProgramming, 1990, pp. 414–431.\nGumperz, J. and Levinson, S. (1996). Rethinking\nLinguistic Relativity. Cambridge University Press.\nGuyon, I. and Elisseeff, A. (2003). An introduction\nto variable and feature selection. JMLR, pp. 1157–\n1182.\nHacking, I. (1975). The Emergence of Probability.\nCambridge University Press.\nHaghighi, A. and Klein, D. (2006).\nPrototype-\ndriven grammar induction. In COLING-06.\nHald, A. (1990). A History of Probability and Statis-\ntics and Their Applications before 1750. Wiley.\nHalevy, A. (2007). Dataspaces: A new paradigm\nfor data integration.\nIn Brazilian Symposium on\nDatabases.\nHalevy, A., Norvig, P., and Pereira, F. (2009). The\nunreasonable effectiveness of data. IEEE Intelligent\nSystems, March/April, 8–12.\nHalpern, J. Y. (1990). An analysis of ﬁrst-order log-\nics of probability. AIJ, 46(3), 311–350.\nHalpern, J. Y. (1999). Technical addendum, Cox’s\ntheorem revisited. JAIR, 11, 429–435.\nHalpern, J. Y. and Weissman, V. (2008). Using ﬁrst-\norder logic to reason about policies. ACM Transac-\ntions on Information and System Security, 11(4).\nHamming, R. W. (1991). The Art of Probability for\nScientists and Engineers. Addison-Wesley.\nHammond, K. (1989). Case-Based Planning: View-\ning Planning as a Memory Task. Academic Press.\nHamscher, W., Console, L., and Kleer, J. D. (1992).\nReadings in Model-based Diagnosis. Morgan Kauf-\nmann.\nHan, X. and Boyden, E. (2007). Multiple-color op-\ntical activation, silencing, and desynchronization of\nneural activity, with single-spike temporal resolu-\ntion. PLoS One, e299.\nHand, D., Mannila, H., and Smyth, P. (2001). Prin-\nciples of Data Mining. MIT Press. Bibliography\n1075\nHandschin, J. E. and Mayne, D. Q. (1969). Monte\nCarlo techniques to estimate the conditional expecta-\ntion in multi-stage nonlinear ﬁltering. Int. J. Control,\n9(5), 547–559.\nHansen, E. (1998). Solving POMDPs by searching\nin policy space. In UAI-98, pp. 211–219.\nHansen, E. and Zilberstein, S. (2001).\nLAO*: a\nheuristic search algorithm that ﬁnds solutions with\nloops. AIJ, 129(1–2), 35–62.",
  "9(5), 547–559.\nHansen, E. (1998). Solving POMDPs by searching\nin policy space. In UAI-98, pp. 211–219.\nHansen, E. and Zilberstein, S. (2001).\nLAO*: a\nheuristic search algorithm that ﬁnds solutions with\nloops. AIJ, 129(1–2), 35–62.\nHansen, P. and Jaumard, B. (1990).\nAlgorithms\nfor the maximum satisﬁability problem. Computing,\n44(4), 279–303.\nHanski, I. and Cambefort, Y. (Eds.). (1991). Dung\nBeetle Ecology. Princeton University Press.\nHansson, O. and Mayer, A. (1989). Heuristic search\nas evidential reasoning. In UAI 5.\nHansson, O., Mayer, A., and Yung, M. (1992). Crit-\nicizing solutions to relaxed models yields powerful\nadmissible heuristics. Information Sciences, 63(3),\n207–227.\nHaralick, R. M. and Elliot, G. L. (1980). Increas-\ning tree search efﬁciency for constraint satisfaction\nproblems. AIJ, 14(3), 263–313.\nHardin, G. (1968). The tragedy of the commons.\nScience, 162, 1243–1248.\nHardy, G. H. (1940). A Mathematician’s Apology.\nCambridge University Press.\nHarman, G. H. (1983). Change in View: Principles\nof Reasoning. MIT Press.\nHarris, Z. (1954). Distributional structure. Word,\n10(2/3).\nHarrison, J. R. and March, J. G. (1984). Decision\nmaking and postdecision surprises. Administrative\nScience Quarterly, 29, 26–42.\nHarsanyi, J. (1967). Games with incomplete infor-\nmation played by Bayesian players.\nManagement\nScience, 14, 159–182.\nHart, P. E., Nilsson, N. J., and Raphael, B. (1968). A\nformal basis for the heuristic determination of mini-\nmum cost paths. IEEE Transactions on Systems Sci-\nence and Cybernetics, SSC-4(2), 100–107.\nHart, P. E., Nilsson, N. J., and Raphael, B. (1972).\nCorrection to “A formal basis for the heuristic deter-\nmination of minimum cost paths”. SIGART Newslet-\nter, 37, 28–29.\nHart, T. P. and Edwards, D. J. (1961).\nThe tree\nprune (TP) algorithm. Artiﬁcial intelligence project\nmemo 30, Massachusetts Institute of Technology.\nHartley, H. (1958). Maximum likelihood estimation\nfrom incomplete data. Biometrics, 14, 174–194.\nHartley, R. and Zisserman, A. (2000). Multiple view\ngeometry in computer vision. Cambridge University\nPress.\nHaslum, P., Botea, A., Helmert, M., Bonet, B., and\nKoenig, S. (2007). Domain-independent construc-\ntion of pattern database heuristics for cost-optimal\nplanning. In AAAI-07, pp. 1007–1012.\nHaslum, P. and Geffner, H. (2001). Heuristic plan-\nning with time and resources.\nIn Proc. IJCAI-01\nWorkshop on Planning with Resources.\nHaslum, P. (2006). Improving heuristics through re-",
  "planning. In AAAI-07, pp. 1007–1012.\nHaslum, P. and Geffner, H. (2001). Heuristic plan-\nning with time and resources.\nIn Proc. IJCAI-01\nWorkshop on Planning with Resources.\nHaslum, P. (2006). Improving heuristics through re-\nlaxed search – An analysis of TP4 and HSP*a in the\n2004 planning competition. JAIR, 25, 233–267.\nHaslum, P., Bonet, B., and Geffner, H. (2005). New\nadmissible heuristics for domain-independent plan-\nning. In AAAI-05.\nHastie, T. and Tibshirani, R. (1996). Discriminant\nadaptive nearest neighbor classiﬁcation and regres-\nsion. In Touretzky, D. S., Mozer, M. C., and Has-\nselmo, M. E. (Eds.), NIPS 8, pp. 409–15. MIT Press.\nHastie, T., Tibshirani, R., and Friedman, J. (2001).\nThe Elements of Statistical Learning: Data Mining,\nInference and Prediction (2nd edition).\nSpringer-\nVerlag.\nHastie, T., Tibshirani, R., and Friedman, J. (2009).\nThe Elements of Statistical Learning: Data Mining,\nInference and Prediction (2nd edition).\nSpringer-\nVerlag.\nHaugeland, J. (Ed.). (1985). Artiﬁcial Intelligence:\nThe Very Idea. MIT Press.\nHauk, T. (2004).\nSearch in Trees with Chance\nNodes. Ph.D. thesis, Univ. of Alberta.\nHaussler, D. (1989). Learning conjunctive concepts\nin structural domains. Machine Learning, 4(1), 7–\n40.\nHavelund, K., Lowry, M., Park, S., Pecheur, C.,\nPenix, J., Visser, W., and White, J. L. (2000). Formal\nanalysis of the remote agent before and after ﬂight.\nIn Proc. 5th NASA Langley Formal Methods Work-\nshop.\nHavenstein, H. (2005). Spring comes to AI winter.\nComputer World.\nHawkins, J. and Blakeslee, S. (2004). On Intelli-\ngence. Henry Holt and Co.\nHayes, P. J. (1978). The naive physics manifesto. In\nMichie, D. (Ed.), Expert Systems in the Microelec-\ntronic Age. Edinburgh University Press.\nHayes, P. J. (1979). The logic of frames. In Metzing,\nD. (Ed.), Frame Conceptions and Text Understand-\ning, pp. 46–61. de Gruyter.\nHayes, P. J. (1985a). Naive physics I: Ontology for\nliquids. In Hobbs, J. R. and Moore, R. C. (Eds.), For-\nmal Theories of the Commonsense World, chap. 3,\npp. 71–107. Ablex.\nHayes, P. J. (1985b). The second naive physics man-\nifesto. In Hobbs, J. R. and Moore, R. C. (Eds.), For-\nmal Theories of the Commonsense World, chap. 1,\npp. 1–36. Ablex.\nHaykin, S. (2008). Neural Networks: A Compre-\nhensive Foundation. Prentice Hall.\nHays, J. and Efros, A. A. (2007). Scene completion\nUsing millions of photographs. ACM Transactions\non Graphics (SIGGRAPH), 26(3).\nHearst, M. A. (1992). Automatic acquisition of hy-",
  "hensive Foundation. Prentice Hall.\nHays, J. and Efros, A. A. (2007). Scene completion\nUsing millions of photographs. ACM Transactions\non Graphics (SIGGRAPH), 26(3).\nHearst, M. A. (1992). Automatic acquisition of hy-\nponyms from large text corpora. In COLING-92.\nHearst, M. A. (2009). Search User Interfaces. Cam-\nbridge University Press.\nHebb, D. O. (1949). The Organization of Behavior.\nWiley.\nHeckerman, D. (1986). Probabilistic interpretation\nfor MYCIN’s certainty factors.\nIn Kanal, L. N.\nand Lemmer, J. F. (Eds.), UAI 2, pp. 167–196.\nElsevier/North-Holland.\nHeckerman, D. (1991).\nProbabilistic Similarity\nNetworks. MIT Press.\nHeckerman, D. (1998). A tutorial on learning with\nBayesian networks. In Jordan, M. I. (Ed.), Learning\nin graphical models. Kluwer.\nHeckerman, D., Geiger, D., and Chickering, D. M.\n(1994). Learning Bayesian networks: The combi-\nnation of knowledge and statistical data. Technical\nreport MSR-TR-94-09, Microsoft Research.\nHeidegger, M. (1927). Being and Time. SCM Press.\nHeinz, E. A. (2000). Scalable search in computer\nchess. Vieweg.\nHeld, M. and Karp, R. M. (1970).\nThe traveling\nsalesman problem and minimum spanning trees. Op-\nerations Research, 18, 1138–1162.\nHelmert, M. (2001). On the complexity of planning\nin transportation domains. In ECP-01.\nHelmert, M. (2003). Complexity results for stan-\ndard benchmark domains in planning. AIJ, 143(2),\n219–262.\nHelmert, M. (2006). The fast downward planning\nsystem. JAIR, 26, 191–246.\nHelmert, M. and Richter, S. (2004). Fast downward\n– Making use of causal dependencies in the prob-\nlem representation. In Proc. International Planning\nCompetition at ICAPS, pp. 41–43.\nHelmert, M. and R¨oger, G. (2008). How good is\nalmost perfect? In AAAI-08.\nHendler, J., Carbonell, J. G., Lenat, D. B., Mi-\nzoguchi, R., and Rosenbloom, P. S. (1995). VERY\nlarge knowledge bases – Architecture vs engineer-\ning. In IJCAI-95, pp. 2033–2036.\nHenrion, M. (1988). Propagation of uncertainty in\nBayesian networks by probabilistic logic sampling.\nIn Lemmer, J. F. and Kanal, L. N. (Eds.), UAI 2, pp.\n149–163. Elsevier/North-Holland.\nHenzinger, T. A. and Sastry, S. (Eds.). (1998). Hy-\nbrid systems: Computation and control. Springer-\nVerlag.\nHerbrand, J. (1930). Recherches sur la Th´eorie de\nla D´emonstration. Ph.D. thesis, University of Paris.\nHewitt, C. (1969). PLANNER: a language for prov-\ning theorems in robots. In IJCAI-69, pp. 295–301.\nHierholzer, C. (1873).\n¨Uber die M¨oglichkeit,\neinen Linienzug ohne Wiederholung und ohne Un-",
  "la D´emonstration. Ph.D. thesis, University of Paris.\nHewitt, C. (1969). PLANNER: a language for prov-\ning theorems in robots. In IJCAI-69, pp. 295–301.\nHierholzer, C. (1873).\n¨Uber die M¨oglichkeit,\neinen Linienzug ohne Wiederholung und ohne Un-\nterbrechung zu umfahren. Mathematische Annalen,\n6, 30–32.\nHilgard, E. R. and Bower, G. H. (1975). Theories of\nLearning (4th edition). Prentice-Hall.\nHintikka, J. (1962). Knowledge and Belief. Cornell\nUniversity Press.\nHinton, G. E. and Anderson, J. A. (1981). Parallel\nModels of Associative Memory. Lawrence Erlbaum\nAssociates.\nHinton, G. E. and Nowlan, S. J. (1987). How learn-\ning can guide evolution.\nComplex Systems, 1(3),\n495–502.\nHinton, G. E., Osindero, S., and Teh, Y. W. (2006).\nA fast learning algorithm for deep belief nets. Neural\nComputation, 18, 1527–15554.\nHinton, G. E. and Sejnowski, T. (1983). Optimal\nperceptual inference. In CVPR, pp. 448–453.\nHinton, G. E. and Sejnowski, T. (1986). Learning\nand relearning in Boltzmann machines. In Rumel-\nhart, D. E. and McClelland, J. L. (Eds.), Paral-\nlel Distributed Processing, chap. 7, pp. 282–317.\nMIT Press.\nHirsh, H. (1987). Explanation-based generalization\nin a logic programming environment. In IJCAI-87.\nHobbs, J. R. (1990). Literature and Cognition. CSLI\nPress.\nHobbs, J. R., Appelt, D., Bear, J., Israel, D.,\nKameyama, M., Stickel, M. E., and Tyson, M.\n(1997). FASTUS: A cascaded ﬁnite-state transducer\nfor extracting information from natural-language\ntext. In Roche, E. and Schabes, Y. (Eds.), Finite-\nState Devices for Natural Language Processing, pp.\n383–406. MIT Press. 1076\nBibliography\nHobbs, J. R. and Moore, R. C. (Eds.). (1985). For-\nmal Theories of the Commonsense World. Ablex.\nHobbs, J. R., Stickel, M. E., Appelt, D., and Martin,\nP. (1993). Interpretation as abduction. AIJ, 63(1–2),\n69–142.\nHoffmann, J. (2001). FF: The fast-forward planning\nsystem. AIMag, 22(3), 57–62.\nHoffmann, J. and Brafman, R. I. (2006). Confor-\nmant planning via heuristic forward search: A new\napproach. AIJ, 170(6–7), 507–541.\nHoffmann, J. and Brafman, R. I. (2005). Contingent\nplanning via heuristic forward search with implicit\nbelief states. In ICAPS-05.\nHoffmann, J. (2005). Where “ignoring delete lists”\nworks: Local search topology in planning bench-\nmarks. JAIR, 24, 685–758.\nHoffmann, J. and Nebel, B. (2001). The FF plan-\nning system: Fast plan generation through heuristic\nsearch. JAIR, 14, 253–302.\nHoffmann, J., Sabharwal, A., and Domshlak, C.",
  "works: Local search topology in planning bench-\nmarks. JAIR, 24, 685–758.\nHoffmann, J. and Nebel, B. (2001). The FF plan-\nning system: Fast plan generation through heuristic\nsearch. JAIR, 14, 253–302.\nHoffmann, J., Sabharwal, A., and Domshlak, C.\n(2006). Friends or foes? An AI planning perspective\non abstraction and search. In ICAPS-06, pp. 294–\n303.\nHogan, N. (1985). Impedance control: An approach\nto manipulation. Parts I, II, and III. J. Dynamic Sys-\ntems, Measurement, and Control, 107(3), 1–24.\nHoiem, D., Efros, A. A., and Hebert, M. (2008).\nPutting objects in perspective. IJCV, 80(1).\nHolland, J. H. (1975). Adaption in Natural and Ar-\ntiﬁcial Systems. University of Michigan Press.\nHolland, J. H. (1995). Hidden Order: How Adapta-\ntion Builds Complexity. Addison-Wesley.\nHolte, R. and Hernadvolgyi, I. (2001). Steps towards\nthe automatic creation of search heuristics.\nTech.\nrep. TR04-02, CS Dept., Univ. of Alberta.\nHolzmann, G. J. (1997). The Spin model checker.\nIEEE Transactions on Software Engineering, 23(5),\n279–295.\nHood, A. (1824).\nCase 4th—28 July 1824 (Mr.\nHood’s cases of injuries of the brain). Phrenologi-\ncal Journal and Miscellany, 2, 82–94.\nHooker, J. (1995). Testing heuristics: We have it all\nwrong. J. Heuristics, 1, 33–42.\nHoos, H. and Tsang, E. (2006). Local search meth-\nods. In Rossi, F., van Beek, P., and Walsh, T. (Eds.),\nHandbook of Constraint Processing, pp. 135–168.\nElsevier.\nHope, J. (1994). The Authorship of Shakespeare’s\nPlays. Cambridge University Press.\nHopﬁeld, J. J. (1982). Neurons with graded response\nhave collective computational properties like those\nof two-state neurons. PNAS, 79, 2554–2558.\nHorn, A. (1951). On sentences which are true of\ndirect unions of algebras. JSL, 16, 14–21.\nHorn, B. K. P. (1970).\nShape from shading: A\nmethod for obtaining the shape of a smooth opaque\nobject from one view. Technical report 232, MIT\nArtiﬁcial Intelligence Laboratory.\nHorn, B. K. P. (1986). Robot Vision. MIT Press.\nHorn, B. K. P. and Brooks, M. J. (1989). Shape from\nShading. MIT Press.\nHorn, K. V. (2003). Constructing a logic of plausi-\nble inference: A guide to cox’s theorem. IJAR, 34,\n3–24.\nHorning, J. J. (1969). A study of grammatical infer-\nence. Ph.D. thesis, Stanford University.\nHorowitz, E. and Sahni, S. (1978). Fundamentals of\nComputer Algorithms. Computer Science Press.\nHorswill, I. (2000).\nFunctional programming of\nbehavior-based systems. Autonomous Robots, 9, 83–\n93.\nHorvitz, E. J. (1987). Problem-solving design: Rea-",
  "Horowitz, E. and Sahni, S. (1978). Fundamentals of\nComputer Algorithms. Computer Science Press.\nHorswill, I. (2000).\nFunctional programming of\nbehavior-based systems. Autonomous Robots, 9, 83–\n93.\nHorvitz, E. J. (1987). Problem-solving design: Rea-\nsoning about computational value, trade-offs, and re-\nsources. In Proc. Second Annual NASA Research Fo-\nrum, pp. 26–43.\nHorvitz, E. J. (1989). Rational metareasoning and\ncompilation for optimizing decisions under bounded\nresources. In Proc. Computational Intelligence 89.\nAssociation for Computing Machinery.\nHorvitz, E. J. and Barry, M. (1995). Display of in-\nformation for time-critical decision making. In UAI-\n95, pp. 296–305.\nHorvitz, E. J., Breese, J. S., Heckerman, D., and\nHovel, D. (1998). The Lumiere project: Bayesian\nuser modeling for inferring the goals and needs of\nsoftware users. In UAI-98, pp. 256–265.\nHorvitz, E. J., Breese, J. S., and Henrion, M. (1988).\nDecision theory in expert systems and artiﬁcial intel-\nligence. IJAR, 2, 247–302.\nHorvitz, E. J. and Breese, J. S. (1996). Ideal parti-\ntion of resources for metareasoning. In AAAI-96, pp.\n1229–1234.\nHorvitz, E. J. and Heckerman, D. (1986). The incon-\nsistent use of measures of certainty in artiﬁcial intel-\nligence research. In Kanal, L. N. and Lemmer, J. F.\n(Eds.), UAI 2, pp. 137–151. Elsevier/North-Holland.\nHorvitz, E. J., Heckerman, D., and Langlotz, C. P.\n(1986). A framework for comparing alternative for-\nmalisms for plausible reasoning. In AAAI-86, Vol. 1,\npp. 210–214.\nHoward, R. A. (1960). Dynamic Programming and\nMarkov Processes. MIT Press.\nHoward, R. A. (1966). Information value theory.\nIEEE Transactions on Systems Science and Cyber-\nnetics, SSC-2, 22–26.\nHoward, R. A. (1977). Risk preference. In Howard,\nR. A. and Matheson, J. E. (Eds.), Readings in De-\ncision Analysis, pp. 429–465. Decision Analysis\nGroup, SRI International.\nHoward, R. A. (1989). Microrisks for medical de-\ncision analysis.\nInt. J. Technology Assessment in\nHealth Care, 5, 357–370.\nHoward, R. A. and Matheson, J. E. (1984). Inﬂu-\nence diagrams.\nIn Howard, R. A. and Matheson,\nJ. E. (Eds.), Readings on the Principles and Appli-\ncations of Decision Analysis, pp. 721–762. Strategic\nDecisions Group.\nHowe, D. (1987). The computational behaviour of\ngirard’s paradox. In LICS-87, pp. 205–214.\nHsu, F.-H. (2004). Behind Deep Blue: Building the\nComputer that Defeated the World Chess Champion.\nPrinceton University Press.\nHsu, F.-H., Anantharaman, T. S., Campbell, M. S.,",
  "girard’s paradox. In LICS-87, pp. 205–214.\nHsu, F.-H. (2004). Behind Deep Blue: Building the\nComputer that Defeated the World Chess Champion.\nPrinceton University Press.\nHsu, F.-H., Anantharaman, T. S., Campbell, M. S.,\nand Nowatzyk, A. (1990). A grandmaster chess ma-\nchine. Scientiﬁc American, 263(4), 44–50.\nHu, J. and Wellman, M. P. (1998). Multiagent re-\ninforcement learning: Theoretical framework and an\nalgorithm. In ICML-98, pp. 242–250.\nHu, J. and Wellman, M. P. (2003). Nash q-learning\nfor general-sum stochastic games. JMLR, 4, 1039–\n1069.\nHuang, T., Koller, D., Malik, J., Ogasawara, G.,\nRao, B., Russell, S. J., and Weber, J. (1994). Au-\ntomatic symbolic trafﬁc scene analysis using belief\nnetworks. In AAAI-94, pp. 966–972.\nHuang, T. and Russell, S. J. (1998). Object iden-\ntiﬁcation: A Bayesian analysis with application to\ntrafﬁc surveillance. AIJ, 103, 1–17.\nHuang, X. D., Acero, A., and Hon, H. (2001). Spo-\nken Language Processing. Prentice Hall.\nHubel, D. H. (1988). Eye, Brain, and Vision. W. H.\nFreeman.\nHuddleston, R. D. and Pullum, G. K. (2002).\nThe Cambridge Grammar of the English Language.\nCambridge University Press.\nHuffman, D. A. (1971). Impossible objects as non-\nsense sentences.\nIn Meltzer, B. and Michie, D.\n(Eds.), Machine Intelligence 6, pp. 295–324. Edin-\nburgh University Press.\nHughes, B. D. (1995). Random Walks and Random\nEnvironments, Vol. 1: Random Walks. Oxford Uni-\nversity Press.\nHughes, G. E. and Cresswell, M. J. (1996). A New\nIntroduction to Modal Logic. Routledge.\nHuhns, M. N. and Singh, M. P. (Eds.). (1998). Read-\nings in Agents. Morgan Kaufmann.\nHume, D. (1739). A Treatise of Human Nature (2nd\nedition). Republished by Oxford University Press,\n1978, Oxford, UK.\nHumphrys, M. (2008). How my program passed the\nturing test. In Epstein, R., Roberts, G., and Beber, G.\n(Eds.), Parsing the Turing Test. Springer.\nHunsberger, L. and Grosz, B. J. (2000). A com-\nbinatorial auction for collaborative planning. In Int.\nConference on Multi-Agent Systems (ICMAS-2000).\nHunt, W. and Brock, B. (1992). A formal HDL and\nits use in the FM9001 veriﬁcation.\nPhilosophical\nTransactions of the Royal Society of London, 339.\nHunter, L. and States, D. J. (1992). Bayesian clas-\nsiﬁcation of protein structure.\nIEEE Expert, 7(4),\n67–75.\nHurst, M. (2000). The Interpretation of Text in Ta-\nbles. Ph.D. thesis, Edinburgh.\nHurwicz, L. (1973). The design of mechanisms for\nresource allocation. American Economic Review Pa-\npers and Proceedings, 63(1), 1–30.",
  "IEEE Expert, 7(4),\n67–75.\nHurst, M. (2000). The Interpretation of Text in Ta-\nbles. Ph.D. thesis, Edinburgh.\nHurwicz, L. (1973). The design of mechanisms for\nresource allocation. American Economic Review Pa-\npers and Proceedings, 63(1), 1–30.\nHusmeier, D. (2003).\nSensitivity and speciﬁcity\nof inferring genetic regulatory interactions from mi-\ncroarray experiments with dynamic bayesian net-\nworks. Bioinformatics, 19(17), 2271–2282.\nHuth, M. and Ryan, M. (2004).\nLogic in com-\nputer science: modelling and reasoning about sys-\ntems (2nd edition). Cambridge University Press.\nHuttenlocher, D. and Ullman, S. (1990). Recogniz-\ning solid objects by alignment with an image. IJCV,\n5(2), 195–212.\nHuygens, C. (1657). De ratiociniis in ludo aleae. In\nvan Schooten, F. (Ed.), Exercitionum Mathematico-\nrum. Elsevirii, Amsterdam. Translated into English\nby John Arbuthnot (1692).\nHuyn, N., Dechter, R., and Pearl, J. (1980). Proba-\nbilistic analysis of the complexity of A*. AIJ, 15(3),\n241–254.\nHwa, R. (1998). An empirical evaluation of proba-\nbilistic lexicalized tree insertion grammars. In ACL-\n98, pp. 557–563.\nHwang, C. H. and Schubert, L. K. (1993). EL: A for-\nmal, yet natural, comprehensive knowledge repre-\nsentation. In AAAI-93, pp. 676–682.\nIngerman, P. Z. (1967). Panini–Backus form sug-\ngested. CACM, 10(3), 137.\nInoue, K. (2001). Inverse entailment for full clausal\ntheories.\nIn LICS-2001 Workshop on Logic and\nLearning. Bibliography\n1077\nIntille, S. and Bobick, A. (1999). A framework for\nrecognizing multi-agent action from visual evidence.\nIn AAAI-99, pp. 518–525.\nIsard, M. and Blake, A. (1996). Contour tracking\nby stochastic propagation of conditional density. In\nECCV, pp. 343–356.\nIwama, K. and Tamaki, S. (2004). Improved upper\nbounds for 3-SAT. In SODA-04.\nJaakkola, T. and Jordan, M. I. (1996). Computing\nupper and lower bounds on likelihoods in intractable\nnetworks. In UAI-96, pp. 340–348. Morgan Kauf-\nmann.\nJaakkola, T., Singh, S. P., and Jordan, M. I. (1995).\nReinforcement learning algorithm for partially ob-\nservable Markov decision problems. In NIPS 7, pp.\n345–352.\nJackson, F. (1982). Epiphenomenal qualia. Philo-\nsophical Quarterly, 32, 127–136.\nJaffar, J. and Lassez, J.-L. (1987). Constraint logic\nprogramming. In Proc. Fourteenth ACM Conference\non Principles of Programming Languages, pp. 111–\n119. Association for Computing Machinery.\nJaffar, J., Michaylov, S., Stuckey, P. J., and Yap,\nR. H. C. (1992). The CLP(R) language and system.",
  "programming. In Proc. Fourteenth ACM Conference\non Principles of Programming Languages, pp. 111–\n119. Association for Computing Machinery.\nJaffar, J., Michaylov, S., Stuckey, P. J., and Yap,\nR. H. C. (1992). The CLP(R) language and system.\nACM Transactions on Programming Languages and\nSystems, 14(3), 339–395.\nJaynes, E. T. (2003). Probability Theory: The Logic\nof Science. Cambridge Univ. Press.\nJefferson, G. (1949). The mind of mechanical man:\nThe Lister Oration delivered at the Royal College\nof Surgeons in England. British Medical Journal,\n1(25), 1105–1121.\nJeffrey, R. C. (1983). The Logic of Decision (2nd\nedition). University of Chicago Press.\nJeffreys, H. (1948). Theory of Probability. Oxford.\nJelinek, F. (1976). Continuous speech recognition\nby statistical methods. Proc. IEEE, 64(4), 532–556.\nJelinek, F. (1997). Statistical Methods for Speech\nRecognition. MIT Press.\nJelinek, F. and Mercer, R. L. (1980). Interpolated\nestimation of Markov source parameters from sparse\ndata. In Proc. Workshop on Pattern Recognition in\nPractice, pp. 381–397.\nJennings, H. S. (1906). Behavior of the Lower Or-\nganisms. Columbia University Press.\nJenniskens, P., Betlem, H., Betlem, J., and Barifaijo,\nE. (1994). The Mbale meteorite shower. Meteoritics,\n29(2), 246–254.\nJensen, F. V. (2001). Bayesian Networks and Deci-\nsion Graphs. Springer-Verlag.\nJensen, F. V. (2007). Bayesian Networks and Deci-\nsion Graphs. Springer-Verlag.\nJevons, W. S. (1874).\nThe Principles of Science.\nRoutledge/Thoemmes Press, London.\nJi, S., Parr, R., Li, H., Liao, X., and Carin, L. (2007).\nPoint-based policy iteration. In AAAI-07.\nJimenez, P. and Torras, C. (2000). An efﬁcient al-\ngorithm for searching implicit AND/OR graphs with\ncycles. AIJ, 124(1), 1–30.\nJoachims, T. (2001). A statistical learning model of\ntext classiﬁcation with support vector machines. In\nSIGIR-01, pp. 128–136.\nJohnson, W. W. and Story, W. E. (1879). Notes on\nthe “15” puzzle. American Journal of Mathematics,\n2, 397–404.\nJohnston, M. D. and Adorf, H.-M. (1992). Schedul-\ning with neural networks: The case of the Hubble\nspace telescope.\nComputers and Operations Re-\nsearch, 19(3–4), 209–240.\nJones, N. D., Gomard, C. K., and Sestoft, P. (1993).\nPartial Evaluation and Automatic Program Genera-\ntion. Prentice-Hall.\nJones, R., Laird, J., and Nielsen, P. E. (1998). Auto-\nmated intelligent pilots for combat ﬂight simulation.\nIn AAAI-98, pp. 1047–54.\nJones, R., McCallum, A., Nigam, K., and Riloff, E.\n(1999).",
  "tion. Prentice-Hall.\nJones, R., Laird, J., and Nielsen, P. E. (1998). Auto-\nmated intelligent pilots for combat ﬂight simulation.\nIn AAAI-98, pp. 1047–54.\nJones, R., McCallum, A., Nigam, K., and Riloff, E.\n(1999).\nBootstrapping for text learning tasks.\nIn\nProc. IJCAI-99 Workshop on Text Mining: Founda-\ntions, Techniques, and Applications, pp. 52–63.\nJones, T. (2007). Artiﬁcial Intelligence: A Systems\nApproach. Inﬁnity Science Press.\nJonsson, A., Morris, P., Muscettola, N., Rajan, K.,\nand Smith, B. (2000).\nPlanning in interplanetary\nspace: Theory and practice. In AIPS-00, pp. 177–\n186.\nJordan, M. I. (1995). Why the logistic function?\na tutorial discussion on probabilities and neural net-\nworks.\nComputational cognitive science technical\nreport 9503, Massachusetts Institute of Technology.\nJordan, M. I. (2005). Dirichlet processes, Chinese\nrestaurant processes and all that. Tutorial presenta-\ntion at the NIPS Conference.\nJordan, M. I., Ghahramani, Z., Jaakkola, T., and\nSaul, L. K. (1998). An introduction to variational\nmethods for graphical models. In Jordan, M. I. (Ed.),\nLearning in Graphical Models. Kluwer.\nJouannaud, J.-P. and Kirchner, C. (1991). Solving\nequations in abstract algebras: A rule-based survey\nof uniﬁcation. In Lassez, J.-L. and Plotkin, G. (Eds.),\nComputational Logic, pp. 257–321. MIT Press.\nJudd, J. S. (1990). Neural Network Design and the\nComplexity of Learning. MIT Press.\nJuels, A. and Wattenberg, M. (1996).\nStochastic\nhillclimbing as a baseline method for evaluating ge-\nnetic algorithms. In Touretzky, D. S., Mozer, M. C.,\nand Hasselmo, M. E. (Eds.), NIPS 8, pp. 430–6.\nMIT Press.\nJunker, U. (2003). The logic of ilog (j)conﬁgurator:\nCombining constraint programming with a descrip-\ntion logic. In Proc. IJCAI-03 Conﬁguration Work-\nshop, pp. 13–20.\nJurafsky, D. and Martin, J. H. (2000).\nSpeech\nand Language Processing: An Introduction to Nat-\nural Language Processing, Computational Linguis-\ntics, and Speech Recognition. Prentice-Hall.\nJurafsky, D. and Martin, J. H. (2008).\nSpeech\nand Language Processing: An Introduction to Nat-\nural Language Processing, Computational Linguis-\ntics, and Speech Recognition (2nd edition). Prentice-\nHall.\nKadane, J. B. and Simon, H. A. (1977). Optimal\nstrategies for a class of constrained sequential prob-\nlems. Annals of Statistics, 5, 237–255.\nKadane, J. B. and Larkey, P. D. (1982). Subjective\nprobability and the theory of games. Management\nScience, 28(2), 113–120.\nKaelbling, L. P., Littman, M. L., and Cassandra,",
  "lems. Annals of Statistics, 5, 237–255.\nKadane, J. B. and Larkey, P. D. (1982). Subjective\nprobability and the theory of games. Management\nScience, 28(2), 113–120.\nKaelbling, L. P., Littman, M. L., and Cassandra,\nA. R. (1998). Planning and actiong in partially ob-\nservable stochastic domains. AIJ, 101, 99–134.\nKaelbling, L. P., Littman, M. L., and Moore, A. W.\n(1996). Reinforcement learning: A survey. JAIR, 4,\n237–285.\nKaelbling, L. P. and Rosenschein, S. J. (1990). Ac-\ntion and planning in embedded agents. Robotics and\nAutonomous Systems, 6(1–2), 35–48.\nKager, R. (1999). Optimality Theory. Cambridge\nUniversity Press.\nKahn, H. and Marshall, A. W. (1953). Methods of\nreducing sample size in Monte Carlo computations.\nOperations Research, 1(5), 263–278.\nKahneman, D., Slovic, P., and Tversky, A. (Eds.).\n(1982). Judgment under Uncertainty: Heuristics and\nBiases. Cambridge University Press.\nKahneman, D. and Tversky, A. (1979). Prospect\ntheory: An analysis of decision under risk. Econo-\nmetrica, pp. 263–291.\nKaindl, H. and Khorsand, A. (1994).\nMemory-\nbounded bidirectional search. In AAAI-94, pp. 1359–\n1364.\nKalman, R. (1960). A new approach to linear ﬁlter-\ning and prediction problems. J. Basic Engineering,\n82, 35–46.\nKambhampati, S. (1994). Exploiting causal struc-\nture to control retrieval and reﬁtting during plan\nreuse. Computational Intelligence, 10, 213–244.\nKambhampati, S., Mali, A. D., and Srivastava, B.\n(1998).\nHybrid planning for partially hierarchical\ndomains. In AAAI-98, pp. 882–888.\nKanal, L. N. and Kumar, V. (1988). Search in Arti-\nﬁcial Intelligence. Springer-Verlag.\nKanazawa, K., Koller, D., and Russell, S. J. (1995).\nStochastic simulation algorithms for dynamic prob-\nabilistic networks. In UAI-95, pp. 346–351.\nKantorovich, L. V. (1939). Mathematical methods\nof organizing and planning production. Publishd in\ntranslation in Management Science, 6(4), 366–422,\nJuly 1960.\nKaplan, D. and Montague, R. (1960). A paradox re-\ngained. Notre Dame Journal of Formal Logic, 1(3),\n79–90.\nKarmarkar, N. (1984). A new polynomial-time al-\ngorithm for linear programming. Combinatorica, 4,\n373–395.\nKarp, R. M. (1972). Reducibility among combina-\ntorial problems. In Miller, R. E. and Thatcher, J. W.\n(Eds.), Complexity of Computer Computations, pp.\n85–103. Plenum.\nKartam, N. A. and Levitt, R. E. (1990).\nA\nconstraint-based approach to construction planning\nof multi-story buildings.\nIn Expert Planning Sys-\ntems, pp. 245–250. Institute of Electrical Engineers.",
  "85–103. Plenum.\nKartam, N. A. and Levitt, R. E. (1990).\nA\nconstraint-based approach to construction planning\nof multi-story buildings.\nIn Expert Planning Sys-\ntems, pp. 245–250. Institute of Electrical Engineers.\nKasami, T. (1965). An efﬁcient recognition and syn-\ntax analysis algorithm for context-free languages.\nTech. rep. AFCRL-65-758, Air Force Cambridge\nResearch Laboratory.\nKasparov, G. (1997).\nIBM owes me a rematch.\nTime, 149(21), 66–67.\nKaufmann, M., Manolios, P., and Moore, J. S.\n(2000). Computer-Aided Reasoning: An Approach.\nKluwer.\nKautz, H. (2006). Deconstructing planning as satis-\nﬁability. In AAAI-06.\nKautz, H., McAllester, D. A., and Selman, B.\n(1996). Encoding plans in propositional logic. In\nKR-96, pp. 374–384.\nKautz, H. and Selman, B. (1992). Planning as satis-\nﬁability. In ECAI-92, pp. 359–363. 1078\nBibliography\nKautz, H. and Selman, B. (1998). BLACKBOX: A\nnew approach to the application of theorem proving\nto problem solving. Working Notes of the AIPS-98\nWorkshop on Planning as Combinatorial Search.\nKavraki, L., Svestka, P., Latombe, J.-C., and Over-\nmars, M. (1996).\nProbabilistic roadmaps for path\nplanning in high-dimensional conﬁguration spaces.\nIEEE Transactions on Robotics and Automation,\n12(4), 566–580.\nKay, M., Gawron, J. M., and Norvig, P. (1994).\nVerbmobil: A Translation System for Face-To-Face\nDialog. CSLI Press.\nKearns, M. (1990). The Computational Complexity\nof Machine Learning. MIT Press.\nKearns, M., Mansour, Y., and Ng, A. Y. (2000). Ap-\nproximate planning in large POMDPs via reusable\ntrajectories. In Solla, S. A., Leen, T. K., and M¨uller,\nK.-R. (Eds.), NIPS 12. MIT Press.\nKearns, M. and Singh, S. P. (1998). Near-optimal\nreinforcement learning in polynomial time.\nIn\nICML-98, pp. 260–268.\nKearns, M. and Vazirani, U. (1994). An Introduction\nto Computational Learning Theory. MIT Press.\nKearns, M. and Mansour, Y. (1998). A fast, bottom-\nup decision tree pruning algorithm with near-optimal\ngeneralization. In ICML-98, pp. 269–277.\nKebeasy, R. M., Hussein, A. I., and Dahy, S. A.\n(1998). Discrimination between natural earthquakes\nand nuclear explosions using the Aswan Seismic\nNetwork. Annali di Geoﬁsica, 41(2), 127–140.\nKeeney, R. L. (1974). Multiplicative utility func-\ntions. Operations Research, 22, 22–34.\nKeeney, R. L. and Raiffa, H. (1976). Decisions with\nMultiple Objectives: Preferences and Value Trade-\noffs. Wiley.\nKemp, M. (Ed.). (1989). Leonardo on Painting: An\nAnthology of Writings. Yale University Press.",
  "tions. Operations Research, 22, 22–34.\nKeeney, R. L. and Raiffa, H. (1976). Decisions with\nMultiple Objectives: Preferences and Value Trade-\noffs. Wiley.\nKemp, M. (Ed.). (1989). Leonardo on Painting: An\nAnthology of Writings. Yale University Press.\nKephart, J. O. and Chess, D. M. (2003). The vision\nof autonomic computing.\nIEEE Computer, 36(1),\n41–50.\nKersting, K., Raedt, L. D., and Kramer, S. (2000).\nInterpreting bayesian logic programs. In Proc. AAAI-\n2000 Workshop on Learning Statistical Models from\nRelational Data.\nKessler, B., Nunberg, G., and Sch¨utze, H. (1997).\nAutomatic detection of text genre.\nCoRR, cmp-\nlg/9707002.\nKeynes, J. M. (1921).\nA Treatise on Probability.\nMacmillan.\nKhare, R. (2006). Microformats: The next (small)\nthing on the semantic web. IEEE Internet Comput-\ning, 10(1), 68–75.\nKhatib, O. (1986). Real-time obstacle avoidance for\nrobot manipulator and mobile robots. Int. J. Robotics\nResearch, 5(1), 90–98.\nKhmelev, D. V. and Tweedie, F. J. (2001). Using\nMarkov chains for identiﬁcation of writer. Literary\nand Linguistic Computing, 16(3), 299–307.\nKietz, J.-U. and Duzeroski, S. (1994).\nInductive\nlogic programming and learnability. SIGART Bul-\nletin, 5(1), 22–32.\nKilgarriff, A. and Grefenstette, G. (2006). Intro-\nduction to the special issue on the web as corpus.\nComputational Linguistics, 29(3), 333–347.\nKim, J. H. (1983). CONVINCE: A Conversational\nInference Consolidation Engine. Ph.D. thesis, De-\npartment of Computer Science, University of Cali-\nfornia at Los Angeles.\nKim, J. H. and Pearl, J. (1983). A computational\nmodel for combined causal and diagnostic reasoning\nin inference systems. In IJCAI-83, pp. 190–193.\nKim, J.-H., Lee, C.-H., Lee, K.-H., and Kup-\npuswamy, N. (2007). Evolving personality of a ge-\nnetic robot in ubiquitous environment. In The 16th\nIEEE International Symposium on Robot and Hu-\nman interactive Communication, pp. 848–853.\nKing, R. D., Rowland, J., Oliver, S. G., and Young,\nM. (2009).\nThe automation of science.\nScience,\n324(5923), 85–89.\nKirk, D. E. (2004). Optimal Control Theory: An\nIntroduction. Dover.\nKirkpatrick, S., Gelatt, C. D., and Vecchi, M. P.\n(1983). Optimization by simulated annealing. Sci-\nence, 220, 671–680.\nKister, J., Stein, P., Ulam, S., Walden, W., and\nWells, M. (1957). Experiments in chess. JACM, 4,\n174–177.\nKisynski, J. and Poole, D. (2009). Lifted aggrega-\ntion in directed ﬁrst-order probabilistic models. In\nIJCAI-09.\nKitano, H., Asada, M., Kuniyoshi, Y., Noda, I., and",
  "Wells, M. (1957). Experiments in chess. JACM, 4,\n174–177.\nKisynski, J. and Poole, D. (2009). Lifted aggrega-\ntion in directed ﬁrst-order probabilistic models. In\nIJCAI-09.\nKitano, H., Asada, M., Kuniyoshi, Y., Noda, I., and\nOsawa, E. (1997a). RoboCup: The robot world cup\ninitiative. In Proc. First International Conference on\nAutonomous Agents, pp. 340–347.\nKitano, H., Asada, M., Kuniyoshi, Y., Noda, I., Os-\nawa, E., and Matsubara, H. (1997b). RoboCup: A\nchallenge problem for AI. AIMag, 18(1), 73–85.\nKjaerulff, U. (1992). A computational scheme for\nreasoning in dynamic probabilistic networks.\nIn\nUAI-92, pp. 121–129.\nKlein, D. and Manning, C. (2001). Parsing with tree-\nbank grammars: Empirical bounds, theoretical mod-\nels, and the structure of the Penn treebank. In ACL-\n01.\nKlein, D. and Manning, C. (2003). A* parsing: Fast\nexact Viterbi parse selection. In HLT-NAACL-03, pp.\n119–126.\nKlein, D., Smarr, J., Nguyen, H., and Manning, C.\n(2003).\nNamed entity recognition with character-\nlevel models. In Conference on Natural Language\nLearning (CoNLL).\nKleinberg, J. M. (1999). Authoritative sources in a\nhyperlinked environment. JACM, 46(5), 604–632.\nKlemperer, P. (2002). What really matters in auc-\ntion design. J. Economic Perspectives, 16(1).\nKneser, R. and Ney, H. (1995). Improved backing-\noff for M-gram language modeling. In ICASSP-95,\npp. 181–184.\nKnight, K. (1999). A statistical MT tutorial work-\nbook. Prepared in connection with the Johns Hop-\nkins University summer workshop.\nKnuth, D. E. (1964). Representing numbers using\nonly one 4. Mathematics Magazine, 37(Nov/Dec),\n308–310.\nKnuth, D. E. (1968). Semantics for context-free lan-\nguages. Mathematical Systems Theory, 2(2), 127–\n145.\nKnuth, D. E. (1973). The Art of Computer Program-\nming (second edition)., Vol. 2: Fundamental Algo-\nrithms. Addison-Wesley.\nKnuth, D. E. (1975).\nAn analysis of alpha–beta\npruning. AIJ, 6(4), 293–326.\nKnuth, D. E. and Bendix, P. B. (1970).\nSimple\nword problems in universal algebras. In Leech, J.\n(Ed.), Computational Problems in Abstract Algebra,\npp. 263–267. Pergamon.\nKocsis, L. and Szepesvari, C. (2006). Bandit-based\nMonte-Carlo planning. In ECML-06.\nKoditschek, D. (1987). Exact robot navigation by\nmeans of potential functions: some topological con-\nsiderations. In ICRA-87, Vol. 1, pp. 1–6.\nKoehler, J., Nebel, B., Hoffmann, J., and Dimopou-\nlos, Y. (1997). Extending planning graphs to an ADL\nsubset. In ECP-97, pp. 273–285.\nKoehn, P. (2009). Statistical Machine Translation.",
  "siderations. In ICRA-87, Vol. 1, pp. 1–6.\nKoehler, J., Nebel, B., Hoffmann, J., and Dimopou-\nlos, Y. (1997). Extending planning graphs to an ADL\nsubset. In ECP-97, pp. 273–285.\nKoehn, P. (2009). Statistical Machine Translation.\nCambridge University Press.\nKoenderink, J. J. (1990). Solid Shape. MIT Press.\nKoenig, S. (1991).\nOptimal probabilistic and\ndecision-theoretic planning using Markovian deci-\nsion theory. Master’s report, Computer Science Di-\nvision, University of California.\nKoenig, S. (2000).\nExploring unknown environ-\nments with real-time search or reinforcement learn-\ning. In Solla, S. A., Leen, T. K., and M¨uller, K.-R.\n(Eds.), NIPS 12. MIT Press.\nKoenig, S. (2001). Agent-centered search. AIMag,\n22(4), 109–131.\nKoller, D., Meggido, N., and von Stengel, B.\n(1996). Efﬁcient computation of equilibria for ex-\ntensive two-person games.\nGames and Economic\nBehaviour, 14(2), 247–259.\nKoller, D. and Pfeffer, A. (1997). Representations\nand solutions for game-theoretic problems.\nAIJ,\n94(1–2), 167–215.\nKoller, D. and Pfeffer, A. (1998).\nProbabilistic\nframe-based systems. In AAAI-98, pp. 580–587.\nKoller, D. and Friedman, N. (2009).\nProbabilis-\ntic Graphical Models: Principles and Techniques.\nMIT Press.\nKoller, D. and Milch, B. (2003). Multi-agent inﬂu-\nence diagrams for representing and solving games.\nGames and Economic Behavior, 45, 181–221.\nKoller, D. and Parr, R. (2000). Policy iteration for\nfactored MDPs. In UAI-00, pp. 326–334.\nKoller, D. and Sahami, M. (1997). Hierarchically\nclassifying documents using very few words.\nIn\nICML-97, pp. 170–178.\nKolmogorov, A. N. (1941). Interpolation und ex-\ntrapolation von stationaren zufalligen folgen. Bul-\nletin of the Academy of Sciences of the USSR, Ser.\nMath. 5, 3–14.\nKolmogorov, A. N. (1950). Foundations of the The-\nory of Probability. Chelsea.\nKolmogorov, A. N. (1963). On tables of random\nnumbers. Sankhya, the Indian Journal of Statistics,\nSeries A 25.\nKolmogorov, A. N. (1965). Three approaches to the\nquantitative deﬁnition of information. Problems in\nInformation Transmission, 1(1), 1–7.\nKolodner, J. (1983).\nReconstructive memory: A\ncomputer model. Cognitive Science, 7, 281–328.\nKolodner, J. (1993). Case-Based Reasoning. Mor-\ngan Kaufmann.\nKondrak, G. and van Beek, P. (1997). A theoretical\nevaluation of selected backtracking algorithms. AIJ,\n89, 365–387.\nKonolige, K. (1997). COLBERT: A language for re-\nactive control in Saphira. In K¨unstliche Intelligenz:\nAdvances in Artiﬁcial Intelligence, LNAI, pp. 31–\n52.",
  "evaluation of selected backtracking algorithms. AIJ,\n89, 365–387.\nKonolige, K. (1997). COLBERT: A language for re-\nactive control in Saphira. In K¨unstliche Intelligenz:\nAdvances in Artiﬁcial Intelligence, LNAI, pp. 31–\n52.\nKonolige, K. (2004). Large-scale map-making. In\nAAAI-04, pp. 457–463. Bibliography\n1079\nKonolige, K. (1982).\nA ﬁrst order formalization\nof knowledge and action for a multi-agent planning\nsystem. In Hayes, J. E., Michie, D., and Pao, Y.-H.\n(Eds.), Machine Intelligence 10. Ellis Horwood.\nKonolige, K. (1994). Easy to be hard: Difﬁcult prob-\nlems for greedy algorithms. In KR-94, pp. 374–378.\nKoo, T., Carreras, X., and Collins, M. (2008). Sim-\nple semi-supervised dependency parsing. In ACL-08.\nKoopmans, T. C. (1972). Representation of pref-\nerence orderings over time.\nIn McGuire, C. B.\nand Radner, R. (Eds.), Decision and Organization.\nElsevier/North-Holland.\nKorb, K. B. and Nicholson, A. (2003). Bayesian\nArtiﬁcial Intelligence. Chapman and Hall.\nKorb, K. B., Nicholson, A., and Jitnah, N. (1999).\nBayesian poker. In UAI-99.\nKorf, R. E. (1985a). Depth-ﬁrst iterative-deepening:\nan optimal admissible tree search. AIJ, 27(1), 97–\n109.\nKorf, R. E. (1985b). Iterative-deepening A*: An op-\ntimal admissible tree search. In IJCAI-85, pp. 1034–\n1036.\nKorf, R. E. (1987). Planning as search: A quantita-\ntive approach. AIJ, 33(1), 65–88.\nKorf, R. E. (1990). Real-time heuristic search. AIJ,\n42(3), 189–212.\nKorf, R. E. (1993). Linear-space best-ﬁrst search.\nAIJ, 62(1), 41–78.\nKorf, R. E. (1995).\nSpace-efﬁcient search algo-\nrithms. ACM Computing Surveys, 27(3), 337–339.\nKorf, R. E. and Chickering, D. M. (1996). Best-ﬁrst\nminimax search. AIJ, 84(1–2), 299–337.\nKorf, R. E. and Felner, A. (2002). Disjoint pattern\ndatabase heuristics. AIJ, 134(1–2), 9–22.\nKorf, R. E., Reid, M., and Edelkamp, S. (2001).\nTime complexity of iterative-deepening-A*.\nAIJ,\n129, 199–218.\nKorf, R. E. and Zhang, W. (2000).\nDivide-and-\nconquer frontier search applied to optimal sequence\nalignment. In American Association for Artiﬁcial In-\ntelligence, pp. 910–916.\nKorf, R. E. (2008). Linear-time disk-based implicit\ngraph search. JACM, 55(6).\nKorf, R. E. and Schultze, P. (2005).\nLarge-scale\nparallel breadth-ﬁrst search. In AAAI-05, pp. 1380–\n1385.\nKotok, A. (1962). A chess playing program for the\nIBM 7090. AI project memo 41, MIT Computation\nCenter.\nKoutsoupias, E. and Papadimitriou, C. H. (1992).\nOn the greedy algorithm for satisﬁability. Informa-\ntion Processing Letters, 43(1), 53–55.",
  "1385.\nKotok, A. (1962). A chess playing program for the\nIBM 7090. AI project memo 41, MIT Computation\nCenter.\nKoutsoupias, E. and Papadimitriou, C. H. (1992).\nOn the greedy algorithm for satisﬁability. Informa-\ntion Processing Letters, 43(1), 53–55.\nKowalski, R. (1974). Predicate logic as a program-\nming language. In Proc. IFIP Congress, pp. 569–\n574.\nKowalski, R. (1979). Logic for Problem Solving.\nElsevier/North-Holland.\nKowalski, R. (1988). The early years of logic pro-\ngramming. CACM, 31, 38–43.\nKowalski, R. and Sergot, M. (1986). A logic-based\ncalculus of events.\nNew Generation Computing,\n4(1), 67–95.\nKoza, J. R. (1992). Genetic Programming: On the\nProgramming of Computers by Means of Natural Se-\nlection. MIT Press.\nKoza, J. R. (1994). Genetic Programming II: Auto-\nmatic discovery of reusable programs. MIT Press.\nKoza, J. R., Bennett, F. H., Andre, D., and Keane,\nM. A. (1999). Genetic Programming III: Darwinian\ninvention and problem solving. Morgan Kaufmann.\nKraus, S., Ephrati, E., and Lehmann, D. (1991).\nNegotiation in a non-cooperative environment. AIJ,\n3(4), 255–281.\nKrause, A. and Guestrin, C. (2009). Optimal value\nof information in graphical models. JAIR, 35, 557–\n591.\nKrause, A., McMahan, B., Guestrin, C., and Gupta,\nA. (2008). Robust submodular observation selection.\nJMLR, 9, 2761–2801.\nKripke, S. A. (1963). Semantical considerations on\nmodal logic. Acta Philosophica Fennica, 16, 83–94.\nKrogh, A., Brown, M., Mian, I. S., Sjolander, K.,\nand Haussler, D. (1994).\nHidden Markov models\nin computational biology: Applications to protein\nmodeling. J. Molecular Biology, 235, 1501–1531.\nK¨ubler, S., McDonald, R., and Nivre, J. (2009). De-\npendency Parsing. Morgan Claypool.\nKuhn, H. W. (1953). Extensive games and the prob-\nlem of information.\nIn Kuhn, H. W. and Tucker,\nA. W. (Eds.), Contributions to the Theory of Games\nII. Princeton University Press.\nKuhn, H. W. (1955).\nThe Hungarian method for\nthe assignment problem. Naval Research Logistics\nQuarterly, 2, 83–97.\nKuipers, B. J. (1985). Qualitative simulation. In Bo-\nbrow, D. (Ed.), Qualitative Reasoning About Physi-\ncal Systems, pp. 169–203. MIT Press.\nKuipers, B. J. and Levitt, T. S. (1988). Navigation\nand mapping in large-scale space. AIMag, 9(2), 25–\n43.\nKuipers, B. J. (2001). Qualitative simulation. In\nMeyers, R. A. (Ed.), Encyclopeida of Physical Sci-\nence and Technology. Academic Press.\nKumar, P. R. and Varaiya, P. (1986). Stochastic Sys-\ntems: Estimation, Identiﬁcation, and Adaptive Con-",
  "43.\nKuipers, B. J. (2001). Qualitative simulation. In\nMeyers, R. A. (Ed.), Encyclopeida of Physical Sci-\nence and Technology. Academic Press.\nKumar, P. R. and Varaiya, P. (1986). Stochastic Sys-\ntems: Estimation, Identiﬁcation, and Adaptive Con-\ntrol. Prentice-Hall.\nKumar, V. (1992). Algorithms for constraint satis-\nfaction problems: A survey. AIMag, 13(1), 32–44.\nKumar, V. and Kanal, L. N. (1983).\nA general\nbranch and bound formulation for understanding and\nsynthesizing and/or tree search procedures. AIJ, 21,\n179–198.\nKumar, V. and Kanal, L. N. (1988). The CDP: A\nunifying formulation for heuristic search, dynamic\nprogramming, and branch-and-bound.\nIn Kanal,\nL. N. and Kumar, V. (Eds.), Search in Artiﬁcial In-\ntelligence, chap. 1, pp. 1–27. Springer-Verlag.\nKumar, V., Nau, D. S., and Kanal, L. N. (1988). A\ngeneral branch-and-bound formulation for AND/OR\ngraph and game tree search. In Kanal, L. N. and\nKumar, V. (Eds.), Search in Artiﬁcial Intelligence,\nchap. 3, pp. 91–130. Springer-Verlag.\nKurien, J., Nayak, P., and Smith, D. E. (2002).\nFragment-based conformant planning. In AIPS-02.\nKurzweil, R. (1990).\nThe Age of Intelligent Ma-\nchines. MIT Press.\nKurzweil, R. (2005).\nThe Singularity is Near.\nViking.\nKwok, C., Etzioni, O., and Weld, D. S. (2001). Scal-\ning question answering to the web. In Proc. 10th\nInternational Conference on the World Wide Web.\nKyburg, H. E. and Teng, C.-M. (2006). Nonmono-\ntonic logic and statistical inference. Computational\nIntelligence, 22(1), 26–51.\nKyburg, H. E. (1977). Randomness and the right\nreference class. J. Philosophy, 74(9), 501–521.\nKyburg, H. E. (1983). The reference class. Philos-\nophy of Science, 50, 374–397.\nLa Mettrie, J. O. (1748).\nL’homme machine.\nE. Luzac, Leyde, France.\nLa Mura, P. and Shoham, Y. (1999). Expected util-\nity networks. In UAI-99, pp. 366–373.\nLaborie, P. (2003). Algorithms for propagating re-\nsource constraints in AI planning and scheduling.\nAIJ, 143(2), 151–188.\nLadkin, P. (1986a). Primitives and units for time\nspeciﬁcation. In AAAI-86, Vol. 1, pp. 354–359.\nLadkin, P. (1986b). Time representation: a taxon-\nomy of interval relations. In AAAI-86, Vol. 1, pp.\n360–366.\nLafferty, J., McCallum, A., and Pereira, F. (2001).\nConditional random ﬁelds: Probabilistic models for\nsegmenting and labeling sequence data. In ICML-01.\nLafferty, J. and Zhai, C. (2001). Probabilistic rele-\nvance models based on document and query genera-\ntion. In Proc. Workshop on Language Modeling and\nInformation Retrieval.",
  "segmenting and labeling sequence data. In ICML-01.\nLafferty, J. and Zhai, C. (2001). Probabilistic rele-\nvance models based on document and query genera-\ntion. In Proc. Workshop on Language Modeling and\nInformation Retrieval.\nLagoudakis, M. G. and Parr, R. (2003).\nLeast-\nsquares policy iteration. JMLR, 4, 1107–1149.\nLaird, J., Newell, A., and Rosenbloom, P. S. (1987).\nSOAR: An architecture for general intelligence. AIJ,\n33(1), 1–64.\nLaird, J., Rosenbloom, P. S., and Newell, A. (1986).\nChunking in Soar: The anatomy of a general learn-\ning mechanism. Machine Learning, 1, 11–46.\nLaird, J. (2008). Extending the Soar cognitive ar-\nchitecture. In Artiﬁcial General Intelligence Confer-\nence.\nLakoff, G. (1987).\nWomen, Fire, and Dangerous\nThings: What Categories Reveal About the Mind.\nUniversity of Chicago Press.\nLakoff, G. and Johnson, M. (1980). Metaphors We\nLive By. University of Chicago Press.\nLakoff, G. and Johnson, M. (1999). Philosophy in\nthe Flesh : The Embodied Mind and Its Challenge to\nWestern Thought. Basic Books.\nLam, J. and Greenspan, M. (2008). Eye-in-hand vi-\nsual servoing for accurate shooting in pool robotics.\nIn 5th Canadian Conference on Computer and Robot\nVision.\nLamarck, J. B. (1809).\nPhilosophie zoologique.\nChez Dentu et L’Auteur, Paris.\nLandhuis, E. (2004).\nLifelong debunker takes\non arbiter of neutral choices:\nMagician-turned-\nmathematician uncovers bias in a ﬂip of a coin. Stan-\nford Report.\nLangdon, W. and Poli, R. (2002). Foundations of\nGenetic Programming. Springer.\nLangley, P., Simon, H. A., Bradshaw, G. L., and\nZytkow, J. M. (1987). Scientiﬁc Discovery: Com-\nputational Explorations of the Creative Processes.\nMIT Press.\nLangton,\nC.\n(Ed.). (1995).\nArtiﬁcial\nLife.\nMIT Press.\nLaplace, P. (1816).\nEssai philosophique sur les\nprobabilit´es (3rd edition).\nCourcier Imprimeur,\nParis. 1080\nBibliography\nLaptev, I. and Perez, P. (2007). Retrieving actions\nin movies. In ICCV, pp. 1–8.\nLari, K. and Young, S. J. (1990). The estimation\nof stochastic context-free grammars using the inside-\noutside algorithm. Computer Speech and Language,\n4, 35–56.\nLarra˜naga, P., Kuijpers, C., Murga, R., Inza, I., and\nDizdarevic, S. (1999).\nGenetic algorithms for the\ntravelling salesman problem: A review of represen-\ntations and operators. Artiﬁcial Intelligence Review,\n13, 129–170.\nLarson, S. C. (1931). The shrinkage of the coef-\nﬁcient of multiple correlation. J. Educational Psy-\nchology, 22, 45–55.\nLaskey, K. B. (2008). MEBN: A language for ﬁrst-",
  "tations and operators. Artiﬁcial Intelligence Review,\n13, 129–170.\nLarson, S. C. (1931). The shrinkage of the coef-\nﬁcient of multiple correlation. J. Educational Psy-\nchology, 22, 45–55.\nLaskey, K. B. (2008). MEBN: A language for ﬁrst-\norder bayesian knowledge bases. AIJ, 172, 140–178.\nLatombe, J.-C. (1991).\nRobot Motion Planning.\nKluwer.\nLauritzen, S. (1995). The EM algorithm for graphi-\ncal association models with missing data. Computa-\ntional Statistics and Data Analysis, 19, 191–201.\nLauritzen, S. (1996). Graphical models. Oxford\nUniversity Press.\nLauritzen, S., Dawid, A. P., Larsen, B., and Leimer,\nH. (1990).\nIndependence properties of directed\nMarkov ﬁelds. Networks, 20(5), 491–505.\nLauritzen, S. and Spiegelhalter, D. J. (1988). Local\ncomputations with probabilities on graphical struc-\ntures and their application to expert systems. J. Royal\nStatistical Society, B 50(2), 157–224.\nLauritzen, S. and Wermuth, N. (1989). Graphical\nmodels for associations between variables, some of\nwhich are qualitative and some quantitative. Annals\nof Statistics, 17, 31–57.\nLaValle, S. (2006).\nPlanning Algorithms.\nCam-\nbridge University Press.\nLavrauc, N. and Duzeroski, S. (1994).\nInductive\nLogic Programming: Techniques and Applications.\nEllis Horwood.\nLawler, E. L., Lenstra, J. K., Kan, A., and Shmoys,\nD. B. (1992). The Travelling Salesman Problem. Wi-\nley Interscience.\nLawler, E. L., Lenstra, J. K., Kan, A., and Shmoys,\nD. B. (1993).\nSequencing and scheduling: Algo-\nrithms and complexity.\nIn Graves, S. C., Zipkin,\nP. H., and Kan, A. H. G. R. (Eds.), Logistics of Pro-\nduction and Inventory: Handbooks in Operations\nResearch and Management Science, Volume 4, pp.\n445–522. North-Holland.\nLawler, E. L. and Wood, D. E. (1966). Branch-and-\nbound methods: A survey.\nOperations Research,\n14(4), 699–719.\nLazanas, A. and Latombe, J.-C. (1992). Landmark-\nbased robot navigation. In AAAI-92, pp. 816–822.\nLeCun, Y., Jackel, L., Boser, B., and Denker, J.\n(1989).\nHandwritten digit recognition: Applica-\ntions of neural network chips and automatic learn-\ning. IEEE Communications Magazine, 27(11), 41–\n46.\nLeCun, Y., Jackel, L., Bottou, L., Brunot, A.,\nCortes, C., Denker, J., Drucker, H., Guyon, I.,\nMuller, U., Sackinger, E., Simard, P., and Vapnik,\nV. N. (1995). Comparison of learning algorithms for\nhandwritten digit recognition. In Int. Conference on\nArtiﬁcial Neural Networks, pp. 53–60.\nLeech, G., Rayson, P., and Wilson, A. (2001). Word\nFrequencies in Written and Spoken English: Based",
  "V. N. (1995). Comparison of learning algorithms for\nhandwritten digit recognition. In Int. Conference on\nArtiﬁcial Neural Networks, pp. 53–60.\nLeech, G., Rayson, P., and Wilson, A. (2001). Word\nFrequencies in Written and Spoken English: Based\non the British National Corpus. Longman.\nLegendre, A. M. (1805). Nouvelles m´ethodes pour\nla d´etermination des orbites des com`etes. .\nLehrer, J. (2009). How We Decide. Houghton Mif-\nﬂin.\nLenat, D. B. (1983). EURISKO: A program that\nlearns new heuristics and domain concepts: The na-\nture of heuristics, III: Program design and results.\nAIJ, 21(1–2), 61–98.\nLenat, D. B. and Brown, J. S. (1984). Why AM and\nEURISKO appear to work. AIJ, 23(3), 269–294.\nLenat, D. B. and Guha, R. V. (1990). Building Large\nKnowledge-Based Systems: Representation and In-\nference in the CYC Project. Addison-Wesley.\nLeonard, H. S. and Goodman, N. (1940). The cal-\nculus of individuals and its uses. JSL, 5(2), 45–55.\nLeonard, J. and Durrant-Whyte, H. (1992). Directed\nsonar sensing for mobile robot navigation. Kluwer.\nLe´sniewski, S. (1916).\nPodstawy og´olnej teorii\nmnogo´sci. Moscow.\nLettvin, J. Y., Maturana, H. R., McCulloch, W. S.,\nand Pitts, W. (1959). What the frog’s eye tells the\nfrog’s brain. Proc. IRE, 47(11), 1940–1951.\nLetz, R., Schumann, J., Bayerl, S., and Bibel, W.\n(1992).\nSETHEO: A high-performance theorem\nprover. JAR, 8(2), 183–212.\nLevesque, H. J. and Brachman, R. J. (1987). Ex-\npressiveness and tractability in knowledge represen-\ntation and reasoning.\nComputational Intelligence,\n3(2), 78–93.\nLevin, D. A., Peres, Y., and Wilmer, E. L. (2008).\nMarkov Chains and Mixing Times. American Math-\nematical Society.\nLevitt, G. M. (2000). The Turk, Chess Automaton.\nMcFarland and Company.\nLevy, D. (Ed.). (1988a).\nComputer Chess Com-\npendium. Springer-Verlag.\nLevy,\nD. (Ed.). (1988b).\nComputer Games.\nSpringer-Verlag.\nLevy, D. (1989). The million pound bridge program.\nIn Levy, D. and Beal, D. (Eds.), Heuristic Program-\nming in Artiﬁcial Intelligence. Ellis Horwood.\nLevy, D. (2007). Love and Sex with Robots. Harper.\nLewis, D. D. (1998). Naive Bayes at forty: The in-\ndependence assumption in information retrieval. In\nECML-98, pp. 4–15.\nLewis, D. K. (1966). An argument for the identity\ntheory. J. Philosophy, 63(1), 17–25.\nLewis, D. K. (1980). Mad pain and Martian pain. In\nBlock, N. (Ed.), Readings in Philosophy of Psychol-\nogy, Vol. 1, pp. 216–222. Harvard University Press.\nLeyton-Brown, K. and Shoham, Y. (2008). Essen-",
  "theory. J. Philosophy, 63(1), 17–25.\nLewis, D. K. (1980). Mad pain and Martian pain. In\nBlock, N. (Ed.), Readings in Philosophy of Psychol-\nogy, Vol. 1, pp. 216–222. Harvard University Press.\nLeyton-Brown, K. and Shoham, Y. (2008). Essen-\ntials of Game Theory: A Concise, Multidisciplinary\nIntroduction. Morgan Claypool.\nLi, C. M. and Anbulagan (1997). Heuristics based\non unit propagation for satisﬁability problems. In\nIJCAI-97, pp. 366–371.\nLi, M. and Vitanyi, P. M. B. (1993). An Introduc-\ntion to Kolmogorov Complexity and Its Applications.\nSpringer-Verlag.\nLiberatore, P. (1997). The complexity of the lan-\nguage A. Electronic Transactions on Artiﬁcial Intel-\nligence, 1, 13–38.\nLifschitz, V. (2001). Answer set programming and\nplan generation. AIJ, 138(1–2), 39–54.\nLighthill, J. (1973). Artiﬁcial intelligence: A gen-\neral survey. In Lighthill, J., Sutherland, N. S., Need-\nham, R. M., Longuet-Higgins, H. C., and Michie, D.\n(Eds.), Artiﬁcial Intelligence: A Paper Symposium.\nScience Research Council of Great Britain.\nLin, S. (1965). Computer solutions of the travelling\nsalesman problem. Bell Systems Technical Journal,\n44(10), 2245–2269.\nLin, S. and Kernighan, B. W. (1973). An effective\nheuristic algorithm for the travelling-salesman prob-\nlem. Operations Research, 21(2), 498–516.\nLindley, D. V. (1956). On a measure of the infor-\nmation provided by an experiment. Annals of Math-\nematical Statistics, 27(4), 986–1005.\nLindsay, R. K., Buchanan, B. G., Feigenbaum,\nE. A., and Lederberg, J. (1980). Applications of Arti-\nﬁcial Intelligence for Organic Chemistry: The DEN-\nDRAL Project. McGraw-Hill.\nLittman, M. L. (1994). Markov games as a frame-\nwork for multi-agent reinforcement learning.\nIn\nICML-94, pp. 157–163.\nLittman, M. L., Keim, G. A., and Shazeer, N. M.\n(1999).\nSolving crosswords with PROVERB.\nIn\nAAAI-99, pp. 914–915.\nLiu, J. S. and Chen, R. (1998). Sequential Monte\nCarlo methods for dynamic systems.\nJASA, 93,\n1022–1031.\nLivescu, K., Glass, J., and Bilmes, J. (2003). Hidden\nfeature modeling for speech recognition using dy-\nnamic Bayesian networks. In EUROSPEECH-2003,\npp. 2529–2532.\nLivnat, A. and Pippenger, N. (2006). An optimal\nbrain can be composed of conﬂicting agents. PNAS,\n103(9), 3198–3202.\nLocke, J. (1690). An Essay Concerning Human Un-\nderstanding. William Tegg.\nLodge, D. (1984). Small World. Penguin Books.\nLoftus, E. and Palmer, J. (1974). Reconstruction of\nautomobile destruction: An example of the interac-",
  "103(9), 3198–3202.\nLocke, J. (1690). An Essay Concerning Human Un-\nderstanding. William Tegg.\nLodge, D. (1984). Small World. Penguin Books.\nLoftus, E. and Palmer, J. (1974). Reconstruction of\nautomobile destruction: An example of the interac-\ntion between language and memory. J. Verbal Learn-\ning and Verbal Behavior, 13, 585–589.\nLohn, J. D., Kraus, W. F., and Colombano, S. P.\n(2001). Evolutionary optimization of yagi-uda an-\ntennas. In Proc. Fourth International Conference on\nEvolvable Systems, pp. 236–243.\nLongley, N. and Sankaran, S. (2005). The NHL’s\novertime-loss rule: Empirically analyzing the unin-\ntended effects. Atlantic Economic Journal.\nLonguet-Higgins, H. C. (1981). A computer algo-\nrithm for reconstructing a scene from two projec-\ntions. Nature, 293, 133–135.\nLoo, B. T., Condie, T., Garofalakis, M., Gay, D. E.,\nHellerstein, J. M., Maniatis, P., Ramakrishnan, R.,\nRoscoe, T., and Stoica, I. (2006). Declarative net-\nworking: Language, execution and optimization. In\nSIGMOD-06.\nLove, N., Hinrichs, T., and Genesereth, M. R.\n(2006).\nGeneral game playing:\nGame descrip-\ntion language speciﬁcation. Tech. rep. LG-2006-01,\nStanford University Computer Science Dept.\nLovejoy, W. S. (1991).\nA survey of algorithmic\nmethods for partially observed Markov decision pro-\ncesses. Annals of Operations Research, 28(1–4), 47–\n66.\nLoveland, D. (1970). A linear format for resolution.\nIn Proc. IRIA Symposium on Automatic Demonstra-\ntion, pp. 147–162. Bibliography\n1081\nLowe, D. (1987). Three-dimensional object recog-\nnition from single two-dimensional images. AIJ, 31,\n355–395.\nLowe, D. (1999).\nObject recognition using local\nscale invariant feature. In ICCV.\nLowe, D. (2004). Distinctive image features from\nscale-invariant keypoints. IJCV, 60(2), 91–110.\nL¨owenheim, L. (1915). ¨Uber m¨oglichkeiten im Rel-\nativkalk¨ul. Mathematische Annalen, 76, 447–470.\nLowerre, B. T. (1976). The HARPY Speech Recog-\nnition System. Ph.D. thesis, Computer Science De-\npartment, Carnegie-Mellon University.\nLowerre, B. T. and Reddy, R. (1980). The HARPY\nspeech recognition system.\nIn Lea, W. A. (Ed.),\nTrends in Speech Recognition, chap. 15. Prentice-\nHall.\nLowry, M. (2008). Intelligent software engineering\ntools for NASA’s crew exploration vehicle. In Proc.\nISMIS.\nLoyd, S. (1959).\nMathematical Puzzles of Sam\nLoyd:\nSelected and Edited by Martin Gardner.\nDover.\nLozano-Perez, T. (1983). Spatial planning: A con-\nﬁguration space approach.\nIEEE Transactions on\nComputers, C-32(2), 108–120.",
  "ISMIS.\nLoyd, S. (1959).\nMathematical Puzzles of Sam\nLoyd:\nSelected and Edited by Martin Gardner.\nDover.\nLozano-Perez, T. (1983). Spatial planning: A con-\nﬁguration space approach.\nIEEE Transactions on\nComputers, C-32(2), 108–120.\nLozano-Perez, T., Mason, M., and Taylor, R.\n(1984). Automatic synthesis of ﬁne-motion strate-\ngies for robots. Int. J. Robotics Research, 3(1), 3–24.\nLu, F. and Milios, E. (1997). Globally consistent\nrange scan alignment for environment mapping. Au-\ntonomous Robots, 4, 333–349.\nLuby, M., Sinclair, A., and Zuckerman, D. (1993).\nOptimal speedup of Las Vegas algorithms. Informa-\ntion Processing Letters, 47, 173–180.\nLucas, J. R. (1961). Minds, machines, and G¨odel.\nPhilosophy, 36.\nLucas, J. R. (1976). This G¨odel is killing me: A\nrejoinder. Philosophia, 6(1), 145–148.\nLucas, P. (1996).\nKnowledge acquisition for\ndecision-theoretic expert systems. AISB Quarterly,\n94, 23–33.\nLucas, P., van der Gaag, L., and Abu-Hanna, A.\n(2004).\nBayesian networks in biomedicine and\nhealth-care. Artiﬁcial Intelligence in Medicine.\nLuce, D. R. and Raiffa, H. (1957). Games and De-\ncisions. Wiley.\nLudlow, P., Nagasawa, Y., and Stoljar, D. (2004).\nThere’s Something About Mary. MIT Press.\nLuger, G. F. (Ed.). (1995). Computation and intelli-\ngence: Collected readings. AAAI Press.\nLyman, P. and Varian, H. R. (2003).\nHow\nmuch information?\nwww.sims.berkeley.\nedu/how-much-info-2003.\nMachina, M. (2005).\nChoice under uncertainty.\nIn Encyclopedia of Cognitive Science, pp. 505–514.\nWiley.\nMacKay, D. J. C. (1992).\nA practical Bayesian\nframework for back-propagation networks. Neural\nComputation, 4(3), 448–472.\nMacKay, D. J. C. (2002). Information Theory, In-\nference and Learning Algorithms. Cambridge Uni-\nversity Press.\nMacKenzie, D. (2004). Mechanizing Proof. MIT\nPress.\nMackworth, A. K. (1977). Consistency in networks\nof relations. AIJ, 8(1), 99–118.\nMackworth, A. K. (1992). Constraint satisfaction.\nIn Shapiro, S. (Ed.), Encyclopedia of Artiﬁcial Intel-\nligence (second edition)., Vol. 1, pp. 285–293. Wi-\nley.\nMahanti, A. and Daniels, C. J. (1993). A SIMD ap-\nproach to parallel heuristic search. AIJ, 60(2), 243–\n282.\nMailath, G. and Samuelson, L. (2006). Repeated\nGames and Reputations: Long-Run Relationships.\nOxford University Press.\nMajercik, S. M. and Littman, M. L. (2003). Contin-\ngent planning under uncertainty via stochastic satis-\nﬁability. AIJ, pp. 119–162.\nMalik, J. and Perona, P. (1990). Preattentive texture\ndiscrimination with early vision mechanisms. J. Opt.",
  "Majercik, S. M. and Littman, M. L. (2003). Contin-\ngent planning under uncertainty via stochastic satis-\nﬁability. AIJ, pp. 119–162.\nMalik, J. and Perona, P. (1990). Preattentive texture\ndiscrimination with early vision mechanisms. J. Opt.\nSoc. Am. A, 7(5), 923–932.\nMalik, J. and Rosenholtz, R. (1994).\nRecovering\nsurface curvature and orientation from texture distor-\ntion: A least squares algorithm and sensitivity anal-\nysis. In ECCV, pp. 353–364.\nMalik, J. and Rosenholtz, R. (1997).\nComputing\nlocal surface orientation and shape from texture for\ncurved surfaces. IJCV, 23(2), 149–168.\nManeva, E., Mossel, E., and Wainwright, M. J.\n(2007). A new look at survey propagation and its\ngeneralizations. JACM, 54(4).\nManna, Z. and Waldinger, R. (1971). Toward auto-\nmatic program synthesis. CACM, 14(3), 151–165.\nManna, Z. and Waldinger, R. (1985). The Logical\nBasis for Computer Programming: Volume 1: De-\nductive Reasoning. Addison-Wesley.\nManning, C. and Sch¨utze, H. (1999). Foundations\nof Statistical Natural Language Processing.\nMIT\nPress.\nManning, C., Raghavan, P., and Sch¨utze, H. (2008).\nIntroduction to Information Retrieval.\nCambridge\nUniversity Press.\nMannion, M. (2002).\nUsing ﬁrst-order logic for\nproduct line model validation. In Software Product\nLines: Second International Conference. Springer.\nManzini, G. (1995). BIDA*: An improved perime-\nter search algorithm. AIJ, 72(2), 347–360.\nMarbach, P. and Tsitsiklis, J. N. (1998). Simulation-\nbased optimization of Markov reward processes.\nTechnical report LIDS-P-2411, Laboratory for Infor-\nmation and Decision Systems, Massachusetts Insti-\ntute of Technology.\nMarcus, G. (2009). Kluge: The Haphazard Evolu-\ntion of the Human Mind. Mariner Books.\nMarcus, M. P., Santorini, B., and Marcinkiewicz,\nM. A. (1993). Building a large annotated corpus of\nenglish: The penn treebank. Computational Linguis-\ntics, 19(2), 313–330.\nMarkov, A. A. (1913). An example of statistical\ninvestigation in the text of “Eugene Onegin” illus-\ntrating coupling of “tests” in chains. Proc. Academy\nof Sciences of St. Petersburg, 7.\nMaron, M. E. (1961). Automatic indexing: An ex-\nperimental inquiry. JACM, 8(3), 404–417.\nMaron, M. E. and Kuhns, J.-L. (1960).\nOn rel-\nevance, probabilistic indexing and information re-\ntrieval. CACM, 7, 219–244.\nMarr, D. (1982). Vision: A Computational Investi-\ngation into the Human Representation and Process-\ning of Visual Information. W. H. Freeman.\nMarriott, K. and Stuckey, P. J. (1998). Program-",
  "trieval. CACM, 7, 219–244.\nMarr, D. (1982). Vision: A Computational Investi-\ngation into the Human Representation and Process-\ning of Visual Information. W. H. Freeman.\nMarriott, K. and Stuckey, P. J. (1998). Program-\nming with Constraints: An Introduction. MIT Press.\nMarsland, A. T. and Schaeffer, J. (Eds.). (1990).\nComputers, Chess, and Cognition. Springer-Verlag.\nMarsland, S. (2009). Machine Learning: An Algo-\nrithmic Perspective. CRC Press.\nMartelli, A. and Montanari, U. (1973).\nAdditive\nAND/OR graphs. In IJCAI-73, pp. 1–11.\nMartelli, A. and Montanari, U. (1978). Optimizing\ndecision trees through heuristically guided search.\nCACM, 21, 1025–1039.\nMartelli, A. (1977). On the complexity of admissi-\nble search algorithms. AIJ, 8(1), 1–13.\nMarthi, B., Pasula, H., Russell, S. J., and Peres, Y.\n(2002). Decayed MCMC ﬁltering. In UAI-02, pp.\n319–326.\nMarthi, B., Russell, S. J., Latham, D., and Guestrin,\nC. (2005).\nConcurrent hierarchical reinforcement\nlearning. In IJCAI-05.\nMarthi, B., Russell, S. J., and Wolfe, J. (2007). An-\ngelic semantics for high-level actions. In ICAPS-07.\nMarthi, B., Russell, S. J., and Wolfe, J. (2008). An-\ngelic hierarchical planning: Optimal and online al-\ngorithms. In ICAPS-08.\nMartin, D., Fowlkes, C., and Malik, J. (2004).\nLearning to detect natural image boundaries using\nlocal brightness, color, and texture cues.\nPAMI,\n26(5), 530–549.\nMartin, J. H. (1990). A Computational Model of\nMetaphor Interpretation. Academic Press.\nMason, M. (1993).\nKicking the sensing habit.\nAIMag, 14(1), 58–59.\nMason, M. (2001). Mechanics of Robotic Manipu-\nlation. MIT Press.\nMason, M. and Salisbury, J. (1985). Robot hands\nand the mechanics of manipulation. MIT Press.\nMataric, M. J. (1997). Reinforcement learning in\nthe multi-robot domain. Autonomous Robots, 4(1),\n73–83.\nMates, B. (1953). Stoic Logic. University of Cali-\nfornia Press.\nMatuszek, C., Cabral, J., Witbrock, M., and DeO-\nliveira, J. (2006). An introduction to the syntax and\nsemantics of cyc. In Proc. AAAI Spring Symposium\non Formalizing and Compiling Background Knowl-\nedge and Its Applications to Knowledge Representa-\ntion and Question Answering.\nMaxwell, J. and Kaplan, R. (1993). The interface\nbetween phrasal and functional constraints. Compu-\ntational Linguistics, 19(4), 571–590.\nMcAllester, D. A. (1980). An outlook on truth main-\ntenance. Ai memo 551, MIT AI Laboratory.\nMcAllester, D. A. (1988). Conspiracy numbers for\nmin-max search. AIJ, 35(3), 287–310.",
  "tational Linguistics, 19(4), 571–590.\nMcAllester, D. A. (1980). An outlook on truth main-\ntenance. Ai memo 551, MIT AI Laboratory.\nMcAllester, D. A. (1988). Conspiracy numbers for\nmin-max search. AIJ, 35(3), 287–310.\nMcAllester, D. A. (1998). What is the most press-\ning issue facing AI and the AAAI today? Candidate\nstatement, election for Councilor of the American\nAssociation for Artiﬁcial Intelligence.\nMcAllester, D. A. and Rosenblitt, D. (1991). Sys-\ntematic nonlinear planning. In AAAI-91, Vol. 2, pp.\n634–639.\nMcCallum, A. (2003). Efﬁciently inducing features\nof conditional random ﬁelds. In UAI-03.\nMcCarthy, J. (1958).\nPrograms with common\nsense.\nIn Proc. Symposium on Mechanisation of\nThought Processes, Vol. 1, pp. 77–84.\nMcCarthy, J. (1963). Situations, actions, and causal\nlaws. Memo 2, Stanford University Artiﬁcial Intelli-\ngence Project. 1082\nBibliography\nMcCarthy, J. (1968).\nPrograms with common\nsense. In Minsky, M. L. (Ed.), Semantic Informa-\ntion Processing, pp. 403–418. MIT Press.\nMcCarthy, J. (1980). Circumscription: A form of\nnon-monotonic reasoning. AIJ, 13(1–2), 27–39.\nMcCarthy, J. (2007). From here to human-level AI.\nAIJ, 171(18), 1174–1182.\nMcCarthy, J. and Hayes, P. J. (1969). Some philo-\nsophical problems from the standpoint of artiﬁcial\nintelligence. In Meltzer, B., Michie, D., and Swann,\nM. (Eds.), Machine Intelligence 4, pp. 463–502. Ed-\ninburgh University Press.\nMcCarthy, J., Minsky, M. L., Rochester, N., and\nShannon, C. E. (1955). Proposal for the Dartmouth\nsummer research project on artiﬁcial intelligence.\nTech. rep., Dartmouth College.\nMcCawley, J. D. (1988). The Syntactic Phenomena\nof English, Vol. 2 volumes. University of Chicago\nPress.\nMcCorduck, P. (2004). Machines who think: a per-\nsonal inquiry into the history and prospects of artiﬁ-\ncial intelligence (Revised edition). A K Peters.\nMcCulloch, W. S. and Pitts, W. (1943). A logical\ncalculus of the ideas immanent in nervous activity.\nBulletin of Mathematical Biophysics, 5, 115–137.\nMcCune, W. (1992). Automated discovery of new\naxiomatizations of the left group and right group cal-\nculi. JAR, 9(1), 1–24.\nMcCune, W. (1997). Solution of the Robbins prob-\nlem. JAR, 19(3), 263–276.\nMcDermott, D. (1976). Artiﬁcial intelligence meets\nnatural stupidity. SIGART Newsletter, 57, 4–9.\nMcDermott, D. (1978a). Planning and acting. Cog-\nnitive Science, 2(2), 71–109.\nMcDermott, D. (1978b).\nTarskian semantics, or,\nno notation without denotation! Cognitive Science,\n2(3).",
  "natural stupidity. SIGART Newsletter, 57, 4–9.\nMcDermott, D. (1978a). Planning and acting. Cog-\nnitive Science, 2(2), 71–109.\nMcDermott, D. (1978b).\nTarskian semantics, or,\nno notation without denotation! Cognitive Science,\n2(3).\nMcDermott, D. (1985). Reasoning about plans. In\nHobbs, J. and Moore, R. (Eds.), Formal theories of\nthe commonsense world. Intellect Books.\nMcDermott, D. (1987). A critique of pure reason.\nComputational Intelligence, 3(3), 151–237.\nMcDermott, D. (1996). A heuristic estimator for\nmeans-ends analysis in planning. In ICAPS-96, pp.\n142–149.\nMcDermott, D. and Doyle, J. (1980).\nNon-\nmonotonic logic: i. AIJ, 13(1–2), 41–72.\nMcDermott, J. (1982). R1: A rule-based conﬁgurer\nof computer systems. AIJ, 19(1), 39–88.\nMcEliece, R. J., MacKay, D. J. C., and Cheng, J.-\nF. (1998). Turbo decoding as an instance of Pearl’s\n“belief propagation” algorithm. IEEE Journal on Se-\nlected Areas in Communications, 16(2), 140–152.\nMcGregor, J. J. (1979). Relational consistency al-\ngorithms and their application in ﬁnding subgraph\nand graph isomorphisms.\nInformation Sciences,\n19(3), 229–250.\nMcIlraith, S. and Zeng, H. (2001). Semantic web\nservices. IEEE Intelligent Systems, 16(2), 46–53.\nMcLachlan, G. J. and Krishnan, T. (1997). The EM\nAlgorithm and Extensions. Wiley.\nMcMillan, K. L. (1993). Symbolic Model Checking.\nKluwer.\nMeehl, P. (1955). Clinical vs. Statistical Prediction.\nUniversity of Minnesota Press.\nMendel, G. (1866).\nVersuche ¨uber pﬂanzen-\nhybriden.\nVerhandlungen des Naturforschenden\nVereins, Abhandlungen, Br¨unn, 4, 3–47. Translated\ninto English by C. T. Druery, published by Bateson\n(1902).\nMercer, J. (1909). Functions of positive and nega-\ntive type and their connection with the theory of in-\ntegral equations. Philos. Trans. Roy. Soc. London, A,\n209, 415–446.\nMerleau-Ponty, M. (1945). Phenomenology of Per-\nception. Routledge.\nMetropolis, N., Rosenbluth, A., Rosenbluth, M.,\nTeller, A., and Teller, E. (1953). Equations of state\ncalculations by fast computing machines. J. Chemi-\ncal Physics, 21, 1087–1091.\nMetzinger, T. (2009). The Ego Tunnel: The Science\nof the Mind and the Myth of the Self. Basic Books.\nM´ezard, M. and Nadal, J.-P. (1989). Learning in\nfeedforward layered networks: The tiling algorithm.\nJ. Physics, 22, 2191–2204.\nMichalski, R. S. (1969). On the quasi-minimal so-\nlution of the general covering problem.\nIn Proc.\nFirst International Symposium on Information Pro-\ncessing, pp. 125–128.",
  "feedforward layered networks: The tiling algorithm.\nJ. Physics, 22, 2191–2204.\nMichalski, R. S. (1969). On the quasi-minimal so-\nlution of the general covering problem.\nIn Proc.\nFirst International Symposium on Information Pro-\ncessing, pp. 125–128.\nMichalski, R. S., Mozetic, I., Hong, J., and Lavrauc,\nN. (1986).\nThe multi-purpose incremental learn-\ning system AQ15 and its testing application to three\nmedical domains. In AAAI-86, pp. 1041–1045.\nMichie, D. (1966).\nGame-playing and game-\nlearning automata.\nIn Fox, L. (Ed.), Advances in\nProgramming and Non-Numerical Computation, pp.\n183–200. Pergamon.\nMichie, D. (1972). Machine intelligence at Edin-\nburgh. Management Informatics, 2(1), 7–12.\nMichie, D. (1974). Machine intelligence at Edin-\nburgh. In On Intelligence, pp. 143–155. Edinburgh\nUniversity Press.\nMichie, D. and Chambers, R. A. (1968). BOXES:\nAn experiment in adaptive control. In Dale, E. and\nMichie, D. (Eds.), Machine Intelligence 2, pp. 125–\n133. Elsevier/North-Holland.\nMichie, D., Spiegelhalter, D. J., and Taylor, C.\n(Eds.). (1994). Machine Learning, Neural and Sta-\ntistical Classiﬁcation. Ellis Horwood.\nMilch, B., Marthi, B., Sontag, D., Russell, S. J.,\nOng, D., and Kolobov, A. (2005). BLOG: Proba-\nbilistic models with unknown objects. In IJCAI-05.\nMilch, B., Zettlemoyer, L. S., Kersting, K., Haimes,\nM., and Kaelbling, L. P. (2008). Lifted probabilistic\ninference with counting formulas. In AAAI-08, pp.\n1062–1068.\nMilgrom, P. (1997). Putting auction theory to work:\nThe simultaneous ascending auction.\nTech. rep.\nTechnical Report 98-0002, Stanford University De-\npartment of Economics.\nMill, J. S. (1843). A System of Logic, Ratiocinative\nand Inductive: Being a Connected View of the Prin-\nciples of Evidence, and Methods of Scientiﬁc Inves-\ntigation. J. W. Parker, London.\nMill, J. S. (1863). Utilitarianism. Parker, Son and\nBourn, London.\nMiller, A. C., Merkhofer, M. M., Howard, R. A.,\nMatheson, J. E., and Rice, T. R. (1976). Develop-\nment of automated aids for decision analysis. Tech-\nnical report, SRI International.\nMinker, J. (2001).\nLogic-Based Artiﬁcial Intelli-\ngence. Kluwer.\nMinsky, M. L. (1975). A framework for represent-\ning knowledge. In Winston, P. H. (Ed.), The Psychol-\nogy of Computer Vision, pp. 211–277. McGraw-Hill.\nOriginally an MIT AI Laboratory memo; the 1975\nversion is abridged, but is the most widely cited.\nMinsky, M. L. (1986). The society of mind. Simon\nand Schuster.\nMinsky, M. L. (2007). The Emotion Machine: Com-",
  "Originally an MIT AI Laboratory memo; the 1975\nversion is abridged, but is the most widely cited.\nMinsky, M. L. (1986). The society of mind. Simon\nand Schuster.\nMinsky, M. L. (2007). The Emotion Machine: Com-\nmonsense Thinking, Artiﬁcial Intelligence, and the\nFuture of the Human Mind. Simon and Schuster.\nMinsky, M. L. and Papert, S. (1969). Perceptrons:\nAn Introduction to Computational Geometry (ﬁrst\nedition). MIT Press.\nMinsky, M. L. and Papert, S. (1988). Perceptrons:\nAn Introduction to Computational Geometry (Ex-\npanded edition). MIT Press.\nMinsky, M. L., Singh, P., and Sloman, A. (2004).\nThe st. thomas common sense symposium:\nDe-\nsigning architectures for human-level intelligence.\nAIMag, 25(2), 113–124.\nMinton, S. (1984). Constraint-based generalization:\nLearning game-playing plans from single examples.\nIn AAAI-84, pp. 251–254.\nMinton, S. (1988). Quantitative results concerning\nthe utility of explanation-based learning. In AAAI-\n88, pp. 564–569.\nMinton, S., Johnston, M. D., Philips, A. B., and\nLaird, P. (1992).\nMinimizing conﬂicts: A heuris-\ntic repair method for constraint satisfaction and\nscheduling problems. AIJ, 58(1–3), 161–205.\nMisak, C. (2004). The Cambridge Companion to\nPeirce. Cambridge University Press.\nMitchell, M. (1996). An Introduction to Genetic Al-\ngorithms. MIT Press.\nMitchell, M., Holland, J. H., and Forrest, S. (1996).\nWhen will a genetic algorithm outperform hill\nclimbing? In Cowan, J., Tesauro, G., and Alspec-\ntor, J. (Eds.), NIPS 6. MIT Press.\nMitchell, T. M. (1977). Version spaces: A candidate\nelimination approach to rule learning. In IJCAI-77,\npp. 305–310.\nMitchell, T. M. (1982). Generalization as search.\nAIJ, 18(2), 203–226.\nMitchell, T. M. (1990). Becoming increasingly reac-\ntive (mobile robots). In AAAI-90, Vol. 2, pp. 1051–\n1058.\nMitchell,\nT. M. (1997).\nMachine Learning.\nMcGraw-Hill.\nMitchell, T. M., Keller, R., and Kedar-Cabelli, S.\n(1986). Explanation-based generalization: A unify-\ning view. Machine Learning, 1, 47–80.\nMitchell, T. M., Utgoff, P. E., and Banerji, R.\n(1983). Learning by experimentation: Acquiring and\nreﬁning problem-solving heuristics.\nIn Michalski,\nR. S., Carbonell, J. G., and Mitchell, T. M. (Eds.),\nMachine Learning: An Artiﬁcial Intelligence Ap-\nproach, pp. 163–190. Morgan Kaufmann.\nMitchell, T. M. (2005). Reading the web: A break-\nthrough goal for AI. AIMag, 26(3), 12–16.\nMitchell, T. M. (2007). Learning, information ex-\ntraction and the web. In ECML/PKDD, p. 1.",
  "proach, pp. 163–190. Morgan Kaufmann.\nMitchell, T. M. (2005). Reading the web: A break-\nthrough goal for AI. AIMag, 26(3), 12–16.\nMitchell, T. M. (2007). Learning, information ex-\ntraction and the web. In ECML/PKDD, p. 1.\nMitchell, T. M., Shinkareva, S. V., Carlson, A.,\nChang, K.-M., Malave, V. L., Mason, R. A., and\nJust, M. A. (2008). Predicting human brain activ-\nity associated with the meanings of nouns. Science,\n320, 1191–1195.\nMohr, R. and Henderson, T. C. (1986). Arc and path\nconsistency revisited. AIJ, 28(2), 225–233. Bibliography\n1083\nMohri, M., Pereira, F., and Riley, M. (2002).\nWeighted ﬁnite-state transducers in speech recogni-\ntion. Computer Speech and Language, 16(1), 69–88.\nMontague, P. R., Dayan, P., Person, C., and Se-\njnowski, T. (1995). Bee foraging in uncertain envi-\nronments using predictive Hebbian learning. Nature,\n377, 725–728.\nMontague, R. (1970). English as a formal language.\nIn Linguaggi nella Societ`a e nella Tecnica, pp. 189–\n224. Edizioni di Comunit`a.\nMontague, R. (1973). The proper treatment of quan-\ntiﬁcation in ordinary English. In Hintikka, K. J. J.,\nMoravcsik, J. M. E., and Suppes, P. (Eds.), Ap-\nproaches to Natural Language. D. Reidel.\nMontanari, U. (1974).\nNetworks of constraints:\nFundamental properties and applications to picture\nprocessing. Information Sciences, 7(2), 95–132.\nMontemerlo, M. and Thrun, S. (2004). Large-scale\nrobotic 3-D mapping of urban structures. In Proc.\nInternational Symposium on Experimental Robotics.\nSpringer Tracts in Advanced Robotics (STAR).\nMontemerlo, M., Thrun, S., Koller, D., and Weg-\nbreit, B. (2002). FastSLAM: A factored solution to\nthe simultaneous localization and mapping problem.\nIn AAAI-02.\nMooney, R. (1999). Learning for semantic interpre-\ntation: Scaling up without dumbing down. In Proc.\n1st Workshop on Learning Language in Logic, pp.\n7–15.\nMoore, A. and Wong, W.-K. (2003). Optimal rein-\nsertion: A new search operator for accelerated and\nmore accurate Bayesian network structure learning.\nIn ICML-03.\nMoore, A. W. and Atkeson, C. G. (1993).\nPrior-\nitized sweeping—Reinforcement learning with less\ndata and less time. Machine Learning, 13, 103–130.\nMoore, A. W. and Lee, M. S. (1997). Cached suf-\nﬁcient statistics for efﬁcient machine learning with\nlarge datasets. JAIR, 8, 67–91.\nMoore, E. F. (1959). The shortest path through a\nmaze. In Proc. an International Symposium on the\nTheory of Switching, Part II, pp. 285–292. Harvard\nUniversity Press.",
  "large datasets. JAIR, 8, 67–91.\nMoore, E. F. (1959). The shortest path through a\nmaze. In Proc. an International Symposium on the\nTheory of Switching, Part II, pp. 285–292. Harvard\nUniversity Press.\nMoore, R. C. (1980). Reasoning about knowledge\nand action.\nArtiﬁcial intelligence center technical\nnote 191, SRI International.\nMoore, R. C. (1985). A formal theory of knowl-\nedge and action. In Hobbs, J. R. and Moore, R. C.\n(Eds.), Formal Theories of the Commonsense World,\npp. 319–358. Ablex.\nMoore, R. C. (2005). Association-based bilingual\nword alignment.\nIn Proc. ACL-05 Workshop on\nBuilding and Using Parallel Texts, pp. 1–8.\nMoravec, H. P. (1983). The stanford cart and the\ncmu rover. Proc. IEEE, 71(7), 872–884.\nMoravec, H. P. and Elfes, A. (1985). High resolu-\ntion maps from wide angle sonar. In ICRA-85, pp.\n116–121.\nMoravec, H. P. (1988). Mind Children: The Future\nof Robot and Human Intelligence. Harvard Univer-\nsity Press.\nMoravec, H. P. (2000). Robot: Mere Machine to\nTranscendent Mind. Oxford University Press.\nMorgenstern, L. (1998). Inheritance comes of age:\nApplying nonmonotonic techniques to problems in\nindustry. AIJ, 103, 237–271.\nMorjaria, M. A., Rink, F. J., Smith, W. D., Klemp-\nner, G., Burns, C., and Stein, J. (1995). Elicitation of\nprobabilities for belief networks: Combining quali-\ntative and quantitative information. In UAI-95, pp.\n141–148.\nMorrison, P. and Morrison, E. (Eds.). (1961).\nCharles Babbage and His Calculating Engines: Se-\nlected Writings by Charles Babbage and Others.\nDover.\nMoskewicz, M. W., Madigan, C. F., Zhao, Y.,\nZhang, L., and Malik, S. (2001). Chaff: Engineer-\ning an efﬁcient SAT solver. In Proc. 38th Design\nAutomation Conference (DAC 2001), pp. 530–535.\nMosteller, F. and Wallace, D. L. (1964). Inference\nand Disputed Authorship: The Federalist. Addison-\nWesley.\nMostow, J. and Prieditis, A. E. (1989). Discovering\nadmissible heuristics by abstracting and optimizing:\nA transformational approach. In IJCAI-89, Vol. 1,\npp. 701–707.\nMotzkin, T. S. and Schoenberg, I. J. (1954). The\nrelaxation method for linear inequalities. Canadian\nJournal of Mathematics, 6(3), 393–404.\nMoutarlier, P. and Chatila, R. (1989). Stochastic\nmultisensory data fusion for mobile robot location\nand environment modeling. In ISRR-89.\nMueller, E. T. (2006).\nCommonsense Reasoning.\nMorgan Kaufmann.\nMuggleton, S. H. (1991). Inductive logic program-\nming. New Generation Computing, 8, 295–318.\nMuggleton, S. H. (1992). Inductive Logic Program-\nming. Academic Press.",
  "Mueller, E. T. (2006).\nCommonsense Reasoning.\nMorgan Kaufmann.\nMuggleton, S. H. (1991). Inductive logic program-\nming. New Generation Computing, 8, 295–318.\nMuggleton, S. H. (1992). Inductive Logic Program-\nming. Academic Press.\nMuggleton, S. H. (1995).\nInverse entailment and\nProgol. New Generation Computing, 13(3-4), 245–\n286.\nMuggleton, S. H. (2000). Learning stochastic logic\nprograms. Proc. AAAI 2000 Workshop on Learning\nStatistical Models from Relational Data.\nMuggleton, S. H. and Buntine, W. (1988). Machine\ninvention of ﬁrst-order predicates by inverting reso-\nlution. In ICML-88, pp. 339–352.\nMuggleton, S. H. and De Raedt, L. (1994). Induc-\ntive logic programming: Theory and methods.\nJ.\nLogic Programming, 19/20, 629–679.\nMuggleton, S. H. and Feng, C. (1990). Efﬁcient in-\nduction of logic programs. In Proc. Workshop on\nAlgorithmic Learning Theory, pp. 368–381.\nM¨uller, M. (2002). Computer Go. AIJ, 134(1–2),\n145–179.\nM¨uller, M. (2003).\nConditional combinatorial\ngames, and their application to analyzing capturing\nraces in go. Information Sciences, 154(3–4), 189–\n202.\nMumford, D. and Shah, J. (1989). Optimal approx-\nimations by piece-wise smooth functions and asso-\nciated variational problems.\nCommun. Pure Appl.\nMath., 42, 577–685.\nMurphy, K., Weiss, Y., and Jordan, M. I. (1999).\nLoopy belief propagation for approximate inference:\nAn empirical study. In UAI-99, pp. 467–475.\nMurphy, K. (2001).\nThe Bayes net toolbox for\nMATLAB. Computing Science and Statistics, 33.\nMurphy, K. (2002). Dynamic Bayesian Networks:\nRepresentation, Inference and Learning. Ph.D. the-\nsis, UC Berkeley.\nMurphy, K. and Mian, I. S. (1999).\nModelling\ngene expression data using Bayesian networks.\npeople.cs.ubc.ca/˜murphyk/Papers/\nismb99.pdf.\nMurphy, K. and Russell, S. J. (2001).\nRao-\nblackwellised\nparticle\nﬁltering\nfor\ndynamic\nBayesian networks.\nIn Doucet, A., de Freitas,\nN., and Gordon, N. J. (Eds.), Sequential Monte\nCarlo Methods in Practice. Springer-Verlag.\nMurphy, K. and Weiss, Y. (2001).\nThe fac-\ntored frontier algorithm for approximate inference in\nDBNs. In UAI-01, pp. 378–385.\nMurphy, R. (2000).\nIntroduction to AI Robotics.\nMIT Press.\nMurray-Rust, P., Rzepa, H. S., Williamson, J., and\nWillighagen, E. L. (2003). Chemical markup, XML\nand the world–wide web. 4. CML schema. J. Chem.\nInf. Comput. Sci., 43, 752–772.\nMurthy, C. and Russell, J. R. (1990). A constructive\nproof of Higman’s lemma. In LICS-90, pp. 257–269.\nMuscettola, N. (2002). Computing the envelope for",
  "and the world–wide web. 4. CML schema. J. Chem.\nInf. Comput. Sci., 43, 752–772.\nMurthy, C. and Russell, J. R. (1990). A constructive\nproof of Higman’s lemma. In LICS-90, pp. 257–269.\nMuscettola, N. (2002). Computing the envelope for\nstepwise-constant resource allocations.\nIn CP-02,\npp. 139–154.\nMuscettola, N., Nayak, P., Pell, B., and Williams,\nB. (1998). Remote agent: To boldly go where no AI\nsystem has gone before. AIJ, 103, 5–48.\nMuslea, I. (1999). Extraction patterns for informa-\ntion extraction tasks: A survey. In Proc. AAAI-99\nWorkshop on Machine Learning for Information Ex-\ntraction.\nMyerson, R. (1981). Optimal auction design. Math-\nematics of Operations Research, 6, 58–73.\nMyerson, R. (1986). Multistage games with com-\nmunication. Econometrica, 54, 323–358.\nMyerson, R. (1991).\nGame Theory: Analysis of\nConﬂict. Harvard University Press.\nNagel, T. (1974). What is it like to be a bat? Philo-\nsophical Review, 83, 435–450.\nNalwa, V. S. (1993). A Guided Tour of Computer\nVision. Addison-Wesley.\nNash, J. (1950).\nEquilibrium points in N-person\ngames. PNAS, 36, 48–49.\nNau, D. S. (1980). Pathology on game trees: A sum-\nmary of results. In AAAI-80, pp. 102–104.\nNau, D. S. (1983). Pathology on game trees revis-\nited, and an alternative to minimaxing. AIJ, 21(1–2),\n221–244.\nNau, D. S., Kumar, V., and Kanal, L. N. (1984).\nGeneral branch and bound, and its relation to A* and\nAO*. AIJ, 23, 29–58.\nNayak, P. and Williams, B. (1997).\nFast context\nswitching in real-time propositional reasoning.\nIn\nAAAI-97, pp. 50–56.\nNeal, R. (1996). Bayesian Learning for Neural Net-\nworks. Springer-Verlag.\nNebel, B. (2000). On the compilability and expres-\nsive power of propositional planning formalisms.\nJAIR, 12, 271–315.\nNeﬁan, A., Liang, L., Pi, X., Liu, X., and Murphy,\nK. (2002). Dynamic bayesian networks for audio-\nvisual speech recognition. EURASIP, Journal of Ap-\nplied Signal Processing, 11, 1–15.\nNesterov, Y. and Nemirovski, A. (1994). Interior-\nPoint Polynomial Methods in Convex Programming.\nSIAM (Society for Industrial and Applied Mathe-\nmatics).\nNetto, E. (1901). Lehrbuch der Combinatorik. B.\nG. Teubner.\nNevill-Manning, C. G. and Witten, I. H. (1997).\nIdentifying hierarchical structures in sequences: A\nlinear-time algorithm. JAIR, 7, 67–82. 1084\nBibliography\nNewell, A. (1982). The knowledge level. AIJ, 18(1),\n82–127.\nNewell, A. (1990). Uniﬁed Theories of Cognition.\nHarvard University Press.\nNewell, A. and Ernst, G. (1965). The search for gen-",
  "linear-time algorithm. JAIR, 7, 67–82. 1084\nBibliography\nNewell, A. (1982). The knowledge level. AIJ, 18(1),\n82–127.\nNewell, A. (1990). Uniﬁed Theories of Cognition.\nHarvard University Press.\nNewell, A. and Ernst, G. (1965). The search for gen-\nerality. In Proc. IFIP Congress, Vol. 1, pp. 17–24.\nNewell, A., Shaw, J. C., and Simon, H. A. (1957).\nEmpirical explorations with the logic theory ma-\nchine. Proc. Western Joint Computer Conference,\n15, 218–239. Reprinted in Feigenbaum and Feld-\nman (1963).\nNewell, A., Shaw, J. C., and Simon, H. A. (1958).\nChess playing programs and the problem of com-\nplexity. IBM Journal of Research and Development,\n4(2), 320–335.\nNewell, A. and Simon, H. A. (1961). GPS, a pro-\ngram that simulates human thought. In Billing, H.\n(Ed.), Lernende Automaten, pp. 109–124. R. Olden-\nbourg.\nNewell, A. and Simon, H. A. (1972). Human Prob-\nlem Solving. Prentice-Hall.\nNewell, A. and Simon, H. A. (1976).\nComputer\nscience as empirical inquiry: Symbols and search.\nCACM, 19, 113–126.\nNewton, I. (1664–1671). Methodus ﬂuxionum et se-\nrierum inﬁnitarum. Unpublished notes.\nNg, A. Y. (2004). Feature selection, l1 vs. l2 regu-\nlarization, and rotational invariance. In ICML-04.\nNg, A. Y., Harada, D., and Russell, S. J. (1999). Pol-\nicy invariance under reward transformations: Theory\nand application to reward shaping. In ICML-99.\nNg, A. Y. and Jordan, M. I. (2000). PEGASUS: A\npolicy search method for large MDPs and POMDPs.\nIn UAI-00, pp. 406–415.\nNg, A. Y., Kim, H. J., Jordan, M. I., and Sastry, S.\n(2004). Autonomous helicopter ﬂight via reinforce-\nment learning. In NIPS 16.\nNguyen, X. and Kambhampati, S. (2001). Reviving\npartial order planning. In IJCAI-01, pp. 459–466.\nNguyen, X., Kambhampati, S., and Nigenda, R. S.\n(2001).\nPlanning graph as the basis for deriving\nheuristics for plan synthesis by state space and CSP\nsearch. Tech. rep., Computer Science and Engineer-\ning Department, Arizona State University.\nNicholson, A. and Brady, J. M. (1992). The data as-\nsociation problem when monitoring robot vehicles\nusing dynamic belief networks.\nIn ECAI-92, pp.\n689–693.\nNiemel¨a, I., Simons, P., and Syrj¨anen, T. (2000).\nSmodels:\nA system for answer set program-\nming. In Proc. 8th International Workshop on Non-\nMonotonic Reasoning.\nNigam, K., McCallum, A., Thrun, S., and Mitchell,\nT. M. (2000). Text classiﬁcation from labeled and\nunlabeled documents using EM. Machine Learning,\n39(2–3), 103–134.\nNiles, I. and Pease, A. (2001). Towards a standard",
  "Monotonic Reasoning.\nNigam, K., McCallum, A., Thrun, S., and Mitchell,\nT. M. (2000). Text classiﬁcation from labeled and\nunlabeled documents using EM. Machine Learning,\n39(2–3), 103–134.\nNiles, I. and Pease, A. (2001). Towards a standard\nupper ontology. In FOIS ’01: Proc. international\nconference on Formal Ontology in Information Sys-\ntems, pp. 2–9.\nNilsson, D. and Lauritzen, S. (2000).\nEvaluating\ninﬂuence diagrams using LIMIDs. In UAI-00, pp.\n436–445.\nNilsson, N. J. (1965). Learning Machines: Foun-\ndations of Trainable Pattern-Classifying Systems.\nMcGraw-Hill. Republished in 1990.\nNilsson, N. J. (1971). Problem-Solving Methods in\nArtiﬁcial Intelligence. McGraw-Hill.\nNilsson, N. J. (1984). Shakey the robot. Technical\nnote 323, SRI International.\nNilsson, N. J. (1986). Probabilistic logic. AIJ, 28(1),\n71–87.\nNilsson, N. J. (1991).\nLogic and artiﬁcial intelli-\ngence. AIJ, 47(1–3), 31–56.\nNilsson, N. J. (1995). Eye on the prize.\nAIMag,\n16(2), 9–17.\nNilsson, N. J. (1998). Artiﬁcial Intelligence: A New\nSynthesis. Morgan Kaufmann.\nNilsson, N. J. (2005). Human-level artiﬁcial intelli-\ngence? be serious! AIMag, 26(4), 68–75.\nNilsson, N. J. (2009). The Quest for Artiﬁcial Intel-\nligence: A History of Ideas and Achievements. Cam-\nbridge University Press.\nNisan, N., Roughgarden, T., Tardos, E., and Vazi-\nrani, V. (Eds.). (2007). Algorithmic Game Theory.\nCambridge University Press.\nNoe, A. (2009). Out of Our Heads: Why You Are\nNot Your Brain, and Other Lessons from the Biology\nof Consciousness. Hill and Wang.\nNorvig, P. (1988). Multiple simultaneous interpreta-\ntions of ambiguous sentences. In COGSCI-88.\nNorvig, P. (1992). Paradigms of Artiﬁcial Intelli-\ngence Programming: Case Studies in Common Lisp.\nMorgan Kaufmann.\nNorvig, P. (2009). Natural language corpus data. In\nSegaran, T. and Hammerbacher, J. (Eds.), Beautiful\nData. O’Reilly.\nNowick, S. M., Dean, M. E., Dill, D. L., and\nHorowitz, M. (1993).\nThe design of a high-\nperformance cache controller: A case study in asyn-\nchronous synthesis. Integration: The VLSI Journal,\n15(3), 241–262.\nNunberg, G. (1979). The non-uniqueness of seman-\ntic solutions: Polysemy. Language and Philosophy,\n3(2), 143–184.\nNussbaum, M. C. (1978). Aristotle’s De Motu Ani-\nmalium. Princeton University Press.\nOaksford, M. and Chater, N. (Eds.). (1998).\nRa-\ntional models of cognition. Oxford University Press.\nOch, F. J. and Ney, H. (2003). A systematic compar-\nison of various statistical alignment model. Compu-",
  "malium. Princeton University Press.\nOaksford, M. and Chater, N. (Eds.). (1998).\nRa-\ntional models of cognition. Oxford University Press.\nOch, F. J. and Ney, H. (2003). A systematic compar-\nison of various statistical alignment model. Compu-\ntational Linguistics, 29(1), 19–51.\nOch, F. J. and Ney, H. (2004).\nThe alignment\ntemplate approach to statistical machine translation.\nComputational Linguistics, 30, 417–449.\nOgawa, S., Lee, T.-M., Kay, A. R., and Tank, D. W.\n(1990). Brain magnetic resonance imaging with con-\ntrast dependent on blood oxygenation.\nPNAS, 87,\n9868–9872.\nOh, S., Russell, S. J., and Sastry, S. (2009). Markov\nchain Monte Carlo data association for multi-target\ntracking. IEEE Transactions on Automatic Control,\n54(3), 481–497.\nOlesen, K. G. (1993). Causal probabilistic networks\nwith both discrete and continuous variables. PAMI,\n15(3), 275–279.\nOliver, N., Garg, A., and Horvitz, E. J. (2004). Lay-\nered representations for learning and inferring ofﬁce\nactivity from multiple sensory channels. Computer\nVision and Image Understanding, 96, 163–180.\nOliver, R. M. and Smith, J. Q. (Eds.). (1990). Inﬂu-\nence Diagrams, Belief Nets and Decision Analysis.\nWiley.\nOmohundro, S. (2008). The basic AI drives. In\nAGI-08 Workshop on the Sociocultural, Ethical and\nFuturological Implications of Artiﬁcial Intelligence.\nO’Reilly, U.-M. and Oppacher, F. (1994). Program\nsearch with a hierarchical variable length represen-\ntation: Genetic programming, simulated annealing\nand hill climbing. In Proc. Third Conference on Par-\nallel Problem Solving from Nature, pp. 397–406.\nOrmoneit, D. and Sen, S. (2002). Kernel-based re-\ninforcement learning. Machine Learning, 49(2–3),\n161–178.\nOsborne, M. J. (2004). An Introduction to Game\nTheory. Oxford University Pres.\nOsborne, M. J. and Rubinstein, A. (1994). A Course\nin Game Theory. MIT Press.\nOsherson, D. N., Stob, M., and Weinstein, S.\n(1986).\nSystems That Learn: An Introduction to\nLearning Theory for Cognitive and Computer Sci-\nentists. MIT Press.\nPadgham, L. and Winikoff, M. (2004). Developing\nIntelligent Agent Systems: A Practical Guide. Wiley.\nPage, C. D. and Srinivasan, A. (2002). ILP: A short\nlook back and a longer look forward. Submitted to\nJournal of Machine Learning Research.\nPalacios, H. and Geffner, H. (2007). From confor-\nmant into classical planning: Efﬁcient translations\nthat may be complete too. In ICAPS-07.\nPalay, A. J. (1985). Searching with Probabilities.\nPitman.\nPalmer, D. A. and Hearst, M. A. (1994). Adaptive",
  "Palacios, H. and Geffner, H. (2007). From confor-\nmant into classical planning: Efﬁcient translations\nthat may be complete too. In ICAPS-07.\nPalay, A. J. (1985). Searching with Probabilities.\nPitman.\nPalmer, D. A. and Hearst, M. A. (1994). Adaptive\nsentence boundary disambiguation. In Proc. Confer-\nence on Applied Natural Language Processing, pp.\n78–83.\nPalmer, S. (1999). Vision Science: Photons to Phe-\nnomenology. MIT Press.\nPapadimitriou, C. H. (1994). Computational Com-\nplexity. Addison Wesley.\nPapadimitriou, C. H., Tamaki, H., Raghavan, P.,\nand Vempala, S. (1998). Latent semantic indexing:\nA probabilistic analysis. In PODS-98, pp. 159–168.\nPapadimitriou, C. H. and Tsitsiklis, J. N. (1987).\nThe complexity of Markov decision processes.\nMathematics of Operations Research, 12(3), 441–\n450.\nPapadimitriou, C. H. and Yannakakis, M. (1991).\nShortest paths without a map. Theoretical Computer\nScience, 84(1), 127–150.\nPapavassiliou, V. and Russell, S. J. (1999). Conver-\ngence of reinforcement learning with general func-\ntion approximators. In IJCAI-99, pp. 748–757.\nParekh, R. and Honavar, V. (2001). DFA learning\nfrom simple examples. Machine Learning, 44, 9–\n35.\nParisi, G. (1988). Statistical ﬁeld theory. Addison-\nWesley.\nParisi, M. M. G. and Zecchina, R. (2002).\nAna-\nlytic and algorithmic solution of random satisﬁabil-\nity problems. Science, 297, 812–815.\nParker, A., Nau, D. S., and Subrahmanian, V. S.\n(2005). Game-tree search with combinatorially large\nbelief states. In IJCAI-05, pp. 254–259.\nParker, D. B. (1985).\nLearning logic.\nTech-\nnical report TR-47, Center for Computational Re-\nsearch in Economics and Management Science,\nMassachusetts Institute of Technology.\nParker, L. E. (1996). On the design of behavior-\nbased multi-robot teams.\nJ. Advanced Robotics,\n10(6).\nParr, R. and Russell, S. J. (1998). Reinforcement\nlearning with hierarchies of machines.\nIn Jordan,\nM. I., Kearns, M., and Solla, S. A. (Eds.), NIPS 10.\nMIT Press. Bibliography\n1085\nParzen, E. (1962). On estimation of a probability\ndensity function and mode. Annals of Mathematical\nStatistics, 33, 1065–1076.\nPasca, M. and Harabagiu, S. M. (2001). High perfor-\nmance question/answering. In SIGIR-01, pp. 366–\n374.\nPasca, M., Lin, D., Bigham, J., Lifchits, A., and\nJain, A. (2006). Organizing and searching the world\nwide web of facts—Step one: The one-million fact\nextraction challenge. In AAAI-06.\nPaskin, M. (2001). Grammatical bigrams. In NIPS.\nPasula, H., Marthi, B., Milch, B., Russell, S. J., and",
  "Jain, A. (2006). Organizing and searching the world\nwide web of facts—Step one: The one-million fact\nextraction challenge. In AAAI-06.\nPaskin, M. (2001). Grammatical bigrams. In NIPS.\nPasula, H., Marthi, B., Milch, B., Russell, S. J., and\nShpitser, I. (2003). Identity uncertainty and citation\nmatching. In NIPS 15. MIT Press.\nPasula, H. and Russell, S. J. (2001). Approximate\ninference for ﬁrst-order probabilistic languages. In\nIJCAI-01.\nPasula, H., Russell, S. J., Ostland, M., and Ritov, Y.\n(1999). Tracking many objects with many sensors.\nIn IJCAI-99.\nPatashnik, O. (1980).\nQubic: 4x4x4 tic-tac-toe.\nMathematics Magazine, 53(4), 202–216.\nPatrick, B. G., Almulla, M., and Newborn, M.\n(1992). An upper bound on the time complexity of\niterative-deepening-A*. AIJ, 5(2–4), 265–278.\nPaul, R. P. (1981). Robot Manipulators: Mathemat-\nics, Programming, and Control. MIT Press.\nPauls, A. and Klein, D. (2009). K-best A* parsing.\nIn ACL-09.\nPeano, G. (1889).\nArithmetices principia, nova\nmethodo exposita. Fratres Bocca, Turin.\nPearce, J., Tambe, M., and Maheswaran, R. (2008).\nSolving multiagent networks using distributed con-\nstraint optimization. AIMag, 29(3), 47–62.\nPearl, J. (1982a). Reverend Bayes on inference en-\ngines: A distributed hierarchical approach. In AAAI-\n82, pp. 133–136.\nPearl, J. (1982b).\nThe solution for the branching\nfactor of the alpha–beta pruning algorithm and its\noptimality. CACM, 25(8), 559–564.\nPearl, J. (1984).\nHeuristics:\nIntelligent Search\nStrategies for Computer Problem Solving. Addison-\nWesley.\nPearl, J. (1986). Fusion, propagation, and structur-\ning in belief networks. AIJ, 29, 241–288.\nPearl, J. (1987). Evidential reasoning using stochas-\ntic simulation of causal models. AIJ, 32, 247–257.\nPearl, J. (1988). Probabilistic Reasoning in Intelli-\ngent Systems: Networks of Plausible Inference. Mor-\ngan Kaufmann.\nPearl, J. (2000). Causality: Models, Reasoning, and\nInference. Cambridge University Press.\nPearl, J. and Verma, T. (1991). A theory of inferred\ncausation. In KR-91, pp. 441–452.\nPearson, J. and Jeavons, P. (1997).\nA survey of\ntractable constraint satisfaction problems. Techni-\ncal report CSD-TR-97-15, Royal Holloway College,\nU. of London.\nPease, A. and Niles, I. (2002). IEEE standard upper\nontology: A progress report. Knowledge Engineer-\ning Review, 17(1), 65–70.\nPednault, E. P. D. (1986). Formulating multiagent,\ndynamic-world problems in the classical planning\nframework. In Reasoning about Actions and Plans:\nProc. 1986 Workshop, pp. 47–82.",
  "ing Review, 17(1), 65–70.\nPednault, E. P. D. (1986). Formulating multiagent,\ndynamic-world problems in the classical planning\nframework. In Reasoning about Actions and Plans:\nProc. 1986 Workshop, pp. 47–82.\nPeirce, C. S. (1870). Description of a notation for the\nlogic of relatives, resulting from an ampliﬁcation of\nthe conceptions of Boole’s calculus of logic. Mem-\noirs of the American Academy of Arts and Sciences,\n9, 317–378.\nPeirce, C. S. (1883). A theory of probable inference.\nNote B. The logic of relatives. In Studies in Logic by\nMembers of the Johns Hopkins University, pp. 187–\n203, Boston.\nPeirce, C. S. (1902). Logic as semiotic: The the-\nory of signs. Unpublished manuscript; reprinted in\n(Buchler 1955).\nPeirce, C. S. (1909). Existential graphs. Unpub-\nlished manuscript; reprinted in (Buchler 1955).\nPelikan, M., Goldberg, D. E., and Cantu-Paz, E.\n(1999).\nBOA: The Bayesian optimization algo-\nrithm. In GECCO-99: Proc. Genetic and Evolution-\nary Computation Conference, pp. 525–532.\nPemberton, J. C. and Korf, R. E. (1992). Incremen-\ntal planning on graphs with cycles. In AIPS-92, pp.\n525–532.\nPenberthy, J. S. and Weld, D. S. (1992). UCPOP:\nA sound, complete, partial order planner for ADL.\nIn KR-92, pp. 103–114.\nPeng, J. and Williams, R. J. (1993). Efﬁcient learn-\ning and planning within the Dyna framework. Adap-\ntive Behavior, 2, 437–454.\nPenrose, R. (1989). The Emperor’s New Mind. Ox-\nford University Press.\nPenrose, R. (1994). Shadows of the Mind. Oxford\nUniversity Press.\nPeot, M. and Smith, D. E. (1992). Conditional non-\nlinear planning. In ICAPS-92, pp. 189–197.\nPereira, F. and Shieber, S. (1987).\nProlog and\nNatural-Language Analysis. Center for the Study of\nLanguage and Information (CSLI).\nPereira, F. and Warren, D. H. D. (1980). Deﬁnite\nclause grammars for language analysis: A survey\nof the formalism and a comparison with augmented\ntransition networks. AIJ, 13, 231–278.\nPereira, F. and Wright, R. N. (1991). Finite-state ap-\nproximation of phrase structure grammars. In ACL-\n91, pp. 246–255.\nPerlis, A. (1982). Epigrams in programming. SIG-\nPLAN Notices, 17(9), 7–13.\nPerrin, B. E., Ralaivola, L., and Mazurie, A. (2003).\nGene networks inference using dynamic Bayesian\nnetworks. Bioinformatics, 19, II 138–II 148.\nPeterson, C. and Anderson, J. R. (1987). A mean\nﬁeld theory learning algorithm for neural networks.\nComplex Systems, 1(5), 995–1019.\nPetrik, M. and Zilberstein, S. (2009). Bilinear pro-\ngramming approach for multiagent planning. JAIR,",
  "Peterson, C. and Anderson, J. R. (1987). A mean\nﬁeld theory learning algorithm for neural networks.\nComplex Systems, 1(5), 995–1019.\nPetrik, M. and Zilberstein, S. (2009). Bilinear pro-\ngramming approach for multiagent planning. JAIR,\n35, 235–274.\nPetrov, S. and Klein, D. (2007a).\nDiscriminative\nlog-linear grammars with latent variables. In NIPS.\nPetrov, S. and Klein, D. (2007b). Improved infer-\nence for unlexicalized parsing. In ACL-07.\nPetrov, S. and Klein, D. (2007c). Learning and in-\nference for hierarchically split pcfgs. In AAAI-07.\nPfeffer, A., Koller, D., Milch, B., and Takusagawa,\nK. T. (1999). SPOOK: A system for probabilistic\nobject-oriented knowledge representation. In UAI-\n99.\nPfeffer, A. (2000).\nProbabilistic Reasoning for\nComplex Systems. Ph.D. thesis, Stanford University.\nPfeffer, A. (2007). The design and implementation\nof IBAL: A general-purpose probabilistic language.\nIn Getoor, L. and Taskar, B. (Eds.), Introduction to\nStatistical Relational Learning. MIT Press.\nPfeifer, R., Bongard, J., Brooks, R. A., and Iwa-\nsawa, S. (2006). How the Body Shapes the Way We\nThink: A New View of Intelligence. Bradford.\nPineau, J., Gordon, G., and Thrun, S. (2003). Point-\nbased value iteration:\nAn anytime algorithm for\nPOMDPs. In IJCAI-03.\nPinedo, M. (2008). Scheduling: Theory, Algorithms,\nand Systems. Springer Verlag.\nPinkas, G. and Dechter, R. (1995). Improving con-\nnectionist energy minimization. JAIR, 3, 223–248.\nPinker, S. (1995). Language acquisition. In Gleit-\nman, L. R., Liberman, M., and Osherson, D. N.\n(Eds.), An Invitation to Cognitive Science (second\nedition)., Vol. 1. MIT Press.\nPinker, S. (2003). The Blank Slate: The Modern\nDenial of Human Nature. Penguin.\nPinto, D., McCallum, A., Wei, X., and Croft, W. B.\n(2003). Table extraction using conditional random\nﬁelds. In SIGIR-03.\nPipatsrisawat, K. and Darwiche, A. (2007). RSat\n2.0: SAT solver description. Tech. rep. D–153, Au-\ntomated Reasoning Group, Computer Science De-\npartment, University of California, Los Angeles.\nPlaat, A., Schaeffer, J., Pijls, W., and de Bruin, A.\n(1996). Best-ﬁrst ﬁxed-depth minimax algorithms.\nAIJ, 87(1–2), 255–293.\nPlace, U. T. (1956). Is consciousness a brain pro-\ncess? British Journal of Psychology, 47, 44–50.\nPlatt, J. (1999). Fast training of support vector ma-\nchines using sequential minimal optimization. In Ad-\nvances in Kernel Methods: Support Vector Learning,\npp. 185–208. MIT Press.\nPlotkin, G. (1971). Automatic Methods of Inductive",
  "Platt, J. (1999). Fast training of support vector ma-\nchines using sequential minimal optimization. In Ad-\nvances in Kernel Methods: Support Vector Learning,\npp. 185–208. MIT Press.\nPlotkin, G. (1971). Automatic Methods of Inductive\nInference. Ph.D. thesis, Edinburgh University.\nPlotkin, G. (1972). Building-in equational theories.\nIn Meltzer, B. and Michie, D. (Eds.), Machine Intel-\nligence 7, pp. 73–90. Edinburgh University Press.\nPohl, I. (1971). Bi-directional search. In Meltzer, B.\nand Michie, D. (Eds.), Machine Intelligence 6, pp.\n127–140. Edinburgh University Press.\nPohl, I. (1973). The avoidance of (relative) catastro-\nphe, heuristic competence, genuine dynamic weight-\ning and computational issues in heuristic problem\nsolving. In IJCAI-73, pp. 20–23.\nPohl, I. (1977). Practical and theoretical considera-\ntions in heuristic search algorithms. In Elcock, E. W.\nand Michie, D. (Eds.), Machine Intelligence 8, pp.\n55–72. Ellis Horwood.\nPoli, R., Langdon, W., and McPhee, N. (2008). A\nField Guide to Genetic Programming. Lulu.com.\nPomerleau, D. A. (1993). Neural Network Percep-\ntion for Mobile Robot Guidance. Kluwer.\nPonte, J. and Croft, W. B. (1998). A language mod-\neling approach to information retrieval. In SIGIR-98,\npp. 275–281.\nPoole, D. (1993). Probabilistic Horn abduction and\nBayesian networks. AIJ, 64, 81–129.\nPoole, D. (2003). First-order probabilistic inference.\nIn IJCAI-03, pp. 985–991.\nPoole, D., Mackworth, A. K., and Goebel, R. (1998).\nComputational intelligence:\nA logical approach.\nOxford University Press. 1086\nBibliography\nPopper, K. R. (1959). The Logic of Scientiﬁc Dis-\ncovery. Basic Books.\nPopper, K. R. (1962). Conjectures and Refutations:\nThe Growth of Scientiﬁc Knowledge. Basic Books.\nPortner, P. and Partee, B. H. (2002). Formal Seman-\ntics: The Essential Readings. Wiley-Blackwell.\nPost, E. L. (1921). Introduction to a general the-\nory of elementary propositions. American Journal\nof Mathematics, 43, 163–185.\nPoundstone, W. (1993). Prisoner’s Dilemma. An-\nchor.\nPourret, O., Na¨ım, P., and Marcot, B. (2008).\nBayesian Networks: A practical guide to applica-\ntions. Wiley.\nPrades, J. L. P., Loomes, G., and Brey, R. (2008).\nTrying to estmate a monetary value for the QALY.\nTech. rep. WP Econ 08.09, Univ. Pablo Olavide.\nPradhan, M., Provan, G. M., Middleton, B., and\nHenrion, M. (1994).\nKnowledge engineering for\nlarge belief networks. In UAI-94, pp. 484–490.\nPrawitz, D. (1960). An improved proof procedure.\nTheoria, 26, 102–139.",
  "Pradhan, M., Provan, G. M., Middleton, B., and\nHenrion, M. (1994).\nKnowledge engineering for\nlarge belief networks. In UAI-94, pp. 484–490.\nPrawitz, D. (1960). An improved proof procedure.\nTheoria, 26, 102–139.\nPress, W. H., Teukolsky, S. A., Vetterling, W. T., and\nFlannery, B. P. (2007). Numerical Recipes: The Art\nof Scientiﬁc Computing (third edition). Cambridge\nUniversity Press.\nPreston, J. and Bishop, M. (2002). Views into the\nChinese Room: New Essays on Searle and Artiﬁcial\nIntelligence. Oxford University Press.\nPrieditis, A. E. (1993). Machine discovery of effec-\ntive admissible heuristics. Machine Learning, 12(1–\n3), 117–141.\nPrinz, D. G. (1952). Robot chess. Research, 5, 261–\n266.\nProsser, P. (1993). Hybrid algorithms for constraint\nsatisfaction problems. Computational Intelligence,\n9, 268–299.\nPullum, G. K. (1991). The Great Eskimo Vocabu-\nlary Hoax (and Other Irreverent Essays on the Study\nof Language). University of Chicago Press.\nPullum, G. K. (1996). Learnability, hyperlearning,\nand the poverty of the stimulus.\nIn 22nd Annual\nMeeting of the Berkeley Linguistics Society.\nPuterman, M. L. (1994).\nMarkov Decision Pro-\ncesses: Discrete Stochastic Dynamic Programming.\nWiley.\nPuterman, M. L. and Shin, M. C. (1978). Modiﬁed\npolicy iteration algorithms for discounted Markov\ndecision problems.\nManagement Science, 24(11),\n1127–1137.\nPutnam, H. (1960). Minds and machines. In Hook,\nS. (Ed.), Dimensions of Mind, pp. 138–164. Macmil-\nlan.\nPutnam, H. (1963). ‘Degree of conﬁrmation’ and\ninductive logic. In Schilpp, P. A. (Ed.), The Philoso-\nphy of Rudolf Carnap, pp. 270–292. Open Court.\nPutnam, H. (1967).\nThe nature of mental states.\nIn Capitan, W. H. and Merrill, D. D. (Eds.), Art,\nMind, and Religion, pp. 37–48. University of Pitts-\nburgh Press.\nPutnam, H. (1975). The meaning of “meaning”. In\nGunderson, K. (Ed.), Language, Mind and Knowl-\nedge: Minnesota Studies in the Philosophy of Sci-\nence. University of Minnesota Press.\nPylyshyn, Z. W. (1974). Minds, machines and phe-\nnomenology: Some reﬂections on Dreyfus’ “What\nComputers Can’t Do”. Int. J. Cognitive Psychology,\n3(1), 57–77.\nPylyshyn, Z. W. (1984). Computation and Cogni-\ntion: Toward a Foundation for Cognitive Science.\nMIT Press.\nQuillian, M. R. (1961). A design for an understand-\ning machine. Paper presented at a colloquium: Se-\nmantic Problems in Natural Language, King’s Col-\nlege, Cambridge, England.\nQuine, W. V. (1953). Two dogmas of empiricism.\nIn From a Logical Point of View, pp. 20–46. Harper",
  "ing machine. Paper presented at a colloquium: Se-\nmantic Problems in Natural Language, King’s Col-\nlege, Cambridge, England.\nQuine, W. V. (1953). Two dogmas of empiricism.\nIn From a Logical Point of View, pp. 20–46. Harper\nand Row.\nQuine, W. V. (1960). Word and Object. MIT Press.\nQuine, W. V. (1982). Methods of Logic (fourth edi-\ntion). Harvard University Press.\nQuinlan, J. R. (1979). Discovering rules from large\ncollections of examples: A case study. In Michie,\nD. (Ed.), Expert Systems in the Microelectronic Age.\nEdinburgh University Press.\nQuinlan, J. R. (1986). Induction of decision trees.\nMachine Learning, 1, 81–106.\nQuinlan, J. R. (1990). Learning logical deﬁnitions\nfrom relations. Machine Learning, 5(3), 239–266.\nQuinlan, J. R. (1993). C4.5: Programs for machine\nlearning. Morgan Kaufmann.\nQuinlan, J. R. and Cameron-Jones, R. M. (1993).\nFOIL: A midterm report. In ECML-93, pp. 3–20.\nQuirk, R., Greenbaum, S., Leech, G., and Svartvik,\nJ. (1985). A Comprehensive Grammar of the English\nLanguage. Longman.\nRabani, Y., Rabinovich, Y., and Sinclair, A. (1998).\nA computational view of population genetics. Ran-\ndom Structures and Algorithms, 12(4), 313–334.\nRabiner, L. R. and Juang, B.-H. (1993). Fundamen-\ntals of Speech Recognition. Prentice-Hall.\nRalphs, T. K., Ladanyi, L., and Saltzman, M. J.\n(2004). A library hierarchy for implementing scal-\nable parallel search algorithms. J. Supercomputing,\n28(2), 215–234.\nRamanan, D., Forsyth, D., and Zisserman, A.\n(2007). Tracking people by learning their appear-\nance. IEEE Pattern Analysis and Machine Intelli-\ngence.\nRamsey, F. P. (1931).\nTruth and probability.\nIn\nBraithwaite, R. B. (Ed.), The Foundations of Math-\nematics and Other Logical Essays. Harcourt Brace\nJovanovich.\nRanzato, M., Poultney, C., Chopra, S., and LeCun,\nY. (2007). Efﬁcient learning of sparse representa-\ntions with an energy-based model. In NIPS 19, pp.\n1137–1144.\nRaphson, J. (1690). Analysis aequationum univer-\nsalis. Apud Abelem Swalle, London.\nRashevsky, N. (1936).\nPhysico-mathematical as-\npects of excitation and conduction in nerves. In Cold\nSprings Harbor Symposia on Quantitative Biology.\nIV: Excitation Phenomena, pp. 90–97.\nRashevsky, N. (1938).\nMathematical Biophysics:\nPhysico-Mathematical Foundations of Biology. Uni-\nversity of Chicago Press.\nRasmussen,\nC.\nE.\nand\nWilliams,\nC.\nK.\nI.\n(2006). Gaussian Processes for Machine Learning.\nMIT Press.\nRassenti, S., Smith, V., and Bulﬁn, R. (1982). A\ncombinatorial auction mechanism for airport time",
  "versity of Chicago Press.\nRasmussen,\nC.\nE.\nand\nWilliams,\nC.\nK.\nI.\n(2006). Gaussian Processes for Machine Learning.\nMIT Press.\nRassenti, S., Smith, V., and Bulﬁn, R. (1982). A\ncombinatorial auction mechanism for airport time\nslot allocation. Bell Journal of Economics, 13, 402–\n417.\nRatner, D. and Warmuth, M. (1986).\nFinding a\nshortest solution for the n × n extension of the\n15-puzzle is intractable. In AAAI-86, Vol. 1, pp. 168–\n172.\nRauch, H. E., Tung, F., and Striebel, C. T. (1965).\nMaximum likelihood estimates of linear dynamic\nsystems. AIAA Journal, 3(8), 1445–1450.\nRayward-Smith, V., Osman, I., Reeves, C., and\nSmith, G. (Eds.). (1996). Modern Heuristic Search\nMethods. Wiley.\nRechenberg, I. (1965). Cybernetic solution path of\nan experimental problem. Library translation 1122,\nRoyal Aircraft Establishment.\nReeson, C. G., Huang, K.-C., Bayer, K. M., and\nChoueiry, B. Y. (2007). An interactive constraint-\nbased approach to sudoku. In AAAI-07, pp. 1976–\n1977.\nRegin, J. (1994).\nA ﬁltering algorithm for con-\nstraints of difference in CSPs. In AAAI-94, pp. 362–\n367.\nReichenbach, H. (1949). The Theory of Probabil-\nity: An Inquiry into the Logical and Mathematical\nFoundations of the Calculus of Probability (second\nedition). University of California Press.\nReid, D. B. (1979). An algorithm for tracking mul-\ntiple targets. IEEE Trans. Automatic Control, 24(6),\n843–854.\nReif, J. (1979).\nComplexity of the mover’s prob-\nlem and generalizations. In FOCS-79, pp. 421–427.\nIEEE.\nReiter, R. (1980). A logic for default reasoning. AIJ,\n13(1–2), 81–132.\nReiter, R. (1991). The frame problem in the situ-\nation calculus: A simple solution (sometimes) and\na completeness result for goal regression.\nIn Lif-\nschitz, V. (Ed.), Artiﬁcial Intelligence and Mathe-\nmatical Theory of Computation: Papers in Honor of\nJohn McCarthy, pp. 359–380. Academic Press.\nReiter, R. (2001). Knowledge in Action: Logical\nFoundations for Specifying and Implementing Dy-\nnamical Systems. MIT Press.\nRenner, G. and Ekart, A. (2003).\nGenetic algo-\nrithms in computer aided design. Computer Aided\nDesign, 35(8), 709–726.\nR´enyi,\nA.\n(1970).\nProbability\nTheory.\nElsevier/North-Holland.\nReynolds, C. W. (1987). Flocks, herds, and schools:\nA distributed behavioral model. Computer Graph-\nics, 21, 25–34.\nSIGGRAPH ’87 Conference Pro-\nceedings.\nRiazanov, A. and Voronkov, A. (2002). The design\nand implementation of VAMPIRE. AI Communica-\ntions, 15(2–3), 91–110.\nRich, E. and Knight, K. (1991). Artiﬁcial Intelli-",
  "ics, 21, 25–34.\nSIGGRAPH ’87 Conference Pro-\nceedings.\nRiazanov, A. and Voronkov, A. (2002). The design\nand implementation of VAMPIRE. AI Communica-\ntions, 15(2–3), 91–110.\nRich, E. and Knight, K. (1991). Artiﬁcial Intelli-\ngence (second edition). McGraw-Hill.\nRichards, M. and Amir, E. (2007). Opponent mod-\neling in Scrabble. In IJCAI-07.\nRichardson, M., Bilmes, J., and Diorio, C. (2000).\nHidden-articulator Markov models:\nPerformance\nimprovements and robustness to noise. In ICASSP-\n00.\nRichter, S. and Westphal, M. (2008). The LAMA\nplanner. In Proc. International Planning Competi-\ntion at ICAPS.\nRidley, M. (2004). Evolution. Oxford Reader.\nRieger, C. (1976). An organization of knowledge for\nproblem solving and language comprehension. AIJ,\n7, 89–127. Bibliography\n1087\nRiley, J. and Samuelson, W. (1981). Optimal auc-\ntions. American Economic Review, 71, 381–392.\nRiloff, E. (1993). Automatically constructing a dic-\ntionary for information extraction tasks. In AAAI-93,\npp. 811–816.\nRintanen, J. (1999). Improvements to the evalua-\ntion of quantiﬁed Boolean formulae. In IJCAI-99,\npp. 1192–1197.\nRintanen, J. (2007). Asymptotically optimal encod-\nings of conformant planning in QBF. In AAAI-07,\npp. 1045–1050.\nRipley, B. D. (1996). Pattern Recognition and Neu-\nral Networks. Cambridge University Press.\nRissanen, J. (1984). Universal coding, information,\nprediction, and estimation.\nIEEE Transactions on\nInformation Theory, IT-30(4), 629–636.\nRissanen, J. (2007). Information and Complexity in\nStatistical Modeling. Springer.\nRitchie, G. D. and Hanna, F. K. (1984). AM: A case\nstudy in AI methodology. AIJ, 23(3), 249–268.\nRivest, R. (1987). Learning decision lists. Machine\nLearning, 2(3), 229–246.\nRoberts, L. G. (1963). Machine perception of three-\ndimensional solids. Technical report 315, MIT Lin-\ncoln Laboratory.\nRobertson, N. and Seymour, P. D. (1986). Graph\nminors. II. Algorithmic aspects of tree-width. J. Al-\ngorithms, 7(3), 309–322.\nRobertson, S. E. (1977). The probability ranking\nprinciple in IR. J. Documentation, 33, 294–304.\nRobertson, S. E. and Sparck Jones, K. (1976). Rel-\nevance weighting of search terms. J. American Soci-\nety for Information Science, 27, 129–146.\nRobinson, A. and Voronkov, A. (2001). Handbook\nof Automated Reasoning. Elsevier.\nRobinson, J. A. (1965). A machine-oriented logic\nbased on the resolution principle. JACM, 12, 23–41.\nRoche, E. and Schabes, Y. (1997). Finite-State Lan-\nguage Processing (Language, Speech and Commu-",
  "of Automated Reasoning. Elsevier.\nRobinson, J. A. (1965). A machine-oriented logic\nbased on the resolution principle. JACM, 12, 23–41.\nRoche, E. and Schabes, Y. (1997). Finite-State Lan-\nguage Processing (Language, Speech and Commu-\nnication). Bradford Books.\nRock, I. (1984). Perception. W. H. Freeman.\nRosenblatt, F. (1957). The perceptron: A perceiv-\ning and recognizing automaton. Report 85-460-1,\nProject PARA, Cornell Aeronautical Laboratory.\nRosenblatt, F. (1960). On the convergence of rein-\nforcement procedures in simple perceptrons. Report\nVG-1196-G-4, Cornell Aeronautical Laboratory.\nRosenblatt, F. (1962). Principles of Neurodynam-\nics: Perceptrons and the Theory of Brain Mecha-\nnisms. Spartan.\nRosenblatt, M. (1956). Remarks on some nonpara-\nmetric estimates of a density function.\nAnnals of\nMathematical Statistics, 27, 832–837.\nRosenblueth, A., Wiener, N., and Bigelow, J.\n(1943). Behavior, purpose, and teleology. Philos-\nophy of Science, 10, 18–24.\nRosenschein, J. S. and Zlotkin, G. (1994). Rules of\nEncounter. MIT Press.\nRosenschein, S. J. (1985).\nFormal theories of\nknowledge in AI and robotics.\nNew Generation\nComputing, 3(4), 345–357.\nRoss, P. E. (2004).\nPsyching out computer chess\nplayers. IEEE Spectrum, 41(2), 14–15.\nRoss, S. M. (1988). A First Course in Probability\n(third edition). Macmillan.\nRossi, F., van Beek, P., and Walsh, T. (2006). Hand-\nbook of Constraint Processing. Elsevier.\nRoussel, P. (1975). Prolog: Manual de reference et\nd’utilization. Tech. rep., Groupe d’Intelligence Arti-\nﬁcielle, Universit´e d’Aix-Marseille.\nRouveirol, C. and Puget, J.-F. (1989). A simple and\ngeneral solution for inverting resolution.\nIn Proc.\nEuropean Working Session on Learning, pp. 201–\n210.\nRowat, P. F. (1979). Representing the Spatial Ex-\nperience and Solving Spatial problems in a Simu-\nlated Robot Environment. Ph.D. thesis, University\nof British Columbia.\nRoweis, S. T. and Ghahramani, Z. (1999). A unify-\ning review of Linear Gaussian Models. Neural Com-\nputation, 11(2), 305–345.\nRowley, H., Baluja, S., and Kanade, T. (1996). Neu-\nral network-based face detection. In CVPR, pp. 203–\n208.\nRoy, N., Gordon, G., and Thrun, S. (2005). Finding\napproximate POMDP solutions through belief com-\npression. JAIR, 23, 1–40.\nRubin, D. (1988). Using the SIR algorithm to sim-\nulate posterior distributions.\nIn Bernardo, J. M.,\nde Groot, M. H., Lindley, D. V., and Smith, A. F. M.\n(Eds.), Bayesian Statistics 3, pp. 395–402. Oxford\nUniversity Press.",
  "pression. JAIR, 23, 1–40.\nRubin, D. (1988). Using the SIR algorithm to sim-\nulate posterior distributions.\nIn Bernardo, J. M.,\nde Groot, M. H., Lindley, D. V., and Smith, A. F. M.\n(Eds.), Bayesian Statistics 3, pp. 395–402. Oxford\nUniversity Press.\nRumelhart, D. E., Hinton, G. E., and Williams, R. J.\n(1986a). Learning internal representations by error\npropagation. In Rumelhart, D. E. and McClelland,\nJ. L. (Eds.), Parallel Distributed Processing, Vol. 1,\nchap. 8, pp. 318–362. MIT Press.\nRumelhart, D. E., Hinton, G. E., and Williams,\nR. J. (1986b).\nLearning representations by back-\npropagating errors. Nature, 323, 533–536.\nRumelhart, D. E. and McClelland, J. L. (Eds.).\n(1986). Parallel Distributed Processing. MIT Press.\nRummery, G. A. and Niranjan, M. (1994).\nOn-\nline Q-learning using connectionist systems. Tech.\nrep. CUED/F-INFENG/TR 166, Cambridge Univer-\nsity Engineering Department.\nRuspini, E. H., Lowrance, J. D., and Strat, T. M.\n(1992). Understanding evidential reasoning. IJAR,\n6(3), 401–424.\nRussell, J. G. B. (1990). Is screening for abdom-\ninal aortic aneurysm worthwhile? Clinical Radiol-\nogy, 41, 182–184.\nRussell, S. J. (1985). The compleat guide to MRS.\nReport STAN-CS-85-1080, Computer Science De-\npartment, Stanford University.\nRussell, S. J. (1986). A quantitative analysis of anal-\nogy by similarity. In AAAI-86, pp. 284–288.\nRussell, S. J. (1988). Tree-structured bias. In AAAI-\n88, Vol. 2, pp. 641–645.\nRussell, S. J. (1992).\nEfﬁcient memory-bounded\nsearch methods. In ECAI-92, pp. 1–5.\nRussell, S. J. (1998). Learning agents for uncertain\nenvironments (extended abstract). In COLT-98, pp.\n101–103.\nRussell, S. J., Binder, J., Koller, D., and Kanazawa,\nK. (1995). Local learning in probabilistic networks\nwith hidden variables. In IJCAI-95, pp. 1146–52.\nRussell, S. J. and Grosof, B. (1987). A declarative\napproach to bias in concept learning. In AAAI-87.\nRussell, S. J. and Norvig, P. (2003). Artiﬁcial Intelli-\ngence: A Modern Approach (2nd edition). Prentice-\nHall.\nRussell, S. J. and Subramanian, D. (1995). Provably\nbounded-optimal agents. JAIR, 3, 575–609.\nRussell, S. J., Subramanian, D., and Parr, R. (1993).\nProvably bounded optimal agents. In IJCAI-93, pp.\n338–345.\nRussell, S. J. and Wefald, E. H. (1989). On optimal\ngame-tree search using rational meta-reasoning. In\nIJCAI-89, pp. 334–340.\nRussell, S. J. and Wefald, E. H. (1991). Do the Right\nThing: Studies in Limited Rationality. MIT Press.\nRussell, S. J. and Wolfe, J. (2005).\nEfﬁcient",
  "game-tree search using rational meta-reasoning. In\nIJCAI-89, pp. 334–340.\nRussell, S. J. and Wefald, E. H. (1991). Do the Right\nThing: Studies in Limited Rationality. MIT Press.\nRussell, S. J. and Wolfe, J. (2005).\nEfﬁcient\nbelief-state AND-OR search, with applications to\nKriegspiel. In IJCAI-05, pp. 278–285.\nRussell, S. J. and Zimdars, A. (2003).\nQ-\ndecomposition of reinforcement learning agents. In\nICML-03.\nRustagi, J. S. (1976). Variational Methods in Statis-\ntics. Academic Press.\nSabin, D. and Freuder, E. C. (1994). Contradicting\nconventional wisdom in constraint satisfaction. In\nECAI-94, pp. 125–129.\nSacerdoti, E. D. (1974). Planning in a hierarchy of\nabstraction spaces. AIJ, 5(2), 115–135.\nSacerdoti, E. D. (1975).\nThe nonlinear nature of\nplans. In IJCAI-75, pp. 206–214.\nSacerdoti, E. D. (1977). A Structure for Plans and\nBehavior. Elsevier/North-Holland.\nSadri, F. and Kowalski, R. (1995). Variants of the\nevent calculus. In ICLP-95, pp. 67–81.\nSahami, M., Dumais, S. T., Heckerman, D., and\nHorvitz, E. J. (1998). A Bayesian approach to ﬁl-\ntering junk E-mail. In Learning for Text Categoriza-\ntion: Papers from the 1998 Workshop.\nSahami, M., Hearst, M. A., and Saund, E. (1996).\nApplying the multiple cause mixture model to text\ncategorization. In ICML-96, pp. 435–443.\nSahin, N. T., Pinker, S., Cash, S. S., Schomer, D.,\nand Halgren, E. (2009).\nSequential processing of\nlexical, grammatical, and phonological information\nwithin Broca’s area. Science, 326(5291), 445–449.\nSakuta, M. and Iida, H. (2002).\nAND/OR-tree\nsearch for solving problems with uncertainty: A case\nstudy using screen-shogi problems.\nIPSJ Journal,\n43(01).\nSalomaa, A. (1969).\nProbabilistic and weighted\ngrammars. Information and Control, 15, 529–544.\nSalton, G., Wong, A., and Yang, C. S. (1975). A\nvector space model for automatic indexing. CACM,\n18(11), 613–620.\nSamuel, A. L. (1959).\nSome studies in machine\nlearning using the game of checkers. IBM Journal\nof Research and Development, 3(3), 210–229.\nSamuel, A. L. (1967).\nSome studies in machine\nlearning using the game of checkers II—Recent\nprogress.\nIBM Journal of Research and Develop-\nment, 11(6), 601–617.\nSamuelsson, C. and Rayner, M. (1991). Quantita-\ntive evaluation of explanation-based learning as an\noptimization tool for a large-scale natural language\nsystem. In IJCAI-91, pp. 609–615.\nSarawagi, S. (2007). Information extraction. Foun-\ndations and Trends in Databases, 1(3), 261–377.\nSatia, J. K. and Lave, R. E. (1973).\nMarkovian",
  "optimization tool for a large-scale natural language\nsystem. In IJCAI-91, pp. 609–615.\nSarawagi, S. (2007). Information extraction. Foun-\ndations and Trends in Databases, 1(3), 261–377.\nSatia, J. K. and Lave, R. E. (1973).\nMarkovian\ndecision processes with probabilistic observation of\nstates. Management Science, 20(1), 1–13.\nSato, T. and Kameya, Y. (1997).\nPRISM: A\nsymbolic-statistical modeling language. In IJCAI-\n97, pp. 1330–1335. 1088\nBibliography\nSaul, L. K., Jaakkola, T., and Jordan, M. I. (1996).\nMean ﬁeld theory for sigmoid belief networks. JAIR,\n4, 61–76.\nSavage, L. J. (1954). The Foundations of Statistics.\nWiley.\nSayre, K. (1993). Three more ﬂaws in the compu-\ntational model. Paper presented at the APA (Central\nDivision) Annual Conference, Chicago, Illinois.\nSchaeffer, J. (2008). One Jump Ahead: Computer\nPerfection at Checkers. Springer-Verlag.\nSchaeffer, J., Burch, N., Bjornsson, Y., Kishimoto,\nA., M¨uller, M., Lake, R., Lu, P., and Sutphen, S.\n(2007). Checkers is solved. Science, 317, 1518–\n1522.\nSchank, R. C. and Abelson, R. P. (1977). Scripts,\nPlans, Goals, and Understanding.\nLawrence Erl-\nbaum Associates.\nSchank, R. C. and Riesbeck, C. (1981). Inside Com-\nputer Understanding: Five Programs Plus Minia-\ntures. Lawrence Erlbaum Associates.\nSchapire, R. E. and Singer, Y. (2000). Boostexter: A\nboosting-based system for text categorization. Ma-\nchine Learning, 39(2/3), 135–168.\nSchapire, R. E. (1990). The strength of weak learn-\nability. Machine Learning, 5(2), 197–227.\nSchapire, R. E. (2003). The boosting approach to\nmachine learning: An overview. In Denison, D. D.,\nHansen, M. H., Holmes, C., Mallick, B., and Yu,\nB. (Eds.), Nonlinear Estimation and Classiﬁcation.\nSpringer.\nSchmid, C. and Mohr, R. (1996). Combining grey-\nvalue invariants with local constraints for object\nrecognition. In CVPR.\nSchmolze, J. G. and Lipkis, T. A. (1983). Classi-\nﬁcation in the KL-ONE representation system. In\nIJCAI-83, pp. 330–332.\nSch¨olkopf, B. and Smola, A. J. (2002). Learning\nwith Kernels. MIT Press.\nSch¨oning, T. (1999). A probabilistic algorithm for k-\nSAT and constraint satisfaction problems. In FOCS-\n99, pp. 410–414.\nSchoppers, M. J. (1987). Universal plans for reac-\ntive robots in unpredictable environments. In IJCAI-\n87, pp. 1039–1046.\nSchoppers, M. J. (1989).\nIn defense of reaction\nplans as caches. AIMag, 10(4), 51–60.\nSchr¨oder, E. (1877).\nDer Operationskreis des\nLogikkalk¨uls. B. G. Teubner, Leipzig.\nSchultz, W., Dayan, P., and Montague, P. R. (1997).",
  "87, pp. 1039–1046.\nSchoppers, M. J. (1989).\nIn defense of reaction\nplans as caches. AIMag, 10(4), 51–60.\nSchr¨oder, E. (1877).\nDer Operationskreis des\nLogikkalk¨uls. B. G. Teubner, Leipzig.\nSchultz, W., Dayan, P., and Montague, P. R. (1997).\nA neural substrate of prediction and reward. Science,\n275, 1593.\nSchulz, D., Burgard, W., Fox, D., and Cremers,\nA. B. (2003). People tracking with mobile robots\nusing sample-based joint probabilistic data associa-\ntion ﬁlters. Int. J. Robotics Research, 22(2), 99–116.\nSchulz, S. (2004). System Description: E 0.81. In\nProc. International Joint Conference on Automated\nReasoning, Vol. 3097 of LNAI, pp. 223–228.\nSch¨utze, H. (1995). Ambiguity in Language Learn-\ning: Computational and Cognitive Models. Ph.D.\nthesis, Stanford University. Also published by CSLI\nPress, 1997.\nSchwartz, J. T., Scharir, M., and Hopcroft, J. (1987).\nPlanning, Geometry and Complexity of Robot Mo-\ntion. Ablex Publishing Corporation.\nSchwartz, S. P. (Ed.). (1977). Naming, Necessity,\nand Natural Kinds. Cornell University Press.\nScott, D. and Krauss, P. (1966). Assigning probabil-\nities to logical formulas. In Hintikka, J. and Suppes,\nP. (Eds.), Aspects of Inductive Logic. North-Holland.\nSearle, J. R. (1980). Minds, brains, and programs.\nBBS, 3, 417–457.\nSearle, J. R. (1984).\nMinds, Brains and Science.\nHarvard University Press.\nSearle, J. R. (1990). Is the brain’s mind a computer\nprogram? Scientiﬁc American, 262, 26–31.\nSearle, J. R. (1992). The Rediscovery of the Mind.\nMIT Press.\nSebastiani, F. (2002).\nMachine learning in auto-\nmated text categorization. ACM Computing Surveys,\n34(1), 1–47.\nSegaran, T. (2007).\nProgramming Collective In-\ntelligence: Building Smart Web 2.0 Applications.\nO’Reilly.\nSelman, B., Kautz, H., and Cohen, B. (1996). Lo-\ncal search strategies for satisﬁability testing. In DI-\nMACS Series in Discrete Mathematics and Theo-\nretical Computer Science, Volume 26, pp. 521–532.\nAmerican Mathematical Society.\nSelman, B. and Levesque, H. J. (1993). The com-\nplexity of path-based defeasible inheritance.\nAIJ,\n62(2), 303–339.\nSelman, B., Levesque, H. J., and Mitchell, D.\n(1992). A new method for solving hard satisﬁability\nproblems. In AAAI-92, pp. 440–446.\nSha, F. and Pereira, F. (2003). Shallow parsing with\nconditional random ﬁelds. Technical report CIS TR\nMS-CIS-02-35, Univ. of Penn.\nShachter, R. D. (1986). Evaluating inﬂuence dia-\ngrams. Operations Research, 34, 871–882.\nShachter, R. D. (1998). Bayes-ball: The rational",
  "conditional random ﬁelds. Technical report CIS TR\nMS-CIS-02-35, Univ. of Penn.\nShachter, R. D. (1986). Evaluating inﬂuence dia-\ngrams. Operations Research, 34, 871–882.\nShachter, R. D. (1998). Bayes-ball: The rational\npastime (for determining irrelevance and requisite\ninformation in belief networks and inﬂuence dia-\ngrams). In UAI-98, pp. 480–487.\nShachter, R. D., D’Ambrosio, B., and Del Favero,\nB. A. (1990).\nSymbolic probabilistic inference in\nbelief networks. In AAAI-90, pp. 126–131.\nShachter, R. D. and Kenley, C. R. (1989). Gaussian\ninﬂuence diagrams.\nManagement Science, 35(5),\n527–550.\nShachter, R. D. and Peot, M. (1989). Simulation ap-\nproaches to general probabilistic inference on belief\nnetworks. In UAI-98.\nShachter, R. D. and Heckerman, D. (1987). Think-\ning backward for knowledge acquisition.\nAIMag,\n3(Fall).\nShafer, G. (1976). A Mathematical Theory of Evi-\ndence. Princeton University Press.\nShahookar, K. and Mazumder, P. (1991). VLSI cell\nplacement techniques.\nComputing Surveys, 23(2),\n143–220.\nShanahan, M. (1997). Solving the Frame Problem.\nMIT Press.\nShanahan, M. (1999). The event calculus explained.\nIn Wooldridge, M. J. and Veloso, M. (Eds.), Ar-\ntiﬁcial Intelligence Today, pp. 409–430. Springer-\nVerlag.\nShankar, N. (1986). Proof-Checking Metamathe-\nmatics. Ph.D. thesis, Computer Science Department,\nUniversity of Texas at Austin.\nShannon, C. E. and Weaver, W. (1949). The Math-\nematical Theory of Communication. University of\nIllinois Press.\nShannon, C. E. (1948). A mathematical theory of\ncommunication. Bell Systems Technical Journal, 27,\n379–423, 623–656.\nShannon, C. E. (1950). Programming a computer\nfor playing chess. Philosophical Magazine, 41(4),\n256–275.\nShaparau, D., Pistore, M., and Traverso, P. (2008).\nFusing procedural and declarative planning goals for\nnondeterministic domains. In AAAI-08.\nShapiro, E. (1981). An algorithm that infers theories\nfrom facts. In IJCAI-81, p. 1064.\nShapiro, S. C. (Ed.). (1992). Encyclopedia of Artiﬁ-\ncial Intelligence (second edition). Wiley.\nShapley, S. (1953). Stochastic games. In PNAS,\nVol. 39, pp. 1095–1100.\nShatkay, H. and Kaelbling, L. P. (1997). Learning\ntopological maps with weak local odometric infor-\nmation. In IJCAI-97.\nShelley, M. (1818). Frankenstein: Or, the Modern\nPrometheus. Pickering and Chatto.\nSheppard, B. (2002). World-championship-caliber\nscrabble. AIJ, 134(1–2), 241–275.\nShi, J. and Malik, J. (2000). Normalized cuts and\nimage segmentation. PAMI, 22(8), 888–905.",
  "Prometheus. Pickering and Chatto.\nSheppard, B. (2002). World-championship-caliber\nscrabble. AIJ, 134(1–2), 241–275.\nShi, J. and Malik, J. (2000). Normalized cuts and\nimage segmentation. PAMI, 22(8), 888–905.\nShieber, S. (1994). Lessons from a restricted Turing\nTest. CACM, 37, 70–78.\nShieber, S. (Ed.). (2004). The Turing Test. MIT\nPress.\nShoham, Y. (1993). Agent-oriented programming.\nAIJ, 60(1), 51–92.\nShoham, Y. (1994).\nArtiﬁcial Intelligence Tech-\nniques in Prolog. Morgan Kaufmann.\nShoham, Y. and Leyton-Brown, K. (2009).\nMul-\ntiagent Systems: Algorithmic, Game-Theoretic, and\nLogical Foundations. Cambridge Univ. Press.\nShoham, Y., Powers, R., and Grenager, T. (2004). If\nmulti-agent learning is the answer, what is the ques-\ntion? In Proc. AAAI Fall Symposium on Artiﬁcial\nMulti-Agent Learning.\nShortliffe, E. H. (1976). Computer-Based Medical\nConsultations: MYCIN. Elsevier/North-Holland.\nSietsma, J. and Dow, R. J. F. (1988).\nNeural net\npruning—Why and how. In IEEE International Con-\nference on Neural Networks, pp. 325–333.\nSiklossy, L. and Dreussi, J. (1973).\nAn efﬁcient\nrobot planner which generates its own procedures.\nIn IJCAI-73, pp. 423–430.\nSilverstein, C., Henzinger, M., Marais, H., and\nMoricz, M. (1998). Analysis of a very large altavista\nquery log. Tech. rep. 1998-014, Digital Systems Re-\nsearch Center.\nSimmons, R. and Koenig, S. (1995).\nProbabilis-\ntic robot navigation in partially observable environ-\nments. In IJCAI-95, pp. 1080–1087. IJCAI, Inc.\nSimon, D. (2006).\nOptimal State Estimation:\nKalman, H Inﬁnity, and Nonlinear Approaches. Wi-\nley.\nSimon, H. A. (1947).\nAdministrative behavior.\nMacmillan.\nSimon, H. A. (1957). Models of Man: Social and\nRational. John Wiley.\nSimon, H. A. (1963). Experiments with a heuristic\ncompiler. JACM, 10, 493–506.\nSimon, H. A. (1981). The Sciences of the Artiﬁcial\n(second edition). MIT Press. Bibliography\n1089\nSimon, H. A. (1982). Models of Bounded Rational-\nity, Volume 1. The MIT Press.\nSimon, H. A. and Newell, A. (1958).\nHeuristic\nproblem solving: The next advance in operations re-\nsearch. Operations Research, 6, 1–10.\nSimon, H. A. and Newell, A. (1961).\nComputer\nsimulation of human thinking and problem solving.\nDatamation, June/July, 35–37.\nSimon, J. C. and Dubois, O. (1989).\nNumber of\nsolutions to satisﬁability instances—Applications to\nknowledge bases. AIJ, 3, 53–65.\nSimonis, H. (2005). Sudoku as a constraint prob-\nlem. In CP Workshop on Modeling and Reformulat-\ning Constraint Satisfaction Problems, pp. 13–27.",
  "Number of\nsolutions to satisﬁability instances—Applications to\nknowledge bases. AIJ, 3, 53–65.\nSimonis, H. (2005). Sudoku as a constraint prob-\nlem. In CP Workshop on Modeling and Reformulat-\ning Constraint Satisfaction Problems, pp. 13–27.\nSinger, P. W. (2009). Wired for War. Penguin Press.\nSingh, P., Lin, T., Mueller, E. T., Lim, G., Perkins,\nT., and Zhu, W. L. (2002).\nOpen mind common\nsense: Knowledge acquisition from the general pub-\nlic. In Proc. First International Conference on On-\ntologies, Databases, and Applications of Semantics\nfor Large Scale Information Systems.\nSinghal, A., Buckley, C., and Mitra, M. (1996). Piv-\noted document length normalization. In SIGIR-96,\npp. 21–29.\nSittler, R. W. (1964). An optimal data association\nproblem in surveillance theory. IEEE Transactions\non Military Electronics, 8(2), 125–139.\nSkinner, B. F. (1953). Science and Human Behav-\nior. Macmillan.\nSkolem, T. (1920). Logisch-kombinatorische Unter-\nsuchungen ¨uber die Erf¨ullbarkeit oder Beweisbarkeit\nmathematischer S¨atze nebst einem Theoreme ¨uber\ndie dichte Mengen. Videnskapsselskapets skrifter, I.\nMatematisk-naturvidenskabelig klasse, 4.\nSkolem, T. (1928). ¨Uber die mathematische Logik.\nNorsk matematisk tidsskrift, 10, 125–142.\nSlagle, J. R. (1963). A heuristic program that solves\nsymbolic integration problems in freshman calculus.\nJACM, 10(4).\nSlate, D. J. and Atkin, L. R. (1977). CHESS 4.5—\nNorthwestern University chess program.\nIn Frey,\nP. W. (Ed.), Chess Skill in Man and Machine, pp.\n82–118. Springer-Verlag.\nSlater, E. (1950). Statistics for the chess computer\nand the factor of mobility. In Symposium on Infor-\nmation Theory, pp. 150–152. Ministry of Supply.\nSleator, D. and Temperley, D. (1993). Parsing En-\nglish with a link grammar. In Third Annual Work-\nshop on Parsing technologies.\nSlocum, J. and Sonneveld, D. (2006). The 15 Puzzle.\nSlocum Puzzle Foundation.\nSloman, A. (1978).\nThe Computer Revolution in\nPhilosophy. Harvester Press.\nSmallwood, R. D. and Sondik, E. J. (1973). The\noptimal control of partially observable Markov pro-\ncesses over a ﬁnite horizon. Operations Research,\n21, 1071–1088.\nSmart, J. J. C. (1959). Sensations and brain pro-\ncesses. Philosophical Review, 68, 141–156.\nSmith, B. (2004). Ontology. In Floridi, L. (Ed.),\nThe Blackwell Guide to the Philosophy of Comput-\ning and Information, pp. 155–166. Wiley-Blackwell.\nSmith, D. E., Genesereth, M. R., and Ginsberg,\nM. L. (1986). Controlling recursive inference. AIJ,\n30(3), 343–389.",
  "The Blackwell Guide to the Philosophy of Comput-\ning and Information, pp. 155–166. Wiley-Blackwell.\nSmith, D. E., Genesereth, M. R., and Ginsberg,\nM. L. (1986). Controlling recursive inference. AIJ,\n30(3), 343–389.\nSmith, D. A. and Eisner, J. (2008).\nDependency\nparsing by belief propagation. In EMNLP, pp. 145–\n156.\nSmith, D. E. and Weld, D. S. (1998). Conformant\nGraphplan. In AAAI-98, pp. 889–896.\nSmith, J. Q. (1988). Decision Analysis. Chapman\nand Hall.\nSmith, J. E. and Winkler, R. L. (2006). The opti-\nmizer’s curse: Skepticism and postdecision surprise\nin decision analysis. Management Science, 52(3),\n311–322.\nSmith, J. M. (1982). Evolution and the Theory of\nGames. Cambridge University Press.\nSmith, J. M. and Szathm´ary, E. (1999). The Ori-\ngins of Life: From the Birth of Life to the Origin of\nLanguage. Oxford University Press.\nSmith, M. K., Welty, C., and McGuinness, D.\n(2004). OWL web ontology language guide. Tech.\nrep., W3C.\nSmith, R. C. and Cheeseman, P. (1986). On the rep-\nresentation and estimation of spatial uncertainty. Int.\nJ. Robotics Research, 5(4), 56–68.\nSmith, S. J. J., Nau, D. S., and Throop, T. A. (1998).\nSuccess in spades: Using AI planning techniques to\nwin the world championship of computer bridge. In\nAAAI-98, pp. 1079–1086.\nSmolensky, P. (1988). On the proper treatment of\nconnectionism. BBS, 2, 1–74.\nSmullyan, R. M. (1995). First-Order Logic. Dover.\nSmyth, P., Heckerman, D., and Jordan, M. I.\n(1997). Probabilistic independence networks for hid-\nden Markov probability models. Neural Computa-\ntion, 9(2), 227–269.\nSnell, M. B. (2008). Do you have free will? John\nSearle reﬂects on various philosophical questions in\nlight of new research on the brain. California Alumni\nMagazine, March/April.\nSoderland, S. and Weld, D. S. (1991). Evaluating\nnonlinear planning. Technical report TR-91-02-03,\nUniversity of Washington Department of Computer\nScience and Engineering.\nSolomonoff, R. J. (1964). A formal theory of in-\nductive inference. Information and Control, 7, 1–22,\n224–254.\nSolomonoff, R. J. (2009). Algorithmic probability–\ntheory and applications. In Emmert-Streib, F. and\nDehmer, M. (Eds.), Information Theory and Statiti-\ncal Learning. Springer.\nSondik, E. J. (1971). The Optimal Control of Par-\ntially Observable Markov Decision Processes. Ph.D.\nthesis, Stanford University.\nSosic, R. and Gu, J. (1994). Efﬁcient local search\nwith conﬂict minimization: A case study of the n-\nqueens problem. IEEE Transactions on Knowledge",
  "tially Observable Markov Decision Processes. Ph.D.\nthesis, Stanford University.\nSosic, R. and Gu, J. (1994). Efﬁcient local search\nwith conﬂict minimization: A case study of the n-\nqueens problem. IEEE Transactions on Knowledge\nand Data Engineering, 6(5), 661–668.\nSowa, J. (1999). Knowledge Representation: Logi-\ncal, Philosophical, and Computational Foundations.\nBlackwell.\nSpaan, M. T. J. and Vlassis, N. (2005).\nPerseus:\nRandomized\npoint-based\nvalue\niteration\nfor\nPOMDPs. JAIR, 24, 195–220.\nSpiegelhalter, D. J., Dawid, A. P., Lauritzen, S., and\nCowell, R. (1993). Bayesian analysis in expert sys-\ntems. Statistical Science, 8, 219–282.\nSpielberg, S. (2001). AI. Movie.\nSpirtes, P., Glymour, C., and Scheines, R. (1993).\nCausation, prediction, and search. Springer-Verlag.\nSrinivasan, A., Muggleton, S. H., King, R. D., and\nSternberg, M. J. E. (1994). Mutagenesis: ILP exper-\niments in a non-determinate biological domain. In\nILP-94, Vol. 237, pp. 217–232.\nSrivas, M. and Bickford, M. (1990). Formal veri-\nﬁcation of a pipelined microprocessor. IEEE Soft-\nware, 7(5), 52–64.\nStaab,\nS. (2004).\nHandbook on Ontologies.\nSpringer.\nStallman, R. M. and Sussman, G. J. (1977). Forward\nreasoning and dependency-directed backtracking in\na system for computer-aided circuit analysis. AIJ,\n9(2), 135–196.\nStanﬁll, C. and Waltz, D. (1986). Toward memory-\nbased reasoning. CACM, 29(12), 1213–1228.\nSteﬁk, M. (1995). Introduction to Knowledge Sys-\ntems. Morgan Kaufmann.\nStein, L. A. (2002).\nInteractive Programming in\nJava (pre-publication draft). Morgan Kaufmann.\nStephenson, T., Bourlard, H., Bengio, S., and Mor-\nris, A. (2000). Automatic speech recognition using\ndynamic bayesian networks with both acoustic and\narticulatory features. In ICSLP-00, pp. 951–954.\nStergiou, K. and Walsh, T. (1999). The difference\nall-difference makes. In IJCAI-99, pp. 414–419.\nStickel, M. E. (1992). A prolog technology theorem\nprover: a new exposition and implementation in pro-\nlog. Theoretical Computer Science, 104, 109–128.\nStiller, L. (1992). KQNKRR. J. International Com-\nputer Chess Association, 15(1), 16–18.\nStiller, L. (1996).\nMultilinear algebra and chess\nendgames. In Nowakowski, R. J. (Ed.), Games of\nNo Chance, MSRI, 29, 1996. Mathematical Sciences\nResearch Institute.\nStockman, G. (1979). A minimax algorithm better\nthan alpha–beta? AIJ, 12(2), 179–196.\nStoffel, K., Taylor, M., and Hendler, J. (1997). Efﬁ-\ncient management of very large ontologies. In Proc.\nAAAI-97, pp. 442–447.",
  "Research Institute.\nStockman, G. (1979). A minimax algorithm better\nthan alpha–beta? AIJ, 12(2), 179–196.\nStoffel, K., Taylor, M., and Hendler, J. (1997). Efﬁ-\ncient management of very large ontologies. In Proc.\nAAAI-97, pp. 442–447.\nStolcke, A. and Omohundro, S. (1994). Inducing\nprobabilistic grammars by Bayesian model merging.\nIn Proc. Second International Colloquium on Gram-\nmatical Inference and Applications (ICGI-94), pp.\n106–118.\nStone, M. (1974). Cross-validatory choice and as-\nsessment of statostical predictions. J. Royal Statisti-\ncal Society, 36(111–133).\nStone, P. (2000). Layered Learning in Multi-Agent\nSystems: A Winning Approach to Robotic Soccer.\nMIT Press.\nStone, P. (2003). Multiagent competitions and re-\nsearch: Lessons from RoboCup and TAC. In Lima,\nP. U. and Rojas, P. (Eds.), RoboCup-2002: Robot\nSoccer World Cup VI, pp. 224–237. Springer Verlag.\nStone, P., Kaminka, G., and Rosenschein, J. S.\n(2009). Leading a best-response teammate in an ad\nhoc team. In AAMAS Workshop in Agent Mediated\nElectronic Commerce.\nStork, D. G. (2004). Optics and realism in rennais-\nsance art. Scientiﬁc American, pp. 77–83.\nStrachey, C. (1952). Logical or non-mathematical\nprogrammes. In Proc. 1952 ACM national meeting\n(Toronto), pp. 46–49.\nStratonovich, R. L. (1959). Optimum nonlinear sys-\ntems which bring about a separation of a signal with\nconstant parameters from noise. Radioﬁzika, 2(6),\n892–901.\nStratonovich, R. L. (1965). On value of informa-\ntion. Izvestiya of USSR Academy of Sciences, Tech-\nnical Cybernetics, 5, 3–12.\nSubramanian, D. and Feldman, R. (1990). The util-\nity of EBL in recursive domain theories. In AAAI-90,\nVol. 2, pp. 942–949. 1090\nBibliography\nSubramanian, D. and Wang, E. (1994). Constraint-\nbased kinematic synthesis.\nIn Proc. International\nConference on Qualitative Reasoning, pp. 228–239.\nSussman, G. J. (1975). A Computer Model of Skill\nAcquisition. Elsevier/North-Holland.\nSutcliffe, G. and Suttner, C. (1998). The TPTP Prob-\nlem Library: CNF Release v1.2.1. JAR, 21(2), 177–\n203.\nSutcliffe, G., Schulz, S., Claessen, K., and Gelder,\nA. V. (2006). Using the TPTP language for writing\nderivations and ﬁnite interpretations. In Proc. Inter-\nnational Joint Conference on Automated Reasoning,\npp. 67–81.\nSutherland, I. (1963). Sketchpad: A man-machine\ngraphical communication system. In Proc. Spring\nJoint Computer Conference, pp. 329–346.\nSutton, C. and McCallum, A. (2007). An introduc-\ntion to conditional random ﬁelds for relational learn-",
  "Sutherland, I. (1963). Sketchpad: A man-machine\ngraphical communication system. In Proc. Spring\nJoint Computer Conference, pp. 329–346.\nSutton, C. and McCallum, A. (2007). An introduc-\ntion to conditional random ﬁelds for relational learn-\ning. In Getoor, L. and Taskar, B. (Eds.), Introduction\nto Statistical Relational Learning. MIT Press.\nSutton, R. S. (1988).\nLearning to predict by the\nmethods of temporal differences. Machine Learn-\ning, 3, 9–44.\nSutton, R. S., McAllester, D. A., Singh, S. P., and\nMansour, Y. (2000). Policy gradient methods for re-\ninforcement learning with function approximation.\nIn Solla, S. A., Leen, T. K., and M¨uller, K.-R. (Eds.),\nNIPS 12, pp. 1057–1063. MIT Press.\nSutton, R. S. (1990).\nIntegrated architectures for\nlearning, planning, and reacting based on approx-\nimating dynamic programming.\nIn ICML-90, pp.\n216–224.\nSutton, R. S. and Barto, A. G. (1998). Reinforce-\nment Learning: An Introduction. MIT Press.\nSvore, K. and Burges, C. (2009).\nA machine\nlearning approach for improved bm25 retrieval. In\nProc. Conference on Information Knowledge Man-\nagement.\nSwade, D. (2000). Difference Engine: Charles Bab-\nbage And The Quest To Build The First Computer.\nDiane Publishing Co.\nSwerling, P. (1959). First order error propagation in\na stagewise smoothing procedure for satellite obser-\nvations. J. Astronautical Sciences, 6, 46–52.\nSwift, T. and Warren, D. S. (1994). Analysis of SLG-\nWAM evaluation of deﬁnite programs. In Logic Pro-\ngramming. Proc. 1994 International Symposium on\nLogic programming, pp. 219–235.\nSyrj¨anen, T. (2000).\nLparse 1.0 user’s manual.\nsaturn.tcs.hut.fi/Software/smodels.\nTadepalli, P. (1993). Learning from queries and ex-\namples with tree-structured bias. In ICML-93, pp.\n322–329.\nTadepalli, P., Givan, R., and Driessens, K. (2004).\nRelational reinforcement learning: An overview. In\nICML-04.\nTait, P. G. (1880). Note on the theory of the “15\npuzzle”. Proc. Royal Society of Edinburgh, 10, 664–\n665.\nTamaki, H. and Sato, T. (1986).\nOLD resolution\nwith tabulation. In ICLP-86, pp. 84–98.\nTarjan, R. E. (1983). Data Structures and Network\nAlgorithms. CBMS-NSF Regional Conference Se-\nries in Applied Mathematics. SIAM (Society for In-\ndustrial and Applied Mathematics).\nTarski, A. (1935). Die Wahrheitsbegriff in den for-\nmalisierten Sprachen. Studia Philosophica, 1, 261–\n405.\nTarski, A. (1941). Introduction to Logic and to the\nMethodology of Deductive Sciences. Dover.\nTarski, A. (1956). Logic, Semantics, Metamathe-",
  "Tarski, A. (1935). Die Wahrheitsbegriff in den for-\nmalisierten Sprachen. Studia Philosophica, 1, 261–\n405.\nTarski, A. (1941). Introduction to Logic and to the\nMethodology of Deductive Sciences. Dover.\nTarski, A. (1956). Logic, Semantics, Metamathe-\nmatics: Papers from 1923 to 1938. Oxford Univer-\nsity Press.\nTash, J. K. and Russell, S. J. (1994). Control strate-\ngies for a stochastic planner. In AAAI-94, pp. 1079–\n1085.\nTaskar, B., Abbeel, P., and Koller, D. (2002). Dis-\ncriminative probabilistic models for relational data.\nIn UAI-02.\nTate, A. (1975a). Interacting goals and their use. In\nIJCAI-75, pp. 215–218.\nTate, A. (1975b). Using Goal Structure to Direct\nSearch in a Problem Solver. Ph.D. thesis, University\nof Edinburgh.\nTate, A. (1977). Generating project networks. In\nIJCAI-77, pp. 888–893.\nTate, A. and Whiter, A. M. (1984). Planning with\nmultiple resource constraints and an application to a\nnaval planning problem. In Proc. First Conference\non AI Applications, pp. 410–416.\nTatman, J. A. and Shachter, R. D. (1990). Dynamic\nprogramming and inﬂuence diagrams. IEEE Trans-\nactions on Systems, Man and Cybernetics, 20(2),\n365–379.\nTattersall, C. (1911). A Thousand End-Games: A\nCollection of Chess Positions That Can be Won or\nDrawn by the Best Play. British Chess Magazine.\nTaylor, G., Stensrud, B., Eitelman, S., and Dunham,\nC. (2007).\nTowards automating airspace manage-\nment. In Proc. Computational Intelligence for Secu-\nrity and Defense Applications (CISDA) Conference,\npp. 1–5.\nTenenbaum, J., Grifﬁths, T., and Niyogi, S. (2007).\nIntuitive theories as grammars for causal inference.\nIn Gopnik, A. and Schulz, L. (Eds.), Causal learn-\ning: Psychology, Philosophy, and Computation. Ox-\nford University Press.\nTesauro, G. (1992). Practical issues in temporal dif-\nference learning. Machine Learning, 8(3–4), 257–\n277.\nTesauro, G. (1995). Temporal difference learning\nand TD-Gammon. CACM, 38(3), 58–68.\nTesauro, G. and Sejnowski, T. (1989). A parallel\nnetwork that learns to play backgammon. AIJ, 39(3),\n357–390.\nTeyssier, M. and Koller, D. (2005). Ordering-based\nsearch: A simple and effective algorithm for learning\nBayesian networks. In UAI-05, pp. 584–590.\nThaler, R. (1992). The Winner’s Curse: Paradoxes\nand Anomalies of Economic Life. Princeton Univer-\nsity Press.\nThaler, R. and Sunstein, C. (2009). Nudge: Improv-\ning Decisions About Health, Wealth, and Happiness.\nPenguin.\nTheocharous, G., Murphy, K., and Kaelbling, L. P.\n(2004).\nRepresenting hierarchical POMDPs as",
  "sity Press.\nThaler, R. and Sunstein, C. (2009). Nudge: Improv-\ning Decisions About Health, Wealth, and Happiness.\nPenguin.\nTheocharous, G., Murphy, K., and Kaelbling, L. P.\n(2004).\nRepresenting hierarchical POMDPs as\nDBNs for multi-scale robot localization. In ICRA-\n04.\nThiele, T. (1880).\nOm anvendelse af mindste\nkvadraters methode i nogle tilfælde, hvor en kom-\nplikation af visse slags uensartede tilfældige fejlk-\nilder giver fejlene en ‘systematisk’ karakter. Vidensk.\nSelsk. Skr. 5. Rk., naturvid. og mat. Afd., 12, 381–\n408.\nThielscher, M. (1999). From situation calculus to\nﬂuent calculus: State update axioms as a solution to\nthe inferential frame problem. AIJ, 111(1–2), 277–\n299.\nThompson, K. (1986). Retrograde analysis of cer-\ntain endgames. J. International Computer Chess As-\nsociation, May, 131–139.\nThompson, K. (1996). 6-piece endgames. J. Inter-\nnational Computer Chess Association, 19(4), 215–\n226.\nThrun, S., Burgard, W., and Fox, D. (2005). Proba-\nbilistic Robotics. MIT Press.\nThrun, S., Fox, D., and Burgard, W. (1998). A prob-\nabilistic approach to concurrent mapping and local-\nization for mobile robots.\nMachine Learning, 31,\n29–53.\nThrun, S. (2006). Stanley, the robot that won the\nDARPA Grand Challenge. J. Field Robotics, 23(9),\n661–692.\nTikhonov, A. N. (1963).\nSolution of incorrectly\nformulated problems and the regularization method.\nSoviet Math. Dokl., 5, 1035–1038.\nTitterington, D. M., Smith, A. F. M., and Makov,\nU. E. (1985). Statistical analysis of ﬁnite mixture\ndistributions. Wiley.\nTofﬂer, A. (1970). Future Shock. Bantam.\nTomasi, C. and Kanade, T. (1992). Shape and mo-\ntion from image streams under orthography: A fac-\ntorization method. IJCV, 9, 137–154.\nTorralba, A., Fergus, R., and Weiss, Y. (2008).\nSmall codes and large image databases for recogni-\ntion. In CVPR, pp. 1–8.\nTrucco, E. and Verri, A. (1998). Introductory Tech-\nniques for 3-D Computer Vision. Prentice Hall.\nTsitsiklis, J. N. and Van Roy, B. (1997). An analy-\nsis of temporal-difference learning with function ap-\nproximation. IEEE Transactions on Automatic Con-\ntrol, 42(5), 674–690.\nTumer, K. and Wolpert, D. (2000). Collective intel-\nligence and braess’ paradox. In AAAI-00, pp. 104–\n109.\nTurcotte, M., Muggleton, S. H., and Sternberg, M.\nJ. E. (2001). Automated discovery of structural sig-\nnatures of protein fold and function. J. Molecular\nBiology, 306, 591–605.\nTuring, A. (1936). On computable numbers, with\nan application to the Entscheidungsproblem. Proc.",
  "J. E. (2001). Automated discovery of structural sig-\nnatures of protein fold and function. J. Molecular\nBiology, 306, 591–605.\nTuring, A. (1936). On computable numbers, with\nan application to the Entscheidungsproblem. Proc.\nLondon Mathematical Society, 2nd series, 42, 230–\n265.\nTuring, A. (1948). Intelligent machinery. Tech. rep.,\nNational Physical Laboratory.\nreprinted in (Ince,\n1992).\nTuring, A. (1950). Computing machinery and intel-\nligence. Mind, 59, 433–460.\nTuring, A., Strachey, C., Bates, M. A., and Bowden,\nB. V. (1953). Digital computers applied to games. In\nBowden, B. V. (Ed.), Faster than Thought, pp. 286–\n310. Pitman.\nTversky, A. and Kahneman, D. (1982).\nCausal\nschemata in judgements under uncertainty. In Kah-\nneman, D., Slovic, P., and Tversky, A. (Eds.), Judge-\nment Under Uncertainty:\nHeuristics and Biases.\nCambridge University Press.\nUllman, J. D. (1985).\nImplementation of logical\nquery languages for databases. ACM Transactions\non Database Systems, 10(3), 289–321.\nUllman, S. (1979). The Interpretation of Visual Mo-\ntion. MIT Press. Bibliography\n1091\nUrmson, C. and Whittaker, W. (2008). Self-driving\ncars and the Urban Challenge. IEEE Intelligent Sys-\ntems, 23(2), 66–68.\nValiant, L. (1984).\nA theory of the learnable.\nCACM, 27, 1134–1142.\nvan Beek, P. (2006).\nBacktracking search algo-\nrithms.\nIn Rossi, F., van Beek, P., and Walsh, T.\n(Eds.), Handbook of Constraint Programming. Else-\nvier.\nvan Beek, P. and Chen, X. (1999). CPlan: A con-\nstraint programming approach to planning. In AAAI-\n99, pp. 585–590.\nvan Beek, P. and Manchak, D. (1996). The design\nand experimental analysis of algorithms for temporal\nreasoning. JAIR, 4, 1–18.\nvan Bentham, J. and ter Meulen, A. (1997). Hand-\nbook of Logic and Language. MIT Press.\nVan Emden, M. H. and Kowalski, R. (1976). The\nsemantics of predicate logic as a programming lan-\nguage. JACM, 23(4), 733–742.\nvan\nHarmelen,\nF.\nand\nBundy,\nA.\n(1988).\nExplanation-based generalisation = partial evalu-\nation. AIJ, 36(3), 401–412.\nvan Harmelen, F., Lifschitz, V., and Porter, B.\n(2007). The Handbook of Knowledge Representa-\ntion. Elsevier.\nvan Heijenoort, J. (Ed.). (1967).\nFrom Frege\nto G¨odel: A Source Book in Mathematical Logic,\n1879–1931. Harvard University Press.\nVan Hentenryck, P., Saraswat, V., and Deville, Y.\n(1998). Design, implementation, and evaluation of\nthe constraint language cc(FD). J. Logic Program-\nming, 37(1–3), 139–164.\nvan Hoeve, W.-J. (2001). The alldifferent constraint:\na survey.",
  "Van Hentenryck, P., Saraswat, V., and Deville, Y.\n(1998). Design, implementation, and evaluation of\nthe constraint language cc(FD). J. Logic Program-\nming, 37(1–3), 139–164.\nvan Hoeve, W.-J. (2001). The alldifferent constraint:\na survey.\nIn 6th Annual Workshop of the ERCIM\nWorking Group on Constraints.\nvan Hoeve, W.-J. and Katriel, I. (2006).\nGlobal\nconstraints. In Rossi, F., van Beek, P., and Walsh,\nT. (Eds.), Handbook of Constraint Processing, pp.\n169–208. Elsevier.\nvan Lambalgen, M. and Hamm, F. (2005).\nThe\nProper Treatment of Events. Wiley-Blackwell.\nvan Nunen, J. A. E. E. (1976).\nA set of succes-\nsive approximation methods for discounted Marko-\nvian decision problems. Zeitschrift fur Operations\nResearch, Serie A, 20(5), 203–208.\nVan Roy, B. (1998). Learning and value function\napproximation in complex decision processes. Ph.D.\nthesis, Laboratory for Information and Decision Sys-\ntems, MIT.\nVan Roy, P. L. (1990).\nCan logic programming\nexecute as fast as imperative programming?\nRe-\nport UCB/CSD 90/600, Computer Science Division,\nUniversity of California, Berkeley, California.\nVapnik, V. N. (1998). Statistical Learning Theory.\nWiley.\nVapnik, V. N. and Chervonenkis, A. Y. (1971). On\nthe uniform convergence of relative frequencies of\nevents to their probabilities. Theory of Probability\nand Its Applications, 16, 264–280.\nVarian, H. R. (1995). Economic mechanism design\nfor computerized agents. In USENIX Workshop on\nElectronic Commerce, pp. 13–21.\nVauquois, B. (1968). A survey of formal grammars\nand algorithms for recognition and transformation in\nmechanical translation. In Proc. IFIP Congress, pp.\n1114–1122.\nVeloso, M. and Carbonell, J. G. (1993). Derivational\nanalogy in PRODIGY: Automating case acquisition,\nstorage, and utilization. Machine Learning, 10, 249–\n278.\nVere, S. A. (1983). Planning in time: Windows and\ndurations for activities and goals. PAMI, 5, 246–267.\nVerma, V., Gordon, G., Simmons, R., and Thrun,\nS. (2004). Particle ﬁlters for rover fault diagnosis.\nIEEE Robotics and Automation Magazine, June.\nVinge, V. (1993).\nThe coming technological sin-\ngularity:\nHow to survive in the post-human era.\nIn VISION-21 Symposium. NASA Lewis Research\nCenter and the Ohio Aerospace Institute.\nViola, P. and Jones, M. (2002a). Fast and robust clas-\nsiﬁcation using asymmetric adaboost and a detector\ncascade. In NIPS 14.\nViola, P. and Jones, M. (2002b). Robust real-time\nobject detection. ICCV.\nVisser, U. and Burkhard, H.-D. (2007). RoboCup",
  "Viola, P. and Jones, M. (2002a). Fast and robust clas-\nsiﬁcation using asymmetric adaboost and a detector\ncascade. In NIPS 14.\nViola, P. and Jones, M. (2002b). Robust real-time\nobject detection. ICCV.\nVisser, U. and Burkhard, H.-D. (2007). RoboCup\n2006: achievements and goals for the future. AIMag,\n28(2), 115–130.\nVisser, U., Ribeiro, F., Ohashi, T., and Dellaert, F.\n(Eds.). (2008). RoboCup 2007: Robot Soccer World\nCup XI. Springer.\nViterbi, A. J. (1967). Error bounds for convolutional\ncodes and an asymptotically optimum decoding al-\ngorithm. IEEE Transactions on Information Theory,\n13(2), 260–269.\nVlassis, N. (2008). A Concise Introduction to Multi-\nagent Systems and Distributed Artiﬁcial Intelligence.\nMorgan and Claypool.\nvon Mises, R. (1928). Wahrscheinlichkeit, Statistik\nund Wahrheit. J. Springer.\nvon Neumann,\nJ. (1928).\nZur Theorie der\nGesellschaftsspiele.\nMathematische\nAnnalen,\n100(295–320).\nvon Neumann, J. and Morgenstern, O. (1944). The-\nory of Games and Economic Behavior (ﬁrst edition).\nPrinceton University Press.\nvon Winterfeldt, D. and Edwards, W. (1986). Deci-\nsion Analysis and Behavioral Research. Cambridge\nUniversity Press.\nVossen, T., Ball, M., Lotem, A., and Nau, D. S.\n(2001). Applying integer programming to AI plan-\nning. Knowledge Engineering Review, 16, 85–100.\nWainwright, M. J. and Jordan, M. I. (2008). Graph-\nical models, exponential families, and variational in-\nference. Machine Learning, 1(1–2), 1–305.\nWaldinger, R. (1975). Achieving several goals si-\nmultaneously.\nIn Elcock, E. W. and Michie, D.\n(Eds.), Machine Intelligence 8, pp. 94–138. Ellis\nHorwood.\nWallace, A. R. (1858). On the tendency of varieties\nto depart indeﬁnitely from the original type. Proc.\nLinnean Society of London, 3, 53–62.\nWaltz, D. (1975). Understanding line drawings of\nscenes with shadows. In Winston, P. H. (Ed.), The\nPsychology of Computer Vision. McGraw-Hill.\nWang, Y. and Gelly, S. (2007).\nModiﬁcations of\nUCT and sequence-like simulations for Monte-Carlo\nGo. In IEEE Symposium on Computational Intelli-\ngence and Games, pp. 175–182.\nWanner, E. (1974). On remembering, forgetting and\nunderstanding sentences. Mouton.\nWarren, D. H. D. (1974). WARPLAN: A System\nfor Generating Plans. Department of Computational\nLogic Memo 76, University of Edinburgh.\nWarren, D. H. D. (1983). An abstract Prolog in-\nstruction set. Technical note 309, SRI International.\nWarren, D. H. D., Pereira, L. M., and Pereira, F.\n(1977).\nPROLOG: The language and its imple-",
  "Logic Memo 76, University of Edinburgh.\nWarren, D. H. D. (1983). An abstract Prolog in-\nstruction set. Technical note 309, SRI International.\nWarren, D. H. D., Pereira, L. M., and Pereira, F.\n(1977).\nPROLOG: The language and its imple-\nmentation compared with LISP. SIGPLAN Notices,\n12(8), 109–115.\nWasserman, L. (2004). All of Statistics. Springer.\nWatkins, C. J. (1989).\nModels of Delayed Rein-\nforcement Learning. Ph.D. thesis, Psychology De-\npartment, Cambridge University.\nWatson, J. D. and Crick, F. H. C. (1953). A structure\nfor deoxyribose nucleic acid. Nature, 171, 737.\nWaugh, K., Schnizlein, D., Bowling, M., and\nSzafron, D. (2009). Abstraction pathologies in ex-\ntensive games. In AAMAS-09.\nWeaver, W. (1949). Translation. In Locke, W. N.\nand Booth, D. (Eds.), Machine translation of lan-\nguages: fourteen essays, pp. 15–23. Wiley.\nWebber, B. L. and Nilsson, N. J. (Eds.). (1981).\nReadings in Artiﬁcial Intelligence.\nMorgan Kauf-\nmann.\nWeibull, J. (1995). Evolutionary Game Theory. MIT\nPress.\nWeidenbach, C. (2001). SPASS: Combining super-\nposition, sorts and splitting.\nIn Robinson, A. and\nVoronkov, A. (Eds.), Handbook of Automated Rea-\nsoning. MIT Press.\nWeiss, G. (2000a). Multiagent systems. MIT Press.\nWeiss, Y. (2000b). Correctness of local probability\npropagation in graphical models with loops. Neural\nComputation, 12(1), 1–41.\nWeiss, Y. and Freeman, W. (2001).\nCorrectness\nof belief propagation in Gaussian graphical models\nof arbitrary topology. Neural Computation, 13(10),\n2173–2200.\nWeizenbaum, J. (1976). Computer Power and Hu-\nman Reason. W. H. Freeman.\nWeld, D. S. (1994). An introduction to least com-\nmitment planning. AIMag, 15(4), 27–61.\nWeld, D. S. (1999). Recent advances in AI planning.\nAIMag, 20(2), 93–122.\nWeld, D. S., Anderson, C. R., and Smith, D. E.\n(1998). Extending graphplan to handle uncertainty\nand sensing actions. In AAAI-98, pp. 897–904.\nWeld, D. S. and de Kleer, J. (1990). Readings in\nQualitative Reasoning about Physical Systems. Mor-\ngan Kaufmann.\nWeld, D. S. and Etzioni, O. (1994). The ﬁrst law of\nrobotics: A call to arms. In AAAI-94.\nWellman, M. P. (1985). Reasoning about preference\nmodels. Technical report MIT/LCS/TR-340, Labo-\nratory for Computer Science, MIT.\nWellman, M. P. (1988). Formulation of Tradeoffs\nin Planning under Uncertainty. Ph.D. thesis, Mas-\nsachusetts Institute of Technology.\nWellman, M. P. (1990a). Fundamental concepts of\nqualitative probabilistic networks. AIJ, 44(3), 257–\n303.",
  "Wellman, M. P. (1988). Formulation of Tradeoffs\nin Planning under Uncertainty. Ph.D. thesis, Mas-\nsachusetts Institute of Technology.\nWellman, M. P. (1990a). Fundamental concepts of\nqualitative probabilistic networks. AIJ, 44(3), 257–\n303.\nWellman, M. P. (1990b). The STRIPS assumption\nfor planning under uncertainty. In AAAI-90, pp. 198–\n203.\nWellman, M. P. (1995).\nThe economic approach\nto artiﬁcial intelligence. ACM Computing Surveys,\n27(3), 360–362.\nWellman, M. P., Breese, J. S., and Goldman, R.\n(1992). From knowledge bases to decision models.\nKnowledge Engineering Review, 7(1), 35–53. 1092\nBibliography\nWellman, M. P. and Doyle, J. (1992). Modular util-\nity representation for decision-theoretic planning. In\nICAPS-92, pp. 236–242.\nWellman,\nM. P.,\nWurman,\nP., O’Malley,\nK.,\nBangera, R., Lin, S., Reeves, D., and Walsh, W.\n(2001). A trading agent competition. IEEE Inter-\nnet Computing.\nWells, H. G. (1898). The War of the Worlds. William\nHeinemann.\nWerbos, P. (1974). Beyond Regression: New Tools\nfor Prediction and Analysis in the Behavioral Sci-\nences. Ph.D. thesis, Harvard University.\nWerbos, P. (1977). Advanced forecasting methods\nfor global crisis warning and models of intelligence.\nGeneral Systems Yearbook, 22, 25–38.\nWesley, M. A. and Lozano-Perez, T. (1979).\nAn\nalgorithm for planning collision-free paths among\npolyhedral objects. CACM, 22(10), 560–570.\nWexler, Y. and Meek, C. (2009). MAS: A multi-\nplicative approximation scheme for probabilistic in-\nference. In NIPS 21.\nWhitehead, A. N. (1911). An Introduction to Math-\nematics. Williams and Northgate.\nWhitehead, A. N. and Russell, B. (1910). Principia\nMathematica. Cambridge University Press.\nWhorf, B. (1956). Language, Thought, and Reality.\nMIT Press.\nWidrow, B. (1962). Generalization and information\nstorage in networks of adaline “neurons”. In Self-\nOrganizing Systems 1962, pp. 435–461.\nWidrow, B. and Hoff, M. E. (1960).\nAdaptive\nswitching circuits. In 1960 IRE WESCON Conven-\ntion Record, pp. 96–104.\nWiedijk, F. (2003).\nComparing mathematical\nprovers. In Mathematical Knowledge Management,\npp. 188–202.\nWiegley, J., Goldberg,\nK., Peshkin, M., and\nBrokowski, M. (1996).\nA complete algorithm for\ndesigning passive fences to orient parts. In ICRA-\n96.\nWiener, N. (1942). The extrapolation, interpolation,\nand smoothing of stationary time series. Osrd 370,\nReport to the Services 19, Research Project DIC-\n6037, MIT.\nWiener, N. (1948). Cybernetics. Wiley.\nWilensky, R. (1978).\nUnderstanding goal-based",
  "Wiener, N. (1942). The extrapolation, interpolation,\nand smoothing of stationary time series. Osrd 370,\nReport to the Services 19, Research Project DIC-\n6037, MIT.\nWiener, N. (1948). Cybernetics. Wiley.\nWilensky, R. (1978).\nUnderstanding goal-based\nstories. Ph.D. thesis, Yale University.\nWilensky, R. (1983). Planning and Understanding.\nAddison-Wesley.\nWilkins, D. E. (1980). Using patterns and plans in\nchess. AIJ, 14(2), 165–203.\nWilkins, D. E. (1988). Practical Planning: Extend-\ning the AI Planning Paradigm. Morgan Kaufmann.\nWilkins, D. E. (1990). Can AI planners solve prac-\ntical problems?\nComputational Intelligence, 6(4),\n232–246.\nWilliams, B., Ingham, M., Chung, S., and Elliott,\nP. (2003). Model-based programming of intelligent\nembedded systems and robotic space explorers. In\nProc. IEEE: Special Issue on Modeling and Design\nof Embedded Software, pp. 212–237.\nWilliams, R. J. (1992). Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine Learning, 8, 229–256.\nWilliams, R. J. and Baird, L. C. I. (1993).\nTight\nperformance bounds on greedy policies based on im-\nperfect value functions. Tech. rep. NU-CCS-93-14,\nCollege of Computer Science, Northeastern Univer-\nsity.\nWilson, R. A. and Keil, F. C. (Eds.). (1999).\nThe MIT Encyclopedia of the Cognitive Sciences.\nMIT Press.\nWilson, R. (2004). Four Colors Sufﬁce. Princeton\nUniversity Press.\nWinograd, S. and Cowan, J. D. (1963). Reliable\nComputation in the Presence of Noise. MIT Press.\nWinograd, T. (1972).\nUnderstanding natural lan-\nguage. Cognitive Psychology, 3(1), 1–191.\nWinston, P. H. (1970). Learning structural descrip-\ntions from examples. Technical report MAC-TR-76,\nDepartment of Electrical Engineering and Computer\nScience, Massachusetts Institute of Technology.\nWinston, P. H. (1992). Artiﬁcial Intelligence (Third\nedition). Addison-Wesley.\nWintermute, S., Xu, J., and Laird, J. (2007).\nSORTS: A human-level approach to real-time strat-\negy AI. In Proc. Third Artiﬁcial Intelligence and In-\nteractive Digital Entertainment Conference (AIIDE-\n07).\nWitten, I. H. and Bell, T. C. (1991).\nThe zero-\nfrequency problem: Estimating the probabilities of\nnovel events in adaptive text compression.\nIEEE\nTransactions on Information Theory, 37(4), 1085–\n1094.\nWitten, I. H. and Frank, E. (2005). Data Mining:\nPractical Machine Learning Tools and Techniques\n(2nd edition). Morgan Kaufmann.\nWitten, I. H., Moffat, A., and Bell, T. C. (1999).\nManaging Gigabytes: Compressing and Indexing",
  "1094.\nWitten, I. H. and Frank, E. (2005). Data Mining:\nPractical Machine Learning Tools and Techniques\n(2nd edition). Morgan Kaufmann.\nWitten, I. H., Moffat, A., and Bell, T. C. (1999).\nManaging Gigabytes: Compressing and Indexing\nDocuments and Images (second edition).\nMorgan\nKaufmann.\nWittgenstein,\nL. (1922).\nTractatus Logico-\nPhilosophicus (second edition).\nRoutledge and\nKegan Paul. Reprinted 1971, edited by D. F. Pears\nand B. F. McGuinness. This edition of the English\ntranslation also contains Wittgenstein’s original Ger-\nman text on facing pages, as well as Bertrand Rus-\nsell’s introduction to the 1922 edition.\nWittgenstein, L. (1953). Philosophical Investiga-\ntions. Macmillan.\nWojciechowski, W. S. and Wojcik, A. S. (1983). Au-\ntomated design of multiple-valued logic circuits by\nautomated theorem proving techniques. IEEE Trans-\nactions on Computers, C-32(9), 785–798.\nWolfe, J. and Russell, S. J. (2007). Exploiting belief\nstate structure in graph search. In ICAPS Workshop\non Planning in Games.\nWoods, W. A. (1973). Progress in natural language\nunderstanding: An application to lunar geology. In\nAFIPS Conference Proceedings, Vol. 42, pp. 441–\n450.\nWoods, W. A. (1975). What’s in a link? Founda-\ntions for semantic networks. In Bobrow, D. G. and\nCollins, A. M. (Eds.), Representation and Under-\nstanding: Studies in Cognitive Science, pp. 35–82.\nAcademic Press.\nWooldridge, M. (2002). An Introduction to MultiA-\ngent Systems. Wiley.\nWooldridge, M. and Rao, A. (Eds.). (1999). Foun-\ndations of rational agency. Kluwer.\nWos, L., Carson, D., and Robinson, G. (1964). The\nunit preference strategy in theorem proving. In Proc.\nFall Joint Computer Conference, pp. 615–621.\nWos, L., Carson, D., and Robinson, G. (1965). Efﬁ-\nciency and completeness of the set-of-support strat-\negy in theorem proving. JACM, 12, 536–541.\nWos, L., Overbeek, R., Lusk, E., and Boyle, J.\n(1992). Automated Reasoning: Introduction and Ap-\nplications (second edition). McGraw-Hill.\nWos, L. and Robinson, G. (1968). Paramodulation\nand set of support. In Proc. IRIA Symposium on Au-\ntomatic Demonstration, pp. 276–310.\nWos, L., Robinson, G., Carson, D., and Shalla, L.\n(1967).\nThe concept of demodulation in theorem\nproving. JACM, 14, 698–704.\nWos, L. and Winker, S. (1983).\nOpen questions\nsolved with the assistance of AURA. In Automated\nTheorem Proving: After 25 Years: Proc. Special Ses-\nsion of the 89th Annual Meeting of the American\nMathematical Society, pp. 71–88. American Math-\nematical Society.",
  "Open questions\nsolved with the assistance of AURA. In Automated\nTheorem Proving: After 25 Years: Proc. Special Ses-\nsion of the 89th Annual Meeting of the American\nMathematical Society, pp. 71–88. American Math-\nematical Society.\nWos, L. and Pieper, G. (2003).\nAutomated Rea-\nsoning and the Discovery of Missing and Elegant\nProofs. Rinton Press.\nWray, R. E. and Jones, R. M. (2005).\nAn intro-\nduction to Soar as an agent architecture. In Sun, R.\n(Ed.), Cognition and Multi-agent Interaction: From\nCognitive Modeling to Social Simulation, pp. 53–78.\nCambridge University Press.\nWright, S. (1921). Correlation and causation. J.\nAgricultural Research, 20, 557–585.\nWright, S. (1931). Evolution in Mendelian popula-\ntions. Genetics, 16, 97–159.\nWright, S. (1934). The method of path coefﬁcients.\nAnnals of Mathematical Statistics, 5, 161–215.\nWu, D. (1993). Estimating probability distributions\nover hypotheses with variable uniﬁcation. In IJCAI-\n93, pp. 790–795.\nWu, F. and Weld, D. S. (2008). Automatically reﬁn-\ning the wikipedia infobox ontology. In 17th World\nWide Web Conference (WWW2008).\nYang, F., Culberson, J., Holte, R., Zahavi, U., and\nFelner, A. (2008). A general theory of additive state\nspace abstractions. JAIR, 32, 631–662.\nYang, Q. (1990). Formalizing planning knowledge\nfor hierarchical planning.\nComputational Intelli-\ngence, 6, 12–24.\nYarowsky, D. (1995). Unsupervised word sense dis-\nambiguation rivaling supervised methods. In ACL-\n95, pp. 189–196.\nYedidia, J., Freeman, W., and Weiss, Y. (2005). Con-\nstructing free-energy approximations and general-\nized belief propagation algorithms. IEEE Transac-\ntions on Information Theory, 51(7), 2282–2312.\nYip, K. M.-K. (1991). KAM: A System for Intelli-\ngently Guiding Numerical Experimentation by Com-\nputer. MIT Press.\nYngve, V. (1955). A model and an hypothesis for\nlanguage structure. In Locke, W. N. and Booth, A. D.\n(Eds.), Machine Translation of Languages, pp. 208–\n226. MIT Press.\nYob, G. (1975). Hunt the wumpus! Creative Com-\nputing, Sep/Oct.\nYoshikawa, T. (1990).\nFoundations of Robotics:\nAnalysis and Control. MIT Press.\nYoung, H. P. (2004). Strategic Learning and Its Lim-\nits. Oxford University Press.\nYounger, D. H. (1967). Recognition and parsing of\ncontext-free languages in time n3. Information and\nControl, 10(2), 189–208. Bibliography\n1093\nYudkowsky, E. (2008).\nArtiﬁcial intelligence as\na positive and negative factor in global risk.\nIn\nBostrom, N. and Cirkovic, M. (Eds.), Global Catas-",
  "context-free languages in time n3. Information and\nControl, 10(2), 189–208. Bibliography\n1093\nYudkowsky, E. (2008).\nArtiﬁcial intelligence as\na positive and negative factor in global risk.\nIn\nBostrom, N. and Cirkovic, M. (Eds.), Global Catas-\ntrophic Risk. Oxford University Press.\nZadeh, L. A. (1965). Fuzzy sets. Information and\nControl, 8, 338–353.\nZadeh, L. A. (1978). Fuzzy sets as a basis for a the-\nory of possibility. Fuzzy Sets and Systems, 1, 3–28.\nZaritskii, V. S., Svetnik, V. B., and Shimelevich,\nL. I. (1975). Monte-Carlo technique in problems of\noptimal information processing. Automation and Re-\nmote Control, 36, 2015–22.\nZelle, J. and Mooney, R. (1996). Learning to parse\ndatabase queries using inductive logic programming.\nIn AAAI-96, pp. 1050–1055.\nZermelo, E. (1913).\nUber Eine Anwendung der\nMengenlehre auf die Theorie des Schachspiels. In\nProc. Fifth International Congress of Mathemati-\ncians, Vol. 2, pp. 501–504.\nZermelo, E. (1976). An application of set theory to\nthe theory of chess-playing. Firbush News, 6, 37–42.\nEnglish translation of (Zermelo 1913).\nZettlemoyer, L. S. and Collins, M. (2005). Learning\nto map sentences to logical form: Structured classi-\nﬁcation with probabilistic categorial grammars. In\nUAI-05.\nZhang, H. and Stickel, M. E. (1996). An efﬁcient\nalgorithm for unit-propagation. In Proc. Fourth In-\nternational Symposium on Artiﬁcial Intelligence and\nMathematics.\nZhang, L., Pavlovic, V., Cantor, C. R., and Kasif, S.\n(2003). Human-mouse gene identiﬁcation by com-\nparative evidence integration and evolutionary anal-\nysis. Genome Research, pp. 1–13.\nZhang, N. L. and Poole, D. (1994). A simple ap-\nproach to Bayesian network computations. In Proc.\n10th Canadian Conference on Artiﬁcial Intelligence,\npp. 171–178.\nZhang, N. L., Qi, R., and Poole, D. (1994). A com-\nputational theory of decision networks.\nIJAR, 11,\n83–158.\nZhou, R. and Hansen, E. (2002). Memory-bounded\nA* graph search. In Proc. 15th International Flairs\nConference.\nZhou, R. and Hansen, E. (2006).\nBreadth-ﬁrst\nheuristic search. AIJ, 170(4–5), 385–408.\nZhu, D. J. and Latombe, J.-C. (1991). New heuris-\ntic algorithms for efﬁcient hierarchical path plan-\nning. IEEE Transactions on Robotics and Automa-\ntion, 7(1), 9–20.\nZimmermann, H.-J. (Ed.). (1999). Practical appli-\ncations of fuzzy technologies. Kluwer.\nZimmermann, H.-J. (2001).\nFuzzy Set Theory—\nAnd Its Applications (Fourth edition). Kluwer.\nZinkevich, M., Johanson, M., Bowling, M., and Pic-",
  "tion, 7(1), 9–20.\nZimmermann, H.-J. (Ed.). (1999). Practical appli-\ncations of fuzzy technologies. Kluwer.\nZimmermann, H.-J. (2001).\nFuzzy Set Theory—\nAnd Its Applications (Fourth edition). Kluwer.\nZinkevich, M., Johanson, M., Bowling, M., and Pic-\ncione, C. (2008). Regret minimization in games with\nincomplete information. In NIPS 20, pp. 1729–1736.\nZollmann, A., Venugopal, A., Och, F. J., and Ponte,\nJ. (2008). A systematic comparison of phrase-based,\nhierarchical and syntax-augmented statistical MT. In\nCOLING-08.\nZweig, G. and Russell, S. J. (1998). Speech recogni-\ntion with dynamic Bayesian networks. In AAAI-98,\npp. 173–180. This page intentionally left blank Index\nPage numbers in bold refer to deﬁnitions of terms and algorithms; page numbers in italics refer to items in\nthe bibliography.\nSymbols\n∧(and), 244\nχ2 (chi squared), 706\n| (cons list cell), 305\n⊢(derives), 242\n≻(determination), 784\n|= (entailment), 240\nϵ-ball, 714\n∃(there exists), 297\n∀(for all), 295\n| (given), 485\n⇔(if and only if), 244\n⇒(implies), 244\n∼(indifferent), 612\nλ (lambda)-expression, 294\n¬ (not), 244\n∨(or), 244\n≻(preferred), 612\n\f→(uncertain rule), 548\nA\nA(s) (actions in a state), 645\nA* search, 93–99\nAAAI (American Association for AI),\n31\nAarup, M., 432, 1064\nAbbeel, P., 556, 857, 1068, 1090\nAbbott, L. F., 763, 854, 1070\nABC computer, 14\nAbdennadher, S., 230, 1073\nAbelson, R. P., 23, 921, 1088\nAbney, S., 921, 1064\nABO (Asymptotic Bounded\nOptimality), 1050\nAbramson, B., 110, 1064\nabsolute error, 98\nabstraction, 69, 677\nabstraction hierarchy, 432\nABSTRIPS, 432\nAbu-Hanna, A., 505, 1081\nAC-3, 209\nAcademy Award, 435\naccessibility relations, 451\naccusative case, 899\nAcero, A., 922, 1076\nAcharya, A., 112, 1068\nAchlioptas, D., 277, 278, 1064\nAckley, D. H., 155, 1064\nacoustic model, 913\nin disambiguation, 906\nACT, 336\nACT*, 799\nacting rationally, 4\naction, 34, 67, 108, 367\nhigh-level, 406\njoint, 427\nmonitoring, 423, 424\nprimitive, 406\nrational, 7, 30\naction-utility function, 627, 831\naction exclusion axiom, 273, 428\naction monitoring, 423, 424\naction schema, 367\nactivation function, 728\nactive learning, 831\nactive sensing, 928\nactive vision, 1025\nactor, 426\nactuator, 34, 41\nhydraulic, 977\npneumatic, 977\nAD-tree, 826\nADABOOST, 751\nadalines, 20\nAdams, J., 450\nAda programming language, 14\nadaptive control theory, 833, 854\nadaptive dynamic programming, 834,\n834–835, 853, 858\nadaptive perception, 985\nadd-one smoothing, 863\nadd list, 368\nAdelson-Velsky, G. M., 192, 1064\nAdida, B., 469, 1064",
  "Adams, J., 450\nAda programming language, 14\nadaptive control theory, 833, 854\nadaptive dynamic programming, 834,\n834–835, 853, 858\nadaptive perception, 985\nadd-one smoothing, 863\nadd list, 368\nAdelson-Velsky, G. M., 192, 1064\nAdida, B., 469, 1064\nADL (Action Description Language),\n394\nadmissible heuristic, 94, 376\nAdorf, H.-M., 432, 1077\nADP (Adaptive Dynamic\nProgramming), 834\nadversarial search, 161\nadversarial task, 866\nadversary argument, 149\nAdvice Taker, 19, 23\nAFSM, 1003\nagent, 4, 34, 59\nactive, 839\narchitecture of, 26, 1047\nautonomous, 236\ncomponents, 1044–1047\ndecision-theoretic, 483, 610, 664–666\ngoal-based, 52–53, 59, 60\ngreedy, 839\nhybrid, 268\nintelligent, 30, 1036, 1044\nknowledge-based, 13, 234–236, 285,\n1044\nlearning, 54–57, 61\nlogical, 265–274, 314\nmodel-based, 50, 50–52\nonline planning, 431\npassive, 832\npassive ADP, 858\npassive learning, 858\nproblem-solving, 64, 64–69\nrational, 4, 4–5, 34, 36–38, 59, 60,\n636, 1044\nreﬂex, 48, 48–50, 59, 647, 831\nsituated, 1025\nsoftware agent, 41\ntaxi-driving, 56, 1047\nutility-based, 53–54, 59, 664\nvacuum, 37, 62–63\nwumpus, 238, 305\nagent function, 35, 647\nagent program, 35, 46, 59\nAgerbeck, C., 228, 1064\nAggarwal, G., 682, 1064\naggregation, 403\nAgichtein, E., 885, 1064\nAgmon, S., 761, 1064\nAgre, P. E., 434, 1064\nagreement (in a sentence), 900\nAguirre, A., 278, 1068\nAho, A. V., 1059, 1064\nAI, see artiﬁcial intelligence\naircraft carrier scheduling, 434\nairport, driving to, 480\nairport siting, 622, 626\nAISB (Society for Artiﬁcial Intelligence\nand Simulation of Behaviour),\n31\nAI Winter, 24, 28\nAizerman, M., 760, 1064\nAl-Chang, M., 28, 1064\n1095 1096\nIndex\nal-Khowarazmi, 8\nAlberti, L. B., 966\nAlbus, J. S., 855, 1064\nAldiss, B., 1040\nAldous, D., 154, 1064\nAlekhnovich, M., 277, 1064\nAlexandria, 15\nalgorithm, 8\nalgorithmic complexity, 759\nAlhazen, 966\nalignment method, 956\nAllais, M., 620, 638, 1064\nAllais paradox, 620\nAlldiff constraint, 206\nAllen, B., 432, 1072\nAllen, C., 638, 1069\nAllen, J. F., 396, 431, 448, 470, 1064\nalliance (in multiplayer games), 166\nAllis, L., 194, 1064\nAlmanac Game, 640\nAlmuallim, H., 799, 1064\nAlmulla, M., 111, 1085\nALPAC., 922, 1064\nAlperin Resnick, L., 457, 471, 1066\nα (normalization constant), 497\nalpha–beta pruning, 167, 199\nalpha–beta search, 167–171, 189, 191\nALPHA-BETA-SEARCH, 170\nAlterman, R., 432, 1064\nAltman, A., 195, 1064\naltruism, 483\nAlvey report, 24\nAM, 800\nAmarel, S., 109, 115, 156, 468, 1064\nambient illumination, 934\nambiguity, 287, 465, 861, 904–912, 919\nlexical, 905",
  "ALPHA-BETA-SEARCH, 170\nAlterman, R., 432, 1064\nAltman, A., 195, 1064\naltruism, 483\nAlvey report, 24\nAM, 800\nAmarel, S., 109, 115, 156, 468, 1064\nambient illumination, 934\nambiguity, 287, 465, 861, 904–912, 919\nlexical, 905\nsemantic, 905\nsyntactic, 905, 920\nambiguity aversion, 620\nAmir, E., 195, 278, 556, 1064, 1070,\n1086\nAmit, D., 761, 1064\nanalogical reasoning, 799\nANALOGY, 19, 31\nanalysis of algorithms, 1053\nAnalytical Engine, 14\nanalytical generalization, 799\nAnantharaman, T. S., 192, 1076\nAnbulagan, 277, 1080\nanchoring effect, 621\nanchor text, 463\nAND–OR graph, 257\nAnd-Elimination, 250\nAND-OR-GRAPH-SEARCH, 136\nAND–OR tree, 135\nAND-SEARCH, 136\nAndersen, S. K., 552, 553, 1064\nAnderson, C. R., 395, 433, 1091\nAnderson, C. W., 855, 1065\nAnderson, J. A., 761, 1075\nAnderson, J. R., 13, 336, 555, 799,\n1064, 1085\nAND node, 135\nAndoni, A., 760, 1064\nAndre, D., 156, 855, 856, 1064, 1070,\n1079\nANGELIC-SEARCH, 414\nangelic semantics, 431\nanswer literal, 350\nanswer set programming, 359\nantecedent, 244\nAnthony, M., 762, 1064\nanytime algorithm, 1048\nAoki, M., 686, 1064\naortic coarctation, 634\napparent motion, 940\nappearance, 942\nappearance model, 959\nAppel, K., 227, 1064\nAppelt, D., 884, 921, 1064, 1075, 1076\nAPPEND, 341\napplicable, 67, 368, 375\napprenticeship learning, 857, 1037\napproximate near-neighbors, 741\nApt, K. R., 228, 230, 1064\nApt´e, C., 884, 1064\nArbuthnot, J., 504, 1064\narc consistency, 208\nArchibald, C., 195, 1064\narchitecture, 46\nagent, 26, 1047\ncognitive, 336\nfor speech recognition, 25\nhybrid, 1003, 1047\nparallel, 112\npipeline, 1005\nreﬂective, 1048\nrule-based, 336\nthree-layer, 1004\narc reversal, 559\nArentoft, M. M., 432, 1064\nargmax, 1059\nargmax, 166\nargument\nfrom disability, 1021–1022\nfrom informality, 1024–1025\nAriely, D., 619, 638, 1064\nAristotle, 4–7, 10, 59, 60, 275, 313, 468,\n469, 471, 758, 966, 1041\narity, 292, 332\nArkin, R., 1013, 1064\nArlazarov, V. L., 192, 1064\nArmando, A., 279, 1064\nArnauld, A., 7, 636, 1064\nArora, S., 110, 1064\nARPAbet, 914\nartiﬁcial ﬂight, 3\nArtiﬁcial General Intelligence, 27\nartiﬁcial intelligence, 1, 1–1052\napplications of, 28–29\nconferences, 31\nfoundations, 5–16, 845\nfuture of, 1051–1052\ngoals of, 1049–1051\nhistory of, 16–28\njournals, 31\nphilosophy of, 1020–1043\npossibility of, 1020–1025\nprogramming language, 19\nreal-time, 1047\nsocieties, 31\nstrong, 1020, 1026–1033, 1040\nsubﬁelds, 1\nas universal ﬁeld, 1\nweak, 1020, 1040\nartiﬁcial life, 155\nartiﬁcial urea, 1027\nArunachalam, R., 688, 1064\nAsada, M., 195, 1014, 1078",
  "programming language, 19\nreal-time, 1047\nsocieties, 31\nstrong, 1020, 1026–1033, 1040\nsubﬁelds, 1\nas universal ﬁeld, 1\nweak, 1020, 1040\nartiﬁcial life, 155\nartiﬁcial urea, 1027\nArunachalam, R., 688, 1064\nAsada, M., 195, 1014, 1078\nasbestos removal, 615\nAshby, W. R., 15, 1064\nAsimov, I., 1011, 1038, 1064\nASKMSR, 872, 873, 885\nassertion (logical), 301\nassignment (in a CSP), 203\nassociative memory, 762\nassumption, 462\nAstrom, K. J., 156, 686, 1064\nastronomer, 562\nasymptotic analysis, 1054, 1053–1054\nasymptotic bounded optimality, 1050\nAtanasoff, J., 14\nAtkeson, C. G., 854, 1083\nAtkin, L. R., 110, 1089\natom, 295\natomic representation, 57, 64\natomic sentence, 244, 295, 294–295,\n299\nattribute, 58\nattribute-based extraction, 874\nauction, 679\nascending-bid, 679\nDutch, 692\nEnglish, 679\nﬁrst-price, 681\nsealed-bid, 681\nsecond-price, 681 Index\n1097\ntruth-revealing, 680\nVickrey, 681\nAudi, R., 1042, 1064\nAuer, S., 439, 469, 1066\naugmentation, 919\naugmented ﬁnite state machine\n(AFSM), 1003\naugmented grammar, 897\nAURA, 356, 360\nAustin, G. A., 798, 1067\nAustralia, 203, 204, 216\nauthority, 872\nAUTOCLASS, 826\nautomata, 1035, 1041\nautomated debugging, 800\nautomated taxi, 40, 56, 236, 480, 694,\n695, 1047\nautomobile insurance, 621\nAuton, L. D., 277, 1069\nautonomic computing, 60\nautonomous underwater vehicle (AUV),\n972\nautonomy, 39\naverage reward, 650\nAxelrod, R., 687, 1064\naxiom, 235, 302\naction exclusion, 273, 428\nof Chinese room, 1032\ndecomposability, 614\ndomain-speciﬁc, 439\neffect axiom, 266\nframe axiom, 267\nKolmogorov’s, 489\nof number theory, 303\nof probability, 489\nPeano, 303, 313, 333\nprecondition, 273\nof probability, 488–490, 1057\nof set theory, 304\nsuccessor-state, 267, 279, 389\nof utility theory, 613\nwumpus world, 305\naxon, 11\nB\nb∗(branching factor), 103\nB* search, 191\nBaader, F., 359, 471, 1064\nBabbage, C., 14, 190\nBacchus, F., 228, 230, 505, 555, 638,\n1064, 1065\nbachelor, 441\nBachmann, P. G. H., 1059, 1065\nBACK-PROP-LEARNING, 734\nback-propagation, 22, 24, 733–736, 761\nbackgammon, 177–178, 186, 194, 846,\n850\nbackground knowledge, 235, 349, 777,\n1024, 1025\nbackground subtraction, 961\nbacking up (in a search tree), 99, 165\nbackjumping, 219, 229\nbackmarking, 229\nbackoff model, 863\nBACKTRACK, 215\nbacktracking\nchronological, 218\ndependency-directed, 229\ndynamic, 229\nintelligent, 218–220, 262\nBACKTRACKING-SEARCH, 215\nbacktracking search, 87, 215, 218–220,\n222, 227\nBackus, J. W., 919, 1065\nBackus–Naur form (BNF), 1060\nbackward chaining, 257, 259, 275,\n337–345, 358",
  "dependency-directed, 229\ndynamic, 229\nintelligent, 218–220, 262\nBACKTRACKING-SEARCH, 215\nbacktracking search, 87, 215, 218–220,\n222, 227\nBackus, J. W., 919, 1065\nBackus–Naur form (BNF), 1060\nbackward chaining, 257, 259, 275,\n337–345, 358\nbackward search for planning, 374–376\nBacon, F., 6\nbagging, 760\nBagnell, J. A., 852, 1013, 1065\nbag of words, 866, 883\nBaird, L. C. I., 685, 1092\nBaker, J., 920, 922, 1065\nBalashek, S., 922, 1070\nBaldi, P., 604, 1065\nBaldwin, J. M., 130, 1065\nBall, M., 396, 1091\nBallard, B. W., 191, 200, 1065\nBaluja, S., 155, 968, 1065, 1087\nBancilhon, F., 358, 1065\nbandit problem, 840, 855\nBanerji, R., 776, 799, 1082\nbang-bang control, 851\nBangera, R., 688, 1092\nBanko, M., 28, 439, 469, 756, 759, 872,\n881, 885, 1065, 1072\nBar-Hillel, Y., 920, 922, 1065\nBar-Shalom, Y., 604, 606, 1065\nBarifaijo, E., 422, 1077\nBarry, M., 553, 1076\nBartak, R., 230, 1065\nBartlett, F., 13\nBartlett, P., 762, 855, 1064, 1065\nBarto, A. G., 157, 685, 854, 855, 857,\n1065, 1067, 1090\nBarwise, J., 280, 314, 1065\nbaseline, 950\nbasic groups, 875\nBasin, D. A., 191, 1072\nbasis function, 845\nBasye, K., 1012, 1070\nBates, E., 921, 1071\nBates, M. A., 14, 192, 1090\nBatman, 435\nbats, 435\nBaum, E., 128, 191, 761, 762, 1065\nBaum, L. E., 604, 826, 1065\nBaumert, L., 228, 1074\nBaxter, J., 855, 1065\nBayardo, R. J., 229, 230, 277, 1065\nBayer, K. M., 228, 1086\nBayerl, S., 359, 1080\nBayes’ rule, 9, 495, 495–497, 503, 508\nBayes, T., 495, 504, 1065\nBayes–Nash equilibrium, 678\nBayesian, 491\nBayesian classiﬁer, 499\nBayesian learning, 752, 803, 803–804,\n825\nBayesian network, 26, 510, 510–517,\n551, 565, 827\ndynamic, 590, 590–599\nhybrid, 520, 552\ninference in, 522–530\nlearning hidden variables in, 824\nlearning in, 813–814\nmulti-entity, 556\nBayes Net toolkit, 558\nBeal, D. F., 191, 1065\nBeal, J., 27, 1065\nBeame, P., 277, 1064\nbeam search, 125, 174\nBear, J., 884, 1075\nBeber, G., 30, 1071\nBeckert, B., 359, 1065\nbeer factory scheduling, 434\nBeeri, C., 229, 1065\nbeetle, dung, 39, 61, 424, 1004\nbehaviorism, 12, 15, 60\nBekey, G., 1014, 1065\nbelief, 450, 453\ndegree of, 482, 489\ndesires and, 610–611\nbelief function, 549\nbelief network, see Bayesian network\nbelief propagation, 555\nbelief revision, 460\nbelief state, 138, 269, 415, 480\nin game theory, 675\nprobabalistic, 566, 570\nwiggly, 271\nbelief update, 460\nBell, C., 408, 431, 1065\nBell, D. A., 826, 1068\nBell, J. L., 314, 1065\nBell, T. C., 883, 884, 1092 1098\nIndex\nBELLE, 192\nBell Labs, 922\nBellman, R. E., 2, 10, 109, 110, 194,\n652, 685, 760, 1065",
  "probabalistic, 566, 570\nwiggly, 271\nbelief update, 460\nBell, C., 408, 431, 1065\nBell, D. A., 826, 1068\nBell, J. L., 314, 1065\nBell, T. C., 883, 884, 1092 1098\nIndex\nBELLE, 192\nBell Labs, 922\nBellman, R. E., 2, 10, 109, 110, 194,\n652, 685, 760, 1065\nBellman equation, 652\nBellman update, 652\nBelongie, S., 755, 762, 1065\nBen-Tal, A., 155, 1065\nbenchmarking, 1053\nBendix, P. B., 359, 1078\nBengio, S., 604, 1089\nBengio, Y., 760, 1047, 1065\nBENINQ, 472\nBennett, B., 473, 1069\nBennett, F. H., 156, 1079\nBennett, J., 360, 1074\nBentham, J., 637, 1065\nBerger, H., 11\nBerger, J. O., 827, 1065\nBerkson, J., 554, 1065\nBerlekamp, E. R., 113, 186, 1065\nBerleur, J., 1034, 1065\nBerliner, H. J., 191, 194, 198, 1065\nBernardo, J. M., 811, 1065\nBerners-Lee, T., 469, 1065\nBernoulli, D., 617, 637, 1065\nBernoulli, J., 9, 504\nBernoulli, N., 641\nBernstein, A., 192, 1065\nBernstein, P. L., 506, 691, 1065\nBerrou, C., 555, 1065\nBerry, C., 14\nBerry, D. A., 855, 1065\nBertele, U., 553, 1066\nBertoli, P., 433, 1066\nBertot, Y., 359, 1066\nBertsekas, D., 60, 506, 685, 857, 1059,\n1066\nBESM, 192\nBessi`ere, C., 228, 1066\nbest-ﬁrst search, 92, 108\nbest possible prize, 615\nbeta distribution, 592, 811\nBetlem, H., 422, 1077\nBetlem, J., 422, 1077\nbetting game, 490\nBezzel, M., 109\nBGBLITZ, 194\nBhar, R., 604, 1066\nBialik, H. N., 908\nbias, declarative, 787\nBibel, W., 359, 360, 1066, 1080\nBickford, M., 356, 1089\nbiconditional, 244\nBiddulph, R., 922, 1070\nbidirectional search, 90–112\nBidlack, C., 1013, 1069\nBiere, A., 278, 1066\nBigelow, J., 15, 1087\nBigham, J., 885, 1085\nbilingual corpus, 910\nbilliards, 195\nBillings, D., 678, 687, 1066\nBilmes, J., 604, 1080, 1086\nbinary decision diagram, 395\nbinary resolution, 347\nBinder, J., 604, 605, 826, 1066, 1087\nbinding list, 301\nBinford, T. O., 967, 1066\nBinmore, K., 687, 1066\nbinocular stereopsis, 949, 949–964\nbinomial nomenclature, 469\nbioinformatics, 884\nbiological naturalism, 1031\nBirbeck, M., 469, 1064\nBishop, C. M., 155, 554, 759, 762, 763,\n827, 1066\nBishop, M., 1042, 1086\nBishop, R. H., 60, 1071\nBisson, T., 1042, 1066\nBistarelli, S., 228, 1066\nBitman, A. R., 192, 1064\nBitner, J. R., 228, 1066\nBizer, C., 439, 469, 1066\nBjornsson, Y., 194, 1088\nBKG (backgammon program), 194\nBLACKBOX, 395\nBlake, A., 605, 1077\nBlakeslee, S., 1047, 1075\nBlazewicz, J., 432, 1066\nBlei, D. M., 883, 1066\nBlinder, A. S., 691, 1066\nblind search, see search, uninformed\nBliss, C. I., 554, 1066\nBlock, H. D., 20, 1066\nblocks world, 20, 23, 370, 370–371, 472\nBLOG, 556\nbluff, 184",
  "Blakeslee, S., 1047, 1075\nBlazewicz, J., 432, 1066\nBlei, D. M., 883, 1066\nBlinder, A. S., 691, 1066\nblind search, see search, uninformed\nBliss, C. I., 554, 1066\nBlock, H. D., 20, 1066\nblocks world, 20, 23, 370, 370–371, 472\nBLOG, 556\nbluff, 184\nBlum, A. L., 395, 752, 761, 885, 1066\nBlumer, A., 759, 1066\nBM25 scoring function, 868, 884\nBNF (Backus–Naur form), 1060\nBO, 1050\nBobick, A., 604, 1077\nBobrow, D. G., 19, 884, 1066\nBoddy, M., 156, 433, 1048, 1070, 1074\nBoden, M. A., 275, 1042, 1066\nbody (of Horn clause), 256\nboid, 429, 435\nBolognesi, A., 192, 1066\nBoltzmann machine, 763\nBonaparte, N., 190\nBoneh, D., 128, 1065\nBonet, B., 156, 394, 395, 433, 686,\n1066, 1075\nBongard, J., 1041, 1085\nBoole, G., 7, 8, 276, 1066\nBoolean keyword model, 867\nboosting, 749, 760\nBooth, J. W., 872\nBooth, T. L., 919, 1066\nbootstrap, 27, 760\nBorel, E., 687, 1066\nBorenstein, J., 1012, 1013, 1066\nBorgida, A., 457, 471, 1066\nBoroditsky, L., 287, 1066\nBoser, B., 760, 762, 1066, 1080\nBOSS, 28, 1007, 1008, 1014\nBosse, M., 1012, 1066\nBotea, A., 395, 1075\nBottou, L., 762, 967, 1080\nboundary set, 774\nbounded optimality (BO), 1050\nbounded rationality, 1049\nbounds consistent, 212\nbounds propagation, 212\nBourlard, H., 604, 1089\nBourzutschky, M., 176, 1066\nBoutilier, C., 434, 553, 686, 1066\nBouzy, B., 194, 1066\nBowden, B. V., 14, 192, 1090\nBower, G. H., 854, 1075\nBowerman, M., 314, 1066\nBowling, M., 687, 1066, 1091, 1093\nBox, G. E. P., 155, 604, 1066\nBOXES, 851\nBoyan, J. A., 154, 854, 1066\nBoyd, S., 155, 1066\nBoyden, E., 11, 1074\nBoyen, X., 605, 1066\nBoyen–Koller algorithm, 605\nBoyer, R. S., 356, 359, 360, 1066\nBoyer–Moore theorem prover, 359, 360\nBoyle, J., 360, 1092\nBrachman, R. J., 457, 471, 473, 1066,\n1067, 1080\nBradshaw, G. L., 800, 1079\nBradtke, S. J., 157, 685, 854, 855, 1065,\n1067\nBrady, J. M., 604, 1084\nBrafman, O., 638, 1067\nBrafman, R., 638, 1067\nBrafman, R. I., 433, 434, 855, 1066,\n1067, 1076\nBrahmagupta, 227\nbrain, 16\ncomputational power, 12\ncomputer vs., 12 Index\n1099\ndamage, optimal, 737\nreplacement, 1029–1031, 1043\nsuper, 9\nin a vat, 1028\nbrains cause minds, 11\nBraitenberg, V., 1013, 1067\nbranching factor, 80, 783\neffective, 103, 111, 169\nBransford, J., 927, 1067\nBrants, T., 29, 883, 921, 1067, 1072\nBratko, I., 112, 359, 793, 1067\nBratman, M. E., 60, 1041, 1067\nBraverman, E., 760, 1064\nBREADTH-FIRST-SEARCH, 82\nbreadth-ﬁrst search, 81, 81–83, 108, 408\nBreese, J. S., 61, 553, 555, 639, 1048,\n1067, 1076, 1091\nBreiman, L., 758, 760, 1067\nBrelaz, D., 228, 1067",
  "Bratman, M. E., 60, 1041, 1067\nBraverman, E., 760, 1064\nBREADTH-FIRST-SEARCH, 82\nbreadth-ﬁrst search, 81, 81–83, 108, 408\nBreese, J. S., 61, 553, 555, 639, 1048,\n1067, 1076, 1091\nBreiman, L., 758, 760, 1067\nBrelaz, D., 228, 1067\nBrent, R. P., 154, 1067\nBresina, J., 28, 1064\nBresnan, J., 920, 1067\nBrewka, G., 472, 1067\nBrey, R., 637, 1086\nBrickley, D., 469, 1067\nbridge (card game), 32, 186, 195\nBridge Baron, 189\nBridle, J. S., 761, 1067\nBriggs, R., 468, 1067\nbrightness, 932\nBrill, E., 28, 756, 759, 872, 885, 1065\nBrin, D., 881, 885, 1036, 1067\nBrin, S., 870, 880, 884, 1067\nBringsjord, S., 30, 1067\nBrioschi, F., 553, 1066\nBritain, 22, 24\nBroadbent, D. E., 13, 1067\nBroadhead, M., 885, 1065\nBroca, P., 10\nBrock, B., 360, 1076\nBrokowski, M., 156, 1092\nBrooks, M. J., 968, 1076\nBrooks, R. A., 60, 275, 278, 434, 1003,\n1012, 1013, 1041, 1067, 1085\nBrouwer, P. S., 854, 1065\nBrown, C., 230, 1067\nBrown, J. S., 472, 800, 1070, 1080\nBrown, K. C., 637, 1067\nBrown, M., 604, 1079\nBrown, P. F., 922, 1067\nBrownston, L., 358, 1067\nBruce, V., 968, 1067\nBrunelleschi, F., 966\nBruner, J. S., 798, 1067\nBrunnstein, K., 1034, 1065\nBrunot, A., 762, 967, 1080\nBryant, B. D., 435, 1067\nBryce, D., 157, 395, 433, 1067\nBryson, A. E., 22, 761, 1067\nBuchanan, B. G., 22, 23, 61, 468, 557,\n776, 799, 1067, 1072, 1080\nBuckley, C., 870, 1089\nBuehler, M., 1014, 1067\nBUGS, 554, 555\nBUILD, 472\nBulﬁn, R., 688, 1086\nbunch, 442\nBundy, A., 799, 1091\nBunt, H. C., 470, 1067\nBuntine, W., 800, 1083\nBurch, N., 194, 678, 687, 1066, 1088\nBurgard, W., 606, 1012–1014, 1067,\n1068, 1072, 1088, 1090\nBurges, C., 884, 1090\nburglar alarm, 511–513\nBurkhard, H.-D., 1014, 1091\nBurns, C., 553, 1083\nBuro, M., 175, 186, 1067\nBurstein, J., 1022, 1067\nBurton, R., 638, 1067\nBuss, D. M., 638, 1067\nButler, S., 1042, 1067\nBylander, T., 393, 395, 1067\nByrd, R. H., 760, 1067\nC\nc (step cost), 68\nCabeza, R., 11, 1067\nCabral, J., 469, 1081\ncaching, 269\nCafarella, M. J., 885, 1065, 1067, 1072\nCajal, S., 10\ncake, eating and having, 380\ncalculus, 131\ncalculus of variations, 155\nCalvanese, D., 471, 1064, 1067\nCambefort, Y., 61, 1075\nCambridge, 13\ncamera\ndigital, 930, 943\nfor robots, 973\npinhole, 930\nstereo, 949, 974\ntime of ﬂight, 974\nvideo, 929, 963\nCameron-Jones, R. M., 793, 1086\nCampbell, M. S., 192, 1067, 1076\nCampbell, W., 637, 1068\ncandidate elimination, 773\ncan machines think?, 1021\nCanny, J., 967, 1013, 1068\nCanny edge detection, 755, 967\ncanonical distribution, 518\ncanonical form, 80\nCantor, C. R., 553, 1093",
  "Campbell, M. S., 192, 1067, 1076\nCampbell, W., 637, 1068\ncandidate elimination, 773\ncan machines think?, 1021\nCanny, J., 967, 1013, 1068\nCanny edge detection, 755, 967\ncanonical distribution, 518\ncanonical form, 80\nCantor, C. R., 553, 1093\nCantu-Paz, E., 155, 1085\nCapek, K., 1011, 1037\nCapen, E., 637, 1068\nCaprara, A., 395, 1068\nCarbone, R., 279, 1064\nCarbonell, J. G., 27, 432, 799, 1068,\n1075, 1091\nCarbonell, J. R., 799, 1068\nCardano, G., 9, 194, 503, 1068\ncard games, 183\nCarin, L., 686, 1077\nCarlin, J. B., 827, 1073\nCarlson, A., 288, 1082\nCARMEL, 1013\nCarnap, R., 6, 490, 491, 504, 505, 555,\n1068\nCarnegie Mellon University, 17, 18\nCarpenter, M., 432, 1070\nCarreras, X., 920, 1079\nCarroll, S., 155, 1068\nCarson, D., 359, 1092\ncart–pole problem, 851\nCasati, R., 470, 1068\ncascaded ﬁnite-state transducers, 875\ncase-based reasoning, 799\ncase agreement, 900\ncase folding, 870\ncase statement (in condition plans), 136\nCash, S. S., 288, 1087\nCassandra, A. R., 686, 1068, 1077\nCassandras, C. G., 60, 1068\nCasteran, P., 359, 1066\nCastro, R., 553, 1068\ncategorization, 865\ncategory, 440, 440–445, 453\ncausal network, see Bayesian network\ncausal probability, 496\ncausal rule, 317, 517\ncausation, 246, 498\ncaveman, 778\nCazenave, T., 194, 1066\nCCD (charge-coupled device), 930, 969\ncell decomposition, 986, 989\nexact, 990\ncell layout, 74\ncenter (in mechanism design), 679\ncentral limit theorem, 1058\ncerebral cortex, 11\ncertainty effect, 620\ncertainty equivalent, 618\ncertainty factor, 23, 548, 557\nCesa-Bianchi, N., 761, 1068 1100\nIndex\nCesta, A., 28, 1068\nCGP, 433\nCHAFF, 277\nChaﬁn, B., 28, 1064\nchain rule (for differentiation), 726\nchain rule (for probabilities), 514\nChakrabarti, P. P., 112, 157, 1068, 1069\nChambers, R. A., 851, 854, 1082\nchance node (decision network), 626\nchance node (game tree), 177\nchance of winning, 172\nChandra, A. K., 358, 1068\nChang, C.-L., 360, 1068\nChang, K.-M., 288, 1082\nChang, K. C., 554, 1073\nchannel routing, 74\nChapman, D., 394, 434, 1064, 1068\nChapman, N., 109\ncharacters, 861\nCharest, L., 28, 1064\ncharge-coupled device, 930, 969\nCharniak, E., 2, 23, 358, 556, 557, 604,\n920, 921, 1068\nchart parser, 893, 919\nChase, A., 28, 1064\nchatbot, 1021\nChater, N., 638, 1068, 1084\nChatﬁeld, C., 604, 1068\nChatila, R., 1012, 1083\nChauvin, Y., 604, 1065\ncheckers, 18, 61, 186, 193, 850\ncheckmate\naccidental, 182\nguaranteed, 181\nprobabilistic, 181\nCheeseman, P., 9, 26, 229, 277, 557,\n826, 1012, 1068, 1089\nChekaluk, R., 1012, 1070\nchemistry, 22\nChen, R., 605, 1080",
  "Chauvin, Y., 604, 1065\ncheckers, 18, 61, 186, 193, 850\ncheckmate\naccidental, 182\nguaranteed, 181\nprobabilistic, 181\nCheeseman, P., 9, 26, 229, 277, 557,\n826, 1012, 1068, 1089\nChekaluk, R., 1012, 1070\nchemistry, 22\nChen, R., 605, 1080\nChen, S. F., 883, 1068\nChen, X., 395, 1091\nCheng, J., 554, 826, 1068\nCheng, J.-F., 555, 1082\nChervonenkis, A. Y., 759, 1091\nchess, 172–173, 185–186\nautomaton, 190\nhistory, 192\nprediction, 21\nChess, D. M., 60, 1078\nCHESS 4.5, 110\nχ2 pruning, 706\nChickering, D. M., 191, 826, 1075, 1079\nChien, S., 431, 1073\nCHILD-NODE, 79\nCHILL, 902\nchimpanzee, 860\nChinese room, 1031–1033\nCHINOOK, 186, 193, 194\nChklovski, T., 439, 1068\nchoice point, 340\nChomsky, C., 920, 1074\nChomsky, N., 13, 16, 883, 889, 919,\n921, 923, 1068\nChomsky Normal Form, 893, 919\nChopra, S., 762, 1086\nChoset, H., 1013, 1014, 1068\nChoueiry, B. Y., 228, 1086\nChristmas, 1026\nchronicles, 470\nchronological backtracking, 218\ncHUGIN, 554\nChung, K. L., 1059, 1068\nChung, S., 278, 1092\nchunking, 799\nChurch, A., 8, 314, 325, 358, 1068\nChurch, K., 883, 894, 920, 923, 1068\nChurchland, P. M., 1042, 1068\nChurchland, P. S., 1030, 1042, 1068\nCiancarini, P., 60, 192, 1066, 1068\nCIGOL, 800\nCimatti, A., 396, 433, 1066, 1068\ncircuit veriﬁcation, 312\ncircumscription, 459, 468, 471\nprioritized, 459\ncity block distance, 103\nClaessen, K., 360, 1090\nclairvoyance, 184\nClamp, S. E., 505, 1070\nClapp, R., 637, 1068\nClark, A., 1025, 1041, 1068\nClark, K. L., 472, 1068\nClark, P., 800, 1068\nClark, S., 920, 1012, 1068, 1071\nClark completion, 472\nClarke, A. C., 552, 1034, 1068\nClarke, E., 395, 1068\nClarke, M. R. B., 195, 1068\nCLASSIC, 457, 458\nclassiﬁcation (in description logic), 456\nclassiﬁcation (in learning), 696\nclass probability, 764\nclause, 253\nClearwater, S. H., 688, 1068\nCLINT, 800\nClocksin, W. F., 359, 1068\nclosed-world assumption, 299, 344, 417,\n468, 541\nclosed class, 890\nclosed list, see explored set\nCLP, 228, 345\nCLP(R), 359\nclustering, 553, 694, 817, 818\nclustering (in Bayesian networks), 529,\n529–530\nclutter (in data association), 602\nCMAC, 855\nCMU, 922\nCN2, 800\nCNF (Conjunctive Normal Form), 253\nCNLP, 433\nco-NP, 1055\nco-NP-complete, 247, 276, 1055\nCoarfa, C., 278, 1068\ncoarticulation, 913, 917\ncoastal navigation, 994\nCoates, A., 857, 1068\nCoates, M., 553, 1068\nCobham, A., 8, 1068\nCocke, J., 922, 1067\ncoercion, 416\ncognitive\narchitecture, 336\ncognitive architecture, 336\ncognitive modeling, 3\ncognitive psychology, 13\ncognitive science, 3\nCohen, B., 277, 1088\nCohen, C., 1013, 1069",
  "Coates, M., 553, 1068\nCobham, A., 8, 1068\nCocke, J., 922, 1067\ncoercion, 416\ncognitive\narchitecture, 336\ncognitive architecture, 336\ncognitive modeling, 3\ncognitive psychology, 13\ncognitive science, 3\nCohen, B., 277, 1088\nCohen, C., 1013, 1069\nCohen, P. R., 25, 30, 434, 1069\nCohen, W. W., 800, 1069\nCohn, A. G., 473, 1069\ncoin ﬂip, 548, 549, 641\nCOLBERT, 1013\nCollin, Z., 230, 1069\nCollins, A. M., 799, 1068\nCollins, F. S., 27, 1069\nCollins, M., 760, 920, 921, 1069, 1079,\n1093\ncollusion, 680\nColmerauer, A., 314, 358, 359, 919,\n1069\nColombano, S. P., 155, 1080\ncolor, 935\ncolor constancy, 935\ncombinatorial explosion, 22\ncommitment\nepistemological, 289, 290, 313, 482\nontological, 289, 313, 482, 547\ncommon sense, 546\ncommon value, 679\ncommunication, 286, 429, 888\ncommutativity (in search problems), 214\nCompagna, L., 279, 1064\ncompetitive ratio, 148\ncompilation, 342, 1047\ncomplementary literals, 252\ncomplete-state formulation, 72 Index\n1101\ncomplete assignment, 203\ncomplete data, 806\ncompleteness\nof inference, 247\nof a proof procedure, 242, 274\nof resolution, 350–353\nof a search algorithm, 80, 108\ncompleting the square, 586\ncompletion (of a data base), 344\ncomplexity, 1053–1055\nsample, 715\nspace, 80, 108\ntime, 80, 108\ncomplexity analysis, 1054\ncomplex phrases, 876\ncomplex sentence, 244, 295\ncomplex words, 875\ncompliant motion, 986, 995\ncomponent (of mixture distribution),\n817\ncomposite decision process, 111\ncomposite object, 442\ncompositionality, 286\ncompositional semantics, 901\ncompression, 846\ncomputability, 8\ncomputational learning theory, 713, 714,\n762\ncomputational linguistics, 16\ncomputer, 13–14\nbrain vs., 12\ncomputer vision, 3, 12, 20, 228,\n929–965\nconclusion (of an implication), 244\nconcurrent action list, 428\ncondensation, 605\nCondie, T., 275, 1080\ncondition–action rule, 633\nconditional distributions, 518\nconditional effect, 419\nconditional Gaussian, 521\nconditional independence, 498, 502,\n503, 517–523, 551, 574\nconditional plan, 660\nconditional probability, 485, 503, 514\nconditional probability table (CPT), 512\nconditional random ﬁeld (CRF), 878\nconditioning, 492\nconditioning case, 512\nCondon, J. H., 192, 1069\nconﬁguration space, 986, 987\nconﬁrmation theory, 6, 505\nconﬂict-directed backjumping, 219, 227\nconﬂict clause learning, 262\nconﬂict set, 219\nconformant planning, 415, 417–421,\n431, 433, 994\nCongdon, C. B., 1013, 1069\nconjugate prior, 811\nconjunct, 244\nconjunction (logic), 244\nconjunctive normal form, 253, 253–254,\n275, 345–347",
  "conﬂict clause learning, 262\nconﬂict set, 219\nconformant planning, 415, 417–421,\n431, 433, 994\nCongdon, C. B., 1013, 1069\nconjugate prior, 811\nconjunct, 244\nconjunction (logic), 244\nconjunctive normal form, 253, 253–254,\n275, 345–347\nconjunct ordering, 333\nConlisk, J., 638, 1069\nconnected component, 222\nConnect Four, 194\nconnectionism, 24, 727\nconnective, logical, 16, 244, 274, 295\nConnell, J., 1013, 1069\nconsciousness, 10, 1026, 1029, 1030,\n1033, 1033\nconsequent, 244\nconservative approximation, 271, 419\nconsistency, 105, 456, 769\narc, 208\nof a CSP assignment, 203\nof a heuristic, 95\npath, 210, 228\nconsistency condition, 110\nCONSISTENT-DET?, 786\nconsistent estimation, 531\nConsole, L., 60, 1074\nConsortium, T. G. O., 469, 1069\nconspiracy number, 191\nconstant symbol, 292, 294\nconstraint\nbinary, 206\nglobal, 206, 211\nnonlinear, 205\npreference constraint, 207\npropagation, 208, 214, 217\nresource constraint, 212\nsymmetry-breaking, 226\nunary, 206\nconstraint-based generalization, 799\nconstraint graph, 203, 223\nconstraint hypergraph, 206\nconstraint language, 205\nconstraint learning, 220, 229\nconstraint logic programming, 344–345,\n359\nconstraint logic programming (CLP),\n228, 345\nconstraint optimization problem, 207\nconstraint satisfaction problem (CSP),\n20, 202, 202–207\nconstraint weighting, 222\nconstructive induction, 791\nconsumable resource, 402\ncontext, 286\ncontext-free grammar, 889, 918, 919,\n1060\ncontext-sensitive grammar, 889\ncontingencies, 161\ncontingency planning, 133, 415,\n421–422, 431\ncontinuation, 341\ncontinuity (of preferences), 612\ncontinuous domains, 206\ncontour (in an image), 948, 953–954\ncontour (of a state space), 97\ncontraction mapping, 654\ncontradiction, 250\ncontroller, 59, 997\ncontrol theory, 15, 15, 60, 155, 393,\n761, 851, 964, 998\nadaptive, 833, 854\nrobust, 836\ncontrol uncertainty, 996\nconvention, 429\nconversion to normal form, 345–347\nconvexity, 133\nconvex optimization, 133, 153\nCONVINCE, 552\nconvolution, 938\nConway, J. H., 113, 1065\nCook, P. J., 1035, 1072\nCook, S. A., 8, 276, 278, 1059, 1069\nCooper, G., 554, 826, 1069\ncooperation, 428\ncoordinate frame, 956\ncoordination, 426, 430\ncoordination game, 670\nCopeland, J., 470, 1042, 1069\nCopernicus., 1035, 1069\nCOQ, 227, 359\nCormen, T. H., 1059, 1069\ncorpus, 861\ncorrelated sampling, 850\nCortellessa, G., 28, 1068\nCortes, C., 760, 762, 967, 1069, 1080\ncotraining, 881, 885\ncount noun, 445\nCournot, A., 687, 1069\nCournot competition, 678\ncovariance, 1059\ncovariance matrix, 1058, 1059\nCover, T., 763, 1069",
  "correlated sampling, 850\nCortellessa, G., 28, 1068\nCortes, C., 760, 762, 967, 1069, 1080\ncotraining, 881, 885\ncount noun, 445\nCournot, A., 687, 1069\nCournot competition, 678\ncovariance, 1059\ncovariance matrix, 1058, 1059\nCover, T., 763, 1069\nCowan, J. D., 20, 761, 1069, 1092\nCoward, N., 1022\nCowell, R., 639, 826, 1069, 1089\nCox, I., 606, 1012, 1069\nCox, R. T., 490, 504, 505, 1069\nCPCS, 519, 552\nCPLAN, 395\nCPSC, ix 1102\nIndex\nCPT, 512\nCraig, J., 1013, 1069\nCraik, K. J., 13, 1069\nCrammer, K., 761, 1071\nCraswell, N., 884, 1069\nCrato, N., 229, 1074\nCrauser, A., 112, 1069\nCraven, M., 885, 1069\nCrawford, J. M., 277, 1069\ncreativity, 16\nCremers, A. B., 606, 1012, 1067, 1088\nCresswell, M. J., 470, 1076\nCRF, 878\nCrick, F. H. C., 130, 1091\nCristianini, N., 760, 1069\ncritic (in learning), 55\ncritical path, 403\nCrocker, S. D., 192, 1074\nCrockett, L., 279, 1069\nCroft, B., 884, 1069\nCroft, W. B., 884, 885, 1085\nCross, S. E., 29, 1069\nCROSS-VALIDATION, 710\ncross-validation, 708, 737, 759, 767\nCROSS-VALIDATION-WRAPPER, 710\ncrossover, 128, 153\ncrossword puzzle, 44, 231\nCruse, D. A., 870, 1069\ncryptarithmetic, 206\nCsorba, M., 1012, 1071\nCuellar, J., 279, 1064\nCulberson, J., 107, 112, 1069, 1092\nculling, 128\nCullingford, R. E., 23, 1069\ncult of computationalism, 1020\nCummins, D., 638, 1069\ncumulative distribution, 564, 623, 1058\ncumulative learning, 791, 797\ncumulative probability density function,\n1058\ncuriosity, 842\nCurran, J. R., 920, 1068\ncurrent-best-hypothesis, 770, 798\nCURRENT-BEST-LEARNING, 771\nCurrie, K. W., 432, 1073\ncurse\nof dimensionality, 739, 760, 989, 997\noptimizer’s, 619, 637\nwinner’s, 637\nCushing, W., 432, 1069\ncutoff test, 171\ncutset\nconditioning, 225, 227, 554\ncutset, cycle, 225\ncutset conditioning, 225, 227, 554\nCybenko, G., 762, 1069\nCYBERLOVER, 1021\ncybernetics, 15, 15\nCYC, 439, 469, 470\ncyclic solution, 137\nCyganiak, R., 439, 469, 1066\nCYK-PARSE, 894\nCYK algorithm, 893, 919\nD\nD’Ambrosio, B., 553, 1088\nd-separation, 517\nDAG, 511, 552\nDaganzo, C., 554, 1069\nDagum, P., 554, 1069\nDahy, S. A., 723, 724, 1078\nDalal, N., 946, 968, 1069\nDALTON, 800\nDamerau, F., 884, 1064\nDaniels, C. J., 112, 1081\nDanish, 907\nDantzig, G. B., 155, 1069\nDARKTHOUGHT, 192\nDARPA, 29, 922\nDARPA Grand Challenge, 1007, 1014\nDartmouth workshop, 17, 18\nDarwiche, A., 277, 517, 554, 557, 558,\n1069, 1085\nDarwin, C., 130, 1035, 1069\nDasgupta, P., 157, 1069\ndata-driven, 258\ndata association, 599, 982\ndatabase, 299\ndatabase semantics, 300, 343, 367, 540\ndata complexity, 334",
  "Dartmouth workshop, 17, 18\nDarwiche, A., 277, 517, 554, 557, 558,\n1069, 1085\nDarwin, C., 130, 1035, 1069\nDasgupta, P., 157, 1069\ndata-driven, 258\ndata association, 599, 982\ndatabase, 299\ndatabase semantics, 300, 343, 367, 540\ndata complexity, 334\ndata compression, 866\nDatalog, 331, 357, 358\ndata matrix, 721\ndata mining, 26\ndata sparsity, 888\ndative case, 899\nDaun, B., 432, 1070\nDavidson, A., 678, 687, 1066\nDavidson, D., 470, 1069\nDavies, T. R., 784, 799, 1069\nDavis, E., 469–473, 1069, 1070\nDavis, G., 432, 1070\nDavis, K. H., 922, 1070\nDavis, M., 260, 276, 350, 358, 1070\nDavis, R., 800, 1070\nDavis–Putnam algorithm, 260\nDawid, A. P., 553, 639, 826, 1069,\n1080, 1089\nDayan, P., 763, 854, 855, 1070, 1083,\n1088\nda Vinci, L., 5, 966\nDBN, 566, 590, 590–599, 603, 604,\n646, 664\nDBPEDIA, 439, 469\nDCG, 898, 919\nDDN (dynamic decision network), 664,\n685\nDeacon, T. W., 25, 1070\ndead end, 149\nDeale, M., 432, 1070\nDean, J., 29, 921, 1067\nDean, M. E., 279, 1084\nDean, T., 431, 557, 604, 686, 1012,\n1013, 1048, 1070\nDearden, R., 686, 855, 1066, 1070\nDebevec, P., 968, 1070\nDebreu, G., 625, 1070\ndebugging, 308\nDechter, R., 110, 111, 228–230, 553,\n1069, 1070, 1076, 1085\ndecision\nrational, 481, 610, 633\nsequential, 629, 645\nDECISION-LIST-LEARNING, 717\nDECISION-TREE-LEARNING, 702\ndecision analysis, 633\ndecision boundary, 723\ndecision list, 715\ndecision maker, 633\ndecision network, 510, 610, 626,\n626–628, 636, 639, 664\ndynamic, 664, 685\nevaluation of, 628\ndecision node, 626\ndecision stump, 750\ndecision theory, 9, 26, 483, 636\ndecision tree, 638, 697, 698\nexpressiveness, 698\npruning, 705\ndeclarative, 286\ndeclarative bias, 787\ndeclarativism, 236, 275\ndecomposability (of lotteries), 613\nDECOMPOSE, 414\ndecomposition, 378\nDeCoste, D., 760, 762, 1070\nDedekind, R., 313, 1070\ndeduction, see logical inference\ndeduction theorem, 249\ndeductive database, 336, 357, 358\ndeductive learning, 694\ndeep belief networks, 1047\nDEEP BLUE, ix, 29, 185, 192\nDEEP FRITZ, 193\nDeep Space One, 60, 392, 432\nDEEP THOUGHT, 192\nDeerwester, S. C., 883, 1070 Index\n1103\ndefault logic, 459, 468, 471\ndefault reasoning, 458–460, 547\ndefault value, 456\nde Finetti’s theorem, 490\ndeﬁnite clause, 256, 330–331\ndeﬁnition (logical), 302\ndeformable template, 957\ndegree heuristic, 216, 228, 261\ndegree of belief, 482, 489\ninterval-valued, 547\ndegree of freedom, 975\ndegree of truth, 289\nDeGroot, M. H., 506, 827, 1070\nDeJong, G., 799, 884, 1070\ndelete list, 368\nDelgrande, J., 471, 1070\ndeliberative layer, 1005",
  "degree heuristic, 216, 228, 261\ndegree of belief, 482, 489\ninterval-valued, 547\ndegree of freedom, 975\ndegree of truth, 289\nDeGroot, M. H., 506, 827, 1070\nDeJong, G., 799, 884, 1070\ndelete list, 368\nDelgrande, J., 471, 1070\ndeliberative layer, 1005\nDellaert, F., 195, 1012, 1072, 1091\nDella Pietra, S. A., 922, 1067\nDella Pietra, V. J., 922, 1067\ndelta rule, 846\nDel Favero, B. A., 553, 1088\nDel Moral, P., 605, 1070\ndemodulation, 354, 359, 364\nDemopoulos, D., 278, 1068\nDe Morgan’s rules, 298\nDe Morgan, A., 227, 313\nDempster, A. P., 557, 604, 826, 1070\nDempster–Shafer theory, 547, 549,\n549–550, 557\nDENDRAL, 22, 23, 468\ndendrite, 11\nDeng, X., 157, 1070\nDenis, F., 921, 1070\nDenis, M., 28, 1068\nDenker, J., 762, 967, 1080\nDennett, D. C., 1024, 1032, 1033, 1042,\n1070\nDenney, E., 360, 1071\ndensity estimation, 806\nnonparametric, 814\nDeOliveira, J., 469, 1081\ndepth-ﬁrst search, 85, 85–87, 108, 408\nDEPTH-LIMITED-SEARCH, 88\ndepth limit, 173\ndepth of ﬁeld, 932\nderivational analogy, 799\nderived sentences, 242\nDescartes, R., 6, 966, 1027, 1041, 1071\ndescendant (in Bayesian networks), 517\nDescotte, Y., 432, 1071\ndescription logic, 454, 456, 456–458,\n468, 471\ndescriptive theory, 619\ndetachment, 547\ndetailed balance, 537\ndetection failure (in data association),\n602\ndetermination, 784, 799, 801\nminimal, 787\ndeterministic environment, 43\ndeterministic node, 518\nDetwarasiti, A., 639, 1071\nDeville, Y., 228, 1091\nDEVISER, 431\nDevroye, L., 827, 1071\nDewey Decimal system, 440\nde Bruin, A., 191, 1085\nde Dombal, F. T., 505, 1070\nde Finetti, B., 489, 504, 1070\nde Freitas, J. F. G., 605, 1070\nde Freitas, N., 605, 1071\nde Kleer, J., 229, 358, 472, 1070, 1072,\n1091\nde Marcken, C., 921, 1070\nDe Morgan, A., 1070\nDe Raedt, L., 800, 921, 1070, 1083\nde Salvo Braz, R., 556, 1070\nde Sarkar, S. C., 112, 157, 1068, 1069\nDiaconis, P., 620\ndiagnosis, 481, 496, 497, 909\ndental, 481\nmedical, 23, 505, 517, 548, 629, 1036\ndiagnostic rule, 317, 517\ndialysis, 616\ndiameter (of a graph), 88\nDias, W., 28, 1064\nDickmanns, E. D., 1014, 1071\ndictionary, 21\nDietterich, T., 799, 856, 1064, 1071\nDifference Engine, 14\ndifferential drive, 976\ndifferential equation, 997\nstochastic, 567\ndifferential GPS, 975\ndifferentiation, 780\ndiffuse albedo, 934\ndiffuse reﬂection, 933\nDigital Equipment Corporation (DEC),\n24, 336\ndigit recognition, 753–755\nDijkstra, E. W., 110, 1021, 1071\nDill, D. L., 279, 1084\nDillenburg, J. F., 111, 1071\nDimopoulos, Y., 395, 1078\nDinh, H., 111, 1071\nDiophantine equations, 227\nDiophantus, 227",
  "Digital Equipment Corporation (DEC),\n24, 336\ndigit recognition, 753–755\nDijkstra, E. W., 110, 1021, 1071\nDill, D. L., 279, 1084\nDillenburg, J. F., 111, 1071\nDimopoulos, Y., 395, 1078\nDinh, H., 111, 1071\nDiophantine equations, 227\nDiophantus, 227\nDiorio, C., 604, 1086\nDiPasquo, D., 885, 1069\nDiplomacy, 166\ndirected acyclic graph (DAG), 511, 552\ndirected arc consistency, 223\ndirect utility estimation, 853\nDirichlet distribution, 811\nDirichlet process, 827\ndisabilities, 1043\ndisambiguation, 904–912, 919\ndiscontinuities, 936\ndiscount factor, 649, 685, 833\ndiscovery system, 800\ndiscrete event, 447\ndiscretization, 131, 519\ndiscriminative model, 878\ndisjoint sets, 441\ndisjunct, 244\ndisjunction, 244\ndisjunctive constraint, 205\ndisjunctive normal form, 283\ndisparity, 949\nDissanayake, G., 1012, 1071\ndistant point light source, 934\ndistortion, 910\ndistribute ∨over ∧, 254, 347\ndistributed constraint satisfaction, 230\ndistribution\nbeta, 592, 811\nconditional, nonparametric, 520\ncumulative, 564, 623, 1058\nmixture, 817\ndivide-and-conquer, 606\nDix, J., 472, 1067\nDizdarevic, S., 158, 1080\nDLV, 472\nDNF (disjunctive normal form), 283\nDo, M. B., 390, 431, 1071\nDoctorow, C., 470, 1071\nDOF, 975\ndolphin, 860\ndomain, 486\ncontinuous, 206\nelement of, 290\nﬁnite, 205, 344\ninﬁnite, 205\nin ﬁrst-order logic, 290\nin knowledge representation, 300\ndomain closure, 299, 540\ndominance\nstochastic, 622, 636\nstrict, 622\ndominant strategy, 668, 680\ndominant strategy equilibrium, 668\ndominated plan (in POMDP), 662\ndomination (of heuristics), 104\nDomingos, P., 505, 556, 826, 1071\nDomshlak, C., 395, 434, 1067, 1076\nDonati, A., 28, 1068\nDonninger, C., 193, 1071 1104\nIndex\nDoorenbos, R., 358, 1071\nDoran, J., 110, 111, 1071\nDorf, R. C., 60, 1071\nDoucet, A., 605, 1070, 1071\nDow, R. J. F., 762, 1088\nDowling, W. F., 277, 1071\nDowney, D., 885, 1072\ndownward reﬁnement property, 410\nDowty, D., 920, 1071\nDoyle, J., 60, 229, 471, 472, 638, 1071,\n1082, 1092\nDPLL, 261, 277, 494\nDPLL-SATISFIABLE?, 261\nDrabble, B., 432, 1071\nDRAGON, 922\nDraper, D., 433, 1072\nDrebbel, C., 15\nDredze, M., 761, 1071\nDreussi, J., 432, 1088\nDreyfus, H. L., 279, 1024, 1049, 1071\nDreyfus, S. E., 109, 110, 685, 1024,\n1065, 1071\nDriessens, K., 857, 1090\ndrilling rights, 629\ndrone, 1009\ndropping conditions, 772\nDrucker, H., 762, 967, 1080\nDruzdzel, M. J., 554, 1068\nDT-AGENT, 484\ndual graph, 206\ndualism, 6, 1027, 1041\nDubois, D., 557, 1071\nDubois, O., 277, 1089\nduck, mechanical, 1011\nDuda, R. O., 505, 557, 763, 825, 827,\n1071",
  "dropping conditions, 772\nDrucker, H., 762, 967, 1080\nDruzdzel, M. J., 554, 1068\nDT-AGENT, 484\ndual graph, 206\ndualism, 6, 1027, 1041\nDubois, D., 557, 1071\nDubois, O., 277, 1089\nduck, mechanical, 1011\nDuda, R. O., 505, 557, 763, 825, 827,\n1071\nDudek, G., 1014, 1071\nDuffy, D., 360, 1071\nDuffy, K., 760, 1069\nDumais, S. T., 29, 872, 883, 885, 1065,\n1070, 1087\ndung beetle, 39, 61, 424, 1004\nDunham, B., 21, 1072\nDunham, C., 358, 1090\nDunn, H. L., 556, 1071\nDuPont, 24\nduration, 402\nD¨urer, A., 966\nDurfee, E. H., 434, 1071\nDurme, B. V., 885, 1071\nDurrant-Whyte, H., 1012, 1071, 1080\nDyer, M., 23, 1071\ndynamical systems, 603\ndynamic backtracking, 229\ndynamic Bayesian network (DBN), 566,\n590, 590–599, 603, 604, 646,\n664\ndynamic decision network, 664, 685\ndynamic environment, 44\ndynamic programming, 60, 106, 110,\n111, 342, 575, 685\nadaptive, 834, 834–835, 853, 858\nnonserial, 553\ndynamic state, 975\ndynamic weighting, 111\nDyson, G., 1042, 1071\ndystopia, 1052\nDuzeroski, S., 796, 800, 1071, 1078,\n1080\nE\nE, 359\nE0 (English fragment), 890\nEarley, J., 920, 1071\nearly stopping, 706\nearthquake, 511\nEastlake, D. E., 192, 1074\nEBL, 432, 778, 780–784, 798, 799\nEcker, K., 432, 1066\nEckert, J., 14\neconomics, 9–10, 59, 616\nEdelkamp, S., 111, 112, 395, 1071,\n1079\nedge (in an image), 936\nedge detection, 936–939\nEdinburgh, 800, 1012\nEdmonds, D., 16\nEdmonds, J., 8, 1071\nEdwards, D. J., 191, 1075\nEdwards, P., 1042, 1071\nEdwards, W., 637, 1091\nEEG, 11\nEen, N., 277, 1071\neffect, 367\nmissing, 423\nnegative, 398\neffector, 971\nefﬁcient auction, 680\nEfros, A. A., 28, 955, 968, 1075, 1076\nEhrenfeucht, A., 759, 1066\n8-puzzle, 70, 102, 105, 109, 113\n8-queens problem, 71, 109\nEinstein, A., 1\nEisner, J., 920, 1089\nEitelman, S., 358, 1090\nEiter, T., 472, 1071\nEkart, A., 155, 1086\nelectric motor, 977\nelectronic circuits domain, 309–312\nElfes, A., 1012, 1083\nELIMINATION-ASK, 528\nElio, R., 638, 1071\nElisseeff, A., 759, 1074\nELIZA, 1021, 1035\nElkan, C., 551, 826, 1071\nEllington, C., 1045, 1072\nElliot, G. L., 228, 1075\nElliott, P., 278, 1092\nEllsberg, D., 638, 1071\nEllsberg paradox, 620\nElman, J., 921, 1071\nEM algorithm, 571, 816–824\nstructural, 824\nembodied cognition, 1026\nemergent behavior, 430, 1002\nEMNLP, 923\nempirical gradient, 132, 849\nempirical loss, 712\nempiricism, 6, 923\nEmpson, W., 921, 1071\nEMV (expected monetary value), 616\nEnderton, H. B., 314, 358, 1071\nEnglish, 21, 32\nfragment, 890\nENIAC, 14\nensemble learning, 748, 748–752\nentailment, 240, 274\ninverse, 795",
  "empirical loss, 712\nempiricism, 6, 923\nEmpson, W., 921, 1071\nEMV (expected monetary value), 616\nEnderton, H. B., 314, 358, 1071\nEnglish, 21, 32\nfragment, 890\nENIAC, 14\nensemble learning, 748, 748–752\nentailment, 240, 274\ninverse, 795\nentailment constraint, 777, 789, 798\nentropy, 703\nENUMERATE-ALL, 525\nENUMERATION-ASK, 525\nenvironment, 34, 40–46\nartiﬁcial, 41\nclass, 45\ncompetitive, 43\ncontinuous, 44\ncooperative, 43\ndeterministic, 43\ndiscrete, 44\ndynamic, 44\ngame-playing, 197, 858\ngenerator, 46\nhistory, 646\nknown, 44\nmultiagent, 42, 425\nnondeterministic, 43\nobservable, 42\none-shot, 43\npartially observable, 42\nproperties, 42\nsemidynamic, 44\nsequential, 43\nsingle-agent, 42\nstatic, 44\nstochastic, 43\ntaxi, 40\nuncertain, 43 Index\n1105\nunknown, 44\nunobservable, 42\nEPAM (Elementary Perceiver And\nMemorizer), 758\nEphrati, E., 434, 1079\nepiphenomenalism, 1030\nepisodic environment, 43\nepistemological commitment, 289, 290,\n313, 482\nEpstein, R., 30, 1071\nEQP, 360\nequality, 353\nequality (in logic), 299\nequality symbol, 299\nequilibrium, 183, 668\nequivalence (logical), 249\nErdmann, M. A., 156, 1071\nergodic, 537\nErnst, G., 110, 1084\nErnst, H. A., 1012, 1071\nErnst, M., 395, 1071\nErol, K., 432, 1071, 1072\nerror (of a hypothesis), 708, 714\nerror function, 1058\nerror rate, 708\nEssig, A., 505, 1074\nEtchemendy, J., 280, 314, 1065\nethics, 1034–1040\nEtzioni, A., 1036, 1072\nEtzioni, O., 61, 433, 439, 469, 881, 885,\n1036, 1050, 1065, 1072, 1079,\n1091\nEuclid, 8, 966\nEURISKO, 800\nEurope, 24\nEuropean Space Agency, 432\nevaluation function, 92, 108, 162,\n171–173, 845\nlinear, 107\nEvans, T. G., 19, 31, 1072\nevent, 446–447, 450\natomic, 506\ndiscrete, 447\nexogenous, 423\nin probability, 484, 522\nliquid, 447\nevent calculus, 446, 447, 470, 903\nEverett, B., 1012, 1066\nevidence, 485, 802\nreversal, 605\nevidence variable, 522\nevolution, 130\nmachine, 21\nevolutionary psychology, 621\nevolution strategies, 155\nexceptions, 438, 456\nexclusive or, 246, 766\nexecution, 66\nexecution monitoring, 422, 422–434\nexecutive layer, 1004\nexhaustive decomposition, 441\nexistence uncertainty, 541\nexistential graph, 454\nExistential Instantiation, 323\nExistential Introduction, 360\nexpansion (of states), 75\nexpectation, 1058\nexpected monetary value, 616\nexpected utility, 53, 61, 483, 610, 611,\n616\nexpected value (in a game tree), 172,\n178\nexpectiminimax, 178, 191\ncomplexity of, 179\nexpert system, 468, 633, 636, 800, 1036\ncommercial, 336\ndecision-theoretic, 633–636\nﬁrst, 23\nﬁrst commercial, 24\nHPP (Heuristic Programming",
  "616\nexpected value (in a game tree), 172,\n178\nexpectiminimax, 178, 191\ncomplexity of, 179\nexpert system, 468, 633, 636, 800, 1036\ncommercial, 336\ndecision-theoretic, 633–636\nﬁrst, 23\nﬁrst commercial, 24\nHPP (Heuristic Programming\nProject), 23\nlogical, 546\nmedical, 557\nProlog-based, 339\nwith uncertainty, 26\nexplaining away, 548\nexplanation, 462, 781\nexplanation-based generalization, 187\nexplanation-based learning (EBL), 432,\n778, 780–784, 798, 799\nexplanatory gap, 1033\nexploitation, 839\nexploration, 39, 147–154, 831, 839, 855\nsafe, 149\nexploration function, 842, 844\nexplored set, 77\nexpressiveness (of a representation\nscheme), 58\nEXTEND-EXAMPLE, 793\nextended Kalman ﬁlter (EKF), 589, 982\nextension (of a concept), 769\nextension (of default theory), 460\nextensive form, 674\nexternalities, 683\nextrinsic property, 445\neyes, 928, 932, 966\nF\nfact, 256\nfactor (in variable elimination), 524\nfactored frontier, 605\nfactored representation, 58, 64, 202,\n367, 486, 664, 694\nfactoring, 253, 347\nFagin, R., 229, 470, 477, 1065, 1072\nFahlman, S. E., 20, 472, 1072\nfailure model, 593\nfalse alarm (in data association), 602\nfalse negative, 770\nfalse positive, 770\nfamily tree, 788\nFarrell, R., 358, 1067\nFAST DIAGONALLY DOWNWARD, 387\nFASTDOWNWARD, 395\nFASTFORWARD, 379\nFASTUS, 874, 875, 884\nFaugeras, O., 968, 1072\nFearing, R. S., 1013, 1072\nFeatherstone, R., 1013, 1072\nfeature (in speech), 915\nfeature (of a state), 107, 172\nfeature extraction, 929\nfeature selection, 713, 866\nfeed-forward network, 729\nfeedback loop, 548\nFeigenbaum, E. A., 22, 23, 468, 758,\n1067, 1072, 1080\nFeiten, W., 1012, 1066\nFeldman, J., 639, 1072\nFeldman, R., 799, 1089\nFellbaum, C., 921, 1072\nFellegi, I., 556, 1072\nFelner, A., 107, 112, 395, 1072, 1079,\n1092\nFelzenszwalb, P., 156, 959, 1072\nFeng, C., 800, 1083\nFeng, L., 1012, 1066\nFergus, R., 741, 1090\nFerguson, T., 192, 827, 1072\nFermat, P., 9, 504\nFerraris, P., 433, 1072\nFerriss, T., 1035, 1072\nFF, 379, 387, 392, 395\n15-puzzle, 109\nFifth Generation project, 24\nﬁgure of speech, 905, 906\nFikes, R. E., 60, 156, 314, 367, 393,\n432, 434, 471, 799, 1012, 1067,\n1072\nﬁltering, 145, 460, 571–573, 603, 659,\n823, 856, 978, 1045\nassumed-density, 605\nFine, S., 604, 1072\nﬁnite-domain, 205, 344\nﬁnite-state automata, 874, 889\nFinkelstein, L., 230, 1067\nFinney, D. J., 554, 1072 1106\nIndex\nFirby, R. J., 431, 1070\nﬁrst-order logic, 285, 285–321\nﬁrst-order probabilistic logic, 539–546\nFirth, J., 923, 1072\nFischer, B., 360, 1071\nFischetti, M., 395, 1068\nFisher, R. A., 504, 1072",
  "Finney, D. J., 554, 1072 1106\nIndex\nFirby, R. J., 431, 1070\nﬁrst-order logic, 285, 285–321\nﬁrst-order probabilistic logic, 539–546\nFirth, J., 923, 1072\nFischer, B., 360, 1071\nFischetti, M., 395, 1068\nFisher, R. A., 504, 1072\nﬁtness (in genetic algorithms), 127\nﬁtness landscape, 155\nFix, E., 760, 1072\nﬁxation, 950\nFIXED-LAG-SMOOTHING, 580\nﬁxed-lag smoothing, 576\nﬁxed point, 258, 331\nFlannery, B. P., 155, 1086\nﬂaw, 390\nFloreano, D., 1045, 1072\nﬂuent, 266, 275, 388, 449–450\nﬂy eyes, 948, 963\nFMP, see planning, ﬁne-motion\nfMRI, 11, 288\nfocal plane, 932\nFOCUS, 799\nfocus of expansion, 948\nFogel, D. B., 156, 1072\nFogel, L. J., 156, 1072\nFOIL, 793\nFOL-BC-ASK, 338\nFOL-FC-ASK, 332\nfolk psychology, 473\nFoo, N., 279, 1072\nFOPC, see logic, ﬁrst-order\nForbes, J., 855, 1072\nFORBIN, 431, 432\nForbus, K. D., 358, 472, 1072\nforce sensor, 975\nFord, K. M., 30, 1072\nforeshortening, 952\nForestier, J.-P., 856, 1072\nForgy, C., 358, 1072\nformulate, search, execute, 66\nForrest, S., 155, 1082\nForsyth, D., 960, 968, 1072, 1086\nFortmann, T. E., 604, 606, 1065\nforward–backward, 575, 822\nFORWARD-BACKWARD, 576\nforward chaining, 257, 257–259, 275,\n277, 330–337, 358\nforward checking, 217, 217–218\nforward pruning, 174\nforward search for planning, 373–374\nfour-color map problem, 227, 1023\nFourier, J., 227, 1072\nFowlkes, C., 941, 967, 1081\nFox, C., 638, 1072\nFox, D., 606, 1012, 1014, 1067, 1072,\n1088, 1090\nFox, M. S., 395, 432, 1072\nframe\nin representation, 24, 471\nin speech, 915\nproblem\ninferential, 267, 279\nframe problem, 266, 279\ninferential, 447\nrepresentational, 267\nframing effect, 621\nFranco, J., 277, 1072\nFrank, E., 763, 1092\nFrank, I., 191, 1072\nFrank, M., 231, 1073\nFrank, R. H., 1035, 1072\nFrankenstein, 1037\nFranz, A., 883, 921, 1072\nFratini, S., 28, 1068\nFREDDY, 74, 156, 1012\nFredkin Prize, 192\nFreeman, W., 555, 1091, 1092\nfree space, 988\nfree will, 6\nFrege, G., 8, 276, 313, 357, 1072\nFreitag, D., 877, 885, 1069, 1072\nfrequentism, 491\nFreuder, E. C., 228–230, 1072, 1087\nFreund, Y., 760, 1072\nFriedberg, R. M., 21, 156, 1072\nFriedgut, E., 278, 1073\nFriedman, G. J., 155, 1073\nFriedman, J., 758, 761, 763, 827, 1067,\n1073, 1075\nFriedman, N., 553, 558, 605, 826, 827,\n855, 1066, 1070, 1073, 1078\nFriendly AI, 27, 1039\nFristedt, B., 855, 1065\nfrontier, 75\nFrost, D., 230, 1070\nFruhwirth, T., 230, 1073\nFRUMP, 884\nFuchs, J. J., 432, 1073\nFudenberg, D., 688, 1073\nFukunaga, A. S., 431, 1073\nfully observable, 658\nfunction, 288\ntotal, 291\nfunctional dependency, 784, 799",
  "Fristedt, B., 855, 1065\nfrontier, 75\nFrost, D., 230, 1070\nFruhwirth, T., 230, 1073\nFRUMP, 884\nFuchs, J. J., 432, 1073\nFudenberg, D., 688, 1073\nFukunaga, A. S., 431, 1073\nfully observable, 658\nfunction, 288\ntotal, 291\nfunctional dependency, 784, 799\nfunctionalism, 60, 1029, 1030, 1041,\n1042\nfunction approximation, 845, 847\nfunction symbol, 292, 294\nFung, R., 554, 1073\nFurnas, G. W., 883, 1070\nFurst, M., 395, 1066\nfutility pruning, 185\nfuzzy control, 550\nfuzzy logic, 240, 289, 547, 550, 557\nfuzzy set, 550, 557\nG\ng (path cost), 78\nG-set, 774\nG¨odel number, 352\nGabor, Z. Z., 640\nGaddum, J. H., 554, 1073\nGaifman, H., 555, 1073\ngain parameter, 998\ngain ratio, 707, 765\ngait, 1001\nGale, W. A., 883, 1068\nGalileo, G., 1, 56, 796\nGallaire, H., 358, 1073\nGallier, J. H., 277, 314, 1071, 1073\nGamba, A., 761, 1073\nGamba perceptrons, 761\nGamberini, L., 761, 1073\ngambling, 9, 613\ngame, 9, 161\nof chance, 177–180\ndice, 183\nGo, 186, 194\nof imperfect information, 162\ninspection game, 666\nmultiplayer, 165–167\nOthello, 186\npartially observable, 180–184\nof perfect information, 161\npoker, 507\npursuit–evasion, 196\nrepeated, 669, 673\nrobot (with humans), 1019\nScrabble, 187, 195\nzero-sum, 161, 162, 199, 670\ngame playing, 161–162, 190\ngame programs, 185–187\nGAMER, 387\ngame show, 616\ngame theory, 9, 161, 645, 666, 666–678,\n685\ncombinatorial, 186\ngame tree, 162\nGamma function, 828\nGarding, J., 968, 1073\nGardner, M., 276, 1073\nGarey, M. R., 1059, 1073\nGarg, A., 604, 1084\nGARI, 432\nGarofalakis, M., 275, 1080 Index\n1107\nGarrett, C., 128, 1065\nGaschnig’s heuristic, 119\nGaschnig, J., 111, 119, 228, 229, 557,\n1071, 1073\nGasquet, A., 432, 1073\nGasser, R., 112, 194, 1073\nGat, E., 1013, 1073\ngate (logic), 309\nGauss, C. F., 227, 603, 759, 1073\nGauss, K. F., 109\nGaussian distribution, 1058\nmultivariate, 584, 1058\nGaussian error model, 592\nGaussian ﬁlter, 938\nGaussian process, 827\nGawande, A., 1036, 1073\nGawron, J. M., 922, 1078\nGay, D. E., 275, 1080\nGearhart, C., 686, 1074\nGee, A. H., 605, 1070\nGeffner, H., 156, 394, 395, 431, 433,\n1066, 1075, 1084\nGeiger, D., 553, 826, 1073, 1075\nGeisel, T., 864, 1073\nGelatt, C. D., 155, 229, 1078\nGelb, A., 604, 1073\nGelder, A. V., 360, 1090\nGelernter, H., 18, 359, 1073\nGelfond, M., 359, 472, 1073\nGelly, S., 194, 1073, 1091\nGelman, A., 827, 1073\nGeman, D., 554, 967, 1073\nGeman, S., 554, 967, 1073\ngenerality, 783\ngeneralization, 770, 772\ngeneralization hierarchy, 776\ngeneralization loss, 711\ngeneralized arc consistent, 210\ngeneralized cylinder, 967",
  "Gelman, A., 827, 1073\nGeman, D., 554, 967, 1073\nGeman, S., 554, 967, 1073\ngenerality, 783\ngeneralization, 770, 772\ngeneralization hierarchy, 776\ngeneralization loss, 711\ngeneralized arc consistent, 210\ngeneralized cylinder, 967\ngeneral ontology, 453\nGeneral Problem Solver, 3, 7, 18, 393\ngeneration (of states), 75\ngenerative capacity, 889\ngenerator, 337\nGenesereth, M. R., 59, 60, 156, 195,\n314, 345, 350, 359, 363, 1019,\n1073, 1080, 1089\nGENETIC-ALGORITHM, 129\ngenetic algorithm, 21, 126–129, 153,\n155–156, 841\ngenetic programming, 155\nGent, I., 230, 1073\nGentner, D., 314, 799, 1073\nGeometry Theorem Prover, 18\nGeorgeson, M., 968, 1067\nGerbault, F., 826, 1074\nGerevini, A., 394, 395, 1073\nGershwin, G., 917, 1073\nGestalt school, 966\nGetoor, L., 556, 1073\nGhahramani, Z., 554, 605, 606, 827,\n1073, 1077, 1087\nGhallab, M., 372, 386, 394–396, 431,\n1073\nGhose, S., 112, 1068\nGIB, 187, 195\nGibbs, R. W., 921, 1073\nGIBBS-ASK, 537\nGibbs sampling, 536, 538, 554\nGibson, J. J., 967, 968, 1073\nGil, Y., 439, 1068\nGilks, W. R., 554, 555, 826, 1073\nGilmore, P. C., 358, 1073\nGinsberg, M. L., 187, 195, 229, 231,\n359, 363, 557, 1069, 1073, 1089\nGionis, A., 760, 1073\nGittins, J. C., 841, 855, 1074\nGittins index, 841, 855\nGiunchiglia, E., 433, 1072\nGivan, R., 857, 1090\nGlanc, A., 1011, 1074\nGlass, J., 604, 1080\nGLAUBER, 800\nGlavieux, A., 555, 1065\nGLIE, 840\nglobal constraint, 206, 211\nGlobal Positioning System (GPS), 974\nGlover, F., 154, 1074\nGlymour, C., 314, 826, 1074, 1089\nGo (game), 186, 194\ngoal, 52, 64, 65, 108, 369\nbased agent, 52–53, 59, 60\nformulation of, 65\ngoal-based agent, 52–53, 59\ngoal-directed reasoning, 259\ninferential, 301\nserializable, 392\ngoal clauses, 256\ngoal monitoring, 423\ngoal predicate, 698\ngoal test, 67, 108\nGod, existence of, 504\nG¨odel, K., 8, 276, 358, 1022, 1074\nGoebel, J., 826, 1074\nGoebel, R., 2, 59, 1085\nGoel, A., 682, 1064\nGoertzel, B., 27, 1074\nGOFAI, 1024, 1041\ngold, 237\nGold, B., 922, 1074\nGold, E. M., 759, 921, 1074\nGoldbach’s conjecture, 800\nGoldberg, A. V., 111, 1074\nGoldberg, D. E., 155, 1085\nGoldberg, K., 156, 1092\nGoldin-Meadow, S., 314, 1073\nGoldman, R., 156, 433, 555, 556, 921,\n1068, 1074, 1091\ngold standard, 634\nGoldszmidt, M., 553, 557, 686, 826,\n1066, 1073, 1074\nGOLEM, 800\nGolgi, C., 10\nGolomb, S., 228, 1074\nGolub, G., 759, 1074\nGomard, C. K., 799, 1077\nGomes, C., 154, 229, 277, 1074\nGonthier, G., 227, 1074\nGood, I. J., 491, 552, 1037, 1042, 1074\nGood–Turing smoothing, 883\ngood and evil, 637\nGooday, J. M., 473, 1069\nGoodman, D., 29, 1074",
  "Golub, G., 759, 1074\nGomard, C. K., 799, 1077\nGomes, C., 154, 229, 277, 1074\nGonthier, G., 227, 1074\nGood, I. J., 491, 552, 1037, 1042, 1074\nGood–Turing smoothing, 883\ngood and evil, 637\nGooday, J. M., 473, 1069\nGoodman, D., 29, 1074\nGoodman, J., 29, 883, 1068, 1074\nGoodman, N., 470, 798, 1074, 1080\nGoodnow, J. J., 798, 1067\ngood old-fashioned AI (GOFAI), 1024,\n1041\nGoogle, 870, 883, 889, 922\nGoogle Translate, 907\nGopnik, A., 314, 1074\nGordon, D. M., 429, 1074\nGordon, G., 605, 686, 1013, 1085, 1087,\n1091\nGordon, M. J., 314, 1074\nGordon, N., 187, 195, 605, 1071, 1074\nGorry, G. A., 505, 1074\nGottlob, G., 230, 1074\nGotts, N., 473, 1069\nGP-CSP, 390\nGPS (General Problem Solver), 3, 7, 18,\n393\nGPS (Global Positioning System), 974\ngraceful degradation, 666\ngradient, 131\nempirical, 132, 849\ngradient descent, 125, 719\nbatch, 720\nstochastic, 720\nGraham, S. L., 920, 1074\nGrama, A., 112, 1074\ngrammar, 860, 890, 1060\nattribute, 919\naugmented, 897\ncategorial, 920\ncontext-free, 889, 918, 919, 1060\nlexicalized, 897\nprobabilistic, 890, 888–897, 919 1108\nIndex\ncontext-sensitive, 889\ndeﬁnite clause (DCG), 898, 919\ndependency, 920\nEnglish, 890–892\ninduction of, 921\nlexical-functional (LFG), 920\nphrase structure, 918\nprobabilistic, 897\nrecursively enumerable, 889\nregular, 889\ngrammatical formalism, 889\nGrand Prix, 185\ngraph, 67\ncoloring, 227\nEulerian, 157\nGRAPH-SEARCH, 77\ngraphical model, 510, 558\nGRAPHPLAN, 379, 383, 392, 394–396,\n402, 433\ngrasping, 1013\nGrassmann, H., 313, 1074\nGravano, L., 885, 1064\nGrayson, C. J., 617, 1074\nGreece, 275, 468, 470\ngreedy search, 92\nGreen, B., 920, 1074\nGreen, C., 19, 314, 356, 358, 1074\nGreen, P., 968, 1067\nGreenbaum, S., 920, 1086\nGreenblatt, R. D., 192, 1074\nGreenspan, M., 195, 1079\nGrefenstette, G., 27, 1078\nGreiner, R., 799, 826, 1068, 1074\nGrenager, T., 857, 1088\ngrid, rectangular, 77\nGrifﬁths, T., 314, 1090\nGrinstead, C., 506, 1074\nGRL, 1013\nGrosof, B., 799, 1087\nGrosz, B. J., 682, 688, 1076\ngrounding, 243\nground resolution theorem, 255, 350\nground term, 295, 323\nGrove, A., 505, 638, 1064, 1065\nGrove, W., 1022, 1074\nGruber, T., 439, 470, 1074\ngrue, 798\nGrumberg, O., 395, 1068\nGSAT, 277\nGu, J., 229, 277, 1074, 1089\nGuard, J., 360, 1074\nGuestrin, C., 639, 686, 856, 857, 1074,\n1079, 1081\nGuha, R. V., 439, 469, 1067, 1080\nGuibas, L. J., 1013, 1074\nGumperz, J., 314, 1074\nGupta, A., 639, 1079\nGUS, 884\nGutfreund, H., 761, 1064\nGuthrie, F., 227\nGuugu Yimithirr, 287\nGuy, R. K., 113, 1065\nGuyon, I., 759, 760, 762, 967, 1066,\n1074, 1080\nH",
  "Guha, R. V., 439, 469, 1067, 1080\nGuibas, L. J., 1013, 1074\nGumperz, J., 314, 1074\nGupta, A., 639, 1079\nGUS, 884\nGutfreund, H., 761, 1064\nGuthrie, F., 227\nGuugu Yimithirr, 287\nGuy, R. K., 113, 1065\nGuyon, I., 759, 760, 762, 967, 1066,\n1074, 1080\nH\nH (entropy), 704\nh (heuristic function), 92\nhMAP (MAP hypothesis), 804\nhML (ML hypothesis), 805\nHACKER, 394\nHacking, I., 506, 1074\nHaghighi, A., 896, 920, 1074\nHahn, M., 760, 1069\nH¨ahnel, D., 1012, 1067\nHaimes, M., 556, 1082\nHaken, W., 227, 1064\nHAL 9000 computer, 552\nHald, A., 506, 1074\nHalevy, A., 28, 358, 470, 759, 885,\n1067, 1074\nHalgren, E., 288, 1087\nHalpern, J. Y., 314, 470, 477, 505, 555,\n1065, 1072, 1074\nHalpin, M. P., 231, 1073\nhalting problem, 325\nham, 865\nHamm, F., 470, 1091\nHamming, R. W., 506, 1074\nHamming distance, 738\nHammond, K., 432, 1074\nHamori, S., 604, 1066\nham sandwich, 906\nHamscher, W., 60, 1074\nHan, X., 11, 1074\nHanan, S., 395, 1072\nHand, D., 763, 1074\nhand–eye machine, 1012\nHandschin, J. E., 605, 1075\nhandwritten digit recognition, 753–755\nHanks, S., 433, 1072\nHanna, F. K., 800, 1087\nHansard, 911\nHansen, E., 112, 156, 422, 433, 686,\n1075, 1093\nHansen, M. O., 228, 1064\nHansen, P., 277, 1075\nHanski, I., 61, 1075\nHansson, O., 112, 119, 1075\nhappy graph, 703\nhaptics, 1013\nHarabagiu, S. M., 885, 1085\nHarada, D., 856, 1084\nHaralick, R. M., 228, 1075\nHardin, G., 688, 1075\nHardy, G. H., 1035, 1075\nHarel, D., 358, 1068\nHarman, G. H., 1041, 1075\nHARPY, 154, 922\nHarris, Z., 883, 1075\nHarrison, J. R., 637, 1075\nHarrison, M. A., 920, 1074\nHarsanyi, J., 687, 1075\nHarshman, R. A., 883, 1070\nHart, P. E., 110, 156, 191, 432, 434,\n505, 557, 763, 799, 825, 827,\n1071, 1072, 1075\nHart, T. P., 191, 1075\nHartley, H., 826, 1075\nHartley, R., 968, 1075\nHarvard, 621\nHaslum, P., 394, 395, 431, 1075\nHastie, T., 760, 761, 763, 827, 1073,\n1075\nHaugeland, J., 2, 30, 1024, 1042, 1075\nHauk, T., 191, 1075\nHaussler, D., 604, 759, 762, 800, 1065,\n1066, 1075, 1079\nHavelund, K., 356, 1075\nHavenstein, H., 28, 1075\nHawkins, J., 1047, 1075\nHayes, P. J., 30, 279, 469–472, 1072,\n1075, 1082\nHaykin, S., 763, 1075\nHays, J., 28, 1075\nhead, 897\nhead (of Horn clause), 256\nHearst, M. A., 879, 881, 883, 884, 922,\n1075, 1084, 1087\nHeath, M., 759, 1074\nHeath Robinson, 14\nheavy-tailed distribution, 154\nHeawood, P., 1023\nHebb, D. O., 16, 20, 854, 1075\nHebbian learning, 16\nHebert, M., 955, 968, 1076\nHeckerman, D., 26, 29, 548, 552, 553,\n557, 605, 634, 640, 826, 1067,\n1074–1076, 1087–1089\nhedonic calculus, 637\nHeidegger, M., 1041, 1075",
  "Heawood, P., 1023\nHebb, D. O., 16, 20, 854, 1075\nHebbian learning, 16\nHebert, M., 955, 968, 1076\nHeckerman, D., 26, 29, 548, 552, 553,\n557, 605, 634, 640, 826, 1067,\n1074–1076, 1087–1089\nhedonic calculus, 637\nHeidegger, M., 1041, 1075\nHeinz, E. A., 192, 1075\nHeld, M., 112, 1075\nHellerstein, J. M., 275, 1080\nHelmert, M., 111, 395, 396, 1075\nHelmholtz, H., 12\nHempel, C., 6\nHenderson, T. C., 210, 228, 1082 Index\n1109\nHendler, J., 27, 396, 432, 469, 1064,\n1065, 1071, 1072, 1075, 1089\nHenrion, M., 61, 519, 552, 554, 639,\n1075, 1076, 1086\nHenzinger, M., 884, 1088\nHenzinger, T. A., 60, 1075\nHephaistos, 1011\nHerbrand’s theorem, 351, 358\nHerbrand, J., 276, 324, 351, 357, 358,\n1075\nHerbrand base, 351\nHerbrand universe, 351, 358\nHernadvolgyi, I., 112, 1076\nHerskovits, E., 826, 1069\nHessian, 132\nHeule, M., 278, 1066\nheuristic, 108\nadmissible, 94, 376\ncomposite, 106\ndegree, 216, 228, 261\nfor planning, 376–379\nfunction, 92, 102–107\nleast-constraining-value, 217\nlevel sum, 382\nManhattan, 103\nmax-level, 382\nmin-conﬂicts, 220\nminimum-remaining-values, 216,\n228, 333, 405\nminimum remaining values, 216, 228,\n333, 405\nnull move, 185\nsearch, 81, 110\nset-level, 382\nstraight-line, 92\nheuristic path algorithm, 118\nHeuristic Programming Project (HPP),\n23\nHewitt, C., 358, 1075\nhexapod robot, 1001\nhidden Markov model\nfactorial, 605\nhidden Markov model (HMM), 25, 566,\n578, 578–583, 590, 603, 604,\n822–823\nhidden Markov model (HMM) (HMM),\n578, 590, 876, 922\nhidden unit, 729\nhidden variable, 522, 816\nHIERARCHICAL-SEARCH, 409\nhierarchical decomposition, 406\nhierarchical lookahead, 415\nhierarchical reinforcement learning,\n856, 1046\nhierarchical structure, 1046\nhierarchical task network (HTN), 406,\n431\nHierholzer, C., 157, 1075\nhigher-order logic, 289\nhigh level action, 406\nHilgard, E. R., 854, 1075\nHILL-CLIMBING, 122\nhill climbing, 122, 153, 158\nﬁrst-choice, 124\nrandom-restart, 124\nstochastic, 124\nHingorani, S. L., 606, 1069\nHinrichs, T., 195, 1080\nHintikka, J., 470, 1075\nHinton, G. E., 155, 761, 763, 1047,\n1075, 1087\nHirsch, E. A., 277, 1064\nHirsh, H., 799, 1075\nHitachi, 408\nhit list, 869\nHITS, 871, 872\nHMM, 578, 590, 876, 922\nHo, Y.-C., 22, 761, 1067\nHoane, A. J., 192, 1067\nHobbes, T., 5, 6\nHobbs, J. R., 473, 884, 921, 1075, 1076\nHodges, J. L., 760, 1072\nHoff, M. E., 20, 833, 854, 1092\nHoffmann, J., 378, 379, 395, 433, 1076,\n1078\nHogan, N., 1013, 1076\nHOG feature, 947\nHoiem, D., 955, 968, 1076\nholdout cross-validation, 708\nholistic context, 1024\nHolland, J. H., 155, 1076, 1082",
  "Hoff, M. E., 20, 833, 854, 1092\nHoffmann, J., 378, 379, 395, 433, 1076,\n1078\nHogan, N., 1013, 1076\nHOG feature, 947\nHoiem, D., 955, 968, 1076\nholdout cross-validation, 708\nholistic context, 1024\nHolland, J. H., 155, 1076, 1082\nHollerbach, J. M., 1013, 1072\nholonomic, 976\nHolte, R., 107, 112, 678, 687, 1066,\n1072, 1076, 1092\nHolzmann, G. J., 356, 1076\nhomeostatic, 15\nhomophones, 913\nHomo sapiens, 1, 860\nHon, H., 922, 1076\nHonavar, V., 921, 1084\nHong, J., 799, 1082\nHood, A., 10, 1076\nHooker, J., 230, 1076\nHoos, H., 229, 1076\nHopcroft, J., 1012, 1059, 1064, 1088\nHope, J., 886, 1076\nHopﬁeld, J. J., 762, 1076\nHopﬁeld network, 762\nHopkins Beast, 1011\nhorizon (in an image), 931\nhorizon (in MDPs), 648\nhorizon effect, 174\nHorn, A., 276, 1076\nHorn, B. K. P., 968, 1076\nHorn, K. V., 505, 1076\nHorn clause, 256, 791\nHorn form, 275, 276\nHorning, J. J., 1076\nHorowitz, E., 110, 1076\nHorowitz, M., 279, 1084\nHorrocks, J. C., 505, 1070\nhorse, 1028\nHorswill, I., 1013, 1076\nHorvitz, E. J., 26, 29, 61, 553, 604, 639,\n1048, 1076, 1084, 1087\nHovel, D., 553, 1076\nHoward, R. A., 626, 637–639, 685,\n1076, 1082\nHowe, A., 394, 1073\nHowe, D., 360, 1076\nHSCP, 433\nHSP, 387, 395\nHSPR, 395\nHsu, F.-H., 192, 1067, 1076\nHsu, J., 28, 1064\nHTML, 463, 875\nHTN, 406, 431\nHTN planning, 856\nHu, J., 687, 857, 1076\nHuang, K.-C., 228, 1086\nHuang, T., 556, 604, 1076\nHuang, X. D., 922, 1076\nhub, 872\nHubble Space Telescope, 206, 221, 432\nHubel, D. H., 968, 1076\nHuber, M., 1013, 1069\nHubs and Authorities, 872\nHuddleston, R. D., 920, 1076\nHuet, G., 359, 1066\nHuffman, D. A., 20, 1076\nHuffman, S., 1013, 1069\nHughes, B. D., 151, 1076\nHughes, G. E., 470, 1076\nHUGIN, 553, 604\nHuhns, M. N., 61, 1076\nhuman-level AI, 27, 1034\nhuman judgment, 546, 557, 619\nhumanoid robot, 972\nhuman performance, 1\nhuman preference, 649\nHume, D., 6, 1076\nHumphrys, M., 1021, 1076\nHungarian algorithm, 601\nHunkapiller, T., 604, 1065\nHunsberger, L., 682, 688, 1076\nHunt, W., 360, 1076 1110\nIndex\nHunter, L., 826, 1076\nHurst, M., 885, 1076\nHurwicz, L., 688, 1076\nHusmeier, D., 605, 1076\nHussein, A. I., 723, 724, 1078\nHutchinson, S., 1013, 1014, 1068\nHuth, M., 314, 1076\nHuttenlocher, D., 959, 967, 1072, 1076\nHuygens, C., 504, 687, 1076\nHuyn, N., 111, 1076\nHwa, R., 920, 1076\nHwang, C. H., 469, 1076\nHYBRID-WUMPUS-AGENT, 270\nhybrid A*, 991\nhybrid architecture, 1003, 1047\nHYDRA, 185, 193\nhyperparameter, 811\nhypertree width, 230\nhypothesis, 695\napproximately correct, 714\nconsistent, 696\nnull, 705\nprior, 803, 810\nhypothesis prior, 803, 810",
  "HYBRID-WUMPUS-AGENT, 270\nhybrid A*, 991\nhybrid architecture, 1003, 1047\nHYDRA, 185, 193\nhyperparameter, 811\nhypertree width, 230\nhypothesis, 695\napproximately correct, 714\nconsistent, 696\nnull, 705\nprior, 803, 810\nhypothesis prior, 803, 810\nhypothesis space, 696, 769\nHyun, S., 1012, 1070\nI\ni.i.d. (independent and identically\ndistributed), 708, 803\nIagnemma, K., 1014, 1067\nIBAL, 556\nIBM, 18, 19, 29, 185, 193, 922\nIBM 704 computer, 193\nice cream, 483\nID3, 800\nIDA* search, 99, 111\nidentiﬁcation in the limit, 759\nidentity matrix (I), 1056\nidentity uncertainty, 541, 876\nidiot Bayes, 499\nIEEE, 469\nignorance, 547, 549\npractical, 481\ntheoretical, 481\nignore delete lists, 377\nignore preconditions heuristic, 376\nIida, H., 192, 1087\nIJCAI (International Joint Conference\non AI), 31\nILOG, 359\nILP, 779, 800\nimage, 929\nformation, 929–935, 965\nprocessing, 965\nsegmentation, 941–942\nimperfect information, 190, 666\nimplementation (of a high-level action),\n407\nimplementation level, 236\nimplication, 244\nimplicative normal form, 282, 345\nimportance sampling, 532, 554\nincentive, 426\nincentive compatible, 680\ninclusion–exclusion principle, 489\nincompleteness, 342\ntheorem, 8, 352, 1022\ninconsistent support, 381\nincremental formulation, 72\nincremental learning, 773, 777\nindependence, 494, 494–495, 498, 503\nabsolute, 494\nconditional, 498, 502, 503, 517–523,\n551, 574\ncontext-speciﬁc, 542, 563\nmarginal, 494\nindependent subproblems, 222\nindex, 869\nindexical, 904\nindexing, 328, 327–329\nIndia, 16, 227, 468\nindicator variable, 819\nindifference, principle of, 491, 504\nindividual (in genetic algorithms), 127\nindividuation, 445\ninduced width, 229\ninduction, 6\nconstructive, 791\nmathematical, 8\ninductive learning, 694, 695–697\ninductive logic programming (ILP),\n779, 800\nIndyk, P., 760, 1064, 1073\ninference, 208, 235\nprobabilistic, 490, 490–494, 510\ninference procedure, 308\ninference rule, 250, 275\ninferential equivalence, 323\ninferential frame problem, 267, 279\ninﬁnite horizon problems, 685\ninﬂuence diagram, 552, 610, 626\nINFORMATION-GATHERING-AGENT,\n632\ninformation extraction, 873, 873–876,\n883\ninformation gain, 704, 705\ninformation gathering, 39, 994\ninformation retrieval (IR), 464, 867,\n867–872, 883, 884\ninformation sets, 675\ninformation theory, 703–704, 758\ninformation value, 629, 639\ninformed search, 64, 81, 92, 92–102,\n108\ninfuence diagram, 510, 610, 626,\n626–628, 636, 639, 664\nIngerman, P. Z., 919, 1076\nIngham, M., 278, 1092\ninheritance, 440, 454, 478\nmultiple, 455",
  "information theory, 703–704, 758\ninformation value, 629, 639\ninformed search, 64, 81, 92, 92–102,\n108\ninfuence diagram, 510, 610, 626,\n626–628, 636, 639, 664\nIngerman, P. Z., 919, 1076\nIngham, M., 278, 1092\ninheritance, 440, 454, 478\nmultiple, 455\ninitial state, 66, 108, 162, 369\nInoue, K., 795, 1076\ninput resolution, 356\ninside–outside algorithm, 896\ninstance (of a schema), 128\ninstance-based learning, 737, 737–739,\n855\ninsufﬁcient reason, principle of, 504\ninsurance premium, 618\nintelligence, 1, 34\nintelligent backtracking, 218–220, 262\nintentionality, 1026, 1042\nintentional state, 1028\nintercausal reasoning, 548\ninterior-point method, 155\ninterleaving, 147\ninterleaving (actions), 394\ninterleaving (search and action), 136\ninterlingua, 908\ninternal state, 50\nInternet search, 464\nInternet shopping, 462–467\ninterpolation smoothing, 883\ninterpretation, 292, 313\nextended, 313\nintended, 292\npragmatic, 904\ninterreﬂections, 934, 953\ninterval, 448\nIntille, S., 604, 1077\nintractability, 21\nintrinsic property, 445\nintrospection, 3, 12\nintuition pump, 1032\ninverse (of a matrix), 1056\ninverse entailment, 795\ninverse game theory, 679\ninverse kinematics, 987\ninverse reinforcement learning, 857\ninverse resolution, 794, 794–797, 800\ninverted pendulum, 851\ninverted spectrum, 1033\nInza, I., 158, 1080\nIPL, 17\nIPP, 387, 395\nIQ test, 19, 31\nIR, 464, 867, 867–872, 883, 884 Index\n1111\nirrationality, 2, 613\nirreversible, 149\nIS-A links, 471\nIsard, M., 605, 1077\nISBN, 374, 541\nISIS, 432\nIsrael, D., 884, 1075\nITEP, 192\nITEP chess program, 192\nITERATIVE-DEEPENING-SEARCH, 89\niterative deepening search, 88, 88–90,\n108, 110, 173, 408\niterative expansion, 111\niterative lengthening search, 117\nITOU, 800\nItsykson, D., 277, 1064\nIwama, K., 277, 1077\nIwasawa, S., 1041, 1085\nIXTET, 395\nJ\nJaakkola, T., 555, 606, 855, 1077, 1088\nJACK, 195\nJackel, L., 762, 967, 1080\nJackson, F., 1042, 1077\nJacobi, C. G., 606\nJacquard, J., 14\nJacquard loom, 14\nJaffar, J., 359, 1077\nJaguar, 431\nJain, A., 885, 1085\nJames, W., 13\njanitorial science, 37\nJapan, 24\nJasra, A., 605, 1070\nJaumard, B., 277, 1075\nJaynes, E. T., 490, 504, 505, 1077\nJeavons, P., 230, 1085\nJefferson, G., 1026, 1077\nJeffrey, R. C., 504, 637, 1077\nJeffreys, H., 883, 1077\nJelinek, F., 883, 922, 923, 1067, 1077\nJenkin, M., 1014, 1071\nJenkins, G., 604, 1066\nJennings, H. S., 12, 1077\nJenniskens, P., 422, 1077\nJensen, F., 552, 553, 1064\nJensen, F. V., 552, 553, 558, 1064, 1077\nJevons, W. S., 276, 799, 1077\nJi, S., 686, 1077",
  "Jenkin, M., 1014, 1071\nJenkins, G., 604, 1066\nJennings, H. S., 12, 1077\nJenniskens, P., 422, 1077\nJensen, F., 552, 553, 1064\nJensen, F. V., 552, 553, 558, 1064, 1077\nJevons, W. S., 276, 799, 1077\nJi, S., 686, 1077\nJimenez, P., 156, 433, 1077\nJitnah, N., 687, 1079\nJoachims, T., 760, 884, 1077\njob, 402\njob-shop scheduling problem, 402\nJohanson, M., 687, 1066, 1093\nJohnson, C. R., 61, 1067\nJohnson, D. S., 1059, 1073\nJohnson, M., 920, 921, 927, 1041, 1067,\n1068, 1071, 1079\nJohnson, W. W., 109, 1077\nJohnston, M. D., 154, 229, 432, 1077,\n1082\njoint action, 427\njoint probability distribution, 487\nfull, 488, 503, 510, 513–517\njoin tree, 529\nJones, M., 968, 1025, 1091\nJones, N. D., 799, 1077\nJones, R., 358, 885, 1077\nJones, R. M., 358, 1092\nJones, T., 59, 1077\nJonsson, A., 28, 60, 431, 1064, 1077\nJordan, M. I., 555, 605, 606, 686, 761,\n827, 850, 852, 855, 857, 883,\n1013, 1066, 1073, 1077, 1083,\n1084, 1088, 1089, 1091\nJouannaud, J.-P., 359, 1077\nJoule, J., 796\nJuang, B.-H., 604, 922, 1086\nJudd, J. S., 762, 1077\nJuels, A., 155, 1077\nJunker, U., 359, 1077\nJurafsky, D., 885, 886, 920, 922, 1077\nJust, M. A., 288, 1082\njustiﬁcation (in a JTMS), 461\nK\nk-consistency, 211\nk-DL (decision list), 716\nk-DT (decision tree), 716\nk-d tree, 739\nk-fold cross-validation, 708\nKadane, J. B., 639, 687, 1077\nKaelbling, L. P., 278, 556, 605, 686,\n857, 1012, 1068, 1070, 1077,\n1082, 1088, 1090\nKager, R., 921, 1077\nKahn, H., 855, 1077\nKahneman, D., 2, 517, 620, 638, 1077,\n1090\nKaindl, H., 112, 1077\nKalman, R., 584, 604, 1077\nKalman ﬁlter, 566, 584, 584–591, 603,\n604, 981\nswitching, 589, 608\nKalman gain matrix, 588\nKambhampati, S., 157, 390, 394, 395,\n431–433, 1067, 1069, 1071,\n1077, 1084\nKameya, Y., 556, 1087\nKameyama, M., 884, 1075\nKaminka, G., 688, 1089\nKan, A., 110, 405, 432, 1080\nKanade, T., 951, 968, 1087, 1090\nKanal, L. N., 111, 112, 1077, 1079,\n1083\nKanazawa, K., 604, 605, 686, 826,\n1012, 1066, 1070, 1077, 1087\nKanefsky, B., 9, 28, 229, 277, 1064,\n1068\nKanodia, N., 686, 1074\nKanoui, H., 314, 358, 1069\nKant, E., 358, 1067\nKantor, G., 1013, 1014, 1068\nKantorovich, L. V., 155, 1077\nKaplan, D., 471, 1077\nKaplan, H., 111, 1074\nKaplan, R., 884, 920, 1066, 1081\nKarmarkar, N., 155, 1077\nKarmiloff-Smith, A., 921, 1071\nKarp, R. M., 8, 110, 112, 1059, 1075,\n1077\nKartam, N. A., 434, 1077\nKasami, T., 920, 1077\nKasif, S., 553, 1093\nKasparov, G., 29, 192, 193, 1077\nKassirer, J. P., 505, 1074\nKatriel, I., 212, 228, 1091\nKatz, S., 230, 1069\nKaufmann, M., 360, 1077\nKautz, D., 432, 1070",
  "1077\nKartam, N. A., 434, 1077\nKasami, T., 920, 1077\nKasif, S., 553, 1093\nKasparov, G., 29, 192, 193, 1077\nKassirer, J. P., 505, 1074\nKatriel, I., 212, 228, 1091\nKatz, S., 230, 1069\nKaufmann, M., 360, 1077\nKautz, D., 432, 1070\nKautz, H., 154, 229, 277, 279, 395,\n1074, 1077, 1078, 1088\nKavraki, L., 1013, 1014, 1068, 1078\nKay, A. R., 11, 1084\nKay, M., 884, 907, 922, 1066, 1078\nKB, 235, 274, 315\nKB-AGENT, 236\nKeane, M. A., 156, 1079\nKearns, M., 686, 759, 763, 764, 855,\n1078\nKebeasy, R. M., 723, 724, 1078\nKedar-Cabelli, S., 799, 1082\nKeene, R., 29, 1074\nKeeney, R. L., 621, 625, 626, 638, 1078\nKeil, F. C., 3, 1042, 1092\nKeim, G. A., 231, 1080\nKeller, R., 799, 1082\nKelly, J., 826, 1068\nKemp, M., 966, 1078\nKempe, A. B., 1023\nKenley, C. R., 553, 1088\nKephart, J. O., 60, 1078\nKepler, J., 966\nkernel, 743\nkernel function, 747, 816 1112\nIndex\npolynomial, 747\nkernelization, 748\nkernel machine, 744–748\nkernel trick, 744, 748, 760\nKernighan, B. W., 110, 1080\nKersting, K., 556, 1078, 1082\nKessler, B., 862, 883, 1078\nKeynes, J. M., 504, 1078\nKhare, R., 469, 1078\nKhatib, O., 1013, 1078\nKhmelev, D. V., 886, 1078\nKhorsand, A., 112, 1077\nKietz, J.-U., 800, 1078\nKilgarriff, A., 27, 1078\nkiller move, 170\nKim, H. J., 852, 857, 1013, 1084\nKim, J.-H., 1022, 1078\nKim, J. H., 552, 1078\nKim, M., 194\nkinematics, 987\nkinematic state, 975\nKing, R. D., 797, 1078, 1089\nKinsey, E., 109\nkinship domain, 301–303\nKirchner, C., 359, 1077\nKirk, D. E., 60, 1078\nKirkpatrick, S., 155, 229, 1078\nKirman, J., 686, 1070\nKishimoto, A., 194, 1088\nKister, J., 192, 1078\nKisynski, J., 556, 1078\nKitano, H., 195, 1014, 1078\nKjaerulff, U., 604, 1078\nKL-ONE, 471\nKleer, J. D., 60, 1074\nKlein, D., 883, 896, 900, 920, 921,\n1074, 1078, 1085\nKleinberg, J. M., 884, 1078\nKlemperer, P., 688, 1078\nKlempner, G., 553, 1083\nKneser, R., 883, 1078\nKnight, B., 20, 1066\nKnight, K., 2, 922, 927, 1078, 1086\nKnoblock, C. A., 394, 432, 1068, 1073\nKNOWITALL, 885\nknowledge\nacquisition, 860\nand action, 7, 453\nbackground, 235, 349, 777, 1024,\n1025\nbase (KB), 235, 274, 315\ncommonsense, 19\ndiagnostic, 497\nengineering, 307, 307–312, 514\nfor decision-theoretic systems, 634\nlevel, 236, 275\nmodel-based, 497\nprior, 39, 768, 778, 787\nknowledge-based agents, 234\nknowledge-based system, 22–24, 845\nknowledge acquisition, 23, 307, 860\nknowledge compilation, 799\nknowledge map, see Bayesian network\nknowledge representation, 2, 16, 19, 24,\n234, 285–290, 437–479\nanalogical, 315\neverything, 437\nlanguage, 235, 274, 285\nuncertain, 510–513",
  "knowledge acquisition, 23, 307, 860\nknowledge compilation, 799\nknowledge map, see Bayesian network\nknowledge representation, 2, 16, 19, 24,\n234, 285–290, 437–479\nanalogical, 315\neverything, 437\nlanguage, 235, 274, 285\nuncertain, 510–513\nKnuth, D. E., 73, 191, 359, 919, 1013,\n1059, 1074, 1078\nKobilarov, G., 439, 469, 1066\nKocsis, L., 194, 1078\nKoditschek, D., 1013, 1078\nKoehler, J., 395, 1078\nKoehn, P., 922, 1078\nKoenderink, J. J., 968, 1078\nKoenig, S., 157, 395, 434, 685, 1012,\n1075, 1078, 1088\nKoller, D., 191, 505, 553, 556, 558, 604,\n605, 639, 677, 686, 687, 826,\n827, 884, 1012, 1065, 1066,\n1073, 1074, 1076–1078, 1083,\n1085, 1087, 1090\nKolmogorov’s axioms, 489\nKolmogorov, A. N., 504, 604, 759, 1078\nKolmogorov complexity, 759\nKolobov, A., 556, 1082\nKolodner, J., 24, 799, 1078\nKondrak, G., 229, 230, 1078\nKonolige, K., 229, 434, 472, 1012,\n1013, 1067, 1078, 1079\nKoo, T., 920, 1079\nKoopmans, T. C., 685, 1079\nKorb, K. B., 558, 687, 1079\nKoren., Y., 1013, 1066\nKorf, R. E., 110–112, 157, 191, 394,\n395, 1072, 1079, 1085\nKortenkamp, D., 1013, 1069\nKoss, F., 1013, 1069\nKotok, A., 191, 192, 1079\nKoutsoupias, E., 154, 277, 1079\nKowalski, R., 282, 314, 339, 345, 359,\n470, 472, 1079, 1087, 1091\nKowalski form, 282, 345\nKoza, J. R., 156, 1079\nKramer, S., 556, 1078\nKraus, S., 434, 1079\nKraus, W. F., 155, 1080\nKrause, A., 639, 1079\nKrauss, P., 555, 1088\nKriegspiel, 180\nKripke, S. A., 470, 1079\nKrishnan, T., 826, 1082\nKrogh, A., 604, 1079\nKRYPTON, 471\nKtesibios of Alexandria, 15\nK¨ubler, S., 920, 1079\nKuhn, H. W., 601, 606, 687, 1079\nKuhns, J.-L., 884, 1081\nKuijpers, C., 158, 1080\nKuipers, B. J., 472, 473, 1012, 1079\nKumar, P. R., 60, 1079\nKumar, V., 111, 112, 230, 1074, 1077,\n1079, 1083\nKuniyoshi, Y., 195, 1014, 1078\nKuppuswamy, N., 1022, 1078\nKurien, J., 157, 1079\nKurzweil, R., 2, 12, 28, 1038, 1079\nKwok, C., 885, 1079\nKyburg, H. E., 505, 1079\nL\nL-BFGS, 760\nlabel (in plans), 137, 158\nLaborie, P., 432, 1079\nLadanyi, L., 112, 1086\nLadkin, P., 470, 1079\nLafferty, J., 884, 885, 1079\nLagoudakis, M. G., 854, 857, 1074,\n1079\nLaguna, M., 154, 1074\nLaird, J., 26, 336, 358, 432, 799, 1047,\n1077, 1079, 1092\nLaird, N., 604, 826, 1070\nLaird, P., 154, 229, 1082\nLake, R., 194, 1088\nLakemeyer, G., 1012, 1067\nLakoff, G., 469, 921, 1041, 1079\nLam, J., 195, 1079\nLAMA, 387, 395\nLamarck, J. B., 130, 1079\nLambert’s cosine law, 934\nLambertian surface, 969\nLandauer, T. K., 883, 1070\nLandhuis, E., 620, 1079\nlandmark, 980\nlandscape (in state space), 121\nLangdon, W., 156, 1079, 1085",
  "Lam, J., 195, 1079\nLAMA, 387, 395\nLamarck, J. B., 130, 1079\nLambert’s cosine law, 934\nLambertian surface, 969\nLandauer, T. K., 883, 1070\nLandhuis, E., 620, 1079\nlandmark, 980\nlandscape (in state space), 121\nLangdon, W., 156, 1079, 1085\nLangley, P., 800, 1079\nLanglotz, C. P., 26, 1076\nLangton, C., 155, 1079\nlanguage, 860, 888, 890\nabhors synonyms, 870\nformal, 860\nmodel, 860, 909, 913\nin disambiguation, 906\nnatural, 4, 286, 861 Index\n1113\nprocessing, 16, 860\ntranslation, 21, 784, 907–912\nunderstanding, 20, 23\nlanguage generation, 899\nlanguage identiﬁcation, 862\nLaplace, P., 9, 491, 504, 546, 883, 1079\nLaplace smoothing, 863\nLaptev, I., 961, 1080\nlarge-scale learning, 712\nLari, K., 896, 920, 1080\nLarkey, P. D., 687, 1077\nLarra˜naga, P., 158, 1080\nLarsen, B., 553, 1080\nLarson, G., 778\nLarson, S. C., 759, 1080\nLaruelle, H., 395, 431, 1073\nLaskey, K. B., 556, 1080\nLassez, J.-L., 359, 1077\nLassila, O., 469, 1065\nlatent Dirichlet allocation, 883\nlatent semantic indexing, 883\nlatent variable, 816\nLatham, D., 856, 1081\nLatombe, J.-C., 432, 1012, 1013, 1071,\n1078, 1080, 1093\nlattice theory, 360\nLaugherty, K., 920, 1074\nLauritzen, S., 553, 558, 639, 826, 1069,\n1080, 1084, 1089\nLaValle, S., 396, 1013, 1014, 1080\nLave, R. E., 686, 1087\nLavrauc, N., 796, 799, 800, 1080, 1082\nLAWALY, 432\nLawler, E. L., 110, 111, 405, 432, 1080\nlaws of thought, 4\nlayers, 729\nLazanas, A., 1013, 1080\nlaziness, 481\nLa Mettrie, J. O., 1035, 1041, 1079\nLa Mura, P., 638, 1079\nLCF, 314\nLeacock, C., 1022, 1067\nleaf node, 75\nleak node, 519\nLeaper, D. J., 505, 1070\nleaping to conclusions, 778\nlearning, 39, 44, 59, 236, 243, 693,\n1021, 1025\nactive, 831\napprenticeship, 857, 1037\nassessing performance, 708–709\nBayesian, 752, 803, 803–804, 825\nBayesian network, 813–814\nblocks-world, 20\ncart–pole problem, 851\ncheckers, 18\ncomputational theory, 713\ndecision lists, 715–717\ndecision trees, 697–703\ndeterminations, 785\nelement, 55\nensemble, 748, 748–752\nexplanation-based, 780–784\ngame playing, 850–851\ngrammar, 921\nheuristics, 107\nhidden Markov model, 822–823\nhidden variables, 820\nhidden variables, 822\nincremental, 773, 777\ninductive, 694, 695–697\nknowledge-based, 779, 788, 798\ninstance-based, 737, 737–739, 855\nknowledge in, 777–780\nlinearly separable functions, 731\nlogical, 768–776\nMAP, 804–805\nmaximum likelihood, 806–810\nmetalevel, 102\nmixtures of Gaussians, 817–820\nnaive Bayes, 808–809\nneural network, 16, 736–737\nnew predicates, 790, 796\nnoise, 705–706\nnonparametric, 737\nonline, 752, 846\nPAC, 714, 759, 784",
  "logical, 768–776\nMAP, 804–805\nmaximum likelihood, 806–810\nmetalevel, 102\nmixtures of Gaussians, 817–820\nnaive Bayes, 808–809\nneural network, 16, 736–737\nnew predicates, 790, 796\nnoise, 705–706\nnonparametric, 737\nonline, 752, 846\nPAC, 714, 759, 784\nparameter, 806, 810–813\npassive, 831\nQ, 831, 843, 844, 848, 973\nrate of, 719, 836\nreinforcement, 685, 695, 830–859,\n1025\ninverse, 857\nrelational, 857\nrelevance-based, 784–787\nrestaurant problem, 698\nstatistical, 802–805\ntemporal difference, 836–838, 853,\n854\ntop-down, 791–794\nto search, 102\nunsupervised, 694, 817–820, 1025\nutility functions, 831\nweak, 749\nlearning curve, 702\nleast-constraining-value heuristic, 217\nleast commitment, 391\nleave-one-out cross-validation\n(LOOCV), 708\nLeCun, Y., 760, 762, 967, 1047, 1065,\n1080, 1086\nLederberg, J., 23, 468, 1072, 1080\nLee, C.-H., 1022, 1078\nLee, K.-H., 1022, 1078\nLee, M. S., 826, 1083\nLee, R. C.-T., 360, 1068\nLee, T.-M., 11, 1084\nLeech, G., 920, 921, 1080, 1086\nlegal reasoning, 32\nLegendre, A. M., 759, 1080\nLehmann, D., 434, 1079\nLehmann, J., 439, 469, 1066\nLehrer, J., 638, 1080\nLeibniz, G. W., 6, 131, 276, 504, 687\nLeimer, H., 553, 1080\nLeipzig, 12\nLeiserson, C. E., 1059, 1069\nLempel-Ziv-Welch compression (LZW),\n867\nLenat, D. B., 27, 439, 469, 474, 800,\n1070, 1075, 1080\nlens system, 931\nLenstra, J. K., 110, 405, 432, 1080\nLenzerini, M., 471, 1067\nLeonard, H. S., 470, 1080\nLeonard, J., 1012, 1066, 1080\nLeone, N., 230, 472, 1071, 1074\nLesh, N., 433, 1072\nLe´sniewski, S., 470, 1080\nLesser, V. R., 434, 1071\nLettvin, J. Y., 963, 1080\nLetz, R., 359, 1080\nlevel (in planning graphs), 379\nlevel cost, 382\nleveled off (planning graph), 381\nLevesque, H. J., 154, 277, 434, 471,\n473, 1067, 1069, 1080, 1088\nLevin, D. A., 604, 1080\nLevinson, S., 314, 1066, 1074\nLevitt, G. M., 190, 1080\nLevitt, R. E., 434, 1077\nLevitt, T. S., 1012, 1079\nLevy, D., 195, 1022, 1080\nLewis, D. D., 884, 1080\nLewis, D. K., 60, 1042, 1080\nLEX, 776, 799\nlexical category, 888\nlexicalized grammar, 897\nlexicalized PCFG, 897, 919, 920\nlexicon, 890, 920\nLeyton-Brown, K., 230, 435, 688, 1080,\n1088\nLFG, 920\nLi, C. M., 277, 1080\nLi, H., 686, 1077\nLi, M., 759, 1080\nliability, 1036 1114\nIndex\nLiang, G., 553, 1068\nLiang, L., 604, 1083\nLiao, X., 686, 1077\nLiberatore, P., 279, 1080\nLifchits, A., 885, 1085\nlife insurance, 621\nLifschitz, V., 472, 473, 1073, 1080,\n1091\nlifting, 326, 325–329, 367\nin probabilistic inference, 544\nlifting lemma, 350, 353\nlight, 932\nLighthill, J., 22, 1080\nLighthill report, 22, 24\nlikelihood, 803",
  "Lifchits, A., 885, 1085\nlife insurance, 621\nLifschitz, V., 472, 473, 1073, 1080,\n1091\nlifting, 326, 325–329, 367\nin probabilistic inference, 544\nlifting lemma, 350, 353\nlight, 932\nLighthill, J., 22, 1080\nLighthill report, 22, 24\nlikelihood, 803\nLIKELIHOOD-WEIGHTING, 534\nlikelihood weighting, 532, 552, 596\nLim, G., 439, 1089\nlimited rationality, 5\nLin, D., 885, 1085\nLin, J., 872, 885, 1065\nLin, S., 110, 688, 1080, 1092\nLin, T., 439, 1089\nLincoln, A., 872\nLindley, D. V., 639, 1080\nLindsay, R. K., 468, 1080\nlinear-chain conditional random ﬁeld,\n878\nlinear algebra, 1055–1057\nlinear constraint, 205\nlinear function, 717\nlinear Gaussian, 520, 553, 584, 809\nlinearization, 981\nlinear programming, 133, 153, 155, 206,\n673\nlinear regression, 718, 810\nlinear resolution, 356, 795\nlinear separability, 723\nlinear separator, 746\nline search, 132\nlinguistics, 15–16\nlink, 870\nlink (in a neural network), 728\nlinkage constraints, 986\nLinnaeus, 469\nLINUS, 796\nLipkis, T. A., 471, 1088\nliquid event, 447\nliquids, 472\nLisp, 19, 294\nlists, 305\nliteral (sentence), 244\nliteral, watched, 277\nLittman, M. L., 155, 231, 433, 686, 687,\n857, 1064, 1068, 1077, 1080,\n1081\nLiu, J. S., 605, 1080\nLiu, W., 826, 1068\nLiu, X., 604, 1083\nLivescu, K., 604, 1080\nLivnat, A., 434, 1080\nlizard toasting, 778\nlocal beam search, 125, 126\nlocal consistency, 208\nlocality, 267, 547\nlocality-sensitive hash (LSH), 740\nlocalization, 145, 581, 979\nMarkov, 1012\nlocally structured system, 515\nlocally weighted regression, 742\nlocal optimum, 669\nlocal search, 120–129, 154, 229,\n262–263, 275, 277\nlocation sensors, 974\nLocke, J., 6, 1042, 1080\nLodge, D., 1051, 1080\nLoebner Prize, 1021\nLoftus, E., 287, 1080\nLogemann, G., 260, 276, 1070\nlogic, 4, 7, 240–243\natoms, 294–295\ndefault, 459, 468, 471\nequality in, 299\nﬁrst-order, 285, 285–321\ninference, 322–325\nsemantics, 290\nsyntax, 290\nfuzzy, 240, 289, 547, 550, 557\nhigher-order, 289\ninductive, 491, 505\ninterpretations, 292–294\nmodel preference, 459\nmodels, 290–292\nnonmonotonic, 251, 458, 458–460,\n471\nnotation, 4\npropositional, 235, 243–247, 274, 286\ninference, 247–263\nsemantics, 245–246\nsyntax, 244–245\nquantiﬁer, 295–298\nresolution, 252–256\nsampling, 554\ntemporal, 289\nterms, 294\nvariable in, 340\nlogical connective, 16, 244, 274, 295\nlogical inference, 242, 322–365\nlogical minimization, 442\nlogical omniscience, 453\nlogical piano, 276\nlogical positivism, 6\nlogical reasoning, 249–264, 284\nlogicism, 4\nlogic programming, 257, 314, 337,\n339–345\nconstraint, 344–345, 359",
  "logical inference, 242, 322–365\nlogical minimization, 442\nlogical omniscience, 453\nlogical piano, 276\nlogical positivism, 6\nlogical reasoning, 249–264, 284\nlogicism, 4\nlogic programming, 257, 314, 337,\n339–345\nconstraint, 344–345, 359\ninductive (ILP), 779, 788–794, 798\ntabled, 343\nLogic Theorist, 17, 276\nLOGISTELLO, 175, 186\nlogistic function, 522, 760\nlogistic regression, 726\nlogit distribution, 522\nlog likelihood, 806\nLohn, J. D., 155, 1080\nLondon, 14\nLong, D., 394, 395, 1072, 1073\nlong-distance dependencies, 904\nlong-term memory, 336\nLongley, N., 692, 1080\nLonguet-Higgins, H. C., 1080\nLoo, B. T., 275, 1080\nLOOCV, 708\nLook ma, no hands, 18\nlookup table, 736\nLoomes, G., 637, 1086\nloosely coupled system, 427\nLorenz, U., 193, 1071\nloss function, 710\nLotem, A., 396, 1091\nlottery, 612, 642\nstandard, 615\nlove, 1021\nLove, N., 195, 1080\nLovejoy, W. S., 686, 1080\nLovelace, A., 14\nLoveland, D., 260, 276, 359, 1070, 1080\nlow-dimensional embedding, 985\nLowe, D., 947, 967, 968, 1081\nL¨owenheim, L., 314, 1081\nLowerre, B. T., 154, 922, 1081\nLowrance, J. D., 557, 1087\nLowry, M., 356, 360, 1075, 1081\nLoyd, S., 109, 1081\nLozano-Perez, T., 1012, 1013, 1067,\n1081, 1092\nLPG, 387, 395\nLRTA*, 151, 157, 415\nLRTA*-AGENT, 152\nLRTA*-COST, 152\nLSH (locality-sensitive hash), 740\nLT, 17\nLu, F., 1012, 1081\nLu, P., 194, 760, 1067, 1088\nLuby, M., 124, 554, 1069, 1081\nLucas, J. R., 1023, 1081\nLucas, P., 505, 634, 1081 Index\n1115\nLuce, D. R., 9, 687, 1081\nLucene, 868\nLudlow, P., 1042, 1081\nLuger, G. F., 31, 1081\nLugosi, G., 761, 1068\nLull, R., 5\nLuong, Q.-T., 968, 1072\nLusk, E., 360, 1092\nLygeros, J., 60, 1068\nLyman, P., 759, 1081\nLynch, K., 1013, 1014, 1068\nLZW, 867\nM\nMA* search, 101, 101–102, 112\nMACHACK-6, 192\nMachina, M., 638, 1081\nmachine evolution, 21\nmachine learning, 2, 4\nmachine reading, 881\nmachine translation, 32, 907–912, 919\nstatistical, 909–912\nMachover, M., 314, 1065\nMacKay, D. J. C., 555, 761, 763, 1081,\n1082\nMacKenzie, D., 360, 1081\nMackworth, A. K., 2, 59, 209, 210, 228,\n230, 1072, 1081, 1085\nmacrop (macro operator), 432, 799\nmadalines, 761\nMadigan, C. F., 277, 1083\nmagic sets, 336, 358\nMahalanobis distance, 739\nMahanti, A., 112, 1081\nMahaviracarya, 503\nMaheswaran, R., 230, 1085\nMaier, D., 229, 358, 1065\nMailath, G., 688, 1081\nMajercik, S. M., 433, 1081\nmajority function, 731\nmakespan, 402\nMakov, U. E., 826, 1090\nMalave, V. L., 288, 1082\nMaldague, P., 28, 1064\nMali, A. D., 432, 1077\nMalik, J., 604, 755, 762, 941, 942, 953,\n967, 968, 1065, 1070, 1076,\n1081, 1088",
  "Majercik, S. M., 433, 1081\nmajority function, 731\nmakespan, 402\nMakov, U. E., 826, 1090\nMalave, V. L., 288, 1082\nMaldague, P., 28, 1064\nMali, A. D., 432, 1077\nMalik, J., 604, 755, 762, 941, 942, 953,\n967, 968, 1065, 1070, 1076,\n1081, 1088\nMalik, S., 277, 1083\nManchak, D., 470, 1091\nManeva, E., 278, 1081\nManiatis, P., 275, 1080\nmanipulator, 971\nManna, Z., 314, 1081\nMannila, H., 763, 1074\nManning, C., 883–885, 920, 921, 1078,\n1081\nMannion, M., 314, 1081\nManolios, P., 360, 1077\nMansour, Y., 686, 764, 855, 856, 1078,\n1090\nmantis shrimp, 935\nManzini, G., 111, 1081\nmap, 65\nMAP (maximum a posteriori), 804\nMAPGEN, 28\nMarais, H., 884, 1088\nMarbach, P., 855, 1081\nMarch, J. G., 637, 1075\nMarcinkiewicz, M. A., 895, 921, 1081\nMarcot, B., 553, 1086\nMarcus, G., 638, 1081\nMarcus, M. P., 895, 921, 1081\nmargin, 745\nmarginalization, 492\nMarkov\nassumption\nsensor, 568\nprocess\nﬁrst-order, 568\nMarkov, A. A., 603, 883, 1081\nMarkov assumption, 568, 603\nMarkov blanket, 517, 560\nMarkov chain, 537, 568, 861\nMarkov chain Monte Carlo (MCMC),\n535, 535–538, 552, 554, 596\ndecayed, 605\nMarkov decision process (MDP), 10,\n647, 684, 686, 830\nfactored, 686\npartially observable (POMDP), 658,\n658–666, 686\nMarkov games, 857\nMarkov network, 553\nMarkov process, 568\nMarkov property, 577, 603, 646\nMaron, M. E., 505, 884, 1081\nMarr, D., 968, 1081\nMarriott, K., 228, 1081\nMarshall, A. W., 855, 1077\nMarsland, A. T., 195, 1081\nMarsland, S., 763, 1081\nMartelli, A., 110, 111, 156, 1081\nMarthi, B., 432, 556, 605, 856, 1081,\n1082, 1085\nMartin, D., 941, 967, 1081\nMartin, J. H., 885, 886, 920–922, 1077,\n1081\nMartin, N., 358, 1067\nMartin, P., 921, 1076\nMason, M., 156, 433, 1013, 1014, 1071,\n1081\nMason, R. A., 288, 1082\nmass (in Dempster–Shafer theory), 549\nmass noun, 445\nmass spectrometer, 22\nMataric, M. J., 1013, 1081\nMateescu, R., 230, 1070\nMateis, C., 472, 1071\nmaterialism, 6\nmaterial value, 172\nMates, B., 276, 1081\nmathematical induction schema, 352\nmathematics, 7–9, 18, 30\nMatheson, J. E., 626, 638, 1076, 1082\nmatrix, 1056\nMatsubara, H., 191, 195, 1072, 1078\nMaturana, H. R., 963, 1080\nMatuszek, C., 469, 1081\nMauchly, J., 14\nMausam., 432, 1069\nMAVEN, 195\nMAX-VALUE, 166, 170\nmaximin, 670\nmaximin equilibrium, 672\nmaximum\nglobal, 121\nlocal, 122\nmaximum a posteriori, 804, 825\nmaximum expected utility, 483, 611\nmaximum likelihood, 805, 806–810,\n825\nmaximum margin separator, 744, 745\nmax norm, 654\nMAXPLAN, 387\nMaxwell, J., 546, 920, 1081\nMayer, A., 112, 119, 1075\nMayne, D. Q., 605, 1075\nMazumder, P., 110, 1088",
  "maximum expected utility, 483, 611\nmaximum likelihood, 805, 806–810,\n825\nmaximum margin separator, 744, 745\nmax norm, 654\nMAXPLAN, 387\nMaxwell, J., 546, 920, 1081\nMayer, A., 112, 119, 1075\nMayne, D. Q., 605, 1075\nMazumder, P., 110, 1088\nMazurie, A., 605, 1085\nMBP, 433\nMcAllester, D. A., 25, 156, 191, 198,\n394, 395, 472, 855, 856, 1072,\n1077, 1081, 1090\nMCC, 24\nMcCallum, A., 877, 884, 885, 1069,\n1072, 1077, 1079, 1081, 1084,\n1085, 1090\nMcCarthy, J., 17–19, 27, 59, 275, 279,\n314, 395, 440, 471, 1020, 1031,\n1081, 1082\nMcCawley, J. D., 920, 1082\nMcClelland, J. L., 24, 1087\nMcClure, M., 604, 1065\nMcCorduck, P., 1042, 1082 1116\nIndex\nMcCulloch, W. S., 15, 16, 20, 278, 727,\n731, 761, 963, 1080, 1082\nMcCune, W., 355, 360, 1082\nMcDermott, D., 2, 156, 358, 394, 433,\n434, 454, 470, 471, 1068, 1073,\n1082\nMcDermott, J., 24, 336, 358, 1082\nMcDonald, R., 288, 920, 1079\nMcEliece, R. J., 555, 1082\nMcGregor, J. J., 228, 1082\nMcGuinness, D., 457, 469, 471, 1064,\n1066, 1089\nMcIlraith, S., 314, 1082\nMcLachlan, G. J., 826, 1082\nMcMahan, B., 639, 1079\nMCMC, 535, 535–538, 552, 554, 596\nMcMillan, K. L., 395, 1082\nMcNealy, S., 1036\nMcPhee, N., 156, 1085\nMDL, 713, 759, 805\nMDP, 10, 647, 684, 686, 830\nmean-ﬁeld approximation, 554\nmeasure, 444\nmeasurement, 444\nmechanism, 679\nstrategy-proof, 680\nmechanism design, 679, 679–685\nmedical diagnosis, 23, 505, 517, 548,\n629, 1036\nMeehan, J., 358, 1068\nMeehl, P., 1022, 1074, 1082\nMeek, C., 553, 1092\nMeet (interval relation), 448\nMegarian school, 275\nmegavariable, 578\nMeggido, N., 677, 687, 1078\nMehlhorn, K., 112, 1069\nmel frequency cepstral coefﬁcient\n(MFCC), 915\nMellish, C. S., 359, 1068\nmemoization, 343, 357, 780\nmemory requirements, 83, 88\nMEMS, 1045\nMendel, G., 130, 1082\nmeningitis, 496–509\nmental model, in disambiguation, 906\nmental objects, 450–453\nmental states, 1028\nMercer’s theorem, 747\nMercer, J., 747, 1082\nMercer, R. L., 883, 922, 1067, 1077\nmereology, 470\nMerkhofer, M. M., 638, 1082\nMerleau-Ponty, M., 1041, 1082\nMeshulam, R., 112, 1072\nMeta-DENDRAL, 776, 798\nmetadata, 870\nmetalevel, 1048\nmetalevel state space, 102\nmetaphor, 906, 921\nmetaphysics, 6\nmetareasoning, 189\ndecision-theoretic, 1048\nmetarule, 345\nmeteorite, 422, 480\nmetonymy, 905, 921\nMetropolis, N., 155, 554, 1082\nMetropolis–Hastings, 564\nMetropolis algorithm, 155, 554\nMetzinger, T., 1042, 1082\nMetzler, D., 884, 1069\nMEXAR2, 28\nMeyer, U., 112, 1069\nM´ezard, M., 762, 1082\nMGONZ, 1021\nMGSS*, 191\nMGU (most general uniﬁer), 327, 329,\n353, 361",
  "Metropolis–Hastings, 564\nMetropolis algorithm, 155, 554\nMetzinger, T., 1042, 1082\nMetzler, D., 884, 1069\nMEXAR2, 28\nMeyer, U., 112, 1069\nM´ezard, M., 762, 1082\nMGONZ, 1021\nMGSS*, 191\nMGU (most general uniﬁer), 327, 329,\n353, 361\nMHT (multiple hypothesis tracker), 606\nMian, I. S., 604, 605, 1079, 1083\nMichalski, R. S., 799, 1082\nMichaylov, S., 359, 1077\nMichie, D., 74, 110, 111, 156, 191, 763,\n851, 854, 1012, 1071, 1082\nmicro-electromechanical systems\n(MEMS), 1045\nmicromort, 616, 637, 642\nMicrosoft, 553, 874\nmicroworld, 19, 20, 21\nMiddleton, B., 519, 552, 1086\nMiikkulainen, R., 435, 1067\nMilch, B., 556, 639, 1078, 1082, 1085\nMilgrom, P., 688, 1082\nMilios, E., 1012, 1081\nmilitary uses of AI, 1035\nMill, J. S., 7, 770, 798, 1082\nMiller, A. C., 638, 1082\nMiller, D., 431, 1070\nmillion queens problem, 221, 229\nMillstein, T., 395, 1071\nMilner, A. J., 314, 1074\nMIN-CONFLICTS, 221\nmin-conﬂicts heuristic, 220, 229\nMIN-VALUE, 166, 170\nmind, 2, 1041\ndualistic view, 1041\nand mysticism, 12\nphilosophy of, 1041\nas physical system, 6\ntheory of, 3\nmind–body problem, 1027\nminesweeper, 284\nMINIMAL-CONSISTENT-DET, 786\nminimal model, 459\nMINIMAX-DECISION, 166\nminimax algorithm, 165, 670\nminimax decision, 165\nminimax search, 165–168, 188, 189\nminimax value, 164, 178\nminimum\nglobal, 121\nlocal, 122\nminimum-remaining-values, 216, 333\nminimum description length (MDL),\n713, 759, 805\nminimum slack, 405\nminimum spanning tree (MST), 112,\n119\nMINISAT, 277\nMinker, J., 358, 473, 1073, 1082\nMinkowski distance, 738\nMinsky, M. L., 16, 18, 19, 22, 24, 27,\n434, 471, 552, 761, 1020, 1039,\n1042, 1082\nMinton, S., 154, 229, 432, 799, 1068,\n1082\nMiranker, D. P., 229, 1065\nMisak, C., 313, 1082\nmissing attribute values, 706\nmissionaries and cannibals, 109, 115,\n468\nMIT, 17–19, 1012\nMitchell, D., 154, 277, 278, 1069, 1088\nMitchell, M., 155, 156, 1082\nMitchell, T. M., 61, 288, 763, 776, 798,\n799, 884, 885, 1047, 1066, 1067,\n1069, 1082, 1084\nMitra, M., 870, 1089\nmixed strategy, 667\nmixing time, 573\nmixture\ndistribution, 817\nmixture distribution, 817\nmixture of Gaussians, 608, 817, 820\nMizoguchi, R., 27, 1075\nML, see maximum likelihood\nmodal logic, 451\nmodel, 50, 240, 274, 289, 313, 451\ncausal, 517\n(in representation), 13\nsensor, 579, 586, 603\ntheory, 314\ntransition, 67, 108, 134, 162, 266,\n566, 597, 603, 646, 684, 832,\n979\nMODEL-BASED-REFLEX-AGENT, 51\nmodel-based reﬂex agents, 59\nmodel checking, 242, 274 Index\n1117\nmodel selection, 709, 825\nModus Ponens, 250, 276, 356, 357, 361\nGeneralized, 325, 326",
  "transition, 67, 108, 134, 162, 266,\n566, 597, 603, 646, 684, 832,\n979\nMODEL-BASED-REFLEX-AGENT, 51\nmodel-based reﬂex agents, 59\nmodel checking, 242, 274 Index\n1117\nmodel selection, 709, 825\nModus Ponens, 250, 276, 356, 357, 361\nGeneralized, 325, 326\nMoffat, A., 884, 1092\nMOGO, 186, 194\nMohr, R., 210, 228, 968, 1082, 1088\nMohri, M., 889, 1083\nMolloy, M., 277, 1064\nmonism, 1028\nmonitoring, 145\nmonkey and bananas, 113, 396\nmonotone condition, 110\nmonotonicity\nof a heuristic, 95\nof a logical system, 251, 458\nof preferences, 613\nMontague, P. R., 854, 1083, 1088\nMontague, R., 470, 471, 920, 1077,\n1083\nMontanari, U., 111, 156, 228, 1066,\n1081, 1083\nMONTE-CARLO-LOCALIZATION, 982\nMonte Carlo (in games), 183\nMonte Carlo, sequential, 605\nMonte Carlo algorithm, 530\nMonte Carlo localization, 981\nMonte Carlo simulation, 180\nMontemerlo, M., 1012, 1083\nMooney, R., 799, 902, 921, 1070, 1083,\n1093\nMoore’s Law, 1038\nMoore, A., 826, 1083\nMoore, A. W., 154, 826, 854, 857, 1066,\n1077, 1083\nMoore, E. F., 110, 1083\nMoore, J. S., 356, 359, 360, 1066, 1077\nMoore, R. C., 470, 473, 922, 1076, 1083\nMoravec, H. P., 1012, 1029, 1038, 1083\nMore, T., 17\nMorgan, J., 434, 1069\nMorgan, M., 27, 1069\nMorgan, N., 922, 1074\nMorgenstern, L., 470, 472, 473, 1070,\n1083\nMorgenstern, O., 9, 190, 613, 637, 1091\nMoricz, M., 884, 1088\nMorjaria, M. A., 553, 1083\nMorris, A., 604, 1089\nMorris, P., 28, 60, 431, 1064, 1077\nMorrison, E., 190, 1083\nMorrison, P., 190, 1083\nMoses, Y., 470, 477, 1072\nMoskewicz, M. W., 277, 1083\nMossel, E., 278, 1081\nMosteller, F., 886, 1083\nmost general uniﬁer (MGU), 327, 329,\n353, 361\nmost likely explanation, 553, 603\nmost likely state, 993\nMostow, J., 112, 119, 1083\nmotion, 948–951\ncompliant, 986, 995\nguarded, 995\nmotion blur, 931\nmotion model, 979\nmotion parallax, 949, 966\nmotion planning, 986\nMotwani, R., 682, 760, 1064, 1073\nMotzkin, T. S., 761, 1083\nMoutarlier, P., 1012, 1083\nmovies\nmovies\n2001: A Space Odyssey, 552\nmovies\nA.I., 1040\nmovies\nThe Matrix, 1037\nmovies\nThe Terminator, 1037\nMozetic, I., 799, 1082\nMPI (mutual preferential\nindependence), 625\nMRS (metalevel reasoning system), 345\nMST, 112, 119\nMueller, E. T., 439, 470, 1083, 1089\nMuggleton, S. H., 789, 795, 797, 800,\n921, 1071, 1083, 1089, 1090\nM¨uller, M., 186, 194, 1083, 1088\nMuller, U., 762, 967, 1080\nmultiagent environments, 161\nmultiagent planning, 425–430\nmultiagent systems, 60, 667\nmultiattribute utility theory, 622, 638\nmultibody planning, 425, 426–428\nmultiplexer, 543\nmultiply connected network, 528",
  "Muller, U., 762, 967, 1080\nmultiagent environments, 161\nmultiagent planning, 425–430\nmultiagent systems, 60, 667\nmultiattribute utility theory, 622, 638\nmultibody planning, 425, 426–428\nmultiplexer, 543\nmultiply connected network, 528\nmultivariate linear regression, 720\nMumford, D., 967, 1083\nMUNIN, 552\nMurakami, T., 186\nMurga, R., 158, 1080\nMurphy, K., 555, 558, 604, 605, 1012,\n1066, 1071, 1073, 1083, 1090\nMurphy, R., 1014, 1083\nMurray-Rust, P., 469, 1083\nMurthy, C., 360, 1083\nMuscettola, N., 28, 60, 431, 432, 1077,\n1083\nmusic, 14\nMuslea, I., 885, 1083\nmutagenicity, 797\nmutation, 21, 128, 153\nmutex, 380\nmutual exclusion, 380\nmutual preferential independence\n(MPI), 625\nmutual utility independence (MUI), 626\nMYCIN, 23, 548, 557\nMyerson, R., 688, 1083\nmyopic policy, 632\nmysticism, 12\nN\nn-armed bandit, 841\nn-gram model, 861\nNadal, J.-P., 762, 1082\nNagasawa, Y., 1042, 1081\nNagel, T., 1042, 1083\nNa¨ım, P., 553, 1086\nnaive Bayes, 499, 503, 505, 808–809,\n820, 821, 825\nnaked, 214\nNalwa, V. S., 12, 1083\nNaor, A., 278, 1064\nNardi, D., 471, 1064, 1067\nnarrow content, 1028\nNASA, 28, 392, 432, 472, 553, 972\nNash, J., 1083\nNash equilibrium, 669, 685\nNASL, 434\nNATACHATA, 1021\nnatural kind, 443\nnatural numbers, 303\nnatural stupidity, 454\nNau, D. S., 111, 187, 191, 192, 195,\n372, 386, 395, 396, 432,\n1071–1073, 1079, 1083, 1084,\n1089, 1091\nnavigation function, 994\nNayak, P., 60, 157, 432, 472, 1079, 1083\nNeal, R., 762, 1083\nNealy, R., 193\nnearest-neighbor ﬁlter, 601\nnearest-neighbors, 738, 814\nnearest-neighbors regression, 742\nneat vs. scruffy, 25\nNebel, B., 394, 395, 1076, 1078, 1083\nneedle in a haystack, 242\nNeﬁan, A., 604, 1083\nnegation, 244\nnegative example, 698\nnegative literal, 244\nnegligence, 1036\nNelson, P. C., 111, 1071\nNemirovski, A., 155, 1065, 1083\nNERO, 430, 435 1118\nIndex\nNesterov, Y., 155, 1083\nNetto, E., 110, 1083\nnetwork tomography, 553\nneural network, 16, 20, 24, 186, 727,\n727–737\nexpressiveness, 16\nfeed-forward, 729\nhardware, 16\nlearning, 16, 736–737\nmultilayer, 22, 731–736\nperceptron, 729–731\nradial basis function, 762\nsingle layer, see perceptron\nneurobiology, 968\nNEUROGAMMON, 851\nneuron, 10, 16, 727, 1030\nneuroscience, 10, 10–12, 728\ncomputational, 728\nNevill-Manning, C. G., 921, 1083\nNEW-CLAUSE, 793\nNewborn, M., 111, 1085\nNewell, A., 3, 17, 18, 26, 60, 109, 110,\n191, 275, 276, 336, 358, 393,\n432, 799, 1047, 1079, 1084,\n1089\nNewman, P., 1012, 1066, 1071\nNewton, I., 1, 47, 131, 154, 570, 760,\n1084\nNewton–Raphson method, 132",
  "NEW-CLAUSE, 793\nNewborn, M., 111, 1085\nNewell, A., 3, 17, 18, 26, 60, 109, 110,\n191, 275, 276, 336, 358, 393,\n432, 799, 1047, 1079, 1084,\n1089\nNewman, P., 1012, 1066, 1071\nNewton, I., 1, 47, 131, 154, 570, 760,\n1084\nNewton–Raphson method, 132\nNey, H., 604, 883, 922, 1078, 1084\nNg, A. Y., 686, 759, 850, 852, 855–857,\n883, 1013, 1066, 1068, 1078,\n1084\nNguyen, H., 883, 1078\nNguyen, X., 394, 395, 1084\nNiblett, T., 800, 1068\nNicholson, A., 558, 604, 686, 687,\n1070, 1079, 1084\nNielsen, P. E., 358, 1077\nNiemel¨a, I., 472, 1084\nNigam, K., 884, 885, 1069, 1077, 1084\nNigenda, R. S., 395, 1084\nNiles, I., 469, 1084, 1085\nNilsson, D., 639, 1084\nNilsson, N. J., 2, 27, 31, 59, 60,\n109–111, 119, 156, 191, 275,\n314, 350, 359, 367, 393, 432,\n434, 555, 761, 799, 1012, 1019,\n1034, 1072, 1073, 1075, 1084,\n1091\nNine-Men’s Morris, 194\nNiranjan, M., 605, 855, 1070, 1087\nNisan, N., 688, 1084\nNIST, 753\nnitroaromatic compounds, 797\nNiv, Y., 854, 1070\nNivre, J., 920, 1079\nNixon, R., 459, 638, 906\nNixon diamond, 459\nNiyogi, S., 314, 1090\nNLP (natural language processing), 2,\n860\nno-good, 220, 385\nno-regret learning, 753\nNOAH, 394, 433\nNobel Prize, 10, 22\nNocedal, J., 760, 1067\nNoda, I., 195, 1014, 1078\nnode\nchild, 75\ncurrent, in local search, 121\nparent, 75\nnode consistency, 208\nNoe, A., 1041, 1084\nnoise, 701, 705–706, 712, 776, 787, 802\nnoisy-AND, 561\nnoisy-OR, 518\nnoisy channel model, 913\nnominative case, 899\nnondeterminism\nangelic, 411\ndemonic, 410\nnondeterministic environment, 43\nnonholonomic, 976\nNONLIN, 394\nNONLIN+, 431, 432\nnonlinear, 589\nnonlinear constraints, 205\nnonmonotonicity, 458\nnonmonotonic logic, 251, 458,\n458–460, 471\nNono, 330\nnonstationary, 857\nnonterminal symbol, 889, 890, 1060\nNormal–Wishart, 811\nnormal distribution, 1058\nstandard, 1058\nnormal form, 667\nnormalization (of a probability\ndistribution), 493\nnormalization (of attribute ranges), 739\nNorman, D. A., 884, 1066\nnormative theory, 619\nNorth, O., 330\nNorth, T., 21, 1072\nNorvig, P., 28, 358, 444, 470, 604, 759,\n883, 921, 922, 1074, 1078, 1084,\n1087\nnotation\ninﬁx, 303\nlogical, 4\npreﬁx, 304\nnoughts and crosses, 162, 190, 197\nNourbakhsh, I., 156, 1073\nNowak, R., 553, 1068\nNowatzyk, A., 192, 1076\nNowick, S. M., 279, 1084\nNowlan, S. J., 155, 1075\nNP (hard problems), 1054–1055\nNP-complete, 8, 71, 109, 250, 276, 471,\n529, 762, 787, 1055\nNQTHM, 360\nNSS chess program, 191\nnuclear power, 561\nnumber theory, 800\nNunberg, G., 862, 883, 921, 1078, 1084\nNUPRL, 360\nNussbaum, M. C., 1041, 1084\nNyberg, L., 11, 1067\nO",
  "NP-complete, 8, 71, 109, 250, 276, 471,\n529, 762, 787, 1055\nNQTHM, 360\nNSS chess program, 191\nnuclear power, 561\nnumber theory, 800\nNunberg, G., 862, 883, 921, 1078, 1084\nNUPRL, 360\nNussbaum, M. C., 1041, 1084\nNyberg, L., 11, 1067\nO\nO() notation, 1054\nO’Malley, K., 688, 1092\nO’Reilly, U.-M., 155, 1084\nO-PLAN, 408, 431, 432\nOaksford, M., 638, 1068, 1084\nobject, 288, 294\ncomposite, 442\nobject-level state space, 102\nobject-oriented programming, 14, 455\nobjective case, 899\nobjective function, 15, 121\nobjectivism, 491\nobject model, 928\nobservable, 42\nobservation model, 568\nobservation prediction, 142\nobservation sentences, 6\noccupancy grid, 1012\noccupied space, 988\noccur check, 327, 340\nOch, F. J., 29, 604, 921, 922, 1067,\n1084, 1093\nOckham’s razor, 696, 757–759, 777,\n793, 805\nOckham, W., 696, 758\nOddi, A., 28, 1068\nodometry, 975\nOdyssey, 1040\nOfﬁce Assistant, 553\nofﬂine search, 147\nOgasawara, G., 604, 1076\nOgawa, S., 11, 1084\nOglesby, F., 360, 1074\nOh, S., 606, 1084\nOhashi, T., 195, 1091\nOlalainty, B., 432, 1073 Index\n1119\nOlesen, K. G., 552–554, 1064, 1084\nOliver, N., 604, 1084\nOliver, R. M., 639, 1084\nOliver, S. G., 797, 1078\nOlshen, R. A., 758, 1067\nomniscience, 38\nOmohundro, S., 27, 920, 1039, 1084,\n1089\nOng, D., 556, 1082\nONLINE-DFS-AGENT, 150\nonline learning, 752, 846\nonline planning, 415\nonline replanning, 993\nonline search, 147, 147–154, 157\nontological commitment, 289, 313, 482,\n547\nontological engineering, 437, 437–440\nontology, 308, 310\nupper, 467\nopen-coding, 341\nopen-loop, 66\nopen-universe probability model\n(OUPM), 545, 552\nopen-world assumption, 417\nopen class, 890\nOPENCYC, 469\nopen list, see frontier\nOPENMIND, 439\noperationality, 783\noperations research, 10, 60, 110, 111\nOppacher, F., 155, 1084\nOPS-5, 336, 358\noptical ﬂow, 939, 964, 967\noptimal brain damage, 737\noptimal controllers, 997\noptimal control theory, 155\noptimality, 121\noptimality (of a search algorithm), 80,\n108\noptimality theory (Linguistics), 921\noptimally efﬁcient algorithm, 98\noptimal solution, 68\noptimism under uncertainty, 151\noptimistic description (of an action), 412\noptimistic prior, 842\noptimization, 709\nconvex, 133, 153\noptimizer’s curse, 619, 637\nOPTIMUM-AIV, 432\nOR-SEARCH, 136\norderability, 612\nordinal utility, 614\nOrganon, 275, 469\norientation, 938\norigin function, 545\nOrmoneit, D., 855, 1084\nOR node, 135\nOsawa, E., 195, 1014, 1078\nOsborne, M. J., 688, 1084\nOscar, 435\nOsherson, D. N., 759, 1084\nOsindero, S., 1047, 1075\nOsman, I., 112, 1086\nOstland, M., 556, 606, 1085",
  "orientation, 938\norigin function, 545\nOrmoneit, D., 855, 1084\nOR node, 135\nOsawa, E., 195, 1014, 1078\nOsborne, M. J., 688, 1084\nOscar, 435\nOsherson, D. N., 759, 1084\nOsindero, S., 1047, 1075\nOsman, I., 112, 1086\nOstland, M., 556, 606, 1085\nOthello, 186\nOTTER, 360, 364\nOUPM, 545, 552\noutcome, 482, 667\nout of vocabulary, 864\nOverbeek, R., 360, 1092\noverﬁtting, 705, 705–706, 736, 802, 805\novergeneration, 892\noverhypotheses, 798\nOvermars, M., 1013, 1078\noverriding, 456\nOwens, A. J., 156, 1072\nOWL, 469\nP\nP (probability vector), 487\nP(s′ | s, a) (transition model), 646, 832\nPAC learning, 714, 716, 759\nPadgham, L., 59, 1084\nPage, C. D., 800, 1069, 1084\nPage, L., 870, 884, 1067\nPageRank, 870\nPalacios, H., 433, 1084\nPalay, A. J., 191, 1084\nPalmer, D. A., 922, 1084\nPalmer, J., 287, 1080\nPalmer, S., 968, 1084\nPalmieri, G., 761, 1073\nPanini, 16, 919\nPapadimitriou, C. H., 154, 157, 277,\n685, 686, 883, 1059, 1070, 1079,\n1084\nPapadopoulo, T., 968, 1072\nPapavassiliou, V., 855, 1084\nPapert, S., 22, 761, 1082\nPARADISE, 189\nparadox, 471, 641\nAllais, 620\nEllsberg, 620\nSt. Petersburg, 637\nparallel distributed processing, see\nneural network\nparallelism\nAND-, 342\nOR-, 342\nparallel lines, 931\nparallel search, 112\nparameter, 520, 806\nparameter independence, 812\nparametric model, 737\nparamodulation, 354, 359\nParekh, R., 921, 1084\nPareto dominated, 668\nPareto optimal, 668\nParisi, D., 921, 1071\nParisi, G., 555, 1084\nParisi, M. M. G., 278, 1084\nPark, S., 356, 1075\nParker, A., 192, 1084\nParker, D. B., 761, 1084\nParker, L. E., 1013, 1084\nParr, R., 686, 854, 856, 857, 1050, 1074,\n1077–1079, 1084, 1087\nParrod, Y., 432, 1064\nparse tree, 890\nparsing, 892, 892–897\nPartee, B. H., 920, 1086\npartial assignment, 203\npartial evaluation, 799\npartial observability, 180, 658\npartial program, 856\nPARTICLE-FILTERING, 598\nparticle ﬁltering, 597, 598, 603, 605\nRao-Blackwellized, 605, 1012\npartition, 441\npart of, 441\npart of speech, 888\nParzen, E., 827, 1085\nParzen window, 827\nPasca, M., 885, 1071, 1085\nPascal’s wager, 504, 637\nPascal, B., 5, 9, 504\nPasero, R., 314, 358, 1069\nPaskin, M., 920, 1085\nPASSIVE-ADP-AGENT, 834\nPASSIVE-TD-AGENT, 837\npassive learning, 831\nPasula, H., 556, 605, 606, 1081, 1085\nPatashnik, O., 194, 1085\nPatel-Schneider, P., 471, 1064\npath, 67, 108, 403\nloopy, 75\nredundant, 76\npath consistency, 210, 228\npath cost, 68, 108\nPATHFINDER, 552\npath planning, 986\nPatil, R., 471, 894, 920, 1068, 1071\nPatrick, B. G., 111, 1085\nPatrinos, A., 27, 1069\npattern database, 106, 112, 379",
  "path, 67, 108, 403\nloopy, 75\nredundant, 76\npath consistency, 210, 228\npath cost, 68, 108\nPATHFINDER, 552\npath planning, 986\nPatil, R., 471, 894, 920, 1068, 1071\nPatrick, B. G., 111, 1085\nPatrinos, A., 27, 1069\npattern database, 106, 112, 379\ndisjoint, 107\npattern matching, 333\nPaul, R. P., 1013, 1085 1120\nIndex\nPaulin-Mohring, C., 359, 1066\nPaull, M., 277, 1072\nPauls, A., 920, 1085\nPavlovic, V., 553, 1093\nPax-6 gene, 966\npayoff function, 162, 667\nPazzani, M., 505, 826, 1071\nPCFG\nlexicalized, 897, 919, 920\nP controller, 998\nPD controller, 999\nPDDL (Planing Domain Deﬁnition\nLanguage), 367\nPDP (parallel distributed processing),\n761\nPeano, G., 313, 1085\nPeano axioms, 303, 313, 333\nPearce, J., 230, 1085\nPearl, J., 26, 61, 92, 110–112, 154, 191,\n229, 509, 511, 517, 549,\n552–555, 557, 558, 644, 826,\n827, 1070, 1073, 1074, 1076,\n1078, 1085\nPearson, J., 230, 1085\nPEAS description, 40, 42\nPease, A., 469, 1084, 1085\nPecheur, C., 356, 1075\nPednault, E. P. D., 394, 434, 1085\npeeking, 708, 737\nPEGASUS, 850, 852, 859\nPeirce, C. S., 228, 313, 454, 471, 920,\n1085\nPelikan, M., 155, 1085\nPell, B., 60, 432, 1083\nPemberton, J. C., 157, 1085\npenalty, 56\nPenberthy, J. S., 394, 1085\nPeng, J., 855, 1085\nPENGI, 434\npenguin, 435\nPenix, J., 356, 1075\nPennachin, C., 27, 1074\nPennsylvania, Univ. of, 14\nPenn Treebank, 881, 895\nPenrose, R., 1023, 1085\nPentagon Papers, 638\nPeot, M., 433, 554, 1085, 1088\npercept, 34\nperception, 34, 305, 928, 928–965\nperception layer, 1005\nperceptron, 20, 729, 729–731, 761\nconvergence theorem, 20\nlearning rule, 724\nnetwork, 729\nrepresentational power, 22\nsigmoid, 729\npercept schema, 416\npercept sequence, 34, 37\nPereira, F., 28, 339, 341, 470, 759, 761,\n884, 885, 889, 919, 1025, 1071,\n1074, 1079, 1083, 1085, 1088,\n1091\nPereira, L. M., 341, 1091\nPeres, Y., 278, 604, 605, 1064, 1080,\n1081\nPerez, P., 961, 1080\nperfect information, 666\nperfect recall, 675\nperformance element, 55, 56\nperformance measure, 37, 40, 59, 481,\n611\nPerkins, T., 439, 1089\nPerlis, A., 1043, 1085\nPerona, P., 967, 1081\nperpetual punishment, 674\nperplexity, 863\nPerrin, B. E., 605, 1085\npersistence action, 380\npersistence arc, 594\npersistent (variable), 1061\npersistent failure model, 593\nPerson, C., 854, 1083\nperspective, 966\nperspective projection, 930\nPesch, E., 432, 1066\nPeshkin, M., 156, 1092\npessimistic description (of an action),\n412\nPeters, S., 920, 1071\nPeterson, C., 555, 1085\nPetrie, K., 230, 1073\nPetrie, T., 604, 826, 1065\nPetrik, M., 434, 1085\nPetrov, S., 896, 900, 920, 1085",
  "Pesch, E., 432, 1066\nPeshkin, M., 156, 1092\npessimistic description (of an action),\n412\nPeters, S., 920, 1071\nPeterson, C., 555, 1085\nPetrie, K., 230, 1073\nPetrie, T., 604, 826, 1065\nPetrik, M., 434, 1085\nPetrov, S., 896, 900, 920, 1085\nPfeffer, A., 191, 541, 556, 687, 1078,\n1085\nPfeifer, G., 472, 1071\nPfeifer, R., 1041, 1085\nphase transition, 277\nphenomenology, 1026\nPhilips, A. B., 154, 229, 1082\nPhilo of Megara, 275\nphilosophy, 5–7, 59, 1020–1043\nphone (speech sound), 914\nphoneme, 915\nphone model, 915\nphonetic alphabet, 914\nphotometry, 932\nphotosensitive spot, 963\nphrase structure, 888, 919\nphysicalism, 1028, 1041\nphysical symbol system, 18\nPi, X., 604, 1083\nPiccione, C., 687, 1093\nPickwick, Mr., 1026\npictorial structure model, 958\nPID controller, 999\nPieper, G., 360, 1092\npigeons, 13\nPijls, W., 191, 1085\npineal gland, 1027\nPineau, J., 686, 1013, 1085\nPinedo, M., 432, 1085\nping-pong, 32, 830\npinhole camera, 930\nPinkas, G., 229, 1085\nPinker, S., 287, 288, 314, 921, 1085,\n1087\nPinto, D., 885, 1085\nPipatsrisawat, K., 277, 1085\nPippenger, N., 434, 1080\nPisa, tower of, 56\nPistore, M., 275, 1088\npit, bottomless, 237\nPitts, W., 15, 16, 20, 278, 727, 731, 761,\n963, 1080, 1082\npixel, 930\nPL-FC-ENTAILS?, 258\nPL-RESOLUTION, 255\nPlaat, A., 191, 1085\nPlace, U. T., 1041, 1085\nPLAN-ERS1, 432\nPLAN-ROUTE, 270\nplanetary rover, 971\nPLANEX, 434\nPlankalk¨ul, 14\nplan monitoring, 423\nPLANNER, 24, 358\nplanning, 52, 366–436\nand acting, 415–417\nas constraint satisfaction, 390\nas deduction, 388\nas reﬁnement, 390\nas satisﬁability, 387\nblocks world, 20\ncase-based, 432\nconformant, 415, 417–421, 431, 433,\n994\ncontingency, 133, 415, 421–422, 431\ndecentralized, 426\nﬁne-motion, 994\ngraph, 379, 379–386, 393\nserial, 382\nhierarchical, 406–415, 431\nhierarchical task network, 406\nhistory of, 393\nlinear, 394\nmultibody, 425, 426–428 Index\n1121\nmultieffector, 425\nnon-interleaved, 398\nonline, 415\nreactive, 434\nregression, 374, 394\nroute, 19\nsearch space, 373–379\nsensorless, 415, 417–421\nplanning and control layer, 1006\nplan recognition, 429\nPlanSAT, 372\nbounded, 372\nplateau (in local search), 123\nPlato, 275, 470, 1041\nPlatt, J., 760, 1085\nplayer (in a game), 667\nPlotkin, G., 359, 800, 1085\nPlunkett, K., 921, 1071\nply, 164\npoetry, 1\nPohl, I., 110, 111, 118, 1085\npoint-to-point motion, 986\npointwise product, 526\npoker, 507\nPoland, 470\nPoli, R., 156, 1079, 1085\nPolicella, N., 28, 1068\npolicy, 176, 434, 647, 684, 994\nevaluation, 656, 832\ngradient, 849\nimprovement, 656",
  "poetry, 1\nPohl, I., 110, 111, 118, 1085\npoint-to-point motion, 986\npointwise product, 526\npoker, 507\nPoland, 470\nPoli, R., 156, 1079, 1085\nPolicella, N., 28, 1068\npolicy, 176, 434, 647, 684, 994\nevaluation, 656, 832\ngradient, 849\nimprovement, 656\niteration, 656, 656–658, 685, 832\nasynchronous, 658\nmodiﬁed, 657\nloss, 655\noptimal, 647\nproper, 650, 858\nsearch, 848, 848–852, 1002\nstochastic, 848\nvalue, 849\nPOLICY-ITERATION, 657\npolite convention (Turing’s), 1026, 1027\nPollack, M. E., 434, 1069\npolytree, 528, 552, 575\nPOMDP-VALUE-ITERATION, 663\nPomerleau, D. A., 1014, 1085\nPonce, J., 968, 1072\nPonte, J., 884, 922, 1085, 1093\nPoole, D., 2, 59, 553, 556, 639, 1078,\n1085, 1093\nPopat, A. C., 29, 921, 1067\nPopescu, A.-M., 885, 1072\nPopper, K. R., 504, 759, 1086\npopulation (in genetic algorithms), 127\nPorphyry, 471\nPort-Royal Logic, 636\nPorter, B., 473, 1091\nPortner, P., 920, 1086\nPortuguese, 778\npose, 956, 958, 975\nPosegga, J., 359, 1065\npositive example, 698\npositive literal, 244\npositivism, logical, 6\npossibility axiom, 388\npossibility theory, 557\npossible world, 240, 274, 313, 451, 540\nPost, E. L., 276, 1086\npost-decision disappointment, 637\nposterior probability, see probability,\nconditional\npotential ﬁeld, 991\npotential ﬁeld control, 999\nPoultney, C., 762, 1086\nPoundstone, W., 687, 1086\nPourret, O., 553, 1086\nPowers, R., 857, 1088\nPrade, H., 557, 1071\nPrades, J. L. P., 637, 1086\nPradhan, M., 519, 552, 1086\npragmatic interpretation, 904\npragmatics, 904\nPrawitz, D., 358, 1086\nprecedence constraints, 204\nprecision, 869\nprecondition, 367\nmissing, 423\nprecondition axiom, 273\npredecessor, 91\npredicate, 902\npredicate calculus, see logic, ﬁrst-order\npredicate indexing, 328\npredicate symbol, 292\nprediction, 139, 142, 573, 603\npreference, 482, 612\nmonotonic, 616\npreference elicitation, 615\npreference independence, 624\npremise, 244\npresident, 449\nPresley, E., 448\nPress, W. H., 155, 1086\nPreston, J., 1042, 1086\nPrice, B., 686, 1066\nPrice Waterhouse, 431\nPrieditis, A. E., 105, 112, 119, 1083,\n1086\nPrinceton, 17\nPrincipia Mathematica, 18\nPrinz, D. G., 192, 1086\nPRIOR-SAMPLE, 531\nprioritized sweeping, 838, 854\npriority queue, 80, 858\nprior knowledge, 39, 768, 778, 787\nprior probability, 485, 503\nprismatic joint, 976\nprisoner’s dilemma, 668\nprivate value, 679\nprobabilistic network, see Bayesian\nnetwork\nprobabilistic roadmap, 993\nprobability, 9, 26, 480–565, 1057–1058\nalternatives to, 546\naxioms of, 488–490\nconditional, 485, 503, 514\nconjunctive, 514",
  "prisoner’s dilemma, 668\nprivate value, 679\nprobabilistic network, see Bayesian\nnetwork\nprobabilistic roadmap, 993\nprobability, 9, 26, 480–565, 1057–1058\nalternatives to, 546\naxioms of, 488–490\nconditional, 485, 503, 514\nconjunctive, 514\ndensity function, 487, 1057\ndistribution, 487, 522\nhistory, 506\njudgments, 516\nmarginal, 492\nmodel, 484, 1057\nopen-universe, 545\nprior, 485, 503\ntheory, 289, 482, 636\nprobably approximately correct (PAC),\n714, 716, 759\nPROBCUT, 175\nprobit distribution, 522, 551, 554\nproblem, 66, 108\nairport-siting, 643\nassembly sequencing, 74\nbandit, 840, 855\nconformant, 138\nconstraint optimization, 207\n8-queens, 71, 109\n8-puzzle, 102, 105\nformulation, 65, 68–69\nframe, 266, 279\ngenerator, 56\nhalting, 325\ninherently hard, 1054–1055\nmillion queens, 221, 229\nmissionaries and cannibals, 115\nmonkey and bananas, 113, 396\nn queens, 263\noptimization, 121\nconstrained, 132\npiano movers, 1012\nreal-world, 69\nrelaxed, 105, 376\nrobot navigation, 74\nsensorless, 138\nsolving, 22\ntouring, 74\ntoy, 69\ntraveling salesperson, 74\nunderconstrained, 263 1122\nIndex\nVLSI layout, 74, 125\nprocedural approach, 236, 286\nprocedural attachment, 456, 466\nprocess, 447, 447\nPRODIGY, 432\nproduction, 48\nproduction system, 322, 336, 357, 358\nproduct rule, 486, 495\nPROGOL, 789, 795, 797, 800\nprogramming language, 285\nprogression, 393\nProlog, 24, 339, 358, 394, 793, 899\nparallel, 342\nProlog Technology Theorem Prover\n(PTTP), 359\npronunciation model, 917\nproof, 250\nproper policy, 650, 858\nproperty (unary relation), 288\nproposal distribution, 565\nproposition\nprobabilistic, 483\nsymbol, 244\npropositional attitude, 450\npropositionalization, 324, 357, 368, 544\npropositional logic, 235, 243–247, 274,\n286\nproprioceptive sensor, 975\nPROSPECTOR, 557\nProsser, P., 229, 1086\nprotein design, 75\nprototypes, 896\nProust, M., 910\nProvan, G. M., 519, 552, 1086\npruning, 98, 162, 167, 705\nforward, 174\nfutility, 185\nin contingency problems, 179\nin EBL, 783\npseudocode, 1061\npseudoexperience, 837\npseudoreward, 856\nPSPACE, 372, 1055\nPSPACE-complete, 385, 393\npsychological reasoning, 473\npsychology, 12–13\nexperimental, 3, 12\npsychophysics, 968\npublic key encryption, 356\nPuget, J.-F., 230, 800, 1073, 1087\nPullum, G. K., 889, 920, 921, 1076,\n1086\nPUMA, 1011\nPurdom, P., 230, 1067\npure strategy, 667\npure symbol, 260\nPuterman, M. L., 60, 685, 1086\nPutnam, H., 60, 260, 276, 350, 358, 505,\n1041, 1042, 1070, 1086\nPuzicha, J., 755, 762, 1065\nPylyshyn, Z. W., 1041, 1086\nQ\nQ(s, a) (value of action in state), 843",
  "pure strategy, 667\npure symbol, 260\nPuterman, M. L., 60, 685, 1086\nPutnam, H., 60, 260, 276, 350, 358, 505,\n1041, 1042, 1070, 1086\nPuzicha, J., 755, 762, 1065\nPylyshyn, Z. W., 1041, 1086\nQ\nQ(s, a) (value of action in state), 843\nQ-function, 627, 831\nQ-learning, 831, 843, 844, 848, 973\nQ-LEARNING-AGENT, 844\nQA3, 314\nQALY, 616, 637\nQi, R., 639, 1093\nQUACKLE, 187\nquadratic dynamical systems, 155\nquadratic programming, 746\nqualia, 1033\nqualiﬁcation problem, 268, 481, 1024,\n1025\nqualitative physics, 444, 472\nqualitative probabilistic network, 557,\n624\nquantiﬁcation, 903\nquantiﬁer, 295, 313\nexistential, 297\nin logic, 295–298\nnested, 297–298\nuniversal, 295–296, 322\nquantization factor, 914\nquasi-logical form, 904\nQubic, 194\nquery (logical), 301\nquery language, 867\nquery variable, 522\nquestion answering, 872, 883\nqueue, 79\nFIFO, 80, 81\nLIFO, 80, 85\npriority, 80, 858\nQuevedo, T., 190\nquiescence, 174\nQuillian, M. R., 471, 1086\nQuine, W. V., 314, 443, 469, 470, 1086\nQuinlan, J. R., 758, 764, 791, 793, 800,\n1086\nQuirk, R., 920, 1086\nQXTRACT, 885\nR\nR1, 24, 336, 358\nRabani, Y., 155, 1086\nRabenau, E., 28, 1068\nRabideau, G., 431, 1073\nRabiner, L. R., 604, 922, 1086\nRabinovich, Y., 155, 1086\nracing cars, 1050\nradar, 10\nradial basis function, 762\nRadio Rex, 922\nRaedt, L. D., 556, 1078\nRaghavan, P., 883, 884, 1081, 1084\nRaiffa, H., 9, 621, 625, 638, 687, 1078,\n1081\nRajan, K., 28, 60, 431, 1064, 1077\nRalaivola, L., 605, 1085\nRalphs, T. K., 112, 1086\nRamakrishnan, R., 275, 1080\nRamanan, D., 960, 1086\nRamsey, F. P., 9, 504, 637, 1086\nRAND Corporation, 638\nrandomization, 35, 50\nrandomized weighted majority\nalgorithm, 752\nrandom restart, 158, 262\nrandom set, 551\nrandom surfer model, 871\nrandom variable, 486, 515\ncontinuous, 487, 519, 553\nindexed, 555\nrandom walk, 150, 585\nrange ﬁnder, 973\nlaser, 974\nrange sensor array, 981\nRanzato, M., 762, 1086\nRao, A., 61, 1092\nRao, B., 604, 1076\nRao, G., 678\nRaphael, B., 110, 191, 358, 1074, 1075\nRaphson, J., 154, 760, 1086\nrapid prototyping, 339\nRaschke, U., 1013, 1069\nRashevsky, N., 10, 761, 1086\nRasmussen, C. E., 827, 1086\nRassenti, S., 688, 1086\nRatio Club, 15\nrational agent, 4, 4–5, 34, 36–38, 59, 60,\n636, 1044\nrationalism, 6, 923\nrationality, 1, 36–38\ncalculative, 1049\nlimited, 5\nperfect, 5, 1049\nrational thought, 4\nRatner, D., 109, 1086\nrats, 13\nRauch, H. E., 604, 1086\nRayner, M., 784, 1087\nRayson, P., 921, 1080\nRayward-Smith, V., 112, 1086 Index\n1123\nRBFS, 99–101, 109\nRBL, 779, 784–787, 798\nRDF, 469\nreachable set, 411",
  "perfect, 5, 1049\nrational thought, 4\nRatner, D., 109, 1086\nrats, 13\nRauch, H. E., 604, 1086\nRayner, M., 784, 1087\nRayson, P., 921, 1080\nRayward-Smith, V., 112, 1086 Index\n1123\nRBFS, 99–101, 109\nRBL, 779, 784–787, 798\nRDF, 469\nreachable set, 411\nreactive control, 1001\nreactive layer, 1004\nreactive planning, 434\nreal-world problem, 69\nrealizability, 697\nreasoning, 4, 19, 234\ndefault, 458–460, 547\nintercausal, 548\nlogical, 249–264, 284\nuncertain, 26\nrecall, 869\nRechenberg, I., 155, 1086\nrecognition, 929\nrecommendation, 539\nreconstruction, 929\nrecurrent network, 729, 762\nRECURSIVE-BEST-FIRST-SEARCH, 99\nRECURSIVE-DLS, 88\nrecursive deﬁnition, 792\nrecursive estimation, 571\nReddy, R., 922, 1081\nreduction, 1059\nReeson, C. G., 228, 1086\nReeves, C., 112, 1086\nReeves, D., 688, 1092\nreference class, 491, 505\nreference controller, 997\nreference path, 997\nreferential transparency, 451\nreﬁnement (in hierarchical planning),\n407\nreﬂectance, 933, 952\nREFLEX-VACUUM-AGENT, 48\nreﬂex agent, 48, 48–50, 59, 647, 831\nrefutation, 250\nrefutation completeness, 350\nregex, 874\nRegin, J., 228, 1086\nregions, 941\nregression, 393, 696, 760\nlinear, 718, 810\nnonlinear, 732\ntree, 707\nregression to the mean, 638\nregret, 620, 752\nregular expression, 874\nregularization, 713, 721\nReichenbach, H., 505, 1086\nReid, D. B., 606, 1086\nReid, M., 111, 1079\nReif, J., 1012, 1013, 1068, 1086\nreiﬁcation, 440\nREINFORCE, 849, 859\nreinforcement, 830\nreinforcement learning, 685, 695,\n830–859, 1025\nactive, 839–845\nBayesian, 835\ndistributed, 856\ngeneralization in, 845–848\nhierarchical, 856, 1046\nmultiagent, 856\noff-policy, 844\non-policy, 844\nReingold, E. M., 228, 1066\nReinsel, G., 604, 1066\nReiter, R., 279, 395, 471, 686, 1066,\n1086\nREJECTION-SAMPLING, 533\nrejection sampling, 532\nrelation, 288\nrelational extraction, 874\nrelational probability model (RPM),\n541, 552\nrelational reinforcement learning, 857\nrelative error, 98\nrelaxed problem, 105, 376\nrelevance, 246, 375, 779, 799\nrelevance (in information retrieval), 867\nrelevance-based learning (RBL), 779,\n784–787, 798\nrelevant-states, 374\nRemote Agent, 28, 60, 356, 392, 432\nREMOTE AGENT, 28\nrenaming, 331\nrendering model, 928\nRenner, G., 155, 1086\nR´enyi, A., 504, 1086\nrepeated game, 669, 673\nreplanning, 415, 422–434\nREPOP, 394\nrepresentation, see knowledge\nrepresentation\natomic, 57\nfactored, 58\nstructured, 58\nrepresentation theorem, 624\nREPRODUCE, 129\nreserve bid, 679\nresolution, 19, 21, 253, 252–256, 275,\n314, 345–357, 801\nclosure, 255, 351",
  "REPOP, 394\nrepresentation, see knowledge\nrepresentation\natomic, 57\nfactored, 58\nstructured, 58\nrepresentation theorem, 624\nREPRODUCE, 129\nreserve bid, 679\nresolution, 19, 21, 253, 252–256, 275,\n314, 345–357, 801\nclosure, 255, 351\ncompleteness proof for, 350\ninput, 356\ninverse, 794, 794–797, 800\nlinear, 356\nstrategies, 355–356\nresolvent, 252, 347, 794\nresource constraints, 401\nresources, 401–405, 430\nresponse, 13\nrestaurant hygiene inspector, 183\nresult, 368\nresult set, 867\nrete, 335, 358\nretrograde, 176\nreusable resource, 402\nrevelation principle, 680\nrevenue equivalence theorem, 682\nReversi, 186\nrevolute joint, 976\nreward, 56, 646, 684, 830\nadditive, 649\ndiscounted, 649\nshaping, 856\nreward-to-go, 833\nreward function, 832, 1046\nrewrite rule, 364, 1060\nReynolds, C. W., 435, 1086\nRiazanov, A., 359, 360, 1086\nRibeiro, F., 195, 1091\nRice, T. R., 638, 1082\nRich, E., 2, 1086\nRichards, M., 195, 1086\nRichardson, M., 556, 604, 1071, 1086\nRichardson, S., 554, 1073\nRichter, S., 395, 1075, 1086\nridge (in local search), 123\nRidley, M., 155, 1086\nRieger, C., 24, 1086\nRiesbeck, C., 23, 358, 921, 1068, 1088\nright thing, doing the, 1, 5, 1049\nRiley, J., 688, 1087\nRiley, M., 889, 1083\nRiloff, E., 885, 1077, 1087\nRink, F. J., 553, 1083\nRintanen, J., 433, 1087\nRipley, B. D., 763, 1087\nrisk aversion, 617\nrisk neutrality, 618\nrisk seeking, 617\nRissanen, J., 759, 1087\nRitchie, G. D., 800, 1087\nRitov, Y., 556, 606, 1085\nRivest, R., 759, 1059, 1069, 1087\nRMS (root mean square), 1059\nRobbins algebra, 360\nRoberts, G., 30, 1071\nRoberts, L. G., 967, 1087\nRoberts, M., 192, 1065\nRobertson, N., 229, 1087\nRobertson, S., 868\nRobertson, S. E., 505, 884, 1069, 1087\nRobinson, A., 314, 358, 360, 1087 1124\nIndex\nRobinson, G., 359, 1092\nRobinson, J. A., 19, 276, 314, 350, 358,\n1087\nRobocup, 1014\nrobot, 971, 1011\ngame (with humans), 1019\nhexapod, 1001\nmobile, 971\nnavigation, 74\nsoccer, 161, 434, 1009\nrobotics, 3, 592, 971–1019\nrobust control, 994\nRoche, E., 884, 1087\nRochester, N., 17, 18, 1020, 1082\nRock, I., 968, 1087\nRockefeller Foundation, 922\nR¨oger, G., 111, 1075\nrollout, 180\nRomania, 65, 203\nRoomba, 1009\nRoossin, P., 922, 1067\nroot mean square, 1059\nRoscoe, T., 275, 1080\nRosenblatt, F., 20, 761, 1066, 1087\nRosenblatt, M., 827, 1087\nRosenblitt, D., 394, 1081\nRosenbloom, P. S., 26, 27, 336, 358,\n432, 799, 1047, 1075, 1079\nRosenblueth, A., 15, 1087\nRosenbluth, A., 155, 554, 1082\nRosenbluth, M., 155, 554, 1082\nRosenholtz, R., 953, 968, 1081\nRosenschein, J. S., 688, 1087, 1089",
  "Rosenblitt, D., 394, 1081\nRosenbloom, P. S., 26, 27, 336, 358,\n432, 799, 1047, 1075, 1079\nRosenblueth, A., 15, 1087\nRosenbluth, A., 155, 554, 1082\nRosenbluth, M., 155, 554, 1082\nRosenholtz, R., 953, 968, 1081\nRosenschein, J. S., 688, 1087, 1089\nRosenschein, S. J., 60, 278, 279, 1077,\n1087\nRoss, P. E., 193, 1087\nRoss, S. M., 1059, 1087\nRossi, F., 228, 230, 1066, 1087\nrotation, 956\nRoth, D., 556, 1070\nRoughgarden, T., 688, 1084\nRoussel, P., 314, 358, 359, 1069, 1087\nroute ﬁnding, 73\nRouveirol, C., 800, 1087\nRoveri, M., 396, 433, 1066, 1068\nRowat, P. F., 1013, 1087\nRoweis, S. T., 554, 605, 1087\nRowland, J., 797, 1078\nRowley, H., 968, 1087\nRoy, N., 1013, 1087\nRozonoer, L., 760, 1064\nRPM, 541, 552\nRSA (Rivest, Shamir, and Adelman),\n356\nRSAT, 277\nRubik’s Cube, 105\nRubin, D., 604, 605, 826, 827, 1070,\n1073, 1087\nRubinstein, A., 688, 1084\nrule, 244\ncausal, 317, 517\ncondition–action, 48\ndefault, 459\ndiagnostic, 317, 517\nif–then, 48, 244\nimplication, 244\nsituation–action, 48\nuncertain, 548\nrule-based system, 547, 1024\nwith uncertainty, 547–549\nRumelhart, D. E., 24, 761, 1087\nRummery, G. A., 855, 1087\nRuspini, E. H., 557, 1087\nRussell, A., 111, 1071\nRussell, B., 6, 16, 18, 357, 1092\nRussell, J. G. B., 637, 1087\nRussell, J. R., 360, 1083\nRussell, S. J., 111, 112, 157, 191, 192,\n198, 278, 345, 432, 444, 556,\n604–606, 686, 687, 799, 800,\n826, 855–857, 1012, 1048, 1050,\n1064, 1066, 1069–1071, 1073,\n1076, 1077, 1081–1085, 1087,\n1090, 1092, 1093\nRussia, 21, 192, 489\nRustagi, J. S., 554, 1087\nRuzzo, W. L., 920, 1074\nRyan, M., 314, 1076\nRYBKA, 186, 193\nRzepa, H. S., 469, 1083\nS\nS-set, 774\nSabharwal, A., 277, 395, 1074, 1076\nSabin, D., 228, 1087\nSacerdoti, E. D., 394, 432, 1087\nSackinger, E., 762, 967, 1080\nSadeh, N. M., 688, 1064\nSadri, F., 470, 1087\nSagiv, Y., 358, 1065\nSahami, M., 29, 883, 884, 1078, 1087\nSahin, N. T., 288, 1087\nSahni, S., 110, 1076\nSAINT, 19, 156\nSt. Petersburg paradox, 637, 641\nSakuta, M., 192, 1087\nSalisbury, J., 1013, 1081\nSalmond, D. J., 605, 1074\nSalomaa, A., 919, 1087\nSalton, G., 884, 1087\nSaltzman, M. J., 112, 1086\nSAM, 360\nsample complexity, 715\nsample space, 484\nsampling, 530–535\nsampling rate, 914\nSamuel, A. L., 17, 18, 61, 193, 850,\n854, 855, 1087\nSamuelson, L., 688, 1081\nSamuelson, W., 688, 1087\nSamuelsson, C., 784, 1087\nSanders, P., 112, 1069\nSankaran, S., 692, 1080\nSanna, R., 761, 1073\nSanskrit, 468, 919\nSantorini, B., 895, 921, 1081\nSAPA, 431\nSapir–Whorf hypothesis, 287\nSaraswat, V., 228, 1091\nSarawagi, S., 885, 1087\nSARSA, 844",
  "Samuelsson, C., 784, 1087\nSanders, P., 112, 1069\nSankaran, S., 692, 1080\nSanna, R., 761, 1073\nSanskrit, 468, 919\nSantorini, B., 895, 921, 1081\nSAPA, 431\nSapir–Whorf hypothesis, 287\nSaraswat, V., 228, 1091\nSarawagi, S., 885, 1087\nSARSA, 844\nSastry, S., 60, 606, 852, 857, 1013,\n1075, 1084\nSAT, 250\nSatia, J. K., 686, 1087\nsatisfaction (in logic), 240\nsatisﬁability, 250, 277\nsatisﬁability threshold conjecture, 264,\n278\nsatisﬁcing, 10, 1049\nSATMC, 279\nSato, T., 359, 556, 1087, 1090\nSATPLAN, 387, 392, 396, 402, 420, 433\nSATPLAN, 272\nsaturation, 351\nSATZ, 277\nSaul, L. K., 555, 606, 1077, 1088\nSaund, E., 883, 1087\nSavage, L. J., 489, 504, 637, 1088\nSayre, K., 1020, 1088\nscaled orthographic projection, 932\nscanning lidars, 974\nScarcello, F., 230, 472, 1071, 1074\nscene, 929\nSchabes, Y., 884, 1087\nSchaeffer, J., 112, 186, 191, 194, 195,\n678, 687, 1066, 1069, 1081,\n1085, 1088\nSchank, R. C., 23, 921, 1088\nSchapire, R. E., 760, 761, 884, 1072,\n1088\nScharir, M., 1012, 1088\nSchaub, T., 471, 1070\nSchauenberg, T., 678, 687, 1066\nscheduling, 403, 401–405\nScheines, R., 826, 1089\nschema (in a genetic algorithm), 128 Index\n1125\nschema acquisition, 799\nSchervish, M. J., 506, 1070\nSchickard, W., 5\nSchmid, C., 968, 1088\nSchmidt, G., 432, 1066\nSchmolze, J. G., 471, 1088\nSchneider, J., 852, 1013, 1065\nSchnitzius, D., 432, 1070\nSchnizlein, D., 687, 1091\nSchoenberg, I. J., 761, 1083\nSch¨olkopf, B., 760, 762, 1069, 1070,\n1088\nSchomer, D., 288, 1087\nSch¨oning, T., 277, 1088\nSchoppers, M. J., 434, 1088\nSchrag, R. C., 230, 277, 1065\nSchr¨oder, E., 276, 1088\nSchubert, L. K., 469, 1076\nSchulster, J., 28, 1068\nSchultz, W., 854, 1088\nSchultze, P., 112, 1079\nSchulz, D., 606, 1012, 1067, 1088\nSchulz, S., 360, 1088, 1090\nSchumann, J., 359, 360, 1071, 1080\nSch¨utze, H., 883–885, 920, 921, 1081,\n1088\nSch¨utze, H., 862, 883, 1078\nSchwartz, J. T., 1012, 1088\nSchwartz, S. P., 469, 1088\nSchwartz, W. B., 505, 1074\nscientiﬁc discovery, 759\nScott, D., 555, 1088\nScrabble, 187, 195\nscruffy vs. neat, 25\nsearch, 22, 52, 66, 108\nA*, 93–99\nalpha–beta, 167–171, 189, 191\nB*, 191\nbacktracking, 87, 215, 218–220, 222,\n227\nbeam, 125, 174\nbest-ﬁrst, 92, 108\nbidirectional, 90–112\nbreadth-ﬁrst, 81, 81–83, 108, 408\nconformant, 138–142\ncontinuous space, 129–133, 155\ncurrent-best-hypothesis, 770\ncutting off, 173–175\ndepth-ﬁrst, 85, 85–87, 108, 408\ndepth-limited, 87, 87–88\ngeneral, 108\ngreedy best-ﬁrst, 92, 92\nheuristic, 81, 110\nhill-climbing, 122–125, 150\nin a CSP, 214–222\nincremental belief-state, 141",
  "current-best-hypothesis, 770\ncutting off, 173–175\ndepth-ﬁrst, 85, 85–87, 108, 408\ndepth-limited, 87, 87–88\ngeneral, 108\ngreedy best-ﬁrst, 92, 92\nheuristic, 81, 110\nhill-climbing, 122–125, 150\nin a CSP, 214–222\nincremental belief-state, 141\ninformed, 64, 81, 92, 92–102, 108\nInternet, 464\niterative deepening, 88, 88–90, 108,\n110, 173, 408\niterative deepening A*, 99, 111\nlearning to, 102\nlocal, 120–129, 154, 229, 262–263,\n275, 277\ngreedy, 122\nlocal, for CSPs, 220–222\nlocal beam, 125, 126\nmemory-bounded, 99–102, 111\nmemory-bounded A*, 101, 101–102,\n112\nminimax, 165–168, 188, 189\nnondeterministic, 133–138\nonline, 147, 147–154, 157\nparallel, 112\npartially observable, 138–146\npolicy, 848, 848–852, 1002\nquiescence, 174\nreal-time, 157, 171–175\nrecursive best-ﬁrst (RBFS), 99–101,\n111\nsimulated annealing, 125\nstochastic beam, 126\nstrategy, 75\ntabu, 154, 222\ntree, 163\nuniform-cost, 83, 83–85, 108\nuninformed, 64, 81, 81–91, 108, 110\nsearch cost, 80\nsearch tree, 75, 163\nSearle, J. R., 11, 1027, 1029–1033,\n1042, 1088\nSebastiani, F., 884, 1088\nSegaran, T., 688, 763, 1088\nsegmentation (of an image), 941\nsegmentation (of words), 886, 913\nSejnowski, T., 763, 850, 854, 1075,\n1083, 1090\nSelf, M., 826, 1068\nSelfridge, O. G., 17\nSelman, B., 154, 229, 277, 279, 395,\n471, 1074, 1077, 1078, 1088\nsemantic interpretation, 900–904, 920\nsemantic networks, 453–456, 468, 471\nsemantics, 240, 860\ndatabase, 300, 343, 367, 540\nlogical, 274\nSemantic Web, 469\nsemi-supervised learning, 695\nsemidecidable, 325, 357\nsemidynamic environment, 44\nSen, S., 855, 1084\nsensitivity analysis, 635\nsensor, 34, 41, 928\nactive, 973\nfailure, 592, 593\nmodel, 579, 586, 603\npassive, 973\nsensor interface layer, 1005\nsensorless planning, 415, 417–421\nsensor model, 566, 579, 586, 603, 658,\n928, 979\nsentence\natomic, 244, 294–295, 299\ncomplex, 244, 295\nin a KB, 235, 274\nas physical conﬁguration, 243\nseparator (in Bayes net), 499\nsequence form, 677\nsequential\nenvironment, 43\nsequential decision problem, 645–651,\n685\nsequential environment, 43\nsequential importance-sampling\nresampling, 605\nserendipity, 424\nSergot, M., 470, 1079\nserializable subgoals, 392\nSerina, I., 395, 1073\nSestoft, P., 799, 1077\nset (in ﬁrst-order logic), 304\nset-cover problem, 376\nSETHEO, 359\nset of support, 355\nset semantics, 367\nSettle, L., 360, 1074\nSeymour, P. D., 229, 1087\nSGP, 395, 433\nSGPLAN, 387\nSha, F., 1025, 1088\nShachter, R. D., 517, 553, 554, 559, 615,\n634, 639, 687, 1071, 1088, 1090\nshading, 933, 948, 952–953\nshadow, 934",
  "set of support, 355\nset semantics, 367\nSettle, L., 360, 1074\nSeymour, P. D., 229, 1087\nSGP, 395, 433\nSGPLAN, 387\nSha, F., 1025, 1088\nShachter, R. D., 517, 553, 554, 559, 615,\n634, 639, 687, 1071, 1088, 1090\nshading, 933, 948, 952–953\nshadow, 934\nShafer, G., 557, 1088\nshaft decoder, 975\nShah, J., 967, 1083\nShahookar, K., 110, 1088\nShaked, T., 885, 1072\nShakey, 19, 60, 156, 393, 397, 434, 1011\nShalla, L., 359, 1092\nShanahan, M., 470, 1088\nShankar, N., 360, 1088\nShannon, C. E., 17, 18, 171, 192, 703,\n758, 763, 883, 913, 1020, 1082,\n1088\nShaparau, D., 275, 1088\nshape, 957 1126\nIndex\nfrom shading, 968\nShapiro, E., 800, 1088\nShapiro, S. C., 31, 1088\nShapley, S., 687, 1088\nSharir, M., 1013, 1074\nSharp, D. H., 761, 1069\nShatkay, H., 1012, 1088\nShaw, J. C., 109, 191, 276, 1084\nShawe-Taylor, J., 760, 1069\nShazeer, N. M., 231, 1080\nShelley, M., 1037, 1088\nSheppard, B., 195, 1088\nShewchuk, J., 1012, 1070\nShi, J., 942, 967, 1088\nShieber, S., 30, 919, 1085, 1088\nShimelevich, L. I., 605, 1093\nShin, M. C., 685, 1086\nShinkareva, S. V., 288, 1082\nShmoys, D. B., 110, 405, 432, 1080\nShoham, Y., 60, 195, 230, 359, 435,\n638, 688, 857, 1064, 1079, 1080,\n1088\nshort-term memory, 336\nshortest path, 114\nShortliffe, E. H., 23, 557, 1067, 1088\nshoulder (in state space), 123\nShpitser, I., 556, 1085\nSHRDLU, 20, 23, 370\nShreve, S. E., 60, 1066\nsibyl attack, 541\nsideways move (in state space), 123\nSietsma, J., 762, 1088\nSIGART, 31\nsigmoid function, 726\nsigmoid perceptron, 729\nsignal processing, 915\nsigniﬁcance test, 705\nsigns, 888\nSiklossy, L., 432, 1088\nSilver, D., 194, 1073\nSilverstein, C., 884, 1088\nSimard, P., 762, 967, 1080\nSimmons, R., 605, 1012, 1088, 1091\nSimon’s predictions, 20\nSimon, D., 60, 1088\nSimon, H. A., 3, 10, 17, 18, 30, 60, 109,\n110, 191, 276, 356, 393, 639,\n800, 1049, 1077, 1079, 1084,\n1088, 1089\nSimon, J. C., 277, 1089\nSimonis, H., 228, 1089\nSimons, P., 472, 1084\nSIMPLE-REFLEX-AGENT, 49\nsimplex algorithm, 155\nSIMULATED-ANNEALING, 126\nsimulated annealing, 120, 125, 153, 155,\n158, 536\nsimulation of world, 1028\nsimultaneous localization and mapping\n(SLAM), 982\nSinclair, A., 124, 155, 1081, 1086\nSinger, P. W., 1035, 1089\nSinger, Y., 604, 884, 1072, 1088\nSingh, M. P., 61, 1076\nSingh, P., 27, 439, 1082, 1089\nSingh, S., 1014, 1067\nSingh, S. P., 157, 685, 855, 856, 1065,\n1077, 1078, 1090\nSinghal, A., 870, 1089\nsingly connected network, 528\nsingular, 1056\nsingular extension, 174\nsingularity, 12\ntechnological, 1038\nsins, seven deadly, 122\nSIPE, 431, 432, 434\nSIR, 605",
  "Singh, S. P., 157, 685, 855, 856, 1065,\n1077, 1078, 1090\nSinghal, A., 870, 1089\nsingly connected network, 528\nsingular, 1056\nsingular extension, 174\nsingularity, 12\ntechnological, 1038\nsins, seven deadly, 122\nSIPE, 431, 432, 434\nSIR, 605\nSittler, R. W., 556, 606, 1089\nsituated agent, 1025\nsituation, 388\nsituation calculus, 279, 388, 447\nSjolander, K., 604, 1079\nskeletonization, 986, 991\nSkinner, B. F., 15, 60, 1089\nSkolem, T., 314, 358, 1089\nSkolem constant, 323, 357\nSkolem function, 346, 358\nskolemization, 323, 346\nslack, 403\nSlagle, J. R., 19, 1089\nSLAM, 982\nslant, 957\nSlate, D. J., 110, 1089\nSlater, E., 192, 1089\nSlattery, S., 885, 1069\nSleator, D., 920, 1089\nsliding-block puzzle, 71, 376\nsliding window, 943\nSlocum, J., 109, 1089\nSloman, A., 27, 1041, 1082, 1089\nSlovic, P., 2, 638, 1077\nsmall-scale learning, 712\nSmallwood, R. D., 686, 1089\nSmarr, J., 883, 1078\nSmart, J. J. C., 1041, 1089\nSMA∗, 109\nSmith, A., 9\nSmith, A. F. M., 605, 811, 826, 1065,\n1074, 1090\nSmith, B., 28, 60, 431, 470, 1077, 1089\nSmith, D. A., 920, 1089\nSmith, D. E., 156, 157, 345, 359, 363,\n395, 433, 1067, 1073, 1079,\n1085, 1089, 1091\nSmith, G., 112, 1086\nSmith, J. E., 619, 637, 1089\nSmith, J. M., 155, 688, 1089\nSmith, J. Q., 638, 639, 1084, 1089\nSmith, M. K., 469, 1089\nSmith, R. C., 1012, 1089\nSmith, R. G., 61, 1067\nSmith, S. J. J., 187, 195, 1089\nSmith, V., 688, 1086\nSmith, W. D., 191, 553, 1065, 1083\nSMODELS, 472\nSmola, A. J., 760, 1088\nSmolensky, P., 24, 1089\nsmoothing, 574–576, 603, 822, 862,\n863, 938\nlinear interpolation, 863\nonline, 580\nSmullyan, R. M., 314, 1089\nSmyth, P., 605, 763, 1074, 1089\nSNARC, 16\nSnell, J., 506, 1074\nSnell, M. B., 1032, 1089\nSNLP, 394\nSnyder, W., 359, 1064\nSOAR, 26, 336, 358, 432, 799, 1047\nsoccer, 195\nsocial laws, 429\nsociety of mind, 434\nSocrates, 4\nSoderland, S., 394, 469, 885, 1065,\n1072, 1089\nsoftbot, 41, 61\nsoft margin, 748\nsoftmax function, 848\nsoft threshold, 521\nsoftware agent, 41\nsoftware architecture, 1003\nSoika, M., 1012, 1066\nSolomonoff, R. J., 17, 27, 759, 1089\nsolution, 66, 68, 108, 134, 203, 668\noptimal, 68\nsolving games, 163–167\nsoma, 11\nSompolinsky, H., 761, 1064\nsonar sensors, 973\nSondik, E. J., 686, 1089\nsonnet, 1026\nSonneveld, D., 109, 1089\nSontag, D., 556, 1082\nS¨orensson, N., 277, 1071\nSosic, R., 229, 1089\nsoul, 1041 Index\n1127\nsoundness (of inference), 242, 247, 258,\n274, 331\nsour grapes, 37\nSowa, J., 473, 1089\nSpaan, M. T. J., 686, 1089\nspace complexity, 80, 108\nspacecraft assembly, 432\nspam detection, 865\nspam email, 886",
  "Sosic, R., 229, 1089\nsoul, 1041 Index\n1127\nsoundness (of inference), 242, 247, 258,\n274, 331\nsour grapes, 37\nSowa, J., 473, 1089\nSpaan, M. T. J., 686, 1089\nspace complexity, 80, 108\nspacecraft assembly, 432\nspam detection, 865\nspam email, 886\nSparck Jones, K., 505, 868, 884, 1087\nsparse model, 721\nsparse system, 515\nSPASS, 359\nspatial reasoning, 473\nspatial substance, 447\nspecialization, 771, 772\nspecies, 25, 130, 439–441, 469, 817,\n860, 888, 948, 1035, 1042\nspectrophotometry, 935\nspecularities, 933\nspecular reﬂection, 933\nspeech act, 904\nspeech recognition, 25, 912, 912–919,\n922\nsphex wasp, 39, 425\nSPI (Symbolic Probabilistic Inference),\n553\nSpiegelhalter, D. J., 553–555, 639, 763,\n826, 1069, 1073, 1080, 1082,\n1089\nSpielberg, S., 1040, 1089\nSPIKE, 432\nSPIN, 356\nspin glass, 761\nSpirtes, P., 826, 1089\nsplit point, 707\nSproull, R. F., 639, 1072\nSputnik, 21\nsquare roots, 47\nSRI, 19, 314, 393, 638\nSrinivasan, A., 797, 800, 1084, 1089\nSrinivasan, M. V., 1045, 1072\nSrivas, M., 356, 1089\nSrivastava, B., 432, 1077\nSSD (sum of squared differences), 940\nSSS* algorithm, 191\nStaab, S., 469, 1089\nstability\nof a controller, 998\nstatic vs. dynamic, 977\nstrict, 998\nstack, 80\nStader, J., 432, 1064\nSTAGE, 154\nSTAHL, 800\nStallman, R. M., 229, 1089\nSTAN, 395\nstandardizing apart, 327, 363, 375\nStanﬁll, C., 760, 1089\nStanford University, 18, 19, 22, 23, 314\nStanhope Demonstrator, 276\nStaniland, J. R., 505, 1070\nSTANLEY, 28, 1007, 1008, 1014, 1025\nstart symbol, 1060\nstate, 367\nrepeated, 75\nworld, 69\nState-Action-Reward-State-Action\n(SARSA), 844\nstate abstraction, 377\nstate estimation, 145, 181, 269, 275,\n570, 978\nrecursive, 145, 571\nStates, D. J., 826, 1076\nstate space, 67, 108\nmetalevel, 102\nstate variable\nmissing, 423\nstatic environment, 44\nstationarity (for preferences), 649\nstationarity assumption, 708\nstationary distribution, 537, 573\nstationary process, 568, 568–570, 603\nstatistical mechanics, 761\nSteﬁk, M., 473, 557, 1089\nStein, J., 553, 1083\nStein, L. A., 1051, 1089\nStein, P., 192, 1078\nSteiner, W., 1012, 1067\nstemming, 870\nStensrud, B., 358, 1090\nstep cost, 68\nStephenson, T., 604, 1089\nstep size, 132\nstereopsis, binocular, 948\nstereo vision, 974\nStergiou, K., 228, 1089\nStern, H. S., 827, 1073\nSternberg, M. J. E., 797, 1089, 1090\nStickel, M. E., 277, 359, 884, 921, 1075,\n1076, 1089, 1093\nstiff neck, 496\nStiller, L., 176, 1089\nstimulus, 13\nStob, M., 759, 1084\nstochastic beam search, 126\nstochastic dominance, 622, 636\nstochastic environment, 43\nstochastic games, 177",
  "Stickel, M. E., 277, 359, 884, 921, 1075,\n1076, 1089, 1093\nstiff neck, 496\nStiller, L., 176, 1089\nstimulus, 13\nStob, M., 759, 1084\nstochastic beam search, 126\nstochastic dominance, 622, 636\nstochastic environment, 43\nstochastic games, 177\nstochastic gradient descent, 720\nStockman, G., 191, 1089\nStoffel, K., 469, 1089\nStoica, I., 275, 1080\nStoic school, 275\nStokes, I., 432, 1064\nStolcke, A., 920, 1089\nStoljar, D., 1042, 1081\nStone, C. J., 758, 1067\nStone, M., 759, 1089\nStone, P., 434, 688, 1089\nStork, D. G., 763, 827, 966, 1071, 1089\nStory, W. E., 109, 1077\nStrachey, C., 14, 192, 193, 1089, 1090\nstraight-line distance, 92\nStrat, T. M., 557, 1087\nstrategic form, 667\nstrategy, 133, 163, 181, 667\nstrategy proﬁle, 667\nStratonovich, R. L., 604, 639, 1089\nstrawberries, enjoy, 1021\nStriebel, C. T., 604, 1086\nstring (in logic), 471\nSTRIPS, 367, 393, 394, 397, 432, 434,\n799\nStroham, T., 884, 1069\nStrohm, G., 432, 1072\nstrong AI, 1020, 1026–1033, 1040\nstrong domination, 668\nstructured representation, 58, 64\nStuckey, P. J., 228, 359, 1077, 1081\nSTUDENT, 19\nstuff, 445\nstupid pet tricks, 39\nStutz, J., 826, 1068\nstylometry, 886\nSu, Y., 111, 1071\nsubcategory, 440\nsubgoal independence, 378\nsubjective case, 899\nsubjectivism, 491\nsubmodularity, 644\nsubproblem, 106\nSubrahmanian, V. S., 192, 1084\nSubramanian, D., 278, 472, 799, 1050,\n1068, 1087, 1089, 1090\nsubstance, 445\nspatial, 447\ntemporal, 447\nsubstitutability (of lotteries), 612\nsubstitution, 301, 323\nsubsumption\nin description logic, 456\nin resolution, 356\nsubsumption architecture, 1003\nsubsumption lattice, 329\nsuccessor-state axiom, 267, 279, 389\nsuccessor function, 67\nSudoku, 212 1128\nIndex\nSulawesi, 223\nSUMMATION, 1053\nsummer’s day, 1026\nsumming out, 492, 527\nsum of squared differences, 940\nSun Microsystems, 1036\nSunstein, C., 638, 1090\nSunter, A., 556, 1072\nSuperman, 286\nsuperpixels, 942\nsupervised learning, 695, 846, 1025\nsupport vector machine, 744, 744–748,\n754\nsure thing, 617\nsurveillance, 1036\nsurvey propagation, 278\nsurvival of the ﬁttest, 605\nSussman, G. J., 229, 394, 1089, 1090\nSussman anomaly, 394, 398\nSutcliffe, G., 360, 1090\nSutherland, G. L., 22, 1067\nSutherland, I., 228, 1090\nSutphen, S., 194, 1088\nSuttner, C., 360, 1090\nSutton, C., 885, 1090\nSutton, R. S., 685, 854–857, 1065, 1090\nSvartvik, J., 920, 1086\nSvestka, P., 1013, 1078\nSvetnik, V. B., 605, 1093\nSvore, K., 884, 1090\nSwade, D., 14, 1090\nSwartz, R., 1022, 1067\nSwedish, 32\nSwerling, P., 604, 1090\nSwift, T., 359, 1090",
  "Sutton, R. S., 685, 854–857, 1065, 1090\nSvartvik, J., 920, 1086\nSvestka, P., 1013, 1078\nSvetnik, V. B., 605, 1093\nSvore, K., 884, 1090\nSwade, D., 14, 1090\nSwartz, R., 1022, 1067\nSwedish, 32\nSwerling, P., 604, 1090\nSwift, T., 359, 1090\nswitching Kalman ﬁlter, 589, 608\nsyllogism, 4, 275\nsymbolic differentiation, 364\nsymbolic integration, 776\nsymmetry breaking (in CSPs), 226\nsynapse, 11\nsynchro drive, 976\nsynchronization, 427\nsynonymy, 465, 870\nsyntactic ambiguity, 905, 920\nsyntactic categories, 888\nsyntactic sugar, 304\nsyntactic theory (of knowledge), 470\nsyntax, 23, 240, 244\nof logic, 274\nof natural language, 888\nof probability, 488\nsynthesis, 356\ndeductive, 356\nsynthesis of algorithms, 356\nSyrj¨anen, T., 472, 1084, 1090\nsystems reply, 1031\nSzafron, D., 678, 687, 1066, 1091\nSzathm´ary, E., 155, 1089\nSzepesvari, C., 194, 1078\nT\nT (ﬂuent holds), 446\nT-SCHED, 432\nT4, 431\nTABLE-DRIVEN-AGENT, 47\ntable lookup, 737\ntable tennis, 32\ntabu search, 154, 222\ntactile sensors, 974\nTadepalli, P., 799, 857, 1090\nTait, P. G., 109, 1090\nTakusagawa, K. T., 556, 1085\nTalos, 1011\nTALPLANNER, 387\nTamaki, H., 359, 883, 1084, 1090\nTamaki, S., 277, 1077\nTambe, M., 230, 1085\nTank, D. W., 11, 1084\nTardos, E., 688, 1084\nTarjan, R. E., 1059, 1090\nTarski, A., 8, 314, 920, 1090\nTash, J. K., 686, 1090\nTaskar, B., 556, 1073, 1090\ntask environment, 40, 59\ntask network, 394\nTasmania, 222\nTate, A., 394, 396, 408, 431, 432, 1064,\n1065, 1090\nTatman, J. A., 687, 1090\nTattersall, C., 176, 1090\ntaxi, 40, 694\nin Athens, 509\nautomated, 56, 236, 480, 695, 1047\ntaxonomic hierarchy, 24, 440\ntaxonomy, 440, 465, 469\nTaylor, C., 763, 968, 1070, 1082\nTaylor, G., 358, 1090\nTaylor, M., 469, 1089\nTaylor, R., 1013, 1081\nTaylor, W., 9, 229, 277, 1068\nTaylor expansion, 982\nTD-GAMMON, 186, 194, 850, 851\nTeh, Y. W., 1047, 1075\ntelescope, 562\ntelevision, 860\nTeller, A., 155, 554, 1082\nTeller, E., 155, 554, 1082\nTeller, S., 1012, 1066\nTemperley, D., 920, 1089\ntemplate, 874\ntemporal difference learning, 836–838,\n853, 854\ntemporal inference, 570–578\ntemporal logic, 289\ntemporal projection, 278\ntemporal reasoning, 566–609\ntemporal substance, 447\nTenenbaum, J., 314, 1090\nTeng, C.-M., 505, 1079\nTennenholtz, M., 855, 1067\ntennis, 426\ntense, 902\nterm (in logic), 294, 294\nter Meulen, A., 314, 1091\nterminal states, 162\nterminal symbol, 890, 1060\nterminal test, 162\ntermination condition, 995\nterm rewriting, 359\nTesauro, G., 180, 186, 194, 846, 850,\n855, 1090\ntest set, 695\nTETRAD, 826\nTeukolsky, S. A., 155, 1086\ntexel, 951",
  "terminal states, 162\nterminal symbol, 890, 1060\nterminal test, 162\ntermination condition, 995\nterm rewriting, 359\nTesauro, G., 180, 186, 194, 846, 850,\n855, 1090\ntest set, 695\nTETRAD, 826\nTeukolsky, S. A., 155, 1086\ntexel, 951\ntext classiﬁcation, 865, 882\nTEXTRUNNER, 439, 881, 882, 885\ntexture, 939, 948, 951\ntexture gradient, 967\nTeyssier, M., 826, 1090\nThaler, R., 637, 638, 1090\nthee and thou, 890\nTHEO, 1047\nTheocharous, G., 605, 1090\ntheorem, 302\nincompleteness, 8, 352, 1022\ntheorem prover, 2, 356\ntheorem proving, 249, 393\nmathematical, 21, 32\nTheseus, 758\nThiele, T., 604, 1090\nThielscher, M., 279, 470, 1090\nthingiﬁcation, 440\nthinking humanly, 3\nthinking rationally, 4\nThitimajshima, P., 555, 1065\nThomas, A., 554, 555, 826, 1073\nThomas, J., 763, 1069\nThompson, H., 884, 1066\nThompson, K., 176, 192, 1069, 1090\nthought, 4, 19, 234\nlaws of, 4\nthrashing, 102\n3-SAT, 277, 334, 362\nthreshold function, 724\nThroop, T. A., 187, 195, 1089 Index\n1129\nThrun, S., 28, 605, 686, 884,\n1012–1014, 1067, 1068, 1072,\n1083–1085, 1087, 1090, 1091\nTibshirani, R., 760, 761, 763, 827, 1073,\n1075\ntic-tac-toe, 162, 190, 197\nTikhonov, A. N., 759, 1090\ntiling, 737\ntilt, 957\ntime (in grammar), 902\ntime complexity, 80, 108\ntime expressions, 925\ntime interval, 470\ntime of ﬂight camera, 974\ntime slice (in DBNs), 567\nTinsley, M., 193\nTirole, J., 688, 1073\nTishby, N., 604, 1072\ntit for tat, 674\nTitterington, D. M., 826, 1090\nTLPLAN, 387\nTMS, 229, 461, 460–462, 472, 1041\nTobarra, L., 279, 1064\nTofﬂer, A., 1034, 1090\ntokenization, 875\nTomasi, C., 951, 968, 1090\ntoothache, 481\ntopological sort, 223\ntorque sensor, 975\nTorralba, A., 741, 1090\nTorrance, M. C., 231, 1073\nTorras, C., 156, 433, 1077\ntotal cost, 80, 102\nToth, P., 395, 1068\ntouring problem, 74\ntoy problem, 69\nTPTP, 360\ntrace, 904\ntractability of inference, 8, 457\ntrading, 477\ntragedy of the commons, 683\ntrail, 340\ntraining\ncurve, 724\nset, 695\nreplicated, 749\nweighted, 749\ntransfer model (in MT), 908\ntranshumanism, 1038\ntransient failure, 592\ntransient failure model, 593\ntransition matrix, 564\ntransition model, 67, 108, 134, 162, 266,\n566, 597, 603, 646, 684, 832,\n979\ntransition probability, 536\ntransitivity (of preferences), 612\ntranslation model, 909\ntranspose, 1056\ntransposition (in a game), 170\ntransposition table, 170\ntraveling salesperson problem, 74\ntraveling salesperson problem (TSP),\n74, 110, 112, 119\nTraverso, P., 275, 372, 386, 395, 396,\n433, 1066, 1068, 1073, 1088\ntree, 223\nTREE-CSP-SOLVER, 224\nTREE-SEARCH, 77",
  "transposition table, 170\ntraveling salesperson problem, 74\ntraveling salesperson problem (TSP),\n74, 110, 112, 119\nTraverso, P., 275, 372, 386, 395, 396,\n433, 1066, 1068, 1073, 1088\ntree, 223\nTREE-CSP-SOLVER, 224\nTREE-SEARCH, 77\ntreebank, 895, 919\nPenn, 881, 895\ntree decomposition, 225, 227\ntree width, 225, 227, 229, 434, 529\ntrial, 832\ntriangle inequality, 95\ntrichromacy, 935\nTriggs, B., 946, 968, 1069\nTroyanskii, P., 922\nTrucco, E., 968, 1090\ntruth, 240, 295\nfunctionality, 547, 552\npreserving inference, 242\ntable, 245, 276\ntruth maintenance system (TMS), 229,\n461, 460–462, 472, 1041\nassumption-based, 462\njustiﬁcation-based, 461\ntruth value, 245\nTsang, E., 229, 1076\nTsitsiklis, J. N., 506, 685, 686, 847, 855,\n857, 1059, 1066, 1081, 1084,\n1090\nTSP, 74, 110, 112, 119\nTT-CHECK-ALL, 248\nTT-ENTAILS?, 248\nTumer, K., 688, 1090\nTung, F., 604, 1086\ntuple, 291\nturbo decoding, 555\nTurcotte, M., 797, 1090\nTuring, A., 2, 8, 14, 16, 17, 19, 30, 31,\n54, 192, 325, 358, 552, 761, 854,\n1021, 1022, 1024, 1026, 1030,\n1043, 1052, 1090\nTuring award, 1059\nTuring machine, 8, 759\nTuring Test, 2, 2–4, 30, 31, 860, 1021\ntotal, 3\nTurk, 190\nTversky, A., 2, 517, 620, 638, 1072,\n1077, 1090\nTWEAK, 394\nTweedie, F. J., 886, 1078\ntwin earths, 1041\ntwo-ﬁnger Morra, 666\n2001: A Space Odyssey, 552\ntype signature, 542\ntypical instance, 443\nTyson, M., 884, 1075\nU\nU (utility), 611\nu⊤(best prize), 615\nu⊥(worst catastrophe), 615\nUCPOP, 394\nUCT (upper conﬁdence bounds on\ntrees), 194\nUI (Universal Instantiation), 323\nUlam, S., 192, 1078\nUllman, J. D., 358, 1059, 1064, 1065,\n1090\nUllman, S., 967, 968, 1076, 1090\nultraintelligent machine, 1037\nUlysses, 1040\nunbiased (estimator), 618\nuncertain environment, 43\nuncertainty, 23, 26, 438, 480–509, 549,\n1025\nexistence, 541\nidentity, 541, 876\nrelational, 543\nrule-based approach to, 547\nsummarizing, 482\nand time, 566–570\nunconditional probability, see\nprobability, prior\nundecidability, 8\nundergeneration, 892\nunicorn, 280\nuniﬁcation, 326, 326–327, 329, 357\nand equality, 353\nequational, 355\nuniﬁer, 326\nmost general (MGU), 327, 329, 353,\n361\nUNIFORM-COST-SEARCH, 84\nuniform-cost search, 83, 83–85, 108\nuniform convergence theory, 759\nuniform prior, 805\nuniform probability distribution, 487\nuniform resource locator (URL), 463\nUNIFY, 328\nUNIFY-VAR, 328\nUnimate, 1011\nuninformed search, 64, 81, 81–91, 108,\n110\nunique action axioms, 389\nunique names assumption, 299, 540 1130\nIndex\nunit (in a neural network), 728\nunit clause, 253, 260, 355",
  "uniform resource locator (URL), 463\nUNIFY, 328\nUNIFY-VAR, 328\nUnimate, 1011\nuninformed search, 64, 81, 81–91, 108,\n110\nunique action axioms, 389\nunique names assumption, 299, 540 1130\nIndex\nunit (in a neural network), 728\nunit clause, 253, 260, 355\nUnited States, 13, 629, 640, 753, 755,\n922, 1034, 1036\nunit preference, 355\nunit preference strategy, 355\nunit propagation, 261\nunit resolution, 252, 355\nunits function, 444\nuniversal grammar, 921\nUniversal Instantiation, 323\nuniversal plan, 434\nunmanned air vehicle (UAV), 971\nunmanned ground vehicle (UGV), 971\nUNPOP, 394\nunrolling, 544, 595\nunsatisﬁability, 274\nunsupervised learning, 694, 817–820,\n1025\nUOSAT-II, 432\nupdate, 142\nupper ontology, 467\nURL, 463\nUrmson, C., 1014, 1091\nurn-and-ball, 803\nURP, 638\nUskov, A. V., 192, 1064\nUtgoff, P. E., 776, 799, 1082\nutilitarianism, 7\nutility, 9, 53, 162, 482\naxioms of, 613\nestimation, 833\nexpected, 53, 61, 483, 610, 611, 616\nfunction, 53, 54, 162, 611, 615–621,\n846\nindependence, 626\nmaximum expected, 483, 611\nof money, 616–618\nmultiattribute, 622–626, 636, 648\nmultiplicative, 626\nnode, 627\nnormalized, 615\nordinal, 614\ntheory, 482, 611–615, 636\nutility-based agent, 1044\nutopia, 1052\nUWL, 433\nV\nvacuum tube, 16\nvacuum world, 35, 37, 62, 159\nerratic, 134\nslippery, 137\nvagueness, 547\nValiant, L., 759, 1091\nvalidation\ncross, 737, 759, 767\nvalidation, cross, 708\nvalidation set, 709\nvalidity, 249, 274\nvalue, 58\nVALUE-ITERATION, 653\nvalue determination, 691\nvalue function, 614\nadditive, 625\nvalue iteration, 652, 652–656, 684\npoint-based, 686\nvalue node, see utility node\nvalue of computation, 1048\nvalue of information, 628–633, 636,\n644, 659, 839, 1025, 1048\nvalue of perfect information, 630\nvalue symmetry, 226\nVAMPIRE, 359, 360\nvan Beek, P., 228–230, 395, 470, 1065,\n1078, 1087, 1091\nvan Bentham, J., 314, 1091\nVandenberghe, L., 155, 1066\nvan Harmelen, F., 473, 799, 1091\nvan Heijenoort, J., 360, 1091\nvan Hoeve, W.-J., 212, 228, 1091\nvanishing point, 931\nvan Lambalgen, M., 470, 1091\nvan Maaren, H., 278, 1066\nvan Nunen, J. A. E. E., 685, 1091\nvan Run, P., 230, 1065\nvan der Gaag, L., 505, 1081\nVan Emden, M. H., 472, 1091\nVan Hentenryck, P., 228, 1091\nVan Roy, B., 847, 855, 1090, 1091\nVan Roy, P. L., 339, 342, 359, 1091\nVapnik, V. N., 759, 760, 762, 763, 967,\n1066, 1069, 1080, 1091\nVaraiya, P., 60, 856, 1072, 1079\nVardi, M. Y., 470, 477, 1072\nvariabilization (in EBL), 781\nvariable, 58\natemporal, 266\nelimination, 524, 524–528, 552, 553,\n596\nin continuous state space, 131",
  "1066, 1069, 1080, 1091\nVaraiya, P., 60, 856, 1072, 1079\nVardi, M. Y., 470, 477, 1072\nvariabilization (in EBL), 781\nvariable, 58\natemporal, 266\nelimination, 524, 524–528, 552, 553,\n596\nin continuous state space, 131\nindicator, 819\nlogic, 340\nin logic, 295\nordering, 216, 527\nrandom, 486, 515\nBoolean, 486\ncontinuous, 487, 519, 553\nrelevance, 528\nVarian, H. R., 688, 759, 1081, 1091\nvariational approximation, 554\nvariational parameter, 554\nVarzi, A., 470, 1068\nVaucanson, J., 1011\nVauquois, B., 909, 1091\nVazirani, U., 154, 763, 1064, 1078\nVazirani, V., 688, 1084\nVC dimension, 759\nVCG, 683\nVecchi, M. P., 155, 229, 1078\nvector, 1055\nvector ﬁeld histograms, 1013\nvector space model, 884\nvehicle interface layer, 1006\nVeloso, M., 799, 1091\nVempala, S., 883, 1084\nVenkataraman, S., 686, 1074\nVenugopal, A., 922, 1093\nVere, S. A., 431, 1091\nveriﬁcation, 356\nhardware, 312\nVerma, T., 553, 826, 1073, 1085\nVerma, V., 605, 1091\nVerri, A., 968, 1090\nVERSION-SPACE-LEARNING, 773\nVERSION-SPACE-UPDATE, 773\nversion space, 773, 774, 798\nversion space collapse, 776\nVetterling, W. T., 155, 1086\nVickrey, W., 681\nVickrey-Clarke-Groves, 683\nVienna, 1028\nviews, multiple, 948\nVinge, V., 12, 1038, 1091\nViola, P., 968, 1025, 1091\nvirtual counts, 812\nvisibility graph, 1013\nvision, 3, 12, 20, 228, 929–965\nVisser, U., 195, 1014, 1091\nVisser, W., 356, 1075\nVitali set, 489\nVitanyi, P. M. B., 759, 1080\nViterbi, A. J., 604, 1091\nViterbi algorithm, 578\nVlassis, N., 435, 686, 1089, 1091\nVLSI layout, 74, 110, 125\nvocabulary, 864\nVolk, K., 826, 1074\nvon Mises, R., 504, 1091\nvon Neumann, J., 9, 15, 17, 190, 613,\n637, 687, 1091\nvon Stengel, B., 677, 687, 1078\nvon Winterfeldt, D., 637, 1091\nvon Kempelen, W., 190\nvon Linne, C., 469\nVoronkov, A., 314, 359, 360, 1086,\n1087 Index\n1131\nVoronoi graph, 991\nVossen, T., 396, 1091\nvoted perceptron, 760\nVPI (value of perfect information), 630\nW\nWadsworth, C. P., 314, 1074\nWahba, G., 759, 1074\nWainwright, M. J., 278, 555, 1081, 1091\nWalden, W., 192, 1078\nWaldinger, R., 314, 394, 1081, 1091\nWalker, E., 29, 1069\nWalker, H., 826, 1074\nWALKSAT, 263, 395\nWall, R., 920, 1071\nWallace, A. R., 130, 1091\nWallace, D. L., 886, 1083\nWalras, L., 9\nWalsh, M. J., 156, 1072\nWalsh, T., 228, 230, 278, 1066, 1087,\n1089\nWalsh, W., 688, 1092\nWalter, G., 1011\nWaltz, D., 20, 228, 760, 1089, 1091\nWAM, 341, 359\nWang, D. Z., 885, 1067\nWang, E., 472, 1090\nWang, Y., 194, 1091\nWanner, E., 287, 1091\nWarmuth, M., 109, 759, 1066, 1086\nWARPLAN, 394\nWarren, D. H. D., 339, 341, 359, 394,",
  "Walter, G., 1011\nWaltz, D., 20, 228, 760, 1089, 1091\nWAM, 341, 359\nWang, D. Z., 885, 1067\nWang, E., 472, 1090\nWang, Y., 194, 1091\nWanner, E., 287, 1091\nWarmuth, M., 109, 759, 1066, 1086\nWARPLAN, 394\nWarren, D. H. D., 339, 341, 359, 394,\n889, 1085, 1091\nWarren, D. S., 359, 1090\nWarren Abstract Machine (WAM), 341,\n359\nwashing clothes, 927\nWashington, G., 450\nwasp, sphex, 39, 425\nWasserman, L., 763, 1091\nWatkins, C. J., 685, 855, 1091\nWatson, J., 12\nWatson, J. D., 130, 1091\nWatt, J., 15\nWattenberg, M., 155, 1077\nWaugh, K., 687, 1091\nWBRIDGE5, 195\nweak AI, 1020, 1040\nweak domination, 668\nweak method, 22\nWeaver, W., 703, 758, 763, 883, 907,\n908, 922, 1088, 1091\nWebber, B. L., 31, 1091\nWeber, J., 604, 1076\nWefald, E. H., 112, 191, 198, 1048,\n1087\nWegbreit, B., 1012, 1083\nWeglarz, J., 432, 1066\nWei, X., 885, 1085\nWeibull, J., 688, 1091\nWeidenbach, C., 359, 1091\nweight, 718\nweight (in a neural network), 728\nWEIGHTED-SAMPLE, 534\nweighted linear function, 172\nweight space, 719\nWeinstein, S., 759, 1084\nWeiss, G., 61, 435, 1091\nWeiss, S., 884, 1064\nWeiss, Y., 555, 605, 741, 1083,\n1090–1092\nWeissman, V., 314, 1074\nWeizenbaum, J., 1035, 1041, 1091\nWeld, D. S., 61, 156, 394–396, 432,\n433, 469, 472, 885, 1036, 1069,\n1071, 1072, 1079, 1085, 1089,\n1091, 1092\nWellman, M. P., 10, 555, 557, 604, 638,\n685–688, 857, 1013, 1070, 1076,\n1091, 1092\nWells, H. G., 1037, 1092\nWells, M., 192, 1078\nWelty, C., 469, 1089\nWerbos, P., 685, 761, 854, 1092\nWermuth, N., 553, 1080\nWerneck, R. F., 111, 1074\nWertheimer, M., 966\nWesley, M. A., 1013, 1092\nWest, Col., 330\nWestinghouse, 432\nWestphal, M., 395, 1086\nWexler, Y., 553, 1092\nWeymouth, T., 1013, 1069\nWhite, J. L., 356, 1075\nWhitehead, A. N., 16, 357, 781, 1092\nWhiter, A. M., 431, 1090\nWhittaker, W., 1014, 1091\nWhorf, B., 287, 314, 1092\nwide content, 1028\nWidrow, B., 20, 761, 833, 854, 1092\nWidrow–Hoff rule, 846\nWiedijk, F., 360, 1092\nWiegley, J., 156, 1092\nWiener, N., 15, 192, 604, 761, 922,\n1087, 1092\nwiggly belief state, 271\nWilczek, F., 761, 1065\nWilensky, R., 23, 24, 1031, 1092\nWilfong, G. T., 1012, 1069\nWilkins, D. E., 189, 431, 434, 1092\nWilliams, B., 60, 278, 432, 472, 1083,\n1092\nWilliams, C. K. I., 827, 1086\nWilliams, R., 640\nWilliams, R. J., 685, 761, 849, 855,\n1085, 1087, 1092\nWilliamson, J., 469, 1083\nWilliamson, M., 433, 1072\nWillighagen, E. L., 469, 1083\nWilmer, E. L., 604, 1080\nWilson, A., 921, 1080\nWilson, R., 227, 1092\nWilson, R. A., 3, 1042, 1092\nWindows, 553\nWinikoff, M., 59, 1084\nWinker, S., 360, 1092",
  "Williamson, J., 469, 1083\nWilliamson, M., 433, 1072\nWillighagen, E. L., 469, 1083\nWilmer, E. L., 604, 1080\nWilson, A., 921, 1080\nWilson, R., 227, 1092\nWilson, R. A., 3, 1042, 1092\nWindows, 553\nWinikoff, M., 59, 1084\nWinker, S., 360, 1092\nWinkler, R. L., 619, 637, 1089\nwinner’s curse, 637\nWinograd, S., 20, 1092\nWinograd, T., 20, 23, 884, 1066, 1092\nWinston, P. H., 2, 20, 27, 773, 798,\n1065, 1092\nWintermute, S., 358, 1092\nWitbrock, M., 469, 1081\nWitten, I. H., 763, 883, 884, 921, 1083,\n1092\nWittgenstein, L., 6, 243, 276, 279, 443,\n469, 1092\nWizard, 553\nW¨ohler, F., 1027\nWojciechowski, W. S., 356, 1092\nWojcik, A. S., 356, 1092\nWolf, A., 920, 1074\nWolfe, D., 186, 1065\nWolfe, J., 157, 192, 432, 1081, 1087,\n1092\nWolpert, D., 688, 1090\nWong, A., 884, 1087\nWong, W.-K., 826, 1083\nWood, D. E., 111, 1080\nWoods, W. A., 471, 921, 1092\nWooldridge, M., 60, 61, 1068, 1092\nWoolsey, K., 851\nworkspace representation, 986\nworld model, in disambiguation, 906\nworld state, 69\nWorld War II, 10, 552, 604\nWorld Wide Web (WWW), 27, 462,\n867, 869\nworst possible catastrophe, 615\nWos, L., 359, 360, 1092\nwrapper (for Internet site), 466\nwrapper (for learning), 709\nWray, R. E., 358, 1092\nWright, O. and W., 3\nWright, R. N., 884, 1085\nWright, S., 155, 552, 1092\nWu, D., 921, 1092 1132\nIndex\nWu, E., 885, 1067\nWu, F., 469, 1092\nwumpus world, 236, 236–240, 246–247,\n279, 305–307, 439, 499–503,\n509\nWundt, W., 12\nWurman, P., 688, 1092\nWWW, 27, 462, 867, 869\nX\nXCON, 336\nXML, 875\nxor, 246, 766\nXu, J., 358, 1092\nXu, P., 29, 921, 1067\nY\nYakimovsky, Y., 639, 1072\nYale, 23\nYan, D., 431, 1073\nYang, C. S., 884, 1087\nYang, F., 107, 1092\nYang, Q., 432, 1092\nYannakakis, M., 157, 229, 1065, 1084\nYap, R. H. C., 359, 1077\nYardi, M., 278, 1068\nYarowsky, D., 27, 885, 1092\nYates, A., 885, 1072\nYedidia, J., 555, 1092\nYglesias, J., 28, 1064\nYip, K. M.-K., 472, 1092\nYngve, V., 920, 1092\nYob, G., 279, 1092\nYoshikawa, T., 1013, 1092\nYoung, H. P., 435, 1092\nYoung, M., 797, 1078\nYoung, S. J., 896, 920, 1080\nYounger, D. H., 920, 1092\nYu, B., 553, 1068\nYudkowsky, E., 27, 1039, 1093\nYung, M., 110, 119, 1064, 1075\nYvanovich, M., 432, 1070\nZ\nZ-3, 14\nZadeh, L. A., 557, 1093\nZahavi, U., 107, 1092\nZapp, A., 1014, 1071\nZaragoza, H., 884, 1069\nZaritskii, V. S., 605, 1093\nzebra puzzle, 231\nZecchina, R., 278, 1084\nZeldner, M., 908\nZelle, J., 902, 921, 1093\nZeng, H., 314, 1082\nZermelo, E., 687, 1093\nzero-sum game, 161, 162, 199, 670\nZettlemoyer, L. S., 556, 921, 1082, 1093\nZhai, C., 884, 1079\nZhang, H., 277, 1093",
  "zebra puzzle, 231\nZecchina, R., 278, 1084\nZeldner, M., 908\nZelle, J., 902, 921, 1093\nZeng, H., 314, 1082\nZermelo, E., 687, 1093\nzero-sum game, 161, 162, 199, 670\nZettlemoyer, L. S., 556, 921, 1082, 1093\nZhai, C., 884, 1079\nZhang, H., 277, 1093\nZhang, L., 277, 553, 1083, 1093\nZhang, N. L., 553, 639, 1093\nZhang, W., 112, 1079\nZhang, Y., 885, 1067\nZhao, Y., 277, 1083\nZhivotovsky, A. A., 192, 1064\nZhou, R., 112, 1093\nZhu, C., 760, 1067\nZhu, D. J., 1012, 1093\nZhu, W. L., 439, 1089\nZilberstein, S., 156, 422, 433, 434,\n1075, 1085\nZimdars, A., 857, 1087\nZimmermann, H.-J., 557, 1093\nZinkevich, M., 687, 1093\nZisserman, A., 960, 968, 1075, 1086\nZlotkin, G., 688, 1087\nZog, 778\nZollmann, A., 922, 1093\nZuckerman, D., 124, 1081\nZufferey, J. C., 1045, 1072\nZuse, K., 14, 192\nZweben, M., 432, 1070\nZweig, G., 604, 1093\nZytkow, J. M., 800, 1079"
]