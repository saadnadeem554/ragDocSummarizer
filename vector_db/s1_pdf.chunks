[
  "OPERATING\nSYSTEM\nCONCEPTS\nNINTH EDITION  OPERATING\nSYSTEM\nCONCEPTS\nABRAHAM SILBERSCHATZ\nYale University\nPETER BAER GALVIN\nPluribus Networks\nGREG GAGNE\nWestminster College\nNINTH EDITION !\nVice!President!and!Executive!Publisher! !\n!\nDon!Fowley!\nExecutive!Editor!\n!\n!\n!\n!\nBeth!Lang!Golub!\nEditorial!Assistant! !\n!\n!\n!\nKatherine!Willis!\nExecutive!Marketing!Manager!\n!\n!\nChristopher!Ruel!\nSenior!Production!Editor!\n!\n!\n!\nKen!Santor!\nCover!and!title!page!illustrations!\n!\n!\nSusan!Cyr!\nCover!Designer!\n!\n!\n!\n!\nMadelyn!Lesure!\nText!Designer!!\n!\n!\n!\n!\nJudy!Allan!\n!\n!\n!\n!\n!\nThis!book!was!set!in!Palatino!by!the!author!using!LaTeX!and!printed!and!bound!by!Courier\"\nKendallville.!The!cover!was!printed!by!Courier.!\n!\n!\nCopyright!©!2013,!2012,!2008!John!Wiley!&!Sons,!Inc.!!All!rights!reserved.!\n!",
  "!\nNo!part!of!this!publication!may!be!reproduced,!stored!in!a!retrieval!system!or!transmitted!in!any!\nform!or!by!any!means,!electronic,!mechanical,!photocopying,!recording,!scanning!or!otherwise,!\nexcept!as!permitted!under!Sections!107!or!108!of!the!1976!United!States!Copyright!Act,!without!\neither!the!prior!written!permission!of!the!Publisher,!or!authorization!through!payment!of!the!\nappropriate!per\"copy!fee!to!the!Copyright!Clearance!Center,!Inc.!222!Rosewood!Drive,!Danvers,!\nMA!01923,!(978)750\"8400,!fax!(978)750\"4470.!Requests!to!the!Publisher!for!permission!should!be!\naddressed!to!the!Permissions!Department,!John!Wiley!&!Sons,!Inc.,!111!River!Street,!Hoboken,!NJ!\n07030!(201)748\"6011,!fax!(201)748\"6008,!E\"Mail:!PERMREQ@WILEY.COM.!!!\n!",
  "!\nEvaluation!copies!are!provided!to!qualified!academics!and!professionals!for!review!purposes!\nonly,!for!use!in!their!courses!during!the!next!academic!year.!!These!copies!are!licensed!and!may!\nnot!be!sold!or!transferred!to!a!third!party.!!Upon!completion!of!the!review!period,!please!return!\nthe!evaluation!copy!to!Wiley.!!Return!instructions!and!a!free\"of\"charge!return!shipping!label!are!\navailable!at!www.wiley.com/go/evalreturn.!Outside!of!the!United!States,!please!contact!your!\nlocal!representative.!\n!\nFounded!in!1807,!John!Wiley!&!Sons,!Inc.!has!been!a!valued!source!of!knowledge!and!\nunderstanding!for!more!than!200!years,!helping!people!around!the!world!meet!their!needs!and!\nfulfill!their!aspirations.!Our!company!is!built!on!a!foundation!of!principles!that!include!",
  "responsibility!to!the!communities!we!serve!and!where!we!live!and!work.!In!2008,!we!launched!a!\nCorporate!Citizenship!Initiative,!a!global!effort!to!address!the!environmental,!social,!economic,!\nand!ethical!challenges!we!face!in!our!business.!Among!the!issues!we!are!addressing!are!carbon!\nimpact,!paper!specifications!and!procurement,!ethical!conduct!within!our!business!and!among!\nour!vendors,!and!community!and!charitable!support.!For!more!information,!please!visit!our!\nwebsite:!www.wiley.com/go/citizenship.!!!\n!\n!\n!\nISBN:!!978\"1\"118\"06333\"0!\nISBN!BRV:!!978\"1\"118\"12938\"8!\n!\nPrinted!in!the!United!States!of!America!\n!\n10!!!9!!!8!!!7!!!6!!!5!!!4!!!3!!!2!!!1! To my children, Lemor, Sivan, and Aaron\nand my Nicolette\nAvi Silberschatz\nTo Brendan and Ellen,",
  "Avi Silberschatz\nTo Brendan and Ellen,\nand Barbara, Anne and Harold, and Walter and Rebecca\nPeter Baer Galvin\nTo my Mom and Dad,\nGreg Gagne  Preface\nOperating systems are an essential part of any computer system. Similarly,\na course on operating systems is an essential part of any computer science\neducation. This ﬁeld is undergoing rapid change, as computers are now\nprevalent in virtually every arena of day-to-day life—from embedded devices\nin automobiles through the most sophisticated planning tools for governments\nand multinational ﬁrms. Yet the fundamental concepts remain fairly clear, and\nit is on these that we base this book.\nWe wrote this book as a text for an introductory course in operating systems\nat the junior or senior undergraduate level or at the ﬁrst-year graduate level. We",
  "hope that practitioners will also ﬁnd it useful. It provides a clear description of\nthe concepts that underlie operating systems. As prerequisites, we assume that\nthe reader is familiar with basic data structures, computer organization, and\na high-level language, such as C or Java. The hardware topics required for an\nunderstanding of operating systems are covered in Chapter 1. In that chapter,\nwe also include an overview of the fundamental data structures that are\nprevalent in most operating systems. For code examples, we use predominantly\nC, with some Java, but the reader can still understand the algorithms without\na thorough knowledge of these languages.\nConcepts are presented using intuitive descriptions. Important theoretical",
  "results are covered, but formal proofs are largely omitted. The bibliographical\nnotes at the end of each chapter contain pointers to research papers in which\nresults were ﬁrst presented and proved, as well as references to recent material\nfor further reading. In place of proofs, ﬁgures and examples are used to suggest\nwhy we should expect the result in question to be true.\nThe fundamental concepts and algorithms covered in the book are often\nbased on those used in both commercial and open-source operating systems.\nOur aim is to present these concepts and algorithms in a general setting that\nis not tied to one particular operating system. However, we present a large\nnumber of examples that pertain to the most popular and the most innovative",
  "operating systems, including Linux, Microsoft Windows, Apple Mac OS X, and\nSolaris. We also include examples of both Android and iOS, currently the two\ndominant mobile operating systems.\nThe organization of the text reﬂects our many years of teaching courses on\noperating systems, as well as curriculum guidelines published by the IEEE\nvii viii\nPreface\nComputing Society and the Association for Computing Machinery (ACM).\nConsideration was also given to the feedback provided by the reviewers of\nthe text, along with the many comments and suggestions we received from\nreaders of our previous editions and from our current and former students.\nContent of This Book\nThe text is organized in eight major parts:\n• Overview. Chapters 1 and 2 explain what operating systems are, what",
  "they do, and how they are designed and constructed. These chapters\ndiscuss what the common features of an operating system are and what an\noperating system does for the user. We include coverage of both traditional\nPC and server operating systems, as well as operating systems for mobile\ndevices. The presentation is motivational and explanatory in nature. We\nhave avoided a discussion of how things are done internally in these\nchapters. Therefore, they are suitable for individual readers or for students\nin lower-level classes who want to learn what an operating system is\nwithout getting into the details of the internal algorithms.\n• Process management. Chapters 3 through 7 describe the process concept\nand concurrency as the heart of modern operating systems. A process",
  "is the unit of work in a system. Such a system consists of a collection\nof concurrently executing processes, some of which are operating-system\nprocesses (those that execute system code) and the rest of which are user\nprocesses (those that execute user code). These chapters cover methods for\nprocess scheduling, interprocess communication, process synchronization,\nand deadlock handling. Also included is a discussion of threads, as well\nas an examination of issues related to multicore systems and parallel\nprogramming.\n• Memory management. Chapters 8 and 9 deal with the management of\nmain memory during the execution of a process. To improve both the\nutilization of the CPU and the speed of its response to its users, the\ncomputer must keep several processesinmemory. There are manydifferent",
  "memory-management schemes, reﬂecting various approaches to memory\nmanagement, and the effectiveness of a particular algorithm depends on\nthe situation.\n• Storage management. Chapters 10 through 13 describe how mass storage,\nthe ﬁle system, and I/O are handled in a modern computer system. The\nﬁle system provides the mechanism for on-line storage of and access\nto both data and programs. We describe the classic internal algorithms\nand structures of storage management and provide a ﬁrm practical\nunderstanding of the algorithms used—their properties, advantages, and\ndisadvantages. Since the I/O devices that attach to a computer vary widely,\nthe operating system needs to provide a wide range of functionality to\napplications to allow them to control all aspects of these devices. We",
  "discuss system I/O in depth, including I/O system design, interfaces, and\ninternal system structures and functions. In many ways, I/O devices are\nthe slowest major components of the computer. Because they represent a Preface\nix\nperformance bottleneck, we also examine performance issues associated\nwith I/O devices.\n• Protection and security. Chapters 14 and 15 discuss the mechanisms\nnecessary for the protection and security of computer systems. The\nprocesses in an operating system must be protected from one another’s\nactivities, and to provide such protection, we must ensure that only\nprocesses that have gained proper authorization from the operating system\ncan operate on the ﬁles, memory, CPU, and other resources of the system.",
  "Protection is a mechanism for controlling the access of programs, processes,\nor users to computer-system resources. This mechanism must provide a\nmeans of specifying the controls to be imposed, as well as a means of\nenforcement. Security protects the integrity of the information stored in\nthe system (both data and code), as well as the physical resources of the\nsystem, from unauthorized access, malicious destruction or alteration, and\naccidental introduction of inconsistency.\n• Advanced topics. Chapters 16 and 17 discuss virtual machines and\ndistributed systems. Chapter 16 is a new chapter that provides an overview\nof virtual machines and their relationship to contemporary operating\nsystems. Included is an overview of the hardware and software techniques",
  "that make virtualization possible. Chapter 17 condenses and updates the\nthree chapters on distributed computing from the previous edition. This\nchange is meant to make it easier for instructors to cover the material in\nthe limited time available during a semester and for students to gain an\nunderstanding of the core ideas of distributed computing more quickly.\n• Case studies. Chapters 18 and 19 in the text, along with Appendices A and\nB (which are available on (http://www.os-book.com), present detailed\ncase studies of real operating systems, including Linux, Windows 7,\nFreeBSD, and Mach. Coverage of both Linux and Windows 7 are presented\nthroughout this text; however, the case studies provide much more detail.\nIt is especially interesting to compare and contrast the design of these two",
  "very different systems. Chapter 20 brieﬂy describes a few other inﬂuential\noperating systems.\nThe Ninth Edition\nAs we wrote this Ninth Edition of Operating System Concepts, we were guided\nby the recent growth in three fundamental areas that affect operating systems:\n1. Multicore systems\n2. Mobile computing\n3. Virtualization\nTo emphasize these topics, we have integrated relevant coverage throughout\nthis new edition—and, in the case of virtualization, have written an entirely\nnew chapter. Additionally, we have rewritten material in almost every chapter\nby bringing older material up to date and removing material that is no longer\ninteresting or relevant. x\nPreface\nWe have also made substantial organizational changes. For example, we",
  "have eliminated the chapter on real-time systems and instead have integrated\nappropriate coverage of these systems throughout the text. We have reordered\nthe chapters on storage management and have moved up the presentation\nof process synchronization so that it appears before process scheduling. Most\nof these organizational changes are based on our experiences while teaching\ncourses on operating systems.\nBelow, we provide a brief outline of the major changes to the various\nchapters:\n• Chapter 1, Introduction, includes updated coverage of multiprocessor\nand multicore systems, as well as a new section on kernel data structures.\nAdditionally, the coverage of computing environments now includes\nmobile systems and cloud computing. We also have incorporated an\noverview of real-time systems.",
  "overview of real-time systems.\n• Chapter 2, Operating-System Structures, provides new coverage of user\ninterfaces for mobile devices, including discussions of iOS and Android,\nand expanded coverage of Mac OS X as a type of hybrid system.\n• Chapter 3, Processes, now includes coverage of multitasking in mobile\noperating systems, support for the multiprocess model in Google’s Chrome\nweb browser, and zombie and orphan processes in UNIX.\n• Chapter 4, Threads, supplies expanded coverage of parallelism and\nAmdahl’s law. It also provides a new section on implicit threading,\nincluding OpenMP and Apple’s Grand Central Dispatch.\n• Chapter 5, Process Synchronization (previously Chapter 6), adds a new\nsection on mutex locks as well as coverage of synchronization using",
  "OpenMP, as well as functional languages.\n• Chapter 6, CPU Scheduling (previously Chapter 5), contains new coverage\nof the Linux CFS scheduler and Windows user-mode scheduling. Coverage\nof real-time scheduling algorithms has also been integrated into this\nchapter.\n• Chapter 7, Deadlocks, has no major changes.\n• Chapter 8, Main Memory, includes new coverage of swapping on mobile\nsystems and Intel 32- and 64-bit architectures. A new section discusses\nARM architecture.\n• Chapter 9, Virtual Memory, updates kernel memory management to\ninclude the Linux SLUB and SLOB memory allocators.\n• Chapter 10, Mass-Storage Structure (previously Chapter 12), adds cover-\nage of solid-state disks.\n• Chapter 11, File-System Interface (previously Chapter 10), is updated",
  "with information about current technologies.\n• Chapter 12, File-System Implementation (previously Chapter 11), is\nupdated with coverage of current technologies.\n• Chapter 13, I/O, updates technologies and performance numbers, expands\ncoverage of synchronous/asynchronous and blocking/nonblocking I/O,\nand adds a section on vectored I/O. Preface\nxi\n• Chapter 14, Protection, has no major changes.\n• Chapter 15, Security, has a revised cryptography section with modern\nnotation and an improved explanation of various encryption methods and\ntheir uses. The chapter also includes new coverage of Windows 7 security.\n• Chapter 16, Virtual Machines, is a new chapter that provides an overview\nof virtualization and how it relates to contemporary operating systems.",
  "• Chapter 17, Distributed Systems, is a new chapter that combines and\nupdates a selection of materials from previous Chapters 16, 17, and 18.\n• Chapter 18, The Linux System (previously Chapter 21), has been updated\nto cover the Linux 3.2 kernel.\n• Chapter 19, Windows 7, is a new chapter presenting a case study of\nWindows 7.\n• Chapter 20, Inﬂuential Operating Systems (previously Chapter 23), has\nno major changes.\nProgramming Environments\nThis book uses examples of many real-world operating systems to illustrate\nfundamental operating-system concepts. Particular attention is paid to Linux\nand Microsoft Windows, but we also refer to various versions of UNIX\n(including Solaris, BSD, and Mac OS X).\nThe text also provides several example programs written in C and",
  "Java. These programs are intended to run in the following programming\nenvironments:\n• POSIX. POSIX (which stands for Portable Operating System Interface) repre-\nsents a set of standards implemented primarily for UNIX-based operating\nsystems. Although Windows systems can also run certain POSIX programs,\nour coverage of POSIX focuses on UNIX and Linux systems. POSIX-compliant\nsystems must implement the POSIX core standard (POSIX.1); Linux, Solaris,\nand Mac OS X are examples of POSIX-compliant systems. POSIX also\ndeﬁnes several extensions to the standards, including real-time extensions\n(POSIX1.b) and an extension for a threads library (POSIX1.c, better known\nas Pthreads). We provide several programming examples written in C",
  "illustrating the POSIX base API, as well as Pthreads and the extensions for\nreal-time programming. These example programs were tested on Linux 2.6\nand 3.2 systems, Mac OS X 10.7, and Solaris 10 using the gcc 4.0 compiler.\n• Java. Java is a widely used programming language with a rich API and\nbuilt-in language support for thread creation and management. Java\nprograms run on any operating system supporting a Java virtual machine\n(or JVM). We illustrate various operating-system and networking concepts\nwith Java programs tested using the Java 1.6 JVM.\n• Windows systems. The primary programming environment for Windows\nsystems is the Windows API, which provides a comprehensive set of func-\ntions for managing processes, threads, memory, and peripheral devices.",
  "We supply several C programs illustrating the use of this API. Programs\nwere tested on systems running Windows XP and Windows 7. xii\nPreface\nWe have chosen these three programming environments because we\nbelieve that they best represent the two most popular operating-system models\n—Windows and UNIX/Linux—along with the widely used Java environment.\nMost programming examples are written in C, and we expect readers to be\ncomfortable with this language. Readers familiar with both the C and Java\nlanguages should easily understand most programs provided in this text.\nIn some instances—such as thread creation—we illustrate a speciﬁc\nconcept using all three programming environments, allowing the reader\nto contrast the three different libraries as they address the same task. In",
  "other situations, we may use just one of the APIs to demonstrate a concept.\nFor example, we illustrate shared memory using just the POSIX API; socket\nprogramming in TCP/IP is highlighted using the Java API.\nLinux Virtual Machine\nTo help students gain a better understanding of the Linux system, we\nprovide\na\nLinux\nvirtual\nmachine,\nincluding\nthe\nLinux\nsource\ncode,\nthat is available for download from the the website supporting this\ntext\n(http://www.os-book.com).\nThis\nvirtual machine\nalso\nincludes\na\ngcc development environment with compilers and editors. Most of the\nprogramming assignments in the book can be completed on this virtual\nmachine, with the exception of assignments that require Java or the Windows\nAPI.\nWe also provide three programming assignments that modify the Linux",
  "kernel through kernel modules:\n1. Adding a basic kernel module to the Linux kernel.\n2. Adding a kernel module that uses various kernel data structures.\n3. Adding a kernel module that iterates over tasks in a running Linux\nsystem.\nOver time it is our intention to add additional kernel module assignments on\nthe supporting website.\nSupporting Website\nWhen you visit the website supporting this text at http://www.os-book.com,\nyou can download the following resources:\n• Linux virtual machine\n• C and Java source code\n• Sample syllabi\n• Set of Powerpoint slides\n• Set of ﬁgures and illustrations\n• FreeBSD and Mach case studies Preface\nxiii\n• Solutions to practice exercises\n• Study guide for students\n• Errata\nNotes to Instructors",
  "• Errata\nNotes to Instructors\nOn the website for this text, we provide several sample syllabi that suggest\nvarious approaches for using the text in both introductory and advanced\ncourses. As a general rule, we encourage instructors to progress sequentially\nthrough the chapters, as this strategy provides the most thorough study of\noperating systems. However, by using the sample syllabi, an instructor can\nselect a different ordering of chapters (or subsections of chapters).\nIn this edition, we have added over sixty new written exercises and over\ntwenty new programming problems and projects. Most of the new program-\nming assignments involve processes, threads, process synchronization, and\nmemory management. Some involve adding kernel modules to the Linux",
  "system which requires using either the Linux virtual machine that accompanies\nthis text or another suitable Linux distribution.\nSolutions to written exercises and programming assignments are available\nto instructors who have adopted this text for their operating-system class. To\nobtain these restricted supplements, contact your local John Wiley & Sons\nsales representative. You can ﬁnd your Wiley representative by going to\nhttp://www.wiley.com/college and clicking “Who’s my rep?”\nNotes to Students\nWe encourage you to take advantage of the practice exercises that appear at\nthe end of each chapter. Solutions to the practice exercises are available for\ndownload from the supporting website http://www.os-book.com. We also",
  "encourage you to read through the study guide, which was prepared by one of\nour students. Finally, for students who are unfamiliar with UNIX and Linux\nsystems, we recommend that you download and install the Linux virtual\nmachine that we include on the supporting website. Not only will this provide\nyou with a new computing experience, but the open-source nature of Linux\nwill allow you to easily examine the inner details of this popular operating\nsystem.\nWe wish you the very best of luck in your study of operating systems.\nContacting Us\nWe have endeavored to eliminate typos, bugs, and the like from the text. But,\nas in new releases of software, bugs almost surely remain. An up-to-date errata\nlist is accessible from the book’s website. We would be grateful if you would",
  "notify us of any errors or omissions in the book that are not on the current list\nof errata.\nWe would be glad to receive suggestions on improvements to the book.\nWe also welcome any contributions to the book website that could be of xiv\nPreface\nuse to other readers, such as programming exercises, project suggestions,\non-line labs and tutorials, and teaching tips. E-mail should be addressed to\nos-book-authors@cs.yale.edu.\nAcknowledgments\nThis book is derived from the previous editions, the ﬁrst three of which\nwere coauthored by James Peterson. Others who helped us with previous\neditions include Hamid Arabnia, Rida Bazzi, Randy Bentson, David Black,\nJoseph Boykin, Jeff Brumﬁeld, Gael Buckley, Roy Campbell, P. C. Capon, John",
  "Carpenter, Gil Carrick, Thomas Casavant, Bart Childs, Ajoy Kumar Datta,\nJoe Deck, Sudarshan K. Dhall, Thomas Doeppner, Caleb Drake, M. Racsit\nEskicio˘glu, Hans Flack, Robert Fowler, G. Scott Graham, Richard Guy, Max\nHailperin, Rebecca Hartman, Wayne Hathaway, Christopher Haynes, Don\nHeller, Bruce Hillyer, Mark Holliday, Dean Hougen, Michael Huang, Ahmed\nKamel, Morty Kewstel, Richard Kieburtz, Carol Kroll, Morty Kwestel, Thomas\nLeBlanc, John Leggett, Jerrold Leichter, Ted Leung, Gary Lippman, Carolyn\nMiller, Michael Molloy, Euripides Montagne, Yoichi Muraoka, Jim M. Ng,\nBanu ¨Ozden, Ed Posnak, Boris Putanec, Charles Qualline, John Quarterman,\nMike Reiter, Gustavo Rodriguez-Rivera, Carolyn J. C. Schauble, Thomas P.",
  "Skinner, Yannis Smaragdakis, Jesse St. Laurent, John Stankovic, Adam Stauffer,\nSteven Stepanek, John Sterling, Hal Stern, Louis Stevens, Pete Thomas, David\nUmbaugh, Steve Vinoski, Tommy Wagner, Larry L. Wear, John Werth, James\nM. Westall, J. S. Weston, and Yang Xiang\nRobert Love updated both Chapter 18 and the Linux coverage throughout\nthe text, as well as answering many of our Android-related questions. Chapter\n19 was written by Dave Probert and was derived from Chapter 22 of the Eighth\nEdition of Operating System Concepts. Jonathan Katz contributed to Chapter\n15. Richard West provided input into Chapter 16. Salahuddin Khan updated\nSection 15.9 to provide new coverage of Windows 7 security.\nParts of Chapter 17 were derived from a paper by Levy and Silberschatz",
  "[1990]. Chapter 18 was derived from an unpublished manuscript by Stephen\nTweedie. Cliff Martin helped with updating the UNIX appendix to cover\nFreeBSD. Some of the exercises and accompanying solutions were supplied by\nArvind Krishnamurthy. Andrew DeNicola prepared the student study guide\nthat is available on our website. Some of the the slides were prepeared by\nMarilyn Turnamian.\nMike Shapiro, Bryan Cantrill, and Jim Mauro answered several Solaris-\nrelated questions, and Bryan Cantrill from Sun Microsystems helped with the\nZFS coverage. Josh Dees and Rob Reynolds contributed coverage of Microsoft’s\nNET. The project for POSIX message queues was contributed by John Trono of\nSaint Michael’s College in Colchester, Vermont.\nJudi Paige helped with generating ﬁgures and presentation of slides.",
  "Thomas Gagne prepared new artwork for this edition. Owen Galvin helped\ncopy-edit Chapter 16. Mark Wogahn has made sure that the software to produce\nthis book (LATEX and fonts) works properly. Ranjan Kumar Meher rewrote some\nof the LATEX software used in the production of this new text. Preface\nxv\nOur Executive Editor, Beth Lang Golub, provided expert guidance as we\nprepared this edition. She was assisted by Katherine Willis, who managed\nmany details of the project smoothly. The Senior Production Editor, Ken Santor,\nwas instrumental in handling all the production details.\nThe cover illustrator was Susan Cyr, and the cover designer was Madelyn\nLesure. Beverly Peavler copy-edited the manuscript. The freelance proofreader\nwas Katrina Avery; the freelance indexer was WordCo, Inc.",
  "Abraham Silberschatz, New Haven, CT, 2012\nPeter Baer Galvin, Boston, MA, 2012\nGreg Gagne, Salt Lake City, UT, 2012  Contents\nPART ONE\nOVERVIEW\nChapter 1\nIntroduction\n1.1 What Operating Systems Do\n4\n1.2 Computer-System Organization\n7\n1.3 Computer-System Architecture\n12\n1.4 Operating-System Structure\n19\n1.5 Operating-System Operations\n21\n1.6 Process Management\n24\n1.7 Memory Management\n25\n1.8 Storage Management\n26\n1.9 Protection and Security\n30\n1.10 Kernel Data Structures\n31\n1.11 Computing Environments\n35\n1.12 Open-Source Operating Systems\n43\n1.13 Summary\n47\nExercises\n49\nBibliographical Notes\n52\nChapter 2\nOperating-System Structures\n2.1 Operating-System Services\n55\n2.2 User and Operating-System\nInterface\n58\n2.3 System Calls\n62\n2.4 Types of System Calls\n66\n2.5 System Programs\n74",
  "66\n2.5 System Programs\n74\n2.6 Operating-System Design and\nImplementation\n75\n2.7 Operating-System Structure\n78\n2.8 Operating-System Debugging\n86\n2.9 Operating-System Generation\n91\n2.10 System Boot\n92\n2.11 Summary\n93\nExercises\n94\nBibliographical Notes\n101\nPART TWO\nPROCESS MANAGEMENT\nChapter 3\nProcesses\n3.1 Process Concept\n105\n3.2 Process Scheduling\n110\n3.3 Operations on Processes\n115\n3.4 Interprocess Communication\n122\n3.5 Examples of IPC Systems\n130\n3.6 Communication in Client–\nServer Systems\n136\n3.7 Summary\n147\nExercises\n149\nBibliographical Notes\n161\nxvii xviii\nContents\nChapter 4\nThreads\n4.1 Overview\n163\n4.2 Multicore Programming\n166\n4.3 Multithreading Models\n169\n4.4 Thread Libraries\n171\n4.5 Implicit Threading\n177\n4.6 Threading Issues\n183\n4.7 Operating-System Examples\n188\n4.8 Summary\n191",
  "4.7 Operating-System Examples\n188\n4.8 Summary\n191\nExercises\n191\nBibliographical Notes\n199\nChapter 5\nProcess Synchronization\n5.1 Background\n203\n5.2 The Critical-Section Problem\n206\n5.3 Peterson’s Solution\n207\n5.4 Synchronization Hardware\n209\n5.5 Mutex Locks\n212\n5.6 Semaphores\n213\n5.7 Classic Problems of\nSynchronization\n219\n5.8 Monitors\n223\n5.9 Synchronization Examples\n232\n5.10 Alternative Approaches\n238\n5.11 Summary\n242\nExercises\n242\nBibliographical Notes\n258\nChapter 6\nCPU Scheduling\n6.1 Basic Concepts\n261\n6.2 Scheduling Criteria\n265\n6.3 Scheduling Algorithms\n266\n6.4 Thread Scheduling\n277\n6.5 Multiple-Processor Scheduling\n278\n6.6 Real-Time CPU Scheduling\n283\n6.7 Operating-System Examples\n290\n6.8 Algorithm Evaluation\n300\n6.9 Summary\n304\nExercises\n305\nBibliographical Notes\n311\nChapter 7",
  "Exercises\n305\nBibliographical Notes\n311\nChapter 7\nDeadlocks\n7.1 System Model\n315\n7.2 Deadlock Characterization\n317\n7.3 Methods for Handling Deadlocks\n322\n7.4 Deadlock Prevention\n323\n7.5 Deadlock Avoidance\n327\n7.6 Deadlock Detection\n333\n7.7 Recovery from Deadlock\n337\n7.8 Summary\n339\nExercises\n339\nBibliographical Notes\n346\nPART THREE\nMEMORY MANAGEMENT\nChapter 8\nMain Memory\n8.1 Background\n351\n8.2 Swapping\n358\n8.3 Contiguous Memory Allocation\n360\n8.4 Segmentation\n364\n8.5 Paging\n366\n8.6 Structure of the Page Table\n378\n8.7 Example: Intel 32 and 64-bit\nArchitectures\n383\n8.8 Example: ARM Architecture\n388\n8.9 Summary\n389\nExercises\n390\nBibliographical Notes\n394 Contents\nxix\nChapter 9\nVirtual Memory\n9.1 Background\n397\n9.2 Demand Paging\n401\n9.3 Copy-on-Write\n408\n9.4 Page Replacement\n409",
  "9.3 Copy-on-Write\n408\n9.4 Page Replacement\n409\n9.5 Allocation of Frames\n421\n9.6 Thrashing\n425\n9.7 Memory-Mapped Files\n430\n9.8 Allocating Kernel Memory\n436\n9.9 Other Considerations\n439\n9.10 Operating-System Examples\n445\n9.11 Summary\n448\nExercises\n449\nBibliographical Notes\n461\nPART FOUR\nSTORAGE MANAGEMENT\nChapter 10\nMass-Storage Structure\n10.1 Overview of Mass-Storage\nStructure\n467\n10.2 Disk Structure\n470\n10.3 Disk Attachment\n471\n10.4 Disk Scheduling\n472\n10.5 Disk Management\n478\n10.6 Swap-Space Management\n482\n10.7 RAID Structure\n484\n10.8 Stable-Storage Implementation\n494\n10.9 Summary\n496\nExercises\n497\nBibliographical Notes\n501\nChapter 11\nFile-System Interface\n11.1 File Concept\n503\n11.2 Access Methods\n513\n11.3 Directory and Disk Structure\n515\n11.4 File-System Mounting\n526\n11.5 File Sharing",
  "11.4 File-System Mounting\n526\n11.5 File Sharing\n528\n11.6 Protection\n533\n11.7 Summary\n538\nExercises\n539\nBibliographical Notes\n541\nChapter 12\nFile-System Implementation\n12.1 File-System Structure\n543\n12.2 File-System Implementation\n546\n12.3 Directory Implementation\n552\n12.4 Allocation Methods\n553\n12.5 Free-Space Management\n561\n12.6 Efﬁciency and Performance\n564\n12.7 Recovery\n568\n12.8 NFS\n571\n12.9 Example: The WAFL File System\n577\n12.10 Summary\n580\nExercises\n581\nBibliographical Notes\n585\nChapter 13\nI/O Systems\n13.1 Overview\n587\n13.2 I/O Hardware\n588\n13.3 Application I/O Interface\n597\n13.4 Kernel I/O Subsystem\n604\n13.5 Transforming I/O Requests to\nHardware Operations\n611\n13.6 STREAMS\n613\n13.7 Performance\n615\n13.8 Summary\n618\nExercises\n619\nBibliographical Notes\n621 xx\nContents\nPART FIVE",
  "Bibliographical Notes\n621 xx\nContents\nPART FIVE\nPROTECTION AND SECURITY\nChapter 14\nProtection\n14.1 Goals of Protection\n625\n14.2 Principles of Protection\n626\n14.3 Domain of Protection\n627\n14.4 Access Matrix\n632\n14.5 Implementation of the Access\nMatrix\n636\n14.6 Access Control\n639\n14.7 Revocation of Access Rights\n640\n14.8 Capability-Based Systems\n641\n14.9 Language-Based Protection\n644\n14.10 Summary\n649\nExercises\n650\nBibliographical Notes\n652\nChapter 15\nSecurity\n15.1 The Security Problem\n657\n15.2 Program Threats\n661\n15.3 System and Network Threats\n669\n15.4 Cryptography as a Security Tool\n674\n15.5 User Authentication\n685\n15.6 Implementing Security Defenses\n689\n15.7 Firewalling to Protect Systems and\nNetworks\n696\n15.8 Computer-Security\nClassiﬁcations\n698\n15.9 An Example: Windows 7\n699",
  "Classiﬁcations\n698\n15.9 An Example: Windows 7\n699\n15.10 Summary\n701\nExercises\n702\nBibliographical Notes\n704\nPART SIX\nADVANCED TOPICS\nChapter 16\nVirtual Machines\n16.1 Overview\n711\n16.2 History\n713\n16.3 Beneﬁts and Features\n714\n16.4 Building Blocks\n717\n16.5 Types of Virtual Machines and Their\nImplementations\n721\n16.6 Virtualization and Operating-System\nComponents\n728\n16.7 Examples\n735\n16.8 Summary\n737\nExercises\n738\nBibliographical Notes\n739\nChapter 17\nDistributed Systems\n17.1 Advantages of Distributed\nSystems\n741\n17.2 Types of Network-\nbased Operating Systems\n743\n17.3 Network Structure\n747\n17.4 Communication Structure\n751\n17.5 Communication Protocols\n756\n17.6 An Example: TCP/IP\n760\n17.7 Robustness\n762\n17.8 Design Issues\n764\n17.9 Distributed File Systems\n765\n17.10 Summary\n773\nExercises\n774",
  "765\n17.10 Summary\n773\nExercises\n774\nBibliographical Notes\n777 Contents\nxxi\nPART SEVEN\nCASE STUDIES\nChapter 18\nThe Linux System\n18.1 Linux History\n781\n18.2 Design Principles\n786\n18.3 Kernel Modules\n789\n18.4 Process Management\n792\n18.5 Scheduling\n795\n18.6 Memory Management\n800\n18.7 File Systems\n809\n18.8 Input and Output\n815\n18.9 Interprocess Communication\n818\n18.10 Network Structure\n819\n18.11 Security\n821\n18.12 Summary\n824\nExercises\n824\nBibliographical Notes\n826\nChapter 19\nWindows 7\n19.1 History\n829\n19.2 Design Principles\n831\n19.3 System Components\n838\n19.4 Terminal Services and Fast User\nSwitching\n862\n19.5 File System\n863\n19.6 Networking\n869\n19.7 Programmer Interface\n874\n19.8 Summary\n883\nExercises\n883\nBibliographical Notes\n885\nChapter 20\nInﬂuential Operating Systems\n20.1 Feature Migration",
  "20.1 Feature Migration\n887\n20.2 Early Systems\n888\n20.3 Atlas\n895\n20.4 XDS-940\n896\n20.5 THE\n897\n20.6 RC 4000\n897\n20.7 CTSS\n898\n20.8 MULTICS\n899\n20.9 IBM OS/360\n899\n20.10 TOPS-20\n901\n20.11 CP/M and MS/DOS\n901\n20.12 Macintosh Operating System and\nWindows\n902\n20.13 Mach\n902\n20.14 Other Systems\n904\nExercises\n904\nBibliographical Notes\n904\nPART EIGHT\nAPPENDICES\nAppendix A\nBSD UNIX\nA.1 UNIX History\nA1\nA.2 Design Principles\nA6\nA.3 Programmer Interface\nA8\nA.4 User Interface\nA15\nA.5 Process Management\nA18\nA.6 Memory Management\nA22\nA.7 File System\nA24\nA.8 I/O System\nA32\nA.9 Interprocess Communication\nA36\nA.10 Summary\nA40\nExercises\nA41\nBibliographical Notes\nA42 xxii\nContents\nAppendix B\nThe Mach System\nB.1 History of the Mach System\nB1\nB.2 Design Principles\nB3\nB.3 System Components\nB4",
  "B.2 Design Principles\nB3\nB.3 System Components\nB4\nB.4 Process Management\nB7\nB.5 Interprocess Communication\nB13\nB.6 Memory Management\nB18\nB.7 Programmer Interface\nB23\nB.8 Summary\nB24\nExercises\nB25\nBibliographical Notes\nB26 Part One\nOverview\nAn operating system acts as an intermediary between the user of a\ncomputer and the computer hardware. The purpose of an operating\nsystem is to provide an environment in which a user can execute\nprograms in a convenient and efﬁcient manner.\nAn operating system is software that manages the computer hard-\nware. The hardware must provide appropriate mechanisms to ensure the\ncorrect operation of the computer system and to prevent user programs\nfrom interfering with the proper operation of the system.",
  "Internally, operating systems vary greatly in their makeup, since they\nare organized along many different lines. The design of a new operating\nsystem is a major task. It is important that the goals of the system be well\ndeﬁned before the design begins. These goals form the basis for choices\namong various algorithms and strategies.\nBecause an operating system is large and complex, it must be created\npiece by piece. Each of these pieces should be a well-delineated portion\nof the system, with carefully deﬁned inputs, outputs, and functions.  1\nC H A P T E R\nIntroduction\nAn operating system is a program that manages a computer’s hardware. It\nalso provides a basis for application programs and acts as an intermediary\nbetween the computer user and the computer hardware. An amazing aspect of",
  "operating systems is how they vary in accomplishing these tasks. Mainframe\noperating systems are designed primarily to optimize utilization of hardware.\nPersonal computer (PC) operating systems support complex games, business\napplications, and everything in between. Operating systems for mobile com-\nputers provide an environment in which a user can easily interface with the\ncomputer to execute programs. Thus, some operating systems are designed to\nbe convenient, others to be efﬁcient, and others to be some combination of the\ntwo.\nBefore we can explore the details of computer system operation, we need to\nknow something about system structure. We thus discuss the basic functions\nof system startup, I/O, and storage early in this chapter. We also describe",
  "the basic computer architecture that makes it possible to write a functional\noperating system.\nBecause an operating system is large and complex, it must be created\npiece by piece. Each of these pieces should be a well-delineated portion of the\nsystem, with carefully deﬁned inputs, outputs, and functions. In this chapter,\nwe provide a general overview of the major components of a contemporary\ncomputer system as well as the functions provided by the operating system.\nAdditionally, we cover several other topics to help set the stage for the\nremainder of this text: data structures used in operating systems, computing\nenvironments, and open-source operating systems.\nCHAPTER OBJECTIVES\n• To describe the basic organization of computer systems.",
  "• To provide a grand tour of the major components of operating systems.\n• To give an overview of the many types of computing environments.\n• To explore several open-source operating systems.\n3 4\nChapter 1\nIntroduction\nuser\n1\nuser\n2\nuser\n3\ncomputer hardware\noperating system\nsystem and application programs\ncompiler\nassembler\ntext editor\ndatabase\nsystem\nuser\nn\n…\n…\nFigure 1.1\nAbstract view of the components of a computer system.\n1.1\nWhat Operating Systems Do\nWe begin our discussion by looking at the operating system’s role in the\noverall computer system. A computer system can be divided roughly into four\ncomponents: the hardware, the operating system, the application programs,\nand the users (Figure 1.1).\nThe hardware—the central processing unit (CPU), the memory, and the",
  "input/output (I/O) devices—provides the basic computing resources for the\nsystem. The application programs—such as word processors, spreadsheets,\ncompilers, and Web browsers—deﬁne the ways in which these resources are\nused to solve users’ computing problems. The operating system controls the\nhardware and coordinates its use among the various application programs for\nthe various users.\nWe can also view a computer system as consisting of hardware, software,\nand data. The operating system provides the means for proper use of these\nresources in the operation of the computer system. An operating system is\nsimilar to a government. Like a government, it performs no useful function by\nitself. It simply provides an environment within which other programs can do\nuseful work.",
  "useful work.\nTo understand more fully the operating system’s role, we next explore\noperating systems from two viewpoints: that of the user and that of the system.\n1.1.1\nUser View\nThe user’s view of the computer varies according to the interface being\nused. Most computer users sit in front of a PC, consisting of a monitor,\nkeyboard, mouse, and system unit. Such a system is designed for one user 1.1\nWhat Operating Systems Do\n5\nto monopolize its resources. The goal is to maximize the work (or play) that\nthe user is performing. In this case, the operating system is designed mostly\nfor ease of use, with some attention paid to performance and none paid\nto resource utilization—how various hardware and software resources are",
  "shared. Performance is, of course, important to the user; but such systems\nare optimized for the single-user experience rather than the requirements of\nmultiple users.\nIn other cases, a user sits at a terminal connected to a mainframe or a\nminicomputer. Other users are accessing the same computer through other\nterminals. These users share resources and may exchange information. The\noperating system in such cases is designed to maximize resource utilization—\nto assure that all available CPU time, memory, and I/O are used efﬁciently and\nthat no individual user takes more than her fair share.\nIn still other cases, users sit at workstations connected to networks of\nother workstations and servers. These users have dedicated resources at",
  "their disposal, but they also share resources such as networking and servers,\nincluding ﬁle, compute, and print servers. Therefore, their operating system is\ndesigned to compromise between individual usability and resource utilization.\nRecently, many varieties of mobile computers, such as smartphones and\ntablets, have come into fashion. Most mobile computers are standalone units for\nindividual users. Quite often, they are connected to networks through cellular\nor other wireless technologies. Increasingly, these mobile devices are replacing\ndesktop and laptop computers for people who are primarily interested in\nusing computers for e-mail and web browsing. The user interface for mobile\ncomputers generally features a touch screen, where the user interacts with the",
  "system by pressing and swiping ﬁngers across the screen rather than using a\nphysical keyboard and mouse.\nSome computers have little or no user view. For example, embedded\ncomputers in home devices and automobiles may have numeric keypads and\nmay turn indicator lights on or off to show status, but they and their operating\nsystems are designed primarily to run without user intervention.\n1.1.2\nSystem View\nFrom the computer’s point of view, the operating system is the program\nmost intimately involved with the hardware. In this context, we can view\nan operating system as a resource allocator. A computer system has many\nresources that may be required to solve a problem: CPU time, memory space,\nﬁle-storage space, I/O devices, and so on. The operating system acts as the",
  "manager of these resources. Facing numerous and possibly conﬂicting requests\nfor resources, the operating system must decide how to allocate them to speciﬁc\nprograms and users so that it can operate the computer system efﬁciently and\nfairly. As we have seen, resource allocation is especially important where many\nusers access the same mainframe or minicomputer.\nA slightly different view of an operating system emphasizes the need to\ncontrol the various I/O devices and user programs. An operating system is a\ncontrol program. A control program manages the execution of user programs\nto prevent errors and improper use of the computer. It is especially concerned\nwith the operation and control of I/O devices. 6\nChapter 1\nIntroduction\n1.1.3\nDeﬁning Operating Systems",
  "Introduction\n1.1.3\nDeﬁning Operating Systems\nBy now, you can probably see that the term operating system covers many roles\nand functions. That is the case, at least in part, because of the myriad designs\nand uses of computers. Computers are present within toasters, cars, ships,\nspacecraft, homes, and businesses. They are the basis for game machines, music\nplayers, cable TV tuners, and industrial control systems. Although computers\nhave a relatively short history, they have evolved rapidly. Computing started\nas an experiment to determine what could be done and quickly moved to\nﬁxed-purpose systems for military uses, such as code breaking and trajectory\nplotting, and governmental uses, such as census calculation. Those early",
  "computers evolved into general-purpose, multifunction mainframes, and\nthat’s when operating systems were born. In the 1960s, Moore’s Law predicted\nthat the number of transistors on an integrated circuit would double every\neighteen months, and that prediction has held true. Computers gained in\nfunctionality and shrunk in size, leading to a vast number of uses and a vast\nnumber and variety of operating systems. (See Chapter 20 for more details on\nthe history of operating systems.)\nHow, then, can we deﬁne what an operating system is? In general, we have\nno completely adequate deﬁnition of an operating system. Operating systems\nexist because they offer a reasonable way to solve the problem of creating a\nusable computing system. The fundamental goal of computer systems is to",
  "execute user programs and to make solving user problems easier. Computer\nhardware is constructed toward this goal. Since bare hardware alone is not\nparticularly easy to use, application programs are developed. These programs\nrequire certain common operations, such as those controlling the I/O devices.\nThe common functions of controlling and allocating resources are then brought\ntogether into one piece of software: the operating system.\nIn addition, we have no universally accepted deﬁnition of what is part of the\noperating system. A simple viewpoint is that it includes everything a vendor\nships when you order “the operating system.” The features included, however,\nvary greatly across systems. Some systems take up less than a megabyte of",
  "space and lack even a full-screen editor, whereas others require gigabytes of\nspace and are based entirely on graphical windowing systems. A more common\ndeﬁnition, and the one that we usually follow, is that the operating system\nis the one program running at all times on the computer—usually called\nthe kernel. (Along with the kernel, there are two other types of programs:\nsystem programs, which are associated with the operating system but are not\nnecessarily part of the kernel, and application programs, which include all\nprograms not associated with the operation of the system.)\nThe matter of what constitutes an operating system became increasingly\nimportant as personal computers became more widespread and operating",
  "systems grew increasingly sophisticated. In 1998, the United States Department\nof Justice ﬁled suit against Microsoft, in essence claiming that Microsoft\nincluded too much functionality in its operating systems and thus prevented\napplication vendors from competing. (For example, a Web browser was an\nintegral part of the operating systems.) As a result, Microsoft was found guilty\nof using its operating-system monopoly to limit competition.\nToday, however, if we look at operating systems for mobile devices, we\nsee that once again the number of features constituting the operating system 1.2\nComputer-System Organization\n7\nis increasing. Mobile operating systems often include not only a core kernel\nbut also middleware—a set of software frameworks that provide additional",
  "services to application developers. For example, each of the two most promi-\nnent mobile operating systems—Apple’s iOS and Google’s Android—features\na core kernel along with middleware that supports databases, multimedia, and\ngraphics (to name a only few).\n1.2\nComputer-System Organization\nBefore we can explore the details of how computer systems operate, we need\ngeneral knowledge of the structure of a computer system. In this section,\nwe look at several parts of this structure. The section is mostly concerned\nwith computer-system organization, so you can skim or skip it if you already\nunderstand the concepts.\n1.2.1\nComputer-System Operation\nA modern general-purpose computer system consists of one or more CPUs\nand a number of device controllers connected through a common bus that",
  "provides access to shared memory (Figure 1.2). Each device controller is in\ncharge of a speciﬁc type of device (for example, disk drives, audio devices,\nor video displays). The CPU and the device controllers can execute in parallel,\ncompeting for memory cycles. To ensure orderly access to the shared memory,\na memory controller synchronizes access to the memory.\nFor a computer to start running—for instance, when it is powered up or\nrebooted—it needs to have an initial program to run. This initial program,\nor bootstrap program, tends to be simple. Typically, it is stored within\nthe computer hardware in read-only memory (ROM) or electrically erasable\nprogrammable read-only memory (EEPROM), known by the general term",
  "ﬁrmware. It initializes all aspects of the system, from CPU registers to device\ncontrollers to memory contents. The bootstrap program must know how to load\nthe operating system and how to start executing that system. To accomplish\nUSB controller\nkeyboard\nprinter\nmouse\nmonitor\ndisks\ngraphics\nadapter\ndisk\ncontroller\nmemory\nCPU\non-line\nFigure 1.2\nA modern computer system. 8\nChapter 1\nIntroduction\nuser\nprocess\nexecuting\nCPU\nI/O interrupt\nprocessing\nI/O\nrequest\ntransfer\ndone\nI/O\nrequest\ntransfer\ndone\nI/O\ndevice\nidle\ntransferring\nFigure 1.3\nInterrupt timeline for a single process doing output.\nthis goal, the bootstrap program must locate the operating-system kernel and\nload it into memory.\nOnce the kernel is loaded and executing, it can start providing services to",
  "the system and its users. Some services are provided outside of the kernel, by\nsystem programs that are loaded into memory at boot time to become system\nprocesses, or system daemons that run the entire time the kernel is running.\nOn UNIX, the ﬁrst system process is “init,” and it starts many other daemons.\nOnce this phase is complete, the system is fully booted, and the system waits\nfor some event to occur.\nThe occurrence of an event is usually signaled by an interrupt from either\nthe hardware or the software. Hardware may trigger an interrupt at any time\nby sending a signal to the CPU, usually by way of the system bus. Software\nmay trigger an interrupt by executing a special operation called a system call\n(also called a monitor call).",
  "(also called a monitor call).\nWhen the CPU is interrupted, it stops what it is doing and immediately\ntransfers execution to a ﬁxed location. The ﬁxed location usually contains\nthe starting address where the service routine for the interrupt is located.\nThe interrupt service routine executes; on completion, the CPU resumes the\ninterrupted computation. A timeline of this operation is shown in Figure 1.3.\nInterrupts are an important part of a computer architecture. Each computer\ndesign has its own interrupt mechanism, but several functions are common.\nThe interrupt must transfer control to the appropriate interrupt service routine.\nThe straightforward method for handling this transfer would be to invoke\na generic routine to examine the interrupt information. The routine, in turn,",
  "would call the interrupt-speciﬁc handler. However, interrupts must be handled\nquickly. Since only a predeﬁned number of interrupts is possible, a table of\npointers to interrupt routines can be used instead to provide the necessary\nspeed. The interrupt routine is called indirectly through the table, with no\nintermediate routine needed. Generally, the table of pointers is stored in low\nmemory (the ﬁrst hundred or so locations). These locations hold the addresses\nof the interrupt service routines for the various devices. This array, or interrupt\nvector, of addresses is then indexed by a unique device number, given with\nthe interrupt request, to provide the address of the interrupt service routine for 1.2\nComputer-System Organization\n9\nSTORAGE DEFINITIONS AND NOTATION",
  "9\nSTORAGE DEFINITIONS AND NOTATION\nThe basic unit of computer storage is the bit. A bit can contain one of two\nvalues, 0 and 1. All other storage in a computer is based on collections of bits.\nGiven enough bits, it is amazing how many things a computer can represent:\nnumbers, letters, images, movies, sounds, documents, and programs, to name\na few. A byte is 8 bits, and on most computers it is the smallest convenient\nchunk of storage. For example, most computers don’t have an instruction to\nmove a bit but do have one to move a byte. A less common term is word,\nwhich is a given computer architecture’s native unit of data. A word is made\nup of one or more bytes. For example, a computer that has 64-bit registers and\n64-bit memory addressing typically has 64-bit (8-byte) words. A computer",
  "executes many operations in its native word size rather than a byte at a time.\nComputer storage, along with most computer throughput, is generally\nmeasured and manipulated in bytes and collections of bytes. A kilobyte, or\nKB, is 1,024 bytes; a megabyte, or MB, is 1,0242 bytes; a gigabyte, or GB, is\n1,0243 bytes; a terabyte, or TB, is 1,0244 bytes; and a petabyte, or PB, is 1,0245\nbytes. Computer manufacturers often round off these numbers and say that\na megabyte is 1 million bytes and a gigabyte is 1 billion bytes. Networking\nmeasurements are an exception to this general rule; they are given in bits\n(because networks move data a bit at a time).\nthe interrupting device. Operating systems as different as Windows and UNIX\ndispatch interrupts in this manner.",
  "dispatch interrupts in this manner.\nThe interrupt architecture must also save the address of the interrupted\ninstruction. Many old designs simply stored the interrupt address in a\nﬁxed location or in a location indexed by the device number. More recent\narchitectures store the return address on the system stack. If the interrupt\nroutine needs to modify the processor state—for instance, by modifying\nregister values—it must explicitly save the current state and then restore that\nstate before returning. After the interrupt is serviced, the saved return address\nis loaded into the program counter, and the interrupted computation resumes\nas though the interrupt had not occurred.\n1.2.2\nStorage Structure\nThe CPU can load instructions only from memory, so any programs to run must",
  "be stored there. General-purpose computers run most of their programs from\nrewritable memory, called main memory (also called random-access memory,\nor RAM). Main memory commonly is implemented in a semiconductor\ntechnology called dynamic random-access memory (DRAM).\nComputers use other forms of memory as well. We have already mentioned\nread-only memory, ROM) and electrically erasable programmable read-only\nmemory, EEPROM). Because ROM cannot be changed, only static programs, such\nas the bootstrap program described earlier, are stored there. The immutability\nof ROM is of use in game cartridges. EEPROM can be changed but cannot\nbe changed frequently and so contains mostly static programs. For example,\nsmartphones have EEPROM to store their factory-installed programs. 10\nChapter 1",
  "Chapter 1\nIntroduction\nAll forms of memory provide an array of bytes. Each byte has its\nown address. Interaction is achieved through a sequence of load or store\ninstructions to speciﬁc memory addresses. The load instruction moves a byte\nor word from main memory to an internal register within the CPU, whereas the\nstore instruction moves the content of a register to main memory. Aside from\nexplicit loads and stores, the CPU automatically loads instructions from main\nmemory for execution.\nA typical instruction–execution cycle, as executed on a system with a von\nNeumann architecture, ﬁrst fetches an instruction from memory and stores\nthat instruction in the instruction register. The instruction is then decoded\nand may cause operands to be fetched from memory and stored in some",
  "internal register. After the instruction on the operands has been executed, the\nresult may be stored back in memory. Notice that the memory unit sees only\na stream of memory addresses. It does not know how they are generated (by\nthe instruction counter, indexing, indirection, literal addresses, or some other\nmeans) or what they are for (instructions or data). Accordingly, we can ignore\nhow a memory address is generated by a program. We are interested only in\nthe sequence of memory addresses generated by the running program.\nIdeally, we want the programs and data to reside in main memory\npermanently. This arrangement usually is not possible for the following two\nreasons:\n1. Main memory is usually too small to store all needed programs and data\npermanently.",
  "permanently.\n2. Main memory is a volatile storage device that loses its contents when\npower is turned off or otherwise lost.\nThus, most computer systems provide secondary storage as an extension of\nmain memory. The main requirement for secondary storage is that it be able to\nhold large quantities of data permanently.\nThe most common secondary-storage device is a magnetic disk, which\nprovides storage for both programs and data. Most programs (system and\napplication) are stored on a disk until they are loaded into memory. Many\nprograms then use the disk as both the source and the destination of their\nprocessing. Hence, the proper management of disk storage is of central\nimportance to a computer system, as we discuss in Chapter 10.",
  "In a larger sense, however, the storage structure that we have described—\nconsisting of registers, main memory, and magnetic disks—is only one of many\npossible storage systems. Others include cache memory, CD-ROM, magnetic\ntapes, and so on. Each storage system provides the basic functions of storing\na datum and holding that datum until it is retrieved at a later time. The main\ndifferences among the various storage systems lie in speed, cost, size, and\nvolatility.\nThe wide variety of storage systems can be organized in a hierarchy (Figure\n1.4) according to speed and cost. The higher levels are expensive, but they are\nfast. As we move down the hierarchy, the cost per bit generally decreases,\nwhereas the access time generally increases. This trade-off is reasonable; if a",
  "given storage system were both faster and less expensive than another—other\nproperties being the same—then there would be no reason to use the slower,\nmore expensive memory. In fact, many early storage devices, including paper 1.2\nComputer-System Organization\n11\nregisters\ncache\nmain memory\nsolid-state disk\nmagnetic disk\noptical disk\nmagnetic tapes\nFigure 1.4\nStorage-device hierarchy.\ntape and core memories, are relegated to museums now that magnetic tape and\nsemiconductor memory have become faster and cheaper. The top four levels\nof memory in Figure 1.4 may be constructed using semiconductor memory.\nIn addition to differing in speed and cost, the various storage systems are\neither volatile or nonvolatile. As mentioned earlier, volatile storage loses its",
  "contents when the power to the device is removed. In the absence of expensive\nbattery and generator backup systems, data must be written to nonvolatile\nstorage for safekeeping. In the hierarchy shown in Figure 1.4, the storage\nsystems above the solid-state disk are volatile, whereas those including the\nsolid-state disk and below are nonvolatile.\nSolid-state disks have several variants but in general are faster than\nmagnetic disks and are nonvolatile. One type of solid-state disk stores data in a\nlarge DRAM array during normal operation but also contains a hidden magnetic\nhard disk and a battery for backup power. If external power is interrupted, this\nsolid-state disk’s controller copies the data from RAM to the magnetic disk.",
  "When external power is restored, the controller copies the data back into RAM.\nAnother form of solid-state disk is ﬂash memory, which is popular in cameras\nand personal digital assistants (PDAs), in robots, and increasingly for storage\non general-purpose computers. Flash memory is slower than DRAM but needs\nno power to retain its contents. Another form of nonvolatile storage is NVRAM,\nwhich is DRAM with battery backup power. This memory can be as fast as\nDRAM and (as long as the battery lasts) is nonvolatile.\nThe design of a complete memory system must balance all the factors just\ndiscussed: it must use only as much expensive memory as necessary while\nproviding as much inexpensive, nonvolatile memory as possible. Caches can 12\nChapter 1\nIntroduction",
  "Chapter 1\nIntroduction\nbe installed to improve performance where a large disparity in access time or\ntransfer rate exists between two components.\n1.2.3\nI/O Structure\nStorage is only one of many types of I/O devices within a computer. A large\nportion of operating system code is dedicated to managing I/O, both because\nof its importance to the reliability and performance of a system and because of\nthe varying nature of the devices. Next, we provide an overview of I/O.\nA general-purpose computer system consists of CPUs and multiple device\ncontrollers that are connected through a common bus. Each device controller\nis in charge of a speciﬁc type of device. Depending on the controller, more\nthan one device may be attached. For instance, seven or more devices can be",
  "attached to the small computer-systems interface (SCSI) controller. A device\ncontroller maintains some local buffer storage and a set of special-purpose\nregisters. The device controller is responsible for moving the data between\nthe peripheral devices that it controls and its local buffer storage. Typically,\noperating systems have a device driver for each device controller. This device\ndriver understands the device controller and provides the rest of the operating\nsystem with a uniform interface to the device.\nTo start an I/O operation, the device driver loads the appropriate registers\nwithin the device controller. The device controller, in turn, examines the\ncontents of these registers to determine what action to take (such as “read",
  "a character from the keyboard”). The controller starts the transfer of data from\nthe device to its local buffer. Once the transfer of data is complete, the device\ncontroller informs the device driver via an interrupt that it has ﬁnished its\noperation. The device driver then returns control to the operating system,\npossibly returning the data or a pointer to the data if the operation was a read.\nFor other operations, the device driver returns status information.\nThis form of interrupt-driven I/O is ﬁne for moving small amounts of data\nbut can produce high overhead when used for bulk data movement such as disk\nI/O. To solve this problem, direct memory access (DMA) is used. After setting\nup buffers, pointers, and counters for the I/O device, the device controller",
  "transfers an entire block of data directly to or from its own buffer storage to\nmemory, with no intervention by the CPU. Only one interrupt is generated per\nblock, to tell the device driver that the operation has completed, rather than\nthe one interrupt per byte generated for low-speed devices. While the device\ncontroller is performing these operations, the CPU is available to accomplish\nother work.\nSome high-end systems use switch rather than bus architecture. On these\nsystems, multiple components can talk to other components concurrently,\nrather than competing for cycles on a shared bus. In this case, DMA is even\nmore effective. Figure 1.5 shows the interplay of all components of a computer\nsystem.\n1.3\nComputer-System Architecture",
  "system.\n1.3\nComputer-System Architecture\nIn Section 1.2, we introduced the general structure of a typical computer system.\nA computer system can be organized in a number of different ways, which we 1.3\nComputer-System Architecture\n13\nthread of execution\ninstructions\nand\ndata\ninstruction execution\ncycle\ndata movement\nDMA\nmemory\ninterrupt\ncache\ndata\nI/O request\nCPU (*N)\ndevice\n(*M)\nFigure 1.5\nHow a modern computer system works.\ncan categorize roughly according to the number of general-purpose processors\nused.\n1.3.1\nSingle-Processor Systems\nUntil recently, most computer systems used a single processor. On a single-\nprocessor system, there is one main CPUcapable of executing a general-purpose\ninstruction set, including instructions from user processes. Almost all single-",
  "processor systems have other special-purpose processors as well. They may\ncome in the form of device-speciﬁc processors, such as disk, keyboard, and\ngraphics controllers; or, on mainframes, they may come in the form of more\ngeneral-purpose processors, such as I/O processors that move data rapidly\namong the components of the system.\nAll of these special-purpose processors run a limited instruction set and\ndo not run user processes. Sometimes, they are managed by the operating\nsystem, in that the operating system sends them information about their next\ntask and monitors their status. For example, a disk-controller microprocessor\nreceives a sequence of requests from the main CPU and implements its own disk\nqueue and scheduling algorithm. This arrangement relieves the main CPU of",
  "the overhead of disk scheduling. PCs contain a microprocessor in the keyboard\nto convert the keystrokes into codes to be sent to the CPU. In other systems\nor circumstances, special-purpose processors are low-level components built\ninto the hardware. The operating system cannot communicate with these\nprocessors; they do their jobs autonomously. The use of special-purpose\nmicroprocessors is common and does not turn a single-processor system into 14\nChapter 1\nIntroduction\na multiprocessor. If there is only one general-purpose CPU, then the system is\na single-processor system.\n1.3.2\nMultiprocessor Systems\nWithin the past several years, multiprocessor systems (also known as parallel\nsystems or multicore systems) have begun to dominate the landscape of",
  "computing. Such systems have two or more processors in close communication,\nsharing the computer bus and sometimes the clock, memory, and peripheral\ndevices. Multiprocessor systems ﬁrst appeared prominently appeared in\nservers and have since migrated to desktop and laptop systems. Recently,\nmultiple processors have appeared on mobile devices such as smartphones\nand tablet computers.\nMultiprocessor systems have three main advantages:\n1. Increased throughput. By increasing the number of processors, we expect\nto get more work done in less time. The speed-up ratio with N processors\nis not N, however; rather, it is less than N. When multiple processors\ncooperate on a task, a certain amount of overhead is incurred in keeping",
  "all the parts working correctly. This overhead, plus contention for shared\nresources, lowers the expected gain from additional processors. Similarly,\nN programmers working closely together do not produce N times the\namount of work a single programmer would produce.\n2. Economy of scale. Multiprocessor systems can cost less than equivalent\nmultiple single-processor systems, because they can share peripherals,\nmass storage, and power supplies. If several programs operate on the\nsame set of data, it is cheaper to store those data on one disk and to have\nall the processors share them than to have many computers with local\ndisks and many copies of the data.\n3. Increased reliability. If functions can be distributed properly among",
  "several processors, then the failure of one processor will not halt the\nsystem, only slow it down. If we have ten processors and one fails, then\neach of the remaining nine processors can pick up a share of the work of\nthe failed processor. Thus, the entire system runs only 10 percent slower,\nrather than failing altogether.\nIncreased reliability of a computer system is crucial in many applications.\nThe ability to continue providing service proportional to the level of surviving\nhardware is called graceful degradation. Some systems go beyond graceful\ndegradation and are called fault tolerant, because they can suffer a failure of\nany single component and still continue operation. Fault tolerance requires\na mechanism to allow the failure to be detected, diagnosed, and, if possible,",
  "corrected. The HP NonStop (formerly Tandem) system uses both hardware and\nsoftware duplication to ensure continued operation despite faults. The system\nconsists of multiple pairs of CPUs, working in lockstep. Both processors in the\npair execute each instruction and compare the results. If the results differ, then\none CPU of the pair is at fault, and both are halted. The process that was being\nexecuted is then moved to another pair of CPUs, and the instruction that failed 1.3\nComputer-System Architecture\n15\nis restarted. This solution is expensive, since it involves special hardware and\nconsiderable hardware duplication.\nThe multiple-processor systems in use today are of two types. Some\nsystems use asymmetric multiprocessing, in which each processor is assigned",
  "a speciﬁc task. A boss processor controls the system; the other processors either\nlook to the boss for instruction or have predeﬁned tasks. This scheme deﬁnes\na boss–worker relationship. The boss processor schedules and allocates work\nto the worker processors.\nThe most common systems use symmetric multiprocessing (SMP), in\nwhich each processor performs all tasks within the operating system. SMP\nmeans that all processors are peers; no boss–worker relationship exists\nbetween processors. Figure 1.6 illustrates a typical SMP architecture. Notice\nthat each processor has its own set of registers, as well as a private—or local\n—cache. However, all processors share physical memory. An example of an\nSMP system is AIX, a commercial version of UNIX designed by IBM. An AIX",
  "system can be conﬁgured to employ dozens of processors. The beneﬁt of this\nmodel is that many processes can run simultaneously—N processes can run\nif there are N CPUs—without causing performance to deteriorate signiﬁcantly.\nHowever, we must carefully control I/O to ensure that the data reach the\nappropriate processor. Also, since the CPUs are separate, one may be sitting\nidle while another is overloaded, resulting in inefﬁciencies. These inefﬁciencies\ncan be avoided if the processors share certain data structures. A multiprocessor\nsystem of this form will allow processes and resources—such as memory—\nto be shared dynamically among the various processors and can lower the\nvariance among the processors. Such a system must be written carefully, as",
  "we shall see in Chapter 5. Virtually all modern operating systems—including\nWindows, Mac OS X, and Linux—now provide support for SMP.\nThe difference between symmetric and asymmetric multiprocessing may\nresult from either hardware or software. Special hardware can differentiate the\nmultiple processors, or the software can be written to allow only one boss and\nmultiple workers. For instance, Sun Microsystems’ operating system SunOS\nVersion 4 provided asymmetric multiprocessing, whereas Version 5 (Solaris) is\nsymmetric on the same hardware.\nMultiprocessing adds CPUs to increase computing power. If the CPU has an\nintegrated memory controller, then adding CPUs can also increase the amount\nCPU0\nregisters\ncache\nCPU1\nregisters\ncache\nCPU2\nregisters\ncache\nmemory\nFigure 1.6",
  "cache\nCPU2\nregisters\ncache\nmemory\nFigure 1.6\nSymmetric multiprocessing architecture. 16\nChapter 1\nIntroduction\nof memory addressable in the system. Either way, multiprocessing can cause\na system to change its memory access model from uniform memory access\n(UMA) to non-uniform memory access (NUMA). UMA is deﬁned as the situation\nin which access to any RAM from any CPU takes the same amount of time. With\nNUMA, some parts of memory may take longer to access than other parts,\ncreating a performance penalty. Operating systems can minimize the NUMA\npenalty through resource management, as discussed in Section 9.5.4.\nA recent trend in CPU design is to include multiple computing cores\non a single chip. Such multiprocessor systems are termed multicore. They",
  "can be more efﬁcient than multiple chips with single cores because on-chip\ncommunication is faster than between-chip communication. In addition, one\nchip with multiple cores uses signiﬁcantly less power than multiple single-core\nchips.\nIt is important to note that while multicore systems are multiprocessor\nsystems, not all multiprocessor systems are multicore, as we shall see in Section\n1.3.3. In our coverage of multiprocessor systems throughout this text, unless\nwe state otherwise, we generally use the more contemporary term multicore,\nwhich excludes some multiprocessor systems.\nIn Figure 1.7, we show a dual-core design with two cores on the same\nchip. In this design, each core has its own register set as well as its own local",
  "cache. Other designs might use a shared cache or a combination of local and\nshared caches. Aside from architectural considerations, such as cache, memory,\nand bus contention, these multicore CPUs appear to the operating system as\nN standard processors. This characteristic puts pressure on operating system\ndesigners—and application programmers—to make use of those processing\ncores.\nFinally, blade servers are a relativelyrecent development in which multiple\nprocessor boards, I/O boards, and networking boards are placed in the same\nchassis. The difference between these and traditional multiprocessor systems\nis that each blade-processor board boots independently and runs its own\noperating system. Some blade-server boards are multiprocessor as well, which",
  "blurs the lines between types of computers. In essence, these servers consist of\nmultiple independent multiprocessor systems.\nCPU core0\nregisters\ncache\nCPU core1\nregisters\ncache\nmemory\nFigure 1.7\nA dual-core design with two cores placed on the same chip. 1.3\nComputer-System Architecture\n17\n1.3.3\nClustered Systems\nAnother type of multiprocessor system is a clustered system, which gathers\ntogether multiple CPUs. Clustered systems differ from the multiprocessor\nsystems described in Section 1.3.2 in that they are composed of two or more\nindividual systems—or nodes—joined together. Such systems are considered\nloosely coupled. Each node may be a single processor system or a multicore\nsystem. We should note that the deﬁnition of clustered is not concrete; many",
  "commercial packages wrestle to deﬁne a clustered system and why one form\nis better than another. The generally accepted deﬁnition is that clustered\ncomputers share storage and are closely linked via a local-area network LAN\n(as described in Chapter 17) or a faster interconnect, such as InﬁniBand.\nClustering is usually used to provide high-availability service—that is,\nservice will continue even if one or more systems in the cluster fail. Generally,\nwe obtain high availability by adding a level of redundancy in the system.\nA layer of cluster software runs on the cluster nodes. Each node can monitor\none or more of the others (over the LAN). If the monitored machine fails,\nthe monitoring machine can take ownership of its storage and restart the",
  "applications that were running on the failed machine. The users and clients of\nthe applications see only a brief interruption of service.\nClustering can be structured asymmetrically or symmetrically. In asym-\nmetric clustering, one machine is in hot-standby mode while the other is\nrunning the applications. The hot-standby host machine does nothing but\nmonitor the active server. If that server fails, the hot-standby host becomes\nthe active server. In symmetric clustering, two or more hosts are running\napplications and are monitoring each other. This structure is obviously more\nefﬁcient, as it uses all of the available hardware. However it does require that\nmore than one application be available to run.\nSince a cluster consists of several computer systems connected via a",
  "network, clusters can also be used to provide high-performance computing\nenvironments. Such systems can supply signiﬁcantly greater computational\npower than single-processor or even SMP systems because they can run an\napplication concurrently on all computers in the cluster. The application must\nhave been written speciﬁcally to take advantage of the cluster, however. This\ninvolves a technique known as parallelization, which divides a program into\nseparate components that run in parallel on individual computers in the cluster.\nTypically, these applications are designed so that once each computing node in\nthe cluster has solved its portion of the problem, the results from all the nodes\nare combined into a ﬁnal solution.\nOther forms of clusters include parallel clusters and clustering over a",
  "wide-area network (WAN) (as described in Chapter 17). Parallel clusters allow\nmultiple hosts to access the same data on shared storage. Because most\noperating systems lack support for simultaneous data access by multiple hosts,\nparallel clusters usually require the use of special versions of software and\nspecial releases of applications. For example, Oracle Real Application Cluster\nis a version of Oracle’s database that has been designed to run on a parallel\ncluster. Each machine runs Oracle, and a layer of software tracks access to the\nshared disk. Each machine has full access to all data in the database. To provide\nthis shared access, the system must also supply access control and locking to 18\nChapter 1\nIntroduction\nBEOWULF CLUSTERS",
  "Chapter 1\nIntroduction\nBEOWULF CLUSTERS\nBeowulf clusters are designed to solve high-performance computing tasks.\nA Beowulf cluster consists of commodity hardware—such as personal\ncomputers—connected via a simple local-area network. No single speciﬁc\nsoftware package is required to construct a cluster. Rather, the nodes use a\nset of open-source software libraries to communicate with one another. Thus,\nthere are a variety of approaches to constructing a Beowulf cluster. Typically,\nthough, Beowulf computing nodes run the Linux operating system. Since\nBeowulf clusters require no special hardware and operate using open-source\nsoftware that is available free, they offer a low-cost strategy for building\na high-performance computing cluster. In fact, some Beowulf clusters built",
  "from discarded personal computers are using hundreds of nodes to solve\ncomputationally expensive scientiﬁc computing problems.\nensure that no conﬂicting operations occur. This function, commonly known\nas a distributed lock manager (DLM), is included in some cluster technology.\nCluster technology is changing rapidly. Some cluster products support\ndozens of systems in a cluster, as well as clustered nodes that are separated\nby miles. Many of these improvements are made possible by storage-area\nnetworks (SANs), as described in Section 10.3.3, which allow many systems\nto attach to a pool of storage. If the applications and their data are stored on\nthe SAN, then the cluster software can assign the application to run on any",
  "host that is attached to the SAN. If the host fails, then any other host can take\nover. In a database cluster, dozens of hosts can share the same database, greatly\nincreasing performance and reliability. Figure 1.8 depicts the general structure\nof a clustered system.\ncomputer\ninterconnect\ncomputer\ninterconnect\ncomputer\nstorage area\nnetwork\nFigure 1.8\nGeneral structure of a clustered system. 1.4\nOperating-System Structure\n19\njob 1\n0\nMax\noperating system\njob 2\njob 3\njob 4\nFigure 1.9\nMemory layout for a multiprogramming system.\n1.4\nOperating-System Structure\nNow that we have discussed basic computer-system organization and archi-\ntecture, we are ready to talk about operating systems. An operating system\nprovides the environment within which programs are executed. Internally,",
  "operating systems vary greatly in their makeup, since they are organized\nalong many different lines. There are, however, many commonalities, which\nwe consider in this section.\nOne of the most important aspects of operating systems is the ability\nto multiprogram. A single program cannot, in general, keep either the CPU\nor the I/O devices busy at all times. Single users frequently have multiple\nprogramsrunning.Multiprogrammingincreases CPUutilizationbyorganizing\njobs (code and data) so that the CPU always has one to execute.\nThe idea is as follows: The operating system keeps several jobs in memory\nsimultaneously (Figure 1.9). Since, in general, main memory is too small to\naccommodate all jobs, the jobs are kept initially on the disk in the job pool.",
  "This pool consists of all processes residing on disk awaiting allocation of main\nmemory.\nThe set of jobs in memory can be a subset of the jobs kept in the job\npool. The operating system picks and begins to execute one of the jobs in\nmemory. Eventually, the job may have to wait for some task, such as an I/O\noperation, to complete. In a non-multiprogrammed system, the CPU would sit\nidle. In a multiprogrammed system, the operating system simply switches to,\nand executes, another job. When that job needs to wait, the CPU switches to\nanother job, and so on. Eventually, the ﬁrst job ﬁnishes waiting and gets the\nCPU back. As long as at least one job needs to execute, the CPU is never idle.\nThis idea is common in other life situations. A lawyer does not work for",
  "only one client at a time, for example. While one case is waiting to go to trial\nor have papers typed, the lawyer can work on another case. If he has enough\nclients, the lawyer will never be idle for lack of work. (Idle lawyers tend to\nbecome politicians, so there is a certain social value in keeping lawyers busy.) 20\nChapter 1\nIntroduction\nMultiprogrammed systems provide an environment in which the various\nsystem resources (for example, CPU, memory, and peripheral devices) are\nutilized effectively, but they do not provide for user interaction with the\ncomputer system. Time sharing (or multitasking) is a logical extension of\nmultiprogramming. In time-sharing systems, the CPU executes multiple jobs\nby switching among them, but the switches occur so frequently that the users",
  "can interact with each program while it is running.\nTime sharing requires an interactive computer system, which provides\ndirect communication between the user and the system. The user gives\ninstructions to the operating system or to a program directly, using a input\ndevice such as a keyboard, mouse, touch pad, or touch screen, and waits for\nimmediate results on an output device. Accordingly, the response time should\nbe short—typically less than one second.\nA time-shared operating system allows many users to share the computer\nsimultaneously. Since each action or command in a time-shared system tends\nto be short, only a little CPU time is needed for each user. As the system switches\nrapidly from one user to the next, each user is given the impression that the",
  "entire computer system is dedicated to his use, even though it is being shared\namong many users.\nA time-shared operating system uses CPU scheduling and multiprogram-\nming to provide each user with a small portion of a time-shared computer.\nEach user has at least one separate program in memory. A program loaded into\nmemory and executing is called a process. When a process executes, it typically\nexecutes for only a short time before it either ﬁnishes or needs to perform I/O.\nI/O may be interactive; that is, output goes to a display for the user, and input\ncomes from a user keyboard, mouse, or other device. Since interactive I/O\ntypically runs at “people speeds,” it may take a long time to complete. Input,\nfor example, may be bounded by the user’s typing speed; seven characters per",
  "second is fast for people but incredibly slow for computers. Rather than let\nthe CPU sit idle as this interactive input takes place, the operating system will\nrapidly switch the CPU to the program of some other user.\nTime sharing and multiprogramming require that several jobs be kept\nsimultaneously in memory. If several jobs are ready to be brought into memory,\nand if there is not enough room for all of them, then the system must choose\namong them. Making this decision involves job scheduling, which we discuss\nin Chapter 6. When the operating system selects a job from the job pool, it loads\nthat job into memory for execution. Having several programs in memory at\nthe same time requires some form of memory management, which we cover in",
  "Chapters 8 and 9. In addition, if several jobs are ready to run at the same time,\nthe system must choose which job will run ﬁrst. Making this decision is CPU\nscheduling, which is also discussed in Chapter 6. Finally, running multiple\njobs concurrently requires that their ability to affect one another be limited in\nall phases of the operating system, including process scheduling, disk storage,\nand memory management. We discuss these considerations throughout the\ntext.\nIn a time-sharing system, the operating system must ensure reasonable\nresponse time. This goal is sometimes accomplished through swapping,\nwhereby processes are swapped in and out of main memory to the disk. A more\ncommon method for ensuring reasonable response time is virtual memory, a",
  "technique that allows the execution of a process that is not completely in 1.5\nOperating-System Operations\n21\nmemory (Chapter 9). The main advantage of the virtual-memory scheme is that\nit enables users to run programs that are larger than actual physical memory.\nFurther, it abstracts main memory into a large, uniform array of storage,\nseparating logical memory as viewed by the user from physical memory.\nThis arrangement frees programmers from concern over memory-storage\nlimitations.\nA time-sharing system must also provide a ﬁle system (Chapters 11 and\n12). The ﬁle system resides on a collection of disks; hence, disk management\nmust be provided (Chapter 10). In addition, a time-sharing system provides\na mechanism for protecting resources from inappropriate use (Chapter 14).",
  "To ensure orderly execution, the system must provide mechanisms for job\nsynchronization and communication (Chapter 5), and it may ensure that jobs\ndo not get stuck in a deadlock, forever waiting for one another (Chapter 7).\n1.5\nOperating-System Operations\nAs mentioned earlier, modern operating systems are interrupt driven. If there\nare no processes to execute, no I/O devices to service, and no users to whom\nto respond, an operating system will sit quietly, waiting for something to\nhappen. Events are almost always signaled by the occurrence of an interrupt\nor a trap. A trap (or an exception) is a software-generated interrupt caused\neither by an error (for example, division by zero or invalid memory access)\nor by a speciﬁc request from a user program that an operating-system service",
  "be performed. The interrupt-driven nature of an operating system deﬁnes\nthat system’s general structure. For each type of interrupt, separate segments\nof code in the operating system determine what action should be taken. An\ninterrupt service routine is provided to deal with the interrupt.\nSince the operating system and the users share the hardware and software\nresources of the computer system, we need to make sure that an error in a\nuser program could cause problems only for the one program running. With\nsharing, many processes could be adversely affected by a bug in one program.\nFor example, if a process gets stuck in an inﬁnite loop, this loop could prevent\nthe correct operation of many other processes. More subtle errors can occur",
  "in a multiprogramming system, where one erroneous program might modify\nanother program, the data of another program, or even the operating system\nitself.\nWithout protection against these sorts of errors, either the computer must\nexecute only one process at a time or all output must be suspect. A properly\ndesigned operating system must ensure that an incorrect (or malicious)\nprogram cannot cause other programs to execute incorrectly.\n1.5.1\nDual-Mode and Multimode Operation\nIn order to ensure the proper execution of the operating system, we must be\nable to distinguish between the execution of operating-system code and user-\ndeﬁned code. The approach taken by most computer systems is to provide\nhardware support that allows us to differentiate among various modes of\nexecution. 22\nChapter 1",
  "execution. 22\nChapter 1\nIntroduction\nuser process executing\nuser process\nkernel\ncalls system call\nreturn from system call\nuser mode\n(mode bit = 1)\ntrap\nmode bit = 0\nreturn\nmode bit = 1\nkernel mode\n(mode bit = 0)\nexecute system call\nFigure 1.10\nTransition from user to kernel mode.\nAt the very least, we need two separate modes of operation: user mode\nand kernel mode (also called supervisor mode, system mode, or privileged\nmode). A bit, called the mode bit, is added to the hardware of the computer\nto indicate the current mode: kernel (0) or user (1). With the mode bit, we can\ndistinguish between a task that is executed on behalf of the operating system\nand one that is executed on behalf of the user. When the computer system is",
  "executing on behalf of a user application, the system is in user mode. However,\nwhen a user application requests a service from the operating system (via a\nsystem call), the system must transition from user to kernel mode to fulﬁll\nthe request. This is shown in Figure 1.10. As we shall see, this architectural\nenhancement is useful for many other aspects of system operation as well.\nAt system boot time, the hardware starts in kernel mode. The operating\nsystem is then loaded and starts user applications in user mode. Whenever a\ntrap or interrupt occurs, the hardware switches from user mode to kernel mode\n(that is, changes the state of the mode bit to 0). Thus, whenever the operating\nsystem gains control of the computer, it is in kernel mode. The system always",
  "switches to user mode (by setting the mode bit to 1) before passing control to\na user program.\nThe dual mode of operation provides us with the means for protecting the\noperating system from errant users—and errant users from one another. We\naccomplishthisprotectionbydesignatingsome ofthe machine instructionsthat\nmay cause harm as privileged instructions. The hardware allows privileged\ninstructions to be executed only in kernel mode. If an attempt is made to\nexecute a privileged instruction in user mode, the hardware does not execute\nthe instruction but rather treats it as illegal and traps it to the operating system.\nThe instruction to switch to kernel mode is an example of a privileged\ninstruction. Some other examples include I/O control, timer management, and",
  "interrupt management. As we shall see throughout the text, there are many\nadditional privileged instructions.\nThe concept of modes can be extended beyond two modes (in which case\nthe CPU uses more than one bit to set and test the mode). CPUs that support\nvirtualization (Section 16.1) frequently have a separate mode to indicate when\nthe virtual machine manager (VMM)—and the virtualization management\nsoftware—is in control of the system. In this mode, the VMM has more\nprivileges than user processes but fewer than the kernel. It needs that level\nof privilege so it can create and manage virtual machines, changing the CPU\nstate to do so. Sometimes, too, different modes are used by various kernel 1.5\nOperating-System Operations\n23",
  "Operating-System Operations\n23\ncomponents. We should note that, as an alternative to modes, the CPU designer\nmay use other methods to differentiate operational privileges. The Intel 64\nfamily of CPUs supports four privilege levels, for example, and supports\nvirtualization but does not have a separate mode for virtualization.\nWe can now see the life cycle of instruction execution in a computer system.\nInitial control resides in the operating system, where instructions are executed\nin kernel mode. When control is given to a user application, the mode is set to\nuser mode. Eventually, control is switched back to the operating system via an\ninterrupt, a trap, or a system call.\nSystem calls provide the means for a user program to ask the operating",
  "system to perform tasks reserved for the operating system on the user\nprogram’s behalf. A system call is invoked in a variety of ways, depending\non the functionality provided by the underlying processor. In all forms, it is the\nmethod used by a process to request action by the operating system. A system\ncall usually takes the form of a trap to a speciﬁc location in the interrupt vector.\nThis trap can be executed by a generic trap instruction, although some systems\n(such as MIPS) have a speciﬁc syscall instruction to invoke a system call.\nWhen a system call is executed, it is typically treated by the hardware\nas a software interrupt. Control passes through the interrupt vector to a\nservice routine in the operating system, and the mode bit is set to kernel",
  "mode. The system-call service routine is a part of the operating system. The\nkernel examines the interrupting instruction to determine what system call\nhas occurred; a parameter indicates what type of service the user program is\nrequesting. Additional information needed for the request may be passed in\nregisters, on the stack, or in memory (with pointers to the memory locations\npassed in registers). The kernel veriﬁes that the parameters are correct and\nlegal, executes the request, and returns control to the instruction following the\nsystem call. We describe system calls more fully in Section 2.3.\nThe lack of a hardware-supported dual mode can cause serious shortcom-\nings in an operating system. For instance, MS-DOS was written for the Intel",
  "8088 architecture, which has no mode bit and therefore no dual mode. A user\nprogram running awry can wipe out the operating system by writing over it\nwith data; and multiple programs are able to write to a device at the same\ntime, with potentially disastrous results. Modern versions of the Intel CPU\ndo provide dual-mode operation. Accordingly, most contemporary operating\nsystems—such as Microsoft Windows 7, as well as Unix and Linux—take\nadvantage of this dual-mode feature and provide greater protection for the\noperating system.\nOnce hardware protection is in place, it detects errors that violate modes.\nThese errors are normally handled by the operating system. If a user program\nfails in some way—such as by making an attempt either to execute an illegal",
  "instruction or to access memory that is not in the user’s address space—then\nthe hardware traps to the operating system. The trap transfers control through\nthe interrupt vector to the operating system, just as an interrupt does. When\na program error occurs, the operating system must terminate the program\nabnormally. This situation is handled by the same code as a user-requested\nabnormal termination. An appropriate error message is given, and the memory\nof the program may be dumped. The memory dump is usually written to a\nﬁle so that the user or programmer can examine it and perhaps correct it and\nrestart the program. 24\nChapter 1\nIntroduction\n1.5.2\nTimer\nWe must ensure that the operating system maintains control over the CPU.",
  "We cannot allow a user program to get stuck in an inﬁnite loop or to fail\nto call system services and never return control to the operating system. To\naccomplish this goal, we can use a timer. A timer can be set to interrupt\nthe computer after a speciﬁed period. The period may be ﬁxed (for example,\n1/60 second) or variable (for example, from 1 millisecond to 1 second). A\nvariable timer is generally implemented by a ﬁxed-rate clock and a counter.\nThe operating system sets the counter. Every time the clock ticks, the counter\nis decremented. When the counter reaches 0, an interrupt occurs. For instance,\na 10-bit counter with a 1-millisecond clock allows interrupts at intervals from\n1 millisecond to 1,024 milliseconds, in steps of 1 millisecond.",
  "Before turning over control to the user, the operating system ensures\nthat the timer is set to interrupt. If the timer interrupts, control transfers\nautomatically to the operating system, which may treat the interrupt as a fatal\nerror or may give the program more time. Clearly, instructions that modify the\ncontent of the timer are privileged.\nWe can use the timer to prevent a user program from running too long.\nA simple technique is to initialize a counter with the amount of time that a\nprogram is allowed to run. A program with a 7-minute time limit, for example,\nwould have its counter initialized to 420. Every second, the timer interrupts,\nand the counter is decremented by 1. As long as the counter is positive, control",
  "is returned to the user program. When the counter becomes negative, the\noperating system terminates the program for exceeding the assigned time\nlimit.\n1.6\nProcess Management\nA program does nothing unless its instructions are executed by a CPU. A\nprogram in execution, as mentioned, is a process. A time-shared user program\nsuch as a compiler is a process. A word-processing program being run by an\nindividual user on a PC is a process. A system task, such as sending output\nto a printer, can also be a process (or at least part of one). For now, you can\nconsider a process to be a job or a time-shared program, but later you will learn\nthat the concept is more general. As we shall see in Chapter 3, it is possible\nto provide system calls that allow processes to create subprocesses to execute",
  "concurrently.\nA process needs certain resources—including CPU time, memory, ﬁles,\nand I/O devices—to accomplish its task. These resources are either given to\nthe process when it is created or allocated to it while it is running. In addition\nto the various physical and logical resources that a process obtains when it is\ncreated, various initialization data (input) may be passed along. For example,\nconsider a process whose function is to display the status of a ﬁle on the screen\nof a terminal. The process will be given the name of the ﬁle as an input and will\nexecute the appropriate instructions and system calls to obtain and display\nthe desired information on the terminal. When the process terminates, the\noperating system will reclaim any reusable resources.",
  "We emphasize that a program by itself is not a process. A program is a\npassive entity, like the contents of a ﬁle stored on disk, whereas a process 1.7\nMemory Management\n25\nis an active entity. A single-threaded process has one program counter\nspecifying the next instruction to execute. (Threads are covered in Chapter\n4.) The execution of such a process must be sequential. The CPU executes one\ninstruction of the process after another, until the process completes. Further,\nat any time, one instruction at most is executed on behalf of the process. Thus,\nalthough two processes may be associated with the same program, they are\nnevertheless considered two separate execution sequences. A multithreaded\nprocess has multiple program counters, each pointing to the next instruction",
  "to execute for a given thread.\nA process is the unit of work in a system. A system consists of a collection\nof processes, some of which are operating-system processes (those that execute\nsystem code) and the rest of which are user processes (those that execute\nuser code). All these processes can potentially execute concurrently—by\nmultiplexing on a single CPU, for example.\nThe operating system is responsible for the following activities in connec-\ntion with process management:\n• Scheduling processes and threads on the CPUs\n• Creating and deleting both user and system processes\n• Suspending and resuming processes\n• Providing mechanisms for process synchronization\n• Providing mechanisms for process communication\nWe discuss process-management techniques in Chapters 3 through 5.\n1.7",
  "1.7\nMemory Management\nAs we discussed in Section 1.2.2, the main memory is central to the operation\nof a modern computer system. Main memory is a large array of bytes, ranging\nin size from hundreds of thousands to billions. Each byte has its own address.\nMain memory is a repository of quickly accessible data shared by the CPU and\nI/O devices. The central processor reads instructions from main memory during\nthe instruction-fetch cycle and both reads and writes data from main memory\nduring the data-fetch cycle (on a von Neumann architecture). As noted earlier,\nthe main memory is generally the only large storage device that the CPU is able\nto address and access directly. For example, for the CPU to process data from\ndisk, those data must ﬁrst be transferred to main memory by CPU-generated",
  "I/O calls. In the same way, instructions must be in memory for the CPU to\nexecute them.\nFor a program to be executed, it must be mapped to absolute addresses and\nloaded into memory. As the program executes, it accesses program instructions\nand data from memory by generating these absolute addresses. Eventually,\nthe program terminates, its memory space is declared available, and the next\nprogram can be loaded and executed.\nTo improve both the utilization of the CPU and the speed of the computer’s\nresponse to its users, general-purpose computers must keep several programs\nin memory, creating a need for memory management. Many different memory- 26\nChapter 1\nIntroduction\nmanagement schemes are used. These schemes reﬂect various approaches, and",
  "the effectiveness of any given algorithm depends on the situation. In selecting a\nmemory-management scheme for a speciﬁc system, we must take into account\nmany factors—especially the hardware design of the system. Each algorithm\nrequires its own hardware support.\nThe operating system is responsible for the following activities in connec-\ntion with memory management:\n• Keeping track of which parts of memory are currently being used and who\nis using them\n• Deciding which processes (or parts of processes) and data to move into\nand out of memory\n• Allocating and deallocating memory space as needed\nMemory-management techniques are discussed in Chapters 8 and 9.\n1.8\nStorage Management\nTo make the computer system convenient for users, the operating system",
  "provides a uniform, logical view of information storage. The operating system\nabstracts from the physical properties of its storage devices to deﬁne a logical\nstorage unit, the ﬁle. The operating system maps ﬁles onto physical media and\naccesses these ﬁles via the storage devices.\n1.8.1\nFile-System Management\nFile management is one of the most visible components of an operating system.\nComputers can store information on several different types of physical media.\nMagnetic disk, optical disk, and magnetic tape are the most common. Each\nof these media has its own characteristics and physical organization. Each\nmedium is controlled by a device, such as a disk drive or tape drive, that\nalso has its own unique characteristics. These properties include access speed,",
  "capacity, data-transfer rate, and access method (sequential or random).\nA ﬁle is a collection of related information deﬁned by its creator. Commonly,\nﬁles represent programs (both source and object forms) and data. Data ﬁles may\nbe numeric, alphabetic, alphanumeric, or binary. Files may be free-form (for\nexample, text ﬁles), or they may be formatted rigidly (for example, ﬁxed ﬁelds).\nClearly, the concept of a ﬁle is an extremely general one.\nThe operating system implements the abstract concept of a ﬁle by managing\nmass-storage media, such as tapes and disks, and the devices that control them.\nIn addition, ﬁles are normally organized into directories to make them easier\nto use. Finally, when multiple users have access to ﬁles, it may be desirable",
  "to control which user may access a ﬁle and how that user may access it (for\nexample, read, write, append).\nThe operating system is responsible for the following activities in connec-\ntion with ﬁle management:\n• Creating and deleting ﬁles 1.8\nStorage Management\n27\n• Creating and deleting directories to organize ﬁles\n• Supporting primitives for manipulating ﬁles and directories\n• Mapping ﬁles onto secondary storage\n• Backing up ﬁles on stable (nonvolatile) storage media\nFile-management techniques are discussed in Chapters 11 and 12.\n1.8.2\nMass-Storage Management\nAs we have already seen, because main memory is too small to accommodate\nall data and programs, and because the data that it holds are lost when power\nis lost, the computer system must provide secondary storage to back up main",
  "memory. Most modern computer systems use disks as the principal on-line\nstorage medium for both programs and data. Most programs—including\ncompilers, assemblers, word processors, editors, and formatters—are stored\non a disk until loaded into memory. They then use the disk as both the source\nand destination of their processing. Hence, the proper management of disk\nstorage is of central importance to a computer system. The operating system is\nresponsible for the following activities in connection with disk management:\n• Free-space management\n• Storage allocation\n• Disk scheduling\nBecause secondary storage is used frequently, it must be used efﬁciently. The\nentire speed of operation of a computer may hinge on the speeds of the disk\nsubsystem and the algorithms that manipulate that subsystem.",
  "There are, however, many uses for storage that is slower and lower in\ncost (and sometimes of higher capacity) than secondary storage. Backups of\ndisk data, storage of seldom-used data, and long-term archival storage are\nsome examples. Magnetic tape drives and their tapes and CD and DVD drives\nand platters are typical tertiary storage devices. The media (tapes and optical\nplatters) vary between WORM (write-once, read-many-times) and RW (read–\nwrite) formats.\nTertiary storage is not crucial to system performance, but it still must\nbe managed. Some operating systems take on this task, while others leave\ntertiary-storage management to application programs. Some of the functions\nthat operating systems can provide include mounting and unmounting media",
  "in devices, allocating and freeing the devices for exclusive use by processes,\nand migrating data from secondary to tertiary storage.\nTechniques for secondary and tertiary storage management are discussed\nin Chapter 10.\n1.8.3\nCaching\nCaching is an important principle of computer systems. Here’s how it works.\nInformation is normally kept in some storage system (such as main memory).\nAs it is used, it is copied into a faster storage system—the cache—on a 28\nChapter 1\nIntroduction\ntemporary basis. When we need a particular piece of information, we ﬁrst\ncheck whether it is in the cache. If it is, we use the information directly from\nthe cache. If it is not, we use the information from the source, putting a copy\nin the cache under the assumption that we will need it again soon.",
  "In addition, internal programmable registers, such as index registers,\nprovide a high-speed cache for main memory. The programmer (or compiler)\nimplements the register-allocation and register-replacement algorithms to\ndecide which information to keep in registers and which to keep in main\nmemory.\nOther caches are implemented totally in hardware. For instance, most\nsystems have an instruction cache to hold the instructions expected to be\nexecuted next. Without this cache, the CPU would have to wait several cycles\nwhile an instruction was fetched from main memory. For similar reasons, most\nsystems have one or more high-speed data caches in the memory hierarchy.\nWe are not concerned with these hardware-only caches in this text, since they\nare outside the control of the operating system.",
  "are outside the control of the operating system.\nBecause caches have limited size, cache management is an important\ndesign problem. Careful selection of the cache size and of a replacement policy\ncan result in greatly increased performance. Figure 1.11 compares storage\nperformance in large workstations and small servers. Various replacement\nalgorithms for software-controlled caches are discussed in Chapter 9.\nMain memory can be viewed as a fast cache for secondary storage, since\ndata in secondary storage must be copied into main memory for use and\ndata must be in main memory before being moved to secondary storage for\nsafekeeping. The ﬁle-system data, which resides permanently on secondary\nstorage, may appear on several levels in the storage hierarchy. At the highest",
  "level, the operating system may maintain a cache of ﬁle-system data in main\nmemory. In addition, solid-state disks may be used for high-speed storage that\nis accessed through the ﬁle-system interface. The bulk of secondary storage\nis on magnetic disks. The magnetic-disk storage, in turn, is often backed up\nonto magnetic tapes or removable disks to protect against data loss in case\nof a hard-disk failure. Some systems automatically archive old ﬁle data from\nsecondary storage to tertiary storage, such as tape jukeboxes, to lower the\nstorage cost (see Chapter 10).\nLevel\nName\nTypical size\nImplementation\ntechnology\nAccess time (ns)\nBandwidth (MB/sec)\nManaged by\nBacked by\n1\nregisters\n< 1 KB\ncustom memory\nwith multiple\nports CMOS\n0.25 - 0.5\n20,000 - 100,000\ncompiler\ncache\n2\ncache\n< 16MB",
  "20,000 - 100,000\ncompiler\ncache\n2\ncache\n< 16MB\non-chip or\noff-chip\nCMOS SRAM\n0.5 - 25\n5,000 - 10,000\nhardware\nmain memory\n3\nmain memory\n< 64GB\nCMOS SRAM\n80 - 250\n1,000 - 5,000\noperating system\ndisk\n4\nsolid state disk\n< 1 TB\nflash memory\n25,000 - 50,000\n500\noperating system\ndisk\n5\nmagnetic disk\n< 10 TB\nmagnetic disk\n5,000,000\n20 - 150\noperating system\ndisk or tape\nFigure 1.11\nPerformance of various levels of storage. 1.8\nStorage Management\n29\nA\nA\nA\nmagnetic\ndisk\nmain\nmemory\nhardware\nregister\ncache\nFigure 1.12\nMigration of integer A from disk to register.\nThe movement of information between levels of a storage hierarchy may\nbe either explicit or implicit, depending on the hardware design and the\ncontrolling operating-system software. For instance, data transfer from cache",
  "to CPU and registers is usually a hardware function, with no operating-system\nintervention. In contrast, transfer of data from disk to memory is usually\ncontrolled by the operating system.\nIn a hierarchical storage structure, the same data may appear in different\nlevels of the storage system. For example, suppose that an integer A that is to\nbe incremented by 1 is located in ﬁle B, and ﬁle B resides on magnetic disk.\nThe increment operation proceeds by ﬁrst issuing an I/O operation to copy the\ndisk block on which A resides to main memory. This operation is followed by\ncopying A to the cache and to an internal register. Thus, the copy of A appears\nin several places: on the magnetic disk, in main memory, in the cache, and in an",
  "internal register (see Figure 1.12). Once the increment takes place in the internal\nregister, the value of A differs in the various storage systems. The value of A\nbecomes the same only after the new value of A is written from the internal\nregister back to the magnetic disk.\nIn a computing environment where only one process executes at a time,\nthis arrangement poses no difﬁculties, since an access to integer A will always\nbe to the copy at the highest level of the hierarchy. However, in a multitasking\nenvironment, where the CPU is switched back and forth among various\nprocesses, extreme care must be taken to ensure that, if several processes wish\nto access A, then each of these processes will obtain the most recently updated\nvalue of A.",
  "value of A.\nThe situation becomes more complicated in a multiprocessor environment\nwhere, in addition to maintaining internal registers, each of the CPUs also\ncontains a local cache (Figure 1.6). In such an environment, a copy of A may\nexist simultaneously in several caches. Since the various CPUs can all execute\nin parallel, we must make sure that an update to the value of A in one cache\nis immediately reﬂected in all other caches where A resides. This situation is\ncalled cache coherency, and it is usually a hardware issue (handled below the\noperating-system level).\nIn a distributed environment, the situation becomes even more complex.\nIn this environment, several copies (or replicas) of the same ﬁle can be kept on",
  "different computers. Since the various replicas may be accessed and updated\nconcurrently, some distributed systems ensure that, when a replica is updated\nin one place, all other replicas are brought up to date as soon as possible. There\nare various ways to achieve this guarantee, as we discuss in Chapter 17.\n1.8.4\nI/O Systems\nOne of the purposes of an operating system is to hide the peculiarities of speciﬁc\nhardware devices from the user. For example, in UNIX, the peculiarities of I/O 30\nChapter 1\nIntroduction\ndevices are hidden from the bulk of the operating system itself by the I/O\nsubsystem. The I/O subsystem consists of several components:\n• A memory-management component that includes buffering, caching, and\nspooling\n• A general device-driver interface",
  "spooling\n• A general device-driver interface\n• Drivers for speciﬁc hardware devices\nOnly the device driver knows the peculiarities of the speciﬁc device to which\nit is assigned.\nWe discussed in Section 1.2.3 how interrupt handlers and device drivers are\nused in the construction of efﬁcient I/O subsystems. In Chapter 13, we discuss\nhow the I/O subsystem interfaces to the other system components, manages\ndevices, transfers data, and detects I/O completion.\n1.9\nProtection and Security\nIf a computer system has multiple users and allows the concurrent execution\nof multiple processes, then access to data must be regulated. For that purpose,\nmechanisms ensure that ﬁles, memory segments, CPU, and other resources can\nbe operated on by only those processes that have gained proper authoriza-",
  "tion from the operating system. For example, memory-addressing hardware\nensures that a process can execute only within its own address space. The\ntimer ensures that no process can gain control of the CPU without eventually\nrelinquishing control. Device-control registers are not accessible to users, so\nthe integrity of the various peripheral devices is protected.\nProtection, then, is any mechanism for controlling the access of processes\nor users to the resources deﬁned by a computer system. This mechanism must\nprovide means to specify the controls to be imposed and to enforce the controls.\nProtection can improve reliability by detecting latent errors at the interfaces\nbetween component subsystems. Early detection of interface errors can often",
  "prevent contamination of a healthy subsystem by another subsystem that is\nmalfunctioning. Furthermore, an unprotected resource cannot defend against\nuse (or misuse) by an unauthorized or incompetent user. A protection-oriented\nsystem provides a means to distinguish between authorized and unauthorized\nusage, as we discuss in Chapter 14.\nA system can have adequate protection but still be prone to failure and\nallow inappropriate access. Consider a user whose authentication information\n(her means of identifying herself to the system) is stolen. Her data could be\ncopied or deleted, even though ﬁle and memory protection are working. It is\nthe job of security to defend a system from external and internal attacks. Such\nattacks spread across a huge range and include viruses and worms, denial-of-",
  "service attacks (which use all of a system’s resources and so keep legitimate\nusers out of the system), identity theft, and theft of service (unauthorized\nuse of a system). Prevention of some of these attacks is considered an\noperating-system function on some systems, while other systems leave it to\npolicy or additional software. Due to the alarming rise in security incidents, 1.10\nKernel Data Structures\n31\noperating-system security features represent a fast-growing area of research\nand implementation. We discuss security in Chapter 15.\nProtection and security require the system to be able to distinguish among\nall its users. Most operating systems maintain a list of user names and\nassociated user identiﬁers (user IDs). In Windows parlance, this is a security",
  "ID (SID). These numerical IDs are unique, one per user. When a user logs in\nto the system, the authentication stage determines the appropriate user ID for\nthe user. That user ID is associated with all of the user’s processes and threads.\nWhen an ID needs to be readable by a user, it is translated back to the user\nname via the user name list.\nIn some circumstances, we wish to distinguish among sets of users rather\nthan individual users. For example, the owner of a ﬁle on a UNIX system may be\nallowed to issue all operations on that ﬁle, whereas a selected set of users may\nbe allowed only to read the ﬁle. To accomplish this, we need to deﬁne a group\nname and the set of users belonging to that group. Group functionality can",
  "be implemented as a system-wide list of group names and group identiﬁers.\nA user can be in one or more groups, depending on operating-system design\ndecisions. The user’s group IDs are also included in every associated process\nand thread.\nIn the course of normal system use, the user ID and group ID for a user\nare sufﬁcient. However, a user sometimes needs to escalate privileges to gain\nextra permissions for an activity. The user may need access to a device that is\nrestricted, for example. Operating systems provide various methods to allow\nprivilege escalation. On UNIX, for instance, the setuid attribute on a program\ncauses that program to run with the user ID of the owner of the ﬁle, rather than\nthe current user’s ID. The process runs with this effective UID until it turns off",
  "the extra privileges or terminates.\n1.10\nKernel Data Structures\nWe turn next to a topic central to operating-system implementation: the way\ndata are structured in the system. In this section, we brieﬂy describe several\nfundamental data structures used extensively in operating systems. Readers\nwho require further details on these structures, as well as others, should consult\nthe bibliography at the end of the chapter.\n1.10.1\nLists, Stacks, and Queues\nAn array is a simple data structure in which each element can be accessed\ndirectly. For example, main memory is constructed as an array. If the data item\nbeing stored is larger than one byte, then multiple bytes can be allocated to the\nitem, and the item is addressed as item number × item size. But what about",
  "storing an item whose size may vary? And what about removing an item if the\nrelative positions of the remaining items must be preserved? In such situations,\narrays give way to other data structures.\nAfter arrays, lists are perhaps the most fundamental data structures in\ncomputer science. Whereas each item in an array can be accessed directly, the\nitems in a list must be accessed in a particular order. That is, a list represents\na collection of data values as a sequence. The most common method for 32\nChapter 1\nIntroduction\ndata\ndata\ndata\nnull\n•\n•\n•\nFigure 1.13\nSingly linked list.\nimplementing this structure is a linked list, in which items are linked to one\nanother. Linked lists are of several types:\n• In a singly linked list, each item points to its successor, as illustrated in",
  "Figure 1.13.\n• In a doubly linked list, a given item can refer either to its predecessor or\nto its successor, as illustrated in Figure 1.14.\n• In a circularly linked list, the last element in the list refers to the ﬁrst\nelement, rather than to null, as illustrated in Figure 1.15.\nLinked lists accommodate items of varying sizes and allow easy insertion\nand deletion of items. One potential disadvantage of using a list is that\nperformance for retrieving a speciﬁed item in a list of size n is linear — O(n),\nas it requires potentially traversing all n elements in the worst case. Lists\nare sometimes used directly by kernel algorithms. Frequently, though, they\nare used for constructing more powerful data structures, such as stacks and\nqueues.",
  "queues.\nA stack is a sequentially ordered data structure that uses the last in, ﬁrst\nout (LIFO) principle for adding and removing items, meaning that the last item\nplaced onto a stack is the ﬁrst item removed. The operations for inserting and\nremoving items from a stack are known as push and pop, respectively. An\noperating system often uses a stack when invoking function calls. Parameters,\nlocal variables, and the return address are pushed onto the stack when a\nfunction is called; returning from the function call pops those items off the\nstack.\nA queue, in contrast, is a sequentially ordered data structure that uses the\nﬁrst in, ﬁrst out (FIFO) principle: items are removed from a queue in the order\nin which they were inserted. There are many everyday examples of queues,",
  "including shoppers waiting in a checkout line at a store and cars waiting in line\nat a trafﬁc signal. Queues are also quite common in operating systems—jobs\nthat are sent to a printer are typically printed in the order in which they were\nsubmitted, for example. As we shall see in Chapter 6, tasks that are waiting to\nbe run on an available CPU are often organized in queues.\ndata null\nnull\ndata\ndata\ndata\n•\n•\n•\nFigure 1.14\nDoubly linked list. 1.10\nKernel Data Structures\n33\ndata\ndata\ndata\ndata\n•\n•\n•\nFigure 1.15\nCircularly linked list.\n1.10.2\nTrees\nA tree is a data structure that can be used to represent data hierarchically. Data\nvalues in a tree structure are linked through parent–child relationships. In a\ngeneral tree, a parent may have an unlimited number of children. In a binary",
  "tree, a parent may have at most two children, which we term the left child\nand the right child. A binary search tree additionally requires an ordering\nbetween the parent’s two children in which le f t child <= right child. Figure\n1.16 provides an example of a binary search tree. When we search for an item in\na binary search tree, the worst-case performance is O(n) (consider how this can\noccur). To remedy this situation, we can use an algorithm to create a balanced\nbinary search tree. Here, a tree containing n items has at most lg n levels, thus\nensuring worst-case performance of O(lg n). We shall see in Section 6.7.1 that\nLinux uses a balanced binary search tree as part its CPU-scheduling algorithm.\n1.10.3\nHash Functions and Maps",
  "1.10.3\nHash Functions and Maps\nA hash function takes data as its input, performs a numeric operation on this\ndata, and returns a numeric value. This numeric value can then be used as an\nindex into a table (typically an array) to quickly retrieve the data. Whereas\nsearching for a data item through a list of size n can require up to O(n)\ncomparisons in the worst case, using a hash function for retrieving data from\ntable can be as good as O(1) in the worst case, depending on implementation\ndetails. Because of this performance, hash functions are used extensively in\noperating systems.\n17\n35\n40\n42\n12\n14\n6\nFigure 1.16\nBinary search tree. 34\nChapter 1\nIntroduction\n0\n1\n.\n.\nn\nvalue\nhash map\nhash_function(key)\nFigure 1.17\nHash map.",
  "hash map\nhash_function(key)\nFigure 1.17\nHash map.\nOne potential difﬁculty with hash functions is that two inputs can result\nin the same output value—that is, they can link to the same table location.\nWe can accommodate this hash collision by having a linked list at that table\nlocation that contains all of the items with the same hash value. Of course, the\nmore collisions there are, the less efﬁcient the hash function is.\nOne use of a hash function is to implement a hash map, which associates\n(or maps) [key:value] pairs using a hash function. For example, we can map\nthe key operating to the value system. Once the mapping is established, we can\napply the hash function to the key to obtain the value from the hash map",
  "(Figure 1.17). For example, suppose that a user name is mapped to a password.\nPassword authentication then proceeds as follows: a user enters his user name\nand password. The hash function is applied to the user name, which is then\nused to retrieve the password. The retrieved password is then compared with\nthe password entered by the user for authentication.\n1.10.4\nBitmaps\nA bitmap is a string of n binary digits that can be used to represent the status of\nn items. For example, suppose we have several resources, and the availability\nof each resource is indicated by the value of a binary digit: 0 means that the\nresource is available, while 1 indicates that it is unavailable (or vice-versa). The\nvalue of the ith position in the bitmap is associated with the ith resource. As an",
  "example, consider the bitmap shown below:\n0 0 1 0 1 1 1 0 1\nResources 2, 4, 5, 6, and 8 are unavailable; resources 0, 1, 3, and 7 are available.\nThe power of bitmaps becomes apparent when we consider their space\nefﬁciency. If we were to use an eight-bit Boolean value instead of a single bit,\nthe resulting data structure would be eight times larger. Thus, bitmaps are\ncommonly used when there is a need to represent the availability of a large\nnumber of resources. Disk drives provide a nice illustration. A medium-sized\ndisk drive might be divided into several thousand individual units, called disk\nblocks. A bitmap can be used to indicate the availability of each disk block.\nData structures are pervasive in operating system implementations. Thus,",
  "we will see the structures discussed here, along with others, throughout this\ntext as we explore kernel algorithms and their implementations. 1.11\nComputing Environments\n35\nLINUX KERNEL DATA STRUCTURES\nThe data structures used in the Linux kernel are available in the kernel source\ncode. The include ﬁle <linux/list.h> provides details of the linked-list\ndata structure used throughout the kernel. A queue in Linux is known as\na kfifo, and its implementation can be found in the kfifo.c ﬁle in the\nkernel directory of the source code. Linux also provides a balanced binary\nsearch tree implementation using red-black trees. Details can be found in the\ninclude ﬁle <linux/rbtree.h>.\n1.11\nComputing Environments\nSo far, we have brieﬂy described several aspects of computer systems and the",
  "operating systems that manage them. We turn now to a discussion of how\noperating systems are used in a variety of computing environments.\n1.11.1\nTraditional Computing\nAs computing has matured, the lines separating many of the traditional com-\nputing environments have blurred. Consider the “typical ofﬁce environment.”\nJust a few years ago, this environment consisted of PCs connected to a network,\nwith servers providing ﬁle and print services. Remote access was awkward,\nand portability was achieved by use of laptop computers. Terminals attached\nto mainframes were prevalent at many companies as well, with even fewer\nremote access and portability options.\nThe current trend is toward providing more ways to access these computing",
  "environments. Web technologies and increasing WAN bandwidth are stretching\nthe boundaries of traditional computing. Companies establish portals, which\nprovide Web accessibility to their internal servers. Network computers (or\nthin clients)—which are essentially terminals that understand web-based\ncomputing—are used in place of traditional workstations where more security\nor easier maintenance is desired. Mobile computers can synchronize with PCs\nto allow very portable use of company information. Mobile computers can also\nconnect to wireless networks and cellular data networks to use the company’s\nWeb portal (as well as the myriad other Web resources).\nAt home, most users once had a single computer with a slow modem\nconnection to the ofﬁce, the Internet, or both. Today, network-connection",
  "speeds once available only at great cost are relatively inexpensive in many\nplaces, giving home users more access to more data. These fast data connections\nare allowing home computers to serve up Web pages and to run networks that\ninclude printers, client PCs, and servers. Many homes use ﬁrewalls to protect\ntheir networks from security breaches.\nIn the latter half of the 20th century, computing resources were relatively\nscarce. (Before that, they were nonexistent!) For a period of time, systems\nwere either batch or interactive. Batch systems processed jobs in bulk, with\npredetermined input from ﬁles or other data sources. Interactive systems\nwaited for input from users. To optimize the use of the computing resources,",
  "multiple users shared time on these systems. Time-sharing systems used a 36\nChapter 1\nIntroduction\ntimer and scheduling algorithms to cycle processes rapidly through the CPU,\ngiving each user a share of the resources.\nToday, traditional time-sharing systems are uncommon. The same schedul-\ning technique is still in use on desktop computers, laptops, servers, and even\nmobile computers, but frequently all the processes are owned by the same\nuser (or a single user and the operating system). User processes, and system\nprocesses that provide services to the user, are managed so that each frequently\ngets a slice of computer time. Consider the windows created while a user\nis working on a PC, for example, and the fact that they may be performing",
  "different tasks at the same time. Even a web browser can be composed of\nmultiple processes, one for each website currently being visited, with time\nsharing applied to each web browser process.\n1.11.2\nMobile Computing\nMobile computing refers to computing on handheld smartphones and tablet\ncomputers. These devices share the distinguishing physical features of being\nportable and lightweight. Historically, compared with desktop and laptop\ncomputers, mobile systems gave up screen size, memory capacity, and overall\nfunctionality in return for handheld mobile access to services such as e-mail\nand web browsing. Over the past few years, however, features on mobile\ndevices have become so rich that the distinction in functionality between, say,",
  "a consumer laptop and a tablet computer may be difﬁcult to discern. In fact,\nwe might argue that the features of a contemporary mobile device allow it to\nprovide functionality that is either unavailable or impractical on a desktop or\nlaptop computer.\nToday, mobile systems are used not only for e-mail and web browsing but\nalso for playing music and video, reading digital books, taking photos, and\nrecording high-deﬁnition video. Accordingly, tremendous growth continues\nin the wide range of applications that run on such devices. Many developers\nare now designing applications that take advantage of the unique features of\nmobile devices, such as global positioning system (GPS) chips, accelerometers,\nand gyroscopes. An embedded GPS chip allows a mobile device to use satellites",
  "to determine its precise location on earth. That functionality is especially useful\nin designing applications that provide navigation—for example, telling users\nwhich way to walk or drive or perhaps directing them to nearby services, such\nas restaurants. An accelerometer allows a mobile device to detect its orientation\nwith respect to the ground and to detect certain other forces, such as tilting\nand shaking. In several computer games that employ accelerometers, players\ninterface with the system not by using a mouse or a keyboard but rather by\ntilting, rotating, and shaking the mobile device! Perhaps more a practical use\nof these features is found in augmented-reality applications, which overlay\ninformation on a display of the current environment. It is difﬁcult to imagine",
  "how equivalent applications could be developed on traditional laptop or\ndesktop computer systems.\nTo provide access to on-line services, mobile devices typically use either\nIEEE standard 802.11 wireless or cellular data networks. The memory capacity\nand processing speed of mobile devices, however, are more limited than those\nof PCs. Whereas a smartphone or tablet may have 64 GB in storage, it is not\nuncommon to ﬁnd 1 TB in storage on a desktop computer. Similarly, because 1.11\nComputing Environments\n37\npower consumption is such a concern, mobile devices often use processors that\nare smaller, are slower, and offer fewer processing cores than processors found\non traditional desktop and laptop computers.\nTwo operating systems currently dominate mobile computing: Apple iOS",
  "and Google Android. iOS was designed to run on Apple iPhone and iPad\nmobile devices. Android powers smartphones and tablet computers available\nfrom many manufacturers. We examine these two mobile operating systems in\nfurther detail in Chapter 2.\n1.11.3\nDistributed Systems\nA distributed system is a collection of physically separate, possibly heteroge-\nneous, computer systems that are networked to provide users with access to\nthe various resources that the system maintains. Access to a shared resource\nincreases computation speed, functionality, data availability, and reliability.\nSome operating systems generalize network access as a form of ﬁle access, with\nthe details of networking contained in the network interface’s device driver.",
  "Others make users speciﬁcally invoke network functions. Generally, systems\ncontain a mix of the two modes—for example FTP and NFS. The protocols\nthat create a distributed system can greatly affect that system’s utility and\npopularity.\nA network, in the simplest terms, is a communication path between\ntwo or more systems. Distributed systems depend on networking for their\nfunctionality. Networks vary by the protocols used, the distances between\nnodes, and the transport media. TCP/IP is the most common network protocol,\nand it provides the fundamental architecture of the Internet. Most operating\nsystems support TCP/IP, including all general-purpose ones. Some systems\nsupport proprietary protocols to suit their needs. To an operating system, a",
  "network protocol simply needs an interface device—a network adapter, for\nexample—with a device driver to manage it, as well as software to handle\ndata. These concepts are discussed throughout this book.\nNetworks are characterized based on the distances between their nodes.\nA local-area network (LAN) connects computers within a room, a building,\nor a campus. A wide-area network (WAN) usually links buildings, cities, or\ncountries. A global company may have a WAN to connect its ofﬁces worldwide,\nfor example. These networks may run one protocol or several protocols. The\ncontinuing advent of new technologies brings about new forms of networks.\nFor example, a metropolitan-area network (MAN) could link buildings within\na city. BlueTooth and 802.11 devices use wireless technology to communicate",
  "over a distance of several feet, in essence creating a personal-area network\n(PAN) between a phone and a headset or a smartphone and a desktop computer.\nThe media to carry networks are equally varied. They include copper wires,\nﬁber strands, and wireless transmissions between satellites, microwave dishes,\nand radios. When computing devices are connected to cellular phones, they\ncreate a network. Even very short-range infrared communication can be used\nfor networking. At a rudimentary level, whenever computers communicate,\nthey use or create a network. These networks also vary in their performance\nand reliability.\nSome operating systems have taken the concept of networks and dis-\ntributed systems further than the notion of providing network connectivity. 38\nChapter 1\nIntroduction",
  "Chapter 1\nIntroduction\nA network operating system is an operating system that provides features\nsuch as ﬁle sharing across the network, along with a communication scheme\nthat allows different processes on different computers to exchange messages.\nA computer running a network operating system acts autonomously from all\nother computers on the network, although it is aware of the network and is\nable to communicate with other networked computers. A distributed operating\nsystem provides a less autonomous environment. The different computers\ncommunicate closely enough to provide the illusion that only a single operating\nsystem controls the network. We cover computer networks and distributed\nsystems in Chapter 17.\n1.11.4\nClient–Server Computing",
  "1.11.4\nClient–Server Computing\nAs PCs have become faster, more powerful, and cheaper, designers have shifted\naway from centralized system architecture. Terminals connected to centralized\nsystems are now being supplanted by PCs and mobile devices. Correspond-\ningly, user-interface functionality once handled directly by centralized systems\nis increasingly being handled by PCs, quite often through a web interface. As\na result, many of today’s systems act as server systems to satisfy requests\ngenerated by client systems. This form of specialized distributed system, called\na client–server system, has the general structure depicted in Figure 1.18.\nServer systems can be broadly categorized as compute servers and ﬁle\nservers:\n• The compute-server system provides an interface to which a client can",
  "send a request to perform an action (for example, read data). In response,\nthe server executes the action and sends the results to the client. A server\nrunning a database that responds to client requests for data is an example\nof such a system.\n• The ﬁle-server system provides a ﬁle-system interface where clients can\ncreate, update, read, and delete ﬁles. An example of such a system is a web\nserver that delivers ﬁles to clients running web browsers.\nServer\nNetwork\nclient\ndesktop\nclient\nlaptop\nclient\nsmartphone\nFigure 1.18\nGeneral structure of a client–server system. 1.11\nComputing Environments\n39\n1.11.5\nPeer-to-Peer Computing\nAnother structure for a distributed system is the peer-to-peer (P2P) system\nmodel. In this model, clients and servers are not distinguished from one",
  "another. Instead, all nodes within the system are considered peers, and each\nmay act as either a client or a server, depending on whether it is requesting or\nproviding a service. Peer-to-peer systems offer an advantage over traditional\nclient-server systems. In a client-server system, the server is a bottleneck; but\nin a peer-to-peer system, services can be provided by several nodes distributed\nthroughout the network.\nTo participate in a peer-to-peer system, a node must ﬁrst join the network\nof peers. Once a node has joined the network, it can begin providing services\nto—and requesting services from—other nodes in the network. Determining\nwhat services are available is accomplished in one of two general ways:\n• When a node joins a network, it registers its service with a centralized",
  "lookup service on the network. Any node desiring a speciﬁc service ﬁrst\ncontacts this centralized lookup service to determine which node provides\nthe service. The remainder of the communication takes place between the\nclient and the service provider.\n• An alternative scheme uses no centralized lookup service. Instead, a peer\nacting as a client must discover what node provides a desired service by\nbroadcasting a request for the service to all other nodes in the network. The\nnode (or nodes) providing that service responds to the peer making the\nrequest. To support this approach, a discovery protocol must be provided\nthat allows peers to discover services provided by other peers in the\nnetwork. Figure 1.19 illustrates such a scenario.",
  "network. Figure 1.19 illustrates such a scenario.\nPeer-to-peer networks gained widespread popularity in the late 1990s with\nseveral ﬁle-sharing services, such as Napster and Gnutella, that enabled peers\nto exchange ﬁles with one another. The Napster system used an approach\nsimilar to the ﬁrst type described above: a centralized server maintained an\nindex of all ﬁles stored on peer nodes in the Napster network, and the actual\nclient\nclient\nclient\nclient\nclient\nFigure 1.19\nPeer-to-peer system with no centralized service. 40\nChapter 1\nIntroduction\nexchange of ﬁles took place between the peer nodes. The Gnutella system used\na technique similar to the second type: a client broadcasted ﬁle requests to\nother nodes in the system, and nodes that could service the request responded",
  "directly to the client. The future of exchanging ﬁles remains uncertain because\npeer-to-peer networks can be used to exchange copyrighted materials (music,\nfor example) anonymously, and there are laws governing the distribution of\ncopyrighted material. Notably, Napster ran into legal trouble for copyright\ninfringement and its services were shut down in 2001.\nSkype is another example of peer-to-peer computing. It allows clients to\nmake voice calls and video calls and to send text messages over the Internet\nusing a technology known as voice over IP (VoIP). Skype uses a hybrid peer-\nto-peer approach. It includes a centralized login server, but it also incorporates\ndecentralized peers and allows two peers to communicate.\n1.11.6\nVirtualization",
  "1.11.6\nVirtualization\nVirtualization is a technology that allows operating systems to run as appli-\ncations within other operating systems. At ﬁrst blush, there seems to be\nlittle reason for such functionality. But the virtualization industry is vast and\ngrowing, which is a testament to its utility and importance.\nBroadly speaking, virtualization is one member of a class of software\nthat also includes emulation. Emulation is used when the source CPU type\nis different from the target CPU type. For example, when Apple switched from\nthe IBM Power CPU to the Intel x86 CPU for its desktop and laptop computers,\nit included an emulation facility called “Rosetta,” which allowed applications\ncompiled for the IBM CPU to run on the Intel CPU. That same concept can be",
  "extended to allow an entire operating system written for one platform to run\non another. Emulation comes at a heavy price, however. Every machine-level\ninstruction that runs natively on the source system must be translated to the\nequivalent function on the target system, frequently resulting in several target\ninstructions. If the source and target CPUs have similar performance levels, the\nemulated code can run much slower than the native code.\nA common example of emulation occurs when a computer language is\nnot compiled to native code but instead is either executed in its high-level\nform or translated to an intermediate form. This is known as interpretation.\nSome languages, such as BASIC, can be either compiled or interpreted. Java, in",
  "contrast, is always interpreted. Interpretation is a form of emulation in that the\nhigh-level language code is translated to native CPU instructions, emulating\nnot another CPU but a theoretical virtual machine on which that language could\nrun natively. Thus, we can run Java programs on “Java virtual machines,” but\ntechnically those virtual machines are Java emulators.\nWith virtualization, in contrast, an operating system that is natively com-\npiled for a particular CPU architecture runs within another operating system\nalso native to that CPU. Virtualization ﬁrst came about on IBM mainframes\nas a method for multiple users to run tasks concurrently. Running multiple\nvirtual machines allowed (and still allows) many users to run tasks on a system",
  "designed for a single user. Later, in response to problems with running multiple\nMicrosoft Windows XP applications on the Intel x86 CPU, VMware created a\nnew virtualization technology in the form of an application that ran on XP.\nThat application ran one or more guest copies of Windows or other native 1.11\nComputing Environments\n41\n(a)\nprocesses\nhardware\nkernel\n(b)\nprogramming\ninterface\nprocesses\nprocesses\nprocesses\nkernel\nkernel\nkernel\nVM2\nVM1\nVM3\nmanager\nhardware\nvirtual machine\nFigure 1.20\nVMware.\nx86 operating systems, each running its own applications. (See Figure 1.20.)\nWindows was the host operating system, and the VMware application was the\nvirtual machine manager VMM. The VMM runs the guest operating systems,\nmanages their resource use, and protects each guest from the others.",
  "Even though modern operating systems are fully capable of running\nmultiple applications reliably, the use of virtualization continues to grow. On\nlaptops and desktops, a VMM allows the user to install multiple operating\nsystems for exploration or to run applications written for operating systems\nother than the native host. For example, an Apple laptop running Mac OS\nX on the x86 CPU can run a Windows guest to allow execution of Windows\napplications. Companies writing software for multiple operating systems\ncan use virtualization to run all of those operating systems on a single\nphysical server for development, testing, and debugging. Within data centers,\nvirtualization has become a common method of executing and managing",
  "computing environments. VMMs like VMware, ESX, and Citrix XenServer no\nlonger run on host operating systems but rather are the hosts. Full details of\nthe features and implementation of virtualization are found in Chapter 16.\n1.11.7\nCloud Computing\nCloud computing is a type of computing that delivers computing, storage,\nand even applications as a service across a network. In some ways, it’s a\nlogical extension of virtualization, because it uses virtualization as a base for\nits functionality. For example, the Amazon Elastic Compute Cloud (EC2) facility\nhas thousands of servers, millions of virtual machines, and petabytes of storage\navailable for use by anyone on the Internet. Users pay per month based on how\nmuch of those resources they use.",
  "much of those resources they use.\nThere are actually many types of cloud computing, including the following:\n• Public cloud—a cloud available via the Internet to anyone willing to pay\nfor the services 42\nChapter 1\nIntroduction\n• Private cloud—a cloud run by a company for that company’s own use\n• Hybrid cloud—a cloud that includes both public and private cloud\ncomponents\n• Software as a service (SaaS)—one or more applications (such as word\nprocessors or spreadsheets) available via the Internet\n• Platform as a service (PaaS)—a software stack ready for application use\nvia the Internet (for example, a database server)\n• Infrastructure as a service (IaaS)—servers or storage available over the\nInternet (for example, storage available for making backup copies of\nproduction data)",
  "production data)\nThese cloud-computing types are not discrete, as a cloud computing environ-\nment may provide a combination of several types. For example, an organization\nmay provide both SaaS and IaaS as a publicly available service.\nCertainly, there are traditional operating systems within many of the\ntypes of cloud infrastructure. Beyond those are the VMMs that manage the\nvirtual machines in which the user processes run. At a higher level, the VMMs\nthemselves are managed by cloud management tools, such as Vware vCloud\nDirector and the open-source Eucalyptus toolset. These tools manage the\nresources within a given cloud and provide interfaces to the cloud components,\nmaking a good argument for considering them a new type of operating system.",
  "Figure 1.21 illustrates a public cloud providing IaaS. Notice that both the\ncloud services and the cloud user interface are protected by a ﬁrewall.\nfirewall\ncloud\ncustomer\ninterface\nload balancer\nvirtual\nmachines\nvirtual\nmachines\nservers\nservers\nstorage\nInternet\ncustomer\nrequests\ncloud\nmanagement\ncommands\ncloud\nmanagment\nservices\nFigure 1.21\nCloud computing. 1.12\nOpen-Source Operating Systems\n43\n1.11.8\nReal-Time Embedded Systems\nEmbedded computers are the most prevalent form of computers in existence.\nThese devices are found everywhere, from car engines and manufacturing\nrobots to DVDs and microwave ovens. They tend to have very speciﬁc tasks.\nThe systems they run on are usually primitive, and so the operating systems",
  "provide limited features. Usually, they have little or no user interface, preferring\nto spend their time monitoring and managing hardware devices, such as\nautomobile engines and robotic arms.\nThese embedded systems vary considerably. Some are general-purpose\ncomputers, running standard operating systems—such as Linux—with\nspecial-purpose applications to implement the functionality. Others are hard-\nware devices with a special-purpose embedded operating system providing\njust the functionality desired. Yet others are hardware devices with application-\nspeciﬁc integrated circuits (ASICs) that perform their tasks without an operat-\ning system.\nThe use of embedded systems continues to expand. The power of these\ndevices, both as standalone units and as elements of networks and the web,",
  "is sure to increase as well. Even now, entire houses can be computerized, so\nthat a central computer—either a general-purpose computer or an embedded\nsystem—can control heating and lighting, alarm systems, and even coffee\nmakers. Web access can enable a home owner to tell the house to heat up\nbefore she arrives home. Someday, the refrigerator can notify the grocery store\nwhen it notices the milk is gone.\nEmbedded systems almost always run real-time operating systems. A\nreal-time system is used when rigid time requirements have been placed on\nthe operation of a processor or the ﬂow of data; thus, it is often used as a\ncontrol device in a dedicated application. Sensors bring data to the computer.\nThe computer must analyze the data and possibly adjust controls to modify",
  "the sensor inputs. Systems that control scientiﬁc experiments, medical imaging\nsystems, industrial control systems, and certain display systems are real-\ntime systems. Some automobile-engine fuel-injection systems, home-appliance\ncontrollers, and weapon systems are also real-time systems.\nA real-time system has well-deﬁned, ﬁxed time constraints. Processing\nmust be done within the deﬁned constraints, or the system will fail. For\ninstance, it would not do for a robot arm to be instructed to halt after it had\nsmashed into the car it was building. A real-time system functions correctly\nonly if it returns the correct result within its time constraints. Contrast this\nsystem with a time-sharing system, where it is desirable (but not mandatory)",
  "to respond quickly, or a batch system, which may have no time constraints at\nall.\nIn Chapter 6, we consider the scheduling facility needed to implement\nreal-time functionality in an operating system. In Chapter 9, we describe the\ndesign of memory management for real-time computing. Finally, in Chapters\n18 and 19, we describe the real-time components of the Linux and Windows 7\noperating systems.\n1.12\nOpen-Source Operating Systems\nWe noted at the beginning of this chapter that the study of operating systems\nhas been made easier by the availability of a vast number of open-source 44\nChapter 1\nIntroduction\nreleases. Open-source operating systems are those available in source-code\nformat rather than as compiled binary code. Linux is the most famous open-",
  "source operating system, while Microsoft Windows is a well-known example\nof the opposite closed-source approach. Apple’s Mac OS X and iOS operating\nsystems comprise a hybrid approach. They contain an open-source kernel\nnamed Darwin yet include proprietary, closed-source components as well.\nStarting with the source code allows the programmer to produce binary\ncode that can be executed on a system. Doing the opposite—reverse engi-\nneering the source code from the binaries—is quite a lot of work, and useful\nitems such as comments are never recovered. Learning operating systems by\nexamining the source code has other beneﬁts as well. With the source code\nin hand, a student can modify the operating system and then compile and",
  "run the code to try out those changes, which is an excellent learning tool.\nThis text includes projects that involve modifying operating-system source\ncode, while also describing algorithms at a high level to be sure all important\noperating-system topics are covered. Throughout the text, we provide pointers\nto examples of open-source code for deeper study.\nThere are many beneﬁts to open-source operating systems, including a\ncommunity of interested (and usually unpaid) programmers who contribute\nto the code by helping to debug it, analyze it, provide support, and suggest\nchanges. Arguably, open-source code is more secure than closed-source code\nbecause many more eyes are viewing the code. Certainly, open-source code has",
  "bugs, but open-source advocates argue that bugs tend to be found and ﬁxed\nfaster owing to the number of people using and viewing the code. Companies\nthat earn revenue from selling their programs often hesitate to open-source\ntheir code, but Red Hat and a myriad of other companies are doing just that\nand showing that commercial companies beneﬁt, rather than suffer, when they\nopen-source their code. Revenue can be generated through support contracts\nand the sale of hardware on which the software runs, for example.\n1.12.1\nHistory\nIn the early days of modern computing (that is, the 1950s), a great deal of\nsoftware was available in open-source format. The original hackers (computer\nenthusiasts) at MIT’s Tech Model Railroad Club left their programs in drawers",
  "for others to work on. “Homebrew” user groups exchanged code during their\nmeetings. Later, company-speciﬁc user groups, such as Digital Equipment\nCorporation’s DEC, accepted contributions of source-code programs, collected\nthem onto tapes, and distributed the tapes to interested members.\nComputer and software companies eventually sought to limit the use of\ntheir software to authorized computers and paying customers. Releasing only\nthe binary ﬁles compiled from the source code, rather than the source code\nitself, helped them to achieve this goal, as well as protecting their code and their\nideas from their competitors. Another issue involved copyrighted material.\nOperating systems and other programs can limit the ability to play back movies",
  "and music or display electronic books to authorized computers. Such copy\nprotection or digital rights management (DRM) would not be effective if the\nsource code that implemented these limits were published. Laws in many\ncountries, including the U.S. Digital Millennium Copyright Act (DMCA), make\nit illegal to reverse-engineer DRM code or otherwise try to circumvent copy\nprotection. 1.12\nOpen-Source Operating Systems\n45\nTo counter the move to limit software use and redistribution, Richard\nStallman in 1983 started the GNU project to create a free, open-source, UNIX-\ncompatible operating system. In 1985, he published the GNU Manifesto, which\nargues that all software should be free and open-sourced. He also formed\nthe Free Software Foundation (FSF) with the goal of encouraging the free",
  "exchange of software source code and the free use of that software. Rather than\ncopyright its software, the FSF “copylefts” the software to encourage sharing\nand improvement. The GNU General Public License (GPL) codiﬁes copylefting\nand is a common license under which free software is released. Fundamentally,\nGPL requires that the source code be distributed with any binaries and that any\nchanges made to the source code be released under the same GPL license.\n1.12.2\nLinux\nAs an example of an open-source operating system, consider GNU/Linux.\nThe GNU project produced many UNIX-compatible tools, including compilers,\neditors, and utilities, but never released a kernel. In 1991, a student in\nFinland, Linus Torvalds, released a rudimentary UNIX-like kernel using the",
  "GNU compilers and tools and invited contributions worldwide. The advent of\nthe Internet meant that anyone interested could download the source code,\nmodify it, and submit changes to Torvalds. Releasing updates once a week\nallowed this so-called Linux operating system to grow rapidly, enhanced by\nseveral thousand programmers.\nThe resulting GNU/Linux operating system has spawned hundreds of\nunique distributions, or custom builds, of the system. Major distributions\ninclude RedHat, SUSE, Fedora, Debian, Slackware, and Ubuntu. Distributions\nvary in function, utility, installed applications, hardware support, user inter-\nface, and purpose. For example, RedHat Enterprise Linux is geared to large\ncommercial use. PCLinuxOS is a LiveCD—an operating system that can be",
  "booted and run from a CD-ROM without being installed on a system’s hard\ndisk. One variant of PCLinuxOS—called “PCLinuxOS Supergamer DVD”—is a\nLiveDVD that includes graphics drivers and games. A gamer can run it on\nany compatible system simply by booting from the DVD. When the gamer is\nﬁnished, a reboot of the system resets it to its installed operating system.\nYou can run Linux on a Windows system using the following simple, free\napproach:\n1. Download the free “VMware Player” tool from\nhttp://www.vmware.com/download/player/\nand install it on your system.\n2. Choose a Linux version from among the hundreds of “appliances,” or\nvirtual machine images, available from VMware at\nhttp://www.vmware.com/appliances/\nThese images are preinstalled with operating systems and applications",
  "and include many ﬂavors of Linux. 46\nChapter 1\nIntroduction\n3. Boot the virtual machine within VMware Player.\nWith this text, we provide a virtual machine image of Linux running the Debian\nrelease. This image contains the Linux source code as well as tools for software\ndevelopment. We cover examples involving that Linux image throughout this\ntext, as well as in a detailed case study in Chapter 18.\n1.12.3\nBSD UNIX\nBSD UNIX has a longer and more complicated history than Linux. It started in\n1978 as a derivative of AT&T’s UNIX. Releases from the University of California\nat Berkeley (UCB) came in source and binary form, but they were not open-\nsource because a license from AT&T was required. BSD UNIX’s development was\nslowed by a lawsuit by AT&T, but eventually a fully functional, open-source",
  "version, 4.4BSD-lite, was released in 1994.\nJust as with Linux, there are many distributions of BSD UNIX, including\nFreeBSD, NetBSD, OpenBSD, and DragonﬂyBSD. To explore the source code\nof FreeBSD, simply download the virtual machine image of the version of\ninterest and boot it within VMware, as described above for Linux. The source\ncode comes with the distribution and is stored in /usr/src/. The kernel\nsource code is in /usr/src/sys. For example, to examine the virtual memory\nimplementation code in the FreeBSD kernel, see the ﬁles in /usr/src/sys/vm.\nDarwin, the core kernel component of Mac OS X, is based on BSD\nUNIX and is open-sourced as well. That source code is available from\nhttp://www.opensource.apple.com/. Every Mac OS X release has its open-",
  "source components posted at that site. The name of the package that contains\nthe kernel begins with “xnu.” Apple also provides extensive developer tools,\ndocumentation, and support at http://connect.apple.com. For more informa-\ntion, see Appendix A.\n1.12.4\nSolaris\nSolaris is the commercial UNIX-based operating system of Sun Microsystems.\nOriginally, Sun’s SunOS operating system was based on BSD UNIX. Sun moved\nto AT&T’s System V UNIX as its base in 1991. In 2005, Sun open-sourced most\nof the Solaris code as the OpenSolaris project. The purchase of Sun by Oracle\nin 2009, however, left the state of this project unclear. The source code as it\nwas in 2005 is still available via a source code browser and for download at\nhttp://src.opensolaris.org/source.",
  "http://src.opensolaris.org/source.\nSeveral groups interested in using OpenSolaris have started from that base\nand expanded its features. Their working set is Project Illumos, which has\nexpanded from the OpenSolaris base to include more features and to be the\nbasis for several products. Illumos is available at http://wiki.illumos.org.\n1.12.5\nOpen-Source Systems as Learning Tools\nThe free software movement is driving legions of programmers to create\nthousands of open-source projects, including operating systems. Sites like\nhttp://freshmeat.net/ and http://distrowatch.com/ provide portals to many\nof these projects. As we stated earlier, open-source projects enable students to\nuse source code as a learning tool. They can modify programs and test them, 1.13\nSummary\n47",
  "Summary\n47\nhelp ﬁnd and ﬁx bugs, and otherwise explore mature, full-featured operating\nsystems, compilers, tools, user interfaces, and other types of programs. The\navailability of source code for historic projects, such as Multics, can help\nstudents to understand those projects and to build knowledge that will help in\nthe implementation of new projects.\nGNU/Linux and BSD UNIX are all open-source operating systems, but each\nhas its own goals, utility, licensing, and purpose. Sometimes, licenses are not\nmutually exclusive and cross-pollination occurs, allowing rapid improvements\nin operating-system projects. For example, several major components of\nOpenSolaris have been ported to BSD UNIX. The advantages of free software",
  "and open sourcing are likely to increase the number and quality of open-source\nprojects, leading to an increase in the number of individuals and companies\nthat use these projects.\n1.13\nSummary\nAn operating system is software that manages the computer hardware, as well\nas providing an environment for application programs to run. Perhaps the\nmost visible aspect of an operating system is the interface to the computer\nsystem it provides to the human user.\nFor a computer to do its job of executing programs, the programs must be in\nmain memory. Main memory is the only large storage area that the processor\ncan access directly. It is an array of bytes, ranging in size from millions to\nbillions. Each byte in memory has its own address. The main memory is usually",
  "a volatile storage device that loses its contents when power is turned off or\nlost. Most computer systems provide secondary storage as an extension of\nmain memory. Secondary storage provides a form of nonvolatile storage that\nis capable of holding large quantities of data permanently. The most common\nsecondary-storage device is a magnetic disk, which provides storage of both\nprograms and data.\nThe wide variety of storage systems in a computer system can be organized\nin a hierarchy according to speed and cost. The higher levels are expensive,\nbut they are fast. As we move down the hierarchy, the cost per bit generally\ndecreases, whereas the access time generally increases.\nThere are several different strategies for designing a computer system.",
  "Single-processor systems have only one processor, while multiprocessor\nsystems contain two or more processors that share physical memory and\nperipheral devices. The most common multiprocessor design is symmetric\nmultiprocessing (or SMP), where all processors are considered peers and run\nindependently of one another. Clustered systems are a specialized form of\nmultiprocessor systems and consist of multiple computer systems connected\nby a local-area network.\nTo best utilize the CPU, modern operating systems employ multiprogram-\nming, whichallowsseveral jobstobe inmemoryatthe same time, thusensuring\nthat the CPU always has a job to execute. Time-sharing systems are an exten-\nsion of multiprogramming wherein CPU scheduling algorithms rapidly switch",
  "between jobs, thus providing the illusion that each job is running concurrently.\nThe operating system must ensure correct operation of the computer\nsystem. To prevent user programs from interfering with the proper operation of 48\nChapter 1\nIntroduction\nTHE STUDY OF OPERATING SYSTEMS\nThere has never been a more interesting time to study operating systems, and\nit has never been easier. The open-source movement has overtaken operating\nsystems, causing many of them to be made available in both source and binary\n(executable) format. The list of operating systems available in both formats\nincludes Linux, BSD UNIX, Solaris, and part of Mac OS X. The availability\nof source code allows us to study operating systems from the inside out.",
  "Questions that we could once answer only by looking at documentation or\nthe behavior of an operating system we can now answer by examining the\ncode itself.\nOperating systems that are no longer commercially viable have been\nopen-sourced as well, enabling us to study how systems operated in a\ntime of fewer CPU, memory, and storage resources. An extensive but\nincomplete list of open-source operating-system projects is available from\nhttp://dmoz.org/Computers/Software/Operating Systems/Open Source/.\nIn addition, the rise of virtualization as a mainstream (and frequently free)\ncomputer function makes it possible to run many operating systems on top of\none core system. For example, VMware ( http://www.vmware.com)provides\na free “player” for Windows on which hundreds of free “virtual appliances”",
  "can run. Virtualbox ( http://www.virtualbox.com) provides a free, open-\nsource virtual machine manager on many operating systems. Using such\ntools, students can try out hundreds of operating systems without dedicated\nhardware.\nIn some cases, simulators of speciﬁc hardware are also available, allowing\nthe operating system to run on “native” hardware, all within the conﬁnes\nof a modern computer and modern operating system. For example, a\nDECSYSTEM-20 simulator running on Mac OS X can boot TOPS-20, load the\nsource tapes, and modify and compile a new TOPS-20 kernel. An interested\nstudent can search the Internet to ﬁnd the original papers that describe the\noperating system, as well as the original manuals.\nThe advent of open-source operating systems has also made it easier to",
  "make the move from student to operating-system developer. With some\nknowledge, some effort, and an Internet connection, a student can even create\na new operating-system distribution. Just a few years ago, it was difﬁcult or\nimpossible to get access to source code. Now, such access is limited only by\nhow much interest, time, and disk space a student has.\nthe system, the hardware has two modes: user mode and kernel mode. Various\ninstructions (such as I/O instructions and halt instructions) are privileged and\ncan be executed only in kernel mode. The memory in which the operating\nsystem resides must also be protected from modiﬁcation by the user. A timer\nprevents inﬁnite loops. These facilities (dual mode, privileged instructions,",
  "memory protection, and timer interrupt) are basic building blocks used by\noperating systems to achieve correct operation.\nA process (or job) is the fundamental unit of work in an operating system.\nProcess management includes creating and deleting processes and providing\nmechanisms for processes to communicate and synchronize with each other. Practice Exercises\n49\nAn operating system manages memory by keeping track of what parts of\nmemory are being used and by whom. The operating system is also responsible\nfor dynamically allocating and freeing memory space. Storage space is also\nmanaged by the operating system; this includes providing ﬁle systems for\nrepresenting ﬁles and directories and managing space on mass-storage devices.",
  "Operating systems must also be concerned with protecting and securing the\noperating system and users. Protection measures control the access of processes\nor users to the resources made available by the computer system. Security\nmeasures are responsible for defending a computer system from external or\ninternal attacks.\nSeveral datastructures that are fundamental tocomputer science are widely\nused in operating systems, including lists, stacks, queues, trees, hash functions,\nmaps, and bitmaps.\nComputing takes place in a variety of environments. Traditional computing\ninvolves desktop and laptop PCs, usually connected to a computer network.\nMobile computing refers to computing on handheld smartphones and tablet\ncomputers, which offer several unique features. Distributed systems allow",
  "users to share resources on geographically dispersed hosts connected via\na computer network. Services may be provided through either the client–\nserver model or the peer-to-peer model. Virtualization involves abstracting\na computer’s hardware into several different execution environments. Cloud\ncomputing uses a distributed system to abstract services into a “cloud,” where\nusers may access the services from remote locations. Real-time operating\nsystems are designed for embedded environments, such as consumer devices,\nautomobiles, and robotics.\nThe free software movement has created thousands of open-source projects,\nincluding operating systems. Because of these projects, students are able to use\nsource code as a learning tool. They can modify programs and test them,",
  "help ﬁnd and ﬁx bugs, and otherwise explore mature, full-featured operating\nsystems, compilers, tools, user interfaces, and other types of programs.\nGNU/Linux and BSD UNIX are open-source operating systems. The advan-\ntages of free software and open sourcing are likely to increase the number\nand quality of open-source projects, leading to an increase in the number of\nindividuals and companies that use these projects.\nPractice Exercises\n1.1\nWhat are the three main purposes of an operating system?\n1.2\nWe have stressed the need for an operating system to make efﬁcient use\nof the computing hardware. When is it appropriate for the operating\nsystem to forsake this principle and to “waste” resources? Why is such\na system not really wasteful?\n1.3",
  "a system not really wasteful?\n1.3\nWhat is the main difﬁculty that a programmer must overcome in writing\nan operating system for a real-time environment?\n1.4\nKeeping in mind the various deﬁnitions of operating system, consider\nwhether the operating system should include applications such as web\nbrowsers and mail programs. Argue both that it should and that it should\nnot, and support your answers. 50\nChapter 1\nIntroduction\n1.5\nHow does the distinction between kernel mode and user mode function\nas a rudimentary form of protection (security) system?\n1.6\nWhich of the following instructions should be privileged?\na.\nSet value of timer.\nb.\nRead the clock.\nc.\nClear memory.\nd.\nIssue a trap instruction.\ne.\nTurn off interrupts.\nf.\nModify entries in device-status table.\ng.",
  "f.\nModify entries in device-status table.\ng.\nSwitch from user to kernel mode.\nh.\nAccess I/O device.\n1.7\nSome early computers protected the operating system by placing it in\na memory partition that could not be modiﬁed by either the user job\nor the operating system itself. Describe two difﬁculties that you think\ncould arise with such a scheme.\n1.8\nSome CPUs provide for more than two modes of operation. What are\ntwo possible uses of these multiple modes?\n1.9\nTimers could be used to compute the current time. Provide a short\ndescription of how this could be accomplished.\n1.10\nGive two reasons why caches are useful. What problems do they solve?\nWhat problems do they cause? If a cache can be made as large as the\ndevice for which it is caching (for instance, a cache as large as a disk),",
  "why not make it that large and eliminate the device?\n1.11\nDistinguish between the client–server and peer-to-peer models of\ndistributed systems.\nExercises\n1.12\nIn a multiprogramming and time-sharing environment, several users\nshare the system simultaneously. This situation can result in various\nsecurity problems.\na.\nWhat are two such problems?\nb.\nCan we ensure the same degree of security in a time-shared\nmachine as in a dedicated machine? Explain your answer.\n1.13\nThe issue of resource utilization shows up in different forms in different\ntypes of operating systems. List what resources must be managed\ncarefully in the following settings:\na.\nMainframe or minicomputer systems\nb.\nWorkstations connected to servers\nc.\nMobile computers Exercises\n51\n1.14",
  "c.\nMobile computers Exercises\n51\n1.14\nUnder what circumstances would a user be better off using a time-\nsharing system than a PC or a single-user workstation?\n1.15\nDescribe the differences between symmetric and asymmetric multipro-\ncessing. What are three advantages and one disadvantage of multipro-\ncessor systems?\n1.16\nHow do clustered systems differ from multiprocessor systems? What is\nrequired for two machines belonging to a cluster to cooperate to provide\na highly available service?\n1.17\nConsider a computing cluster consisting of two nodes running a\ndatabase. Describe two ways in which the cluster software can manage\naccess to the data on the disk. Discuss the beneﬁts and disadvantages of\neach.\n1.18\nHow are network computers different from traditional personal com-",
  "puters? Describe some usage scenarios in which it is advantageous to\nuse network computers.\n1.19\nWhat is the purpose of interrupts? How does an interrupt differ from a\ntrap? Can traps be generated intentionally by a user program? If so, for\nwhat purpose?\n1.20\nDirect memory access is used for high-speed I/O devices in order to\navoid increasing the CPU’s execution load.\na.\nHow does the CPU interface with the device to coordinate the\ntransfer?\nb.\nHow does the CPU know when the memory operations are com-\nplete?\nc.\nThe CPU is allowed to execute other programs while the DMA\ncontroller is transferring data. Does this process interfere with\nthe execution of the user programs? If so, describe what forms\nof interference are caused.\n1.21",
  "of interference are caused.\n1.21\nSome computer systems do not provide a privileged mode of operation\nin hardware. Is it possible to construct a secure operating system for\nthese computer systems? Give arguments both that it is and that it is not\npossible.\n1.22\nMany SMP systems have different levels of caches; one level is local to\neach processing core, and another level is shared among all processing\ncores. Why are caching systems designed this way?\n1.23\nConsider an SMP system similar to the one shown in Figure 1.6. Illustrate\nwith an example how data residing in memory could in fact have a\ndifferent value in each of the local caches.\n1.24\nDiscuss, with examples, how the problem of maintaining coherence of\ncached data manifests itself in the following processing environments:\na.",
  "a.\nSingle-processor systems\nb.\nMultiprocessor systems\nc.\nDistributed systems 52\nChapter 1\nIntroduction\n1.25\nDescribe a mechanism for enforcing memory protection in order to\nprevent a program from modifying the memory associated with other\nprograms.\n1.26\nWhich network conﬁguration—LAN or WAN—would best suit the\nfollowing environments?\na.\nA campus student union\nb.\nSeveral campus locations across a statewide university system\nc.\nA neighborhood\n1.27\nDescribe some of the challenges of designing operating systems for\nmobile devices compared with designing operating systems for tradi-\ntional PCs.\n1.28\nWhat are some advantages of peer-to-peer systems over client-server\nsystems?\n1.29\nDescribe some distributed applications that would be appropriate for a\npeer-to-peer system.\n1.30",
  "peer-to-peer system.\n1.30\nIdentify several advantages and several disadvantages of open-source\noperating systems. Include the types of people who would ﬁnd each\naspect to be an advantage or a disadvantage.\nBibliographical Notes\n[Brookshear (2012)] provides an overview of computer science in general.\nThorough coverage of data structures can be found in [Cormen et al. (2009)].\n[Russinovich and Solomon (2009)] give an overview of Microsoft Windows\nand covers considerable technical detail about the system internals and\ncomponents. [McDougall and Mauro (2007)] cover the internals of the Solaris\noperating system. Mac OS X internals are discussed in [Singh (2007)]. [Love\n(2010)] provides an overview of the Linux operating system and great detail\nabout data structures used in the Linux kernel.",
  "about data structures used in the Linux kernel.\nMany general textbooks cover operating systems, including [Stallings\n(2011)], [Deitel et al. (2004)], and [Tanenbaum (2007)]. [Kurose and Ross (2013)]\nprovides a general overview of computer networks, including a discussion\nof client-server and peer-to-peer systems. [Tarkoma and Lagerspetz (2011)]\nexamines several different mobile operating systems, including Android and\niOS.\n[Hennessyand Patterson(2012)]provide coverage of I/O systemsand buses\nand of system architecture in general. [Bryant and O’Hallaron (2010)] provide\na thorough overview of a computer system from the perspective of a computer\nprogrammer. Details of the Intel 64 instruction set and privilege modes can be\nfound in [Intel (2011)].",
  "found in [Intel (2011)].\nThe history of open sourcing and its beneﬁts and challenges appears in\n[Raymond (1999)]. The Free Software Foundation has published its philosophy\nin http://www.gnu.org/philosophy/free-software-for-freedom.html. The open\nsource of Mac OS X are available from http://www.apple.com/opensource/. Bibliography\n53\nWikipedia has an informative entry about the contributions of Richard\nStallman at http://en.wikipedia.org/wiki/Richard Stallman.\nThe source code of Multicsisavailable at http://web.mit.edu/multics-history\n/source/Multics Internet Server/Multics sources.html.\nBibliography\n[Brookshear (2012)]\nJ. G. Brookshear, Computer Science: An Overview, Eleventh\nEdition, Addison-Wesley (2012).\n[Bryant and O’Hallaron (2010)]\nR. Bryant and D. O’Hallaron, Computer Systems:",
  "R. Bryant and D. O’Hallaron, Computer Systems:\nA Programmers Perspective, Second Edition, Addison-Wesley (2010).\n[Cormen et al. (2009)]\nT. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein,\nIntroduction to Algorithms, Third Edition, MIT Press (2009).\n[Deitel et al. (2004)]\nH. Deitel, P. Deitel, and D. Choffnes, Operating Systems,\nThird Edition, Prentice Hall (2004).\n[Hennessy and Patterson (2012)]\nJ. Hennessy and D. Patterson, Computer Archi-\ntecture: A Quantitative Approach, Fifth Edition, Morgan Kaufmann (2012).\n[Intel (2011)]\nIntel 64 and IA-32 Architectures Software Developer’s Manual, Com-\nbined Volumes: 1, 2A, 2B, 3A and 3B. Intel Corporation (2011).\n[Kurose and Ross (2013)]\nJ. Kurose and K. Ross, Computer Networking—A Top–\nDown Approach, Sixth Edition, Addison-Wesley (2013).",
  "[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[McDougall and Mauro (2007)]\nR. McDougall and J. Mauro, Solaris Internals,\nSecond Edition, Prentice Hall (2007).\n[Raymond (1999)]\nE. S. Raymond, The Cathedral and the Bazaar, O’Reilly &\nAssociates (1999).\n[Russinovich and Solomon (2009)]\nM. E. Russinovich and D. A. Solomon, Win-\ndows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition,\nMicrosoft Press (2009).\n[Singh (2007)]\nA. Singh, Mac OS X Internals: A Systems Approach, Addison-\nWesley (2007).\n[Stallings (2011)]\nW. Stallings, Operating Systems, Seventh Edition, Prentice Hall\n(2011).\n[Tanenbaum (2007)]\nA. S. Tanenbaum, Modern Operating Systems, Third Edition,\nPrentice Hall (2007).\n[Tarkoma and Lagerspetz (2011)]",
  "[Tarkoma and Lagerspetz (2011)]\nS. Tarkoma and E. Lagerspetz, “Arching over\nthe Mobile Computing Chasm: Platforms and Runtimes”, IEEE Computer,\nVolume 44, (2011), pages 22–28.  2\nC H A P T E R\nOperating-\nSystem\nStructures\nAn operating system provides the environment within which programs are\nexecuted. Internally, operating systems vary greatly in their makeup, since\nthey are organized along many different lines. The design of a new operating\nsystem is a major task. It is important that the goals of the system be well\ndeﬁned before the design begins. These goals form the basis for choices among\nvarious algorithms and strategies.\nWe can view an operating system from several vantage points. One view\nfocuses on the services that the system provides; another, on the interface that",
  "it makes available to users and programmers; a third, on its components and\ntheir interconnections. In this chapter, we explore all three aspects of operating\nsystems, showing the viewpoints of users, programmers, and operating system\ndesigners. We consider what services an operating system provides, how they\nare provided, how they are debugged, and what the various methodologies\nare for designing such systems. Finally, we describe how operating systems\nare created and how a computer starts its operating system.\nCHAPTER OBJECTIVES\n• To describe the services an operating system provides to users, processes,\nand other systems.\n• To discuss the various ways of structuring an operating system.\n• To explain how operating systems are installed and customized and how\nthey boot.\n2.1",
  "they boot.\n2.1\nOperating-System Services\nAn operating system provides an environment for the execution of programs.\nIt provides certain services to programs and to the users of those programs.\nThe speciﬁc services provided, of course, differ from one operating system to\nanother, but we can identify common classes. These operating system services\nare provided for the convenience of the programmer, to make the programming\n55 56\nChapter 2\nOperating-System Structures\nuser and other system programs\nservices\noperating system\nhardware\nsystem calls\nGUI\nbatch\nuser interfaces\ncommand line\nprogram\nexecution\nI/O\noperations\nfile\nsystems\ncommunication\nresource\nallocation\naccounting\nprotection\nand\nsecurity\nerror\ndetection\nFigure 2.1\nA view of operating system services.",
  "Figure 2.1\nA view of operating system services.\ntask easier. Figure 2.1 shows one view of the various operating-system services\nand how they interrelate.\nOne set of operating system services provides functions that are helpful to\nthe user.\n• User interface. Almost all operating systems have a user interface (UI).\nThis interface can take several forms. One is a command-line interface\n(CLI), which uses text commands and a method for entering them (say,\na keyboard for typing in commands in a speciﬁc format with speciﬁc\noptions). Another is a batch interface, in which commands and directives\nto control those commands are entered into ﬁles, and those ﬁles are\nexecuted. Most commonly, a graphical user interface (GUI) is used. Here,",
  "the interface is a window system with a pointing device to direct I/O,\nchoose from menus, and make selections and a keyboard to enter text.\nSome systems provide two or all three of these variations.\n• Program execution. The system must be able to load a program into\nmemory and to run that program. The program must be able to end its\nexecution, either normally or abnormally (indicating error).\n• I/O operations. A running program may require I/O, which may involve a\nﬁle or an I/O device. For speciﬁc devices, special functions may be desired\n(such as recording to a CD or DVD drive or blanking a display screen). For\nefﬁciency and protection, users usually cannot control I/O devices directly.\nTherefore, the operating system must provide a means to do I/O.",
  "• File-system manipulation. The ﬁle system is of particular interest. Obvi-\nously, programs need to read and write ﬁles and directories. They also\nneed to create and delete them by name, search for a given ﬁle, and\nlist ﬁle information. Finally, some operating systems include permissions\nmanagement to allow or deny access to ﬁles or directories based on ﬁle\nownership. Many operating systems provide a variety of ﬁle systems,\nsometimes to allow personal choice and sometimes to provide speciﬁc\nfeatures or performance characteristics. 2.1\nOperating-System Services\n57\n• Communications. There are many circumstances in which one process\nneeds to exchange information with another process. Such communication\nmay occur between processes that are executing on the same computer or",
  "between processes that are executing on different computer systems tied\ntogether by a computer network. Communications may be implemented\nvia shared memory, in which two or more processes read and write to\na shared section of memory, or message passing, in which packets of\ninformation in predeﬁned formats are moved between processes by the\noperating system.\n• Error detection. The operating system needs to be detecting and correcting\nerrors constantly. Errors may occur in the CPU and memory hardware (such\nas a memory error or a power failure), in I/O devices (such as a parity error\non disk, a connection failure on a network, or lack of paper in the printer),\nand in the user program (such as an arithmetic overﬂow, an attempt to",
  "access an illegal memory location, or a too-great use of CPU time). For\neach type of error, the operating system should take the appropriate action\nto ensure correct and consistent computing. Sometimes, it has no choice\nbut to halt the system. At other times, it might terminate an error-causing\nprocess or return an error code to a process for the process to detect and\npossibly correct.\nAnother set of operating system functions exists not for helping the user\nbut rather for ensuring the efﬁcient operation of the system itself. Systems with\nmultiple users can gain efﬁciency by sharing the computer resources among\nthe users.\n• Resource allocation. When there are multiple users or multiple jobs\nrunning at the same time, resources must be allocated to each of them. The",
  "operating system manages many different types of resources. Some (such\nas CPU cycles, main memory, and ﬁle storage) may have special allocation\ncode, whereas others (such as I/O devices) may have much more general\nrequest and release code. For instance, in determining how best to use\nthe CPU, operating systems have CPU-scheduling routines that take into\naccount the speed of the CPU, the jobs that must be executed, the number of\nregisters available, and other factors. There may also be routines to allocate\nprinters, USB storage drives, and other peripheral devices.\n• Accounting. We want to keep track of which users use how much and\nwhat kinds of computer resources. This record keeping may be used for\naccounting (so that users can be billed) or simply for accumulating usage",
  "statistics. Usage statistics may be a valuable tool for researchers who wish\nto reconﬁgure the system to improve computing services.\n• Protection and security. The owners of information stored in a multiuser or\nnetworked computer system may want to control use of that information.\nWhen several separate processes execute concurrently, it should not be\npossible for one process to interfere with the others or with the operating\nsystem itself. Protection involves ensuring that all access to system\nresources is controlled. Security of the system from outsiders is also\nimportant. Such security starts with requiring each user to authenticate 58\nChapter 2\nOperating-System Structures\nhimself or herself to the system, usually by means of a password, to gain",
  "access to system resources. It extends to defending external I/O devices,\nincluding network adapters, from invalid access attempts and to recording\nall such connections for detection of break-ins. If a system is to be protected\nand secure, precautions must be instituted throughout it. A chain is only\nas strong as its weakest link.\n2.2\nUser and Operating-System Interface\nWe mentioned earlier that there are several ways for users to interface with\nthe operating system. Here, we discuss two fundamental approaches. One\nprovides a command-line interface, or command interpreter, that allows users\nto directly enter commands to be performed by the operating system. The\nother allows users to interface with the operating system via a graphical user\ninterface, or GUI.\n2.2.1\nCommand Interpreters",
  "interface, or GUI.\n2.2.1\nCommand Interpreters\nSome operating systems include the command interpreter in the kernel. Others,\nsuch as Windows and UNIX, treat the command interpreter as a special program\nthat is running when a job is initiated or when a user ﬁrst logs on (on interactive\nsystems). On systems with multiple command interpreters to choose from, the\ninterpreters are known as shells. For example, on UNIX and Linux systems, a\nuser may choose among several different shells, including the Bourne shell, C\nshell, Bourne-Again shell, Korn shell, and others. Third-party shells and free\nuser-written shells are also available. Most shells provide similar functionality,\nand a user’s choice of which shell to use is generally based on personal",
  "preference. Figure 2.2 shows the Bourne shell command interpreter being used\non Solaris 10.\nThe main function of the command interpreter is to get and execute the next\nuser-speciﬁed command. Many of the commands given at this level manipulate\nﬁles: create, delete, list, print, copy, execute, and so on. The MS-DOS and UNIX\nshells operate in this way. These commands can be implemented in two general\nways.\nIn one approach, the command interpreter itself contains the code to\nexecute the command. For example, a command to delete a ﬁle may cause\nthe command interpreter to jump to a section of its code that sets up the\nparameters and makes the appropriate system call. In this case, the number of\ncommands that can be given determines the size of the command interpreter,",
  "since each command requires its own implementing code.\nAn alternative approach—used by UNIX, among other operating systems\n—implements most commands through system programs. In this case, the\ncommand interpreter does not understand the command in any way; it merely\nuses the command to identify a ﬁle to be loaded into memory and executed.\nThus, the UNIX command to delete a ﬁle\nrm file.txt\nwould search for a ﬁle called rm, load the ﬁle into memory, and execute it with\nthe parameter file.txt. The function associated with the rm command would 2.2\nUser and Operating-System Interface\n59\nFigure 2.2\nThe Bourne shell command interpreter in Solrais 10.\nbe deﬁned completely by the code in the ﬁle rm. In this way, programmers can",
  "add new commands to the system easily by creating new ﬁles with the proper\nnames. The command-interpreter program, which can be small, does not have\nto be changed for new commands to be added.\n2.2.2\nGraphical User Interfaces\nA second strategy for interfacing with the operating system is through a user-\nfriendly graphical user interface, or GUI. Here, rather than entering commands\ndirectly via a command-line interface, users employ a mouse-based window-\nand-menu system characterized by a desktop metaphor. The user moves the\nmouse to position its pointer on images, or icons, on the screen (the desktop)\nthat represent programs, ﬁles, directories, and system functions. Depending\non the mouse pointer’s location, clicking a button on the mouse can invoke a",
  "program, select a ﬁle or directory—known as a folder—or pull down a menu\nthat contains commands.\nGraphical user interfaces ﬁrst appeared due in part to research taking place\nin the early 1970s at Xerox PARC research facility. The ﬁrst GUI appeared on\nthe Xerox Alto computer in 1973. However, graphical interfaces became more\nwidespread with the advent of Apple Macintosh computers in the 1980s. The\nuser interface for the Macintosh operating system (Mac OS) has undergone\nvarious changes over the years, the most signiﬁcant being the adoption of\nthe Aqua interface that appeared with Mac OS X. Microsoft’s ﬁrst version of\nWindows—Version 1.0—was based on the addition of a GUI interface to the\nMS-DOS operating system. Later versions of Windows have made cosmetic 60\nChapter 2",
  "Chapter 2\nOperating-System Structures\nchanges in the appearance of the GUI along with several enhancements in its\nfunctionality.\nBecause a mouse is impractical for most mobile systems, smartphones and\nhandheld tablet computers typically use a touchscreen interface. Here, users\ninteract by making gestures on the touchscreen—for example, pressing and\nswiping ﬁngers across the screen. Figure 2.3 illustrates the touchscreen of the\nApple iPad. Whereas earlier smartphones included a physical keyboard, most\nsmartphones now simulate a keyboard on the touchscreen.\nTraditionally, UNIX systems have been dominated by command-line inter-\nfaces. Various GUI interfaces are available, however. These include the Common\nDesktop Environment (CDE) and X-Windows systems, which are common",
  "on commercial versions of UNIX, such as Solaris and IBM’s AIX system. In\naddition, there has been signiﬁcant development in GUI designs from various\nopen-source projects, such as K Desktop Environment (or KDE) and the GNOME\ndesktop by the GNU project. Both the KDE and GNOME desktops run on Linux\nand various UNIX systems and are available under open-source licenses, which\nmeans their source code is readily available for reading and for modiﬁcation\nunder speciﬁc license terms.\nFigure 2.3\nThe iPad touchscreen. 2.2\nUser and Operating-System Interface\n61\n2.2.3\nChoice of Interface\nThe choice of whether to use a command-line or GUI interface is mostly\none of personal preference. System administrators who manage computers\nand power users who have deep knowledge of a system frequently use the",
  "command-line interface. For them, it is more efﬁcient, giving them faster\naccess to the activities they need to perform. Indeed, on some systems, only a\nsubset of system functions is available via the GUI, leaving the less common\ntasks to those who are command-line knowledgeable. Further, command-\nline interfaces usually make repetitive tasks easier, in part because they have\ntheir own programmability. For example, if a frequent task requires a set of\ncommand-line steps, those steps can be recorded into a ﬁle, and that ﬁle can\nbe run just like a program. The program is not compiled into executable code\nbut rather is interpreted by the command-line interface. These shell scripts are\nvery common on systems that are command-line oriented, such as UNIX and\nLinux.",
  "Linux.\nIn contrast, most Windows users are happy to use the Windows GUI\nenvironment and almost never use the MS-DOS shell interface. The various\nchanges undergone by the Macintosh operating systems provide a nice study\nin contrast. Historically, Mac OS has not provided a command-line interface,\nalways requiring its users to interface with the operating system using its GUI.\nHowever, with the release of Mac OS X (which is in part implemented using a\nUNIX kernel), the operating system now provides both a Aqua interface and a\ncommand-line interface. Figure 2.4 is a screenshot of the Mac OS X GUI.\nFigure 2.4\nThe Mac OS X GUI. 62\nChapter 2\nOperating-System Structures\nThe user interface can vary from system to system and even from user",
  "to user within a system. It typically is substantially removed from the actual\nsystem structure. The design of a useful and friendly user interface is therefore\nnot a direct function of the operating system. In this book, we concentrate on\nthe fundamental problems of providing adequate service to user programs.\nFrom the point of view of the operating system, we do not distinguish between\nuser programs and system programs.\n2.3\nSystem Calls\nSystem calls provide an interface to the services made available by an operating\nsystem. These calls are generally available as routines written in C and\nC++, although certain low-level tasks (for example, tasks where hardware\nmust be accessed directly) may have to be written using assembly-language\ninstructions.",
  "instructions.\nBefore we discuss how an operating system makes system calls available,\nlet’s ﬁrst use an example to illustrate how system calls are used: writing a\nsimple program to read data from one ﬁle and copy them to another ﬁle. The\nﬁrst input that the program will need is the names of the two ﬁles: the input ﬁle\nand the output ﬁle. These names can be speciﬁed in many ways, depending on\nthe operating-system design. One approach is for the program to ask the user\nfor the names. In an interactive system, this approach will require a sequence of\nsystem calls, ﬁrst to write a prompting message on the screen and then to read\nfrom the keyboard the characters that deﬁne the two ﬁles. On mouse-based and\nicon-based systems, a menu of ﬁle names is usually displayed in a window.",
  "The user can then use the mouse to select the source name, and a window\ncan be opened for the destination name to be speciﬁed. This sequence requires\nmany I/O system calls.\nOnce the two ﬁle names have been obtained, the program must open the\ninput ﬁle and create the output ﬁle. Each of these operations requires another\nsystem call. Possible error conditions for each operation can require additional\nsystem calls. When the program tries to open the input ﬁle, for example, it may\nﬁnd that there is no ﬁle of that name or that the ﬁle is protected against access.\nIn these cases, the program should print a message on the console (another\nsequence of system calls) and then terminate abnormally (another system call).\nIf the input ﬁle exists, then we must create a new output ﬁle. We may ﬁnd that",
  "there is already an output ﬁle with the same name. This situation may cause\nthe program to abort (a system call), or we may delete the existing ﬁle (another\nsystem call) and create a new one (yet another system call). Another option,\nin an interactive system, is to ask the user (via a sequence of system calls to\noutput the prompting message and to read the response from the terminal)\nwhether to replace the existing ﬁle or to abort the program.\nWhen both ﬁles are set up, we enter a loop that reads from the input ﬁle\n(a system call) and writes to the output ﬁle (another system call). Each read\nand write must return status information regarding various possible error\nconditions. On input, the program may ﬁnd that the end of the ﬁle has been",
  "reached or that there was a hardware failure in the read (such as a parity error).\nThe write operation may encounter various errors, depending on the output\ndevice (for example, no more disk space). 2.3\nSystem Calls\n63\nFinally, after the entire ﬁle is copied, the program may close both ﬁles\n(another system call), write a message to the console or window (more system\ncalls), and ﬁnally terminate normally (the ﬁnal system call). This system-call\nsequence is shown in Figure 2.5.\nAs you can see, even simple programs may make heavy use of the\noperating system. Frequently, systems execute thousands of system calls\nper second. Most programmers never see this level of detail, however.\nTypically, application developers design programs according to an application",
  "programming interface (API). The API speciﬁes a set of functions that are\navailable to an application programmer, including the parameters that are\npassed to each function and the return values the programmer can expect.\nThree of the most common APIs available to application programmers are\nthe Windows API for Windows systems, the POSIX API for POSIX-based systems\n(which include virtually all versions of UNIX, Linux, and Mac OS X), and the Java\nAPI for programs that run on the Java virtual machine. A programmer accesses\nan API via a library of code provided by the operating system. In the case of\nUNIX and Linux for programs written in the C language, the library is called\nlibc. Note that—unless speciﬁed—the system-call names used throughout",
  "this text are generic examples. Each operating system has its own name for\neach system call.\nBehind the scenes, the functions that make up an API typically invoke the\nactual system calls on behalf of the application programmer. For example, the\nWindows function CreateProcess() (which unsurprisingly is used to create\na new process) actually invokes the NTCreateProcess() system call in the\nWindows kernel.\nWhy would an application programmer prefer programming according to\nan API rather than invoking actual system calls? There are several reasons for\ndoing so. One beneﬁt concerns program portability. An application program-\nsource file\ndestination  file\nExample System Call Sequence\nAcquire input file name \n  Write prompt to screen \n  Accept input\nAcquire output file name",
  "Accept input\nAcquire output file name\n  Write prompt to screen \n  Accept input \nOpen the input file \n  if file doesn't exist, abort \nCreate output file \n  if file exists, abort \nLoop \n  Read from input file \n  Write to output file \nUntil read fails \nClose output file\nWrite completion message to screen \nTerminate normally\nFigure 2.5\nExample of how system calls are used. 64\nChapter 2\nOperating-System Structures\nEXAMPLE OF STANDARD API\nAs an example of a standard API, consider the read() function that is\navailable in UNIX and Linux systems. The API for this function is obtained\nfrom the man page by invoking the command\nman read\non the command line. A description of this API appears below:\n#include <unistd.h>\nssize_t\nread(int fd, void *buf, size_t count)\nreturn\nvalue\nfunction\nname",
  "return\nvalue\nfunction\nname\nparameters\nA program that uses the read() function must include the unistd.h header\nﬁle, as this ﬁle deﬁnes the ssize t and size t data types (among other\nthings). The parameters passed to read() are as follows:\n• int fd—the ﬁle descriptor to be read\n• void *buf—a buffer where the data will be read into\n• size t count—the maximum number of bytes to be read into the\nbuffer\nOn a successful read, the number of bytes read is returned. A return value of\n0 indicates end of ﬁle. If an error occurs, read() returns −1.\nmer designing a program using an API can expect her program to compile and\nrun on any system that supports the same API (although, in reality, architectural\ndifferences often make this more difﬁcult than it may appear). Furthermore,",
  "actual system calls can often be more detailed and difﬁcult to work with than\nthe API available to an application programmer. Nevertheless, there often exists\na strong correlation between a function in the API and its associated system call\nwithin the kernel. In fact, many of the POSIX and Windows APIs are similar to\nthe native system calls provided by the UNIX, Linux, and Windows operating\nsystems.\nFor most programming languages, the run-time support system (a set of\nfunctions built into libraries included with a compiler) provides a system-\ncall interface that serves as the link to system calls made available by the\noperating system. The system-call interface intercepts function calls in the API\nand invokes the necessary system calls within the operating system. Typically,",
  "a number is associated with each system call, and the system-call interface\nmaintains a table indexed according to these numbers. The system call interface 2.3\nSystem Calls\n65\nImplementation\nof open ( )\nsystem call\nopen ( )\nuser \nmode\nreturn\nuser application\nsystem call interface\nkernel\nmode\ni\nopen ( )\nFigure 2.6\nThe handling of a user application invoking the open() system call.\nthen invokes the intended system call in the operating-system kernel and\nreturns the status of the system call and any return values.\nThe caller need know nothing about how the system call is implemented\nor what it does during execution. Rather, the caller need only obey the API and\nunderstand what the operating system will do as a result of the execution of",
  "that system call. Thus, most of the details of the operating-system interface\nare hidden from the programmer by the API and are managed by the run-time\nsupport library. The relationship between an API, the system-call interface,\nand the operating system is shown in Figure 2.6, which illustrates how the\noperating system handles a user application invoking the open() system call.\nSystem calls occur in different ways, depending on the computer in use.\nOften, more information is required than simply the identity of the desired\nsystem call. The exact type and amount of information vary according to the\nparticular operating system and call. For example, to get input, we may need\nto specify the ﬁle or device to use as the source, as well as the address and",
  "length of the memory buffer into which the input should be read. Of course,\nthe device or ﬁle and length may be implicit in the call.\nThree general methods are used to pass parameters to the operating system.\nThe simplest approach is to pass the parameters in registers. In some cases,\nhowever, there may be more parameters than registers. In these cases, the\nparameters are generally stored in a block, or table, in memory, and the\naddress of the block is passed as a parameter in a register (Figure 2.7). This\nis the approach taken by Linux and Solaris. Parameters also can be placed,\nor pushed, onto the stack by the program and popped off the stack by the\noperating system. Some operating systems prefer the block or stack method",
  "because those approaches do not limit the number or length of parameters\nbeing passed. 66\nChapter 2\nOperating-System Structures\ncode for \nsystem \ncall 13\noperating system\nuser program\nuse parameters\nfrom table X\nregister\nX\nX: parameters\nfor call\nload address X\nsystem call 13\nFigure 2.7\nPassing of parameters as a table.\n2.4\nTypes of System Calls\nSystem calls can be grouped roughly into six major categories: process\ncontrol, ﬁle manipulation, device manipulation, information maintenance,\ncommunications, and protection. In Sections 2.4.1 through 2.4.6, we brieﬂy\ndiscuss the types of system calls that may be provided by an operating system.\nMost of these system calls support, or are supported by, concepts and functions",
  "that are discussed in later chapters. Figure 2.8 summarizes the types of system\ncalls normally provided by an operating system. As mentioned, in this text,\nwe normally refer to the system calls by generic names. Throughout the text,\nhowever, we provide examples of the actual counterparts to the system calls\nfor Windows, UNIX, and Linux systems.\n2.4.1\nProcess Control\nA running program needs to be able to halt its execution either normally\n(end()) or abnormally (abort()). If a system call is made to terminate the\ncurrently running program abnormally, or if the program runs into a problem\nand causes an error trap, a dump of memory is sometimes taken and an error\nmessage generated. The dump is written to disk and may be examined by",
  "a debugger—a system program designed to aid the programmer in ﬁnding\nand correcting errors, or bugs—to determine the cause of the problem. Under\neither normal or abnormal circumstances, the operating system must transfer\ncontrol to the invoking command interpreter. The command interpreter then\nreads the next command. In an interactive system, the command interpreter\nsimply continues with the next command; it is assumed that the user will\nissue an appropriate command to respond to any error. In a GUI system, a\npop-up window might alert the user to the error and ask for guidance. In a\nbatch system, the command interpreter usually terminates the entire job and\ncontinues with the next job. Some systems may allow for special recovery",
  "actions in case an error occurs. If the program discovers an error in its input\nand wants to terminate abnormally, it may also want to deﬁne an error level.\nMore severe errors can be indicated by a higher-level error parameter. It is then 2.4\nTypes of System Calls\n67\n• Process control\n◦end, abort\n◦load, execute\n◦create process, terminate process\n◦get process attributes, set process attributes\n◦wait for time\n◦wait event, signal event\n◦allocate and free memory\n• File management\n◦create ﬁle, delete ﬁle\n◦open, close\n◦read, write, reposition\n◦get ﬁle attributes, set ﬁle attributes\n• Device management\n◦request device, release device\n◦read, write, reposition\n◦get device attributes, set device attributes\n◦logically attach or detach devices\n• Information maintenance",
  "• Information maintenance\n◦get time or date, set time or date\n◦get system data, set system data\n◦get process, ﬁle, or device attributes\n◦set process, ﬁle, or device attributes\n• Communications\n◦create, delete communication connection\n◦send, receive messages\n◦transfer status information\n◦attach or detach remote devices\nFigure 2.8\nTypes of system calls.\npossible to combine normal and abnormal termination by deﬁning a normal\ntermination as an error at level 0. The command interpreter or a following\nprogram can use this error level to determine the next action automatically.\nA process or job executing one program may want to load() and\nexecute() another program. This feature allows the command interpreter to\nexecute a program as directed by, for example, a user command, the click of a 68",
  "Chapter 2\nOperating-System Structures\nEXAMPLES OF WINDOWS AND UNIX SYSTEM CALLS\nWindows\nUnix\nProcess\nCreateProcess()\nfork()\nControl\nExitProcess()\nexit()\nWaitForSingleObject()\nwait()\nFile\nCreateFile()\nopen()\nManipulation\nReadFile()\nread()\nWriteFile()\nwrite()\nCloseHandle()\nclose()\nDevice\nSetConsoleMode()\nioctl()\nManipulation\nReadConsole()\nread()\nWriteConsole()\nwrite()\nInformation\nGetCurrentProcessID()\ngetpid()\nMaintenance\nSetTimer()\nalarm()\nSleep()\nsleep()\nCommunication\nCreatePipe()\npipe()\nCreateFileMapping()\nshm open()\nMapViewOfFile()\nmmap()\nProtection\nSetFileSecurity()\nchmod()\nInitlializeSecurityDescriptor()\numask()\nSetSecurityDescriptorGroup()\nchown()\nmouse, or a batch command. An interesting question is where to return control",
  "when the loaded program terminates. This question is related to whether the\nexisting program is lost, saved, or allowed to continue execution concurrently\nwith the new program.\nIf control returns to the existing program when the new program termi-\nnates, we must save the memory image of the existing program; thus, we have\neffectively created a mechanism for one program to call another program. If\nboth programs continue concurrently, we have created a new job or process to\nbe multiprogrammed. Often, there is a system call speciﬁcally for this purpose\n(create process() or submit job()).\nIf we create a new job or process, or perhaps even a set of jobs or\nprocesses, we should be able to control its execution. This control requires",
  "the ability to determine and reset the attributes of a job or process, includ-\ning the job’s priority, its maximum allowable execution time, and so on\n(get process attributes() and set process attributes()). We may also\nwant to terminate a job or process that we created (terminate process()) if\nwe ﬁnd that it is incorrect or is no longer needed. 2.4\nTypes of System Calls\n69\nEXAMPLE OF STANDARD C LIBRARY\nThe standard C library provides a portion of the system-call interface for\nmany versions of UNIX and Linux. As an example, let’s assume a C program\ninvokes the printf() statement. The C library intercepts this call and\ninvokes the necessary system call (or calls) in the operating system—in this\ninstance, the write() system call. The C library takes the value returned by",
  "write() and passes it back to the user program. This is shown below:\nwrite ( )\nsystem call\nuser\nmode\nkernel\nmode\n#include <stdio.h>\nint main ( )\n{\n   •\n   •\n   •\n   printf (\"Greetings\");\n   •\n   •\n   •\n   return 0;\n}\nstandard C library\nwrite ( )\nHaving created new jobs or processes, we may need to wait for them to\nﬁnish their execution. We may want to wait for a certain amount of time to\npass (wait time()). More probably, we will want to wait for a speciﬁc event\nto occur (wait event()). The jobs or processes should then signal when that\nevent has occurred (signal event()).\nQuite often, two or more processes may share data. To ensure the integrity\nof the data being shared, operating systems often provide system calls allowing",
  "a process to lock shared data. Then, no other process can access the data until\nthe lock is released. Typically, such system calls include acquire lock() and\nrelease lock(). System calls of these types, dealing with the coordination of\nconcurrent processes, are discussed in great detail in Chapter 5.\nThere are so many facets of and variations in process and job control that\nwe next use two examples—one involving a single-tasking system and the\nother a multitasking system—to clarify these concepts. The MS-DOS operating\nsystem is an example of a single-tasking system. It has a command interpreter\nthat is invoked when the computer is started (Figure 2.9(a)). Because MS-DOS\nis single-tasking, it uses a simple method to run a program and does not create",
  "a new process. It loads the program into memory, writing over most of itself to 70\nChapter 2\nOperating-System Structures\n(a)\n(b)\nfree memory\ncommand \ninterpreter\nkernel\nprocess\nfree memory\ncommand \ninterpreter\nkernel\nFigure 2.9\nMS-DOS execution. (a) At system startup. (b) Running a program.\ngive the program as much memory as possible (Figure 2.9(b)). Next, it sets the\ninstruction pointer to the ﬁrst instruction of the program. The program then\nruns, and either an error causes a trap, or the program executes a system call\nto terminate. In either case, the error code is saved in the system memory for\nlater use. Following this action, the small portion of the command interpreter\nthat was not overwritten resumes execution. Its ﬁrst task is to reload the rest",
  "of the command interpreter from disk. Then the command interpreter makes\nthe previous error code available to the user or to the next program.\nFreeBSD (derived from Berkeley UNIX) is an example of a multitasking\nsystem. When a user logs on to the system, the shell of the user’s choice\nis run. This shell is similar to the MS-DOS shell in that it accepts commands\nand executes programs that the user requests. However, since FreeBSD is a\nmultitasking system, the command interpreter may continue running while\nanother program is executed (Figure 2.10). To start a new process, the shell\nfree memory\ninterpreter\nkernel\nprocess D\nprocess C\nprocess B\nFigure 2.10\nFreeBSD running multiple programs. 2.4\nTypes of System Calls\n71\nexecutes a fork() system call. Then, the selected program is loaded into",
  "memory via an exec() system call, and the program is executed. Depending\non the way the command was issued, the shell then either waits for the process\nto ﬁnish or runs the process “in the background.” In the latter case, the shell\nimmediately requests another command. When a process is running in the\nbackground, it cannot receive input directly from the keyboard, because the\nshell is using this resource. I/O is therefore done through ﬁles or through a GUI\ninterface. Meanwhile, the user is free to ask the shell to run other programs, to\nmonitor the progress of the running process, to change that program’s priority,\nand so on. When the process is done, it executes an exit() system call to\nterminate, returning to the invoking process a status code of 0 or a nonzero",
  "error code. This status or error code is then available to the shell or other\nprograms. Processes are discussed in Chapter 3 with a program example using\nthe fork() and exec() system calls.\n2.4.2\nFile Management\nThe ﬁle system is discussed in more detail in Chapters 11 and 12. We can,\nhowever, identify several common system calls dealing with ﬁles.\nWe ﬁrst need to be able to create() and delete() ﬁles. Either system call\nrequires the name of the ﬁle and perhaps some of the ﬁle’s attributes. Once\nthe ﬁle is created, we need to open() it and to use it. We may also read(),\nwrite(), or reposition() (rewind or skip to the end of the ﬁle, for example).\nFinally, we need to close() the ﬁle, indicating that we are no longer using it.",
  "We may need these same sets of operations for directories if we have a\ndirectory structure for organizing ﬁles in the ﬁle system. In addition, for either\nﬁles or directories, we need to be able to determine the values of various\nattributes and perhaps to reset them if necessary. File attributes include the ﬁle\nname, ﬁle type, protection codes, accounting information, and so on. At least\ntwo system calls, get file attributes() and set file attributes(), are\nrequired for this function. Some operating systems provide many more calls,\nsuch as calls for ﬁle move() and copy(). Others might provide an API that\nperforms those operations using code and other system calls, and others might\nprovide system programs to perform those tasks. If the system programs are",
  "callable by other programs, then each can be considered an API by other system\nprograms.\n2.4.3\nDevice Management\nA process may need several resources to execute—main memory, disk drives,\naccess to ﬁles, and so on. If the resources are available, they can be granted,\nand control can be returned to the user process. Otherwise, the process will\nhave to wait until sufﬁcient resources are available.\nThe various resources controlled by the operating system can be thought\nof as devices. Some of these devices are physical devices (for example, disk\ndrives), while others can be thought of as abstract or virtual devices (for\nexample, ﬁles). A system with multiple users may require us to ﬁrst request()\na device, to ensure exclusive use of it. After we are ﬁnished with the device, we",
  "release() it. These functions are similar to the open() and close() system\ncalls for ﬁles. Other operating systems allow unmanaged access to devices. 72\nChapter 2\nOperating-System Structures\nThe hazard then is the potential for device contention and perhaps deadlock,\nwhich are described in Chapter 7.\nOnce the device has been requested (and allocated to us), we can read(),\nwrite(), and (possibly) reposition() the device, just as we can with ﬁles. In\nfact, the similarity between I/O devices and ﬁles is so great that many operating\nsystems, including UNIX, merge the two into a combined ﬁle–device structure.\nIn this case, a set of system calls is used on both ﬁles and devices. Sometimes,\nI/O devices are identiﬁed by special ﬁle names, directory placement, or ﬁle\nattributes.",
  "attributes.\nThe user interface can also make ﬁles and devices appear to be similar, even\nthough the underlying system calls are dissimilar. This is another example of\nthe many design decisions that go into building an operating system and user\ninterface.\n2.4.4\nInformation Maintenance\nMany system calls exist simply for the purpose of transferring information\nbetween the user program and the operating system. For example, most\nsystems have a system call to return the current time() and date(). Other\nsystem calls may return information about the system, such as the number of\ncurrent users, the version number of the operating system, the amount of free\nmemory or disk space, and so on.\nAnother set of system calls is helpful in debugging a program. Many",
  "systems provide system calls to dump() memory. This provision is useful for\ndebugging. A program trace lists each system call as it is executed. Even\nmicroprocessors provide a CPU mode known as single step, in which a trap\nis executed by the CPU after every instruction. The trap is usually caught by a\ndebugger.\nMany operating systems provide a time proﬁle of a program to indicate\nthe amount of time that the program executes at a particular location or set\nof locations. A time proﬁle requires either a tracing facility or regular timer\ninterrupts. At every occurrence of the timer interrupt, the value of the program\ncounter is recorded. With sufﬁciently frequent timer interrupts, a statistical\npicture of the time spent on various parts of the program can be obtained.",
  "In addition, the operating system keeps information about all its processes,\nand system calls are used to access this information. Generally, calls are\nalso used to reset the process information (get process attributes() and\nset process attributes()). In Section 3.1.3, we discuss what information is\nnormally kept.\n2.4.5\nCommunication\nThere are two common models of interprocess communication: the message-\npassing model and the shared-memory model. In the message-passing model,\nthe communicating processes exchange messages with one another to transfer\ninformation. Messages can be exchanged between the processes either directly\nor indirectly through a common mailbox. Before communication can take\nplace, a connection must be opened. The name of the other communicator",
  "must be known, be it another process on the same system or a process on\nanother computer connected by a communications network. Each computer in\na network has a host name by which it is commonly known. A host also has a 2.4\nTypes of System Calls\n73\nnetwork identiﬁer, such as an IP address. Similarly, each process has a process\nname, and this name is translated into an identiﬁer by which the operating\nsystem can refer to the process. The get hostid() and get processid()\nsystem calls do this translation. The identiﬁers are then passed to the general-\npurpose open() and close() calls provided by the ﬁle system or to speciﬁc\nopen connection()andclose connection()systemcalls, dependingonthe\nsystem’s model of communication. The recipient process usually must give its",
  "permission for communication to take place with an accept connection()\ncall. Most processes that will be receiving connections are special-purpose\ndaemons, which are system programs provided for that purpose. They execute\na wait for connection() call and are awakened when a connection is made.\nThe source of the communication, known as the client, and the receiving\ndaemon, known as a server, then exchange messages by using read message()\nand write message() system calls. The close connection() call terminates\nthe communication.\nIn the shared-memory model, processes use shared memory create()\nand shared memory attach() system calls to create and gain access to regions\nof memory owned by other processes. Recall that, normally, the operating",
  "system tries to prevent one process from accessing another process’s memory.\nShared memory requires that two or more processes agree to remove this\nrestriction. They can then exchange information by reading and writing data\nin the shared areas. The form of the data is determined by the processes and is\nnot under the operating system’s control. The processes are also responsible for\nensuring that they are not writing to the same location simultaneously. Such\nmechanisms are discussed in Chapter 5. In Chapter 4, we look at a variation of\nthe process scheme—threads—in which memory is shared by default.\nBoth of the models just discussed are common in operating systems,\nand most systems implement both. Message passing is useful for exchanging",
  "smaller amounts of data, because no conﬂicts need be avoided. It is also easier to\nimplement than is shared memory for intercomputer communication. Shared\nmemory allows maximum speed and convenience of communication, since it\ncan be done at memory transfer speeds when it takes place within a computer.\nProblems exist, however, in the areas of protection and synchronization\nbetween the processes sharing memory.\n2.4.6\nProtection\nProtection provides a mechanism for controlling access to the resources\nprovided by a computer system. Historically, protection was a concern only on\nmultiprogrammed computer systems with several users. However, with the\nadvent of networking and the Internet, all computer systems, from servers to\nmobile handheld devices, must be concerned with protection.",
  "Typically, system calls providing protection include set permission()\nand get permission(), which manipulate the permission settings of\nresources such as ﬁles and disks. The allow user() and deny user() system\ncalls specify whether particular users can—or cannot—be allowed access to\ncertain resources.\nWe cover protection in Chapter 14 and the much larger issue of security in\nChapter 15. 74\nChapter 2\nOperating-System Structures\n2.5\nSystem Programs\nAnother aspect of a modern system is its collection of system programs. Recall\nFigure 1.1, which depicted the logical computer hierarchy. At the lowest level is\nhardware. Next is the operating system, then the system programs, and ﬁnally\nthe application programs. System programs, also known as system utilities,",
  "provide a convenient environment for program development and execution.\nSome of them are simply user interfaces to system calls. Others are considerably\nmore complex. They can be divided into these categories:\n• File management. These programs create, delete, copy, rename, print,\ndump, list, and generally manipulate ﬁles and directories.\n• Status information. Some programs simply ask the system for the date,\ntime, amount of available memory or disk space, number of users, or\nsimilar status information. Others are more complex, providing detailed\nperformance, logging, and debugging information. Typically, these pro-\ngrams format and print the output to the terminal or other output devices\nor ﬁles or display it in a window of the GUI. Some systems also support a",
  "registry, which is used to store and retrieve conﬁguration information.\n• File modiﬁcation. Several text editors may be available to create and\nmodify the content of ﬁles stored on disk or other storage devices. There\nmay also be special commands to search contents of ﬁles or perform\ntransformations of the text.\n• Programming-language support. Compilers, assemblers, debuggers, and\ninterpreters for common programming languages (such as C, C++, Java,\nand PERL) are often provided with the operating system or available as a\nseparate download.\n• Program loading and execution. Once a program is assembled or com-\npiled, it must be loaded into memory to be executed. The system may\nprovide absolute loaders, relocatable loaders, linkage editors, and overlay",
  "loaders. Debugging systems for either higher-level languages or machine\nlanguage are needed as well.\n• Communications. These programs provide the mechanism for creating\nvirtual connections among processes, users, and computer systems. They\nallow users to send messages to one another’s screens, to browse Web\npages, to send e-mail messages, to log in remotely, or to transfer ﬁles from\none machine to another.\n• Background services. All general-purpose systems have methods for\nlaunching certain system-program processes at boot time. Some of these\nprocesses terminate after completing their tasks, while others continue\nto run until the system is halted. Constantly running system-program\nprocesses are known as services, subsystems, or daemons. One example is",
  "the network daemon discussed in Section 2.4.5. In that example, a system\nneeded a service to listen for network connections in order to connect\nthose requests to the correct processes. Other examples include process\nschedulers that start processes according to a speciﬁed schedule, system\nerror monitoring services, and print servers. Typical systems have dozens 2.6\nOperating-System Design and Implementation\n75\nof daemons. In addition, operating systems that run important activities\nin user context rather than in kernel context may use daemons to run these\nactivities.\nAlong with system programs, most operating systems are supplied with\nprograms that are useful in solving common problems or performing common\noperations. Such application programs include Web browsers, word proces-",
  "sors and text formatters, spreadsheets, database systems, compilers, plotting\nand statistical-analysis packages, and games.\nThe view of the operating system seen by most users is deﬁned by the\napplication and system programs, rather than by the actual system calls.\nConsider a user’s PC. When a user’s computer is running the Mac OS X\noperating system, the user might see the GUI, featuring a mouse-and-windows\ninterface. Alternatively, or even in one of the windows, the user might have a\ncommand-line UNIX shell. Both use the same set of system calls, but the system\ncalls look different and act in different ways. Further confusing the user view,\nconsider the user dual-booting from Mac OS X into Windows. Now the same",
  "user on the same hardware has two entirely different interfaces and two sets of\napplications using the same physical resources. On the same hardware, then,\na user can be exposed to multiple user interfaces sequentially or concurrently.\n2.6\nOperating-System Design and Implementation\nIn this section, we discuss problems we face in designing and implementing an\noperating system. There are, of course, no complete solutions to such problems,\nbut there are approaches that have proved successful.\n2.6.1\nDesign Goals\nThe ﬁrst problem in designing a system is to deﬁne goals and speciﬁcations.\nAt the highest level, the design of the system will be affected by the choice of\nhardware and the type of system: batch, time sharing, single user, multiuser,\ndistributed, real time, or general purpose.",
  "distributed, real time, or general purpose.\nBeyond this highest design level, the requirements may be much harder\nto specify. The requirements can, however, be divided into two basic groups:\nuser goals and system goals.\nUsers want certain obvious properties in a system. The system should be\nconvenient to use, easy to learn and to use, reliable, safe, and fast. Of course,\nthese speciﬁcations are not particularly useful in the system design, since there\nis no general agreement on how to achieve them.\nA similar set of requirements can be deﬁned by those people who must\ndesign, create, maintain, and operate the system. The system should be easy to\ndesign, implement, and maintain; and it should be ﬂexible, reliable, error free,",
  "and efﬁcient. Again, these requirements are vague and may be interpreted in\nvarious ways.\nThere is, in short, no unique solution to the problem of deﬁning the\nrequirements for an operating system. The wide range of systems in existence\nshows that different requirements can result in a large variety of solutions for\ndifferent environments. For example, the requirements for VxWorks, a real- 76\nChapter 2\nOperating-System Structures\ntime operating system for embedded systems, must have been substantially\ndifferent from those for MVS, a large multiuser, multiaccess operating system\nfor IBM mainframes.\nSpecifying and designing an operating system is a highly creative task.\nAlthough no textbook can tell you how to do it, general principles have",
  "been developed in the ﬁeld of software engineering, and we turn now to\na discussion of some of these principles.\n2.6.2\nMechanisms and Policies\nOne important principle is the separation of policy from mechanism. Mecha-\nnisms determine how to do something; policies determine what will be done.\nFor example, the timer construct (see Section 1.5.2) is a mechanism for ensuring\nCPU protection, but deciding how long the timer is to be set for a particular\nuser is a policy decision.\nThe separation of policy and mechanism is important for ﬂexibility. Policies\nare likely to change across places or over time. In the worst case, each change\nin policy would require a change in the underlying mechanism. A general\nmechanism insensitive to changes in policy would be more desirable. A change",
  "in policy would then require redeﬁnition of only certain parameters of the\nsystem. For instance, consider a mechanism for giving priority to certain types\nof programs over others. If the mechanism is properly separated from policy,\nit can be used either to support a policy decision that I/O-intensive programs\nshould have priority over CPU-intensive ones or to support the opposite policy.\nMicrokernel-based operating systems (Section 2.7.3) take the separation of\nmechanism and policy to one extreme by implementing a basic set of primitive\nbuilding blocks. These blocks are almost policy free, allowing more advanced\nmechanisms and policies to be added via user-created kernel modules or user\nprograms themselves. As an example, consider the history of UNIX. At ﬁrst,",
  "it had a time-sharing scheduler. In the latest version of Solaris, scheduling\nis controlled by loadable tables. Depending on the table currently loaded,\nthe system can be time sharing, batch processing, real time, fair share, or\nany combination. Making the scheduling mechanism general purpose allows\nvast policy changes to be made with a single load-new-table command. At\nthe other extreme is a system such as Windows, in which both mechanism\nand policy are encoded in the system to enforce a global look and feel. All\napplications have similar interfaces, because the interface itself is built into\nthe kernel and system libraries. The Mac OS X operating system has similar\nfunctionality.\nPolicy decisions are important for all resource allocation. Whenever it is",
  "necessary to decide whether or not to allocate a resource, a policy decision must\nbe made. Whenever the question is how rather than what, it is a mechanism\nthat must be determined.\n2.6.3\nImplementation\nOnce an operating system is designed, it must be implemented. Because\noperating systems are collections of many programs, written by many people\nover a long period of time, it is difﬁcult to make general statements about how\nthey are implemented. 2.6\nOperating-System Design and Implementation\n77\nEarly operating systems were written in assembly language. Now, although\nsome operating systems are still written in assembly language, most are written\nin a higher-level language such as C or an even higher-level language such as",
  "C++. Actually, an operating system can be written in more than one language.\nThe lowest levels of the kernel might be assembly language. Higher-level\nroutines might be in C, and system programs might be in C or C++, in\ninterpreted scripting languages like PERL or Python, or in shell scripts. In\nfact, a given Linux distribution probably includes programs written in all of\nthose languages.\nThe ﬁrst system that was not written in assembly language was probably\nthe Master Control Program (MCP) for Burroughs computers. MCP was written\nin a variant of ALGOL. MULTICS, developed at MIT, was written mainly in\nthe system programming language PL/1. The Linux and Windows operating\nsystem kernels are written mostly in C, although there are some small sections",
  "of assembly code for device drivers and for saving and restoring the state of\nregisters.\nThe advantages of using a higher-level language, or at least a systems-\nimplementation language, for implementing operating systems are the same\nas those gained when the language is used for application programs: the code\ncan be written faster, is more compact, and is easier to understand and debug.\nIn addition, improvements in compiler technology will improve the generated\ncode for the entire operating system by simple recompilation. Finally, an\noperating system is far easier to port—to move to some other hardware—\nif it is written in a higher-level language. For example, MS-DOS was written in\nIntel 8088 assembly language. Consequently, it runs natively only on the Intel",
  "X86 family of CPUs. (Note that although MS-DOS runs natively only on Intel\nX86, emulators of the X86 instruction set allow the operating system to run on\nother CPUs—but more slowly, and with higher resource use. As we mentioned\nin Chapter 1, emulators are programs that duplicate the functionality of one\nsystem on another system.) The Linux operating system, in contrast, is written\nmostly in C and is available natively on a number of different CPUs, including\nIntel X86, Oracle SPARC, and IBMPowerPC.\nThe only possible disadvantages of implementing an operating system in a\nhigher-level language are reduced speed and increased storage requirements.\nThis, however, is no longer a major issue in today’s systems. Although an\nexpert assembly-language programmer can produce efﬁcient small routines,",
  "for large programs a modern compiler can perform complex analysis and apply\nsophisticated optimizations that produce excellent code. Modern processors\nhave deep pipelining and multiple functional units that can handle the details\nof complex dependencies much more easily than can the human mind.\nAs is true in other systems, major performance improvements in oper-\nating systems are more likely to be the result of better data structures and\nalgorithms than of excellent assembly-language code. In addition, although\noperating systems are large, only a small amount of the code is critical to high\nperformance; the interrupt handler, I/O manager, memory manager, and CPU\nscheduler are probably the most critical routines. After the system is written",
  "and is working correctly, bottleneck routines can be identiﬁed and can be\nreplaced with assembly-language equivalents. 78\nChapter 2\nOperating-System Structures\n2.7\nOperating-System Structure\nA system as large and complex as a modern operating system must be\nengineered carefully if it is to function properly and be modiﬁed easily. A\ncommon approach is to partition the task into small components, or modules,\nrather than have one monolithic system. Each of these modules should be\na well-deﬁned portion of the system, with carefully deﬁned inputs, outputs,\nand functions. We have already discussed brieﬂy in Chapter 1 the common\ncomponents of operating systems. In this section, we discuss how these\ncomponents are interconnected and melded into a kernel.\n2.7.1\nSimple Structure",
  "2.7.1\nSimple Structure\nMany operating systems do not have well-deﬁned structures. Frequently, such\nsystems started as small, simple, and limited systems and then grew beyond\ntheir original scope. MS-DOS is an example of such a system. It was originally\ndesigned and implemented by a few people who had no idea that it would\nbecome so popular. It was written to provide the most functionality in the\nleast space, so it was not carefully divided into modules. Figure 2.11 shows its\nstructure.\nIn MS-DOS, the interfaces and levels of functionality are not well separated.\nFor instance, application programs are able to access the basic I/O routines\nto write directly to the display and disk drives. Such freedom leaves MS-DOS\nvulnerable to errant (or malicious) programs, causing entire system crashes",
  "when user programs fail. Of course, MS-DOS was also limited by the hardware\nof its era. Because the Intel 8088 for which it was written provides no dual\nmode and no hardware protection, the designers of MS-DOS had no choice but\nto leave the base hardware accessible.\nAnother example of limited structuring is the original UNIX operating\nsystem. Like MS-DOS, UNIX initially was limited by hardware functionality. It\nconsists of two separable parts: the kernel and the system programs. The kernel\nROM BIOS device drivers\napplication program\nMS-DOS device drivers\nresident system program\nFigure 2.11\nMS-DOS layer structure. 2.7\nOperating-System Structure\n79\nkernel\n(the users)\nshells and commands\ncompilers and interpreters\nsystem libraries\nsystem-call interface to the kernel\nsignals terminal\nhandling",
  "signals terminal\nhandling\ncharacter I/O system\nterminal drivers\nfile system\nswapping block I/O\nsystem\ndisk and tape drivers\nCPU scheduling\npage replacement\ndemand paging\nvirtual memory\nkernel interface to the hardware\nterminal controllers\nterminals\ndevice controllers\ndisks and tapes\nmemory controllers\nphysical memory\nFigure 2.12\nTraditional UNIX system structure.\nis further separated into a series of interfaces and device drivers, which have\nbeen added and expanded over the years as UNIX has evolved. We can view the\ntraditional UNIX operating system as being layered to some extent, as shown in\nFigure 2.12. Everything below the system-call interface and above the physical\nhardware is the kernel. The kernel provides the ﬁle system, CPU scheduling,",
  "memory management, and other operating-system functions through system\ncalls. Taken in sum, that is an enormous amount of functionality to be combined\ninto one level. This monolithic structure was difﬁcult to implement and\nmaintain. It had a distinct performance advantage, however: there is very little\noverhead in the system call interface or in communication within the kernel.\nWe still see evidence of this simple, monolithic structure in the UNIX, Linux,\nand Windows operating systems.\n2.7.2\nLayered Approach\nWith proper hardware support, operating systems can be broken into pieces\nthat are smaller and more appropriate than those allowed by the original\nMS-DOS and UNIX systems. The operating system can then retain much greater",
  "control over the computer and over the applications that make use of that\ncomputer. Implementers have more freedom in changing the inner workings\nof the system and in creating modular operating systems. Under a top-\ndown approach, the overall functionality and features are determined and\nare separated into components. Information hiding is also important, because\nit leaves programmers free to implement the low-level routines as they see ﬁt,\nprovided that the external interface of the routine stays unchanged and that\nthe routine itself performs the advertised task.\nA system can be made modular in many ways. One method is the layered\napproach, in which the operating system is broken into a number of layers\n(levels). The bottom layer (layer 0) is the hardware; the highest (layer N) is the",
  "user interface. This layering structure is depicted in Figure 2.13. 80\nChapter 2\nOperating-System Structures\nlayer N\nuser interface\n•••\nlayer 1\nlayer 0\nhardware\nFigure 2.13\nA layered operating system.\nAn operating-system layer is an implementation of an abstract object made\nup of data and the operations that can manipulate those data. A typical\noperating-system layer—say, layer M—consists of data structures and a set\nof routines that can be invoked by higher-level layers. Layer M, in turn, can\ninvoke operations on lower-level layers.\nThe main advantage of the layered approach is simplicity of construction\nand debugging. The layers are selected so that each uses functions (operations)\nand services of only lower-level layers. This approach simpliﬁes debugging",
  "and system veriﬁcation. The ﬁrst layer can be debugged without any concern\nfor the rest of the system, because, by deﬁnition, it uses only the basic hardware\n(which is assumed correct) to implement its functions. Once the ﬁrst layer is\ndebugged, its correct functioning can be assumed while the second layer is\ndebugged, and so on. If an error is found during the debugging of a particular\nlayer, the error must be on that layer, because the layers below it are already\ndebugged. Thus, the design and implementation of the system are simpliﬁed.\nEach layer is implemented only with operations provided by lower-level\nlayers. A layer does not need to know how these operations are implemented;\nit needs to know only what these operations do. Hence, each layer hides the",
  "existence of certain data structures, operations, and hardware from higher-level\nlayers.\nThe major difﬁculty with the layered approach involves appropriately\ndeﬁning the various layers. Because a layer can use only lower-level layers,\ncareful planning is necessary. For example, the device driver for the backing\nstore (disk space used by virtual-memory algorithms) must be at a lower\nlevel than the memory-management routines, because memory management\nrequires the ability to use the backing store.\nOther requirements may not be so obvious. The backing-store driver would\nnormally be above the CPU scheduler, because the driver may need to wait for\nI/O and the CPU can be rescheduled during this time. However, on a large 2.7\nOperating-System Structure\n81",
  "Operating-System Structure\n81\nsystem, the CPU scheduler may have more information about all the active\nprocesses than can ﬁt in memory. Therefore, this information may need to be\nswapped in and out of memory, requiring the backing-store driver routine to\nbe below the CPU scheduler.\nA ﬁnal problem with layered implementations is that they tend to be less\nefﬁcient than other types. For instance, when a user program executes an I/O\noperation, it executes a system call that is trapped to the I/O layer, which calls\nthe memory-management layer, which in turn calls the CPU-scheduling layer,\nwhich is then passed to the hardware. At each layer, the parameters may be\nmodiﬁed, data may need to be passed, and so on. Each layer adds overhead to",
  "the system call. The net result is a system call that takes longer than does one\non a nonlayered system.\nThese limitations have caused a small backlash against layering in recent\nyears. Fewer layers with more functionality are being designed, providing\nmost of the advantages of modularized code while avoiding the problems of\nlayer deﬁnition and interaction.\n2.7.3\nMicrokernels\nWe have already seen that as UNIX expanded, the kernel became large\nand difﬁcult to manage. In the mid-1980s, researchers at Carnegie Mellon\nUniversity developed an operating system called Mach that modularized\nthe kernel using the microkernel approach. This method structures the\noperating system by removing all nonessential components from the kernel and",
  "implementing them as system and user-level programs. The result is a smaller\nkernel. There is little consensus regarding which services should remain in the\nkernel and which should be implemented in user space. Typically, however,\nmicrokernels provide minimal process and memory management, in addition\nto a communication facility. Figure 2.14 illustrates the architecture of a typical\nmicrokernel.\nThe main function of the microkernel is to provide communication between\nthe client program and the various services that are also running in user space.\nCommunication is provided through message passing, which was described\nin Section 2.4.5. For example, if the client program wishes to access a ﬁle, it\nApplication\nProgram\nFile\nSystem\nDevice\nDriver\nInterprocess\nCommunication\nmemory\nmanagment\nCPU",
  "Interprocess\nCommunication\nmemory\nmanagment\nCPU\nscheduling\nmessages\nmessages\nmicrokernel\nhardware\nuser\nmode\nkernel\nmode\nFigure 2.14\nArchitecture of a typical microkernel. 82\nChapter 2\nOperating-System Structures\nmust interact with the ﬁle server. The client program and service never interact\ndirectly. Rather, they communicate indirectly by exchanging messages with the\nmicrokernel.\nOne beneﬁt of the microkernel approach is that it makes extending\nthe operating system easier. All new services are added to user space and\nconsequently do not require modiﬁcation of the kernel. When the kernel does\nhave to be modiﬁed, the changes tend to be fewer, because the microkernel is\na smaller kernel. The resulting operating system is easier to port from one",
  "hardware design to another. The microkernel also provides more security\nand reliability, since most services are running as user—rather than kernel—\nprocesses. If a service fails, the rest of the operating system remains untouched.\nSome contemporary operating systems have used the microkernel\napproach. Tru64 UNIX (formerly Digital UNIX) provides a UNIX interface to the\nuser, but it is implemented with a Mach kernel. The Mach kernel maps UNIX\nsystem calls into messages to the appropriate user-level services. The Mac OS X\nkernel (also known as Darwin) is also partly based on the Mach microkernel.\nAnother example is QNX, a real-time operating system for embedded\nsystems. The QNX Neutrino microkernel provides services for message passing",
  "and process scheduling. It also handles low-level network communication\nand hardware interrupts. All other services in QNX are provided by standard\nprocesses that run outside the kernel in user mode.\nUnfortunately, the performance of microkernels can suffer due to increased\nsystem-function overhead. Consider the history of Windows NT. The ﬁrst\nrelease had a layered microkernel organization. This version’s performance\nwas low compared with that of Windows 95. Windows NT 4.0 partially\ncorrected the performance problem by moving layers from user space to\nkernel space and integrating them more closely. By the time Windows XP\nwas designed, Windows architecture had become more monolithic than\nmicrokernel.\n2.7.4\nModules\nPerhaps the best current methodology for operating-system design involves",
  "using loadable kernel modules. Here, the kernel has a set of core components\nand links in additional services via modules, either at boot time or during run\ntime. This type of design is common in modern implementations of UNIX, such\nas Solaris, Linux, and Mac OS X, as well as Windows.\nThe idea of the design is for the kernel to provide core services while\nother services are implemented dynamically, as the kernel is running. Linking\nservices dynamically is preferable to adding new features directly to the kernel,\nwhich would require recompiling the kernel every time a change was made.\nThus, for example, we might build CPU scheduling and memory management\nalgorithms directly into the kernel and then add support for different ﬁle\nsystems by way of loadable modules.",
  "systems by way of loadable modules.\nThe overall result resembles a layered system in that each kernel section\nhas deﬁned, protected interfaces; but it is more ﬂexible than a layered system,\nbecause any module can call any other module. The approach is also similar to\nthe microkernel approach in that the primary module has only core functions\nand knowledge of how to load and communicate with other modules; but it 2.7\nOperating-System Structure\n83\ncore Solaris\nkernel\nfile systems\nloadable\nsystem calls\nexecutable\nformats\nSTREAMS\nmodules\nmiscellaneous\nmodules\ndevice and\nbus drivers\nscheduling\nclasses\nFigure 2.15\nSolaris loadable modules.\nis more efﬁcient, because modules do not need to invoke message passing in\norder to communicate.",
  "order to communicate.\nThe Solaris operating system structure, shown in Figure 2.15, is organized\naround a core kernel with seven types of loadable kernel modules:\n1. Scheduling classes\n2. File systems\n3. Loadable system calls\n4. Executable formats\n5.\nSTREAMS modules\n6. Miscellaneous\n7. Device and bus drivers\nLinux also uses loadable kernel modules, primarily for supporting device\ndrivers and ﬁle systems. We cover creating loadable kernel modules in Linux\nas a programming exercise at the end of this chapter.\n2.7.5\nHybrid Systems\nIn practice, very few operating systems adopt a single, strictly deﬁned\nstructure. Instead, they combine different structures, resulting in hybrid\nsystems that address performance, security, and usability issues. For example,",
  "both Linux and Solaris are monolithic, because having the operating system\nin a single address space provides very efﬁcient performance. However,\nthey are also modular, so that new functionality can be dynamically added\nto the kernel. Windows is largely monolithic as well (again primarily for\nperformance reasons), but it retains some behavior typical of microkernel\nsystems, including providing support for separate subsystems (known as\noperating-system personalities) that run as user-mode processes. Windows\nsystems also provide support for dynamically loadable kernel modules. We\nprovide case studies of Linux and Windows 7 in in Chapters 18 and 19,\nrespectively. In the remainder of this section, we explore the structure of 84\nChapter 2\nOperating-System Structures",
  "Chapter 2\nOperating-System Structures\nthree hybrid systems: the Apple Mac OS X operating system and the two most\nprominent mobile operating systems—iOS and Android.\n2.7.5.1\nMac OS X\nThe Apple Mac OS X operating system uses a hybrid structure. As shown in\nFigure 2.16, it is a layered system. The top layers include the Aqua user interface\n(Figure 2.4) and a set of application environments and services. Notably,\nthe Cocoa environment speciﬁes an API for the Objective-C programming\nlanguage, which is used for writing Mac OS X applications. Below these\nlayers is the kernel environment, which consists primarily of the Mach\nmicrokernel and the BSD UNIX kernel. Mach provides memory management;\nsupport for remote procedure calls (RPCs) and interprocess communication",
  "(IPC) facilities, including message passing; and thread scheduling. The BSD\ncomponent provides a BSD command-line interface, support for networking\nand ﬁle systems, and an implementation of POSIX APIs, including Pthreads.\nIn addition to Mach and BSD, the kernel environment provides an I/O kit\nfor development of device drivers and dynamically loadable modules (which\nMac OS X refers to as kernel extensions). As shown in Figure 2.16, the BSD\napplication environment can make use of BSD facilities directly.\n2.7.5.2\niOS\niOS is a mobile operating system designed by Apple to run its smartphone, the\niPhone, as well as its tablet computer, the iPad. iOS is structured on the Mac\nOS X operating system, with added functionality pertinent to mobile devices,",
  "but does not directly run Mac OS X applications. The structure of iOS appears\nin Figure 2.17.\nCocoa Touch is an API for Objective-C that provides several frameworks for\ndeveloping applications that run on iOS devices. The fundamental difference\nbetween Cocoa, mentioned earlier, and Cocoa Touch is that the latter provides\nsupport for hardware features unique to mobile devices, such as touch screens.\nThe media services layer provides services for graphics, audio, and video.\ngraphical user interface\nAqua\napplication environments and services\nkernel environment\nJava\nCocoa\nQuicktime\nBSD\nMach\nI/O kit\nkernel extensions\nBSD\nFigure 2.16\nThe Mac OS X structure. 2.7\nOperating-System Structure\n85\nCocoa Touch\nMedia Services\nCore Services\nCore OS\nFigure 2.17\nArchitecture of Apple’s iOS.",
  "Core OS\nFigure 2.17\nArchitecture of Apple’s iOS.\nThe core services layer provides a variety of features, including support for\ncloud computing and databases. The bottom layer represents the core operating\nsystem, which is based on the kernel environment shown in Figure 2.16.\n2.7.5.3\nAndroid\nThe Android operating system was designed by the Open Handset Alliance\n(led primarily by Google) and was developed for Android smartphones and\ntablet computers. Whereas iOS is designed to run on Apple mobile devices\nand is close-sourced, Android runs on a variety of mobile platforms and is\nopen-sourced, partly explaining its rapid rise in popularity. The structure of\nAndroid appears in Figure 2.18.\nAndroid is similar to iOS in that it is a layered stack of software that",
  "provides a rich set of frameworks for developing mobile applications. At the\nbottom of this software stack is the Linux kernel, although it has been modiﬁed\nby Google and is currently outside the normal distribution of Linux releases.\nApplications\nApplication Framework\nAndroid runtime\nCore Libraries\nDalvik\nvirtual machine\nLibraries\nLinux kernel\nSQLite\nopenGL\nsurface\nmanager\nwebkit\nlibc\nmedia\nframework\nFigure 2.18\nArchitecture of Google’s Android. 86\nChapter 2\nOperating-System Structures\nLinux is used primarily for process, memory, and device-driver support for\nhardware and has been expanded to include power management. The Android\nruntime environment includes a core set of libraries as well as the Dalvik virtual\nmachine. Software designers for Android devices develop applications in the",
  "Java language. However, rather than using the standard Java API, Google has\ndesigned a separate Android API for Java development. The Java class ﬁles are\nﬁrst compiled to Java bytecode and then translated into an executable ﬁle that\nruns on the Dalvik virtual machine. The Dalvik virtual machine was designed\nfor Android and is optimized for mobile devices with limited memory and\nCPU processing capabilities.\nThe set of libraries available for Android applications includes frameworks\nfor developing web browsers (webkit), database support (SQLite), and multi-\nmedia. The libc library is similar to the standard C library but is much smaller\nand has been designed for the slower CPUs that characterize mobile devices.\n2.8\nOperating-System Debugging",
  "2.8\nOperating-System Debugging\nWe have mentioned debugging frequently in this chapter. Here, we take a closer\nlook. Broadly, debugging is the activity of ﬁnding and ﬁxing errors in a system,\nboth in hardware and in software. Performance problems are considered bugs,\nso debugging can also include performance tuning, which seeks to improve\nperformance by removing processing bottlenecks. In this section, we explore\ndebugging process and kernel errors and performance problems. Hardware\ndebugging is outside the scope of this text.\n2.8.1\nFailure Analysis\nIf a process fails, most operating systems write the error information to a log\nﬁle to alert system operators or users that the problem occurred. The operating\nsystem can also take a core dump—a capture of the memory of the process—",
  "and store it in a ﬁle for later analysis. (Memory was referred to as the “core”\nin the early days of computing.) Running programs and core dumps can be\nprobed by a debugger, which allows a programmer to explore the code and\nmemory of a process.\nDebugging user-level process code is a challenge. Operating-system kernel\ndebugging is even more complex because of the size and complexity of the\nkernel, its control of the hardware, and the lack of user-level debugging tools.\nA failure in the kernel is called a crash. When a crash occurs, error information\nis saved to a log ﬁle, and the memory state is saved to a crash dump.\nOperating-system debugging and process debugging frequently use dif-\nferent tools and techniques due to the very different nature of these two tasks.",
  "Consider that a kernel failure in the ﬁle-system code would make it risky for\nthe kernel to try to save its state to a ﬁle on the ﬁle system before rebooting.\nA common technique is to save the kernel’s memory state to a section of disk\nset aside for this purpose that contains no ﬁle system. If the kernel detects\nan unrecoverable error, it writes the entire contents of memory, or at least the\nkernel-owned parts of the system memory, to the disk area. When the system\nreboots, a process runs to gather the data from that area and write it to a crash 2.8\nOperating-System Debugging\n87\nKernighan’s Law\n“Debugging is twice as hard as writing the code in the ﬁrst place. Therefore,\nif you write the code as cleverly as possible, you are, by deﬁnition, not smart\nenough to debug it.”",
  "enough to debug it.”\ndump ﬁle within a ﬁle system for analysis. Obviously, such strategies would\nbe unnecessary for debugging ordinary user-level processes.\n2.8.2\nPerformance Tuning\nWe mentioned earlier that performance tuning seeks to improve performance\nby removing processing bottlenecks. To identify bottlenecks, we must be able\nto monitor system performance. Thus, the operating system must have some\nmeans of computing and displaying measures of system behavior. In a number\nof systems, the operating system does this by producing trace listings of system\nbehavior. All interesting events are logged with their time and important\nparameters and are written to a ﬁle. Later, an analysis program can process\nthe log ﬁle to determine system performance and to identify bottlenecks and",
  "inefﬁciencies. These same traces can be run as input for a simulation of a\nsuggested improved system. Traces also can help people to ﬁnd errors in\noperating-system behavior.\nAnother approach to performance tuning uses single-purpose, interactive\ntools that allow users and administrators to question the state of various system\ncomponents to look for bottlenecks. One such tool employs the UNIX command\ntop to display the resources used on the system, as well as a sorted list of\nthe “top” resource-using processes. Other tools display the state of disk I/O,\nmemory allocation, and network trafﬁc.\nThe Windows Task Manager is a similar tool for Windows systems. The\ntaskmanagerincludesinformationforcurrentapplicationsaswell asprocesses,",
  "CPU and memory usage, and networking statistics. A screen shot of the task\nmanager appears in Figure 2.19.\nMaking operating systems easier to understand, debug, and tune as they\nrun is an active area of research and implementation. A new generation of\nkernel-enabled performance analysis tools has made signiﬁcant improvements\nin how this goal can be achieved. Next, we discuss a leading example of such\na tool: the Solaris 10 DTrace dynamic tracing facility.\n2.8.3\nDTrace\nDTrace is a facility that dynamically adds probes to a running system, both\nin user processes and in the kernel. These probes can be queried via the D\nprogramming language to determine an astonishing amount about the kernel,\nthe system state, and process activities. For example, Figure 2.20 follows an",
  "application as it executes a system call (ioctl()) and shows the functional\ncalls within the kernel as they execute to perform the system call. Lines ending\nwith “U” are executed in user mode, and lines ending in “K” in kernel mode. 88\nChapter 2\nOperating-System Structures\nFigure 2.19\nThe Windows task manager.\nDebugging the interactions between user-level and kernel code is nearly\nimpossible without a toolset that understands both sets of code and can\ninstrument the interactions. For that toolset to be truly useful, it must be able\nto debug any area of a system, including areas that were not written with\ndebugging in mind, and do so without affecting system reliability. This tool\nmust also have a minimum performance impact—ideally it should have no",
  "impact when not in use and a proportional impact during use. The DTrace tool\nmeets these requirements and provides a dynamic, safe, low-impact debugging\nenvironment.\nUntil the DTrace framework and tools became available with Solaris 10,\nkernel debugging was usually shrouded in mystery and accomplished via\nhappenstance and archaic code and tools. For example, CPUs have a breakpoint\nfeature that will halt execution and allow a debugger to examine the state of the\nsystem. Then execution can continue until the next breakpoint or termination.\nThis method cannot be used in a multiuser operating-system kernel without\nnegatively affecting all of the users on the system. Proﬁling, which periodically\nsamples the instruction pointer to determine which code is being executed, can",
  "show statistical trends but not individual activities. Code can be included in\nthe kernel to emit speciﬁc data under speciﬁc circumstances, but that code\nslows down the kernel and tends not to be included in the part of the kernel\nwhere the speciﬁc problem being debugged is occurring. 2.8\nOperating-System Debugging\n89\n# ./all.d ‘pgrep xclock‘ XEventsQueued\ndtrace: script ’./all.d’ matched 52377 probes\nCPU FUNCTION\n  0 –> XEventsQueued \n \n \nU\n  0   –> _XEventsQueued \n \n \nU\n  0     –> _X11TransBytesReadable  \nU\n  0     <– _X11TransBytesReadable  \nU\n  0     –> _X11TransSocketBytesReadable U\n  0     <– _X11TransSocketBytesreadable U\n  0     –> ioctl \n \n \n \nU\n  0       –> ioctl  \n \n \nK\n  0         –> getf  \n \n \nK\n  0           –> set_active_fd \n \nK\n  0           <– set_active_fd \n \nK",
  "K\n  0           <– set_active_fd \n \nK\n  0         <– getf  \n \n \nK\n  0         –> get_udatamodel \n \nK\n  0         <– get_udatamodel \n \nK\n...\n  0         –> releasef \n \n \nK\n  0           –> clear_active_fd  \nK\n  0           <– clear_active_fd  \nK\n  0           –> cv_broadcast \n \nK\n  0           <– cv_broadcast \n \nK\n  0         <– releasef \n \n \nK\n  0       <– ioctl  \n \n \nK\n  0     <– ioctl \n \n \n \nU\n  0   <– _XEventsQueued \n \n \nU\n  0 <– XEventsQueued \n \n \nU\nFigure 2.20\nSolaris 10 dtrace follows a system call within the kernel.\nIn contrast, DTrace runs on production systems—systems that are running\nimportant or critical applications—and causes no harm to the system. It\nslows activities while enabled, but after execution it resets the system to its",
  "pre-debugging state. It is also a broad and deep tool. It can broadly debug\neverything happening in the system (both at the user and kernel levels and\nbetween the user and kernel layers). It can also delve deep into code, showing\nindividual CPU instructions or kernel subroutine activities.\nDTrace is composed of a compiler, a framework, providers of probes\nwritten within that framework, and consumers of those probes. DTrace\nproviders create probes. Kernel structures exist to keep track of all probes that\nthe providers have created. The probes are stored in a hash-table data structure\nthat is hashed by name and indexed according to unique probe identiﬁers.\nWhen a probe is enabled, a bit of code in the area to be probed is rewritten",
  "to call dtrace probe(probe identifier) and then continue with the code’s\noriginal operation. Different providers create different kinds of probes. For\nexample, a kernel system-call probe works differently from a user-process\nprobe, and that is different from an I/O probe.\nDTrace features a compiler that generates a byte code that is run in the\nkernel. This code is assured to be “safe” by the compiler. For example, no loops\nare allowed, and only speciﬁc kernel state modiﬁcations are allowed when\nspeciﬁcally requested. Only users with DTrace “privileges” (or “root” users) 90\nChapter 2\nOperating-System Structures\nare allowed to use DTrace, as it can retrieve private kernel data (and modify\ndata if requested). The generated code runs in the kernel and enables probes.",
  "It also enables consumers in user mode and enables communications between\nthe two.\nA DTrace consumer is code that is interested in a probe and its results.\nA consumer requests that the provider create one or more probes. When a\nprobe ﬁres, it emits data that are managed by the kernel. Within the kernel,\nactions called enabling control blocks, or ECBs, are performed when probes\nﬁre. One probe can cause multiple ECBs to execute if more than one consumer\nis interested in that probe. Each ECB contains a predicate (“if statement”) that\ncan ﬁlter out that ECB. Otherwise, the list of actions in the ECB is executed. The\nmost common action is to capture some bit of data, such as a variable’s value at\nthat point of the probe execution. By gathering such data, a complete picture of",
  "a user or kernel action can be built. Further, probes ﬁring from both user space\nand the kernel can show how a user-level action caused kernel-level reactions.\nSuch data are invaluable for performance monitoring and code optimization.\nOnce the probe consumer terminates, its ECBs are removed. If there are no\nECBs consuming a probe, the probe is removed. That involves rewriting the\ncode to remove the dtrace probe() call and put back the original code. Thus,\nbefore a probe is created and after it is destroyed, the system is exactly the\nsame, as if no probing occurred.\nDTrace takes care to assure that probes do not use too much memory or\nCPU capacity, which could harm the running system. The buffers used to hold\nthe probe results are monitored for exceeding default and maximum limits.",
  "CPU time for probe execution is monitored as well. If limits are exceeded, the\nconsumer is terminated, along with the offending probes. Buffers are allocated\nper CPU to avoid contention and data loss.\nAn example of D code and its output shows some of its utility. The following\nprogram shows the DTrace code to enable scheduler probes and record the\namount of CPU time of each process running with user ID 101 while those\nprobes are enabled (that is, while the program runs):\nsched:::on-cpu\nuid == 101\n{\nself->ts = timestamp;\n}\nsched:::off-cpu\nself->ts\n{\n@time[execname] = sum(timestamp - self->ts);\nself->ts = 0;\n}\nThe output of the program, showing the processes and how much time (in\nnanoseconds) they spend running on the CPUs, is shown in Figure 2.21.",
  "Because DTrace is part of the open-source OpenSolaris version of the Solaris\n10 operating system, it has been added to other operating systems when those 2.9\nOperating-System Generation\n91\n# dtrace -s sched.d\ndtrace: script ’sched.d’ matched 6 probes\nˆC\ngnome-settings-d\n142354\ngnome-vfs-daemon\n158243\ndsdm\n189804\nwnck-applet\n200030\ngnome-panel\n277864\nclock-applet\n374916\nmapping-daemon\n385475\nxscreensaver\n514177\nmetacity\n539281\nXorg\n2579646\ngnome-terminal\n5007269\nmixer applet2\n7388447\njava\n10769137\nFigure 2.21\nOutput of the D code.\nsystems do not have conﬂicting license agreements. For example, DTrace has\nbeen added to Mac OS X and FreeBSD and will likely spread further due to its\nunique capabilities. Other operating systems, especially the Linux derivatives,",
  "are adding kernel-tracing functionality as well. Still other operating systems\nare beginning to include performance and tracing tools fostered by research at\nvarious institutions, including the Paradyn project.\n2.9\nOperating-System Generation\nIt is possible to design, code, and implement an operating system speciﬁcally\nfor one machine at one site. More commonly, however, operating systems\nare designed to run on any of a class of machines at a variety of sites with\na variety of peripheral conﬁgurations. The system must then be conﬁgured\nor generated for each speciﬁc computer site, a process sometimes known as\nsystem generation SYSGEN.\nThe operating system is normally distributed on disk, on CD-ROM or\nDVD-ROM, or as an “ISO” image, which is a ﬁle in the format of a CD-ROM",
  "or DVD-ROM. To generate a system, we use a special program. This SYSGEN\nprogram reads from a given ﬁle, or asks the operator of the system for\ninformation concerning the speciﬁc conﬁguration of the hardware system, or\nprobes the hardware directly to determine what components are there. The\nfollowing kinds of information must be determined.\n• What CPU is to be used? What options (extended instruction sets, ﬂoating-\npoint arithmetic, and so on) are installed? For multiple CPU systems, each\nCPU may be described.\n• How will the boot disk be formatted? How many sections, or “partitions,”\nwill it be separated into, and what will go into each partition? 92\nChapter 2\nOperating-System Structures\n• How much memory is available? Some systems will determine this value",
  "themselves by referencing memory location after memory location until an\n“illegal address” fault is generated. This procedure deﬁnes the ﬁnal legal\naddress and hence the amount of available memory.\n• What devices are available? The system will need to know how to address\neach device (the device number), the device interrupt number, the device’s\ntype and model, and any special device characteristics.\n• What operating-system options are desired, or what parameter values are\nto be used? These options or values might include how many buffers of\nwhich sizes should be used, what type of CPU-scheduling algorithm is\ndesired, what the maximum number of processes to be supported is, and\nso on.\nOnce this information is determined, it can be used in several ways. At one",
  "extreme, a system administrator can use it to modify a copy of the source code of\nthe operating system. The operating system then is completely compiled. Data\ndeclarations, initializations, and constants, along with conditional compilation,\nproduce an output-object version of the operating system that is tailored to the\nsystem described.\nAt a slightly less tailored level, the system description can lead to the\ncreation of tables and the selection of modules from a precompiled library.\nThese modules are linked together to form the generated operating system.\nSelection allows the library to contain the device drivers for all supported I/O\ndevices, but only those needed are linked into the operating system. Because\nthe system is not recompiled, system generation is faster, but the resulting",
  "system may be overly general.\nAt the other extreme, it is possible to construct a system that is completely\ntable driven. All the code is always part of the system, and selection occurs at\nexecution time, rather than at compile or link time. System generation involves\nsimply creating the appropriate tables to describe the system.\nThe major differences among these approaches are the size and generality\nof the generated system and the ease of modifying it as the hardware\nconﬁguration changes. Consider the cost of modifying the system to support a\nnewly acquired graphics terminal or another disk drive. Balanced against that\ncost, of course, is the frequency (or infrequency) of such changes.\n2.10 System Boot\nAfter an operating system is generated, it must be made available for use by",
  "the hardware. But how does the hardware know where the kernel is or how to\nload that kernel? The procedure of starting a computer by loading the kernel\nis known as booting the system. On most computer systems, a small piece of\ncode known as the bootstrap program or bootstrap loader locates the kernel,\nloads it into main memory, and starts its execution. Some computer systems,\nsuch as PCs, use a two-step process in which a simple bootstrap loader fetches\na more complex boot program from disk, which in turn loads the kernel.\nWhen a CPU receives a reset event—for instance, when it is powered up\nor rebooted—the instruction register is loaded with a predeﬁned memory 2.11\nSummary\n93\nlocation, and execution starts there. At that location is the initial bootstrap",
  "program. This program is in the form of read-only memory (ROM), because\nthe RAM is in an unknown state at system startup. ROM is convenient because\nit needs no initialization and cannot easily be infected by a computer virus.\nThe bootstrap program can perform a variety of tasks. Usually, one task\nis to run diagnostics to determine the state of the machine. If the diagnostics\npass, the program can continue with the booting steps. It can also initialize all\naspects of the system, from CPU registers to device controllers and the contents\nof main memory. Sooner or later, it starts the operating system.\nSome systems—such as cellular phones, tablets, and game consoles—store\nthe entire operating system in ROM. Storing the operating system in ROM is",
  "suitable for small operating systems, simple supporting hardware, and rugged\noperation. A problem with this approach is that changing the bootstrap code\nrequires changing the ROM hardware chips. Some systems resolve this problem\nby using erasable programmable read-only memory (EPROM), which is read-\nonly except when explicitly given a command to become writable. All forms\nof ROM are also known as ﬁrmware, since their characteristics fall somewhere\nbetween those of hardware and those of software. A problem with ﬁrmware\nin general is that executing code there is slower than executing code in RAM.\nSome systems store the operating system in ﬁrmware and copy it to RAM for\nfast execution. A ﬁnal issue with ﬁrmware is that it is relatively expensive, so\nusually only small amounts are available.",
  "usually only small amounts are available.\nFor large operating systems (including most general-purpose operating\nsystems like Windows, Mac OS X, and UNIX) or for systems that change\nfrequently, the bootstrap loader is stored in ﬁrmware, and the operating system\nis on disk. In this case, the bootstrap runs diagnostics and has a bit of code\nthat can read a single block at a ﬁxed location (say block zero) from disk into\nmemory and execute the code from that boot block. The program stored in the\nboot block may be sophisticated enough to load the entire operating system\ninto memory and begin its execution. More typically, it is simple code (as it ﬁts\nin a single disk block) and knows only the address on disk and length of the",
  "remainder of the bootstrap program. GRUB is an example of an open-source\nbootstrap program for Linux systems. All of the disk-bound bootstrap, and the\noperating system itself, can be easily changed by writing new versions to disk.\nA disk that has a boot partition (more on that in Section 10.5.1) is called a boot\ndisk or system disk.\nNow that the full bootstrap program has been loaded, it can traverse the\nﬁle system to ﬁnd the operating system kernel, load it into memory, and start\nits execution. It is only at this point that the system is said to be running.\n2.11\nSummary\nOperating systems provide a number of services. At the lowest level, system\ncalls allow a running program to make requests from the operating system\ndirectly. At a higher level, the command interpreter or shell provides a",
  "mechanism for a user to issue a request without writing a program. Commands\nmay come from ﬁles during batch-mode execution or directly from a terminal\nor desktop GUI when in an interactive or time-shared mode. System programs\nare provided to satisfy many common user requests. 94\nChapter 2\nOperating-System Structures\nThe types of requests vary according to level. The system-call level must\nprovide the basic functions, such as process control and ﬁle and device\nmanipulation. Higher-level requests, satisﬁed by the command interpreter or\nsystem programs, are translated into a sequence of system calls. System services\ncan be classiﬁed into several categories: program control, status requests, and\nI/O requests. Program errors can be considered implicit requests for service.",
  "The design of a new operating system is a major task. It is important that\nthe goals of the system be well deﬁned before the design begins. The type of\nsystem desired is the foundation for choices among various algorithms and\nstrategies that will be needed.\nThroughout the entire design cycle, we must be careful to separate policy\ndecisions from implementation details (mechanisms). This separation allows\nmaximum ﬂexibility if policy decisions are to be changed later.\nOnce an operating system is designed, it must be implemented. Oper-\nating systems today are almost always written in a systems-implementation\nlanguage or in a higher-level language. This feature improves their implemen-\ntation, maintenance, and portability.\nA system as large and complex as a modern operating system must",
  "be engineered carefully. Modularity is important. Designing a system as a\nsequence of layers or using a microkernel is considered a good technique. Many\noperating systems now support dynamically loaded modules, which allow\nadding functionality to an operating system while it is executing. Generally,\noperating systems adopt a hybrid approach that combines several different\ntypes of structures.\nDebugging process and kernel failures can be accomplished through the\nuse of debuggers and other tools that analyze core dumps. Tools such as DTrace\nanalyze production systems to ﬁnd bottlenecks and understand other system\nbehavior.\nTo create an operating system for a particular machine conﬁguration, we\nmust perform system generation. For the computer system to begin running,",
  "the CPU must initialize and start executing the bootstrap program in ﬁrmware.\nThe bootstrap can execute the operating system directly if the operating system\nis also in the ﬁrmware, or it can complete a sequence in which it loads\nprogressively smarter programs from ﬁrmware and disk until the operating\nsystem itself is loaded into memory and executed.\nPractice Exercises\n2.1\nWhat is the purpose of system calls?\n2.2\nWhat are the ﬁve major activities of an operating system with regard to\nprocess management?\n2.3\nWhat are the three major activities of an operating system with regard\nto memory management?\n2.4\nWhat are the three major activities of an operating system with regard\nto secondary-storage management?\n2.5\nWhat is the purpose of the command interpreter? Why is it usually",
  "separate from the kernel? Exercises\n95\n2.6\nWhat system calls have to be executed by a command interpreter or shell\nin order to start a new process?\n2.7\nWhat is the purpose of system programs?\n2.8\nWhat is the main advantage of the layered approach to system design?\nWhat are the disadvantages of the layered approach?\n2.9\nList ﬁve services provided by an operating system, and explain how each\ncreates convenience for users. In which cases would it be impossible for\nuser-level programs to provide these services? Explain your answer.\n2.10\nWhy do some systems store the operating system in ﬁrmware, while\nothers store it on disk?\n2.11\nHow could a system be designed to allow a choice of operating systems\nfrom which to boot? What would the bootstrap program need to do?\nExercises\n2.12",
  "Exercises\n2.12\nThe services and functions provided by an operating system can be\ndivided into two main categories. Brieﬂy describe the two categories,\nand discuss how they differ.\n2.13\nDescribe three general methods for passing parameters to the operating\nsystem.\n2.14\nDescribe how you could obtain a statistical proﬁle of the amount of time\nspent by a program executing different sections of its code. Discuss the\nimportance of obtaining such a statistical proﬁle.\n2.15\nWhat are the ﬁve major activities of an operating system with regard to\nﬁle management?\n2.16\nWhat are the advantages and disadvantages of using the same system-\ncall interface for manipulating both ﬁles and devices?\n2.17\nWould it be possible for the user to develop a new command interpreter",
  "using the system-call interface provided by the operating system?\n2.18\nWhat are the two models of interprocess communication? What are the\nstrengths and weaknesses of the two approaches?\n2.19\nWhy is the separation of mechanism and policy desirable?\n2.20\nIt is sometimes difﬁcult to achieve a layered approach if two components\nof the operating system are dependent on each other. Identify a scenario\nin which it is unclear how to layer two system components that require\ntight coupling of their functionalities.\n2.21\nWhat is the main advantage of the microkernel approach to system\ndesign? How do user programs and system services interact in a\nmicrokernel architecture? What are the disadvantages of using the\nmicrokernel approach?\n2.22\nWhat are the advantages of using loadable kernel modules? 96",
  "Chapter 2\nOperating-System Structures\n2.23\nHow are iOS and Android similar? How are they different?\n2.24\nExplain why Java programs running on Android systems do not use the\nstandard Java API and virtual machine.\n2.25\nThe experimental Synthesis operating system has an assembler incor-\nporated in the kernel. To optimize system-call performance, the kernel\nassembles routines within kernel space to minimize the path that the\nsystem call must take through the kernel. This approach is the antithesis\nof the layered approach, in which the path through the kernel is extended\nto make building the operating system easier. Discuss the pros and cons\nof the Synthesis approach to kernel design and system-performance\noptimization.\nProgramming Problems\n2.26",
  "optimization.\nProgramming Problems\n2.26\nIn Section 2.3, we described a program that copies the contents of one ﬁle\nto a destination ﬁle. This program works by ﬁrst prompting the user for\nthe name of the source and destination ﬁles. Write this program using\neither the Windows or POSIX API. Be sure to include all necessary error\nchecking, including ensuring that the source ﬁle exists.\nOnce you have correctly designed and tested the program, if you\nused a system that supports it, run the program using a utility that traces\nsystem calls. Linux systems provide the strace utility, and Solaris and\nMac OS X systems use the dtrace command. As Windows systems do\nnot provide such features, you will have to trace through the Windows\nversion of this program using a debugger.\nProgramming Projects",
  "Programming Projects\nLinux Kernel Modules\nIn this project, you will learn how to create a kernel module and load it into the\nLinux kernel. The project can be completed using the Linux virtual machine\nthat is available with this text. Although you may use an editor to write these\nC programs, you will have to use the terminal application to compile the\nprograms, and you will have to enter commands on the command line to\nmanage the modules in the kernel.\nAs you’ll discover, the advantage of developing kernel modules is that it\nis a relatively easy method of interacting with the kernel, thus allowing you to\nwrite programs that directly invoke kernel functions. It is important for you\nto keep in mind that you are indeed writing kernel code that directly interacts",
  "with the kernel. That normally means that any errors in the code could crash\nthe system! However, since you will be using a virtual machine, any failures\nwill at worst only require rebooting the system.\nPart I—Creating Kernel Modules\nThe ﬁrst part of this project involves following a series of steps for creating and\ninserting a module into the Linux kernel. Programming Projects\n97\nYou can list all kernel modules that are currently loaded by entering the\ncommand\nlsmod\nThis command will list the current kernel modules in three columns: name,\nsize, and where the module is being used.\nThe following program (named simple.c and available with the source\ncode for this text) illustrates a very basic kernel module that prints appropriate\nmessages when the kernel module is loaded and unloaded.",
  "#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n/* This function is called when the module is loaded. */\nint simple init(void)\n{\nprintk(KERN INFO \"Loading Module\\n\");\nreturn 0;\n}\n/* This function is called when the module is removed. */\nvoid simple exit(void)\n{\nprintk(KERN INFO \"Removing Module\\n\");\n}\n/* Macros for registering module entry and exit points. */\nmodule init(simple init);\nmodule exit(simple exit);\nMODULE LICENSE(\"GPL\");\nMODULE DESCRIPTION(\"Simple Module\");\nMODULE AUTHOR(\"SGG\");\nThe function simple init() is the module entry point, which represents\nthe function that is invoked when the module is loaded into the kernel.\nSimilarly, the simple exit() function is the module exit point—the function",
  "that is called when the module is removed from the kernel.\nThe module entry point function must return an integer value, with 0\nrepresenting success and any other value representing failure. The module exit\npoint function returns void. Neither the module entry point nor the module\nexit point is passed any parameters. The two following macros are used for\nregistering the module entry and exit points with the kernel:\nmodule init()\nmodule exit() 98\nChapter 2\nOperating-System Structures\nNotice how both the module entry and exit point functions make calls\nto the printk() function. printk() is the kernel equivalent of printf(),\nyet its output is sent to a kernel log buffer whose contents can be read by\nthe dmesg command. One difference between printf() and printk() is that",
  "printk() allows us to specify a priority ﬂag whose values are given in the\n<linux/printk.h> include ﬁle. In this instance, the priority is KERN INFO,\nwhich is deﬁned as an informational message.\nThe ﬁnal lines—MODULE LICENSE(), MODULE DESCRIPTION(), and MOD-\nULE AUTHOR()—represent details regarding the software license, description\nof the module, and author. For our purposes, we do not depend on this\ninformation, but we include it because it is standard practice in developing\nkernel modules.\nThis kernel module simple.c is compiled using the Makefile accom-\npanying the source code with this project. To compile the module, enter the\nfollowing on the command line:\nmake\nThe compilation produces several ﬁles. The ﬁle simple.ko represents the",
  "compiled kernel module. The following step illustrates inserting this module\ninto the Linux kernel.\nLoading and Removing Kernel Modules\nKernel modulesare loaded usingtheinsmodcommand, whichisrunasfollows:\nsudo insmod simple.ko\nTo check whether the module has loaded, enter the lsmod command and search\nfor the module simple. Recall that the module entry point is invoked when\nthe module is inserted into the kernel. To check the contents of this message in\nthe kernel log buffer, enter the command\ndmesg\nYou should see the message \"Loading Module.\"\nRemoving the kernel module involves invoking the rmmod command\n(notice that the .ko sufﬁx is unnecessary):\nsudo rmmod simple\nBe sure to check with the dmesg command to ensure the module has been\nremoved.",
  "removed.\nBecause the kernel log buffer can ﬁll up quickly, it often makes sense to\nclear the buffer periodically. This can be accomplished as follows:\nsudo dmesg -c Programming Projects\n99\nPart I Assignment\nProceed through the steps described above to create the kernel module and to\nload and unload the module. Be sure to check the contents of the kernel log\nbuffer using dmesg to ensure you have properly followed the steps.\nPart II—Kernel Data Structures\nThe second part of this project involves modifying the kernel module so that\nit uses the kernel linked-list data structure.\nIn Section 1.10, we covered various data structures that are common in\noperating systems. The Linux kernel provides several of these structures. Here,",
  "we explore using the circular, doubly linked list that is available to kernel\ndevelopers. Much of what we discuss is available in the Linux source code—\nin this instance, the include ﬁle <linux/list.h>—and we recommend that\nyou examine this ﬁle as you proceed through the following steps.\nInitially, you must deﬁne a struct containing the elements that are to be\ninserted in the linked list. The following C struct deﬁnes birthdays:\nstruct birthday {\nint day;\nint month;\nint year;\nstruct list head list;\n}\nNotice the member struct list head list. The list head structure is\ndeﬁned in the include ﬁle <linux/types.h>. Its intention is to embed the\nlinked list within the nodes that comprise the list. This list head structure is",
  "quite simple—it merely holds two members, next and prev, that point to the\nnext and previous entries in the list. By embedding the linked list within the\nstructure, Linux makes it possible to manage the data structure with a series of\nmacro functions.\nInserting Elements into the Linked List\nWe can declare a list head object, which we use as a reference to the head of\nthe list by using the LIST HEAD() macro\nstatic LIST HEAD(birthday list);\nThis macro deﬁnes and initializes the variable birthday list, which is of type\nstruct list head. 100\nChapter 2\nOperating-System Structures\nWe create and initialize instances of struct birthday as follows:\nstruct birthday *person;\nperson = kmalloc(sizeof(*person), GFP KERNEL);\nperson->day = 2;\nperson->month= 8;\nperson->year = 1995;",
  "person->month= 8;\nperson->year = 1995;\nINIT LIST HEAD(&person->list);\nThe kmalloc() function is the kernel equivalent of the user-level malloc()\nfunction for allocating memory, except that kernel memory is being allocated.\n(The GFP KERNEL ﬂag indicates routine kernel memory allocation.) The macro\nINIT LIST HEAD() initializes the list member in struct birthday. We can\nthen add this instance to the end of the linked list using the list add tail()\nmacro:\nlist add tail(&person->list, &birthday list);\nTraversing the Linked List\nTraversing the list involves using the list for each entry() Macro, which\naccepts three parameters:\n• A pointer to the structure being iterated over\n• A pointer to the head of the list being iterated over\n• The name of the variable containing the list head structure",
  "The following code illustrates this macro:\nstruct birthday *ptr;\nlist for each entry(ptr, &birthday list, list) {\n/* on each iteration ptr points */\n/* to the next birthday struct */\n}\nRemoving Elements from the Linked List\nRemoving elements from the list involves using the list del() macro, which\nis passed a pointer to struct list head\nlist del(struct list head *element)\nThis removes element from the list while maintaining the structure of the\nremainder of the list.\nPerhaps the simplest approach for removing all elements from a\nlinked list is to remove each element as you traverse the list. The macro\nlist for each entry safe() behaves much like list for each entry() Bibliographical Notes\n101\nexcept that it is passed an additional argument that maintains the value of the",
  "next pointer of the item being deleted. (This is necessary for preserving the\nstructure of the list.) The following code example illustrates this macro:\nstruct birthday *ptr, *next\nlist for each entry safe(ptr,next,&birthday list,list) {\n/* on each iteration ptr points */\n/* to the next birthday struct */\nlist del(&ptr->list);\nkfree(ptr);\n}\nNotice that after deleting each element, we return memory that was previously\nallocated with kmalloc() back to the kernel with the call to kfree(). Careful\nmemory management—which includes releasing memory to prevent memory\nleaks—is crucial when developing kernel-level code.\nPart II Assignment\nIn the module entry point, create a linked list containing ﬁve struct birthday",
  "elements. Traverse the linked list and output its contents to the kernel log buffer.\nInvoke the dmesg command to ensure the list is properly constructed once the\nkernel module has been loaded.\nIn the module exit point, delete the elements from the linked list and return\nthe free memory back to the kernel. Again, invoke the dmesg command to check\nthat the list has been removed once the kernel module has been unloaded.\nBibliographical Notes\n[Dijkstra (1968)] advocated the layered approach to operating-system design.\n[Brinch-Hansen (1970)] was an early proponent of constructing an operating\nsystem as a kernel (or nucleus) on which more complete systems could be\nbuilt. [Tarkoma and Lagerspetz (2011)] provide an overview of various mobile\noperating systems, including Android and iOS.",
  "operating systems, including Android and iOS.\nMS-DOS, Version 3.1, is described in [Microsoft (1986)]. Windows NT\nand Windows 2000 are described by [Solomon (1998)] and [Solomon and\nRussinovich (2000)]. Windows XP internals are described in [Russinovich\nand Solomon (2009)]. [Hart (2005)] covers Windows systems programming\nin detail. BSD UNIX is described in [McKusick et al. (1996)]. [Love (2010)] and\n[Mauerer (2008)] thoroughly discuss the Linux kernel. In particular, [Love\n(2010)] covers Linux kernel modules as well as kernel data structures. Several\nUNIX systems—including Mach—are treated in detail in [Vahalia (1996)]. Mac\nOS X is presented at http://www.apple.com/macosx and in [Singh (2007)].\nSolaris is fully described in [McDougall and Mauro (2007)].",
  "DTrace is discussed in [Gregg and Mauro (2011)]. The DTrace source code\nis available at http://src.opensolaris.org/source/. 102\nChapter 2\nOperating-System Structures\nBibliography\n[Brinch-Hansen (1970)]\nP. Brinch-Hansen, “The Nucleus of a Multiprogram-\nming System”, Communications of the ACM, Volume 13, Number 4 (1970), pages\n238–241 and 250.\n[Dijkstra (1968)]\nE. W. Dijkstra, “The Structure of the THE Multiprogramming\nSystem”, Communications of the ACM, Volume 11, Number 5 (1968), pages\n341–346.\n[Gregg and Mauro (2011)]\nB. Gregg and J. Mauro, DTrace—Dynamic Tracing in\nOracle Solaris, Mac OS X, and FreeBSD, Prentice Hall (2011).\n[Hart (2005)]\nJ. M. Hart, Windows System Programming, Third Edition, Addison-\nWesley (2005).\n[Love (2010)]",
  "Wesley (2005).\n[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[Mauerer (2008)]\nW. Mauerer, Professional Linux Kernel Architecture, John Wiley\nand Sons (2008).\n[McDougall and Mauro (2007)]\nR. McDougall and J. Mauro, Solaris Internals,\nSecond Edition, Prentice Hall (2007).\n[McKusick et al. (1996)]\nM. K. McKusick, K. Bostic, and M. J. Karels, The Design\nand Implementation of the 4.4 BSD UNIX Operating System, John Wiley and Sons\n(1996).\n[Microsoft (1986)]\nMicrosoft MS-DOS User’s Reference and Microsoft MS-DOS\nProgrammer’s Reference. Microsoft Press (1986).\n[Russinovich and Solomon (2009)]\nM. E. Russinovich and D. A. Solomon, Win-\ndows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition,\nMicrosoft Press (2009).\n[Singh (2007)]",
  "Microsoft Press (2009).\n[Singh (2007)]\nA. Singh, Mac OS X Internals: A Systems Approach, Addison-\nWesley (2007).\n[Solomon (1998)]\nD. A. Solomon, Inside Windows NT, Second Edition, Microsoft\nPress (1998).\n[Solomon and Russinovich (2000)]\nD. A. Solomon and M. E. Russinovich, Inside\nMicrosoft Windows 2000, Third Edition, Microsoft Press (2000).\n[Tarkoma and Lagerspetz (2011)]\nS. Tarkoma and E. Lagerspetz, “Arching over\nthe Mobile Computing Chasm: Platforms and Runtimes”, IEEE Computer,\nVolume 44, (2011), pages 22–28.\n[Vahalia (1996)]\nU. Vahalia, Unix Internals: The New Frontiers, Prentice Hall\n(1996). Part Two\nProcess\nManagement\nA process can be thought of as a program in execution. A process will\nneed certain resources—such as CPU time, memory, ﬁles, and I/O devices",
  "—to accomplish its task. These resources are allocated to the process\neither when it is created or while it is executing.\nA process is the unit of work in most systems. Systems consist of\na collection of processes: operating-system processes execute system\ncode, and user processes execute user code. All these processes may\nexecute concurrently.\nAlthough traditionally a process contained only a single thread of\ncontrol as it ran, most modern operating systems now support processes\nthat have multiple threads.\nThe operating system is responsible for several important aspects of\nprocess and thread management: the creation and deletion of both user\nand system processes; the scheduling of processes; and the provision of\nmechanisms for synchronization, communication, and deadlock handling",
  "for processes.  3\nC H A P T E R\nProcesses\nEarly computers allowed only one program to be executed at a time. This\nprogram had complete control of the system and had access to all the system’s\nresources. In contrast, contemporary computer systems allow multiple pro-\ngrams to be loaded into memory and executed concurrently. This evolution\nrequired ﬁrmer control and more compartmentalization of the various pro-\ngrams; and these needs resulted in the notion of a process, which is a program\nin execution. A process is the unit of work in a modern time-sharing system.\nThe more complex the operating system is, the more it is expected to do on\nbehalf of its users. Although its main concern is the execution of user programs,",
  "it also needs to take care of various system tasks that are better left outside the\nkernel itself. A system therefore consists of a collection of processes: operating-\nsystem processes executing system code and user processes executing user\ncode. Potentially, all these processes can execute concurrently, with the CPU (or\nCPUs) multiplexed among them. By switching the CPU between processes, the\noperating system can make the computer more productive. In this chapter, you\nwill read about what processes are and how they work.\nCHAPTER OBJECTIVES\n• To introduce the notion of a process—a program in execution, which forms\nthe basis of all computation.\n• To describe the various features of processes, including scheduling,\ncreation, and termination.",
  "creation, and termination.\n• To explore interprocess communication using shared memory and mes-\nsage passing.\n• To describe communication in client–server systems.\n3.1\nProcess Concept\nA question that arises in discussing operating systems involves what to call\nall the CPU activities. A batch system executes jobs, whereas a time-shared\n105 106\nChapter 3\nProcesses\nsystem has user programs, or tasks. Even on a single-user system, a user may\nbe able to run several programs at one time: a word processor, a Web browser,\nand an e-mail package. And even if a user can execute only one program at a\ntime, such as on an embedded device that does not support multitasking, the\noperating system may need to support its own internal programmed activities,",
  "such as memory management. In many respects, all these activities are similar,\nso we call all of them processes.\nThe terms job and process are used almost interchangeably in this text.\nAlthough we personally prefer the term process, much of operating-system\ntheory and terminology was developed during a time when the major activity\nof operating systems was job processing. It would be misleading to avoid\nthe use of commonly accepted terms that include the word job (such as job\nscheduling) simply because process has superseded job.\n3.1.1\nThe Process\nInformally, as mentioned earlier, a process is a program in execution. A process\nis more than the program code, which is sometimes known as the text section.\nIt also includes the current activity, as represented by the value of the program",
  "counter and the contents of the processor’s registers. A process generally also\nincludes the process stack, which contains temporary data (such as function\nparameters, return addresses, and local variables), and a data section, which\ncontains global variables. A process may also include a heap, which is memory\nthat is dynamically allocated during process run time. The structure of a process\nin memory is shown in Figure 3.1.\nWe emphasize that a program by itself is not a process. A program is a\npassive entity, such as a ﬁle containing a list of instructions stored on disk\n(often called an executable ﬁle). In contrast, a process is an active entity,\nwith a program counter specifying the next instruction to execute and a set",
  "of associated resources. A program becomes a process when an executable ﬁle\nis loaded into memory. Two common techniques for loading executable ﬁles\ntext\n0\nmax\ndata\nheap\nstack\nFigure 3.1\nProcess in memory. 3.1\nProcess Concept\n107\nare double-clicking an icon representing the executable ﬁle and entering the\nname of the executable ﬁle on the command line (as in prog.exe or a.out).\nAlthough two processes may be associated with the same program, they\nare nevertheless considered two separate execution sequences. For instance,\nseveral users may be running different copies of the mail program, or the same\nuser may invoke many copies of the web browser program. Each of these is a\nseparate process; and although the text sections are equivalent, the data, heap,",
  "and stack sections vary. It is also common to have a process that spawns many\nprocesses as it runs. We discuss such matters in Section 3.4.\nNote that a process itself can be an execution environment for other\ncode. The Java programming environment provides a good example. In most\ncircumstances, an executable Java program is executed within the Java virtual\nmachine (JVM). The JVM executes as a process that interprets the loaded Java\ncode and takes actions (via native machine instructions) on behalf of that code.\nFor example, to run the compiled Java program Program.class, we would\nenter\njava Program\nThe command java runs the JVM as an ordinary process, which in turns\nexecutes the Java program Program in the virtual machine. The concept is the",
  "same as simulation, except that the code, instead of being written for a different\ninstruction set, is written in the Java language.\n3.1.2\nProcess State\nAs a process executes, it changes state. The state of a process is deﬁned in part\nby the current activity of that process. A process may be in one of the following\nstates:\n• New. The process is being created.\n• Running. Instructions are being executed.\n• Waiting. The process is waiting for some event to occur (such as an I/O\ncompletion or reception of a signal).\n• Ready. The process is waiting to be assigned to a processor.\n• Terminated. The process has ﬁnished execution.\nThese names are arbitrary, and they vary across operating systems. The states\nthat they represent are found on all systems, however. Certain operating",
  "systems also more ﬁnely delineate process states. It is important to realize\nthat only one process can be running on any processor at any instant. Many\nprocesses may be ready and waiting, however. The state diagram corresponding\nto these states is presented in Figure 3.2.\n3.1.3\nProcess Control Block\nEach process is represented in the operating system by a process control block\n(PCB)—also called a task control block. A PCBis shown in Figure 3.3. It contains\nmany pieces of information associated with a speciﬁc process, including these: 108\nChapter 3\nProcesses\nnew\nterminated\nrunning\nready\nadmitted\ninterrupt\nscheduler dispatch\nI/O or event completion\nI/O or event wait\nexit\nwaiting\nFigure 3.2\nDiagram of process state.\n• Process state. The state may be new, ready, running, waiting, halted, and",
  "so on.\n• Program counter. The counter indicates the address of the next instruction\nto be executed for this process.\n• CPU registers. The registers vary in number and type, depending on\nthe computer architecture. They include accumulators, index registers,\nstack pointers, and general-purpose registers, plus any condition-code\ninformation. Along with the program counter, this state information must\nbe saved when an interrupt occurs, to allow the process to be continued\ncorrectly afterward (Figure 3.4).\n• CPU-scheduling information. This information includes a process priority,\npointers to scheduling queues, and any other scheduling parameters.\n(Chapter 6 describes process scheduling.)\n• Memory-management information. This information may include such",
  "items as the value of the base and limit registers and the page tables, or the\nsegment tables, depending on the memory system used by the operating\nsystem (Chapter 8).\nprocess state\nprocess number\nprogram counter\nmemory limits\nlist of open files\nregisters\n•\n•\n•\nFigure 3.3\nProcess control block (PCB). 3.1\nProcess Concept\n109\nprocess P0\nprocess P1\nsave state into PCB0\nsave state into PCB1\nreload state from PCB1\nreload state from PCB0\noperating system\nidle\nidle\nexecuting\nidle\nexecuting\nexecuting\ninterrupt or system call\ninterrupt or system call\n•\n•\n•\n•\n•\n•\nFigure 3.4\nDiagram showing CPU switch from process to process.\n• Accounting information. This information includes the amount of CPU\nand real time used, time limits, account numbers, job or process numbers,\nand so on.",
  "and so on.\n• I/O status information. This information includes the list of I/O devices\nallocated to the process, a list of open ﬁles, and so on.\nIn brief, the PCB simply serves as the repository for any information that may\nvary from process to process.\n3.1.4\nThreads\nThe process model discussed so far has implied that a process is a program that\nperforms a single thread of execution. For example, when a process is running\na word-processor program, a single thread of instructions is being executed.\nThis single thread of control allows the process to perform only one task at\na time. The user cannot simultaneously type in characters and run the spell\nchecker within the same process, for example. Most modern operating systems",
  "have extended the process concept to allow a process to have multiple threads\nof execution and thus to perform more than one task at a time. This feature\nis especially beneﬁcial on multicore systems, where multiple threads can run\nin parallel. On a system that supports threads, the PCB is expanded to include\ninformation for each thread. Other changes throughout the system are also\nneeded to support threads. Chapter 4 explores threads in detail. 110\nChapter 3\nProcesses\nPROCESS REPRESENTATION IN LINUX\nThe process control block in the Linux operating system is represented by\nthe C structure task struct, which is found in the <linux/sched.h>\ninclude ﬁle in the kernel source-code directory. This structure contains all the",
  "necessary information for representing a process, including the state of the\nprocess, scheduling and memory-management information, list of open ﬁles,\nand pointers to the process’s parent and a list of its children and siblings. (A\nprocess’s parent is the process that created it; its children are any processes\nthat it creates. Its siblings are children with the same parent process.) Some\nof these ﬁelds include:\nlong state; /* state of the process */\nstruct sched entity se; /* scheduling information */\nstruct task struct *parent; /* this process’s parent */\nstruct list head children; /* this process’s children */\nstruct files struct *files; /* list of open files */\nstruct mm struct *mm; /* address space of this process */",
  "For example, the state of a process is represented by the ﬁeld long state\nin this structure. Within the Linux kernel, all active processes are represented\nusing a doubly linked list of task struct. The kernel maintains a pointer—\ncurrent—to the process currently executing on the system, as shown below:\nstruct task_struct\nprocess information\n• \n• \n•\nstruct task_struct\nprocess information\n•\n•\n•\ncurrent\n(currently executing proccess)\nstruct task_struct\nprocess information\n•\n•\n•\n•  •  •\nAs an illustration of how the kernel might manipulate one of the ﬁelds in\nthe task struct for a speciﬁed process, let’s assume the system would like\nto change the state of the process currently running to the value new state.\nIf current is a pointer to the process currently executing, its state is changed",
  "with the following:\ncurrent->state = new state;\n3.2\nProcess Scheduling\nThe objective of multiprogramming is to have some process running at all\ntimes, to maximize CPU utilization. The objective of time sharing is to switch the\nCPU among processes so frequently that users can interact with each program 3.2\nProcess Scheduling\n111\nqueue header\nPCB7\nPCB3\nPCB5\nPCB14\nPCB6\nPCB2\nhead\nhead\nhead\nhead\nhead\nready\nqueue\ndisk \nunit 0\nterminal \nunit 0\nmag\ntape\nunit 0\nmag\ntape\nunit 1\ntail\nregisters\nregisters\ntail\ntail\ntail\ntail\n•\n•\n•\n•\n•\n•\n•\n•\n•\nFigure 3.5\nThe ready queue and various I/O device queues.\nwhile it is running. To meet these objectives, the process scheduler selects\nan available process (possibly from a set of several available processes) for",
  "program execution on the CPU. For a single-processor system, there will never\nbe more than one running process. If there are more processes, the rest will\nhave to wait until the CPU is free and can be rescheduled.\n3.2.1\nScheduling Queues\nAs processes enter the system, they are put into a job queue, which consists\nof all processes in the system. The processes that are residing in main memory\nand are ready and waiting to execute are kept on a list called the ready queue.\nThis queue is generally stored as a linked list. A ready-queue header contains\npointers to the ﬁrst and ﬁnal PCBs in the list. Each PCB includes a pointer ﬁeld\nthat points to the next PCB in the ready queue.\nThe system also includes other queues. When a process is allocated the",
  "CPU, it executes for a while and eventually quits, is interrupted, or waits for\nthe occurrence of a particular event, such as the completion of an I/O request.\nSuppose the process makes an I/O request to a shared device, such as a disk.\nSince there are many processes in the system, the disk may be busy with the\nI/O request of some other process. The process therefore may have to wait for\nthe disk. The list of processes waiting for a particular I/O device is called a\ndevice queue. Each device has its own device queue (Figure 3.5). 112\nChapter 3\nProcesses\nready queue\nCPU\nI/O\nI/O queue\nI/O request\ntime slice\nexpired\nfork a\nchild\nwait for an\ninterrupt\ninterrupt\noccurs\nchild\nexecutes\nFigure 3.6\nQueueing-diagram representation of process scheduling.",
  "A common representation of process scheduling is a queueing diagram,\nsuch as that in Figure 3.6. Each rectangular box represents a queue. Two types\nof queues are present: the ready queue and a set of device queues. The circles\nrepresent the resources that serve the queues, and the arrows indicate the ﬂow\nof processes in the system.\nA new process is initially put in the ready queue. It waits there until it is\nselected for execution, or dispatched. Once the process is allocated the CPU\nand is executing, one of several events could occur:\n• The process could issue an I/O request and then be placed in an I/O queue.\n• The process could create a new child process and wait for the child’s\ntermination.\n• The process could be removed forcibly from the CPU, as a result of an",
  "interrupt, and be put back in the ready queue.\nIn the ﬁrst two cases, the process eventually switches from the waiting state\nto the ready state and is then put back in the ready queue. A process continues\nthis cycle until it terminates, at which time it is removed from all queues and\nhas its PCB and resources deallocated.\n3.2.2\nSchedulers\nA process migrates among the various scheduling queues throughout its\nlifetime. The operating system must select, for scheduling purposes, processes\nfrom these queues in some fashion. The selection process is carried out by the\nappropriate scheduler.\nOften, in a batch system, more processes are submitted than can be executed\nimmediately. These processes are spooled to a mass-storage device (typically a",
  "disk), where they are kept for later execution. The long-term scheduler, or job\nscheduler, selects processes from this pool and loads them into memory for 3.2\nProcess Scheduling\n113\nexecution. The short-term scheduler, or CPU scheduler, selects from among\nthe processes that are ready to execute and allocates the CPU to one of them.\nThe primary distinction between these two schedulers lies in frequency\nof execution. The short-term scheduler must select a new process for the CPU\nfrequently. A process may execute for only a few milliseconds before waiting\nfor an I/O request. Often, the short-term scheduler executes at least once every\n100 milliseconds. Because of the short time between executions, the short-term",
  "scheduler must be fast. If it takes 10 milliseconds to decide to execute a process\nfor 100 milliseconds, then 10/(100 + 10) = 9 percent of the CPU is being used\n(wasted) simply for scheduling the work.\nThe long-term scheduler executes much less frequently; minutes may sep-\narate the creation of one new process and the next. The long-term scheduler\ncontrols the degree of multiprogramming (the number of processes in mem-\nory). If the degree of multiprogramming is stable, then the average rate of\nprocess creation must be equal to the average departure rate of processes\nleaving the system. Thus, the long-term scheduler may need to be invoked\nonly when a process leaves the system. Because of the longer interval between\nexecutions, the long-term scheduler can afford to take more time to decide",
  "which process should be selected for execution.\nIt is important that the long-term scheduler make a careful selection. In\ngeneral, most processes can be described as either I/O bound or CPU bound.\nAn I/O-bound process is one that spends more of its time doing I/O than\nit spends doing computations. A CPU-bound process, in contrast, generates\nI/O requests infrequently, using more of its time doing computations. It is\nimportant that the long-term scheduler select a good process mix of I/O-bound\nand CPU-bound processes. If all processes are I/O bound, the ready queue will\nalmost always be empty, and the short-term scheduler will have little to do.\nIf all processes are CPU bound, the I/O waiting queue will almost always be",
  "empty, devices will go unused, and again the system will be unbalanced. The\nsystem with the best performance will thus have a combination of CPU-bound\nand I/O-bound processes.\nOn some systems, the long-term scheduler may be absent or minimal.\nFor example, time-sharing systems such as UNIX and Microsoft Windows\nsystems often have no long-term scheduler but simply put every new process in\nmemory for the short-term scheduler. The stability of these systems depends\neither on a physical limitation (such as the number of available terminals)\nor on the self-adjusting nature of human users. If performance declines to\nunacceptable levels on a multiuser system, some users will simply quit.\nSome operating systems, such as time-sharing systems, may introduce an",
  "additional, intermediate level of scheduling. This medium-term scheduler is\ndiagrammed in Figure 3.7. The key idea behind a medium-term scheduler is\nthat sometimes it can be advantageous to remove a process from memory\n(and from active contention for the CPU) and thus reduce the degree of\nmultiprogramming. Later, the process can be reintroduced into memory, and its\nexecution can be continued where it left off. This scheme is called swapping.\nThe process is swapped out, and is later swapped in, by the medium-term\nscheduler. Swapping may be necessary to improve the process mix or because\na change in memory requirements has overcommitted available memory,\nrequiring memory to be freed up. Swapping is discussed in Chapter 8. 114\nChapter 3\nProcesses\nswap in\nswap out\nend\nCPU\nI/O\nI/O waiting",
  "swap in\nswap out\nend\nCPU\nI/O\nI/O waiting\nqueues\nready queue\npartially executed\nswapped-out processes\nFigure 3.7\nAddition of medium-term scheduling to the queueing diagram.\n3.2.3\nContext Switch\nAs mentioned in Section 1.2.1, interrupts cause the operating system to change\na CPU from its current task and to run a kernel routine. Such operations happen\nfrequently on general-purpose systems. When an interrupt occurs, the system\nneeds to save the current context of the process running on the CPU so that\nit can restore that context when its processing is done, essentially suspending\nthe process and then resuming it. The context is represented in the PCB of the\nprocess. It includes the value of the CPU registers, the process state (see Figure",
  "3.2), and memory-management information. Generically, we perform a state\nsave of the current state of the CPU, be it in kernel or user mode, and then a\nstate restore to resume operations.\nSwitching the CPU to another process requires performing a state save of\nthe current process and a state restore of a different process. This task is known\nas a context switch. When a context switch occurs, the kernel saves the context\nof the old process in its PCB and loads the saved context of the new process\nscheduled to run. Context-switch time is pure overhead, because the system\ndoes no useful work while switching. Switching speed varies from machine to\nmachine, depending on the memory speed, the number of registers that must",
  "be copied, and the existence of special instructions (such as a single instruction\nto load or store all registers). A typical speed is a few milliseconds.\nContext-switch times are highly dependent on hardware support. For\ninstance, some processors (such as the Sun UltraSPARC) provide multiple sets\nof registers. A context switch here simply requires changing the pointer to the\ncurrent register set. Of course, if there are more active processes than there are\nregister sets, the system resorts to copying register data to and from memory,\nas before. Also, the more complex the operating system, the greater the amount\nof work that must be done during a context switch. As we will see in Chapter\n8, advanced memory-management techniques may require that extra data be",
  "switched with each context. For instance, the address space of the current\nprocess must be preserved as the space of the next task is prepared for use.\nHow the address space is preserved, and what amount of work is needed\nto preserve it, depend on the memory-management method of the operating\nsystem. 3.3\nOperations on Processes\n115\nMULTITASKING IN MOBILE SYSTEMS\nBecause of the constraints imposed on mobile devices, early versions of iOS\ndid not provide user-application multitasking; only one application runs in\nthe foreground and all other user applications are suspended. Operating-\nsystem tasks were multitasked because they were written by Apple and well\nbehaved. However, beginning with iOS 4, Apple now provides a limited",
  "form of multitasking for user applications, thus allowing a single foreground\napplication to run concurrently with multiple background applications. (On\na mobile device, the foreground application is the application currently\nopen and appearing on the display. The background application remains\nin memory, but does not occupy the display screen.) The iOS 4 programming\nAPI provides support for multitasking, thus allowing a process to run in\nthe background without being suspended. However, it is limited and only\navailable for a limited number of application types, including applications\n• running a single, ﬁnite-length task (such as completing a download of\ncontent from a network);\n• receiving notiﬁcations of an event occurring (such as a new email\nmessage);",
  "message);\n• with long-running background tasks (such as an audio player.)\nApple probably limits multitasking due to battery life and memory use\nconcerns. The CPU certainly has the features to support multitasking, but\nApple chooses to not take advantage of some of them in order to better\nmanage resource use.\nAndroid does not place such constraints on the types of applications that\ncan run in the background. If an application requires processing while in\nthe background, the application must use a service, a separate application\ncomponent that runs on behalf of the background process. Consider a\nstreaming audio application: if the application moves to the background, the\nservice continues to send audio ﬁles to the audio device driver on behalf of",
  "the background application. In fact, the service will continue to run even if the\nbackground application is suspended. Services do not have a user interface\nand have a small memory footprint, thus providing an efﬁcient technique for\nmultitasking in a mobile environment.\n3.3\nOperations on Processes\nThe processes in most systems can execute concurrently, and they may\nbe created and deleted dynamically. Thus, these systems must provide a\nmechanism for process creation and termination. In this section, we explore\nthe mechanisms involved in creating processes and illustrate process creation\non UNIX and Windows systems. 116\nChapter 3\nProcesses\n3.3.1\nProcess Creation\nDuring the course of execution, a process may create several new processes. As",
  "mentioned earlier, the creating process is called a parent process, and the new\nprocesses are called the children of that process. Each of these new processes\nmay in turn create other processes, forming a tree of processes.\nMost operating systems (including UNIX, Linux, and Windows) identify\nprocesses according to a unique process identiﬁer (or pid), which is typically\nan integer number. The pid provides a unique value for each process in the\nsystem, and it can be used as an index to access various attributes of a process\nwithin the kernel.\nFigure 3.8 illustrates a typical process tree for the Linux operating system,\nshowing the name of each process and its pid. (We use the term process rather\nloosely, as Linux prefers the term task instead.) The init process (which always",
  "has a pid of 1) serves as the root parent process for all user processes. Once the\nsystem has booted, the init process can also create various user processes, such\nas a web or print server, an ssh server, and the like. In Figure 3.8, we see two\nchildren of init—kthreadd and sshd. The kthreadd process is responsible\nfor creating additional processes that perform tasks on behalf of the kernel\n(in this situation, khelper and pdflush). The sshd process is responsible for\nmanaging clients that connect to the system by using ssh (which is short for\nsecure shell). The login process is responsible for managing clients that directly\nlog onto the system. In this example, a client has logged on and is using the\nbash shell, which has been assigned pid 8416. Using the bash command-line",
  "interface, this user has created the process ps as well as the emacs editor.\nOn UNIX and Linux systems, we can obtain a listing of processes by using\nthe ps command. For example, the command\nps -el\nwill list complete information for all processes currently active in the system.\nIt is easy to construct a process tree similar to the one shown in Figure 3.8 by\nrecursively tracing parent processes all the way to the init process.\ninit\npid = 1\nsshd\npid = 3028\nlogin\npid = 8415\nkthreadd\npid = 2\nsshd\npid = 3610\npdflush\npid = 200\nkhelper\npid = 6\ntcsch\npid = 4005\nemacs\npid = 9204\nbash\npid = 8416\nps\npid = 9298\nFigure 3.8\nA tree of processes on a typical Linux system. 3.3\nOperations on Processes\n117\nIn general, when a process creates a child process, that child process will",
  "need certain resources (CPU time, memory, ﬁles, I/O devices) to accomplish\nits task. A child process may be able to obtain its resources directly from\nthe operating system, or it may be constrained to a subset of the resources\nof the parent process. The parent may have to partition its resources among\nits children, or it may be able to share some resources (such as memory or\nﬁles) among several of its children. Restricting a child process to a subset of\nthe parent’s resources prevents any process from overloading the system by\ncreating too many child processes.\nIn addition to supplying various physical and logical resources, the parent\nprocess may pass along initialization data (input) to the child process. For\nexample, consider a process whose function is to display the contents of a ﬁle",
  "—say, image.jpg—on the screen of a terminal. When the process is created,\nit will get, as an input from its parent process, the name of the ﬁle image.jpg.\nUsing that ﬁle name, it will open the ﬁle and write the contents out. It may\nalso get the name of the output device. Alternatively, some operating systems\npass resources to child processes. On such a system, the new process may get\ntwo open ﬁles, image.jpg and the terminal device, and may simply transfer\nthe datum between the two.\nWhen a process creates a new process, two possibilities for execution exist:\n1. The parent continues to execute concurrently with its children.\n2. The parent waits until some or all of its children have terminated.\nThere are also two address-space possibilities for the new process:",
  "1. The child process is a duplicate of the parent process (it has the same\nprogram and data as the parent).\n2. The child process has a new program loaded into it.\nTo illustrate these differences, let’s ﬁrst consider the UNIX operating system.\nIn UNIX, as we’ve seen, each process is identiﬁed by its process identiﬁer,\nwhich is a unique integer. A new process is created by the fork() system\ncall. The new process consists of a copy of the address space of the original\nprocess. This mechanism allows the parent process to communicate easily with\nits child process. Both processes (the parent and the child) continue execution\nat the instruction after the fork(), with one difference: the return code for\nthe fork() is zero for the new (child) process, whereas the (nonzero) process",
  "identiﬁer of the child is returned to the parent.\nAfter a fork() system call, one of the two processes typically uses the\nexec() system call to replace the process’s memory space with a new program.\nThe exec() system call loads a binary ﬁle into memory (destroying the\nmemory image of the program containing the exec() system call) and starts\nits execution. In this manner, the two processes are able to communicate and\nthen go their separate ways. The parent can then create more children; or, if it\nhas nothing else to do while the child runs, it can issue a wait() system call to\nmove itself off the ready queue until the termination of the child. Because the 118\nChapter 3\nProcesses\n#include <sys/types.h>\n#include <stdio.h>\n#include <unistd.h>\nint main()\n{\npid t pid;\n/* fork a child process */",
  "{\npid t pid;\n/* fork a child process */\npid = fork();\nif (pid < 0) { /* error occurred */\nfprintf(stderr, \"Fork Failed\");\nreturn 1;\n}\nelse if (pid == 0) { /* child process */\nexeclp(\"/bin/ls\",\"ls\",NULL);\n}\nelse { /* parent process */\n/* parent will wait for the child to complete */\nwait(NULL);\nprintf(\"Child Complete\");\n}\nreturn 0;\n}\nFigure 3.9\nCreating a separate process using the UNIX fork() system call.\ncall to exec() overlays the process’s address space with a new program, the\ncall to exec() does not return control unless an error occurs.\nThe C program shown in Figure 3.9 illustrates the UNIX system calls\npreviously described. We now have two different processes running copies\nof the same program. The only difference is that the value of pid (the process",
  "identiﬁer) for the child process is zero, while that for the parent is an integer\nvalue greater than zero (in fact, it is the actual pid of the child process). The\nchild process inherits privileges and scheduling attributes from the parent,\nas well certain resources, such as open ﬁles. The child process then overlays\nits address space with the UNIX command /bin/ls (used to get a directory\nlisting) using the execlp() system call (execlp() is a version of the exec()\nsystem call). The parent waits for the child process to complete with the wait()\nsystem call. When the child process completes (by either implicitly or explicitly\ninvoking exit()), the parent process resumes from the call to wait(), where it\ncompletes using the exit() system call. This is also illustrated in Figure 3.10.",
  "Of course, there is nothing to prevent the child from not invoking exec()\nand instead continuing to execute as a copy of the parent process. In this\nscenario, the parent and child are concurrent processes running the same code 3.3\nOperations on Processes\n119\npid = fork()\nexec()\nparent\nparent (pid > 0)\nchild (pid = 0)\nwait()\nexit()\nparent resumes\nFigure 3.10\nProcess creation using the fork() system call.\ninstructions. Because the child is a copy of the parent, each process has its own\ncopy of any data.\nAs an alternative example, we next consider process creation in Windows.\nProcesses are created in the Windows API using the CreateProcess() func-\ntion, which is similar to fork() in that a parent creates a new child process.",
  "However, whereas fork() has the child process inheriting the address space\nof its parent, CreateProcess() requires loading a speciﬁed program into the\naddress space of the child process at process creation. Furthermore, whereas\nfork() is passed no parameters, CreateProcess() expects no fewer than ten\nparameters.\nThe C program shown in Figure 3.11 illustrates the CreateProcess()\nfunction, which creates a child process that loads the application mspaint.exe.\nWe opt for many of the default values of the ten parameters passed to\nCreateProcess(). Readers interested in pursuing the details of process\ncreation and management in the Windows API are encouraged to consult the\nbibliographical notes at the end of this chapter.\nThe two parameters passed to the CreateProcess() function are instances",
  "of the\nSTARTUPINFO and PROCESS INFORMATION structures. STARTUPINFO\nspeciﬁes many properties of the new process, such as window size and\nappearance and handles to standard input and output ﬁles. The PRO-\nCESS INFORMATION structure contains a handle and the identiﬁers to the\nnewly created process and its thread. We invoke the ZeroMemory() func-\ntion to allocate memory for each of these structures before proceeding with\nCreateProcess().\nThe ﬁrst two parameters passed to CreateProcess() are the application\nname and command-line parameters. If the application name is NULL (as it is\nin this case), the command-line parameter speciﬁes the application to load. In\nthis instance, we are loading the Microsoft Windows mspaint.exe application.",
  "Beyond these two initial parameters, we use the default parameters for\ninheriting process and thread handles as well as specifying that there will be no\ncreation ﬂags. We also use the parent’s existing environment block and starting\ndirectory. Last, we provide two pointers to the STARTUPINFO and PROCESS -\nINFORMATION structures created at the beginning of the program. In Figure\n3.9, the parent process waits for the child to complete by invoking the wait()\nsystem call. The equivalent of this in Windows is WaitForSingleObject(),\nwhich is passed a handle of the child process—pi.hProcess—and waits for\nthis process to complete. Once the child process exits, control returns from the\nWaitForSingleObject() function in the parent process. 120\nChapter 3\nProcesses\n#include <stdio.h>",
  "Chapter 3\nProcesses\n#include <stdio.h>\n#include <windows.h>\nint main(VOID)\n{\nSTARTUPINFO si;\nPROCESS INFORMATION pi;\n/* allocate memory */\nZeroMemory(&si, sizeof(si));\nsi.cb = sizeof(si);\nZeroMemory(&pi, sizeof(pi));\n/* create child process */\nif (!CreateProcess(NULL, /* use command line */\n\"C:\\\\WINDOWS\\\\system32\\\\mspaint.exe\", /* command */\nNULL, /* don’t inherit process handle */\nNULL, /* don’t inherit thread handle */\nFALSE, /* disable handle inheritance */\n0, /* no creation flags */\nNULL, /* use parent’s environment block */\nNULL, /* use parent’s existing directory */\n&si,\n&pi))\n{\nfprintf(stderr, \"Create Process Failed\");\nreturn -1;\n}\n/* parent will wait for the child to complete */\nWaitForSingleObject(pi.hProcess, INFINITE);\nprintf(\"Child Complete\");\n/* close handles */",
  "printf(\"Child Complete\");\n/* close handles */\nCloseHandle(pi.hProcess);\nCloseHandle(pi.hThread);\n}\nFigure 3.11\nCreating a separate process using the Windows API.\n3.3.2\nProcess Termination\nA process terminates when it ﬁnishes executing its ﬁnal statement and asks the\noperating system to delete it by using the exit() system call. At that point, the\nprocess may return a status value (typically an integer) to its parent process\n(via the wait() system call). All the resources of the process—including\nphysical and virtual memory, open ﬁles, and I/O buffers—are deallocated\nby the operating system.\nTermination can occur in other circumstances as well. A process can cause\nthe termination of another process via an appropriate system call (for example,",
  "TerminateProcess() in Windows). Usually, such a system call can be invoked 3.3\nOperations on Processes\n121\nonly by the parent of the process that is to be terminated. Otherwise, users could\narbitrarily kill each other’s jobs. Note that a parent needs to know the identities\nof its children if it is to terminate them. Thus, when one process creates a new\nprocess, the identity of the newly created process is passed to the parent.\nA parent may terminate the execution of one of its children for a variety of\nreasons, such as these:\n• The child has exceeded its usage of some of the resources that it has been\nallocated. (To determine whether this has occurred, the parent must have\na mechanism to inspect the state of its children.)\n• The task assigned to the child is no longer required.",
  "• The parent is exiting, and the operating system does not allow a child to\ncontinue if its parent terminates.\nSome systems do not allow a child to exist if its parent has terminated. In\nsuch systems, if a process terminates (either normally or abnormally), then\nall its children must also be terminated. This phenomenon, referred to as\ncascading termination, is normally initiated by the operating system.\nTo illustrate process execution and termination, consider that, in Linux\nand UNIX systems, we can terminate a process by using the exit() system\ncall, providing an exit status as a parameter:\n/* exit with status 1 */\nexit(1);\nIn fact, under normal termination, exit() may be called either directly (as\nshown above) or indirectly (by a return statement in main()).",
  "A parent process may wait for the termination of a child process by using\nthe wait() system call. The wait() system call is passed a parameter that\nallows the parent to obtain the exit status of the child. This system call also\nreturns the process identiﬁer of the terminated child so that the parent can tell\nwhich of its children has terminated:\npid t pid;\nint status;\npid = wait(&status);\nWhen a process terminates, its resources are deallocated by the operating\nsystem. However, its entry in the process table must remain there until the\nparent calls wait(), because the process table contains the process’s exit status.\nA process that has terminated, but whose parent has not yet called wait(), is\nknown as a zombie process. All processes transition to this state when they",
  "terminate, but generally they exist as zombies only brieﬂy. Once the parent\ncalls wait(), the process identiﬁer of the zombie process and its entry in the\nprocess table are released.\nNow consider what would happen if a parent did not invoke wait() and\ninstead terminated, thereby leaving its child processes as orphans. Linux and\nUNIX address this scenario by assigning the init process as the new parent to 122\nChapter 3\nProcesses\norphan processes. (Recall from Figure 3.8 that the init process is the root of the\nprocess hierarchy in UNIX and Linux systems.) The init process periodically\ninvokes wait(), thereby allowing the exit status of any orphaned process to be\ncollected and releasing the orphan’s process identiﬁer and process-table entry.\n3.4\nInterprocess Communication",
  "3.4\nInterprocess Communication\nProcesses executing concurrently in the operating system may be either\nindependent processes or cooperating processes. A process is\nindependent\nif it cannot affect or be affected by the other processes executing in the system.\nAny process that does not share data with any other process is independent. A\nprocess is cooperating if it can affect or be affected by the other processes\nexecuting in the system. Clearly, any process that shares data with other\nprocesses is a cooperating process.\nThere are several reasons for providing an environment that allows process\ncooperation:\n• Information sharing. Since several users may be interested in the same\npiece of information (for instance, a shared ﬁle), we must provide an",
  "environment to allow concurrent access to such information.\n• Computation speedup. If we want a particular task to run faster, we must\nbreak it into subtasks, each of which will be executing in parallel with the\nothers. Notice that such a speedup can be achieved only if the computer\nhas multiple processing cores.\n• Modularity. We may want to construct the system in a modular fashion,\ndividing the system functions into separate processes or threads, as we\ndiscussed in Chapter 2.\n• Convenience. Even an individual user may work on many tasks at the\nsame time. For instance, a user may be editing, listening to music, and\ncompiling in parallel.\nCooperating processes require an interprocess communication (IPC) mech-\nanism that will allow them to exchange data and information. There are two",
  "fundamental models of interprocess communication: shared memory and mes-\nsage passing. In the shared-memory model, a region of memory that is shared\nby cooperating processes is established. Processes can then exchange informa-\ntion by reading and writing data to the shared region. In the message-passing\nmodel, communication takes place by means of messages exchanged between\nthe cooperating processes. The two communications models are contrasted in\nFigure 3.12.\nBoth of the models just mentioned are common in operating systems,\nand many systems implement both. Message passing is useful for exchanging\nsmaller amounts of data, because no conﬂicts need be avoided. Message\npassing is also easier to implement in a distributed system than shared memory.",
  "(Although there are systems that provide distributed shared memory, we do not\nconsider them in this text.) Shared memory can be faster than message passing,\nsince message-passing systems are typically implemented using system calls 3.4\nInterprocess Communication\n123\nMULTIPROCESS ARCHITECTURE—CHROME BROWSER\nMany websites contain active content such as JavaScript, Flash, and HTML5 to\nprovide a rich and dynamic web-browsing experience. Unfortunately, these\nweb applications may also contain software bugs, which can result in sluggish\nresponse times and can even cause the web browser to crash. This isn’t a big\nproblem in a web browser that displays content from only one website. But\nmost contemporary web browsers provide tabbed browsing, which allows a",
  "single instance of a web browser application to open several websites at the\nsame time, with each site in a separate tab. To switch between the different\nsites , a user need only click on the appropriate tab. This arrangement is\nillustrated below:\nA problem with this approach is that if a web application in any tab crashes,\nthe entire process—including all other tabs displaying additional websites\n—crashes as well.\nGoogle’s Chrome web browser was designed to address this issue by\nusing a multiprocess architecture. Chrome identiﬁes three different types of\nprocesses: browser, renderers, and plug-ins.\n• The browser process is responsible for managing the user interface as\nwell as disk and network I/O. A new browser process is created when",
  "Chrome is started. Only one browser process is created.\n• Renderer processes contain logic for rendering web pages. Thus, they\ncontain the logic for handling HTML, Javascript, images, and so forth. As\na general rule, a new renderer process is created for each website opened\nin a new tab, and so several renderer processes may be active at the same\ntime.\n• A plug-in process is created for each type of plug-in (such as Flash or\nQuickTime) in use. Plug-in processes contain the code for the plug-in as\nwell as additional code that enables the plug-in to communicate with\nassociated renderer processes and the browser process.\nThe advantage of the multiprocess approach is that websites run in\nisolation from one another. If one website crashes, only its renderer process",
  "is affected; all other processes remain unharmed. Furthermore, renderer\nprocesses run in a sandbox, which means that access to disk and network\nI/O is restricted, minimizing the effects of any security exploits.\nand thus require the more time-consuming task of kernel intervention. In\nshared-memory systems, system calls are required only to establish shared- 124\nChapter 3\nProcesses\nprocess A\nmessage queue\nkernel\n(a)\n(b)\nprocess A\nshared memory\nkernel\nprocess B\nm0 m1 m2\n...\nm3\nmn\nprocess B\nFigure 3.12\nCommunications models. (a) Message passing. (b) Shared memory.\nmemory regions. Once shared memory is established, all accesses are treated\nas routine memory accesses, and no assistance from the kernel is required.\nRecent research on systems with several processing cores indicates that",
  "message passing provides better performance than shared memory on such\nsystems. Shared memory suffers from cache coherency issues, which arise\nbecause shared data migrate among the several caches. As the number of\nprocessing cores on systems increases, it is possible that we will see message\npassing as the preferred mechanism for IPC.\nIn the remainder of this section, we explore shared-memory and message-\npassing systems in more detail.\n3.4.1\nShared-Memory Systems\nInterprocess communication using shared memory requires communicating\nprocesses to establish a region of shared memory. Typically, a shared-memory\nregion resides in the address space of the process creating the shared-memory\nsegment. Other processes that wish to communicate using this shared-memory",
  "segment must attach it to their address space. Recall that, normally, the\noperating system tries to prevent one process from accessing another process’s\nmemory. Shared memory requires that two or more processes agree to remove\nthis restriction. They can then exchange information by reading and writing\ndata in the shared areas. The form of the data and the location are determined by\nthese processes and are not under the operating system’s control. The processes\nare also responsible for ensuring that they are not writing to the same location\nsimultaneously.\nTo illustrate the concept of cooperating processes, let’s consider the\nproducer–consumer problem, which is a common paradigm for cooperating\nprocesses. A producer process produces information that is consumed by a",
  "consumer process. For example, a compiler may produce assembly code that\nis consumed by an assembler. The assembler, in turn, may produce object\nmodules that are consumed by the loader. The producer–consumer problem 3.4\nInterprocess Communication\n125\nitem next produced;\nwhile (true) {\n/* produce an item in next produced */\nwhile (((in + 1) % BUFFER SIZE) == out)\n; /* do nothing */\nbuffer[in] = next produced;\nin = (in + 1) % BUFFER SIZE;\n}\nFigure 3.13\nThe producer process using shared memory.\nalso provides a useful metaphor for the client–server paradigm. We generally\nthink of a server as a producer and a client as a consumer. For example, a web\nserver produces (that is, provides) HTML ﬁles and images, which are consumed\n(that is, read) by the client web browser requesting the resource.",
  "One solution to the producer–consumer problem uses shared memory. To\nallow producer and consumer processes to run concurrently, we must have\navailable a buffer of items that can be ﬁlled by the producer and emptied by\nthe consumer. This buffer will reside in a region of memory that is shared by\nthe producer and consumer processes. A producer can produce one item while\nthe consumer is consuming another item. The producer and consumer must\nbe synchronized, so that the consumer does not try to consume an item that\nhas not yet been produced.\nTwo types of buffers can be used. The unbounded buffer places no practical\nlimit on the size of the buffer. The consumer may have to wait for new items,\nbut the producer can always produce new items. The bounded buffer assumes",
  "a ﬁxed buffer size. In this case, the consumer must wait if the buffer is empty,\nand the producer must wait if the buffer is full.\nLet’s look more closely at how the bounded buffer illustrates interprocess\ncommunication using shared memory. The following variables reside in a\nregion of memory shared by the producer and consumer processes:\n#define BUFFER SIZE 10\ntypedef struct {\n. . .\n}item;\nitem buffer[BUFFER SIZE];\nint in = 0;\nint out = 0;\nThe shared buffer is implemented as a circular array with two logical pointers:\nin and out. The variable in points to the next free position in the buffer; out\npoints to the ﬁrst full position in the buffer. The buffer is empty when in ==\nout; the buffer is full when ((in + 1) % BUFFER SIZE) == out.",
  "The code for the producer process is shown in Figure 3.13, and the code\nfor the consumer process is shown in Figure 3.14. The producer process has a 126\nChapter 3\nProcesses\nitem next consumed;\nwhile (true) {\nwhile (in == out)\n; /* do nothing */\nnext consumed = buffer[out];\nout = (out + 1) % BUFFER SIZE;\n/* consume the item in next consumed */\n}\nFigure 3.14\nThe consumer process using shared memory.\nlocal variable next produced in which the new item to be produced is stored.\nThe consumer process has a local variable next consumed in which the item\nto be consumed is stored.\nThis scheme allows at most BUFFER SIZE −1 items in the buffer at the\nsame time. We leave it as an exercise for you to provide a solution in which",
  "BUFFER SIZE items can be in the buffer at the same time. In Section 3.5.1, we\nillustrate the POSIX API for shared memory.\nOne issue this illustration does not address concerns the situation in which\nboth the producer process and the consumer process attempt to access the\nshared buffer concurrently. In Chapter 5, we discuss how synchronization\namong cooperating processes can be implemented effectively in a shared-\nmemory environment.\n3.4.2\nMessage-Passing Systems\nIn Section 3.4.1, we showed how cooperating processes can communicate in a\nshared-memory environment. The scheme requires that these processes share a\nregion of memory and that the code for accessing and manipulating the shared\nmemory be written explicitly by the application programmer. Another way to",
  "achieve the same effect is for the operating system to provide the means for\ncooperating processes to communicate with each other via a message-passing\nfacility.\nMessage passing provides a mechanism to allow processes to communicate\nand to synchronize their actions without sharing the same address space. It is\nparticularly useful in a distributed environment, where the communicating\nprocesses may reside on different computers connected by a network. For\nexample, an Internet chat program could be designed so that chat participants\ncommunicate with one another by exchanging messages.\nA message-passing facility provides at least two operations:\nsend(message)\nreceive(message)\nMessages sent by a process can be either ﬁxed or variable in size. If only",
  "ﬁxed-sized messages can be sent, the system-level implementation is straight-\nforward. This restriction, however, makes the task of programming more\ndifﬁcult. Conversely, variable-sized messages require a more complex system- 3.4\nInterprocess Communication\n127\nlevel implementation, but the programming task becomes simpler. This is a\ncommon kind of tradeoff seen throughout operating-system design.\nIf processes P and Q want to communicate, they must send messages to and\nreceive messages from each other: a communication link must exist between\nthem. This link can be implemented in a variety of ways. We are concerned here\nnot with the link’s physical implementation (such as shared memory, hardware\nbus, or network, which are covered in Chapter 17) but rather with its logical",
  "implementation. Here are several methods for logically implementing a link\nand the send()/receive() operations:\n• Direct or indirect communication\n• Synchronous or asynchronous communication\n• Automatic or explicit buffering\nWe look at issues related to each of these features next.\n3.4.2.1\nNaming\nProcesses that want to communicate must have a way to refer to each other.\nThey can use either direct or indirect communication.\nUnder direct communication, each process that wants to communicate\nmust explicitly name the recipient or sender of the communication. In this\nscheme, the send() and receive() primitives are deﬁned as:\n• send(P, message)—Send a message to process P.\n• receive(Q, message)—Receive a message from process Q.\nA communication link in this scheme has the following properties:",
  "• A link is established automatically between every pair of processes that\nwant to communicate. The processes need to know only each other’s\nidentity to communicate.\n• A link is associated with exactly two processes.\n• Between each pair of processes, there exists exactly one link.\nThis scheme exhibits symmetry in addressing; that is, both the sender\nprocess and the receiver process must name the other to communicate. A\nvariant of this scheme employs asymmetry in addressing. Here, only the sender\nnames the recipient; the recipient is not required to name the sender. In this\nscheme, the send() and receive() primitives are deﬁned as follows:\n• send(P, message)—Send a message to process P.\n• receive(id, message)—Receive a message from any process. The",
  "variable id is set to the name of the process with which communication\nhas taken place. 128\nChapter 3\nProcesses\nThe disadvantage in both of these schemes (symmetric and asymmetric)\nis the limited modularity of the resulting process deﬁnitions. Changing the\nidentiﬁer of a process may necessitate examining all other process deﬁnitions.\nAll references to the old identiﬁer must be found, so that they can be modiﬁed\nto the new identiﬁer. In general, any such hard-coding techniques, where\nidentiﬁersmustbe explicitlystated, are lessdesirable thantechniquesinvolving\nindirection, as described next.\nWith indirect communication, the messages are sent to and received from\nmailboxes, or ports. A mailbox can be viewed abstractly as an object into which",
  "messages can be placed by processes and from which messages can be removed.\nEach mailbox has a unique identiﬁcation. For example, POSIX message queues\nuse an integer value to identify a mailbox. A process can communicate with\nanother process via a number of different mailboxes, but two processes can\ncommunicate only if they have a shared mailbox. The send() and receive()\nprimitives are deﬁned as follows:\n• send(A, message)—Send a message to mailbox A.\n• receive(A, message)—Receive a message from mailbox A.\nIn this scheme, a communication link has the following properties:\n• A link is established between a pair of processes only if both members of\nthe pair have a shared mailbox.\n• A link may be associated with more than two processes.",
  "• Between each pair of communicating processes, a number of different links\nmay exist, with each link corresponding to one mailbox.\nNow suppose that processes P1, P2, and P3 all share mailbox A. Process\nP1 sends a message to A, while both P2 and P3 execute a receive() from A.\nWhich process will receive the message sent by P1? The answer depends on\nwhich of the following methods we choose:\n• Allow a link to be associated with two processes at most.\n• Allow at most one process at a time to execute a receive() operation.\n• Allow the system to select arbitrarily which process will receive the\nmessage (that is, either P2 or P3, but not both, will receive the message). The\nsystem may deﬁne an algorithm for selecting which process will receive the",
  "message (for example, round robin, where processes take turns receiving\nmessages). The system may identify the receiver to the sender.\nA mailbox may be owned either by a process or by the operating system.\nIf the mailbox is owned by a process (that is, the mailbox is part of the address\nspace of the process), then we distinguish between the owner (which can\nonly receive messages through this mailbox) and the user (which can only\nsend messages to the mailbox). Since each mailbox has a unique owner, there\ncan be no confusion about which process should receive a message sent to\nthis mailbox. When a process that owns a mailbox terminates, the mailbox 3.4\nInterprocess Communication\n129\ndisappears. Any process that subsequently sends a message to this mailbox",
  "must be notiﬁed that the mailbox no longer exists.\nIn contrast, a mailbox that is owned by the operating system has an\nexistence of its own. It is independent and is not attached to any particular\nprocess. The operating system then must provide a mechanism that allows a\nprocess to do the following:\n• Create a new mailbox.\n• Send and receive messages through the mailbox.\n• Delete a mailbox.\nThe process that creates a new mailbox is that mailbox’s owner by default.\nInitially, the owner is the only process that can receive messages through this\nmailbox. However, the ownership and receiving privilege may be passed to\nother processes through appropriate system calls. Of course, this provision\ncould result in multiple receivers for each mailbox.\n3.4.2.2\nSynchronization",
  "3.4.2.2\nSynchronization\nCommunication between processes takes place through calls to send() and\nreceive() primitives. There are different design options for implementing\neach primitive. Message passing may be either blocking or nonblocking—\nalso known as synchronous and asynchronous. (Throughout this text, you\nwill encounter the concepts of synchronous and asynchronous behavior in\nrelation to various operating-system algorithms.)\n• Blocking send. The sending process is blocked until the message is\nreceived by the receiving process or by the mailbox.\n• Nonblocking send. The sending process sends the message and resumes\noperation.\n• Blocking receive. The receiver blocks until a message is available.\n• Nonblocking receive. The receiver retrieves either a valid message or a\nnull.",
  "null.\nDifferent combinations of send() and receive() are possible. When both\nsend() and receive() are blocking, we have a rendezvous between the\nsender and the receiver. The solution to the producer–consumer problem\nbecomes trivial when we use blocking send() and receive() statements.\nThe producer merely invokes the blocking send() call and waits until the\nmessage is delivered to either the receiver or the mailbox. Likewise, when the\nconsumer invokes receive(), it blocks until a message is available. This is\nillustrated in Figures 3.15 and 3.16.\n3.4.2.3\nBuffering\nWhether communication is direct or indirect, messages exchanged by commu-\nnicating processes reside in a temporary queue. Basically, such queues can be\nimplemented in three ways: 130\nChapter 3\nProcesses\nmessage next produced;",
  "Chapter 3\nProcesses\nmessage next produced;\nwhile (true) {\n/* produce an item in next produced */\nsend(next produced);\n}\nFigure 3.15\nThe producer process using message passing.\n• Zero capacity. The queue has a maximum length of zero; thus, the link\ncannot have any messages waiting in it. In this case, the sender must block\nuntil the recipient receives the message.\n• Bounded capacity. The queue has ﬁnite length n; thus, at most n messages\ncan reside in it. If the queue is not full when a new message is sent, the\nmessage is placed in the queue (either the message is copied or a pointer\nto the message is kept), and the sender can continue execution without\nwaiting. The link’s capacity is ﬁnite, however. If the link is full, the sender\nmust block until space is available in the queue.",
  "must block until space is available in the queue.\n• Unbounded capacity. The queue’s length is potentially inﬁnite; thus, any\nnumber of messages can wait in it. The sender never blocks.\nThe zero-capacity case is sometimes referred to as a message system with no\nbuffering. The other cases are referred to as systems with automatic buffering.\n3.5\nExamples of IPC Systems\nIn this section, we explore three different IPC systems. We ﬁrst cover the POSIX\nAPI forshared memoryand thendiscussmessage passinginthe Machoperating\nsystem. We conclude with Windows, which interestingly uses shared memory\nas a mechanism for providing certain types of message passing.\n3.5.1\nAn Example: POSIX Shared Memory\nSeveral IPC mechanisms are available for POSIX systems, including shared",
  "memory and message passing. Here, we explore the POSIX API for shared\nmemory.\nPOSIX shared memory is organized using memory-mapped ﬁles, which\nassociate the region of shared memory with a ﬁle. A process must ﬁrst create\nmessage next consumed;\nwhile (true) {\nreceive(next consumed);\n/* consume the item in next consumed */\n}\nFigure 3.16\nThe consumer process using message passing. 3.5\nExamples of IPC Systems\n131\na shared-memory object using the shm open() system call, as follows:\nshm fd = shm open(name, O CREAT | O RDRW, 0666);\nThe ﬁrst parameter speciﬁes the name of the shared-memory object. Processes\nthat wish to access this shared memory must refer to the object by this name.\nThe subsequent parameters specify that the shared-memory object is to be",
  "created if it does not yet exist (O CREAT) and that the object is open for reading\nand writing (O RDRW). The last parameter establishes the directory permissions\nof the shared-memory object. A successful call to shm open() returns an integer\nﬁle descriptor for the shared-memory object.\nOnce the object is established, the ftruncate() function is used to\nconﬁgure the size of the object in bytes. The call\nftruncate(shm fd, 4096);\nsets the size of the object to 4,096 bytes.\nFinally, the mmap() function establishes a memory-mapped ﬁle containing\nthe shared-memory object. It also returns a pointer to the memory-mapped ﬁle\nthat is used for accessing the shared-memory object.\nThe programs shown in Figure 3.17 and 3.18 use the producer–consumer",
  "model in implementing shared memory. The producer establishes a shared-\nmemory object and writes to shared memory, and the consumer reads from\nshared memory.\nThe producer, shown in Figure 3.17, creates a shared-memory object named\nOS and writes the infamous string \"Hello World!\" to shared memory. The\nprogram memory-maps a shared-memory object of the speciﬁed size and\nallows writing to the object. (Obviously, only writing is necessary for the\nproducer.) The ﬂag MAP SHARED speciﬁes that changes to the shared-memory\nobject will be visible to all processes sharing the object. Notice that we write to\nthe shared-memory object by calling the sprintf() function and writing the\nformatted string to the pointer ptr. After each write, we must increment the\npointer by the number of bytes written.",
  "pointer by the number of bytes written.\nThe consumer process, shown in Figure 3.18, reads and outputs the contents\nof the shared memory. The consumer also invokes the shm unlink() function,\nwhich removes the shared-memory segment after the consumer has accessed\nit. We provide further exercises using the POSIX shared-memory API in the\nprogramming exercises at the end of this chapter. Additionally, we provide\nmore detailed coverage of memory mapping in Section 9.7.\n3.5.2\nAn Example: Mach\nAs an example of message passing, we next consider the Mach operating\nsystem. You may recall that we introduced Mach in Chapter 2 as part of the Mac\nOS X operating system. The Mach kernel supports the creation and destruction\nof multiple tasks, which are similar to processes but have multiple threads",
  "of control and fewer associated resources. Most communication in Mach—\nincluding all intertask information—is carried out by messages. Messages are\nsent to and received from mailboxes, called ports in Mach. 132\nChapter 3\nProcesses\n#include <stdio.h>\n#include <stlib.h>\n#include <string.h>\n#include <fcntl.h>\n#include <sys/shm.h>\n#include <sys/stat.h>\nint main()\n{\n/* the size (in bytes) of shared memory object */\nconst int SIZE 4096;\n/* name of the shared memory object */\nconst char *name = \"OS\";\n/* strings written to shared memory */\nconst char *message 0 = \"Hello\";\nconst char *message 1 = \"World!\";\n/* shared memory file descriptor */\nint shm fd;\n/* pointer to shared memory obect */\nvoid *ptr;\n/* create the shared memory object */\nshm fd = shm open(name, O CREAT | O RDRW, 0666);",
  "shm fd = shm open(name, O CREAT | O RDRW, 0666);\n/* configure the size of the shared memory object */\nftruncate(shm fd, SIZE);\n/* memory map the shared memory object */\nptr = mmap(0, SIZE, PROT WRITE, MAP SHARED, shm fd, 0);\n/* write to the shared memory object */\nsprintf(ptr,\"%s\",message 0);\nptr += strlen(message 0);\nsprintf(ptr,\"%s\",message 1);\nptr += strlen(message 1);\nreturn 0;\n}\nFigure 3.17\nProducer process illustrating POSIX shared-memory API.\nEven system calls are made by messages. When a task is created, two\nspecial mailboxes—the Kernel mailbox and the Notify mailbox—are also\ncreated. The kernel uses the Kernel mailbox to communicate with the task and\nsends notiﬁcation of event occurrences to the Notify port. Only three system",
  "calls are needed for message transfer. The msg send() call sends a message\nto a mailbox. A message is received via msg receive(). Remote procedure\ncalls (RPCs) are executed via msg rpc(), which sends a message and waits for\nexactly one return message from the sender. In this way, the RPC models a 3.5\nExamples of IPC Systems\n133\n#include <stdio.h>\n#include <stlib.h>\n#include <fcntl.h>\n#include <sys/shm.h>\n#include <sys/stat.h>\nint main()\n{\n/* the size (in bytes) of shared memory object */\nconst int SIZE 4096;\n/* name of the shared memory object */\nconst char *name = \"OS\";\n/* shared memory file descriptor */\nint shm fd;\n/* pointer to shared memory obect */\nvoid *ptr;\n/* open the shared memory object */\nshm fd = shm open(name, O RDONLY, 0666);\n/* memory map the shared memory object */",
  "/* memory map the shared memory object */\nptr = mmap(0, SIZE, PROT READ, MAP SHARED, shm fd, 0);\n/* read from the shared memory object */\nprintf(\"%s\",(char *)ptr);\n/* remove the shared memory object */\nshm unlink(name);\nreturn 0;\n}\nFigure 3.18\nConsumer process illustrating POSIX shared-memory API.\ntypical subroutine procedure call but can work between systems—hence the\nterm remote. Remote procedure calls are covered in detail in Section 3.6.2.\nThe port allocate() system call creates a new mailbox and allocates\nspace for its queue of messages. The maximum size of the message queue\ndefaults to eight messages. The task that creates the mailbox is that mailbox’s\nowner. The owner is also allowed to receive from the mailbox. Only one task",
  "at a time can either own or receive from a mailbox, but these rights can be sent\nto other tasks.\nThe mailbox’s message queue is initially empty. As messages are sent to\nthe mailbox, the messages are copied into the mailbox. All messages have the\nsame priority. Mach guarantees that multiple messages from the same sender\nare queued in ﬁrst-in, ﬁrst-out (FIFO) order but does not guarantee an absolute\nordering. For instance, messages from two senders may be queued in any order.\nThe messages themselves consist of a ﬁxed-length header followed by a\nvariable-length data portion. The header indicates the length of the message\nand includes two mailbox names. One mailbox name speciﬁes the mailbox 134\nChapter 3\nProcesses\nto which the message is being sent. Commonly, the sending thread expects a",
  "reply, so the mailbox name of the sender is passed on to the receiving task,\nwhich can use it as a “return address.”\nThe variable part of a message is a list of typed data items. Each entry\nin the list has a type, size, and value. The type of the objects speciﬁed in the\nmessage is important, since objects deﬁned by the operating system—such as\nownership or receive access rights, task states, and memory segments—may\nbe sent in messages.\nThe send and receive operations themselves are ﬂexible. For instance, when\na message is sent to a mailbox, the mailbox may be full. If the mailbox is not\nfull, the message is copied to the mailbox, and the sending thread continues. If\nthe mailbox is full, the sending thread has four options:\n1. Wait indeﬁnitely until there is room in the mailbox.",
  "2. Wait at most n milliseconds.\n3. Do not wait at all but rather return immediately.\n4. Temporarily cache a message. Here, a message is given to the operating\nsystem to keep, even though the mailbox to which that message is being\nsent is full. When the message can be put in the mailbox, a message is sent\nback to the sender. Only one message to a full mailbox can be pending at\nany time for a given sending thread.\nThe ﬁnal option is meant for server tasks, such as a line-printer driver. After\nﬁnishing a request, such tasks may need to send a one-time reply to the task\nthat requested service, but they must also continue with other service requests,\neven if the reply mailbox for a client is full.\nThe receive operation must specify the mailbox or mailbox set from which a",
  "message is to be received. A mailbox set is a collection of mailboxes, as declared\nby the task, which can be grouped together and treated as one mailbox for the\npurposes of the task. Threads in a task can receive only from a mailbox or\nmailbox set for which the task has receive access. A port status() system\ncall returns the number of messages in a given mailbox. The receive operation\nattempts to receive from (1) any mailbox in a mailbox set or (2) a speciﬁc\n(named) mailbox. If no message is waiting to be received, the receiving thread\ncan either wait at most n milliseconds or not wait at all.\nThe Mach system was especially designed for distributed systems, which\nwe discuss in Chapter 17, but Mach was shown to be suitable for systems",
  "with fewer processing cores, as evidenced by its inclusion in the Mac OS X\nsystem. The major problem with message systems has generally been poor\nperformance caused by double copying of messages: the message is copied\nﬁrst from the sender to the mailbox and then from the mailbox to the receiver.\nThe Mach message system attempts to avoid double-copy operations by using\nvirtual-memory-management techniques (Chapter 9). Essentially, Mach maps\nthe address space containing the sender’s message into the receiver’s address\nspace. The message itself is never actually copied. This message-management\ntechnique provides a large performance boost but works for only intrasystem\nmessages. The Mach operating system is discussed in more detail in the online\nAppendix B. 3.5\nExamples of IPC Systems\n135",
  "Appendix B. 3.5\nExamples of IPC Systems\n135\n3.5.3\nAn Example: Windows\nThe Windows operating system is an example of modern design that employs\nmodularity to increase functionality and decrease the time needed to imple-\nment new features. Windows provides support for multiple operating envi-\nronments, or subsystems. Application programs communicate with these\nsubsystems via a message-passing mechanism. Thus, application programs\ncan be considered clients of a subsystem server.\nThe message-passing facility in Windows is called the advanced local\nprocedure call (ALPC) facility. It is used for communication between two\nprocesses on the same machine. It is similar to the standard remote procedure\ncall (RPC) mechanism that is widely used, but it is optimized for and speciﬁc",
  "to Windows. (Remote procedure calls are covered in detail in Section 3.6.2.)\nLike Mach, Windows uses a port object to establish and maintain a connection\nbetween two processes. Windows uses two types of ports: connection ports\nand communication ports.\nServer processes publish connection-port objects that are visible to all\nprocesses. When a client wants services from a subsystem, it opens a handle to\nthe server’s connection-port object and sends a connection request to that port.\nThe server then creates a channel and returns a handle to the client. The channel\nconsists of a pair of private communication ports: one for client—server\nmessages, the other for server—client messages. Additionally, communication\nchannels support a callback mechanism that allows the client and server to",
  "accept requests when they would normally be expecting a reply.\nWhen an ALPC channel is created, one of three message-passing techniques\nis chosen:\n1. For small messages (up to 256 bytes), the port’s message queue is used\nas intermediate storage, and the messages are copied from one process to\nthe other.\n2. Larger messages must be passed through a section object, which is a\nregion of shared memory associated with the channel.\n3. When the amount of data is too large to ﬁt into a section object, an API is\navailable that allows server processes to read and write directly into the\naddress space of a client.\nThe client has to decide when it sets up the channel whether it will need\nto send a large message. If the client determines that it does want to send",
  "large messages, it asks for a section object to be created. Similarly, if the server\ndecides that replies will be large, it creates a section object. So that the section\nobject can be used, a small message is sent that contains a pointer and size\ninformation about the section object. This method is more complicated than\nthe ﬁrst method listed above, but it avoids data copying. The structure of\nadvanced local procedure calls in Windows is shown in Figure 3.19.\nIt is important to note that the ALPC facility in Windows is not part of the\nWindows API and hence is not visible to the application programmer. Rather,\napplications using the Windows API invoke standard remote procedure calls.\nWhen the RPC is being invoked on a process on the same system, the RPC is",
  "handled indirectly through an ALPC. procedure call. Additionally, many kernel\nservices use ALPC to communicate with client processes. 136\nChapter 3\nProcesses\nConnection\nPort\nConnection\nrequest\nHandle\nHandle\nHandle\nClient\nCommunication Port\nServer\nCommunication Port\nShared\nSection Object\n(> 256 bytes)\nServer\nClient\nFigure 3.19\nAdvanced local procedure calls in Windows.\n3.6\nCommunication in Client–Server Systems\nIn Section 3.4, we described how processes can communicate using shared\nmemory and message passing. These techniques can be used for communica-\ntion in client–server systems (Section 1.11.4) as well. In this section, we explore\nthree other strategies for communication in client–server systems: sockets,\nremote procedure calls (RPCs), and pipes.\n3.6.1\nSockets",
  "3.6.1\nSockets\nA socket is deﬁned as an endpoint for communication. A pair of processes\ncommunicating over a network employs a pair of sockets—one for each\nprocess. A socket is identiﬁed by an IP address concatenated with a port\nnumber. In general, sockets use a client–server architecture. The server waits\nfor incoming client requests by listening to a speciﬁed port. Once a request\nis received, the server accepts a connection from the client socket to complete\nthe connection. Servers implementing speciﬁc services (such as telnet, FTP, and\nHTTP) listen to well-known ports (a telnet server listens to port 23; an FTP\nserver listens to port 21; and a web, or HTTP, server listens to port 80). All\nports below 1024 are considered well known; we can use them to implement\nstandard services.",
  "standard services.\nWhen a client process initiates a request for a connection, it is assigned a\nport by its host computer. This port has some arbitrary number greater than\n1024. For example, if a client on host X with IP address 146.86.5.20 wishes to\nestablish a connection with a web server (which is listening on port 80) at\naddress 161.25.19.8, host X may be assigned port 1625. The connection will\nconsist of a pair of sockets: (146.86.5.20:1625) on host X and (161.25.19.8:80)\non the web server. This situation is illustrated in Figure 3.20. The packets\ntraveling between the hosts are delivered to the appropriate process based on\nthe destination port number.\nAll connections must be unique. Therefore, if another process also on host",
  "X wished to establish another connection with the same web server, it would be\nassigned a port number greater than 1024 and not equal to 1625. This ensures\nthat all connections consist of a unique pair of sockets. 3.6\nCommunication in Client–Server Systems\n137\nsocket\n(146.86.5.20:1625)\nhost X\n(146.86.5.20)\nsocket\n(161.25.19.8:80)\nweb server\n(161.25.19.8)\nFigure 3.20\nCommunication using sockets.\nAlthough most program examples in this text use C, we will illustrate\nsockets using Java, as it provides a much easier interface to sockets and has a\nrich library for networking utilities. Those interested in socket programming\nin C or C++ should consult the bibliographical notes at the end of the chapter.\nJava provides three different types of sockets. Connection-oriented (TCP)",
  "sockets are implemented with the Socket class. Connectionless (UDP) sockets\nuse the DatagramSocket class. Finally, the MulticastSocket class is a subclass\nof the DatagramSocket class. A multicast socket allows data to be sent to\nmultiple recipients.\nOur example describes a date server that uses connection-oriented TCP\nsockets. The operation allows clients to request the current date and time from\nthe server. The server listens to port 6013, although the port could have any\narbitrary number greater than 1024. When a connection is received, the server\nreturns the date and time to the client.\nThe date server is shown in Figure 3.21. The server creates a ServerSocket\nthat speciﬁes that it will listen to port 6013. The server then begins listening",
  "to the port with the accept() method. The server blocks on the accept()\nmethod waiting for a client to request a connection. When a connection request\nis received, accept() returns a socket that the server can use to communicate\nwith the client.\nThe details of how the server communicates with the socket are as follows.\nThe server ﬁrst establishesaPrintWriterobjectthatitwill use tocommunicate\nwith the client. A PrintWriter object allows the server to write to the socket\nusing the routine print() and println() methods for output. The server\nprocess sends the date to the client, calling the method println(). Once it\nhas written the date to the socket, the server closes the socket to the client and\nresumes listening for more requests.",
  "resumes listening for more requests.\nA client communicates with the server by creating a socket and connecting\nto the port on which the server is listening. We implement such a client in the\nJava program shown in Figure 3.22. The client creates a Socket and requests\na connection with the server at IP address 127.0.0.1 on port 6013. Once the\nconnection is made, the client can read from the socket using normal stream\nI/O statements. After it has received the date from the server, the client closes 138\nChapter 3\nProcesses\nimport java.net.*;\nimport java.io.*;\npublic class DateServer\n{\npublic static void main(String[] args) {\ntry {\nServerSocket sock = new ServerSocket(6013);\n/* now listen for connections */\nwhile (true) {\nSocket client = sock.accept();\nPrintWriter pout = new",
  "PrintWriter pout = new\nPrintWriter(client.getOutputStream(), true);\n/* write the Date to the socket */\npout.println(new java.util.Date().toString());\n/* close the socket and resume */\n/* listening for connections */\nclient.close();\n}\n}\ncatch (IOException ioe) {\nSystem.err.println(ioe);\n}\n}\n}\nFigure 3.21\nDate server.\nthe socket and exits. The IP address 127.0.0.1 is a special IP address known as the\nloopback. When a computer refers to IP address 127.0.0.1, it is referring to itself.\nThis mechanism allows a client and server on the same host to communicate\nusing the TCP/IP protocol. The IP address 127.0.0.1 could be replaced with the\nIP address of another host running the date server. In addition to an IP address,\nan actual host name, such as www.westminstercollege.edu, can be used as\nwell.",
  "well.\nCommunication using sockets—although common and efﬁcient—is con-\nsidered a low-level form of communication between distributed processes.\nOne reason is that sockets allow only an unstructured stream of bytes to be\nexchanged between the communicating threads. It is the responsibility of the\nclient or server application to impose a structure on the data. In the next two\nsubsections, we look at two higher-level methods of communication: remote\nprocedure calls (RPCs) and pipes.\n3.6.2\nRemote Procedure Calls\nOne of the most common forms of remote service is the RPC paradigm, which\nwe discussed brieﬂy in Section 3.5.2. The RPC was designed as a way to 3.6\nCommunication in Client–Server Systems\n139\nimport java.net.*;\nimport java.io.*;\npublic class DateClient\n{",
  "import java.io.*;\npublic class DateClient\n{\npublic static void main(String[] args) {\ntry {\n/* make connection to server socket */\nSocket sock = new Socket(\"127.0.0.1\",6013);\nInputStream in = sock.getInputStream();\nBufferedReader bin = new\nBufferedReader(new InputStreamReader(in));\n/* read the date from the socket */\nString line;\nwhile ( (line = bin.readLine()) != null)\nSystem.out.println(line);\n/* close the socket connection*/\nsock.close();\n}\ncatch (IOException ioe) {\nSystem.err.println(ioe);\n}\n}\n}\nFigure 3.22\nDate client.\nabstract the procedure-call mechanism for use between systems with network\nconnections. It is similar in many respects to the IPC mechanism described in\nSection 3.4, and it is usually built on top of such a system. Here, however,",
  "because we are dealing with an environment in which the processes are\nexecuting on separate systems, we must use a message-based communication\nscheme to provide remote service.\nIn contrast to IPC messages, the messages exchanged in RPCcommunication\nare well structured and are thus no longer just packets of data. Each message is\naddressed to an RPC daemon listening to a port on the remote system, and each\ncontains an identiﬁer specifying the function to execute and the parameters\nto pass to that function. The function is then executed as requested, and any\noutput is sent back to the requester in a separate message.\nA port is simply a number included at the start of a message packet.\nWhereas a system normally has one network address, it can have many ports",
  "within that address to differentiate the many network services it supports. If a\nremote process needs a service, it addresses a message to the proper port. For\ninstance, if a system wished to allow other systems to be able to list its current\nusers, it would have a daemon supporting such an RPC attached to a port—\nsay, port 3027. Any remote system could obtain the needed information (that 140\nChapter 3\nProcesses\nis, the list of current users) by sending an RPC message to port 3027 on the\nserver. The data would be received in a reply message.\nThe semantics of RPCs allows a client to invoke a procedure on a remote\nhost as it would invoke a procedure locally. The RPC system hides the details\nthat allow communication to take place by providing a stub on the client side.",
  "Typically, a separate stub exists for each separate remote procedure. When the\nclient invokes a remote procedure, the RPC system calls the appropriate stub,\npassing it the parameters provided to the remote procedure. This stub locates\nthe port on the server and marshals the parameters. Parameter marshalling\ninvolves packaging the parameters into a form that can be transmitted over\na network. The stub then transmits a message to the server using message\npassing. A similar stub on the server side receives this message and invokes\nthe procedure on the server. If necessary, return values are passed back to the\nclient using the same technique. On Windows systems, stub code is compiled\nfrom a speciﬁcation written in the Microsoft Interface Deﬁnition Language",
  "(MIDL), which is used for deﬁning the interfaces between client and server\nprograms.\nOne issue that must be dealt with concerns differences in data representa-\ntion on the client and server machines. Consider the representation of 32-bit\nintegers. Some systems (known as big-endian) store the most signiﬁcant byte\nﬁrst, while other systems (known as little-endian) store the least signiﬁcant\nbyte ﬁrst. Neither order is “better” per se; rather, the choice is arbitrary within\na computer architecture. To resolve differences like this, many RPC systems\ndeﬁne a machine-independent representation of data. One such representation\nis known as external data representation (XDR). On the client side, parameter\nmarshalling involves converting the machine-dependent data into XDR before",
  "they are sent to the server. On the server side, the XDR data are unmarshalled\nand converted to the machine-dependent representation for the server.\nAnother important issue involves the semantics of a call. Whereas local\nprocedure calls fail only under extreme circumstances, RPCs can fail, or be\nduplicated and executed more than once, as a result of common network\nerrors. One way to address this problem is for the operating system to ensure\nthat messages are acted on exactly once, rather than at most once. Most local\nprocedure calls have the “exactly once” functionality, but it is more difﬁcult to\nimplement.\nFirst, consider “at most once.” This semantic can be implemented by\nattaching a timestamp to each message. The server must keep a history of",
  "all the timestamps of messages it has already processed or a history large\nenough to ensure that repeated messages are detected. Incoming messages\nthat have a timestamp already in the history are ignored. The client can then\nsend a message one or more times and be assured that it only executes once.\nFor “exactly once,” we need to remove the risk that the server will never\nreceive the request. To accomplish this, the server must implement the “at\nmost once” protocol described above but must also acknowledge to the client\nthat the RPC call was received and executed. These ACK messages are common\nthroughout networking. The client must resend each RPC call periodically until\nit receives the ACK for that call.\nYet another important issue concerns the communication between a server",
  "and a client. With standard procedure calls, some form of binding takes place\nduring link, load, or execution time (Chapter 8) so that a procedure call’s name 3.6\nCommunication in Client–Server Systems\n141\nclient\nuser calls kernel\nto send RPC\nmessage to\nprocedure X\nmatchmaker\nreceives\nmessage, looks\nup answer\nmatchmaker\nreplies to client\nwith port P\ndaemon\nlistening to\nport P receives\nmessage\ndaemon\nprocesses\nrequest and\nprocesses send\noutput\nkernel sends\nmessage to\nmatchmaker to\nfind port number\nFrom: client\nTo: server\nPort: matchmaker\nRe: address\nfor RPC X\nFrom: client\nTo: server\nPort: port P\n<contents>\nFrom: RPC\nPort: P\nTo: client\nPort: kernel\n<output>\nFrom: server\nTo: client\nPort: kernel\nRe: RPC X\nPort: P\nkernel places\nport P in user\nRPC message\nkernel sends\nRPC\nkernel receives",
  "RPC message\nkernel sends\nRPC\nkernel receives\nreply, passes\nit to user\nmessages\nserver\nFigure 3.23\nExecution of a remote procedure call (RPC).\nis replaced by the memory address of the procedure call. The RPC scheme\nrequires a similar binding of the client and the server port, but how does a client\nknow the port numbers on the server? Neither system has full information\nabout the other, because they do not share memory.\nTwo approaches are common. First, the binding information may be\npredetermined, in the form of ﬁxed port addresses. At compile time, an RPC\ncall has a ﬁxed port number associated with it. Once a program is compiled,\nthe server cannot change the port number of the requested service. Second,\nbinding can be done dynamically by a rendezvous mechanism. Typically, an",
  "operating system provides a rendezvous (also called a matchmaker) daemon\non a ﬁxed RPC port. A client then sends a message containing the name of\nthe RPC to the rendezvous daemon requesting the port address of the RPC it\nneeds to execute. The port number is returned, and the RPC calls can be sent\nto that port until the process terminates (or the server crashes). This method\nrequires the extra overhead of the initial request but is more ﬂexible than the\nﬁrst approach. Figure 3.23 shows a sample interaction.\nThe RPC scheme is useful in implementing a distributed ﬁle system\n(Chapter 17). Such a system can be implemented as a set of RPC daemons 142\nChapter 3\nProcesses\nand clients. The messages are addressed to the distributed ﬁle system port on a",
  "server on which a ﬁle operation is to take place. The message contains the disk\noperation to be performed. The disk operation might be read, write, rename,\ndelete, or status, corresponding to the usual ﬁle-related system calls. The\nreturn message contains any data resulting from that call, which is executed by\nthe DFS daemon on behalf of the client. For instance, a message might contain\na request to transfer a whole ﬁle to a client or be limited to a simple block\nrequest. In the latter case, several requests may be needed if a whole ﬁle is to\nbe transferred.\n3.6.3\nPipes\nA pipe acts as a conduit allowing two processes to communicate. Pipes were\none of the ﬁrst IPC mechanisms in early UNIX systems. They typically provide\none of the simpler ways for processes to communicate with one another,",
  "although they also have some limitations. In implementing a pipe, four issues\nmust be considered:\n1. Does the pipe allow bidirectional communication, or is communication\nunidirectional?\n2. If two-way communication is allowed, is it half duplex (data can travel\nonly one way at a time) or full duplex (data can travel in both directions\nat the same time)?\n3. Must a relationship (such as parent–child) exist between the communi-\ncating processes?\n4. Can the pipes communicate over a network, or must the communicating\nprocesses reside on the same machine?\nIn the following sections, we explore two common types of pipes used on both\nUNIX and Windows systems: ordinary pipes and named pipes.\n3.6.3.1\nOrdinary Pipes\nOrdinary pipes allow two processes to communicate in standard producer–",
  "consumer fashion: the producer writes to one end of the pipe (the write-end)\nand the consumer reads from the other end (the read-end). As a result, ordinary\npipes are unidirectional, allowing only one-way communication. If two-way\ncommunication is required, two pipes must be used, with each pipe sending\ndata in a different direction. We next illustrate constructing ordinary pipes\non both UNIX and Windows systems. In both program examples, one process\nwrites the message Greetings to the pipe, while the other process reads this\nmessage from the pipe.\nOn UNIX systems, ordinary pipes are constructed using the function\npipe(int fd[])\nThis function creates a pipe that is accessed through the int fd[] ﬁle\ndescriptors: fd[0] is the read-end of the pipe, and fd[1] is the write-end. 3.6",
  "Communication in Client–Server Systems\n143\nparent\nfd(0)\nfd(1)\nchild\nfd(0)\nfd(1)\npipe\nFigure 3.24\nFile descriptors for an ordinary pipe.\nUNIX treats a pipe as a special type of ﬁle. Thus, pipes can be accessed using\nordinary read() and write() system calls.\nAn ordinary pipe cannot be accessed from outside the process that created\nit. Typically, a parent process creates a pipe and uses it to communicate with\na child process that it creates via fork(). Recall from Section 3.3.1 that a child\nprocess inherits open ﬁles from its parent. Since a pipe is a special type of ﬁle,\nthe child inherits the pipe from its parent process. Figure 3.24 illustrates the\nrelationship of the ﬁle descriptor fd to the parent and child processes.",
  "In the UNIX program shown in Figure 3.25, the parent process creates a\npipe and then sends a fork() call creating the child process. What occurs after\nthe fork() call depends on how the data are to ﬂow through the pipe. In\nthis instance, the parent writes to the pipe, and the child reads from it. It is\nimportant to notice that both the parent process and the child process initially\nclose their unused ends of the pipe. Although the program shown in Figure\n3.25 does not require this action, it is an important step to ensure that a process\nreading from the pipe can detect end-of-ﬁle (read() returns 0) when the writer\nhas closed its end of the pipe.\nOrdinary pipes on Windows systems are termed anonymous pipes, and\nthey behave similarly to their UNIX counterparts: they are unidirectional and",
  "#include <sys/types.h>\n#include <stdio.h>\n#include <string.h>\n#include <unistd.h>\n#define BUFFER SIZE 25\n#define READ END 0\n#define WRITE END 1\nint main(void)\n{\nchar write msg[BUFFER SIZE] = \"Greetings\";\nchar read msg[BUFFER SIZE];\nint fd[2];\npid t pid;\n/* Program continues in Figure 3.26 */\nFigure 3.25\nOrdinary pipe in UNIX. 144\nChapter 3\nProcesses\n/* create the pipe */\nif (pipe(fd) == -1) {\nfprintf(stderr,\"Pipe failed\");\nreturn 1;\n}\n/* fork a child process */\npid = fork();\nif (pid < 0) { /* error occurred */\nfprintf(stderr, \"Fork Failed\");\nreturn 1;\n}\nif (pid > 0) { /* parent process */\n/* close the unused end of the pipe */\nclose(fd[READ END]);\n/* write to the pipe */\nwrite(fd[WRITE END], write msg, strlen(write msg)+1);\n/* close the write end of the pipe */\nclose(fd[WRITE END]);\n}",
  "close(fd[WRITE END]);\n}\nelse { /* child process */\n/* close the unused end of the pipe */\nclose(fd[WRITE END]);\n/* read from the pipe */\nread(fd[READ END], read msg, BUFFER SIZE);\nprintf(\"read %s\",read msg);\n/* close the write end of the pipe */\nclose(fd[READ END]);\n}\nreturn 0;\n}\nFigure 3.26\nFigure 3.25, continued.\nemploy parent–child relationships between the communicating processes.\nIn addition, reading and writing to the pipe can be accomplished with the\nordinary ReadFile() and WriteFile() functions. The Windows API for\ncreating pipes is the CreatePipe() function, which is passed four parameters.\nThe parameters provide separate handles for (1) reading and (2) writing to the\npipe, as well as (3) an instance of the STARTUPINFO structure, which is used to",
  "specify that the child process is to inherit the handles of the pipe. Furthermore,\n(4) the size of the pipe (in bytes) may be speciﬁed.\nFigure 3.27 illustrates a parent process creating an anonymous pipe for\ncommunicating with its child. Unlike UNIX systems, in which a child process 3.6\nCommunication in Client–Server Systems\n145\n#include <stdio.h>\n#include <stdlib.h>\n#include <windows.h>\n#define BUFFER SIZE 25\nint main(VOID)\n{\nHANDLE ReadHandle, WriteHandle;\nSTARTUPINFO si;\nPROCESS INFORMATION pi;\nchar message[BUFFER SIZE] = \"Greetings\";\nDWORD written;\n/* Program continues in Figure 3.28 */\nFigure 3.27\nWindows anonymous pipe—parent process.\nautomatically inherits a pipe created by its parent, Windows requires the",
  "programmer to specify which attributes the child process will inherit. This is\naccomplished by ﬁrst initializing the SECURITY ATTRIBUTES structure to allow\nhandles to be inherited and then redirecting the child process’s handles for\nstandard input or standard output to the read or write handle of the pipe.\nSince the child will be reading from the pipe, the parent must redirect the\nchild’s standard input to the read handle of the pipe. Furthermore, as the\npipes are half duplex, it is necessary to prohibit the child from inheriting the\nwrite-end of the pipe. The program to create the child process is similar to the\nprogram in Figure 3.11, except that the ﬁfth parameter is set to TRUE, indicating\nthat the child process is to inherit designated handles from its parent. Before",
  "writing to the pipe, the parent ﬁrst closes its unused read end of the pipe. The\nchild process that reads from the pipe is shown in Figure 3.29. Before reading\nfrom the pipe, this program obtains the read handle to the pipe by invoking\nGetStdHandle().\nNote that ordinary pipes require a parent–child relationship between the\ncommunicating processes on both UNIX and Windows systems. This means\nthat these pipes can be used only for communication between processes on the\nsame machine.\n3.6.3.2\nNamed Pipes\nOrdinary pipes provide a simple mechanism for allowing a pair of processes\nto communicate. However, ordinary pipes exist only while the processes are\ncommunicating with one another. On both UNIX and Windows systems, once",
  "the processes have ﬁnished communicating and have terminated, the ordinary\npipe ceases to exist.\nNamed pipes provide a much more powerful communication tool. Com-\nmunication can be bidirectional, and no parent–child relationship is required.\nOnce a named pipe is established, several processes can use it for communi-\ncation. In fact, in a typical scenario, a named pipe has several writers. Addi-\ntionally, named pipes continue to exist after communicating processes have 146\nChapter 3\nProcesses\n/* set up security attributes allowing pipes to be inherited */\nSECURITY ATTRIBUTES sa = {sizeof(SECURITY ATTRIBUTES),NULL,TRUE};\n/* allocate memory */\nZeroMemory(&pi, sizeof(pi));\n/* create the pipe */\nif (!CreatePipe(&ReadHandle, &WriteHandle, &sa, 0)) {\nfprintf(stderr, \"Create Pipe Failed\");",
  "fprintf(stderr, \"Create Pipe Failed\");\nreturn 1;\n}\n/* establish the START INFO structure for the child process */\nGetStartupInfo(&si);\nsi.hStdOutput = GetStdHandle(STD OUTPUT HANDLE);\n/* redirect standard input to the read end of the pipe */\nsi.hStdInput = ReadHandle;\nsi.dwFlags = STARTF USESTDHANDLES;\n/* don’t allow the child to inherit the write end of pipe */\nSetHandleInformation(WriteHandle, HANDLE FLAG INHERIT, 0);\n/* create the child process */\nCreateProcess(NULL, \"child.exe\", NULL, NULL,\nTRUE, /* inherit handles */\n0, NULL, NULL, &si, &pi);\n/* close the unused end of the pipe */\nCloseHandle(ReadHandle);\n/* the parent writes to the pipe */\nif (!WriteFile(WriteHandle, message,BUFFER SIZE,&written,NULL))\nfprintf(stderr, \"Error writing to pipe.\");\n/* close the write end of the pipe */",
  "/* close the write end of the pipe */\nCloseHandle(WriteHandle);\n/* wait for the child to exit */\nWaitForSingleObject(pi.hProcess, INFINITE);\nCloseHandle(pi.hProcess);\nCloseHandle(pi.hThread);\nreturn 0;\n}\nFigure 3.28\nFigure 3.27, continued.\nﬁnished. Both UNIX and Windows systems support named pipes, although the\ndetails of implementation differ greatly. Next, we explore named pipes in each\nof these systems. 3.7\nSummary\n147\n#include <stdio.h>\n#include <windows.h>\n#define BUFFER SIZE 25\nint main(VOID)\n{\nHANDLE Readhandle;\nCHAR buffer[BUFFER SIZE];\nDWORD read;\n/* get the read handle of the pipe */\nReadHandle = GetStdHandle(STD INPUT HANDLE);\n/* the child reads from the pipe */\nif (ReadFile(ReadHandle, buffer, BUFFER SIZE, &read, NULL))\nprintf(\"child read %s\",buffer);\nelse",
  "printf(\"child read %s\",buffer);\nelse\nfprintf(stderr, \"Error reading from pipe\");\nreturn 0;\n}\nFigure 3.29\nWindows anonymous pipes—child process.\nNamed pipes are referred to as FIFOs in UNIX systems. Once created, they\nappear as typical ﬁles in the ﬁle system. A FIFO is created with the mkfifo()\nsystem call and manipulated with the ordinary open(), read(), write(),\nand close() system calls. It will continue to exist until it is explicitly deleted\nfrom the ﬁle system. Although FIFOs allow bidirectional communication, only\nhalf-duplex transmission is permitted. If data must travel in both directions,\ntwo FIFOs are typically used. Additionally, the communicating processes must\nreside on the same machine. If intermachine communication is required,\nsockets (Section 3.6.1) must be used.",
  "sockets (Section 3.6.1) must be used.\nNamed pipes on Windows systems provide a richer communication mech-\nanism than their UNIX counterparts. Full-duplex communication is allowed,\nand the communicating processes may reside on either the same or different\nmachines. Additionally, only byte-oriented data may be transmitted across a\nUNIX FIFO, whereas Windows systems allow either byte- or message-oriented\ndata. Named pipes are created with the CreateNamedPipe() function, and a\nclient can connect to a named pipe using ConnectNamedPipe(). Communi-\ncation over the named pipe can be accomplished using the ReadFile() and\nWriteFile() functions.\n3.7\nSummary\nA process is a program in execution. As a process executes, it changes state. The",
  "state of a process is deﬁned by that process’s current activity. Each process may\nbe in one of the following states: new, ready, running, waiting, or terminated. 148\nChapter 3\nProcesses\nPIPES IN PRACTICE\nPipes are used quite often in the UNIX command-line environment for\nsituations in which the output of one command serves as input to another. For\nexample, the UNIX ls command produces a directory listing. For especially\nlong directory listings, the output may scroll through several screens. The\ncommand more manages output by displaying only one screen of output at\na time; the user must press the space bar to move from one screen to the next.\nSetting up a pipe between the ls and more commands (which are running as",
  "individual processes) allows the output of ls to be delivered as the input to\nmore, enabling the user to display a large directory listing a screen at a time.\nA pipe can be constructed on the command line using the | character. The\ncomplete command is\nls | more\nIn this scenario, the ls command serves as the producer, and its output is\nconsumed by the more command.\nWindows systems provide a more command for the DOS shell with\nfunctionality similar to that of its UNIX counterpart. The DOS shell also uses\nthe | character for establishing a pipe. The only difference is that to get\na directory listing, DOS uses the dir command rather than ls, as shown\nbelow:\ndir | more\nEach process is represented in the operating system by its own process control\nblock (PCB).",
  "block (PCB).\nA process, when it is not executing, is placed in some waiting queue. There\nare two major classes of queues in an operating system: I/O request queues\nand the ready queue. The ready queue contains all the processes that are ready\nto execute and are waiting for the CPU. Each process is represented by a PCB.\nThe operating system must select processes from various scheduling\nqueues. Long-term (job) scheduling is the selection of processes that will be\nallowed to contend for the CPU. Normally, long-term scheduling is heavily\ninﬂuenced by resource-allocation considerations, especially memory manage-\nment. Short-term (CPU) scheduling is the selection of one process from the\nready queue.\nOperating systems must provide a mechanism for parent processes to",
  "create new child processes. The parent may wait for its children to terminate\nbefore proceeding, or the parent and children may execute concurrently. There\nare several reasons for allowing concurrent execution: information sharing,\ncomputation speedup, modularity, and convenience.\nThe processes executing in the operating system may be either independent\nprocesses or cooperating processes. Cooperating processes require an interpro-\ncess communication mechanism to communicate with each other. Principally,\ncommunication is achieved through two schemes: shared memory and mes-\nsage passing. The shared-memory method requires communicating processes Practice Exercises\n149\n#include <sys/types.h>\n#include <stdio.h>\n#include <unistd.h>\nint value = 5;\nint main()\n{\npid t pid;\npid = fork();",
  "int main()\n{\npid t pid;\npid = fork();\nif (pid == 0) { /* child process */\nvalue += 15;\nreturn 0;\n}\nelse if (pid > 0) { /* parent process */\nwait(NULL);\nprintf(\"PARENT: value = %d\",value); /* LINE A */\nreturn 0;\n}\n}\nFigure 3.30\nWhat output will be at Line A?\nto share some variables. The processes are expected to exchange information\nthrough the use of these shared variables. In a shared-memory system, the\nresponsibility for providing communication rests with the application pro-\ngrammers; the operating system needs to provide only the shared memory.\nThe message-passing method allows the processes to exchange messages.\nThe responsibility for providing communication may rest with the operating\nsystem itself. These two schemes are not mutually exclusive and can be used",
  "simultaneously within a single operating system.\nCommunication in client–server systems may use (1) sockets, (2) remote\nprocedure calls (RPCs), or (3) pipes. A socket is deﬁned as an endpoint for\ncommunication. A connection between a pair of applications consists of a pair\nof sockets, one at each end of the communication channel. RPCs are another\nform of distributed communication. An RPC occurs when a process (or thread)\ncalls a procedure on a remote application. Pipes provide a relatively simple\nways for processes to communicate with one another. Ordinary pipes allow\ncommunication between parent and child processes, while named pipes permit\nunrelated processes to communicate.\nPractice Exercises\n3.1\nUsing the program shown in Figure 3.30, explain what the output will\nbe at LINE A.\n3.2",
  "be at LINE A.\n3.2\nIncluding the initial parent process, how many processes are created by\nthe program shown in Figure 3.31? 150\nChapter 3\nProcesses\n#include <stdio.h>\n#include <unistd.h>\nint main()\n{\n/* fork a child process */\nfork();\n/* fork another child process */\nfork();\n/* and fork another */\nfork();\nreturn 0;\n}\nFigure 3.31\nHow many processes are created?\n3.3\nOriginal versions of Apple’s mobile iOS operating system provided no\nmeans of concurrent processing. Discuss three major complications that\nconcurrent processing adds to an operating system.\n3.4\nThe Sun UltraSPARC processor has multiple register sets. Describe what\nhappens when a context switch occurs if the new context is already\nloaded into one of the register sets. What happens if the new context is",
  "in memory rather than in a register set and all the register sets are in\nuse?\n3.5\nWhen a process creates a new process using the fork() operation, which\nof the following states is shared between the parent process and the child\nprocess?\na.\nStack\nb.\nHeap\nc.\nShared memory segments\n3.6\nConsider the “exactly once”semantic with respect to the RPC mechanism.\nDoes the algorithm for implementing this semantic execute correctly\neven if the ACK message sent back to the client is lost due to a network\nproblem? Describe the sequence of messages, and discuss whether\n“exactly once” is still preserved.\n3.7\nAssume that a distributed system is susceptible to server failure. What\nmechanisms would be required to guarantee the “exactly once” semantic\nfor execution of RPCs?\nExercises\n3.8",
  "for execution of RPCs?\nExercises\n3.8\nDescribe the differences among short-term, medium-term, and long-\nterm scheduling. Exercises\n151\n#include <stdio.h>\n#include <unistd.h>\nint main()\n{\nint i;\nfor (i = 0; i < 4; i++)\nfork();\nreturn 0;\n}\nFigure 3.32\nHow many processes are created?\n3.9\nDescribe the actions taken by a kernel to context-switch between\nprocesses.\n3.10\nConstruct a process tree similar to Figure 3.8. To obtain process infor-\nmation for the UNIX or Linux system, use the command ps -ael.\n#include <sys/types.h>\n#include <stdio.h>\n#include <unistd.h>\nint main()\n{\npid t pid;\n/* fork a child process */\npid = fork();\nif (pid < 0) { /* error occurred */\nfprintf(stderr, \"Fork Failed\");\nreturn 1;\n}\nelse if (pid == 0) { /* child process */\nexeclp(\"/bin/ls\",\"ls\",NULL);\nprintf(\"LINE J\");\n}",
  "execlp(\"/bin/ls\",\"ls\",NULL);\nprintf(\"LINE J\");\n}\nelse { /* parent process */\n/* parent will wait for the child to complete */\nwait(NULL);\nprintf(\"Child Complete\");\n}\nreturn 0;\n}\nFigure 3.33\nWhen will LINE J be reached? 152\nChapter 3\nProcesses\nUse the command man ps to get more information about the ps com-\nmand. The task manager on Windows systems does not provide the\nparent process ID, but the process monitor tool, available from tech-\nnet.microsoft.com, provides a process-tree tool.\n3.11\nExplain the role of the init process on UNIX and Linux systems in regard\nto process termination.\n3.12\nIncluding the initial parent process, how many processes are created by\nthe program shown in Figure 3.32?\n3.13\nExplain the circumstances under which which the line of code marked",
  "printf(\"LINE J\") in Figure 3.33 will be reached.\n3.14\nUsing the program in Figure 3.34, identify the values of pid at lines A, B,\nC, and D. (Assume that the actual pids of the parent and child are 2600\nand 2603, respectively.)\n#include <sys/types.h>\n#include <stdio.h>\n#include <unistd.h>\nint main()\n{\npid t pid, pid1;\n/* fork a child process */\npid = fork();\nif (pid < 0) { /* error occurred */\nfprintf(stderr, \"Fork Failed\");\nreturn 1;\n}\nelse if (pid == 0) { /* child process */\npid1 = getpid();\nprintf(\"child: pid = %d\",pid); /* A */\nprintf(\"child: pid1 = %d\",pid1); /* B */\n}\nelse { /* parent process */\npid1 = getpid();\nprintf(\"parent: pid = %d\",pid); /* C */\nprintf(\"parent: pid1 = %d\",pid1); /* D */\nwait(NULL);\n}\nreturn 0;\n}\nFigure 3.34\nWhat are the pid values? Exercises\n153",
  "What are the pid values? Exercises\n153\n#include <sys/types.h>\n#include <stdio.h>\n#include <unistd.h>\n#define SIZE 5\nint nums[SIZE] = {0,1,2,3,4};\nint main()\n{\nint i;\npid t pid;\npid = fork();\nif (pid == 0) {\nfor (i = 0; i < SIZE; i++) {\nnums[i] *= -i;\nprintf(\"CHILD: %d \",nums[i]); /* LINE X */\n}\n}\nelse if (pid > 0) {\nwait(NULL);\nfor (i = 0; i < SIZE; i++)\nprintf(\"PARENT: %d \",nums[i]); /* LINE Y */\n}\nreturn 0;\n}\nFigure 3.35\nWhat output will be at Line X and Line Y?\n3.15\nGive an example of a situation in which ordinary pipes are more suitable\nthan named pipes and an example of a situation in which named pipes\nare more suitable than ordinary pipes.\n3.16\nConsider the RPC mechanism. Describe the undesirable consequences\nthat could arise from not enforcing either the “at most once” or “exactly",
  "once” semantic. Describe possible uses for a mechanism that has neither\nof these guarantees.\n3.17\nUsing the program shown in Figure 3.35, explain what the output will\nbe at lines X and Y.\n3.18\nWhat are the beneﬁts and the disadvantages of each of the following?\nConsider both the system level and the programmer level.\na.\nSynchronous and asynchronous communication\nb.\nAutomatic and explicit buffering\nc.\nSend by copy and send by reference\nd.\nFixed-sized and variable-sized messages 154\nChapter 3\nProcesses\nProgramming Problems\n3.19\nUsing either a UNIX or a Linux system, write a C program that forks\na child process that ultimately becomes a zombie process. This zombie\nprocess must remain in the system for at least 10 seconds. Process states\ncan be obtained from the command\nps -l",
  "can be obtained from the command\nps -l\nThe process states are shown below the S column; processes with a state\nof Z are zombies. The process identiﬁer (pid) of the child process is listed\nin the PID column, and that of the parent is listed in the PPID column.\nPerhaps the easiest way to determine that the child process is indeed\na zombie is to run the program that you have written in the background\n(using the &) and then run the command ps -l to determine whether\nthe child is a zombie process. Because you do not want too many zombie\nprocesses existing in the system, you will need to remove the one that\nyou have created. The easiest way to do that is to terminate the parent\nprocess using the kill command. For example, if the process id of the\nparent is 4884, you would enter\nkill -9 4884",
  "parent is 4884, you would enter\nkill -9 4884\n3.20\nAn operating system’s pid manager is responsible for managing process\nidentiﬁers. When a process is ﬁrst created, it is assigned a unique pid\nby the pid manager. The pid is returned to the pid manager when the\nprocess completes execution, and the manager may later reassign this\npid. Process identiﬁers are discussed more fully in Section 3.3.1. What\nis most important here is to recognize that process identiﬁers must be\nunique; no two active processes can have the same pid.\nUse the following constants to identify the range of possible pid values:\n#define MIN PID 300\n#define MAX PID 5000\nYou may use any data structure of your choice to represent the avail-\nability of process identiﬁers. One strategy is to adopt what Linux has",
  "done and use a bitmap in which a value of 0 at position i indicates that\na process id of value i is available and a value of 1 indicates that the\nprocess id is currently in use.\nImplement the following API for obtaining and releasing a pid:\n• int allocate map(void)—Creates and initializes a data structure\nfor representing pids; returns—1 if unsuccessful, 1 if successful\n• int allocate pid(void)—Allocates and returns a pid; returns—\n1 if unable to allocate a pid (all pids are in use)\n• void release pid(int pid)—Releases a pid\nThis programming problem will be modiﬁed later on in Chpaters 4 and\n5. Programming Problems\n155\n3.21\nThe Collatz conjecture concerns what happens when we take any\npositive integer n and apply the following algorithm:\nn =\n!\nn/2,\nif n is even\n3 × n + 1,\nif n is odd",
  "n =\n!\nn/2,\nif n is even\n3 × n + 1,\nif n is odd\nThe conjecture states that when this algorithm is continually applied,\nall positive integers will eventually reach 1. For example, if n = 35, the\nsequence is\n35, 106, 53, 160, 80, 40, 20, 10, 5, 16, 8, 4, 2, 1\nWrite a C program using the fork() system call that generates this\nsequence in the child process. The starting number will be provided\nfrom the command line. For example, if 8 is passed as a parameter on\nthe command line, the child process will output 8, 4, 2, 1. Because the\nparent and child processes have their own copies of the data, it will be\nnecessary for the child to output the sequence. Have the parent invoke\nthe wait() call to wait for the child process to complete before exiting",
  "the program. Perform necessary error checking to ensure that a positive\ninteger is passed on the command line.\n3.22\nIn Exercise 3.21, the child process must output the sequence of numbers\ngenerated from the algorithm speciﬁed by the Collatz conjecture because\nthe parent and child have their own copies of the data. Another\napproach to designing this program is to establish a shared-memory\nobject between the parent and child processes. This technique allows the\nchild to write the contents of the sequence to the shared-memory object.\nThe parent can then output the sequence when the child completes.\nBecause the memory is shared, any changes the child makes will be\nreﬂected in the parent process as well.\nThis program will be structured using POSIX shared memory as",
  "described in Section 3.5.1. The parent process will progress through the\nfollowing steps:\na.\nEstablish the shared-memory object (shm open(), ftruncate(),\nand mmap()).\nb.\nCreate the child process and wait for it to terminate.\nc.\nOutput the contents of shared memory.\nd.\nRemove the shared-memory object.\nOne area of concern with cooperating processes involves synchro-\nnization issues. In this exercise, the parent and child processes must be\ncoordinated so that the parent does not output the sequence until the\nchild ﬁnishes execution. These two processes will be synchronized using\nthe wait() system call: the parent process will invoke wait(), which\nwill suspend it until the child process exits.\n3.23\nSection 3.6.1 describes port numbers below 1024 as being well known—",
  "that is, they provide standard services. Port 17 is known as the quote-of- 156\nChapter 3\nProcesses\nthe-day service. When a client connects to port 17 on a server, the server\nresponds with a quote for that day.\nModify the date server shown in Figure 3.21 so that it delivers a quote\nof the day rather than the current date. The quotes should be printable\nASCII characters and should contain fewer than 512 characters, although\nmultiple lines are allowed. Since port 17 is well known and therefore\nunavailable, have your server listen to port 6017. The date client shown\nin Figure 3.22 can be used to read the quotes returned by your server.\n3.24\nA haiku is a three-line poem in which the ﬁrst line contains ﬁve syllables,\nthe second line contains seven syllables, and the third line contains ﬁve",
  "syllables. Write a haiku server that listens to port 5575. When a client\nconnects to this port, the server responds with a haiku. The date client\nshown in Figure 3.22 can be used to read the quotes returned by your\nhaiku server.\n3.25\nAn echo server echoes back whatever it receives from a client. For\nexample, if a client sends the server the string Hello there!, the server\nwill respond with Hello there!\nWrite an echo server using the Java networking API described in\nSection 3.6.1. This server will wait for a client connection using the\naccept() method. When a client connection is received, the server will\nloop, performing the following steps:\n• Read data from the socket into a buffer.\n• Write the contents of the buffer back to the client.",
  "The server will break out of the loop only when it has determined that\nthe client has closed the connection.\nThe\ndate\nserver\nshown\nin\nFigure\n3.21\nuses\nthe\njava.io.BufferedReader\nclass.\nBufferedReader\nextends\nthe\njava.io.Reader class, which is used for reading character streams.\nHowever,\nthe\necho\nserver\ncannot\nguarantee\nthat\nit\nwill\nread\ncharacters from clients; it may receive binary data as well. The\nclass java.io.InputStream deals with data at the byte level rather\nthan the character level. Thus, your echo server must use an object\nthat extends java.io.InputStream. The read() method in the\njava.io.InputStream class returns −1 when the client has closed its\nend of the socket connection.\n3.26\nDesign a program using ordinary pipes in which one process sends a",
  "string message to a second process, and the second process reverses\nthe case of each character in the message and sends it back to the ﬁrst\nprocess. For example, if the ﬁrst process sends the messageHi There, the\nsecond process will return hI tHERE. This will require using two pipes,\none for sending the original message from the ﬁrst to the second process\nand the other for sending the modiﬁed message from the second to the\nﬁrst process. You can write this program using either UNIX or Windows\npipes.\n3.27\nDesign a ﬁle-copying program named filecopy using ordinary pipes.\nThis program will be passed two parameters: the name of the ﬁle to be Programming Projects\n157\ncopied and the name of the copied ﬁle. The program will then create",
  "an ordinary pipe and write the contents of the ﬁle to be copied to the\npipe. The child process will read this ﬁle from the pipe and write it to\nthe destination ﬁle. For example, if we invoke the program as follows:\nfilecopy input.txt copy.txt\nthe ﬁle input.txt will be written to the pipe. The child process will\nread the contents of this ﬁle and write it to the destination ﬁle copy.txt.\nYou may write this program using either UNIX or Windows pipes.\nProgramming Projects\nProject 1—UNIX Shell and History Feature\nThis project consists of designing a C program to serve as a shell interface\nthat accepts user commands and then executes each command in a separate\nprocess. This project can be completed on any Linux, UNIX, or Mac OS X system.",
  "A shell interface gives the user a prompt, after which the next command\nis entered. The example below illustrates the prompt osh> and the user’s\nnext command: cat prog.c. (This command displays the ﬁle prog.c on the\nterminal using the UNIX cat command.)\nosh> cat prog.c\nOne technique for implementing a shell interface is to have the parent process\nﬁrst read what the user enters on the command line (in this case, cat\nprog.c), and then create a separate child process that performs the command.\nUnless otherwise speciﬁed, the parent process waits for the child to exit\nbefore continuing. This is similar in functionality to the new process creation\nillustrated in Figure 3.10. However, UNIX shells typically also allow the child",
  "process to run in the background, or concurrently. To accomplish this, we add\nan ampersand (&) at the end of the command. Thus, if we rewrite the above\ncommand as\nosh> cat prog.c &\nthe parent and child processes will run concurrently.\nThe separate child process is created using the fork() system call, and the\nuser’s command is executed using one of the system calls in the exec() family\n(as described in Section 3.3.1).\nA C program that provides the general operations of a command-line shell\nis supplied in Figure 3.36. The main() function presents the prompt osh->\nand outlines the steps to be taken after input from the user has been read. The\nmain() function continually loops as long as should run equals 1; when the\nuser enters exit at the prompt, your program will set should run to 0 and",
  "terminate.\nThis project is organized into two parts: (1) creating the child process and\nexecuting the command in the child, and (2) modifying the shell to allow a\nhistory feature. 158\nChapter 3\nProcesses\n#include <stdio.h>\n#include <unistd.h>\n#define MAX LINE 80 /* The maximum length command */\nint main(void)\n{\nchar *args[MAX LINE/2 + 1]; /* command line arguments */\nint should run = 1; /* flag to determine when to exit program */\nwhile (should run) {\nprintf(\"osh>\");\nfflush(stdout);\n/**\n* After reading user input, the steps are:\n* (1) fork a child process using fork()\n* (2) the child process will invoke execvp()\n* (3) if command included &, parent will invoke wait()\n*/\n}\nreturn 0;\n}\nFigure 3.36\nOutline of simple shell.\nPart I— Creating a Child Process",
  "Part I— Creating a Child Process\nThe ﬁrst task is to modify the main() function in Figure 3.36 so that a child\nprocess is forked and executes the command speciﬁed by the user. This will\nrequire parsing what the user has entered into separate tokens and storing the\ntokens in an array of character strings (args in Figure 3.36). For example, if the\nuser enters the command ps -ael at the osh> prompt, the values stored in the\nargs array are:\nargs[0] = \"ps\"\nargs[1] = \"-ael\"\nargs[2] = NULL\nThis args array will be passed to the execvp() function, which has the\nfollowing prototype:\nexecvp(char *command, char *params[]);\nHere, command represents the command to be performed and params stores the\nparameters to this command. For this project, the execvp() function should",
  "be invoked as execvp(args[0], args). Be sure to check whether the user\nincluded an & to determine whether or not the parent process is to wait for the\nchild to exit. Programming Projects\n159\nPart II—Creating a History Feature\nThe next task is to modify the shell interface program so that it provides\na history feature that allows the user to access the most recently entered\ncommands. The user will be able to access up to 10 commands by using the\nfeature. The commands will be consecutively numbered starting at 1, and\nthe numbering will continue past 10. For example, if the user has entered 35\ncommands, the 10 most recent commands will be numbered 26 to 35.\nThe user will be able to list the command history by entering the command\nhistory",
  "history\nat the osh> prompt. As an example, assume that the history consists of the\ncommands (from most to least recent):\nps, ls -l, top, cal, who, date\nThe command history will output:\n6 ps\n5 ls -l\n4 top\n3 cal\n2 who\n1 date\nYour program should support two techniques for retrieving commands\nfrom the command history:\n1. When the user enters !!, the most recent command in the history is\nexecuted.\n2. When the user enters a single ! followed by an integer N, the Nth\ncommand in the history is executed.\nContinuing our example from above, if the user enters !!, the ps command\nwill be performed; if the user enters !3, the command cal will be executed.\nAny command executed in this fashion should be echoed on the user’s screen.",
  "The command should also be placed in the history buffer as the next command.\nThe program should also manage basic error handling. If there are\nno commands in the history, entering !! should result in a message “No\ncommands in history.” If there is no command corresponding to the number\nentered with the single !, the program should output \"No such command in\nhistory.\"\nProject 2—Linux Kernel Module for Listing Tasks\nIn this project, you will write a kernel module that lists all current tasks in a\nLinux system. Be sure to review the programming project in Chapter 2, which\ndeals with creating Linux kernel modules, before you begin this project. The\nproject can be completed using the Linux virtual machine provided with this\ntext. 160\nChapter 3\nProcesses\nPart I—Iterating over Tasks Linearly",
  "Processes\nPart I—Iterating over Tasks Linearly\nAs illustrated in Section 3.1, the PCB in Linux is represented by the structure\ntask struct, which is found in the <linux/sched.h> include ﬁle. In Linux,\nthe for each process() macro easily allows iteration over all current tasks\nin the system:\n#include <linux/sched.h>\nstruct task struct *task;\nfor each process(task) {\n/* on each iteration task points to the next task */\n}\nThe various ﬁelds in task struct can then be displayed as the program loops\nthrough the for each process() macro.\nPart I Assignment\nDesign a kernel module that iterates through all tasks in the system using the\nfor each process() macro. In particular, output the task name (known as\nexecutable name), state, and process id of each task. (You will probably have",
  "to read through the task struct structure in <linux/sched.h> to obtain the\nnames of these ﬁelds.) Write this code in the module entry point so that its\ncontents will appear in the kernel log buffer, which can be viewed using the\ndmesg command. To verify that your code is working correctly, compare the\ncontents of the kernel log buffer with the output of the following command,\nwhich lists all tasks in the system:\nps -el\nThe two values should be very similar. Because tasks are dynamic, however, it\nis possible that a few tasks may appear in one listing but not the other.\nPart II—Iterating over Tasks with a Depth-First Search Tree\nThe second portion of this project involves iterating over all tasks in the system\nusing a depth-ﬁrst search (DFS) tree. (As an example: the DFS iteration of the",
  "processes in Figure 3.8 is 1, 8415, 8416, 9298, 9204, 2, 6, 200, 3028, 3610, 4005.)\nLinux maintains its process tree as a series of lists. Examining the\ntask struct in <linux/sched.h>, we see two struct list head objects:\nchildren\nand\nsibling Bibliographical Notes\n161\nThese objects are pointers to a list of the task’s children, as well as its sib-\nlings. Linux also maintains references to the init task (struct task struct\ninit task). Using this information as well as macro operations on lists, we\ncan iterate over the children of init as follows:\nstruct task struct *task;\nstruct list head *list;\nlist for each(list, &init task->children) {\ntask = list entry(list, struct task struct, sibling);\n/* task points to the next child in the list */\n}",
  "/* task points to the next child in the list */\n}\nThe list for each() macro is passed two parameters, both of type struct\nlist head:\n• A pointer to the head of the list to be traversed\n• A pointer to the head node of the list to be traversed\nAt each iteration of list for each(), the ﬁrst parameter is set to the list\nstructure of the next child. We then use this value to obtain each structure in\nthe list using the list entry() macro.\nPart II Assignment\nBeginning from the init task, design a kernel module that iterates over all tasks\nin the system using a DFS tree. Just as in the ﬁrst part of this project, output\nthe name, state, and pid of each task. Perform this iteration in the kernel entry\nmodule so that its output appears in the kernel log buffer.",
  "If you output all tasks in the system, you may see many more tasks than\nappear with the ps -ael command. This is because some threads appear as\nchildren but do not show up as ordinary processes. Therefore, to check the\noutput of the DFS tree, use the command\nps -eLf\nThis command lists all tasks—including threads—in the system. To verify\nthat you have indeed performed an appropriate DFS iteration, you will have to\nexamine the relationships among the various tasks output by the ps command.\nBibliographical Notes\nProcess creation, management, and IPC in UNIX and Windows systems,\nrespectively, are discussed in [Robbins and Robbins (2003)] and [Russinovich\nand Solomon (2009)]. [Love (2010)] covers support for processes in the Linux",
  "kernel, and [Hart (2005)] covers Windows systems programming in detail.\nCoverage of the multiprocess model used in Google’s Chrome can be found at\nhttp://blog.chromium.org/2008/09/multi-process-architecture.html. 162\nChapter 3\nProcesses\nMessage passing for multicore systems is discussed in [Holland and\nSeltzer (2011)]. [Baumann et al. (2009)] describe performance issues in shared-\nmemory and message-passing systems. [Vahalia (1996)] describes interprocess\ncommunication in the Mach system.\nThe implementation of RPCs is discussed by [Birrell and Nelson (1984)].\n[Staunstrup (1982)] discusses procedure calls versus message-passing com-\nmunication. [Harold (2005)] provides coverage of socket programming in\nJava.\n[Hart (2005)] and [Robbins and Robbins (2003)] cover pipes in Windows",
  "and UNIX systems, respectively.\nBibliography\n[Baumann et al. (2009)]\nA. Baumann, P. Barham, P.-E. Dagand, T. Harris,\nR. Isaacs, P. Simon, T. Roscoe, A. Sch¨upbach, and A. Singhania, “The multikernel:\na new OS architecture for scalable multicore systems” (2009), pages 29–44.\n[Birrell and Nelson (1984)]\nA. D. Birrell and B. J. Nelson, “Implementing\nRemote Procedure Calls”, ACM Transactions on Computer Systems, Volume 2,\nNumber 1 (1984), pages 39–59.\n[Harold (2005)]\nE. R. Harold, Java Network Programming, Third Edition, O’Reilly\n& Associates (2005).\n[Hart (2005)]\nJ. M. Hart, Windows System Programming, Third Edition, Addison-\nWesley (2005).\n[Holland and Seltzer (2011)]\nD. Holland and M. Seltzer, “Multicore OSes: look-",
  "D. Holland and M. Seltzer, “Multicore OSes: look-\ning forward from 1991, er, 2011”, Proceedings of the 13th USENIX conference on\nHot topics in operating systems (2011), pages 33–33.\n[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[Robbins and Robbins (2003)]\nK. Robbins and S. Robbins, Unix Systems Pro-\ngramming: Communication, Concurrency and Threads, Second Edition, Prentice\nHall (2003).\n[Russinovich and Solomon (2009)]\nM. E. Russinovich and D. A. Solomon, Win-\ndows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition,\nMicrosoft Press (2009).\n[Staunstrup (1982)]\nJ. Staunstrup, “Message Passing Communication Versus\nProcedure Call Communication”, Software—Practice and Experience, Volume 12,\nNumber 3 (1982), pages 223–234.",
  "Number 3 (1982), pages 223–234.\n[Vahalia (1996)]\nU. Vahalia, Unix Internals: The New Frontiers, Prentice Hall\n(1996). 4\nC H A P T E R\nThreads\nThe process model introduced in Chapter 3 assumed that a process was\nan executing program with a single thread of control. Virtually all modern\noperating systems, however, provide features enabling a process to contain\nmultiple threads of control. In this chapter, we introduce many concepts\nassociated with multithreaded computer systems, including a discussion of\nthe APIs for the Pthreads, Windows, and Java thread libraries. We look at a\nnumber of issues related to multithreaded programming and its effect on the\ndesign of operating systems. Finally, we explore how the Windows and Linux\noperating systems support threads at the kernel level.",
  "CHAPTER OBJECTIVES\n• To introduce the notion of a thread—a fundamental unit of CPU utilization\nthat forms the basis of multithreaded computer systems.\n• To discuss the APIs for the Pthreads, Windows, and Java thread libraries.\n• To explore several strategies that provide implicit threading.\n• To examine issues related to multithreaded programming.\n• To cover operating system support for threads in Windows and Linux.\n4.1\nOverview\nA thread is a basic unit of CPU utilization; it comprises a thread ID, a program\ncounter, a register set, and a stack. It shares with other threads belonging\nto the same process its code section, data section, and other operating-system\nresources, such as open ﬁles and signals. A traditional (or heavyweight) process",
  "has a single thread of control. If a process has multiple threads of control, it\ncan perform more than one task at a time. Figure 4.1 illustrates the difference\nbetween a traditional single-threaded process and a multithreaded process.\n4.1.1\nMotivation\nMost software applications that run on modern computers are multithreaded.\nAn application typically is implemented as a separate process with several\n163 164\nChapter 4\nThreads\nregisters\ncode\ndata\nfiles\nstack\nregisters\nregisters\nregisters\ncode\ndata\nfiles\nstack\nstack\nstack\nthread\nthread\nsingle-threaded process\nmultithreaded process\nFigure 4.1\nSingle-threaded and multithreaded processes.\nthreads of control. A web browser might have one thread display images or\ntext while another thread retrieves data from the network, for example. A",
  "word processor may have a thread for displaying graphics, another thread for\nresponding to keystrokes from the user, and a third thread for performing\nspelling and grammar checking in the background. Applications can also\nbe designed to leverage processing capabilities on multicore systems. Such\napplications can perform several CPU-intensive tasks in parallel across the\nmultiple computing cores.\nIn certain situations, a single application may be required to perform\nseveral similar tasks. For example, a web server accepts client requests for\nweb pages, images, sound, and so forth. A busy web server may have several\n(perhaps thousands of) clients concurrently accessing it. If the web server ran\nas a traditional single-threaded process, it would be able to service only one",
  "client at a time, and a client might have to wait a very long time for its request\nto be serviced.\nOne solution is to have the server run as a single process that accepts\nrequests. When the server receives a request, it creates a separate process\nto service that request. In fact, this process-creation method was in common\nuse before threads became popular. Process creation is time consuming and\nresource intensive, however. If the new process will perform the same tasks as\nthe existing process, why incur all that overhead? It is generally more efﬁcient\nto use one process that contains multiple threads. If the web-server process is\nmultithreaded, the server will create a separate thread that listens for client\nrequests. When a request is made, rather than creating another process, the",
  "server creates a new thread to service the request and resume listening for\nadditional requests. This is illustrated in Figure 4.2.\nThreads also play a vital role in remote procedure call (RPC) systems. Recall\nfrom Chapter 3 that RPCs allow interprocess communication by providing a\ncommunication mechanism similar to ordinary function or procedure calls.\nTypically, RPC servers are multithreaded. When a server receives a message, it 4.1\nOverview\n165\nclient\n(1) request\n(2) create new\nthread to service\nthe request\n(3) resume listening\nfor additional\nclient requests\nserver\nthread\nFigure 4.2\nMultithreaded server architecture.\nservices the message using a separate thread. This allows the server to service\nseveral concurrent requests.",
  "several concurrent requests.\nFinally, most operating-system kernels are now multithreaded. Several\nthreads operate in the kernel, and each thread performs a speciﬁc task, such\nas managing devices, managing memory, or interrupt handling. For example,\nSolaris has a set of threads in the kernel speciﬁcally for interrupt handling;\nLinux uses a kernel thread for managing the amount of free memory in the\nsystem.\n4.1.2\nBeneﬁts\nThe beneﬁts of multithreaded programming can be broken down into four\nmajor categories:\n1. Responsiveness. Multithreading an interactive application may allow\na program to continue running even if part of it is blocked or is\nperforming a lengthy operation, thereby increasing responsiveness to\nthe user. This quality is especially useful in designing user interfaces. For",
  "instance, consider what happens when a user clicks a button that results\nin the performance of a time-consuming operation. A single-threaded\napplication would be unresponsive to the user until the operation had\ncompleted. In contrast, if the time-consuming operation is performed in\na separate thread, the application remains responsive to the user.\n2. Resource sharing. Processes can only share resources through techniques\nsuch as shared memory and message passing. Such techniques must\nbe explicitly arranged by the programmer. However, threads share the\nmemory and the resources of the process to which they belong by default.\nThe beneﬁt of sharing code and data is that it allows an application to\nhave several different threads of activity within the same address space.",
  "3. Economy. Allocating memory and resources for process creation is costly.\nBecause threads share the resources of the process to which they belong,\nit is more economical to create and context-switch threads. Empirically\ngauging the difference in overhead can be difﬁcult, but in general it is\nsigniﬁcantly more time consuming to create and manage processes than\nthreads. In Solaris, for example, creating a process is about thirty times 166\nChapter 4\nThreads\nT1\nT2\nT3\nT4\nT1\nT2\nT3\nT4\nT1\nsingle core\ntime\n…\nFigure 4.3\nConcurrent execution on a single-core system.\nslower than is creating a thread, and context switching is about ﬁve times\nslower.\n4. Scalability. The beneﬁts of multithreading can be even greater in a\nmultiprocessor architecture, where threads may be running in parallel",
  "on different processing cores. A single-threaded process can run on only\none processor, regardless how many are available. We explore this issue\nfurther in the following section.\n4.2\nMulticore Programming\nEarlier in the history of computer design, in response to the need for more\ncomputing performance, single-CPU systems evolved into multi-CPU systems.\nA more recent, similar trend in system design is to place multiple computing\ncores on a single chip. Each core appears as a separate processor to the\noperating system (Section 1.3.2). Whether the cores appear across CPU chips or\nwithin CPU chips, we call these systems multicore or multiprocessor systems.\nMultithreaded programming provides a mechanism for more efﬁcient use",
  "of these multiple computing cores and improved concurrency. Consider an\napplication with four threads. On a system with a single computing core,\nconcurrency merely means that the execution of the threads will be interleaved\nover time (Figure 4.3), because the processing core is capable of executing only\none thread at a time. On a system with multiple cores, however, concurrency\nmeans that the threads can run in parallel, because the system can assign a\nseparate thread to each core (Figure 4.4).\nNotice the distinction between parallelism and concurrency in this discus-\nsion. A system is parallel if it can perform more than one task simultaneously.\nIn contrast, a concurrent system supports more than one task by allowing all",
  "the tasks to make progress. Thus, it is possible to have concurrency without\nparallelism. Before the advent of SMP and multicore architectures, most com-\nputer systems had only a single processor. CPU schedulers were designed to\nprovide the illusion of parallelism by rapidly switching between processes in\nT1\nT3\nT1\nT3\nT1\ncore 1\nT2\nT4\nT2\nT4\nT2\ncore 2\ntime\n…\n…\nFigure 4.4\nParallel execution on a multicore system. 4.2\nMulticore Programming\n167\nAMDAHL’S LAW\nAmdahl’s Law is a formula that identiﬁes potential performance gains from\nadding additional computing cores to an application that has both serial\n(nonparallel) and parallel components. If S is the portion of the application\nthat must be performed serially on a system with N processing cores, the\nformula appears as follows:\nspeedup ≤\n1",
  "formula appears as follows:\nspeedup ≤\n1\nS + (1−S)\nN\nAs an example, assume we have an application that is 75 percent parallel and\n25 percent serial. If we run this application on a system with two processing\ncores, we can get a speedup of 1.6 times. If we add two additional cores (for\na total of four), the speedup is 2.28 times.\nOne interesting fact about Amdahl’s Law is that as N approaches inﬁnity,\nthe speedup converges to 1/S. For example, if 40 percent of an application\nis performed serially, the maximum speedup is 2.5 times, regardless of\nthe number of processing cores we add. This is the fundamental principle\nbehind Amdahl’s Law: the serial portion of an application can have a\ndisproportionate effect on the performance we gain by adding additional\ncomputing cores.",
  "computing cores.\nSome argue that Amdahl’s Law does not take into account the hardware\nperformance enhancements used in the design of contemporary multicore\nsystems. Such arguments suggest Amdahl’s Law may cease to be applicable\nas the number of processing cores continues to increase on modern computer\nsystems.\nthe system, thereby allowing each process to make progress. Such processes\nwere running concurrently, but not in parallel.\nAs systems have grown from tens of threads to thousands of threads, CPU\ndesigners have improved system performance by adding hardware to improve\nthread performance. Modern Intel CPUs frequently support two threads per\ncore, while the Oracle T4 CPU supports eight threads per core. This support",
  "means that multiple threads can be loaded into the core for fast switching.\nMulticore computers will no doubt continue to increase in core counts and\nhardware thread support.\n4.2.1\nProgramming Challenges\nThe trend towards multicore systems continues to place pressure on system\ndesigners and application programmers to make better use of the multiple\ncomputing cores. Designers of operating systems must write scheduling\nalgorithms that use multiple processing cores to allow the parallel execution\nshown in Figure 4.4. For application programmers, the challenge is to modify\nexisting programs as well as design new programs that are multithreaded.\nIn general, ﬁve areas present challenges in programming for multicore\nsystems: 168\nChapter 4\nThreads",
  "systems: 168\nChapter 4\nThreads\n1. Identifying tasks. This involves examining applications to ﬁnd areas\nthat can be divided into separate, concurrent tasks. Ideally, tasks are\nindependent of one another and thus can run in parallel on individual\ncores.\n2. Balance. While identifying tasks that can run in parallel, programmers\nmust also ensure that the tasks perform equal work of equal value. In\nsome instances, a certain task may not contribute as much value to the\noverall process as other tasks. Using a separate execution core to run that\ntask may not be worth the cost.\n3. Data splitting. Just as applications are divided into separate tasks, the\ndata accessed and manipulated by the tasks must be divided to run on\nseparate cores.",
  "separate cores.\n4. Data dependency. The data accessed by the tasks must be examined for\ndependencies between two or more tasks. When one task depends on\ndata from another, programmers must ensure that the execution of the\ntasks is synchronized to accommodate the data dependency. We examine\nsuch strategies in Chapter 5.\n5. Testing and debugging. When a program is running in parallel on\nmultiple cores, many different execution paths are possible. Testing and\ndebugging such concurrent programs is inherently more difﬁcult than\ntesting and debugging single-threaded applications.\nBecause of these challenges, many software developers argue that the advent of\nmulticore systems will require an entirely new approach to designing software",
  "systems in the future. (Similarly, many computer science educators believe that\nsoftware development must be taught with increased emphasis on parallel\nprogramming.)\n4.2.2\nTypes of Parallelism\nIn general, there are two types of parallelism: data parallelism and task\nparallelism. Data parallelism focuses on distributing subsets of the same data\nacross multiple computing cores and performing the same operation on each\ncore. Consider, for example, summing the contents of an array of size N. On a\nsingle-core system, one thread would simply sum the elements [0] . . . [N −1].\nOn a dual-core system, however, thread A, running on core 0, could sum the\nelements [0] . . . [N/2 −1] while thread B, running on core 1, could sum the",
  "elements [N/2] . . . [N −1]. The two threads would be running in parallel on\nseparate computing cores.\nTask parallelism involves distributing not data but tasks (threads) across\nmultiple computing cores. Each thread is performing a unique operation.\nDifferent threads may be operating on the same data, or they may be operating\non different data. Consider again our example above. In contrast to that\nsituation, an example of task parallelism might involve two threads, each\nperforming a unique statistical operation on the array of elements. The threads\nagain are operating in parallel on separate computing cores, but each is\nperforming a unique operation. 4.3\nMultithreading Models\n169\nFundamentally, then, data parallelism involves the distribution of data",
  "across multiple cores and task parallelism on the distribution of tasks across\nmultiple cores. In practice, however, few applications strictly follow either data\nor task parallelism. In most instances, applications use a hybrid of these two\nstrategies.\n4.3\nMultithreading Models\nOur discussion so far has treated threads in a generic sense. However, support\nfor threads may be provided either at the user level, for user threads, or by the\nkernel, for kernel threads. User threads are supported above the kernel and\nare managed without kernel support, whereas kernel threads are supported\nand managed directly by the operating system. Virtually all contemporary\noperating systems—including Windows, Linux, Mac OS X, and Solaris—\nsupport kernel threads.",
  "support kernel threads.\nUltimately, a relationship must exist between user threads and kernel\nthreads. In this section, we look at three common ways of establishing such a\nrelationship: the many-to-one model, the one-to-one model, and the many-to-\nmany model.\n4.3.1\nMany-to-One Model\nThe many-to-one model (Figure 4.5) maps many user-level threads to one\nkernel thread. Thread management is done by the thread library in user space,\nso it is efﬁcient (we discuss thread libraries in Section 4.4). However, the entire\nprocess will block if a thread makes a blocking system call. Also, because only\none thread can access the kernel at a time, multiple threads are unable to run in\nparallel on multicore systems. Green threads—a thread library available for",
  "Solaris systems and adopted in early versions of Java—used the many-to-one\nmodel. However, very few systems continue to use the model because of its\ninability to take advantage of multiple processing cores.\nuser thread\nkernel thread\nk\nFigure 4.5\nMany-to-one model. 170\nChapter 4\nThreads\nuser thread\nkernel thread\nk\nk\nk\nk\nFigure 4.6\nOne-to-one model.\n4.3.2\nOne-to-One Model\nThe one-to-one model (Figure 4.6) maps each user thread to a kernel thread. It\nprovides more concurrency than the many-to-one model by allowing another\nthread to run when a thread makes a blocking system call. It also allows\nmultiple threads to run in parallel on multiprocessors. The only drawback to\nthis model is that creating a user thread requires creating the corresponding",
  "kernel thread. Because the overhead of creating kernel threads can burden the\nperformance of an application, most implementations of this model restrict the\nnumber of threads supported by the system. Linux, along with the family of\nWindows operating systems, implement the one-to-one model.\n4.3.3\nMany-to-Many Model\nThe many-to-many model (Figure 4.7) multiplexes many user-level threads to\na smaller or equal number of kernel threads. The number of kernel threads\nmay be speciﬁc to either a particular application or a particular machine (an\napplication may be allocated more kernel threads on a multiprocessor than on\na single processor).\nLet’s consider the effect of this design on concurrency. Whereas the many-\nto-one model allows the developer to create as many user threads as she wishes,",
  "it does not result in true concurrency, because the kernel can schedule only\none thread at a time. The one-to-one model allows greater concurrency, but the\ndeveloper has to be careful not to create too many threads within an application\n(and in some instances may be limited in the number of threads she can\nuser thread\nkernel thread\nk\nk\nk\nFigure 4.7\nMany-to-many model. 4.4\nThread Libraries\n171\nuser thread\nkernel thread\nk\nk\nk\nk\nFigure 4.8\nTwo-level model.\ncreate). The many-to-many model suffers from neither of these shortcomings:\ndevelopers can create as many user threads as necessary, and the corresponding\nkernel threads can run in parallel on a multiprocessor. Also, when a thread\nperforms a blocking system call, the kernel can schedule another thread for\nexecution.",
  "execution.\nOne variation on the many-to-many model still multiplexes many user-\nlevel threads to a smaller or equal number of kernel threads but also allows a\nuser-level thread to be bound to a kernel thread. This variation is sometimes\nreferred to as the two-level model (Figure 4.8). The Solaris operating system\nsupported the two-level model in versions older than Solaris 9. However,\nbeginning with Solaris 9, this system uses the one-to-one model.\n4.4\nThread Libraries\nA thread library provides the programmer with an API for creating and\nmanaging threads. There are two primary ways of implementing a thread\nlibrary. The ﬁrst approach is to provide a library entirely in user space with no\nkernel support. All code and data structures for the library exist in user space.",
  "This means that invoking a function in the library results in a local function\ncall in user space and not a system call.\nThe second approach is to implement a kernel-level library supported\ndirectly by the operating system. In this case, code and data structures for\nthe library exist in kernel space. Invoking a function in the API for the library\ntypically results in a system call to the kernel.\nThree main thread libraries are in use today: POSIX Pthreads, Windows, and\nJava. Pthreads, the threads extension of the POSIX standard, may be provided\nas either a user-level or a kernel-level library. The Windows thread library\nis a kernel-level library available on Windows systems. The Java thread API\nallows threads to be created and managed directly in Java programs. However,",
  "because in most instances the JVM is running on top of a host operating system,\nthe Java thread API is generally implemented using a thread library available\non the host system. This means that on Windows systems, Java threads are\ntypically implemented using the Windows API; UNIX and Linux systems often\nuse Pthreads. 172\nChapter 4\nThreads\nFor POSIX and Windows threading, any data declared globally—that is,\ndeclared outside of any function—are shared among all threads belonging to\nthe same process. Because Java has no notion of global data, access to shared\ndata must be explicitly arranged between threads. Data declared local to a\nfunction are typically stored on the stack. Since each thread has its own stack,\neach thread has its own copy of local data.",
  "each thread has its own copy of local data.\nIn the remainder of this section, we describe basic thread creation using\nthese three thread libraries. As an illustrative example, we design a multi-\nthreaded program that performs the summation of a non-negative integer in a\nseparate thread using the well-known summation function:\nsum =\nN\n\"\ni=0\ni\nFor example, if N were 5, this function would represent the summation of\nintegers from 0 to 5, which is 15. Each of the three programs will be run with\nthe upper bounds of the summation entered on the command line. Thus, if the\nuser enters 8, the summation of the integer values from 0 to 8 will be output.\nBefore we proceed with our examples of thread creation, we introduce\ntwo general strategies for creating multiple threads: asynchronous threading",
  "and synchronous threading. With asynchronous threading, once the parent\ncreates a child thread, the parent resumes its execution, so that the parent\nand child execute concurrently. Each thread runs independently of every other\nthread, and the parent thread need not know when its child terminates. Because\nthe threads are independent, there is typically little data sharing between\nthreads. Asynchronous threading is the strategy used in the multithreaded\nserver illustrated in Figure 4.2.\nSynchronous threading occurs when the parent thread creates one or more\nchildren and then must wait for all of its children to terminate before it resumes\n—the so-called fork-join strategy. Here, the threads created by the parent\nperform work concurrently, but the parent cannot continue until this work",
  "has been completed. Once each thread has ﬁnished its work, it terminates\nand joins with its parent. Only after all of the children have joined can the\nparent resume execution. Typically, synchronous threading involves signiﬁcant\ndata sharing among threads. For example, the parent thread may combine the\nresults calculated by its various children. All of the following examples use\nsynchronous threading.\n4.4.1\nPthreads\nPthreads refers to the POSIX standard (IEEE 1003.1c) deﬁning an API for thread\ncreation and synchronization. This is a speciﬁcation for thread behavior,\nnot an implementation. Operating-system designers may implement the\nspeciﬁcation in any way they wish. Numerous systems implement the Pthreads\nspeciﬁcation; most are UNIX-type systems, including Linux, Mac OS X, and",
  "Solaris. Although Windows doesn’t support Pthreads natively, some third-\nparty implementations for Windows are available.\nThe C program shown in Figure 4.9 demonstrates the basic Pthreads API for\nconstructing a multithreaded program that calculates the summation of a non-\nnegative integer in a separate thread. In a Pthreads program, separate threads 4.4\nThread Libraries\n173\n#include <pthread.h>\n#include <stdio.h>\nint sum; /* this data is shared by the thread(s) */\nvoid *runner(void *param); /* threads call this function */\nint main(int argc, char *argv[])\n{\npthread t tid; /* the thread identifier */\npthread attr t attr; /* set of thread attributes */\nif (argc != 2) {\nfprintf(stderr,\"usage: a.out <integer value>\\n\");\nreturn -1;\n}\nif (atoi(argv[1]) < 0) {",
  "return -1;\n}\nif (atoi(argv[1]) < 0) {\nfprintf(stderr,\"%d must be >= 0\\n\",atoi(argv[1]));\nreturn -1;\n}\n/* get the default attributes */\npthread attr init(&attr);\n/* create the thread */\npthread create(&tid,&attr,runner,argv[1]);\n/* wait for the thread to exit */\npthread join(tid,NULL);\nprintf(\"sum = %d\\n\",sum);\n}\n/* The thread will begin control in this function */\nvoid *runner(void *param)\n{\nint i, upper = atoi(param);\nsum = 0;\nfor (i = 1; i <= upper; i++)\nsum += i;\npthread exit(0);\n}\nFigure 4.9\nMultithreaded C program using the Pthreads API.\nbegin execution in a speciﬁed function. In Figure 4.9, this is the runner()\nfunction. When this program begins, a single thread of control begins in\nmain(). After some initialization, main() creates a second thread that begins",
  "control in the runner() function. Both threads share the global data sum.\nLet’s look more closely at this program. All Pthreads programs must\ninclude the pthread.h header ﬁle. The statement pthread t tid declares 174\nChapter 4\nThreads\n#define NUM THREADS 10\n/* an array of threads to be joined upon */\npthread t workers[NUM THREADS];\nfor (int i = 0; i < NUM THREADS; i++)\npthread join(workers[i], NULL);\nFigure 4.10\nPthread code for joining ten threads.\nthe identiﬁer for the thread we will create. Each thread has a set of attributes,\nincluding stack size and scheduling information. The pthread attr t attr\ndeclaration represents the attributes for the thread. We set the attributes in the\nfunction call pthread attr init(&attr). Because we did not explicitly set",
  "any attributes, we use the default attributes provided. (In Chapter 6, we discuss\nsome of the scheduling attributes provided by the Pthreads API.) A separate\nthread is created with the pthread create() function call. In addition to\npassing the thread identiﬁer and the attributes for the thread, we also pass the\nname of the function where the new thread will begin execution—in this case,\nthe runner() function. Last, we pass the integer parameter that was provided\non the command line, argv[1].\nAt this point, the program has two threads: the initial (or parent) thread\nin main() and the summation (or child) thread performing the summation\noperation in the runner() function. This program follows the fork-join strategy\ndescribed earlier: after creating the summation thread, the parent thread",
  "will wait for it to terminate by calling the pthread join() function. The\nsummation thread will terminate when it calls the function pthread exit().\nOnce the summation thread has returned, the parent thread will output the\nvalue of the shared data sum.\nThis example program creates only a single thread. With the growing\ndominance of multicore systems, writing programs containing several threads\nhas become increasingly common. A simple method for waiting on several\nthreads using the pthread join() function is to enclose the operation within\na simple for loop. For example, you can join on ten threads using the Pthread\ncode shown in Figure 4.10.\n4.4.2\nWindows Threads\nThe technique for creating threads using the Windows thread library is similar",
  "to the Pthreads technique in several ways. We illustrate the Windows thread\nAPI in the C program shown in Figure 4.11. Notice that we must include the\nwindows.h header ﬁle when using the Windows API.\nJust as in the Pthreads version shown in Figure 4.9, data shared by the\nseparate threads—in this case, Sum—are declared globally (the DWORD data\ntype is an unsigned 32-bit integer). We also deﬁne the Summation() function\nthat is to be performed in a separate thread. This function is passed a pointer\nto a void, which Windows deﬁnes as LPVOID. The thread performing this\nfunction sets the global data Sum to the value of the summation from 0 to the\nparameter passed to Summation(). 4.4\nThread Libraries\n175\n#include <windows.h>\n#include <stdio.h>\nDWORD Sum; /* data is shared by the thread(s) */",
  "DWORD Sum; /* data is shared by the thread(s) */\n/* the thread runs in this separate function */\nDWORD WINAPI Summation(LPVOID Param)\n{\nDWORD Upper = *(DWORD*)Param;\nfor (DWORD i = 0; i <= Upper; i++)\nSum += i;\nreturn 0;\n}\nint main(int argc, char *argv[])\n{\nDWORD ThreadId;\nHANDLE ThreadHandle;\nint Param;\nif (argc != 2) {\nfprintf(stderr,\"An integer parameter is required\\n\");\nreturn -1;\n}\nParam = atoi(argv[1]);\nif (Param < 0) {\nfprintf(stderr,\"An integer >= 0 is required\\n\");\nreturn -1;\n}\n/* create the thread */\nThreadHandle = CreateThread(\nNULL, /* default security attributes */\n0, /* default stack size */\nSummation, /* thread function */\n&Param, /* parameter to thread function */\n0, /* default creation flags */\n&ThreadId); /* returns the thread identifier */\nif (ThreadHandle != NULL) {",
  "if (ThreadHandle != NULL) {\n/* now wait for the thread to finish */\nWaitForSingleObject(ThreadHandle,INFINITE);\n/* close the thread handle */\nCloseHandle(ThreadHandle);\nprintf(\"sum = %d\\n\",Sum);\n}\n}\nFigure 4.11\nMultithreaded C program using the Windows API. 176\nChapter 4\nThreads\nThreads are created in the Windows API using the CreateThread()\nfunction, and—just as in Pthreads—a set of attributes for the thread is passed\nto this function. These attributes include security information, the size of the\nstack, and a ﬂag that can be set to indicate if the thread is to start in a suspended\nstate. In this program, we use the default values for these attributes. (The\ndefault values do not initially set the thread to a suspended state and instead",
  "make it eligible to be run by the CPU scheduler.) Once the summation thread\nis created, the parent must wait for it to complete before outputting the value\nof Sum, as the value is set by the summation thread. Recall that the Pthread\nprogram (Figure 4.9) had the parent thread wait for the summation thread\nusing the pthread join() statement. We perform the equivalent of this in the\nWindows API using the WaitForSingleObject() function, which causes the\ncreating thread to block until the summation thread has exited.\nIn situations that require waiting for multiple threads to complete, the\nWaitForMultipleObjects() function is used. This function is passed four\nparameters:\n1. The number of objects to wait for\n2. A pointer to the array of objects",
  "2. A pointer to the array of objects\n3. A ﬂag indicating whether all objects have been signaled\n4. A timeout duration (or INFINITE)\nFor example, if THandles is an array of thread HANDLE objects of size N, the\nparent thread can wait for all its child threads to complete with this statement:\nWaitForMultipleObjects(N, THandles, TRUE, INFINITE);\n4.4.3\nJava Threads\nThreads are the fundamental model of program execution in a Java program,\nand the Java language and its API provide a rich set of features for the creation\nand management of threads. All Java programs comprise at least a single thread\nof control—even a simple Java program consisting of only a main() method\nruns as a single thread in the JVM. Java threads are available on any system that",
  "provides a JVM including Windows, Linux, and Mac OS X. The Java thread API\nis available for Android applications as well.\nThere are two techniques for creating threads in a Java program. One\napproach is to create a new class that is derived from the Thread class and\nto override its run() method. An alternative—and more commonly used—\ntechnique is to deﬁne a class that implements the Runnable interface. The\nRunnable interface is deﬁned as follows:\npublic interface Runnable\n{\npublic abstract void run();\n}\nWhen a class implements Runnable, it must deﬁne a run() method. The code\nimplementing the run() method is what runs as a separate thread. 4.5\nImplicit Threading\n177\nFigure 4.12 shows the Java version of a multithreaded program that",
  "determines the summation of a non-negative integer. The Summation class\nimplements the Runnable interface. Thread creation is performed by creating\nan object instance of the Thread class and passing the constructor a Runnable\nobject.\nCreating a Thread object does not speciﬁcally create the new thread; rather,\nthe start() method creates the new thread. Calling the start() method for\nthe new object does two things:\n1. It allocates memory and initializes a new thread in the JVM.\n2. It calls the run() method, making the thread eligible to be run by the JVM.\n(Note again that we never call the run() method directly. Rather, we call\nthe start() method, and it calls the run() method on our behalf.)\nWhen the summation program runs, the JVM creates two threads. The ﬁrst",
  "is the parent thread, which starts execution in the main() method. The second\nthread is created when the start() method on the Thread object is invoked.\nThis child thread begins execution in the run() method of the Summation class.\nAfter outputting the value of the summation, this thread terminates when it\nexits from its run() method.\nData sharing between threads occurs easily in Windows and Pthreads, since\nshared data are simply declared globally. As a pure object-oriented language,\nJava has no such notion of global data. If two or more threads are to share\ndata in a Java program, the sharing occurs by passing references to the shared\nobject to the appropriate threads. In the Java program shown in Figure 4.12,\nthe main thread and the summation thread share the object instance of the Sum",
  "class. This shared object is referenced through the appropriate getSum() and\nsetSum() methods. (You might wonder why we don’t use an Integer object\nrather than designing a new sum class. The reason is that the Integer class is\nimmutable—that is, once its value is set, it cannot change.)\nRecall that the parent threads in the Pthreads and Windows libraries\nuse pthread join() and WaitForSingleObject() (respectively) to wait\nfor the summation threads to ﬁnish before proceeding. The join() method\nin Java provides similar functionality. (Notice that join() can throw an\nInterruptedException, which we choose to ignore.) If the parent must wait\nfor several threads to ﬁnish, the join() method can be enclosed in a for loop\nsimilar to that shown for Pthreads in Figure 4.10.\n4.5\nImplicit Threading",
  "4.5\nImplicit Threading\nWith the continued growth of multicore processing, applications containing\nhundreds—or even thousands—of threads are looming on the horizon.\nDesigning such applications is not a trivial undertaking: programmers must\naddressnotonlythe challengesoutlined inSection4.2but additional difﬁculties\nas well. These difﬁculties, which relate to program correctness, are covered in\nChapters 5 and 7.\nOne way to address these difﬁculties and better support the design of\nmultithreaded applications is to transfer the creation and management of 178\nChapter 4\nThreads\nclass Sum\n{\nprivate int sum;\npublic int getSum() {\nreturn sum;\n}\npublic void setSum(int sum) {\nthis.sum = sum;\n}\n}\nclass Summation implements Runnable\n{\nprivate int upper;\nprivate Sum sumValue;",
  "{\nprivate int upper;\nprivate Sum sumValue;\npublic Summation(int upper, Sum sumValue) {\nthis.upper = upper;\nthis.sumValue = sumValue;\n}\npublic void run() {\nint sum = 0;\nfor (int i = 0; i <= upper; i++)\nsum += i;\nsumValue.setSum(sum);\n}\n}\npublic class Driver\n{\npublic static void main(String[] args) {\nif (args.length > 0) {\nif (Integer.parseInt(args[0]) < 0)\nSystem.err.println(args[0] + \" must be >= 0.\");\nelse {\nSum sumObject = new Sum();\nint upper = Integer.parseInt(args[0]);\nThread thrd = new Thread(new Summation(upper, sumObject));\nthrd.start();\ntry {\nthrd.join();\nSystem.out.println\n(\"The sum of \"+upper+\" is \"+sumObject.getSum());\n} catch (InterruptedException ie) { }\n}\n}\nelse\nSystem.err.println(\"Usage: Summation <integer value>\"); }\n}\nFigure 4.12",
  "}\nFigure 4.12\nJava program for the summation of a non-negative integer. 4.5\nImplicit Threading\n179\nTHE JVM AND THE HOST OPERATING SYSTEM\nThe JVM is typically implemented on top of a host operating system (see\nFigure 16.10). This setup allows the JVM to hide the implementation details\nof the underlying operating system and to provide a consistent, abstract\nenvironment that allows Java programs to operate on any platform that\nsupports a JVM. The speciﬁcation for the JVM does not indicate how Java\nthreads are to be mapped to the underlying operating system, instead leaving\nthat decision to the particular implementation of the JVM. For example, the\nWindows XP operating system uses the one-to-one model; therefore, each\nJava thread for a JVM running on such a system maps to a kernel thread. On",
  "operating systems that use the many-to-many model (such as Tru64 UNIX), a\nJava thread is mapped according to the many-to-many model. Solaris initially\nimplemented the JVMusing the many-to-one model (the green threads library,\nmentioned earlier). Later releases of the JVM were implemented using the\nmany-to-many model. Beginning with Solaris 9, Java threads were mapped\nusing the one-to-one model. In addition, there may be a relationship between\nthe Java thread library and the thread library on the host operating system.\nFor example, implementations of a JVM for the Windows family of operating\nsystems might use the Windows API when creating Java threads; Linux,\nSolaris, and Mac OS X systems might use the Pthreads API.\nthreading from application developers to compilers and run-time libraries.",
  "This strategy, termed implicit threading, is a popular trend today. In this\nsection, we explore three alternative approaches for designing multithreaded\nprograms that can take advantage of multicore processors through implicit\nthreading.\n4.5.1\nThread Pools\nIn Section 4.1, we described a multithreaded web server. In this situation,\nwhenever the server receives a request, it creates a separate thread to service\nthe request. Whereas creating a separate thread is certainly superior to creating\na separate process, a multithreaded server nonetheless has potential problems.\nThe ﬁrst issue concerns the amount of time required to create the thread,\ntogether with the fact that the thread will be discarded once it has completed",
  "its work. The second issue is more troublesome. If we allow all concurrent\nrequests to be serviced in a new thread, we have not placed a bound on the\nnumber of threads concurrently active in the system. Unlimited threads could\nexhaust system resources, such as CPU time or memory. One solution to this\nproblem is to use a thread pool.\nThe general idea behind a thread pool is to create a number of threads at\nprocess startup and place them into a pool, where they sit and wait for work.\nWhen a server receives a request, it awakens a thread from this pool—if one\nis available—and passes it the request for service. Once the thread completes\nits service, it returns to the pool and awaits more work. If the pool contains no\navailable thread, the server waits until one becomes free. 180\nChapter 4",
  "Chapter 4\nThreads\nThread pools offer these beneﬁts:\n1. Servicing a request with an existing thread is faster than waiting to create\na thread.\n2. A thread pool limits the number of threads that exist at any one point.\nThis is particularly important on systems that cannot support a large\nnumber of concurrent threads.\n3. Separating the task to be performed from the mechanics of creating the\ntask allows us to use different strategies for running the task. For example,\nthe task could be scheduled to execute after a time delay or to execute\nperiodically.\nThe number of threads in the pool can be set heuristically based on factors\nsuch as the number of CPUs in the system, the amount of physical memory,\nand the expected number of concurrent client requests. More sophisticated",
  "thread-pool architectures can dynamically adjust the number of threads in the\npool according to usage patterns. Such architectures provide the further beneﬁt\nof having a smaller pool—thereby consuming less memory—when the load\non the system is low. We discuss one such architecture, Apple’s Grand Central\nDispatch, later in this section.\nThe Windows API provides several functions related to thread pools. Using\nthe thread pool API is similar to creating a thread with the Thread Create()\nfunction, as described in Section 4.4.2. Here, a function that is to run as a\nseparate thread is deﬁned. Such a function may appear as follows:\nDWORD WINAPI PoolFunction(AVOID Param) {\n/*\n* this function runs as a separate thread.\n*/\n}",
  "* this function runs as a separate thread.\n*/\n}\nA pointer to PoolFunction() is passed to one of the functions in the thread\npool API, and a thread from the pool executes this function. One such member\nin the thread pool API is the QueueUserWorkItem() function, which is passed\nthree parameters:\n• LPTHREAD START ROUTINE Function—a pointer to the function that is to\nrun as a separate thread\n• PVOID Param—the parameter passed to Function\n• ULONG Flags—ﬂags indicating how the thread pool is to create and\nmanage execution of the thread\nAn example of invoking a function is the following:\nQueueUserWorkItem(&PoolFunction, NULL, 0);\nThis causes a thread from the thread pool to invoke PoolFunction() on behalf\nof the programmer. In this instance, we pass no parameters to PoolFunc- 4.5",
  "Implicit Threading\n181\ntion(). Because we specify 0 as a ﬂag, we provide the thread pool with no\nspecial instructions for thread creation.\nOther members in the Windows thread pool API include utilities that invoke\nfunctions at periodic intervals or when an asynchronous I/O request completes.\nThe java.util.concurrent package in the Java API provides a thread-pool\nutility as well.\n4.5.2\nOpenMP\nOpenMP is a set of compiler directives as well as an API for programs written\nin C, C++, or FORTRAN that provides support for parallel programming in\nshared-memory environments. OpenMP identiﬁes parallel regions as blocks\nof code that may run in parallel. Application developers insert compiler\ndirectives into their code at parallel regions, and these directives instruct the",
  "OpenMP run-time library to execute the region in parallel. The following C\nprogram illustrates a compiler directive above the parallel region containing\nthe printf() statement:\n#include <omp.h>\n#include <stdio.h>\nint main(int argc, char *argv[])\n{\n/* sequential code */\n#pragma omp parallel\n{\nprintf(\"I am a parallel region.\");\n}\n/* sequential code */\nreturn 0;\n}\nWhen OpenMP encounters the directive\n#pragma omp parallel\nit creates as many threads are there are processing cores in the system. Thus, for\na dual-core system, two threads are created, for a quad-core system, four are\ncreated; and so forth. All the threads then simultaneously execute the parallel\nregion. As each thread exits the parallel region, it is terminated.",
  "OpenMP provides several additional directives for running code regions\nin parallel, including parallelizing loops. For example, assume we have two\narrays a and b of size N. We wish to sum their contents and place the results\nin array c. We can have this task run in parallel by using the following code\nsegment, which contains the compiler directive for parallelizing for loops: 182\nChapter 4\nThreads\n#pragma omp parallel for\nfor (i = 0; i < N; i++) {\nc[i] = a[i] + b[i];\n}\nOpenMP divides the work contained in the for loop among the threads it has\ncreated in response to the directive\n#pragma omp parallel for\nIn addition to providing directives for parallelization, OpenMP allows devel-\nopers to choose among several levels of parallelism. For example, they can set",
  "the number of threads manually. It also allows developers to identify whether\ndata are shared between threads or are private to a thread. OpenMP is available\non several open-source and commercial compilers for Linux, Windows, and\nMac OS X systems. We encourage readers interested in learning more about\nOpenMP to consult the bibliography at the end of the chapter.\n4.5.3\nGrand Central Dispatch\nGrand Central Dispatch (GCD)—a technology for Apple’s Mac OS X and iOS\noperating systems—is a combination of extensions to the C language, an API,\nand a run-time library that allows application developers to identify sections\nof code to run in parallel. Like OpenMP, GCD manages most of the details of\nthreading.\nGCD identiﬁes extensions to the C and C++ languages known as blocks. A",
  "block is simply a self-contained unit of work. It is speciﬁed by a caret ˆ inserted\nin front of a pair of braces { }. A simple example of a block is shown below:\nˆ{ printf(\"I am a block\"); }\nGCD schedules blocks for run-time execution by placing them on a dispatch\nqueue. When it removes a block from a queue, it assigns the block to an\navailable thread from the thread pool it manages. GCD identiﬁes two types of\ndispatch queues: serial and concurrent.\nBlocks placed on a serial queue are removed in FIFO order. Once a block has\nbeen removed from the queue, it must complete execution before another block\nis removed. Each process has its own serial queue (known as its main queue).\nDevelopers can create additional serial queues that are local to particular",
  "processes. Serial queues are useful for ensuring the sequential execution of\nseveral tasks.\nBlocks placed on a concurrent queue are also removed in FIFO order, but\nseveral blocks may be removed at a time, thus allowing multiple blocks to\nexecute in parallel. There are three system-wide concurrent dispatch queues,\nand they are distinguished according to priority: low, default, and high.\nPriorities represent an approximation of the relative importance of blocks.\nQuite simply, blocks with a higher priority should be placed on the high-\npriority dispatch queue.\nThe following code segment illustrates obtaining the default-priority\nconcurrent\nqueue\nand\nsubmitting\na\nblock\nto\nthe\nqueue\nusing\nthe\ndispatch async() function: 4.6\nThreading Issues\n183\ndispatch queue t queue = dispatch get global queue",
  "(DISPATCH QUEUE PRIORITY DEFAULT, 0);\ndispatch async(queue, ˆ{ printf(\"I am a block.\"); });\nInternally, GCD’s thread pool is composed of POSIX threads. GCD actively\nmanages the pool, allowing the number of threads to grow and shrink\naccording to application demand and system capacity.\n4.5.4\nOther Approaches\nThread pools, OpenMP, and Grand Central Dispatch are just a few of many\nemerging technologies for managing multithreaded applications. Other com-\nmercial approaches include parallel and concurrent libraries, such as Intel’s\nThreading Building Blocks (TBB) and several products from Microsoft. The Java\nlanguage and API have seen signiﬁcant movement toward supporting concur-\nrent programming as well. A notable example is the java.util.concurrent",
  "package, which supports implicit thread creation and management.\n4.6\nThreading Issues\nIn this section, we discuss some of the issues to consider in designing\nmultithreaded programs.\n4.6.1\nThe fork() and exec() System Calls\nIn Chapter 3, we described how the fork() system call is used to create a\nseparate, duplicate process. The semantics of the fork() and exec() system\ncalls change in a multithreaded program.\nIf one thread in a program calls fork(), does the new process duplicate\nall threads, or is the new process single-threaded? Some UNIX systems have\nchosen to have two versions of fork(), one that duplicates all threads and\nanother that duplicates only the thread that invoked the fork() system call.\nThe exec() system call typically works in the same way as described",
  "in Chapter 3. That is, if a thread invokes the exec() system call, the program\nspeciﬁed in the parameter to exec() will replace the entire process—including\nall threads.\nWhich of the two versions of fork() to use depends on the application.\nIf exec() is called immediately after forking, then duplicating all threads is\nunnecessary, as the program speciﬁed in the parameters to exec() will replace\nthe process. In this instance, duplicating only the calling thread is appropriate.\nIf, however, the separate process does not call exec() after forking, the separate\nprocess should duplicate all threads.\n4.6.2\nSignal Handling\nA signal is used in UNIX systems to notify a process that a particular event has\noccurred. A signal may be received either synchronously or asynchronously, 184\nChapter 4",
  "Chapter 4\nThreads\ndepending on the source of and the reason for the event being signaled. All\nsignals, whether synchronous or asynchronous, follow the same pattern:\n1. A signal is generated by the occurrence of a particular event.\n2. The signal is delivered to a process.\n3. Once delivered, the signal must be handled.\nExamples of synchronous signal include illegal memory access and divi-\nsion by 0. If a running program performs either of these actions, a signal\nis generated. Synchronous signals are delivered to the same process that\nperformed the operation that caused the signal (that is the reason they are\nconsidered synchronous).\nWhen a signal is generated by an event external to a running process, that\nprocess receives the signal asynchronously. Examples of such signals include",
  "terminating a process with speciﬁc keystrokes (such as <control><C>) and\nhaving a timer expire. Typically, an asynchronous signal is sent to another\nprocess.\nA signal may be handled by one of two possible handlers:\n1. A default signal handler\n2. A user-deﬁned signal handler\nEvery signal has a default signal handler that the kernel runs when\nhandling that signal. This default action can be overridden by a user-deﬁned\nsignal handler that is called to handle the signal. Signals are handled in\ndifferent ways. Some signals (such as changing the size of a window) are\nsimply ignored; others (such as an illegal memory access) are handled by\nterminating the program.\nHandling signals in single-threaded programs is straightforward: signals",
  "are always delivered to a process. However, delivering signals is more\ncomplicated in multithreaded programs, where a process may have several\nthreads. Where, then, should a signal be delivered?\nIn general, the following options exist:\n1. Deliver the signal to the thread to which the signal applies.\n2. Deliver the signal to every thread in the process.\n3. Deliver the signal to certain threads in the process.\n4. Assign a speciﬁc thread to receive all signals for the process.\nThe method for delivering a signal depends on the type of signal generated.\nFor example, synchronous signals need to be delivered to the thread causing\nthe signal and not to other threads in the process. However, the situation with\nasynchronous signals is not as clear. Some asynchronous signals—such as a",
  "signal that terminates a process (<control><C>, for example)—should be\nsent to all threads. 4.6\nThreading Issues\n185\nThe standard UNIX function for delivering a signal is\nkill(pid t pid, int signal)\nThis function speciﬁes the process (pid) to which a particular signal (signal) is\nto be delivered. Most multithreaded versions of UNIX allow a thread to specify\nwhich signals it will accept and which it will block. Therefore, in some cases,\nan asynchronous signal may be delivered only to those threads that are not\nblocking it. However, because signals need to be handled only once, a signal is\ntypically delivered only to the ﬁrst thread found that is not blocking it. POSIX\nPthreads provides the following function, which allows a signal to be delivered\nto a speciﬁed thread (tid):",
  "to a speciﬁed thread (tid):\npthread kill(pthread t tid, int signal)\nAlthough Windows does not explicitly provide support for signals, it\nallows us to emulate them using asynchronous procedure calls (APCs). The\nAPC facility enables a user thread to specify a function that is to be called\nwhen the user thread receives notiﬁcation of a particular event. As indicated\nby its name, an APC is roughly equivalent to an asynchronous signal in UNIX.\nHowever, whereas UNIX must contend with how to deal with signals in a\nmultithreaded environment, the APC facility is more straightforward, since an\nAPC is delivered to a particular thread rather than a process.\n4.6.3\nThread Cancellation\nThread cancellation involves terminating a thread before it has completed. For",
  "example, if multiple threads are concurrently searching through a database and\none thread returns the result, the remaining threads might be canceled. Another\nsituation might occur when a user presses a button on a web browser that stops\na web page from loading any further. Often, a web page loads using several\nthreads—each image is loaded in a separate thread. When a user presses the\nstop button on the browser, all threads loading the page are canceled.\nA thread that is to be canceled is often referred to as the target thread.\nCancellation of a target thread may occur in two different scenarios:\n1. Asynchronous cancellation. One thread immediately terminates the\ntarget thread.\n2. Deferred cancellation. The target thread periodically checks whether it",
  "should terminate, allowing it an opportunity to terminate itself in an\norderly fashion.\nThe difﬁculty with cancellation occurs in situations where resources have\nbeen allocated to a canceled thread or where a thread is canceled while in\nthe midst of updating data it is sharing with other threads. This becomes\nespecially troublesome with asynchronous cancellation. Often, the operating\nsystem will reclaim system resources from a canceled thread but will not\nreclaim all resources. Therefore, canceling a thread asynchronously may not\nfree a necessary system-wide resource. 186\nChapter 4\nThreads\nWith deferred cancellation, in contrast, one thread indicates that a target\nthread is to be canceled, but cancellation occurs only after the target thread has",
  "checked a ﬂag to determine whether or not it should be canceled. The thread\ncan perform this check at a point at which it can be canceled safely.\nIn Pthreads, thread cancellation is initiated using the pthread cancel()\nfunction. The identiﬁer of the target thread is passed as a parameter to\nthe function. The following code illustrates creating—and then canceling—\na thread:\npthread t tid;\n/* create the thread */\npthread create(&tid, 0, worker, NULL);\n. . .\n/* cancel the thread */\npthread cancel(tid);\nInvoking pthread cancel()indicates only a request to cancel the target\nthread, however; actual cancellation depends on how the target thread is set\nup to handle the request. Pthreads supports three cancellation modes. Each",
  "mode is deﬁned as a state and a type, as illustrated in the table below. A thread\nmay set its cancellation state and type using an API.\nMode\nState\nType\nOff\nDisabled\n–\nDeferred\nEnabled\nDeferred\nAsynchronous\nEnabled\nAsynchronous\nAs the table illustrates, Pthreads allows threads to disable or enable\ncancellation. Obviously, a thread cannot be canceled if cancellation is disabled.\nHowever, cancellation requests remain pending, so the thread can later enable\ncancellation and respond to the request.\nThe default cancellation type is deferred cancellation. Here, cancellation\noccurs only when a thread reaches a cancellation point. One technique for\nestablishing a cancellation point is to invoke the pthread testcancel()\nfunction. If a cancellation request is found to be pending, a function known",
  "as a cleanup handler is invoked. This function allows any resources a thread\nmay have acquired to be released before the thread is terminated.\nThe following code illustrates how a thread may respond to a cancellation\nrequest using deferred cancellation:\nwhile (1) {\n/* do some work for awhile */\n/* . . . */\n/* check if there is a cancellation request */\npthread testcancel();\n} 4.6\nThreading Issues\n187\nBecause of the issues described earlier, asynchronous cancellation is not\nrecommended in Pthreads documentation. Thus, we do not cover it here. An\ninteresting note is that on Linux systems, thread cancellation using the Pthreads\nAPI is handled through signals (Section 4.6.2).\n4.6.4\nThread-Local Storage\nThreads belonging to a process share the data of the process. Indeed, this",
  "data sharing provides one of the beneﬁts of multithreaded programming.\nHowever, in some circumstances, each thread might need its own copy of\ncertain data. We will call such data thread-local storage (or TLS.) For example,\nin a transaction-processing system, we might service each transaction in a\nseparate thread. Furthermore, each transaction might be assigned a unique\nidentiﬁer. To associate each thread with its unique identiﬁer, we could use\nthread-local storage.\nIt is easy to confuse TLS with local variables. However, local variables\nare visible only during a single function invocation, whereas TLS data are\nvisible across function invocations. In some ways, TLS is similar to static\ndata. The difference is that TLS data are unique to each thread. Most thread",
  "libraries—including Windows and Pthreads—provide some form of support\nfor thread-local storage; Java provides support as well.\n4.6.5\nScheduler Activations\nA ﬁnal issue to be considered with multithreaded programs concerns com-\nmunication between the kernel and the thread library, which may be required\nby the many-to-many and two-level models discussed in Section 4.3.3. Such\ncoordination allows the number of kernel threads to be dynamically adjusted\nto help ensure the best performance.\nMany systems implementing either the many-to-many or the two-level\nmodel place an intermediate data structure between the user and kernel\nthreads. This data structure—typically known as a lightweight process, or\nLWP—is shown in Figure 4.13. To the user-thread library, the LWP appears to",
  "be a virtual processor on which the application can schedule a user thread to\nrun. Each LWP is attached to a kernel thread, and it is kernel threads that the\nLWP\nuser thread\nkernel thread\nk\nlightweight process\nFigure 4.13\nLightweight process (LWP). 188\nChapter 4\nThreads\noperating system schedules to run on physical processors. If a kernel thread\nblocks (such as while waiting for an I/O operation to complete), the LWP blocks\nas well. Up the chain, the user-level thread attached to the LWP also blocks.\nAn application may require any number of LWPs to run efﬁciently. Consider\na CPU-bound application running on a single processor. In this scenario, only\none thread can run at at a time, so one LWP is sufﬁcient. An application that is",
  "I/O-intensive may require multiple LWPs to execute, however. Typically, an LWP\nis required for each concurrent blocking system call. Suppose, for example, that\nﬁve different ﬁle-read requests occur simultaneously. Five LWPs are needed,\nbecause all could be waiting for I/O completion in the kernel. If a process has\nonly four LWPs, then the ﬁfth request must wait for one of the LWPs to return\nfrom the kernel.\nOne scheme for communication between the user-thread library and the\nkernel is known as scheduler activation. It works as follows: The kernel\nprovides an application with a set of virtual processors (LWPs), and the\napplication can schedule user threads onto an available virtual processor.\nFurthermore, the kernel must inform an application about certain events. This",
  "procedure is known as an upcall. Upcalls are handled by the thread library\nwith an upcall handler, and upcall handlers must run on a virtual processor.\nOne event that triggers an upcall occurs when an application thread is about to\nblock. In this scenario, the kernel makes an upcall to the application informing\nit that a thread is about to block and identifying the speciﬁc thread. The kernel\nthen allocates a new virtual processor to the application. The application runs\nan upcall handler on this new virtual processor, which saves the state of the\nblocking thread and relinquishes the virtual processor on which the blocking\nthread is running. The upcall handler then schedules another thread that is\neligible to run on the new virtual processor. When the event that the blocking",
  "thread was waiting for occurs, the kernel makes another upcall to the thread\nlibrary informing it that the previously blocked thread is now eligible to run.\nThe upcall handler for this event also requires a virtual processor, and the kernel\nmay allocate a new virtual processor or preempt one of the user threads and\nrun the upcall handler on its virtual processor. After marking the unblocked\nthread as eligible to run, the application schedules an eligible thread to run on\nan available virtual processor.\n4.7\nOperating-System Examples\nAt this point, we have examined a number of concepts and issues related to\nthreads. We conclude the chapter by exploring how threads are implemented\nin Windows and Linux systems.\n4.7.1\nWindows Threads",
  "4.7.1\nWindows Threads\nWindows implements the Windows API, which is the primary API for the\nfamily of Microsoft operating systems (Windows 98, NT, 2000, and XP, as well\nas Windows 7). Indeed, much of what is mentioned in this section applies to\nthis entire family of operating systems.\nA Windows application runs as a separate process, and each process may\ncontain one or more threads. The Windows API for creating threads is covered in 4.7\nOperating-System Examples\n189\nSection 4.4.2. Additionally, Windows uses the one-to-one mapping described\nin Section 4.3.2, where each user-level thread maps to an associated kernel\nthread.\nThe general components of a thread include:\n• A thread ID uniquely identifying the thread\n• A register set representing the status of the processor",
  "• A user stack, employed when the thread is running in user mode, and a\nkernel stack, employed when the thread is running in kernel mode\n• A private storage area used by various run-time libraries and dynamic link\nlibraries (DLLs)\nThe register set, stacks, and private storage area are known as the context of\nthe thread.\nThe primary data structures of a thread include:\n• ETHREAD—executive thread block\n• KTHREAD—kernel thread block\n• TEB—thread environment block\nThe key components of the ETHREAD include a pointer to the process\nto which the thread belongs and the address of the routine in which the\nthread starts control. The ETHREAD also contains a pointer to the corresponding\nKTHREAD.\nThe KTHREAD includes scheduling and synchronization information for",
  "the thread. In addition, the KTHREAD includes the kernel stack (used when the\nthread is running in kernel mode) and a pointer to the TEB.\nThe ETHREAD and the KTHREAD exist entirely in kernel space; this means\nthat only the kernel can access them. The TEB is a user-space data structure\nthat is accessed when the thread is running in user mode. Among other ﬁelds,\nthe TEB contains the thread identiﬁer, a user-mode stack, and an array for\nthread-local storage. The structure of a Windows thread is illustrated in Figure\n4.14.\n4.7.2\nLinux Threads\nLinux provides the fork() system call with the traditional functionality of\nduplicating a process, as described in Chapter 3. Linux also provides the ability\nto create threads using the clone() system call. However, Linux does not",
  "distinguish between processes and threads. In fact, Linux uses the term task\n—rather than process or thread— when referring to a ﬂow of control within a\nprogram.\nWhen clone() is invoked, it is passed a set of ﬂags that determine how\nmuch sharing is to take place between the parent and child tasks. Some of these\nﬂags are listed in Figure 4.15. For example, suppose that clone() is passed\nthe ﬂags CLONE FS, CLONE VM, CLONE SIGHAND, and CLONE FILES. The parent\nand child tasks will then share the same ﬁle-system information (such as the\ncurrent working directory), the same memory space, the same signal handlers, 190\nChapter 4\nThreads\nuser space\nkernel space\npointer to  \nparent process\nthread start \n address\nETHREAD\nKTHREAD\n• \n• \n•\nkernel \nstack\nscheduling \nand \nsynchronization\ninformation\n•",
  "scheduling \nand \nsynchronization\ninformation\n• \n• \n•\nuser \nstack\nthread-local \nstorage\nthread identifier\nTEB\n• \n• \n•\nFigure 4.14\nData structures of a Windows thread.\nand the same set of open ﬁles. Using clone() in this fashion is equivalent to\ncreating a thread as described in this chapter, since the parent task shares most\nof its resources with its child task. However, if none of these ﬂags is set when\nclone() is invoked, no sharing takes place, resulting in functionality similar\nto that provided by the fork() system call.\nThe varying level of sharing is possible because of the way a task is\nrepresented in the Linux kernel. A unique kernel data structure (speciﬁcally,\nstruct task struct) exists for each task in the system. This data structure,",
  "instead of storing data for the task, contains pointers to other data structures\nwhere these data are stored—for example, data structures that represent the list\nof open ﬁles, signal-handling information, and virtual memory. When fork()\nis invoked, a new task is created, along with a copy of all the associated data\nflag\nmeaning\nCLONE_FS\nCLONE_VM\nCLONE_SIGHAND\nCLONE_FILES\nFile-system information is shared.\nThe same memory space is shared.\nSignal handlers are shared.\nThe set of open files is shared.\nFigure 4.15\nSome of the ﬂags passed when clone() is invoked. Practice Exercises\n191\nstructures of the parent process. A new task is also created when the clone()\nsystem call is made. However, rather than copying all data structures, the new",
  "task points to the data structures of the parent task, depending on the set of\nﬂags passed to clone().\n4.8\nSummary\nA thread is a ﬂow of control within a process. A multithreaded process contains\nseveral different ﬂows of control within the same address space. The beneﬁts of\nmultithreading include increased responsiveness to the user, resource sharing\nwithin the process, economy, and scalability factors, such as more efﬁcient use\nof multiple processing cores.\nUser-level threads are threads that are visible to the programmer and are\nunknown to the kernel. The operating-system kernel supports and manages\nkernel-level threads. In general, user-level threads are faster to create and\nmanage than are kernel threads, because no intervention from the kernel is\nrequired.",
  "required.\nThree different types of models relate user and kernel threads. The many-\nto-one model maps many user threads to a single kernel thread. The one-to-one\nmodel maps each user thread to a corresponding kernel thread. The many-to-\nmany model multiplexes many user threads to a smaller or equal number of\nkernel threads.\nMost modern operating systems provide kernel support for threads. These\ninclude Windows, Mac OS X, Linux, and Solaris.\nThread libraries provide the application programmer with an API for\ncreating and managing threads. Three primary thread libraries are in common\nuse: POSIX Pthreads, Windows threads, and Java threads.\nIn addition to explicitly creating threads using the API provided by a\nlibrary, we can use implicit threading, in which the creation and management",
  "of threading is transferred to compilers and run-time libraries. Strategies for\nimplicit threading include thread pools, OpenMP, and Grand Central Dispatch.\nMultithreaded programs introduce many challenges for programmers,\nincluding the semantics of the fork() and exec() system calls. Other\nissues include signal handling, thread cancellation, thread-local storage, and\nscheduler activations.\nPractice Exercises\n4.1\nProvide two programming examples in which multithreading provides\nbetter performance than a single-threaded solution.\n4.2\nWhat are two differences between user-level threads and kernel-level\nthreads? Under what circumstances is one type better than the other?\n4.3\nDescribe the actions taken by a kernel to context-switch between kernel-\nlevel threads.\n4.4",
  "level threads.\n4.4\nWhat resources are used when a thread is created? How do they differ\nfrom those used when a process is created? 192\nChapter 4\nThreads\n4.5\nAssume that an operating system maps user-level threads to the kernel\nusing the many-to-many model and that the mapping is done through\nLWPs. Furthermore, the system allows developers to create real-time\nthreads for use in real-time systems. Is it necessary to bind a real-time\nthread to an LWP? Explain.\nExercises\n4.6\nProvide two programming examples in which multithreading does not\nprovide better performance than a single-threaded solution.\n4.7\nUnder what circumstances does a multithreaded solution using multi-\nple kernel threads provide better performance than a single-threaded\nsolution on a single-processor system?\n4.8",
  "solution on a single-processor system?\n4.8\nWhich of the following components of program state are shared across\nthreads in a multithreaded process?\na.\nRegister values\nb.\nHeap memory\nc.\nGlobal variables\nd.\nStack memory\n4.9\nCan a multithreaded solution using multiple user-level threads achieve\nbetter performance on a multiprocessor system than on a single-\nprocessor system? Explain.\n4.10\nIn Chapter 3, we discussed Google’s Chrome browser and its practice\nof opening each new website in a separate process. Would the same\nbeneﬁts have been achieved if instead Chrome had been designed to\nopen each new website in a separate thread? Explain.\n4.11\nIs it possible to have concurrency but not parallelism? Explain.\n4.12\nUsing Amdahl’s Law, calculate the speedup gain of an application that",
  "has a 60 percent parallel component for (a) two processing cores and (b)\nfour processing cores.\n4.13\nDetermine if the following problems exhibit task or data parallelism:\n• The multithreaded statistical program described in Exercise 4.21\n• The multithreaded Sudoku validator described in Project 1 in this\nchapter\n• The multithreaded sorting program described in Project 2 in this\nchapter\n• The multithreaded web server described in Section 4.1\n4.14\nA system with two dual-core processors has four processors available\nfor scheduling. A CPU-intensive application is running on this system.\nAll input is performed at program start-up, when a single ﬁle must\nbe opened. Similarly, all output is performed just before the program Exercises\n193",
  "193\nterminates, when the program results must be written to a single\nﬁle. Between startup and termination, the program is entirely CPU-\nbound. Your task is to improve the performance of this application\nby multithreading it. The application runs on a system that uses the\none-to-one threading model (each user thread maps to a kernel thread).\n• How many threads will you create to perform the input and output?\nExplain.\n• How many threads will you create for the CPU-intensive portion of\nthe application? Explain.\n4.15\nConsider the following code segment:\npid t pid;\npid = fork();\nif (pid == 0) { /* child process */\nfork();\nthread create( . . .);\n}\nfork();\na.\nHow many unique processes are created?\nb.\nHow many unique threads are created?\n4.16",
  "b.\nHow many unique threads are created?\n4.16\nAs described in Section 4.7.2, Linux does not distinguish between\nprocesses and threads. Instead, Linux treats both in the same way,\nallowing a task to be more akin to a process or a thread depending on the\nset of ﬂags passed to the clone() system call. However, other operating\nsystems, such as Windows, treat processes and threads differently.\nTypically, such systems use a notation in which the data structure for\na process contains pointers to the separate threads belonging to the\nprocess. Contrast these two approaches for modeling processes and\nthreads within the kernel.\n4.17\nThe program shown in Figure 4.16 uses the Pthreads API. What would\nbe the output from the program at LINE C and LINE P?\n4.18",
  "4.18\nConsider a multicore system and a multithreaded program written\nusing the many-to-many threading model. Let the number of user-level\nthreads in the program be greater than the number of processing cores\nin the system. Discuss the performance implications of the following\nscenarios.\na.\nThe number of kernel threads allocated to the program is less than\nthe number of processing cores.\nb.\nThe number of kernel threads allocated to the program is equal to\nthe number of processing cores.\nc.\nThe number of kernel threads allocated to the program is greater\nthan the number of processing cores but less than the number of\nuser-level threads. 194\nChapter 4\nThreads\n#include <pthread.h>\n#include <stdio.h>\n#include <types.h>\nint value = 0;\nvoid *runner(void *param); /* the thread */",
  "void *runner(void *param); /* the thread */\nint main(int argc, char *argv[])\n{\npid t pid;\npthread t tid;\npthread attr t attr;\npid = fork();\nif (pid == 0) { /* child process */\npthread attr init(&attr);\npthread create(&tid,&attr,runner,NULL);\npthread join(tid,NULL);\nprintf(\"CHILD: value = %d\",value); /* LINE C */\n}\nelse if (pid > 0) { /* parent process */\nwait(NULL);\nprintf(\"PARENT: value = %d\",value); /* LINE P */\n}\n}\nvoid *runner(void *param) {\nvalue = 5;\npthread exit(0);\n}\nFigure 4.16\nC program for Exercise 4.17.\n4.19\nPthreads provides an API for managing thread cancellation. The\npthread setcancelstate() function is used to set the cancellation\nstate. Its prototype appears as follows:\npthread setcancelstate(int state, int *oldstate)",
  "pthread setcancelstate(int state, int *oldstate)\nThe two possible values for the state are PTHREAD CANCEL ENABLE and\nPTHREAD CANCEL DISABLE.\nUsing the code segment shown in Figure 4.17, provide examples of\ntwo operations that would be suitable to perform between the calls to\ndisable and enable thread cancellation. Programming Problems\n195\nint oldstate;\npthread setcancelstate(PTHREAD CANCEL DISABLE, &oldstate);\n/* What operations would be performed here? */\npthread setcancelstate(PTHREAD CANCEL ENABLE, &oldstate);\nFigure 4.17\nC program for Exercise 4.19.\nProgramming Problems\n4.20\nModify programming problem Exercise 3.20 from Chapter 3, which asks\nyou to design a pid manager. This modiﬁcation will consist of writing\na multithreaded program that tests your solution to Exercise 3.20. You",
  "will create a number of threads—for example, 100—and each thread will\nrequest a pid, sleep for a random period of time, and then release the pid.\n(Sleeping for a random period of time approximates the typical pid usage\nin which a pid is assigned to a new process, the process executes and\nthen terminates, and the pid is released on the process’s termination.) On\nUNIX and Linux systems, sleeping is accomplished through the sleep()\nfunction, which is passed an integer value representing the number of\nseconds to sleep. This problem will be modiﬁed in Chapter 5.\n4.21\nWrite a multithreaded program that calculates various statistical values\nfor a list of numbers. This program will be passed a series of numbers on\nthe command line and will then create three separate worker threads.",
  "One thread will determine the average of the numbers, the second\nwill determine the maximum value, and the third will determine the\nminimum value. For example, suppose your program is passed the\nintegers\n90 81 78 95 79 72 85\nThe program will report\nThe average value is 82\nThe minimum value is 72\nThe maximum value is 95\nThe variables representing the average, minimum, and maximum values\nwill be stored globally. The worker threads will set these values, and the\nparent thread will output the values once the workers have exited. (We\ncould obviously expand this program by creating additional threads\nthat determine other statistical values, such as median and standard\ndeviation.)\n4.22\nAn interesting way of calculating ! is to use a technique known as Monte",
  "Carlo, which involves randomization. This technique works as follows:\nSuppose you have a circle inscribed within a square, as shown in Figure 196\nChapter 4\nThreads\n(−1, 1)\n(−1, −1)\n(1, 1)\n(1, −1)\n(0, 0)\nFigure 4.18\nMonte Carlo technique for calculating pi.\n4.18. (Assume that the radius of this circle is 1.) First, generate a series of\nrandom points as simple (x, y) coordinates. These points must fall within\nthe Cartesian coordinates that bound the square. Of the total number of\nrandom points that are generated, some will occur within the circle.\nNext, estimate ! by performing the following calculation:\n! = 4× (number of points in circle) / (total number of points)\nWrite a multithreaded version of this algorithm that creates a separate",
  "thread to generate a number of random points. The thread will count\nthe number of points that occur within the circle and store that result\nin a global variable. When this thread has exited, the parent thread will\ncalculate and output the estimated value of !. It is worth experimenting\nwith the number of random points generated. As a general rule, the\ngreater the number of points, the closer the approximation to !.\nIn the source-code download for this text, we provide a sample program\nthat provides a technique for generating random numbers, as well as\ndetermining if the random (x, y) point occurs within the circle.\nReaders interested in the details of the Monte Carlo method for esti-\nmating ! should consult the bibliography at the end of this chapter. In",
  "Chapter 5, we modify this exercise using relevant material from that\nchapter.\n4.23\nRepeat Exercise 4.22, but instead of using a separate thread to generate\nrandom points, use OpenMP to parallelize the generation of points. Be\ncareful not to place the calculcation of ! in the parallel region, since you\nwant to calculcate ! only once.\n4.24\nWrite a multithreaded program that outputs prime numbers. This\nprogram should work as follows: The user will run the program and\nwill enter a number on the command line. The program will then create\na separate thread that outputs all the prime numbers less than or equal\nto the number entered by the user.\n4.25\nModify the socket-based date server (Figure 3.21) in Chapter 3 so that",
  "the server services each client request in a separate thread. Programming Projects\n197\n4.26\nThe Fibonacci sequence is the series of numbers 0, 1, 1, 2, 3, 5, 8, ....\nFormally, it can be expressed as:\nf ib0 = 0\nf ib1 = 1\nf ibn = f ibn−1 + f ibn−2\nWrite a multithreaded program that generates the Fibonacci sequence.\nThis program should work as follows: On the command line, the user\nwill enter the number of Fibonacci numbers that the program is to\ngenerate. The program will then create a separate thread that will\ngenerate the Fibonacci numbers, placing the sequence in data that can\nbe shared by the threads (an array is probably the most convenient\ndata structure). When the thread ﬁnishes execution, the parent thread\nwill output the sequence generated by the child thread. Because the",
  "parent thread cannot begin outputting the Fibonacci sequence until the\nchild thread ﬁnishes, the parent thread will have to wait for the child\nthread to ﬁnish. Use the techniques described in Section 4.4 to meet this\nrequirement.\n4.27\nExercise 3.25 in Chapter 3 involves designing an echo server using the\nJava threading API. This server is single-threaded, meaning that the\nserver cannot respond to concurrent echo clients until the current client\nexits. Modify the solution to Exercise 3.25 so that the echo server services\neach client in a separate request.\nProgramming Projects\nProject 1—Sudoku Solution Validator\nA Sudoku puzzle uses a 9 × 9 grid in which each column and row, as well as\neach of the nine 3 × 3 subgrids, must contain all of the digits 1 · · · 9. Figure",
  "4.19 presents an example of a valid Sudoku puzzle. This project consists of\ndesigning a multithreaded application that determines whether the solution to\na Sudoku puzzle is valid.\nThere are several different ways of multithreading this application. One\nsuggested strategy is to create threads that check the following criteria:\n• A thread to check that each column contains the digits 1 through 9\n• A thread to check that each row contains the digits 1 through 9\n• Nine threads to check that each of the 3 × 3 subgrids contains the digits 1\nthrough 9\nThis would result in a total of eleven separate threads for validating a\nSudoku puzzle. However, you are welcome to create even more threads for\nthis project. For example, rather than creating one thread that checks all nine 198\nChapter 4\nThreads\n6",
  "Chapter 4\nThreads\n6\n2\n4\n5\n3\n9\n1\n8\n7\n5\n1\n9\n7\n2\n8\n6\n3\n4\n8\n3\n7\n6\n1\n4\n2\n9\n5\n1\n4\n3\n8\n6\n5\n7\n2\n9\n9\n5\n8\n2\n4\n7\n3\n6\n1\n7\n6\n2\n3\n9\n1\n4\n5\n8\n3\n7\n1\n9\n5\n6\n8\n4\n2\n4\n9\n6\n1\n8\n2\n5\n7\n3\n2\n8\n5\n4\n7\n3\n9\n1\n6\nFigure 4.19\nSolution to a 9 × 9 Sudoku puzzle.\ncolumns, you could create nine separate threads and have each of them check\none column.\nPassing Parameters to Each Thread\nThe parent thread will create the worker threads, passing each worker the\nlocation that it must check in the Sudoku grid. This step will require passing\nseveral parameters to each thread. The easiest approach is to create a data\nstructure using a struct. For example, a structure to pass the row and column\nwhere a thread must begin validating would appear as follows:\n/* structure for passing data to threads */\ntypedef struct\n{\nint row;\nint column;",
  "typedef struct\n{\nint row;\nint column;\n} parameters;\nBoth Pthreads and Windows programs will create worker threads using a\nstrategy similar to that shown below:\nparameters *data = (parameters *) malloc(sizeof(parameters));\ndata->row = 1;\ndata->column = 1;\n/* Now create the thread passing it data as a parameter */\nThe data pointer will be passed to either the pthread create() (Pthreads)\nfunction or the CreateThread() (Windows) function, which in turn will pass\nit as a parameter to the function that is to run as a separate thread.\nReturning Results to the Parent Thread\nEach worker thread is assigned the task of determining the validity of a\nparticular region of the Sudoku puzzle. Once a worker has performed this Bibliographical Notes\n199\n7, 12, 19, 3, 18\n7, 12, 19, 3, 18, 4, 2, 6, 15, 8",
  "7, 12, 19, 3, 18\n7, 12, 19, 3, 18, 4, 2, 6, 15, 8\nOriginal List\n2, 3, 4, 6, 7, 8, 12, 15, 18, 19\nMerge Thread\nSorted List\nSorting\nThread0\nSorting\nThread1\n4, 2, 6, 15, 8\nFigure 4.20\nMultithreaded sorting.\ncheck, it must pass its results back to the parent. One good way to handle this\nis to create an array of integer values that is visible to each thread. The ith\nindex in this array corresponds to the ith worker thread. If a worker sets its\ncorresponding value to 1, it is indicating that its region of the Sudoku puzzle\nis valid. A value of 0 would indicate otherwise. When all worker threads have\ncompleted, the parent thread checks each entry in the result array to determine\nif the Sudoku puzzle is valid.\nProject 2—Multithreaded Sorting Application",
  "Project 2—Multithreaded Sorting Application\nWrite a multithreaded sorting program that works as follows: A list of integers\nis divided into two smaller lists of equal size. Two separate threads (which we\nwill term sorting threads) sort each sublist using a sorting algorithm of your\nchoice. The two sublists are then merged by a third thread—a merging thread\n—which merges the two sublists into a single sorted list.\nBecause global data are shared cross all threads, perhaps the easiest way\nto set up the data is to create a global array. Each sorting thread will work on\none half of this array. A second global array of the same size as the unsorted\ninteger array will also be established. The merging thread will then merge",
  "the two sublists into this second array. Graphically, this program is structured\naccording to Figure 4.20.\nThis programming project will require passing parameters to each of the\nsorting threads. In particular, it will be necessary to identify the starting index\nfrom which each thread is to begin sorting. Refer to the instructions in Project\n1 for details on passing parameters to a thread.\nThe parent thread will output the sorted array once all sorting threads have\nexited.\nBibliographical Notes\nThreads have had a long evolution, starting as “cheap concurrency” in\nprogramming languages and moving to “lightweight processes,” with early\nexamples that included the Thoth system ([Cheriton et al. (1979)]) and the Pilot 200\nChapter 4\nThreads",
  "Chapter 4\nThreads\nsystem ([Redell et al. (1980)]). [Binding (1985)] described moving threads into\nthe UNIX kernel. Mach ([Accetta et al. (1986)], [Tevanian et al. (1987)]), and V\n([Cheriton (1988)]) made extensive use of threads, and eventually almost all\nmajor operating systems implemented them in some form or another.\n[Vahalia (1996)] covers threading in several versions of UNIX. [McDougall\nand Mauro (2007)] describes developments in threading the Solaris kernel.\n[Russinovich and Solomon (2009)] discuss threading in the Windows operating\nsystem family. [Mauerer (2008)] and [Love (2010)] explain how Linux handles\nthreading, and [Singh (2007)] covers threads in Mac OS X.\nInformation on Pthreads programming is given in [Lewis and Berg",
  "(1998)] and [Butenhof (1997)]. [Oaks and Wong (1999)] and [Lewis and\nBerg (2000)] discuss multithreading in Java. [Goetz et al. (2006)] present a\ndetailed discussion of concurrent programming in Java. [Hart (2005)] describes\nmultithreading using Windows. Details on using OpenMP can be found at\nhttp://openmp.org.\nAn analysis of an optimal thread-pool size can be found in [Ling et al.\n(2000)]. Scheduler activations were ﬁrst presented in [Anderson et al. (1991)],\nand [Williams (2002)] discusses scheduler activations in the NetBSD system.\n[Breshears (2009)] and [Pacheco (2011)] cover parallel programming in\ndetail. [Hill and Marty (2008)] examine Amdahl’s Law with respect to multicore\nsystems. The Monte Carlo technique for estimating ! is further discussed in",
  "http://math.fullerton.edu/mathews/n2003/montecarlopimod.html.\nBibliography\n[Accetta et al. (1986)]\nM. Accetta, R. Baron, W. Bolosky, D. B. Golub, R. Rashid,\nA. Tevanian, and M. Young, “Mach: A New Kernel Foundation for UNIX\nDevelopment”, Proceedings of the Summer USENIX Conference (1986), pages\n93–112.\n[Anderson et al. (1991)]\nT. E. Anderson, B. N. Bershad, E. D. Lazowska, and\nH. M. Levy, “Scheduler Activations: Effective Kernel Support for the User-Level\nManagement of Parallelism”, Proceedings of the ACM Symposium on Operating\nSystems Principles (1991), pages 95–109.\n[Binding (1985)]\nC. Binding, “Cheap Concurrency in C”, SIGPLAN Notices,\nVolume 20, Number 9 (1985), pages 21–27.\n[Breshears (2009)]\nC. Breshears, The Art of Concurrency, O’Reilly & Associates\n(2009).\n[Butenhof (1997)]",
  "(2009).\n[Butenhof (1997)]\nD. Butenhof, Programming with POSIX Threads, Addison-\nWesley (1997).\n[Cheriton (1988)]\nD. Cheriton, “The V Distributed System”, Communications of\nthe ACM, Volume 31, Number 3 (1988), pages 314–333.\n[Cheriton et al. (1979)]\nD. R. Cheriton, M. A. Malcolm, L. S. Melen, and G. R.\nSager, “Thoth, a Portable Real-Time Operating System”, Communications of the\nACM, Volume 22, Number 2 (1979), pages 105–115. Bibliography\n201\n[Goetz et al. (2006)]\nB. Goetz, T. Peirls, J. Bloch, J. Bowbeer, D. Holmes, and\nD. Lea, Java Concurrency in Practice, Addison-Wesley (2006).\n[Hart (2005)]\nJ. M. Hart, Windows System Programming, Third Edition, Addison-\nWesley (2005).\n[Hill and Marty (2008)]\nM. Hill and M. Marty, “Amdahl’s Law in the Multicore",
  "Era”, IEEE Computer, Volume 41, Number 7 (2008), pages 33–38.\n[Lewis and Berg (1998)]\nB. Lewis and D. Berg, Multithreaded Programming with\nPthreads, Sun Microsystems Press (1998).\n[Lewis and Berg (2000)]\nB. Lewis and D. Berg, Multithreaded Programming with\nJava Technology, Sun Microsystems Press (2000).\n[Ling et al. (2000)]\nY. Ling, T. Mullen, and X. Lin, “Analysis of Optimal Thread\nPool Size”, Operating System Review, Volume 34, Number 2 (2000), pages 42–55.\n[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[Mauerer (2008)]\nW. Mauerer, Professional Linux Kernel Architecture, John Wiley\nand Sons (2008).\n[McDougall and Mauro (2007)]\nR. McDougall and J. Mauro, Solaris Internals,\nSecond Edition, Prentice Hall (2007).\n[Oaks and Wong (1999)]",
  "[Oaks and Wong (1999)]\nS. Oaks and H. Wong, Java Threads, Second Edition,\nO’Reilly & Associates (1999).\n[Pacheco (2011)]\nP. S. Pacheco, An Introduction to Parallel Programming, Morgan\nKaufmann (2011).\n[Redell et al. (1980)]\nD. D. Redell, Y. K. Dalal, T. R. Horsley, H. C. Lauer, W. C.\nLynch, P. R. McJones, H. G. Murray, and S. P. Purcell, “Pilot: An Operating System\nfor a Personal Computer”, Communications of the ACM, Volume 23, Number 2\n(1980), pages 81–92.\n[Russinovich and Solomon (2009)]\nM. E. Russinovich and D. A. Solomon, Win-\ndows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition,\nMicrosoft Press (2009).\n[Singh (2007)]\nA. Singh, Mac OS X Internals: A Systems Approach, Addison-\nWesley (2007).\n[Tevanian et al. (1987)]",
  "Wesley (2007).\n[Tevanian et al. (1987)]\nA. Tevanian, Jr., R. F. Rashid, D. B. Golub, D. L. Black,\nE. Cooper, and M. W. Young, “Mach Threads and the Unix Kernel: The Battle\nfor Control”, Proceedings of the Summer USENIX Conference (1987).\n[Vahalia (1996)]\nU. Vahalia, Unix Internals: The New Frontiers, Prentice Hall\n(1996).\n[Williams (2002)]\nN. Williams, “An Implementation of Scheduler Activations\non the NetBSD Operating System”, 2002 USENIX Annual Technical Conference,\nFREENIX Track (2002).  5\nC H A P T E R\nProcess\nSynchronization\nA cooperating process is one that can affect or be affected by other processes\nexecuting in the system. Cooperating processes can either directly share a\nlogical address space (that is, both code and data) or be allowed to share data",
  "only through ﬁles or messages. The former case is achieved through the use of\nthreads, discussed in Chapter 4. Concurrent access to shared data may result in\ndata inconsistency, however. In this chapter, we discuss various mechanisms\nto ensure the orderly execution of cooperating processes that share a logical\naddress space, so that data consistency is maintained.\nCHAPTER OBJECTIVES\n• To introduce the critical-section problem, whose solutions can be used to\nensure the consistency of shared data.\n• To present both software and hardware solutions of the critical-section\nproblem.\n• To examine several classical process-synchronization problems.\n• To explore several tools that are used to solve process synchronization\nproblems.\n5.1\nBackground",
  "problems.\n5.1\nBackground\nWe’ve already seen that processes can execute concurrently or in parallel.\nSection 3.2.2 introduced the role of process scheduling and described how\nthe CPU scheduler switches rapidly between processes to provide concurrent\nexecution. This means that one process may only partially complete execution\nbefore another process is scheduled. In fact, a process may be interrupted at\nany point in its instruction stream, and the processing core may be assigned\nto execute instructions of another process. Additionally, Section 4.2 introduced\nparallel execution, in which two instruction streams (representing different\nprocesses) execute simultaneously on separate processing cores. In this chapter,\n203 204\nChapter 5\nProcess Synchronization",
  "203 204\nChapter 5\nProcess Synchronization\nwe explain how concurrent or parallel execution can contribute to issues\ninvolving the integrity of data shared by several processes.\nLet’s consider an example of how this can happen. In Chapter 3, we devel-\noped a model of a system consisting of cooperating sequential processes or\nthreads, all running asynchronously and possibly sharing data. We illustrated\nthis model with the producer–consumer problem, which is representative of\noperating systems. Speciﬁcally, in Section 3.4.1, we described how a bounded\nbuffer could be used to enable processes to share memory.\nWe now return to our consideration of the bounded buffer. As we pointed\nout, our original solution allowed at most BUFFER SIZE −1 items in the buffer",
  "at the same time. Suppose we want to modify the algorithm to remedy this\ndeﬁciency. One possibility is to add an integer variable counter, initialized to\n0. counter is incremented every time we add a new item to the buffer and is\ndecremented every time we remove one item from the buffer. The code for the\nproducer process can be modiﬁed as follows:\nwhile (true) {\n/* produce an item in next produced */\nwhile (counter == BUFFER SIZE)\n; /* do nothing */\nbuffer[in] = next produced;\nin = (in + 1) % BUFFER SIZE;\ncounter++;\n}\nThe code for the consumer process can be modiﬁed as follows:\nwhile (true) {\nwhile (counter == 0)\n; /* do nothing */\nnext consumed = buffer[out];\nout = (out + 1) % BUFFER SIZE;\ncounter--;\n/* consume the item in next consumed */\n}",
  "/* consume the item in next consumed */\n}\nAlthough the producer and consumer routines shown above are correct\nseparately, they may not function correctly when executed concurrently. As\nan illustration, suppose that the value of the variable counter is currently\n5 and that the producer and consumer processes concurrently execute the\nstatements “counter++” and “counter--”. Following the execution of these\ntwo statements, the value of the variable counter may be 4, 5, or 6! The only\ncorrect result, though, is counter == 5, which is generated correctly if the\nproducer and consumer execute separately. 5.1\nBackground\n205\nWe can show that the value of counter may be incorrect as follows. Note\nthat the statement “counter++” may be implemented in machine language (on\na typical machine) as follows:",
  "a typical machine) as follows:\nregister1 = counter\nregister1 = register1 + 1\ncounter = register1\nwhere register1 is one of the local CPU registers. Similarly, the statement\n“counter--” is implemented as follows:\nregister2 = counter\nregister2 = register2 −1\ncounter = register2\nwhere again register2 is one of the local CPU registers. Even though register1 and\nregister2 may be the same physical register (an accumulator, say), remember\nthat the contents of this register will be saved and restored by the interrupt\nhandler (Section 1.2.3).\nThe concurrent execution of “counter++” and “counter--” is equivalent\nto a sequential execution in which the lower-level statements presented\npreviously are interleaved in some arbitrary order (but the order within each",
  "high-level statement is preserved). One such interleaving is the following:\nT0:\nproducer\nexecute\nregister1 = counter\n{register1 = 5}\nT1:\nproducer\nexecute\nregister1 = register1 + 1\n{register1 = 6}\nT2:\nconsumer\nexecute\nregister2 = counter\n{register2 = 5}\nT3:\nconsumer\nexecute\nregister2 = register2 −1\n{register2 = 4}\nT4:\nproducer\nexecute\ncounter = register1\n{counter = 6}\nT5:\nconsumer\nexecute\ncounter = register2\n{counter = 4}\nNotice that we have arrived at the incorrect state “counter == 4”, indicating\nthat four buffers are full, when, in fact, ﬁve buffers are full. If we reversed the\norder of the statements at T4 and T5, we would arrive at the incorrect state\n“counter == 6”.\nWe would arrive at this incorrect state because we allowed both processes",
  "to manipulate the variable counter concurrently. A situation like this, where\nseveral processes access and manipulate the same data concurrently and the\noutcome of the execution depends on the particular order in which the access\ntakes place, is called a race condition. To guard against the race condition\nabove, we need to ensure that only one process at a time can be manipulating\nthe variable counter. To make such a guarantee, we require that the processes\nbe synchronized in some way.\nSituations such as the one just described occur frequently in operating\nsystems as different parts of the system manipulate resources. Furthermore, as\nwe have emphasized in earlier chapters, the growing importance of multicore\nsystems has brought an increased emphasis on developing multithreaded",
  "applications. In such applications, several threads—which are quite possibly\nsharing data—are running in parallel on different processing cores. Clearly, 206\nChapter 5\nProcess Synchronization\ndo {\nentry section\ncritical section\nexit section\nremainder section\n} while (true);\nFigure 5.1\nGeneral structure of a typical process Pi .\nwe want any changes that result from such activities not to interfere with one\nanother. Because of the importance of this issue, we devote a major portion of\nthis chapter to process synchronization and coordination among cooperating\nprocesses.\n5.2\nThe Critical-Section Problem\nWe begin our consideration of process synchronization by discussing the so-\ncalled critical-section problem. Consider a system consisting of n processes",
  "{P0, P1, ..., Pn−1}. Each process has a segment of code, called a critical section,\nin which the process may be changing common variables, updating a table,\nwriting a ﬁle, and so on. The important feature of the system is that, when\none process is executing in its critical section, no other process is allowed to\nexecute in its critical section. That is, no two processes are executing in their\ncritical sections at the same time. The critical-section problem is to design a\nprotocol that the processes can use to cooperate. Each process must request\npermission to enter its critical section. The section of code implementing this\nrequest is the entry section. The critical section may be followed by an exit\nsection. The remaining code is the remainder section. The general structure of",
  "a typical process Pi is shown in Figure 5.1. The entry section and exit section\nare enclosed in boxes to highlight these important segments of code.\nA solution to the critical-section problem must satisfy the following three\nrequirements:\n1. Mutual exclusion. If process Pi is executing in its critical section, then no\nother processes can be executing in their critical sections.\n2. Progress. If no process is executing in its critical section and some\nprocesses wish to enter their critical sections, then only those processes\nthat are not executing in their remainder sections can participate in\ndeciding which will enter its critical section next, and this selection cannot\nbe postponed indeﬁnitely.\n3. Bounded waiting. There exists a bound, or limit, on the number of times",
  "that other processes are allowed to enter their critical sections after a 5.3\nPeterson’s Solution\n207\nprocess has made a request to enter its critical section and before that\nrequest is granted.\nWe assume that each process is executing at a nonzero speed. However, we can\nmake no assumption concerning the relative speed of the n processes.\nAt a given point in time, many kernel-mode processes may be active in\nthe operating system. As a result, the code implementing an operating system\n(kernel code) is subject to several possible race conditions. Consider as an\nexample a kernel data structure that maintains a list of all open ﬁles in the\nsystem. This list must be modiﬁed when a new ﬁle is opened or closed (adding",
  "the ﬁle to the list or removing it from the list). If two processes were to open ﬁles\nsimultaneously, the separate updates to this list could result in a race condition.\nOther kernel data structures that are prone to possible race conditions include\nstructures for maintaining memory allocation, for maintaining process lists,\nand for interrupt handling. It is up to kernel developers to ensure that the\noperating system is free from such race conditions.\nTwo general approaches are used to handle critical sections in operating\nsystems: preemptive kernels and nonpreemptive kernels. A preemptive\nkernel allows a process to be preempted while it is running in kernel mode. A\nnonpreemptive kernel does not allow a process running in kernel mode to be",
  "preempted; a kernel-mode process will run until it exits kernel mode, blocks,\nor voluntarily yields control of the CPU.\nObviously, a nonpreemptive kernel is essentially free from race conditions\non kernel data structures, as only one process is active in the kernel at a time.\nWe cannot say the same about preemptive kernels, so they must be carefully\ndesigned to ensure that shared kernel data are free from race conditions.\nPreemptive kernels are especially difﬁcult to design for SMP architectures,\nsince in these environments it is possible for two kernel-mode processes to run\nsimultaneously on different processors.\nWhy, then, would anyone favor a preemptive kernel over a nonpreemptive\none? A preemptive kernel may be more responsive, since there is less risk that a",
  "kernel-mode process will run for an arbitrarily long period before relinquishing\nthe processor to waiting processes. (Of course, this risk can also be minimized\nby designing kernel code that does not behave in this way.) Furthermore, a\npreemptive kernel is more suitable for real-time programming, as it will allow\na real-time process to preempt a process currently running in the kernel. Later\nin this chapter, we explore how various operating systems manage preemption\nwithin the kernel.\n5.3\nPeterson’s Solution\nNext, we illustrate a classic software-based solution to the critical-section\nproblem known as Peterson’s solution. Because of the way modern computer\narchitectures perform basic machine-language instructions, such as load and",
  "store, there are no guarantees that Peterson’s solution will work correctly on\nsuch architectures. However, we present the solution because it provides a good\nalgorithmic description of solving the critical-section problem and illustrates\nsome of the complexities involved in designing software that addresses the\nrequirements of mutual exclusion, progress, and bounded waiting. 208\nChapter 5\nProcess Synchronization\ndo {\nflag[i] = true;\nturn = j;\nwhile (flag[j] && turn == j);\ncritical section\nflag[i] = false;\nremainder section\n} while (true);\nFigure 5.2\nThe structure of process Pi in Peterson’s solution.\nPeterson’s solution is restricted to two processes that alternate execution\nbetween their critical sections and remainder sections. The processes are",
  "numbered P0 and P1. For convenience, when presenting Pi, we use Pj to\ndenote the other process; that is, j equals 1 −i.\nPeterson’s solution requires the two processes to share two data items:\nint turn;\nboolean flag[2];\nThe variable turn indicates whose turn it is to enter its critical section. That is,\nif turn == i, then process Pi is allowed to execute in its critical section. The\nflag array is used to indicate if a process is ready to enter its critical section.\nFor example, if flag[i] is true, this value indicates that Pi is ready to enter\nits critical section. With an explanation of these data structures complete, we\nare now ready to describe the algorithm shown in Figure 5.2.\nTo enter the critical section, process Pi ﬁrst sets flag[i] to be true and",
  "then sets turn to the value j, thereby asserting that if the other process wishes\nto enter the critical section, it can do so. If both processes try to enter at the same\ntime, turn will be set to both i and j at roughly the same time. Only one of these\nassignments will last; the other will occur but will be overwritten immediately.\nThe eventual value of turn determines which of the two processes is allowed\nto enter its critical section ﬁrst.\nWe now prove that this solution is correct. We need to show that:\n1. Mutual exclusion is preserved.\n2. The progress requirement is satisﬁed.\n3. The bounded-waiting requirement is met.\nTo prove property 1, we note that each Pi enters its critical section only\nif either flag[j] == false or turn == i. Also note that, if both processes",
  "can be executing in their critical sections at the same time, then flag[0] ==\nflag[1] == true. These two observations imply that P0 and P1 could not have\nsuccessfully executed their while statements at about the same time, since the 5.4\nSynchronization Hardware\n209\nvalue of turn can be either 0 or 1 but cannot be both. Hence, one of the processes\n—say, Pj —must have successfully executed the while statement, whereas Pi\nhad to execute at least one additional statement (“turn == j”). However, at\nthat time, flag[j] == true and turn == j, and this condition will persist as\nlong as Pj is in its critical section; as a result, mutual exclusion is preserved.\nTo prove properties 2 and 3, we note that a process Pi can be prevented from",
  "entering the critical section only if it is stuck in the while loop with the condition\nflag[j] == true and turn == j; this loop is the only one possible. If Pj is not\nready to enter the critical section, then flag[j] == false, and Pi can enter its\ncritical section. If Pj has set flag[j] to true and is also executing in its while\nstatement, then either turn == i or turn == j. If turn == i, then Pi will enter\nthe critical section. If turn == j, then Pj will enter the critical section. However,\nonce Pj exits its critical section, it will reset flag[j] to false, allowing Pi to\nenter its critical section. If Pj resets flag[j] to true, it must also set turn to i.\nThus, since Pi does not change the value of the variable turn while executing",
  "the while statement, Pi will enter the critical section (progress) after at most\none entry by Pj (bounded waiting).\n5.4\nSynchronization Hardware\nWe have just described one software-based solution to the critical-section\nproblem. However, as mentioned, software-based solutions such as Peterson’s\nare not guaranteed to work on modern computer architectures. In the following\ndiscussions, we explore several more solutions to the critical-section problem\nusing techniques ranging from hardware to software-based APIs available to\nboth kernel developers and application programmers. All these solutions are\nbased on the premise of locking —that is, protecting critical regions through\nthe use of locks. As we shall see, the designs of such locks can be quite\nsophisticated.",
  "sophisticated.\nWe start by presenting some simple hardware instructions that are available\non many systems and showing how they can be used effectively in solving the\ncritical-section problem. Hardware features can make any programming task\neasier and improve system efﬁciency.\nThe critical-section problem could be solved simply in a single-processor\nenvironment if we could prevent interrupts from occurring while a shared\nvariable was being modiﬁed. In this way, we could be sure that the current\nsequence of instructions would be allowed to execute in order without pre-\nemption. No other instructions would be run, so no unexpected modiﬁcations\ncould be made to the shared variable. This is often the approach taken by\nnonpreemptive kernels.\nboolean test and set(boolean *target) {",
  "boolean test and set(boolean *target) {\nboolean rv = *target;\n*target = true;\nreturn rv;\n}\nFigure 5.3\nThe deﬁnition of the test and set() instruction. 210\nChapter 5\nProcess Synchronization\ndo {\nwhile (test and set(&lock))\n; /* do nothing */\n/* critical section */\nlock = false;\n/* remainder section */\n} while (true);\nFigure 5.4\nMutual-exclusion implementation with test and set().\nUnfortunately, this solution is not as feasible in a multiprocessor environ-\nment. Disabling interrupts on a multiprocessor can be time consuming, since\nthe message is passed to all the processors. This message passing delays entry\ninto each critical section, and system efﬁciency decreases. Also consider the\neffect on a system’s clock if the clock is kept updated by interrupts.",
  "Many modern computer systems therefore provide special hardware\ninstructions that allow us either to test and modify the content of a word or\nto swap the contents of two words atomically—that is, as one uninterruptible\nunit. We can use these special instructions to solve the critical-section problem\nin a relatively simple manner. Rather than discussing one speciﬁc instruction\nfor one speciﬁc machine, we abstract the main concepts behind these types\nof instructions by describing the test and set() and compare and swap()\ninstructions.\nThe test and set() instruction can be deﬁned as shown in Figure 5.3.\nThe important characteristic of this instruction is that it is executed atomically.\nThus, if two test and set() instructions are executed simultaneously (each",
  "on a different CPU), they will be executed sequentially in some arbitrary order. If\nthe machine supports the test and set() instruction, then we can implement\nmutual exclusion by declaring a boolean variable lock, initialized to false.\nThe structure of process Pi is shown in Figure 5.4.\nThe compare and swap() instruction, in contrast to the test and set()\ninstruction, operates on three operands; it is deﬁned in Figure 5.5. The operand\nvalue is set to new value only if the expression (*value == exected) is\ntrue. Regardless, compare and swap() always returns the original value of the\nvariable value. Like the test and set() instruction, compare and swap() is\nint compare and swap(int *value, int expected, int new value) {\nint temp = *value;\nif (*value == expected)\n*value = new value;",
  "if (*value == expected)\n*value = new value;\nreturn temp;\n}\nFigure 5.5\nThe deﬁnition of the compare and swap() instruction. 5.4\nSynchronization Hardware\n211\ndo {\nwhile (compare and swap(&lock, 0, 1) != 0)\n; /* do nothing */\n/* critical section */\nlock = 0;\n/* remainder section */\n} while (true);\nFigure 5.6\nMutual-exclusion implementation with the compare and swap()\ninstruction.\nexecuted atomically. Mutual exclusion can be provided as follows: a global\nvariable (lock) is declared and is initialized to 0. The ﬁrst process that invokes\ncompare and swap() will set lock to 1. It will then enter its critical section,\nbecause the original value of lock was equal to the expected value of 0.\nSubsequent calls to compare and swap() will not succeed, because lock now",
  "is not equal to the expected value of 0. When a process exits its critical section,\nit sets lock back to 0, which allows another process to enter its critical section.\nThe structure of process Pi is shown in Figure 5.6.\nAlthough these algorithms satisfy the mutual-exclusion requirement, they\ndo not satisfy the bounded-waiting requirement. In Figure 5.7, we present\nanother algorithm using the test and set() instruction that satisﬁes all the\ncritical-section requirements. The common data structures are\ndo {\nwaiting[i] = true;\nkey = true;\nwhile (waiting[i] && key)\nkey = test and set(&lock);\nwaiting[i] = false;\n/* critical section */\nj = (i + 1) % n;\nwhile ((j != i) && !waiting[j])\nj = (j + 1) % n;\nif (j == i)\nlock = false;\nelse\nwaiting[j] = false;\n/* remainder section */\n} while (true);",
  "/* remainder section */\n} while (true);\nFigure 5.7\nBounded-waiting mutual exclusion with test and set(). 212\nChapter 5\nProcess Synchronization\nboolean waiting[n];\nboolean lock;\nThese data structures are initialized to false. To prove that the mutual-\nexclusion requirement is met, we note that process Pi can enter its critical\nsection only if either waiting[i] == false or key == false. The value\nof key can become false only if the test and set() is executed. The ﬁrst\nprocess to execute the test and set() will ﬁnd key == false; all others must\nwait. The variable waiting[i] can become false only if another process\nleaves its critical section; only one waiting[i] is set to false, maintaining the\nmutual-exclusion requirement.",
  "mutual-exclusion requirement.\nTo prove that the progress requirement is met, we note that the arguments\npresented for mutual exclusion also apply here, since a process exiting the\ncritical section either sets lock to false or sets waiting[j] to false. Both\nallow a process that is waiting to enter its critical section to proceed.\nTo prove that the bounded-waiting requirement is met, we note that, when\na process leaves its critical section, it scans the array waiting in the cyclic\nordering (i + 1, i + 2, ..., n −1, 0, ..., i −1). It designates the ﬁrst process in this\nordering that is in the entry section (waiting[j] == true) as the next one to\nenter the critical section. Any process waiting to enter its critical section will\nthus do so within n −1 turns.",
  "thus do so within n −1 turns.\nDetails describing the implementation of the atomic test and set()\nand compare and swap() instructions are discussed more fully in books on\ncomputer architecture.\n5.5\nMutex Locks\nThe hardware-based solutions to the critical-section problem presented in\nSection 5.4 are complicated as well as generally inaccessible to application\nprogrammers. Instead, operating-systems designers build software tools to\nsolve the critical-section problem. The simplest of these tools is the mutex\nlock. (In fact, the term mutex is short for mutual exclusion.) We use the mutex\nlock to protect critical regions and thus prevent race conditions. That is, a\nprocess must acquire the lock before entering a critical section; it releases the",
  "lock when it exits the critical section. The acquire()function acquires the lock,\nand the release() function releases the lock, as illustrated in Figure 5.8.\nA mutex lock has a boolean variable available whose value indicates if\nthe lock is available or not. If the lock is available, a call to acquire() succeeds,\nand the lock is then considered unavailable. A process that attempts to acquire\nan unavailable lock is blocked until the lock is released.\nThe deﬁnition of acquire() is as follows:\nacquire() {\nwhile (!available)\n; /* busy wait */\navailable = false;;\n} 5.6\nSemaphores\n213\ndo {\nacquire lock\ncritical section\nrelease lock\nremainder section\n} while (true);\nFigure 5.8\nSolution to the critical-section problem using mutex locks.\nThe deﬁnition of release() is as follows:\nrelease() {",
  "release() {\navailable = true;\n}\nCalls to either acquire() or release() must be performed atomically.\nThus, mutex locks are often implemented using one of the hardware mecha-\nnisms described in Section 5.4, and we leave the description of this technique\nas an exercise.\nThe main disadvantage of the implementation given here is that it requires\nbusy waiting. While a process is in its critical section, any other process that\ntries to enter its critical section must loop continuously in the call to acquire().\nIn fact, this type of mutex lock is also called a spinlock because the process\n“spins” while waiting for the lock to become available. (We see the same issue\nwith the code examples illustrating the test and set() instruction and the",
  "compare and swap() instruction.) This continual looping is clearly a problem\nin a real multiprogramming system, where a single CPU is shared among many\nprocesses. Busy waiting wastes CPU cycles that some other process might be\nable to use productively.\nSpinlocks do have an advantage, however, in that no context switch is\nrequired when a process must wait on a lock, and a context switch may\ntake considerable time. Thus, when locks are expected to be held for short\ntimes, spinlocks are useful. They are often employed on multiprocessor systems\nwhere one thread can “spin” on one processor while another thread performs\nits critical section on another processor.\nLater in this chapter (Section 5.7), we examine how mutex locks can be",
  "used to solve classical synchronization problems. We also discuss how these\nlocks are used in several operating systems, as well as in Pthreads.\n5.6\nSemaphores\nMutex locks, as we mentioned earlier, are generally considered the simplest of\nsynchronization tools. In this section, we examine a more robust tool that can 214\nChapter 5\nProcess Synchronization\nbehave similarly to a mutex lock but can also provide more sophisticated ways\nfor processes to synchronize their activities.\nA semaphore S is an integer variable that, apart from initialization, is\naccessed only through two standard atomic operations: wait() and signal().\nThe wait() operation was originally termed P (from the Dutch proberen, “to\ntest”); signal() was originally called V (from verhogen, “to increment”). The",
  "deﬁnition of wait() is as follows:\nwait(S) {\nwhile (S <= 0)\n; // busy wait\nS--;\n}\nThe deﬁnition of signal() is as follows:\nsignal(S) {\nS++;\n}\nAll modiﬁcations to the integer value of the semaphore in the wait() and\nsignal() operations must be executed indivisibly. That is, when one process\nmodiﬁes the semaphore value, no other process can simultaneously modify\nthat same semaphore value. In addition, in the case of wait(S), the testing of\nthe integer value of S (S ≤0), as well as its possible modiﬁcation (S--), must\nbe executed without interruption. We shall see how these operations can be\nimplemented in Section 5.6.2. First, let’s see how semaphores can be used.\n5.6.1\nSemaphore Usage\nOperating systems often distinguish between counting and binary semaphores.",
  "The value of a counting semaphore can range over an unrestricted domain.\nThe value of a binary semaphore can range only between 0 and 1. Thus, binary\nsemaphores behave similarly to mutex locks. In fact, on systems that do not\nprovide mutex locks, binary semaphores can be used instead for providing\nmutual exclusion.\nCounting semaphores can be used to control access to a given resource\nconsisting of a ﬁnite number of instances. The semaphore is initialized to the\nnumber of resources available. Each process that wishes to use a resource\nperforms a wait() operation on the semaphore (thereby decrementing the\ncount). When a process releases a resource, it performs a signal() operation\n(incrementing the count). When the count for the semaphore goes to 0, all",
  "resources are being used. After that, processes that wish to use a resource will\nblock until the count becomes greater than 0.\nWe can also use semaphores to solve various synchronization problems.\nFor example, consider two concurrently running processes: P1 with a statement\nS1 and P2 with a statement S2. Suppose we require that S2 be executed only\nafter S1 has completed. We can implement this scheme readily by letting P1\nand P2 share a common semaphore synch, initialized to 0. In process P1, we\ninsert the statements 5.6\nSemaphores\n215\nS1;\nsignal(synch);\nIn process P2, we insert the statements\nwait(synch);\nS2;\nBecause synch is initialized to 0, P2 will execute S2 only after P1 has invoked\nsignal(synch), which is after statement S1 has been executed.\n5.6.2\nSemaphore Implementation",
  "5.6.2\nSemaphore Implementation\nRecall that the implementation of mutex locks discussed in Section 5.5 suffers\nfrom busy waiting. The deﬁnitions of the wait() and signal() semaphore\noperations just described present the same problem. To overcome the need\nfor busy waiting, we can modify the deﬁnition of the wait() and signal()\noperations as follows: When a process executes the wait() operation and ﬁnds\nthat the semaphore value is not positive, it must wait. However, rather than\nengaging in busy waiting, the process can block itself. The block operation\nplaces a process into a waiting queue associated with the semaphore, and the\nstate of the process is switched to the waiting state. Then control is transferred\nto the CPU scheduler, which selects another process to execute.",
  "A process that is blocked, waiting on a semaphore S, should be restarted\nwhen some other process executes a signal() operation. The process is\nrestarted by a wakeup() operation, which changes the process from the waiting\nstate to the ready state. The process is then placed in the ready queue. (The\nCPU may or may not be switched from the running process to the newly ready\nprocess, depending on the CPU-scheduling algorithm.)\nTo implement semaphores under this deﬁnition, we deﬁne a semaphore as\nfollows:\ntypedef struct {\nint value;\nstruct process *list;\n} semaphore;\nEach semaphore has an integer value and a list of processes list. When\na process must wait on a semaphore, it is added to the list of processes. A\nsignal() operation removes one process from the list of waiting processes",
  "and awakens that process.\nNow, the wait() semaphore operation can be deﬁned as\nwait(semaphore *S) {\nS->value--;\nif (S->value < 0) {\nadd this process to S->list;\nblock();\n}\n} 216\nChapter 5\nProcess Synchronization\nand the signal() semaphore operation can be deﬁned as\nsignal(semaphore *S) {\nS->value++;\nif (S->value <= 0) {\nremove a process P from S->list;\nwakeup(P);\n}\n}\nThe block() operation suspends the process that invokes it. The wakeup(P)\noperation resumes the execution of a blocked process P. These two operations\nare provided by the operating system as basic system calls.\nNote that in this implementation, semaphore values may be negative,\nwhereas semaphore values are never negative under the classical deﬁnition of",
  "semaphores with busy waiting. If a semaphore value is negative, its magnitude\nis the number of processes waiting on that semaphore. This fact results from\nswitching the order of the decrement and the test in the implementation of the\nwait() operation.\nThe list of waiting processes can be easily implemented by a link ﬁeld in\neach process control block (PCB). Each semaphore contains an integer value and\na pointer to a list of PCBs. One way to add and remove processes from the list\nso as to ensure bounded waiting is to use a FIFO queue, where the semaphore\ncontains both head and tail pointers to the queue. In general, however, the list\ncan use any queueing strategy. Correct usage of semaphores does not depend\non a particular queueing strategy for the semaphore lists.",
  "It is critical that semaphore operations be executed atomically. We must\nguarantee that no two processes can execute wait() and signal() operations\non the same semaphore at the same time. This is a critical-section problem;\nand in a single-processor environment, we can solve it by simply inhibiting\ninterrupts during the time the wait() and signal() operations are executing.\nThis scheme works in a single-processor environment because, once interrupts\nare inhibited, instructions from different processes cannot be interleaved. Only\nthe currently running process executes until interrupts are reenabled and the\nscheduler can regain control.\nIn a multiprocessor environment, interrupts must be disabled on every pro-\ncessor. Otherwise, instructions from different processes (running on different",
  "processors) may be interleaved in some arbitrary way. Disabling interrupts on\nevery processor can be a difﬁcult task and furthermore can seriously diminish\nperformance. Therefore, SMP systems must provide alternative locking tech-\nniques—such as compare and swap() or spinlocks—to ensure that wait()\nand signal() are performed atomically.\nIt is important to admit that we have not completely eliminated busy\nwaiting with this deﬁnition of the wait() and signal() operations. Rather,\nwe have moved busy waiting from the entry section to the critical sections\nof application programs. Furthermore, we have limited busy waiting to the\ncritical sections of the wait() and signal() operations, and these sections are\nshort (if properly coded, they should be no more than about ten instructions).",
  "Thus, the critical section is almost never occupied, and busy waiting occurs 5.6\nSemaphores\n217\nrarely, and then for only a short time. An entirely different situation exists\nwith application programs whose critical sections may be long (minutes or\neven hours) or may almost always be occupied. In such cases, busy waiting is\nextremely inefﬁcient.\n5.6.3\nDeadlocks and Starvation\nThe implementation of a semaphore with a waiting queue may result in a\nsituation where two or more processes are waiting indeﬁnitely for an event\nthat can be caused only by one of the waiting processes. The event in question\nis the execution of a signal() operation. When such a state is reached, these\nprocesses are said to be deadlocked.\nTo illustrate this, consider a system consisting of two processes, P0 and P1,",
  "each accessing two semaphores, S and Q, set to the value 1:\nP0\nP1\nwait(S);\nwait(Q);\nwait(Q);\nwait(S);\n.\n.\n.\n.\n.\n.\nsignal(S);\nsignal(Q);\nsignal(Q);\nsignal(S);\nSuppose that P0 executes wait(S) and then P1 executes wait(Q). When P0\nexecutes wait(Q), it must wait until P1 executes signal(Q). Similarly, when\nP1 executes wait(S), it must wait until P0 executes signal(S). Since these\nsignal() operations cannot be executed, P0 and P1 are deadlocked.\nWe say that a set of processes is in a deadlocked state when every process\nin the set is waiting for an event that can be caused only by another process\nin the set. The events with which we are mainly concerned here are resource\nacquisition and release. Other types of events may result in deadlocks, as we",
  "show in Chapter 7. In that chapter, we describe various mechanisms for dealing\nwith the deadlock problem.\nAnother problem related to deadlocks is indeﬁnite blocking or starvation,\na situation in which processes wait indeﬁnitely within the semaphore. Indeﬁ-\nnite blocking may occur if we remove processes from the list associated with a\nsemaphore in LIFO (last-in, ﬁrst-out) order.\n5.6.4\nPriority Inversion\nA scheduling challenge arises when a higher-priority process needs to read\nor modify kernel data that are currently being accessed by a lower-priority\nprocess—or a chain of lower-priority processes. Since kernel data are typically\nprotected with a lock, the higher-priority process will have to wait for a\nlower-priority one to ﬁnish with the resource. The situation becomes more",
  "complicated if the lower-priority process is preempted in favor of another\nprocess with a higher priority.\nAs an example, assume we have three processes—L, M, and H —whose\npriorities follow the order L < M < H. Assume that process H requires 218\nChapter 5\nProcess Synchronization\nPRIORITY INVERSION AND THE MARS PATHFINDER\nPriority inversion can be more than a scheduling inconvenience. On systems\nwith tight time constraints—such as real-time systems—priority inversion\ncan cause a process to take longer than it should to accomplish a task. When\nthat happens, other failures can cascade, resulting in system failure.\nConsider the Mars Pathﬁnder, a NASA space probe that landed a robot, the\nSojourner rover, on Mars in 1997 to conduct experiments. Shortly after the",
  "Sojourner began operating, it started to experience frequent computer resets.\nEach reset reinitialized all hardware and software, including communica-\ntions. If the problem had not been solved, the Sojourner would have failed in\nits mission.\nThe problem was caused by the fact that one high-priority task, “bc dist,”\nwas taking longer than expected to complete its work. This task was being\nforced to wait for a shared resource that was held by the lower-priority\n“ASI/MET” task, which in turn was preempted by multiple medium-priority\ntasks. The “bc dist” task would stall waiting for the shared resource, and\nultimately the “bc sched” task would discover the problem and perform the\nreset. The Sojourner was suffering from a typical case of priority inversion.",
  "The operating system on the Sojourner was the VxWorks real-time operat-\ning system, which had a global variable to enable priority inheritance on all\nsemaphores. After testing, the variable was set on the Sojourner (on Mars!),\nand the problem was solved.\nA\nfull\ndescription\nof\nthe\nproblem,\nits\ndetection,\nand\nits\nsolu-\ntion\nwas\nwritten\nby\nthe\nsoftware\nteam\nlead\nand\nis\navailable\nat\nhttp://research.microsoft.com/en-us/um/people/mbj/mars pathﬁnder/\nauthoritative account.html.\nresource R, which is currently being accessed by process L. Ordinarily, process\nH would wait for L to ﬁnish using resource R. However, now suppose that\nprocess M becomes runnable, thereby preempting process L. Indirectly, a\nprocess with a lower priority—process M—has affected how long process",
  "H must wait for L to relinquish resource R.\nThis problem is known as priority inversion. It occurs only in systems with\nmore than two priorities, so one solution is to have only two priorities. That is\ninsufﬁcient for most general-purpose operating systems, however. Typically\nthese systems solve the problem by implementing a priority-inheritance\nprotocol. According to this protocol, all processes that are accessing resources\nneeded by a higher-priority process inherit the higher priority until they are\nﬁnished with the resources in question. When they are ﬁnished, their priorities\nrevert to their original values. In the example above, a priority-inheritance\nprotocol would allow process L to temporarily inherit the priority of process",
  "H, thereby preventing process M from preempting its execution. When process\nL had ﬁnished using resource R, it would relinquish its inherited priority from\nH and assume its original priority. Because resource R would now be available,\nprocess H —not M—would run next. 5.7\nClassic Problems of Synchronization\n219\ndo {\n. . .\n/* produce an item in next produced */\n. . .\nwait(empty);\nwait(mutex);\n. . .\n/* add next produced to the buffer */\n. . .\nsignal(mutex);\nsignal(full);\n} while (true);\nFigure 5.9\nThe structure of the producer process.\n5.7\nClassic Problems of Synchronization\nIn this section, we present a number of synchronization problems as examples\nof a large class of concurrency-control problems. These problems are used for",
  "testing nearly every newly proposed synchronization scheme. In our solutions\nto the problems, we use semaphores for synchronization, since that is the\ntraditional way to present such solutions. However, actual implementations of\nthese solutions could use mutex locks in place of binary semaphores.\n5.7.1\nThe Bounded-Buffer Problem\nThe bounded-buffer problem was introduced in Section 5.1; it is commonly\nused to illustrate the power of synchronization primitives. Here, we present a\ngeneral structure of this scheme without committing ourselves to any particular\nimplementation. We provide a related programming project in the exercises at\nthe end of the chapter.\nIn our problem, the producer and consumer processes share the following\ndata structures:\nint n;\nsemaphore mutex = 1;",
  "data structures:\nint n;\nsemaphore mutex = 1;\nsemaphore empty = n;\nsemaphore full = 0\nWe assume that the pool consists of n buffers, each capable of holding one item.\nThe mutex semaphore provides mutual exclusion for accesses to the buffer pool\nand is initialized to the value 1. The empty and full semaphores count the\nnumber of empty and full buffers. The semaphore empty is initialized to the\nvalue n; the semaphore full is initialized to the value 0.\nThe code for the producer process is shown in Figure 5.9, and the code\nfor the consumer process is shown in Figure 5.10. Note the symmetry between\nthe producer and the consumer. We can interpret this code as the producer\nproducing full buffers for the consumer or as the consumer producing empty\nbuffers for the producer. 220\nChapter 5",
  "buffers for the producer. 220\nChapter 5\nProcess Synchronization\ndo {\nwait(full);\nwait(mutex);\n. . .\n/* remove an item from buffer to next consumed */\n. . .\nsignal(mutex);\nsignal(empty);\n. . .\n/* consume the item in next consumed */\n. . .\n} while (true);\nFigure 5.10\nThe structure of the consumer process.\n5.7.2\nThe Readers–Writers Problem\nSuppose that a database is to be shared among several concurrent processes.\nSome of these processes may want only to read the database, whereas others\nmay want to update (that is, to read and write) the database. We distinguish\nbetween these two types of processes by referring to the former as readers\nand to the latter as writers. Obviously, if two readers access the shared data\nsimultaneously, no adverse effects will result. However, if a writer and some",
  "other process (either a reader or a writer) access the database simultaneously,\nchaos may ensue.\nTo ensure that these difﬁculties do not arise, we require that the writers\nhave exclusive access to the shared database while writing to the database. This\nsynchronization problem is referred to as the readers–writers problem. Since it\nwas originally stated, it has been used to test nearly every new synchronization\nprimitive. The readers–writers problem has several variations, all involving\npriorities. The simplest one, referred to as the ﬁrst readers–writers problem,\nrequires that no reader be kept waiting unless a writer has already obtained\npermission to use the shared object. In other words, no reader should wait for",
  "other readers to ﬁnish simply because a writer is waiting. The second readers\n–writers problem requires that, once a writer is ready, that writer perform its\nwrite as soon as possible. In other words, if a writer is waiting to access the\nobject, no new readers may start reading.\nA solution to either problem may result in starvation. In the ﬁrst case,\nwriters may starve; in the second case, readers may starve. For this reason,\nother variants of the problem have been proposed. Next, we present a solution\nto the ﬁrst readers–writers problem. See the bibliographical notes at the end\nof the chapter for references describing starvation-free solutions to the second\nreaders–writers problem.\nIn the solution to the ﬁrst readers–writers problem, the reader processes",
  "share the following data structures:\nsemaphore rw mutex = 1;\nsemaphore mutex = 1;\nint read count = 0;\nThe semaphores mutex and rw mutex are initialized to 1; read count is\ninitialized to 0. The semaphore rw mutex is common to both reader and writer 5.7\nClassic Problems of Synchronization\n221\ndo {\nwait(rw mutex);\n. . .\n/* writing is performed */\n. . .\nsignal(rw mutex);\n} while (true);\nFigure 5.11\nThe structure of a writer process.\nprocesses. The mutex semaphore is used to ensure mutual exclusion when the\nvariable read count is updated. The read count variable keeps track of how\nmany processes are currently reading the object. The semaphore rw mutex\nfunctions as a mutual exclusion semaphore for the writers. It is also used by",
  "the ﬁrst or last reader that enters or exits the critical section. It is not used by\nreaders who enter or exit while other readers are in their critical sections.\nThe code for a writer process is shown in Figure 5.11; the code for a\nreader process is shown in Figure 5.12. Note that, if a writer is in the critical\nsection and n readers are waiting, then one reader is queued on rw mutex, and\nn −1 readers are queued on mutex. Also observe that, when a writer executes\nsignal(rw mutex), we may resume the execution of either the waiting readers\nor a single waiting writer. The selection is made by the scheduler.\nThe readers–writers problem and its solutions have been generalized to\nprovide reader–writer locks on some systems. Acquiring a reader–writer lock",
  "requires specifying the mode of the lock: either read or write access. When a\nprocess wishes only to read shared data, it requests the reader–writer lock\nin read mode. A process wishing to modify the shared data must request the\nlock in write mode. Multiple processes are permitted to concurrently acquire\na reader–writer lock in read mode, but only one process may acquire the lock\nfor writing, as exclusive access is required for writers.\nReader–writer locks are most useful in the following situations:\ndo {\nwait(mutex);\nread count++;\nif (read count == 1)\nwait(rw mutex);\nsignal(mutex);\n. . .\n/* reading is performed */\n. . .\nwait(mutex);\nread count--;\nif (read count == 0)\nsignal(rw mutex);\nsignal(mutex);\n} while (true);\nFigure 5.12\nThe structure of a reader process. 222\nChapter 5",
  "The structure of a reader process. 222\nChapter 5\nProcess Synchronization\nRICE\nFigure 5.13\nThe situation of the dining philosophers.\n• In applications where it is easy to identify which processes only read shared\ndata and which processes only write shared data.\n• In applications that have more readers than writers. This is because reader–\nwriter locks generally require more overhead to establish than semaphores\nor mutual-exclusion locks. The increased concurrency of allowing multiple\nreaders compensates for the overhead involved in setting up the reader–\nwriter lock.\n5.7.3\nThe Dining-Philosophers Problem\nConsider ﬁve philosophers who spend their lives thinking and eating. The\nphilosophers share a circular table surrounded by ﬁve chairs, each belonging",
  "to one philosopher. In the center of the table is a bowl of rice, and the table is laid\nwith ﬁve single chopsticks (Figure 5.13). When a philosopher thinks, she does\nnot interact with her colleagues. From time to time, a philosopher gets hungry\nand tries to pick up the two chopsticks that are closest to her (the chopsticks\nthat are between her and her left and right neighbors). A philosopher may pick\nup only one chopstick at a time. Obviously, she cannot pick up a chopstick that\nis already in the hand of a neighbor. When a hungry philosopher has both her\nchopsticks at the same time, she eats without releasing the chopsticks. When\nshe is ﬁnished eating, she puts down both chopsticks and starts thinking again.\nThe dining-philosophers problem is considered a classic synchronization",
  "problem neither because of its practical importance nor because computer\nscientists dislike philosophers but because it is an example of a large class\nof concurrency-control problems. It is a simple representation of the need\nto allocate several resources among several processes in a deadlock-free and\nstarvation-free manner.\nOne simple solution is to represent each chopstick with a semaphore. A\nphilosopher tries to grab a chopstick by executing a wait() operation on that\nsemaphore. She releases her chopsticks by executing the signal() operation\non the appropriate semaphores. Thus, the shared data are\nsemaphore chopstick[5]; 5.8\nMonitors\n223\ndo {\nwait(chopstick[i]);\nwait(chopstick[(i+1) % 5]);\n. . .\n/* eat for awhile */\n. . .\nsignal(chopstick[i]);\nsignal(chopstick[(i+1) % 5]);\n. . .",
  "signal(chopstick[(i+1) % 5]);\n. . .\n/* think for awhile */\n. . .\n} while (true);\nFigure 5.14\nThe structure of philosopher i.\nwhere all the elements of chopstick are initialized to 1. The structure of\nphilosopher i is shown in Figure 5.14.\nAlthough this solution guarantees that no two neighbors are eating\nsimultaneously, it nevertheless must be rejected because it could create a\ndeadlock. Suppose that all ﬁve philosophers become hungry at the same time\nand each grabs her left chopstick. All the elements of chopstick will now be\nequal to 0. When each philosopher tries to grab her right chopstick, she will be\ndelayed forever.\nSeveral possible remedies to the deadlock problem are replaced by:\n• Allow at most four philosophers to be sitting simultaneously at the table.",
  "• Allow a philosopher to pick up her chopsticks only if both chopsticks are\navailable (to do this, she must pick them up in a critical section).\n• Use an asymmetric solution—that is, an odd-numbered philosopher picks\nup ﬁrst her left chopstick and then her right chopstick, whereas an even-\nnumbered philosopher picks up her right chopstick and then her left\nchopstick.\nIn Section 5.8, we present a solution to the dining-philosophers problem\nthat ensures freedom from deadlocks. Note, however, that any satisfactory\nsolution to the dining-philosophers problem must guard against the possibility\nthat one of the philosophers will starve to death. A deadlock-free solution does\nnot necessarily eliminate the possibility of starvation.\n5.8\nMonitors",
  "5.8\nMonitors\nAlthough semaphores provide a convenient and effective mechanism for\nprocess synchronization, using them incorrectly can result in timing errors\nthat are difﬁcult to detect, since these errors happen only if particular execution\nsequences take place and these sequences do not always occur.\nWe have seen an example of such errors in the use of counters in our\nsolution to the producer–consumer problem (Section 5.1). In that example,\nthe timing problem happened only rarely, and even then the counter value 224\nChapter 5\nProcess Synchronization\nappeared to be reasonable—off by only 1. Nevertheless, the solution is\nobviously not an acceptable one. It is for this reason that semaphores were\nintroduced in the ﬁrst place.",
  "introduced in the ﬁrst place.\nUnfortunately, such timing errors can still occur when semaphores are\nused. To illustrate how, we review the semaphore solution to the critical-section\nproblem. All processes share a semaphore variable mutex, which is initialized\nto 1. Each process must executewait(mutex) before entering the critical section\nand signal(mutex) afterward. If this sequence is not observed, two processes\nmay be in their critical sections simultaneously. Next, we examine the various\ndifﬁculties that may result. Note that these difﬁculties will arise even if a\nsingle process is not well behaved. This situation may be caused by an honest\nprogramming error or an uncooperative programmer.\n• Suppose that a process interchanges the order in which the wait() and",
  "signal() operations on the semaphore mutex are executed, resulting in\nthe following execution:\nsignal(mutex);\n...\ncritical section\n...\nwait(mutex);\nIn this situation, several processes may be executing in their critical sections\nsimultaneously, violating the mutual-exclusion requirement. This error\nmay be discovered only if several processes are simultaneously active\nin their critical sections. Note that this situation may not always be\nreproducible.\n• Suppose that a process replaces signal(mutex) with wait(mutex). That\nis, it executes\nwait(mutex);\n...\ncritical section\n...\nwait(mutex);\nIn this case, a deadlock will occur.\n• Suppose that a process omits the wait(mutex), or the signal(mutex), or\nboth. In this case, either mutual exclusion is violated or a deadlock will\noccur.",
  "occur.\nThese examples illustrate that various types of errors can be generated easily\nwhen programmers use semaphores incorrectly to solve the critical-section\nproblem. Similar problems may arise in the other synchronization models\ndiscussed in Section 5.7.\nTo deal with such errors, researchers have developed high-level language\nconstructs. In this section, we describe one fundamental high-level synchro-\nnization construct—the monitor type. 5.8\nMonitors\n225\nmonitor monitor name\n{\n/* shared variable declarations */\nfunction P1 ( . . . ) {\n. . .\n}\nfunction P2 ( . . . ) {\n. . .\n}\n.\n.\n.\nfunction Pn ( . . . ) {\n. . .\n}\ninitialization code ( . . . ) {\n. . .\n}\n}\nFigure 5.15\nSyntax of a monitor.\n5.8.1\nMonitor Usage\nAn abstract data type—or ADT—encapsulates data with a set of functions",
  "to operate on that data that are independent of any speciﬁc implementation\nof the ADT. A monitor type is an ADT that includes a set of programmer-\ndeﬁned operations that are provided with mutual exclusion within the monitor.\nThe monitor type also declares the variables whose values deﬁne the state\nof an instance of that type, along with the bodies of functions that operate\non those variables. The syntax of a monitor type is shown in Figure 5.15.\nThe representation of a monitor type cannot be used directly by the various\nprocesses. Thus, a function deﬁned within a monitor can access only those\nvariables declared locally within the monitor and its formal parameters.\nSimilarly, the local variables of a monitor can be accessed by only the local\nfunctions.",
  "functions.\nThe monitor construct ensures that only one process at a time is active\nwithin the monitor. Consequently, the programmer does not need to code\nthis synchronization constraint explicitly (Figure 5.16). However, the monitor\nconstruct, as deﬁned so far, is not sufﬁciently powerful for modeling some\nsynchronization schemes. For this purpose, we need to deﬁne additional syn-\nchronization mechanisms. These mechanisms are provided by the condition\nconstruct. A programmer who needs to write a tailor-made synchronization\nscheme can deﬁne one or more variables of type condition:\ncondition x, y; 226\nChapter 5\nProcess Synchronization\nentry queue\nshared data\noperations\ninitialization\ncode\n. . .\nFigure 5.16\nSchematic view of a monitor.",
  ". . .\nFigure 5.16\nSchematic view of a monitor.\nThe only operations that can be invoked on a condition variable are wait()\nand signal(). The operation\nx.wait();\nmeans that the process invoking this operation is suspended until another\nprocess invokes\nx.signal();\nThe x.signal() operation resumes exactly one suspended process. If no\nprocess is suspended, then the signal() operation has no effect; that is, the\nstate of x is the same as if the operation had never been executed (Figure\n5.17). Contrast this operation with the signal() operation associated with\nsemaphores, which always affects the state of the semaphore.\nNow suppose that, when the x.signal() operation is invoked by a process\nP, there exists a suspended process Q associated with condition x. Clearly, if the",
  "suspended process Q is allowed to resume its execution, the signaling process\nP must wait. Otherwise, both P and Q would be active simultaneously within\nthe monitor. Note, however, that conceptually both processes can continue\nwith their execution. Two possibilities exist:\n1. Signal and wait. P either waits until Q leaves the monitor or waits for\nanother condition.\n2. Signal and continue. Q either waits until P leaves the monitor or waits\nfor another condition. 5.8\nMonitors\n227\noperations\nqueues associated with\nx, y conditions\nentry queue\nshared data\nx \ny\ninitialization\ncode\n• • •\nFigure 5.17\nMonitor with condition variables.\nThere are reasonable arguments in favor of adopting either option. On\nthe one hand, since P was already executing in the monitor, the signal-and-",
  "continue method seems more reasonable. On the other, if we allow thread P\nto continue, then by the time Q is resumed, the logical condition for which Q\nwas waiting may no longer hold. A compromise between these two choices\nwas adopted in the language Concurrent Pascal. When thread P executes the\nsignal operation, it immediately leaves the monitor. Hence, Q is immediately\nresumed.\nMany programming languages have incorporated the idea of the monitor\nas described in this section, including Java and C# (pronounced “C-sharp”).\nOther languages—such as Erlang—provide some type of concurrency support\nusing a similar mechanism.\n5.8.2\nDining-Philosophers Solution Using Monitors\nNext, we illustrate monitor concepts by presenting a deadlock-free solution to",
  "the dining-philosophers problem. This solution imposes the restriction that a\nphilosopher may pick up her chopsticks only if both of them are available. To\ncode this solution, we need to distinguish among three states in which we may\nﬁnd a philosopher. For this purpose, we introduce the following data structure:\nenum {THINKING, HUNGRY, EATING} state[5];\nPhilosopher i can set the variable state[i] = EATING only if her two\nneighbors are not eating: (state[(i+4) % 5] != EATING) and (state[(i+1)\n% 5] != EATING). 228\nChapter 5\nProcess Synchronization\nmonitor DiningPhilosophers\n{\nenum {THINKING, HUNGRY, EATING} state[5];\ncondition self[5];\nvoid pickup(int i) {\nstate[i] = HUNGRY;\ntest(i);\nif (state[i] != EATING)\nself[i].wait();\n}\nvoid putdown(int i) {\nstate[i] = THINKING;\ntest((i + 4) % 5);",
  "state[i] = THINKING;\ntest((i + 4) % 5);\ntest((i + 1) % 5);\n}\nvoid test(int i) {\nif ((state[(i + 4) % 5] != EATING) &&\n(state[i] == HUNGRY) &&\n(state[(i + 1) % 5] != EATING)) {\nstate[i] = EATING;\nself[i].signal();\n}\n}\ninitialization code() {\nfor (int i = 0; i < 5; i++)\nstate[i] = THINKING;\n}\n}\nFigure 5.18\nA monitor solution to the dining-philosopher problem.\nWe also need to declare\ncondition self[5];\nThis allows philosopher i to delay herself when she is hungry but is unable to\nobtain the chopsticks she needs.\nWe are now in a position to describe our solution to the dining-philosophers\nproblem. The distribution of the chopsticks is controlled by the monitor Din-\ningPhilosophers, whose deﬁnition is shown in Figure 5.18. Each philosopher,",
  "before starting to eat, must invoke the operation pickup(). This act may result\nin the suspension of the philosopher process. After the successful completion of\nthe operation, the philosopher may eat. Following this, the philosopher invokes\nthe putdown() operation. Thus, philosopher i must invoke the operations\npickup() and putdown() in the following sequence: 5.8\nMonitors\n229\nDiningPhilosophers.pickup(i);\n...\neat\n...\nDiningPhilosophers.putdown(i);\nIt is easy to show that this solution ensures that no two neighbors are eating\nsimultaneously and that no deadlocks will occur. We note, however, that it is\npossible for a philosopher to starve to death. We do not present a solution to\nthis problem but rather leave it as an exercise for you.\n5.8.3\nImplementing a Monitor Using Semaphores",
  "5.8.3\nImplementing a Monitor Using Semaphores\nWe now consider a possible implementation of the monitor mechanism using\nsemaphores. For each monitor, a semaphore mutex (initialized to 1) is provided.\nA process must execute wait(mutex) before entering the monitor and must\nexecute signal(mutex) after leaving the monitor.\nSince a signaling process must wait until the resumed process either leaves\nor waits, an additional semaphore, next, is introduced, initialized to 0. The\nsignaling processes can use next to suspend themselves. An integer variable\nnext count is also provided to count the number of processes suspended on\nnext. Thus, each external function F is replaced by\nwait(mutex);\n...\nbody of F\n...\nif (next count > 0)\nsignal(next);\nelse\nsignal(mutex);",
  "signal(next);\nelse\nsignal(mutex);\nMutual exclusion within a monitor is ensured.\nWe can now describe how condition variables are implemented as well.\nFor each condition x, we introduce a semaphore x sem and an integer\nvariable x count, both initialized to 0. The operation x.wait() can now be\nimplemented as\nx count++;\nif (next count > 0)\nsignal(next);\nelse\nsignal(mutex);\nwait(x sem);\nx count--;\nThe operation x.signal() can be implemented as 230\nChapter 5\nProcess Synchronization\nif (x count > 0) {\nnext count++;\nsignal(x sem);\nwait(next);\nnext count--;\n}\nThis implementation is applicable to the deﬁnitions of monitors given by\nboth Hoare and Brinch-Hansen (see the bibliographical notes at the end of\nthe chapter). In some cases, however, the generality of the implementation is",
  "unnecessary, and a signiﬁcant improvement in efﬁciency is possible. We leave\nthis problem to you in Exercise 5.30.\n5.8.4\nResuming Processes within a Monitor\nWe turn now to the subject of process-resumption order within a monitor. If\nseveral processes are suspended on condition x, and an x.signal() operation\nis executed by some process, then how do we determine which of the\nsuspended processes should be resumed next? One simple solution is to use a\nﬁrst-come, ﬁrst-served (FCFS) ordering, so that the process that has been waiting\nthe longest is resumed ﬁrst. In many circumstances, however, such a simple\nscheduling scheme is not adequate. For this purpose, the conditional-wait\nconstruct can be used. This construct has the form\nx.wait(c);",
  "x.wait(c);\nwhere c is an integer expression that is evaluated when the wait() operation\nis executed. The value of c, which is called a priority number, is then stored\nwith the name of the process that is suspended. When x.signal() is executed,\nthe process with the smallest priority number is resumed next.\nTo illustrate this new mechanism, consider the ResourceAllocator mon-\nitor shown in Figure 5.19, which controls the allocation of a single resource\namong competing processes. Each process, when requesting an allocation of\nthis resource, speciﬁes the maximum time it plans to use the resource. The mon-\nitor allocates the resource to the process that has the shortest time-allocation\nrequest. A process that needs to access the resource in question must observe\nthe following sequence:",
  "the following sequence:\nR.acquire(t);\n...\naccess the resource;\n...\nR.release();\nwhere R is an instance of type ResourceAllocator.\nUnfortunately, the monitor concept cannot guarantee that the preceding\naccess sequence will be observed. In particular, the following problems can\noccur:\n• A process might access a resource without ﬁrst gaining access permission\nto the resource. 5.8\nMonitors\n231\nmonitor ResourceAllocator\n{\nboolean busy;\ncondition x;\nvoid acquire(int time) {\nif (busy)\nx.wait(time);\nbusy = true;\n}\nvoid release() {\nbusy = false;\nx.signal();\n}\ninitialization code() {\nbusy = false;\n}\n}\nFigure 5.19\nA monitor to allocate a single resource.\n• A process might never release a resource once it has been granted access\nto the resource.",
  "to the resource.\n• A process might attempt to release a resource that it never requested.\n• A process might request the same resource twice (without ﬁrst releasing\nthe resource).\nThe same difﬁculties are encountered with the use of semaphores, and\nthese difﬁculties are similar in nature to those that encouraged us to develop\nthe monitor constructs in the ﬁrst place. Previously, we had to worry about\nthe correct use of semaphores. Now, we have to worry about the correct use of\nhigher-level programmer-deﬁned operations, with which the compiler can no\nlonger assist us.\nOne possible solution to the current problem is to include the resource-\naccess operations within the ResourceAllocator monitor. However, using\nthis solution will mean that scheduling is done according to the built-in",
  "monitor-scheduling algorithm rather than the one we have coded.\nTo ensure that the processes observe the appropriate sequences, we must\ninspect all the programs that make use of the ResourceAllocator monitor\nand its managed resource. We must check two conditions to establish the\ncorrectness of this system. First, user processes must always make their calls\non the monitor in a correct sequence. Second, we must be sure that an\nuncooperative process does not simply ignore the mutual-exclusion gateway\nprovided by the monitor and try to access the shared resource directly, without\nusing the access protocols. Only if these two conditions can be ensured can we\nguarantee that no time-dependent errors will occur and that the scheduling\nalgorithm will not be defeated. 232\nChapter 5",
  "algorithm will not be defeated. 232\nChapter 5\nProcess Synchronization\nJAVA MONITORS\nJava provides a monitor-like concurrency mechanism for thread synchro-\nnization. Every object in Java has associated with it a single lock. When a\nmethod is declared to be synchronized, calling the method requires owning\nthe lock for the object. We declare a synchronized method by placing the\nsynchronized keyword in the method deﬁnition. The following deﬁnes\nsafeMethod() as synchronized, for example:\npublic class SimpleClass\n{\n. . .\npublic synchronized void safeMethod() {\n. . .\n/* Implementation of safeMethod() */\n. . .\n}\n}\nNext, we create an object instance of SimpleClass, such as the following:\nSimpleClass sc = new SimpleClass();\nInvoking sc.safeMethod() method requires owning the lock on the object",
  "instance sc. If the lock is already owned by another thread, the thread calling\nthe synchronized methodblocks and is placed in the entry set for the object’s\nlock. The entry set represents the set of threads waiting for the lock to become\navailable. If the lock is available when a synchronized method is called,\nthe calling thread becomes the owner of the object’s lock and can enter the\nmethod. The lock is released when the thread exits the method. A thread from\nthe entry set is then selected as the new owner of the lock.\nJava also provides wait() and notify() methods, which are similar in\nfunction to the wait() and signal() statements for a monitor. The Java\nAPI provides support for semaphores, condition variables, and mutex locks",
  "(among other concurrency mechanisms) in the java.util.concurrent\npackage.\nAlthough this inspection may be possible for a small, static system, it is\nnot reasonable for a large system or a dynamic system. This access-control\nproblem can be solved only through the use of the additional mechanisms that\nare described in Chapter 14.\n5.9\nSynchronization Examples\nWe next describe the synchronization mechanisms provided by the Windows,\nLinux, and Solaris operating systems, as well as the Pthreads API. We have\nchosen these three operating systems because they provide good examples of\ndifferent approaches to synchronizing the kernel, and we have included the 5.9\nSynchronization Examples\n233\nPthreads API because it is widely used for thread creation and synchronization",
  "by developers on UNIX and Linux systems. As you will see in this section, the\nsynchronization methods available in these differing systems vary in subtle\nand signiﬁcant ways.\n5.9.1\nSynchronization in Windows\nThe Windows operating system is a multithreaded kernel that provides support\nfor real-time applications and multiple processors. When the Windows kernel\naccesses a global resource on a single-processor system, it temporarily masks\ninterrupts for all interrupt handlers that may also access the global resource.\nOn a multiprocessor system, Windows protects access to global resources\nusing spinlocks, although the kernel uses spinlocks only to protect short code\nsegments. Furthermore, for reasons of efﬁciency, the kernel ensures that a",
  "thread will never be preempted while holding a spinlock.\nFor thread synchronization outside the kernel, Windows provides dis-\npatcher objects. Using a dispatcher object, threads synchronize according to\nseveral different mechanisms, including mutex locks, semaphores, events, and\ntimers. The system protects shared data by requiring a thread to gain ownership\nof a mutex to access the data and to release ownership when it is ﬁnished.\nSemaphores behave as described in Section 5.6. Events are similar to condition\nvariables; that is, they may notify a waiting thread when a desired condition\noccurs. Finally, timers are used to notify one (or more than one) thread that a\nspeciﬁed amount of time has expired.\nDispatcher objects may be in either a signaled state or a nonsignaled state.",
  "An object in a signaled state is available, and a thread will not block when\nacquiring the object. An object in a nonsignaled state is not available, and a\nthread will block when attempting to acquire the object. We illustrate the state\ntransitions of a mutex lock dispatcher object in Figure 5.20.\nA relationship exists between the state of a dispatcher object and the state\nof a thread. When a thread blocks on a nonsignaled dispatcher object, its state\nchanges from ready to waiting, and the thread is placed in a waiting queue\nfor that object. When the state for the dispatcher object moves to signaled, the\nkernel checks whether any threads are waiting on the object. If so, the kernel\nmoves one thread—or possibly more—from the waiting state to the ready",
  "state, where they can resume executing. The number of threads the kernel\nselects from the waiting queue depends on the type of dispatcher object for\nwhich it is waiting. The kernel will select only one thread from the waiting\nqueue for a mutex, since a mutex object may be “owned” by only a single\nnonsignaled\nsignaled\nowner thread releases mutex lock\nthread acquires mutex lock\nFigure 5.20\nMutex dispatcher object. 234\nChapter 5\nProcess Synchronization\nthread. For an event object, the kernel will select all threads that are waiting\nfor the event.\nWe can use a mutex lock as an illustration of dispatcher objects and\nthread states. If a thread tries to acquire a mutex dispatcher object that is in a\nnonsignaled state, that thread will be suspended and placed in a waiting queue",
  "for the mutex object. When the mutex moves to the signaled state (because\nanother thread has released the lock on the mutex), the thread waiting at the\nfront of the queue will be moved from the waiting state to the ready state and\nwill acquire the mutex lock.\nA critical-section object is a user-mode mutex that can often be acquired\nand released without kernel intervention. On a multiprocessor system, a\ncritical-section object ﬁrst uses a spinlock while waiting for the other thread to\nrelease the object. If it spins too long, the acquiring thread will then allocate a\nkernel mutex and yield its CPU. Critical-section objects are particularly efﬁcient\nbecause the kernel mutex is allocated only when there is contention for the",
  "object. In practice, there is very little contention, so the savings are signiﬁcant.\nWe provide a programming project at the end of this chapter that uses\nmutex locks and semaphores in the Windows API.\n5.9.2\nSynchronization in Linux\nPrior to Version 2.6, Linux was a nonpreemptive kernel, meaning that a process\nrunning in kernel mode could not be preempted—even if a higher-priority\nprocess became available to run. Now, however, the Linux kernel is fully\npreemptive, so a task can be preempted when it is running in the kernel.\nLinux provides several different mechanisms for synchronization in the\nkernel. As most computer architectures provide instructions for atomic ver-\nsions of simple math operations, the simplest synchronization technique within",
  "the Linux kernel is an atomic integer, which is represented using the opaque\ndata type atomic t. As the name implies, all math operations using atomic\nintegers are performed without interruption. The following code illustrates\ndeclaring an atomic integer counter and then performing various atomic\noperations:\natomic t counter;\nint value;\natomic set(&counter,5); /* counter = 5 */\natomic add(10, &counter); /* counter = counter + 10 */\natomic sub(4, &counter); /* counter = counter - 4 */\natomic inc(&counter); /* counter = counter + 1 */\nvalue = atomic read(&counter); /* value = 12 */\nAtomic integers are particularly efﬁcient in situations where an integer variable\n—such as a counter—needs to be updated, since atomic operations do not",
  "require the overhead of locking mechanisms. However, their usage is limited\nto these sorts of scenarios. In situations where there are several variables\ncontributing to a possible race condition, more sophisticated locking tools\nmust be used.\nMutex locks are available in Linux for protecting critical sections within the\nkernel. Here, a task must invoke the mutex lock() function prior to entering 5.9\nSynchronization Examples\n235\na critical section and the mutex unlock() function after exiting the critical\nsection. If the mutex lock is unavailable, a task calling mutex lock() is put into\na sleep state and is awakened when the lock’s owner invokes mutex unlock().\nLinux also provides spinlocks and semaphores (as well as reader–writer",
  "versions of these two locks) for locking in the kernel. On SMP machines, the\nfundamental locking mechanism is a spinlock, and the kernel is designed\nso that the spinlock is held only for short durations. On single-processor\nmachines, such as embedded systems with only a single processing core,\nspinlocks are inappropriate for use and are replaced by enabling and disabling\nkernel preemption. That is, on single-processor systems, rather than holding a\nspinlock, the kernel disables kernel preemption; and rather than releasing the\nspinlock, it enables kernel preemption. This is summarized below:\nsingle processor\nmultiple processors\nAcquire spin lock.\nRelease spin lock.\nDisable kernel preemption.\nEnable kernel preemption.\nLinux uses an interesting approach to disable and enable kernel preemp-",
  "tion. It provides two simple system calls—preempt disable() and pre-\nempt enable()—for disabling and enabling kernel preemption. The kernel\nis not preemptible, however, if a task running in the kernel is holding a lock.\nTo enforce this rule, each task in the system has a thread-info structure\ncontaining a counter, preempt count, to indicate the number of locks being\nheld by the task. When a lock is acquired, preempt count is incremented. It\nis decremented when a lock is released. If the value of preempt count for\nthe task currently running in the kernel is greater than 0, it is not safe to\npreempt the kernel, as this task currently holds a lock. If the count is 0, the\nkernel can safely be interrupted (assuming there are no outstanding calls to\npreempt disable()).",
  "preempt disable()).\nSpinlocks—along with enabling and disabling kernel preemption—are\nused in the kernel only when a lock (or disabling kernel preemption) is held\nfor a short duration. When a lock must be held for a longer period, semaphores\nor mutex locks are appropriate for use.\n5.9.3\nSynchronization in Solaris\nTo control access to critical sections, Solaris provides adaptive mutex locks,\ncondition variables, semaphores, reader–writer locks, and turnstiles. Solaris\nimplements semaphores and condition variables essentially as they are pre-\nsented in Sections 5.6 and 5.7 In this section, we describe adaptive mutex locks,\nreader–writer locks, and turnstiles.\nAn adaptive mutex protects access to every critical data item. On a",
  "multiprocessor system, an adaptive mutex starts as a standard semaphore\nimplemented as a spinlock. If the data are locked and therefore already in use,\nthe adaptive mutex does one of two things. If the lock is held by a thread that\nis currently running on another CPU, the thread spins while waiting for the\nlock to become available, because the thread holding the lock is likely to ﬁnish\nsoon. If the thread holding the lock is not currently in run state, the thread 236\nChapter 5\nProcess Synchronization\nblocks, going to sleep until it is awakened by the release of the lock. It is put\nto sleep so that it will not spin while waiting, since the lock will not be freed\nvery soon. A lock held by a sleeping thread is likely to be in this category. On",
  "a single-processor system, the thread holding the lock is never running if the\nlock is being tested by another thread, because only one thread can run at a\ntime. Therefore, on this type of system, threads always sleep rather than spin\nif they encounter a lock.\nSolaris uses the adaptive-mutex method to protect only data that are\naccessed by short code segments. That is, a mutex is used if a lock will be\nheld for less than a few hundred instructions. If the code segment is longer\nthan that, the spin-waiting method is exceedingly inefﬁcient. For these longer\ncode segments, condition variables and semaphores are used. If the desired\nlock is already held, the thread issues a wait and sleeps. When a thread frees\nthe lock, it issues a signal to the next sleeping thread in the queue. The extra",
  "cost of putting a thread to sleep and waking it, and of the associated context\nswitches, is less than the cost of wasting several hundred instructions waiting\nin a spinlock.\nReader–writer locks are used to protect data that are accessed frequently\nbut are usually accessed in a read-only manner. In these circumstances,\nreader–writer locks are more efﬁcient than semaphores, because multiple\nthreads canread dataconcurrently, whereas semaphores always serialize access\nto the data. Reader–writer locks are relatively expensive to implement, so again\nthey are used only on long sections of code.\nSolaris uses turnstiles to order the list of threads waiting to acquire either\nan adaptive mutex or a reader–writer lock. A turnstile is a queue structure",
  "containing threads blocked on a lock. For example, if one thread currently\nowns the lock for a synchronized object, all other threads trying to acquire the\nlock will block and enter the turnstile for that lock. When the lock is released,\nthe kernel selects a thread from the turnstile as the next owner of the lock.\nEach synchronized object with at least one thread blocked on the object’s lock\nrequires a separate turnstile. However, rather than associating a turnstile with\neach synchronized object, Solaris gives each kernel thread its own turnstile.\nBecause a thread can be blocked only on one object at a time, this is more\nefﬁcient than having a turnstile for each object.\nThe turnstile for the ﬁrst thread to block on a synchronized object becomes",
  "the turnstile for the object itself. Threads subsequently blocking on the lock will\nbe added to this turnstile. When the initial thread ultimately releases the lock,\nit gains a new turnstile from a list of free turnstiles maintained by the kernel. To\nprevent a priority inversion, turnstiles are organized according to a priority-\ninheritance protocol. This means that if a lower-priority thread currently holds\na lock on which a higher-priority thread is blocked, the thread with the lower\npriority will temporarily inherit the priority of the higher-priority thread. Upon\nreleasing the lock, the thread will revert to its original priority.\nNote that the locking mechanisms used by the kernel are implemented\nfor user-level threads as well, so the same types of locks are available inside",
  "and outside the kernel. A crucial implementation difference is the priority-\ninheritance protocol. Kernel-locking routines adhere to the kernel priority-\ninheritance methods used by the scheduler, as described in Section 5.6.4.\nUser-level thread-locking mechanisms do not provide this functionality. 5.9\nSynchronization Examples\n237\nTo optimize Solaris performance, developers have reﬁned and ﬁne-tuned\nthe locking methods. Because locks are used frequently and typically are used\nfor crucial kernel functions, tuning their implementation and use can produce\ngreat performance gains.\n5.9.4\nPthreads Synchronization\nAlthough the locking mechanisms used in Solaris are available to user-level\nthreads as well as kernel threads, basically the synchronization methods",
  "discussed thus far pertain to synchronization within the kernel. In contrast,\nthe Pthreads API is available for programmers at the user level and is not part\nof any particular kernel. This API provides mutex locks, condition variables,\nand read–write locks for thread synchronization.\nMutex locks represent the fundamental synchronization technique used\nwith Pthreads. A mutex lock is used to protect critical sections of code—that\nis, a thread acquires the lock before entering a critical section and releases it\nupon exiting the critical section. Pthreads uses the pthread mutex t data type\nfor mutex locks. A mutex is created with the pthread mutex init() function.\nThe ﬁrst parameter is a pointer to the mutex. By passing NULL as a second",
  "parameter, we initialize the mutex to its default attributes. This is illustrated\nbelow:\n#include <pthread.h>\npthread mutex t mutex;\n/* create the mutex lock */\npthread mutex init(&mutex,NULL);\nThe mutex is acquired and released with the pthread mutex lock()\nand pthread mutex unlock() functions. If the mutex lock is unavailable\nwhen pthread mutex lock() is invoked, the calling thread is blocked until\nthe owner invokes pthread mutex unlock(). The following code illustrates\nprotecting a critical section with mutex locks:\n/* acquire the mutex lock */\npthread mutex lock(&mutex);\n/* critical section */\n/* release the mutex lock */\npthread mutex unlock(&mutex);\nAll mutex functions return a value of 0 with correct operation; if an error",
  "occurs, these functions return a nonzero error code. Condition variables and\nread–write locks behave similarly to the way they are described in Sections 5.8\nand 5.7.2, respectively.\nMany systems that implement Pthreads also provide semaphores, although\nsemaphores are not part of the Pthreads standard and instead belong to the\nPOSIX SEM extension. POSIX speciﬁes two types of semaphores—named and 238\nChapter 5\nProcess Synchronization\nunnamed. The fundamental distinction between the two is that a named\nsemaphore has an actual name in the ﬁle system and can be shared by\nmultiple unrelated processes. Unnamed semaphores can be used only by\nthreads belonging to the same process. In this section, we describe unnamed\nsemaphores.\nThe code below illustrates the sem init() function for creating and",
  "initializing an unnamed semaphore:\n#include <semaphore.h>\nsem t sem;\n/* Create the semaphore and initialize it to 1 */\nsem init(&sem, 0, 1);\nThe sem init() function is passed three parameters:\n1. A pointer to the semaphore\n2. A ﬂag indicating the level of sharing\n3. The semaphore’s initial value\nIn this example, by passing the ﬂag 0, we are indicating that this semaphore can\nbe shared only by threads belonging to the process that created the semaphore.\nA nonzero value would allow other processes to access the semaphore as well.\nIn addition, we initialize the semaphore to the value 1.\nIn Section 5.6, we described the classical wait() and signal() semaphore\noperations. Pthreads names these operations sem wait() and sem post(),",
  "respectively. The following code sample illustrates protecting a critical section\nusing the semaphore created above:\n/* acquire the semaphore */\nsem wait(&sem);\n/* critical section */\n/* release the semaphore */\nsem post(&sem);\nJust like mutex locks, all semaphore functions return 0 when successful, and\nnonzero when an error condition occurs.\nThere are other extensions to the Pthreads API — including spinlocks —\nbut it is important to note that not all extensions are considered portable from\none implementation to another. We provide several programming problems\nand projects at the end of this chapter that use Pthreads mutex locks and\ncondition variables as well as POSIX semaphores.\n5.10 Alternative Approaches\nWith the emergence of multicore systems has come increased pressure to",
  "develop multithreaded applications that take advantage of multiple processing 5.10\nAlternative Approaches\n239\ncores. However, multithreaded applications present an increased risk of race\nconditions and deadlocks. Traditionally, techniques such as mutex locks,\nsemaphores, and monitors have been used to address these issues, but as the\nnumber of processing cores increases, it becomes increasingly difﬁcult to design\nmultithreaded applications that are free from race conditions and deadlocks.\nIn this section, we explore various features provided in both program-\nming languages and hardware that support designing thread-safe concurrent\napplications.\n5.10.1\nTransactional Memory\nQuite often in computer science, ideas from one area of study can be used",
  "to solve problems in other areas. The concept of transactional memory\noriginated in database theory, for example, yet it provides a strategy for process\nsynchronization. A memory transaction is a sequence of memory read–write\noperations that are atomic. If all operations in a transaction are completed, the\nmemory transaction is committed. Otherwise, the operations must be aborted\nand rolled back. The beneﬁts of transactional memory can be obtained through\nfeatures added to a programming language.\nConsider an example. Suppose we have a function update() that modiﬁes\nshared data. Traditionally, this function would be written using mutex locks\n(or semaphores) such as the following:\nvoid update ()\n{\nacquire();\n/* modify shared data */\nrelease();\n}",
  "acquire();\n/* modify shared data */\nrelease();\n}\nHowever, using synchronization mechanisms such as mutex locks and\nsemaphores involves many potential problems, including deadlock. Addition-\nally, as the number of threads increases, traditional locking scales less well,\nbecause the level of contention among threads for lock ownership becomes\nvery high.\nAs an alternative to traditional locking methods, new features that take\nadvantage of transactional memory can be added to a programming language.\nIn our example, suppose we add the construct atomic{S}, which ensures\nthat the operations in S execute as a transaction. This allows us to rewrite the\nupdate() function as follows:\nvoid update ()\n{\natomic {\n/* modify shared data */\n}\n}",
  "{\natomic {\n/* modify shared data */\n}\n}\nThe advantage of using such a mechanism rather than locks is that\nthe transactional memory system—not the developer—is responsible for 240\nChapter 5\nProcess Synchronization\nguaranteeing atomicity. Additionally, because no locks are involved, deadlock\nis not possible. Furthermore, a transactional memory system can identify which\nstatements in atomic blocks can be executed concurrently, such as concurrent\nread access to a shared variable. It is, of course, possible for a programmer\nto identify these situations and use reader–writer locks, but the task becomes\nincreasingly difﬁcult as the number of threads within an application grows.\nTransactional memory can be implemented in either software or hardware.",
  "Software transactional memory (STM), as the name suggests, implements\ntransactional memory exclusively in software—no special hardware is needed.\nSTM works by inserting instrumentation code inside transaction blocks. The\ncode is inserted by a compiler and manages each transaction by examining\nwhere statements may run concurrently and where speciﬁc low-level locking\nis required. Hardware transactional memory (HTM) uses hardware cache\nhierarchies and cache coherency protocols to manage and resolve conﬂicts\ninvolving shared data residing in separate processors’ caches. HTM requires no\nspecial code instrumentation and thus has less overhead than STM. However,\nHTM does require that existing cache hierarchies and cache coherency protocols\nbe modiﬁed to support transactional memory.",
  "be modiﬁed to support transactional memory.\nTransactional memory has existed for several years without widespread\nimplementation. However, the growth of multicore systems and the associ-\nated emphasis on concurrent and parallel programming have prompted a\nsigniﬁcant amount of research in this area on the part of both academics and\ncommercial software and hardware vendors.\n5.10.2\nOpenMP\nIn Section 4.5.2, we provided an overview of OpenMP and its support of parallel\nprogramming in a shared-memory environment. Recall that OpenMP includes\na set of compiler directives and an API. Any code following the compiler\ndirective #pragma omp parallel is identiﬁed as a parallel region and is\nperformed by a number of threads equal to the number of processing cores",
  "in the system. The advantage of OpenMP (and similar tools) is that thread\ncreation and management are handled by the OpenMP library and are not the\nresponsibility of application developers.\nAlong with its #pragma omp parallel compiler directive, OpenMP pro-\nvides the compiler directive #pragma omp critical, which speciﬁes the code\nregion following the directive as a critical section in which only one thread may\nbe active at a time. In this way, OpenMP provides support for ensuring that\nthreads do not generate race conditions.\nAs an example of the use of the critical-section compiler directive, ﬁrst\nassume that the shared variable counter can be modiﬁed in the update()\nfunction as follows:\nvoid update(int value)\n{\ncounter += value;\n}",
  "void update(int value)\n{\ncounter += value;\n}\nIf the update() function can be part of—or invoked from—a parallel region,\na race condition is possible on the variable counter. 5.10\nAlternative Approaches\n241\nThe critical-section compiler directive can be used to remedy this race\ncondition and is coded as follows:\nvoid update(int value)\n{\n#pragma omp critical\n{\ncounter += value;\n}\n}\nThe critical-section compiler directive behaves much like a binary semaphore\nor mutex lock, ensuring that only one thread at a time is active in the critical\nsection. If a thread attempts to enter a critical section when another thread is\ncurrently active in that section (that is, owns the section), the calling thread is\nblocked until the owner thread exits. If multiple critical sections must be used,",
  "each critical section can be assigned a separate name, and a rule can specify\nthat no more than one thread may be active in a critical section of the same\nname simultaneously.\nAn advantage of using the critical-section compiler directive in OpenMP\nis that it is generally considered easier to use than standard mutex locks.\nHowever, a disadvantage is that application developers must still identify\npossible race conditions and adequately protect shared data using the compiler\ndirective. Additionally, because the critical-section compiler directive behaves\nmuch like a mutex lock, deadlock is still possible when two or more critical\nsections are identiﬁed.\n5.10.3\nFunctional Programming Languages\nMost well-known programming languages—such as C, C++, Java, and C#—",
  "are known as imperative (or procedural) languages. Imperative languages are\nused for implementing algorithms that are state-based. In these languages, the\nﬂow of the algorithm is crucial to its correct operation, and state is represented\nwith variables and other data structures. Of course, program state is mutable,\nas variables may be assigned different values over time.\nWith the current emphasis on concurrent and parallel programming for\nmulticore systems, there has been greater focus on functional programming\nlanguages, which follow a programming paradigm much different from\nthat offered by imperative languages. The fundamental difference between\nimperative and functional languages is that functional languages do not",
  "maintain state. That is, once a variable has been deﬁned and assigned a value, its\nvalue is immutable—it cannot change. Because functional languages disallow\nmutable state, they need not be concerned with issues such as race conditions\nand deadlocks. Essentially, most of the problems addressed in this chapter are\nnonexistent in functional languages.\nSeveral functional languages are presently in use, and we brieﬂy mention\ntwo of them here: Erlang and Scala. The Erlang language has gained signiﬁcant\nattention because of its support for concurrency and the ease with which it\ncan be used to develop applications that run on parallel systems. Scala is a\nfunctional language that is also object-oriented. In fact, much of the syntax of",
  "Scala is similar to the popular object-oriented languages Java and C#. Readers 242\nChapter 5\nProcess Synchronization\ninterested in Erlang and Scala, and in further details about functional languages\nin general, are encouraged to consult the bibliography at the end of this chapter\nfor additional references.\n5.11\nSummary\nGiven a collection of cooperating sequential processes that share data, mutual\nexclusion must be provided to ensure that a critical section of code is used by\nonly one process or thread at a time. Typically, computer hardware provides\nseveral operations that ensure mutual exclusion. However, such hardware-\nbased solutions are too complicated for most developers to use. Mutex locks\nand semaphores overcome this obstacle. Both tools can be used to solve various",
  "synchronization problems and can be implemented efﬁciently, especially if\nhardware support for atomic operations is available.\nVarious synchronization problems (such as the bounded-buffer problem,\nthe readers–writers problem, and the dining-philosophers problem) are impor-\ntant mainly because they are examples of a large class of concurrency-control\nproblems. These problems are used to test nearly every newly proposed\nsynchronization scheme.\nThe operating system must provide the means to guard against timing\nerrors, and several language constructs have been proposed to deal with\nthese problems. Monitors provide a synchronization mechanism for sharing\nabstract data types. A condition variable provides a method by which a monitor",
  "function can block its execution until it is signaled to continue.\nOperating systems also provide support for synchronization. For example,\nWindows, Linux, and Solaris provide mechanisms such as semaphores, mutex\nlocks, spinlocks, and condition variables to control access to shared data. The\nPthreads API provides support for mutex locks and semaphores, as well as\ncondition variables.\nSeveral alternative approaches focus on synchronization for multicore\nsystems. One approach uses transactional memory, which may address syn-\nchronization issues using either software or hardware techniques. Another\napproach uses the compiler extensions offered by OpenMP. Finally, func-\ntional programming languages address synchronization issues by disallowing\nmutability.\nPractice Exercises\n5.1",
  "mutability.\nPractice Exercises\n5.1\nIn Section 5.4, we mentioned that disabling interrupts frequently can\naffect the system’s clock. Explain why this can occur and how such\neffects can be minimized.\n5.2\nExplain why Windows, Linux, and Solaris implement multiple locking\nmechanisms. Describe the circumstances under which they use spin-\nlocks, mutex locks, semaphores, adaptive mutex locks, and condition\nvariables. In each case, explain why the mechanism is needed. Exercises\n243\n5.3\nWhat is the meaning of the term busy waiting? What other kinds of\nwaiting are there in an operating system? Can busy waiting be avoided\naltogether? Explain your answer.\n5.4\nExplain why spinlocks are not appropriate for single-processor systems\nyet are often used in multiprocessor systems.\n5.5",
  "yet are often used in multiprocessor systems.\n5.5\nShow that, if the wait() and signal() semaphore operations are not\nexecuted atomically, then mutual exclusion may be violated.\n5.6\nIllustrate how a binary semaphore can be used to implement mutual\nexclusion among n processes.\nExercises\n5.7\nRace conditions are possible in many computer systems. Consider a\nbanking system that maintains an account balance with two functions:\ndeposit(amount) and withdraw(amount). These two functions are\npassed the amount that is to be deposited or withdrawn from the bank\naccount balance. Assume that a husband and wife share a bank account.\nConcurrently, the husband calls the withdraw() function and the wife\ncalls deposit(). Describe how a race condition is possible and what",
  "might be done to prevent the race condition from occurring.\n5.8\nThe ﬁrst known correct software solution to the critical-section problem\nfor two processes was developed by Dekker. The two processes, P0 and\nP1, share the following variables:\nboolean flag[2]; /* initially false */\nint turn;\nThe structure of process Pi (i == 0 or 1) is shown in Figure 5.21. The\nother process is Pj (j == 1 or 0). Prove that the algorithm satisﬁes all\nthree requirements for the critical-section problem.\n5.9\nThe ﬁrst known correct software solution to the critical-section problem\nfor n processes with a lower bound on waiting of n −1 turns was\npresented by Eisenberg and McGuire. The processes share the following\nvariables:\nenum pstate {idle, want in, in cs};\npstate flag[n];\nint turn;",
  "pstate flag[n];\nint turn;\nAll the elements of flag are initially idle. The initial value of turn is\nimmaterial (between 0 and n-1). The structure of process Pi is shown in\nFigure 5.22. Prove that the algorithm satisﬁes all three requirements for\nthe critical-section problem.\n5.10\nExplain why implementing synchronization primitives by disabling\ninterrupts is not appropriate in a single-processor system if the syn-\nchronization primitives are to be used in user-level programs. 244\nChapter 5\nProcess Synchronization\ndo {\nflag[i] = true;\nwhile (flag[j]) {\nif (turn == j) {\nflag[i] = false;\nwhile (turn == j)\n; /* do nothing */\nflag[i] = true;\n}\n}\n/* critical section */\nturn = j;\nflag[i] = false;\n/* remainder section */\n} while (true);\nFigure 5.21",
  "} while (true);\nFigure 5.21\nThe structure of process Pi in Dekker’s algorithm.\n5.11\nExplain why interrupts are not appropriate for implementing synchro-\nnization primitives in multiprocessor systems.\n5.12\nThe Linux kernel has a policy that a process cannot hold a spinlock while\nattempting to acquire a semaphore. Explain why this policy is in place.\n5.13\nDescribe two kernel data structures in which race conditions are possible.\nBe sure to include a description of how a race condition can occur.\n5.14\nDescribe how the compare and swap() instruction can be used to pro-\nvide mutual exclusion that satisﬁes the bounded-waiting requirement.\n5.15\nConsider how to implement a mutex lock using an atomic hardware\ninstruction. Assume that the following structure deﬁning the mutex\nlock is available:",
  "lock is available:\ntypedef struct {\nint available;\n} lock;\n(available == 0) indicates that the lock is available, and a value of 1\nindicates that the lock is unavailable. Using this struct, illustrate how\nthe following functions can be implemented using the test and set()\nand compare and swap() instructions:\n• void acquire(lock *mutex)\n• void release(lock *mutex)\nBe sure to include any initialization that may be necessary. Exercises\n245\ndo {\nwhile (true) {\nflag[i] = want in;\nj = turn;\nwhile (j != i) {\nif (flag[j] != idle) {\nj = turn;\nelse\nj = (j + 1) % n;\n}\nflag[i] = in cs;\nj = 0;\nwhile ( (j < n) && (j == i || flag[j] != in cs))\nj++;\nif ( (j >= n) && (turn == i || flag[turn] == idle))\nbreak;\n}\n/* critical section */\nj = (turn + 1) % n;\nwhile (flag[j] == idle)\nj = (j + 1) % n;\nturn = j;",
  "j = (j + 1) % n;\nturn = j;\nflag[i] = idle;\n/* remainder section */\n} while (true);\nFigure 5.22\nThe structure of process Pi in Eisenberg and McGuire’s algorithm.\n5.16\nThe implementation of mutex locks provided in Section 5.5 suffers from\nbusy waiting. Describe what changes would be necessary so that a\nprocess waiting to acquire a mutex lock would be blocked and placed\ninto a waiting queue until the lock became available.\n5.17\nAssume that a system has multiple processing cores. For each of the\nfollowing scenarios, describe which is a better locking mechanism—a\nspinlock or a mutex lock where waiting processes sleep while waiting\nfor the lock to become available:\n• The lock is to be held for a short duration.\n• The lock is to be held for a long duration.",
  "• The lock is to be held for a long duration.\n• A thread may be put to sleep while holding the lock. 246\nChapter 5\nProcess Synchronization\n#define MAX PROCESSES 255\nint number of processes = 0;\n/* the implementation of fork() calls this function */\nint allocate process() {\nint new pid;\nif (number of processes == MAX PROCESSES)\nreturn -1;\nelse {\n/* allocate necessary process resources */\n++number of processes;\nreturn new pid;\n}\n}\n/* the implementation of exit() calls this function */\nvoid release process() {\n/* release process resources */\n--number of processes;\n}\nFigure 5.23\nAllocating and releasing processes.\n5.18\nAssume that a context switch takes T time. Suggest an upper bound\n(in terms of T) for holding a spinlock. If the spinlock is held for any",
  "longer, a mutex lock (where waiting threads are put to sleep) is a better\nalternative.\n5.19\nA multithreaded web server wishes to keep track of the number\nof requests it services (known as hits). Consider the two following\nstrategies to prevent a race condition on the variable hits. The ﬁrst\nstrategy is to use a basic mutex lock when updating hits:\nint hits;\nmutex lock hit lock;\nhit lock.acquire();\nhits++;\nhit lock.release();\nA second strategy is to use an atomic integer:\natomic t hits;\natomic inc(&hits);\nExplain which of these two strategies is more efﬁcient.\n5.20\nConsider the code example for allocating and releasing processes shown\nin Figure 5.23. Exercises\n247\na.\nIdentify the race condition(s).\nb.\nAssume you have a mutex lock named mutex with the operations",
  "acquire() and release(). Indicate where the locking needs to\nbe placed to prevent the race condition(s).\nc.\nCould we replace the integer variable\nint number of processes = 0\nwith the atomic integer\natomic t number of processes = 0\nto prevent the race condition(s)?\n5.21\nServers can be designed to limit the number of open connections. For\nexample, a server may wish to have only N socket connections at any\npoint in time. As soon as N connections are made, the server will\nnot accept another incoming connection until an existing connection\nis released. Explain how semaphores can be used by a server to limit the\nnumber of concurrent connections.\n5.22\nWindows Vista provides a lightweight synchronization tool called slim\nreader–writer locks. Whereas most implementations of reader–writer",
  "locks favor either readers or writers, or perhaps order waiting threads\nusing a FIFO policy, slim reader–writer locks favor neither readers nor\nwriters, nor are waiting threads ordered in a FIFO queue. Explain the\nbeneﬁts of providing such a synchronization tool.\n5.23\nShow how to implement the wait() and signal() semaphore oper-\nations in multiprocessor environments using the test and set()\ninstruction. The solution should exhibit minimal busy waiting.\n5.24\nExercise 4.26 requires the parent thread to wait for the child thread to\nﬁnish its execution before printing out the computed values. If we let the\nparent thread access the Fibonacci numbers as soon as they have been\ncomputed by the child thread—rather than waiting for the child thread",
  "to terminate—what changes would be necessary to the solution for this\nexercise? Implement your modiﬁed solution.\n5.25\nDemonstrate that monitors and semaphores are equivalent insofar\nas they can be used to implement solutions to the same types of\nsynchronization problems.\n5.26\nDesign an algorithm for a bounded-buffer monitor in which the buffers\n(portions) are embedded within the monitor itself.\n5.27\nThe strict mutual exclusion within a monitor makes the bounded-buffer\nmonitor of Exercise 5.26 mainly suitable for small portions.\na.\nExplain why this is true.\nb.\nDesign a new scheme that is suitable for larger portions.\n5.28\nDiscuss the tradeoff between fairness and throughput of operations\nin the readers–writers problem. Propose a method for solving the",
  "readers–writers problem without causing starvation. 248\nChapter 5\nProcess Synchronization\n5.29\nHow does the signal() operation associated with monitors differ from\nthe corresponding operation deﬁned for semaphores?\n5.30\nSuppose the signal() statement can appear only as the last statement\nin a monitor function. Suggest how the implementation described in\nSection 5.8 can be simpliﬁed in this situation.\n5.31\nConsider a system consisting of processes P1, P2, ..., Pn, each of which has\na unique priority number. Write a monitor that allocates three identical\nprinters to these processes, using the priority numbers for deciding the\norder of allocation.\n5.32\nA ﬁle is to be shared among different processes, each of which has\na unique number. The ﬁle can be accessed simultaneously by several",
  "processes, subject to the following constraint: the sum of all unique\nnumbers associated with all the processes currently accessing the ﬁle\nmust be less than n. Write a monitor to coordinate access to the ﬁle.\n5.33\nWhen a signal is performed on a condition inside a monitor, the signaling\nprocess can either continue its execution or transfer control to the process\nthat is signaled. How would the solution to the preceding exercise differ\nwith these two different ways in which signaling can be performed?\n5.34\nSuppose we replace the wait() and signal() operations of moni-\ntors with a single construct await(B), where B is a general Boolean\nexpression that causes the process executing it to wait until B becomes\ntrue.\na.\nWrite a monitor using this scheme to implement the readers–",
  "writers problem.\nb.\nExplain why, in general, this construct cannot be implemented\nefﬁciently.\nc.\nWhat restrictions need to be put on the await statement so that it\ncan be implemented efﬁciently? (Hint: Restrict the generality of B;\nsee [Kessels (1977)].)\n5.35\nDesign an algorithm for a monitor that implements an alarm clock that\nenables a calling program to delay itself for a speciﬁed number of time\nunits (ticks). You may assume the existence of a real hardware clock that\ninvokes a function tick() in your monitor at regular intervals.\nProgramming Problems\n5.36\nProgramming Exercise 3.20 required you to design a PID manager that\nallocated a unique process identiﬁer to each process. Exercise 4.20\nrequired you to modify your solution to Exercise 3.20 by writing a",
  "program that created a number of threads that requested and released\nprocess identiﬁers. Now modify your solution to Exercise 4.20 by\nensuring that the data structure used to represent the availability of\nprocess identiﬁers is safe from race conditions. Use Pthreads mutex\nlocks, described in Section 5.9.4. Programming Problems\n249\n5.37\nAssume that a ﬁnite number of resources of a single resource type must\nbe managed. Processes may ask for a number of these resources and will\nreturn them once ﬁnished. As an example, many commercial software\npackages provide a given number of licenses, indicating the number of\napplications that may run concurrently. When the application is started,\nthe license count is decremented. When the application is terminated, the",
  "license count is incremented. If all licenses are in use, requests to start\nthe application are denied. Such requests will only be granted when\nan existing license holder terminates the application and a license is\nreturned.\nThe following program segment is used to manage a ﬁnite number of\ninstances of an available resource. The maximum number of resources\nand the number of available resources are declared as follows:\n#define MAX RESOURCES 5\nint available resources = MAX RESOURCES;\nWhen a process wishes to obtain a number of resources, it invokes the\ndecrease count() function:\n/* decrease available resources by count resources */\n/* return 0 if sufficient resources available, */\n/* otherwise return -1 */\nint decrease count(int count) {\nif (available resources < count)\nreturn -1;\nelse {",
  "return -1;\nelse {\navailable resources -= count;\nreturn 0;\n}\n}\nWhen a process wants to return a number of resources, it calls the\nincrease count() function:\n/* increase available resources by count */\nint increase count(int count) {\navailable resources += count;\nreturn 0;\n}\nThe preceding program segment produces a race condition. Do the\nfollowing:\na.\nIdentify the data involved in the race condition.\nb.\nIdentify the location (or locations) in the code where the race\ncondition occurs. 250\nChapter 5\nProcess Synchronization\nc.\nUsing a semaphore or mutex lock, ﬁx the race condition. It is\npermissible to modify the decrease count() function so that the\ncalling process is blocked until sufﬁcient resources are available.\n5.38\nThe decrease count() function in the previous exercise currently",
  "returns 0 if sufﬁcient resources are available and −1 otherwise. This\nleads to awkward programming for a process that wishes to obtain a\nnumber of resources:\nwhile (decrease count(count) == -1)\n;\nRewrite the resource-manager code segment using a monitor and\ncondition variables so that the decrease count() function suspends\nthe process until sufﬁcient resources are available. This will allow a\nprocess to invoke decrease count() by simply calling\ndecrease count(count);\nThe process will return from this function call only when sufﬁcient\nresources are available.\n5.39\nExercise 4.22 asked you to design a multithreaded program that esti-\nmated ! using the Monte Carlo technique. In that exercise, you were\nasked to create a single thread that generated random points, storing",
  "the result in a global variable. Once that thread exited, the parent thread\nperformed the calcuation that estimated the value of !. Modify that\nprogram so that you create several threads, each of which generates\nrandom points and determines if the points fall within the circle. Each\nthread will have to update the global count of all points that fall within\nthe circle. Protect against race conditions on updates to the shared global\nvariable by using mutex locks.\n5.40\nExercise 4.23 asked you to design a program using OpenMP that\nestimated ! using the Monte Carlo technique. Examine your solution to\nthat program looking for any possible race conditions. If you identify a\nrace condition, protect against it using the strategy outlined in Section\n5.10.2.\n5.41",
  "5.10.2.\n5.41\nA barrier is a tool for synchronizing the activity of a number of threads.\nWhen a thread reaches a barrier point, it cannot proceed until all other\nthreads have reached this point as well. When the last thread reaches\nthe barrier point, all threads are released and can resume concurrent\nexecution.\nAssume that the barrier is initialized to N—the number of threads that\nmust wait at the barrier point:\ninit(N);\nEach thread then performs some work until it reaches the barrier point: Programming Projects\n251\n/* do some work for awhile */\nbarrier point();\n/* do some work for awhile */\nUsing synchronization tools described in this chapter, construct a barrier\nthat implements the following API:\n• int init(int n)—Initializes the barrier to the speciﬁed size.",
  "• int barrier point(void)—Identiﬁes\nthe\nbarrier\npoint.\nAll\nthreads are released from the barrier when the last thread reaches\nthis point.\nThe return value of each function is used to identify error conditions.\nEach function will return 0 under normal operation and will return\n−1 if an error occurs. A testing harness is provided in the source code\ndownload to test your implementation of the barrier.\nProgramming Projects\nProject 1—The Sleeping Teaching Assistant\nA university computer science department has a teaching assistant (TA) who\nhelps undergraduate students with their programming assignments during\nregular ofﬁce hours. The TA’s ofﬁce is rather small and has room for only one\ndesk with a chair and computer. There are three chairs in the hallway outside",
  "the ofﬁce where students can sit and wait if the TA is currently helping another\nstudent. When there are no students who need help during ofﬁce hours, the\nTA sits at the desk and takes a nap. If a student arrives during ofﬁce hours\nand ﬁnds the TA sleeping, the student must awaken the TA to ask for help. If a\nstudent arrives and ﬁnds the TA currently helping another student, the student\nsits on one of the chairs in the hallway and waits. If no chairs are available, the\nstudent will come back at a later time.\nUsing POSIX threads, mutex locks, and semaphores, implement a solution\nthat coordinates the activities of the TA and the students. Details for this\nassignment are provided below.\nThe Students and the TA\nUsing Pthreads (Section 4.4.1), begin by creating n students. Each will run as a",
  "separate thread. The TA will run as a separate thread as well. Student threads\nwill alternate between programming for a period of time and seeking help\nfrom the TA. If the TA is available, they will obtain help. Otherwise, they will\neither sit in a chair in the hallway or, if no chairs are available, will resume\nprogramming and will seek help at a later time. If a student arrives and notices\nthat the TA is sleeping, the student must notify the TA using a semaphore. When\nthe TA ﬁnishes helping a student, the TA must check to see if there are students\nwaiting for help in the hallway. If so, the TA must help each of these students\nin turn. If no students are present, the TA may return to napping. 252\nChapter 5\nProcess Synchronization",
  "Chapter 5\nProcess Synchronization\nPerhaps the best option for simulating students programming—as well as\nthe TA providing help to a student—is to have the appropriate threads sleep\nfor a random period of time.\nPOSIX Synchronization\nCoverage of POSIX mutex locks and semaphores is provided in Section 5.9.4.\nConsult that section for details.\nProject 2—The Dining Philosophers Problem\nIn Section 5.7.3, we provide an outline of a solution to the dining-philosophers\nproblem using monitors. This problem will require implementing a solution\nusing Pthreads mutex locks and condition variables.\nThe Philosophers\nBegin by creating ﬁve philosophers, each identiﬁed by a number 0 . . 4. Each\nphilosopher will run as a separate thread. Thread creation using Pthreads is",
  "covered in Section 4.4.1. Philosophers alternate between thinking and eating.\nTo simulate both activities, have the thread sleep for a random period between\none and three seconds. When a philosopher wishes to eat, she invokes the\nfunction\npickup forks(int philosopher number)\nwhere philosopher number identiﬁes the number of the philosopher wishing\nto eat. When a philosopher ﬁnishes eating, she invokes\nreturn forks(int philosopher number)\nPthreads Condition Variables\nCondition variables in Pthreads behave similarly to those described in Section\n5.8. However, in that section, condition variables are used within the context\nof a monitor, which provides a locking mechanism to ensure data integrity.\nSince Pthreads is typically used in C programs—and since C does not have",
  "a monitor— we accomplish locking by associating a condition variable with\na mutex lock. Pthreads mutex locks are covered in Section 5.9.4. We cover\nPthreads condition variables here.\nCondition variables in Pthreads use the pthread cond t data type and\nare initialized using the pthread cond init() function. The following code\ncreates and initializes a condition variable as well as its associated mutex lock:\npthread mutex t mutex;\npthread cond t cond var;\npthread mutex init(&mutex,NULL);\npthread cond init(&cond var,NULL); Programming Projects\n253\nThe pthread cond wait() function is used for waiting on a condition\nvariable. The following code illustrates how a thread can wait for the condition\na == b to become true using a Pthread condition variable:\npthread mutex lock(&mutex);",
  "pthread mutex lock(&mutex);\nwhile (a != b)\npthread cond wait(&mutex, &cond var);\npthread mutex unlock(&mutex);\nThe mutex lock associated with the condition variable must be locked\nbefore the pthread cond wait() function is called, since it is used to protect\nthe data in the conditional clause from a possible race condition. Once this\nlock is acquired, the thread can check the condition. If the condition is not true,\nthe thread then invokes pthread cond wait(), passing the mutex lock and\nthe condition variable as parameters. Calling pthread cond wait() releases\nthe mutex lock, thereby allowing another thread to access the shared data and\npossibly update its value so that the condition clause evaluates to true. (To",
  "protect against program errors, it is important to place the conditional clause\nwithin a loop so that the condition is rechecked after being signaled.)\nA\nthread\nthat\nmodiﬁes\nthe\nshared\ndata\ncan\ninvoke\nthe\npthread cond signal() function, thereby signaling one thread waiting\non the condition variable. This is illustrated below:\npthread mutex lock(&mutex);\na = b;\npthread cond signal(&cond var);\npthread mutex unlock(&mutex);\nIt is important to note that the call to pthread cond signal() does not\nrelease the mutex lock. It is the subsequent call to pthread mutex unlock()\nthat releases the mutex. Once the mutex lock is released, the signaled thread\nbecomes the owner of the mutex lock and returns control from the call to\npthread cond wait().\nProject 3—Producer–Consumer Problem",
  "Project 3—Producer–Consumer Problem\nIn Section 5.7.1, we presented a semaphore-based solution to the producer–\nconsumer problem using a bounded buffer. In this project, you will design a\nprogramming solution to the bounded-buffer problem using the producer and\nconsumer processes shown in Figures 5.9 and 5.10. The solution presented in\nSection 5.7.1 uses three semaphores: empty and full, which count the number\nof empty and full slots in the buffer, and mutex, which is a binary (or mutual-\nexclusion) semaphore that protects the actual insertion or removal of items\nin the buffer. For this project, you will use standard counting semaphores for\nempty and full and a mutex lock, rather than a binary semaphore, to represent\nmutex. The producer and consumer—running as separate threads—will move",
  "items to and from a buffer that is synchronized with the empty, full, and mutex\nstructures. You can solve this problem using either Pthreads or the Windows\nAPI. 254\nChapter 5\nProcess Synchronization\n#include \"buffer.h\"\n/* the buffer */\nbuffer item buffer[BUFFER SIZE];\nint insert item(buffer item item) {\n/* insert item into buffer\nreturn 0 if successful, otherwise\nreturn -1 indicating an error condition */\n}\nint remove item(buffer item *item) {\n/* remove an object from buffer\nplacing it in item\nreturn 0 if successful, otherwise\nreturn -1 indicating an error condition */\n}\nFigure 5.24\nOutline of buffer operations.\nThe Buffer\nInternally, the buffer will consist of a ﬁxed-size array of type buffer item\n(which will be deﬁned using a typedef). The array of buffer item objects",
  "will be manipulated as a circular queue. The deﬁnition of buffer item, along\nwith the size of the buffer, can be stored in a header ﬁle such as the following:\n/* buffer.h */\ntypedef int buffer item;\n#define BUFFER SIZE 5\nThe buffer will be manipulated with two functions, insert item() and\nremove item(), which are called by the producer and consumer threads,\nrespectively. A skeleton outlining these functions appears in Figure 5.24.\nThe insert item() and remove item() functions will synchronize the\nproducer and consumer using the algorithms outlined in Figures 5.9 and\n5.10. The buffer will also require an initialization function that initializes the\nmutual-exclusion object mutex along with the empty and full semaphores.\nThe main() function will initialize the buffer and create the separate",
  "producer and consumer threads. Once it has created the producer and\nconsumer threads, the main() function will sleep for a period of time and,\nupon awakening, will terminate the application. The main() function will be\npassed three parameters on the command line:\n1. How long to sleep before terminating\n2. The number of producer threads\n3. The number of consumer threads Programming Projects\n255\n#include \"buffer.h\"\nint main(int argc, char *argv[]) {\n/* 1. Get command line arguments argv[1],argv[2],argv[3] */\n/* 2. Initialize buffer */\n/* 3. Create producer thread(s) */\n/* 4. Create consumer thread(s) */\n/* 5. Sleep */\n/* 6. Exit */\n}\nFigure 5.25\nOutline of skeleton program.\nA skeleton for this function appears in Figure 5.25.\nThe Producer and Consumer Threads",
  "The Producer and Consumer Threads\nThe producer thread will alternate between sleeping for a random period of\ntime and inserting a random integer into the buffer. Random numbers will\nbe produced using the rand() function, which produces random integers\nbetween 0 and RAND MAX. The consumer will also sleep for a random period\nof time and, upon awakening, will attempt to remove an item from the buffer.\nAn outline of the producer and consumer threads appears in Figure 5.26.\nAs noted earlier, you can solve this problem using either Pthreads or the\nWindows API. In the following sections, we supply more information on each\nof these choices.\nPthreads Thread Creation and Synchronization\nCreating threads using the Pthreads API is discussed in Section 4.4.1. Coverage",
  "of mutex locks and semaphores using Pthreads is provided in Section 5.9.4.\nRefer to those sections for speciﬁc instructions on Pthreads thread creation and\nsynchronization.\nWindows\nSection 4.4.2 discusses thread creation using the Windows API. Refer to that\nsection for speciﬁc instructions on creating threads.\nWindows Mutex Locks\nMutex locks are a type of dispatcher object, as described in Section 5.9.1. The\nfollowing illustrates how to create a mutex lock using the CreateMutex()\nfunction:\n#include <windows.h>\nHANDLE Mutex;\nMutex = CreateMutex(NULL, FALSE, NULL); 256\nChapter 5\nProcess Synchronization\n#include <stdlib.h> /* required for rand() */\n#include \"buffer.h\"\nvoid *producer(void *param) {\nbuffer item item;\nwhile (true) {\n/* sleep for a random period of time */\nsleep(...);",
  "sleep(...);\n/* generate a random number */\nitem = rand();\nif (insert item(item))\nfprintf(\"report error condition\");\nelse\nprintf(\"producer produced %d\\n\",item);\n}\nvoid *consumer(void *param) {\nbuffer item item;\nwhile (true) {\n/* sleep for a random period of time */\nsleep(...);\nif (remove item(&item))\nfprintf(\"report error condition\");\nelse\nprintf(\"consumer consumed %d\\n\",item);\n}\nFigure 5.26\nAn outline of the producer and consumer threads.\nThe ﬁrst parameter refers to a security attribute for the mutex lock. By setting\nthis attribute to NULL, we disallow any children of the process creating this\nmutex lock to inherit the handle of the lock. The second parameter indicates\nwhether the creator of the mutex lock is the lock’s initial owner. Passing a value",
  "of FALSE indicates that the thread creating the mutex is not the initial owner.\n(We shall soon see how mutex locks are acquired.) The third parameter allows\nus to name the mutex. However, because we provide a value of NULL, we do\nnot name the mutex. If successful, CreateMutex() returns a HANDLE to the\nmutex lock; otherwise, it returns NULL.\nIn Section 5.9.1, we identiﬁed dispatcher objects as being either signaled or\nnonsignaled. A signaled dispatcher object (such as a mutex lock) is available\nfor ownership. Once it is acquired, it moves to the nonsignaled state. When it\nis released, it returns to signaled.\nMutex locks are acquired by invoking the WaitForSingleObject() func-\ntion. The function is passed the HANDLE to the lock along with a ﬂag indicating",
  "how long to wait. The following code demonstrates how the mutex lock created\nabove can be acquired:\nWaitForSingleObject(Mutex, INFINITE); Programming Projects\n257\nThe parameter value INFINITE indicates that we will wait an inﬁnite amount\nof time for the lock to become available. Other values could be used that would\nallow the calling thread to time out if the lock did not become available within\na speciﬁed time. If the lock is in a signaled state, WaitForSingleObject()\nreturns immediately, and the lock becomes nonsignaled. A lock is released\n(moves to the signaled state) by invoking ReleaseMutex()—for example, as\nfollows:\nReleaseMutex(Mutex);\nWindows Semaphores\nSemaphores in the Windows API are dispatcher objects and thus use the same",
  "signaling mechanism as mutex locks. Semaphores are created as follows:\n#include <windows.h>\nHANDLE Sem;\nSem = CreateSemaphore(NULL, 1, 5, NULL);\nThe ﬁrst and last parameters identify a security attribute and a name for the\nsemaphore, similar to what we described for mutex locks. The second and third\nparameters indicate the initial value and maximum value of the semaphore. In\nthis instance, the initial value of the semaphore is 1, and its maximum value\nis 5. If successful, CreateSemaphore() returns a HANDLE to the mutex lock;\notherwise, it returns NULL.\nSemaphores are acquired with the same WaitForSingleObject() func-\ntion as mutex locks. We acquire the semaphore Sem created in this example by\nusing the following statement:\nWaitForSingleObject(Semaphore, INFINITE);",
  "WaitForSingleObject(Semaphore, INFINITE);\nIf the value of the semaphore is > 0, the semaphore is in the signaled state\nand thus is acquired by the calling thread. Otherwise, the calling thread blocks\nindeﬁnitely—as we are specifying INFINITE—until the semaphore returns to\nthe signaled state.\nThe equivalent of the signal() operation for Windows semaphores is the\nReleaseSemaphore() function. This function is passed three parameters:\n1. The HANDLE of the semaphore\n2. How much to increase the value of the semaphore\n3. A pointer to the previous value of the semaphore\nWe can use the following statement to increase Sem by 1:\nReleaseSemaphore(Sem, 1, NULL);\nBoth ReleaseSemaphore() and ReleaseMutex() return a nonzero value if\nsuccessful and 0 otherwise. 258\nChapter 5\nProcess Synchronization",
  "Chapter 5\nProcess Synchronization\nBibliographical Notes\nThe mutual-exclusion problem was ﬁrst discussed in a classic paper by\n[Dijkstra (1965)]. Dekker’s algorithm (Exercise 5.8)—the ﬁrst correct software\nsolution to the two-process mutual-exclusion problem—was developed by the\nDutch mathematician T. Dekker. This algorithm also was discussed by [Dijkstra\n(1965)]. A simpler solution to the two-process mutual-exclusion problem has\nsince been presented by [Peterson (1981)] (Figure 5.2). The semaphore concept\nwas suggested by [Dijkstra (1965)].\nThe classic process-coordination problems that we have described are\nparadigms for a large class of concurrency-control problems. The bounded-\nbuffer problem and the dining-philosophers problem were suggested in",
  "[Dijkstra (1965)] and [Dijkstra (1971)]. The readers–writers problem was\nsuggested by [Courtois et al. (1971)].\nThe critical-region concept was suggested by [Hoare (1972)] and\nby\n[Brinch-Hansen\n(1972)].\nThe\nmonitor\nconcept\nwas\ndeveloped\nby\n[Brinch-Hansen (1973)]. [Hoare (1974)] gave a complete description of\nthe monitor.\nSome details of the locking mechanisms used in Solaris were presented\nin [Mauro and McDougall (2007)]. As noted earlier, the locking mechanisms\nused by the kernel are implemented for user-level threads as well, so the same\ntypes of locks are available inside and outside the kernel. Details of Windows\n2000 synchronization can be found in [Solomon and Russinovich (2000)]. [Love\n(2010)] describes synchronization in the Linux kernel.",
  "Information on Pthreads programming can be found in [Lewis and Berg\n(1998)] and [Butenhof (1997)]. [Hart (2005)] describes thread synchronization\nusing Windows. [Goetz et al. (2006)] present a detailed discussion of concur-\nrent programming in Java as well as the java.util.concurrent package.\n[Breshears (2009)] and [Pacheco (2011)] provide detailed coverage of synchro-\nnization issues in relation to parallel programming. [Lu et al. (2008)] provide a\nstudy of concurrency bugs in real-world applications.\n[Adl-Tabatabai et al. (2007)] discuss transactional memory. Details on using\nOpenMP can be found at http://openmp.org. Functional programming using\nErlang and Scala is covered in [Armstrong (2007)] and [Odersky et al. ()]\nrespectively.\nBibliography\n[Adl-Tabatabai et al. (2007)]",
  "Bibliography\n[Adl-Tabatabai et al. (2007)]\nA.-R. Adl-Tabatabai, C. Kozyrakis, and B. Saha,\n“Unlocking Concurrency”, Queue, Volume 4, Number 10 (2007), pages 24–33.\n[Armstrong (2007)]\nJ. Armstrong, Programming Erlang Software for a Concurrent\nWorld, The Pragmatic Bookshelf (2007).\n[Breshears (2009)]\nC. Breshears, The Art of Concurrency, O’Reilly & Associates\n(2009).\n[Brinch-Hansen (1972)]\nP. Brinch-Hansen, “Structured Multiprogramming”,\nCommunications of the ACM, Volume 15, Number 7 (1972), pages 574–578. Bibliography\n259\n[Brinch-Hansen (1973)]\nP. Brinch-Hansen, Operating System Principles, Prentice\nHall (1973).\n[Butenhof (1997)]\nD. Butenhof, Programming with POSIX Threads, Addison-\nWesley (1997).\n[Courtois et al. (1971)]\nP. J. Courtois, F. Heymans, and D. L. Parnas, “Concurrent",
  "Control with ‘Readers’ and ‘Writers’”, Communications of the ACM, Volume 14,\nNumber 10 (1971), pages 667–668.\n[Dijkstra (1965)]\nE. W. Dijkstra, “Cooperating Sequential Processes”, Technical\nreport, Technological University, Eindhoven, the Netherlands (1965).\n[Dijkstra (1971)]\nE. W. Dijkstra, “Hierarchical Ordering of Sequential Processes”,\nActa Informatica, Volume 1, Number 2 (1971), pages 115–138.\n[Goetz et al. (2006)]\nB. Goetz, T. Peirls, J. Bloch, J. Bowbeer, D. Holmes, and\nD. Lea, Java Concurrency in Practice, Addison-Wesley (2006).\n[Hart (2005)]\nJ. M. Hart, Windows System Programming, Third Edition, Addison-\nWesley (2005).\n[Hoare (1972)]\nC. A. R. Hoare, “Towards a Theory of Parallel Programming”, in\n[Hoare and Perrott 1972] (1972), pages 61–71.\n[Hoare (1974)]",
  "[Hoare (1974)]\nC. A. R. Hoare, “Monitors: An Operating System Structuring\nConcept”, Communications of the ACM, Volume 17, Number 10 (1974), pages\n549–557.\n[Kessels (1977)]\nJ. L. W. Kessels, “An Alternative to Event Queues for Synchro-\nnization in Monitors”, Communications of the ACM, Volume 20, Number 7 (1977),\npages 500–503.\n[Lewis and Berg (1998)]\nB. Lewis and D. Berg, Multithreaded Programming with\nPthreads, Sun Microsystems Press (1998).\n[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[Lu et al. (2008)]\nS. Lu, S. Park, E. Seo, and Y. Zhou, “Learning from mistakes: a\ncomprehensive study on real world concurrency bug characteristics”, SIGPLAN\nNotices, Volume 43, Number 3 (2008), pages 329–339.\n[Mauro and McDougall (2007)]",
  "[Mauro and McDougall (2007)]\nJ. Mauro and R. McDougall, Solaris Internals:\nCore Kernel Architecture, Prentice Hall (2007).\n[Odersky et al. ()]\nM. Odersky, V. Cremet, I. Dragos, G. Dubochet, B. Emir,\nS. Mcdirmid, S. Micheloud, N. Mihaylov, M. Schinz, E. Stenman, L. Spoon,\nand M. Zenger.\n[Pacheco (2011)]\nP. S. Pacheco, An Introduction to Parallel Programming, Morgan\nKaufmann (2011).\n[Peterson (1981)]\nG. L. Peterson, “Myths About the Mutual Exclusion Problem”,\nInformation Processing Letters, Volume 12, Number 3 (1981).\n[Solomon and Russinovich (2000)]\nD. A. Solomon and M. E. Russinovich, Inside\nMicrosoft Windows 2000, Third Edition, Microsoft Press (2000).  6\nC H A P T E R\nCPU\nScheduling\nCPU scheduling is the basis of multiprogrammed operating systems. By",
  "switching the CPU among processes, the operating system can make the\ncomputer more productive. In this chapter, we introduce basic CPU-scheduling\nconcepts and present several CPU-scheduling algorithms. We also consider the\nproblem of selecting an algorithm for a particular system.\nIn Chapter 4, we introduced threads to the process model. On operating\nsystems that support them, it is kernel-level threads—not processes—that\nare in fact being scheduled by the operating system. However, the terms\n\"process scheduling\" and \"thread scheduling\" are often used interchangeably.\nIn this chapter, we use process scheduling when discussing general scheduling\nconcepts and thread scheduling to refer to thread-speciﬁc ideas.\nCHAPTER OBJECTIVES",
  "CHAPTER OBJECTIVES\n• To introduce CPU scheduling, which is the basis for multiprogrammed\noperating systems.\n• To describe various CPU-scheduling algorithms.\n• To discuss evaluation criteria for selecting a CPU-scheduling algorithm for\na particular system.\n• To examine the scheduling algorithms of several operating systems.\n6.1\nBasic Concepts\nIn a single-processor system, only one process can run at a time. Others\nmust wait until the CPU is free and can be rescheduled. The objective of\nmultiprogramming is to have some process running at all times, to maximize\nCPU utilization. The idea is relatively simple. A process is executed until\nit must wait, typically for the completion of some I/O request. In a simple\ncomputer system, the CPU then just sits idle. All this waiting time is wasted;",
  "no useful work is accomplished. With multiprogramming, we try to use this\ntime productively. Several processes are kept in memory at one time. When\n261 262\nChapter 6\nCPU Scheduling\nCPU burst\nload store\nadd store\nread from file\nstore increment\nindex\nwrite to file\nload store\nadd store\nread from file\nwait for I/O\nwait for I/O\nwait for I/O\nI/O burst\nI/O burst\nI/O burst\nCPU burst\nCPU burst\n•\n•\n•\n•\n•\n•\nFigure 6.1\nAlternating sequence of CPU and I/O bursts.\none process has to wait, the operating system takes the CPU away from that\nprocess and gives the CPU to another process. This pattern continues. Every\ntime one process has to wait, another process can take over use of the CPU.\nScheduling of this kind is a fundamental operating-system function.",
  "Almost all computer resources are scheduled before use. The CPU is, of course,\none of the primary computer resources. Thus, its scheduling is central to\noperating-system design.\n6.1.1\nCPU–I/O Burst Cycle\nThe success of CPU scheduling depends on an observed property of processes:\nprocess execution consists of a cycle of CPU execution and I/O wait. Processes\nalternate between these two states. Process execution begins with a CPU burst.\nThat is followed by an I/O burst, which is followed by another CPU burst, then\nanother I/O burst, and so on. Eventually, the ﬁnal CPU burst ends with a system\nrequest to terminate execution (Figure 6.1).\nThe durations of CPU bursts have been measured extensively. Although\nthey vary greatly from process to process and from computer to computer,",
  "they tend to have a frequency curve similar to that shown in Figure 6.2. The\ncurve is generally characterized as exponential or hyperexponential, with a\nlarge number of short CPU bursts and a small number of long CPU bursts. 6.1\nBasic Concepts\n263\nfrequency\n160\n140\n120\n100\n80\n60\n40\n20\n0\n8\n16\n24\n32\n40\nburst duration (milliseconds)\nFigure 6.2\nHistogram of CPU-burst durations.\nAn I/O-bound program typically has many short CPU bursts. A CPU-bound\nprogram might have a few long CPU bursts. This distribution can be important\nin the selection of an appropriate CPU-scheduling algorithm.\n6.1.2\nCPU Scheduler\nWhenever the CPU becomes idle, the operating system must select one of the\nprocesses in the ready queue to be executed. The selection process is carried out",
  "by the short-term scheduler, or CPU scheduler. The scheduler selects a process\nfrom the processes in memory that are ready to execute and allocates the CPU\nto that process.\nNote that the ready queue is not necessarily a ﬁrst-in, ﬁrst-out (FIFO) queue.\nAs we shall see when we consider the various scheduling algorithms, a ready\nqueue can be implemented as a FIFO queue, a priority queue, a tree, or simply\nan unordered linked list. Conceptually, however, all the processes in the ready\nqueue are lined up waiting for a chance to run on the CPU. The records in the\nqueues are generally process control blocks (PCBs) of the processes.\n6.1.3\nPreemptive Scheduling\nCPU-scheduling decisions may take place under the following four circum-\nstances:",
  "stances:\n1. When a process switches from the running state to the waiting state (for\nexample, as the result of an I/O request or an invocation of wait() for\nthe termination of a child process) 264\nChapter 6\nCPU Scheduling\n2. When a process switches from the running state to the ready state (for\nexample, when an interrupt occurs)\n3. When a process switches from the waiting state to the ready state (for\nexample, at completion of I/O)\n4. When a process terminates\nFor situations 1 and 4, there is no choice in terms of scheduling. A new process\n(if one exists in the ready queue) must be selected for execution. There is a\nchoice, however, for situations 2 and 3.\nWhen scheduling takes place only under circumstances 1 and 4, we say",
  "that the scheduling scheme is nonpreemptive or cooperative. Otherwise,\nit is preemptive. Under nonpreemptive scheduling, once the CPU has been\nallocated to a process, the process keeps the CPU until it releases the CPU either\nby terminating or by switching to the waiting state. This scheduling method\nwas used by Microsoft Windows 3.x. Windows 95 introduced preemptive\nscheduling, and all subsequent versions of Windows operating systems have\nused preemptive scheduling. The Mac OS X operating system for the Macintosh\nalso uses preemptive scheduling; previous versions of the Macintosh operating\nsystem relied on cooperative scheduling. Cooperative scheduling is the only\nmethod that can be used on certain hardware platforms, because it does not",
  "require the special hardware (for example, a timer) needed for preemptive\nscheduling.\nUnfortunately, preemptive scheduling can result in race conditions when\ndata are shared among several processes. Consider the case of two processes\nthat share data. While one process is updating the data, it is preempted so that\nthe second process can run. The second process then tries to read the data,\nwhich are in an inconsistent state. This issue was explored in detail in Chapter\n5.\nPreemption also affects the design of the operating-system kernel. During\nthe processing of a system call, the kernel may be busy with an activity on behalf\nof a process. Such activities may involve changing important kernel data (for\ninstance, I/O queues). What happens if the process is preempted in the middle",
  "of these changes and the kernel (or the device driver) needs to read or modify\nthe same structure? Chaos ensues. Certain operating systems, including most\nversions of UNIX, deal with this problem by waiting either for a system call\nto complete or for an I/O block to take place before doing a context switch.\nThis scheme ensures that the kernel structure is simple, since the kernel will\nnot preempt a process while the kernel data structures are in an inconsistent\nstate. Unfortunately, this kernel-execution model is a poor one for supporting\nreal-time computing where tasks must complete execution within a given time\nframe. In Section 6.6, we explore scheduling demands of real-time systems.\nBecause interrupts can, by deﬁnition, occur at any time, and because",
  "they cannot always be ignored by the kernel, the sections of code affected\nby interrupts must be guarded from simultaneous use. The operating system\nneeds to accept interrupts at almost all times. Otherwise, input might be lost or\noutput overwritten. So that these sections of code are not accessed concurrently\nby several processes, they disable interrupts at entry and reenable interrupts\nat exit. It is important to note that sections of code that disable interrupts do\nnot occur very often and typically contain few instructions. 6.2\nScheduling Criteria\n265\n6.1.4\nDispatcher\nAnother component involved in the CPU-scheduling function is the dispatcher.\nThe dispatcher is the module that gives control of the CPUto the process selected",
  "by the short-term scheduler. This function involves the following:\n• Switching context\n• Switching to user mode\n• Jumping to the proper location in the user program to restart that program\nThe dispatcher should be as fast as possible, since it is invoked during every\nprocess switch. The time it takes for the dispatcher to stop one process and\nstart another running is known as the dispatch latency.\n6.2\nScheduling Criteria\nDifferent CPU-scheduling algorithms have different properties, and the choice\nof a particular algorithm may favor one class of processes over another. In\nchoosing which algorithm to use in a particular situation, we must consider\nthe properties of the various algorithms.\nMany criteria have been suggested for comparing CPU-scheduling algo-",
  "rithms. Which characteristics are used for comparison can make a substantial\ndifference in which algorithm is judged to be best. The criteria include the\nfollowing:\n• CPU utilization. We want to keep the CPU as busy as possible. Concep-\ntually, CPU utilization can range from 0 to 100 percent. In a real system, it\nshould range from 40 percent (for a lightly loaded system) to 90 percent\n(for a heavily loaded system).\n• Throughput. If the CPU is busy executing processes, then work is being\ndone. One measure of work is the number of processes that are completed\nper time unit, called throughput. For long processes, this rate may be one\nprocess per hour; for short transactions, it may be ten processes per second.\n• Turnaround time. From the point of view of a particular process, the",
  "important criterion is how long it takes to execute that process. The interval\nfrom the time of submission of a process to the time of completion is the\nturnaround time. Turnaround time is the sum of the periods spent waiting\nto get into memory, waiting in the ready queue, executing on the CPU, and\ndoing I/O.\n• Waiting time. The CPU-scheduling algorithm does not affect the amount\nof time during which a process executes or does I/O. It affects only the\namount of time that a process spends waiting in the ready queue. Waiting\ntime is the sum of the periods spent waiting in the ready queue.\n• Response time. In an interactive system, turnaround time may not be\nthe best criterion. Often, a process can produce some output fairly early",
  "and can continue computing new results while previous results are being 266\nChapter 6\nCPU Scheduling\noutput to the user. Thus, another measure is the time from the submission\nof a request until the ﬁrst response is produced. This measure, called\nresponse time, is the time it takes to start responding, not the time it takes\nto output the response. The turnaround time is generally limited by the\nspeed of the output device.\nIt is desirable to maximize CPU utilization and throughput and to minimize\nturnaround time, waiting time, and response time. In most cases, we optimize\nthe average measure. However, under some circumstances, we prefer to\noptimize the minimum or maximum values rather than the average. For\nexample, to guarantee that all users get good service, we may want to minimize",
  "the maximum response time.\nInvestigators have suggested that, for interactive systems (such as desktop\nsystems), it is more important to minimize the variance in the response time\nthan to minimize the average response time. A system with reasonable and\npredictable response time may be considered more desirable than a system\nthat is faster on the average but is highly variable. However, little work has\nbeen done on CPU-scheduling algorithms that minimize variance.\nAs we discuss various CPU-scheduling algorithms in the following section,\nwe illustrate their operation. An accurate illustration should involve many\nprocesses, each a sequence of several hundred CPU bursts and I/O bursts.\nFor simplicity, though, we consider only one CPU burst (in milliseconds) per",
  "process in our examples. Our measure of comparison is the average waiting\ntime. More elaborate evaluation mechanisms are discussed in Section 6.8.\n6.3\nScheduling Algorithms\nCPU schedulingdealswiththe problemofdecidingwhichofthe processesinthe\nreadyqueue istobe allocated the CPU. There are manydifferent CPU-scheduling\nalgorithms. In this section, we describe several of them.\n6.3.1\nFirst-Come, First-Served Scheduling\nBy far the simplest CPU-scheduling algorithm is the ﬁrst-come, ﬁrst-served\n(FCFS) scheduling algorithm. With this scheme, the process that requests the\nCPU ﬁrst is allocated the CPU ﬁrst. The implementation of the FCFS policy is\neasily managed with a FIFO queue. When a process enters the ready queue, its",
  "PCB is linked onto the tail of the queue. When the CPU is free, it is allocated to\nthe process at the head of the queue. The running process is then removed from\nthe queue. The code for FCFS scheduling is simple to write and understand.\nOn the negative side, the average waiting time under the FCFS policy is\noften quite long. Consider the following set of processes that arrive at time 0,\nwith the length of the CPU burst given in milliseconds:\nProcess\nBurst Time\nP1\n24\nP2\n3\nP3\n3 6.3\nScheduling Algorithms\n267\nIf the processes arrive in the order P1, P2, P3, and are served in FCFS order,\nwe get the result shown in the following Gantt chart, which is a bar chart that\nillustrates a particular schedule, including the start and ﬁnish times of each of\nthe participating processes:\nP1\nP2\nP3\n30\n27\n24",
  "the participating processes:\nP1\nP2\nP3\n30\n27\n24\n0\nThe waiting time is 0 milliseconds for process P1, 24 milliseconds for process\nP2, and 27 milliseconds for process P3. Thus, the average waiting time is (0\n+ 24 + 27)/3 = 17 milliseconds. If the processes arrive in the order P2, P3, P1,\nhowever, the results will be as shown in the following Gantt chart:\nP1\nP2\nP3\n30\n0\n3\n6\nThe average waiting time is now (6 + 0 + 3)/3 = 3 milliseconds. This reduction\nis substantial. Thus, the average waiting time under an FCFS policy is generally\nnot minimal and may vary substantially if the processes’ CPU burst times vary\ngreatly.\nIn addition, consider the performance of FCFS scheduling in a dynamic\nsituation. Assume we have one CPU-bound process and many I/O-bound",
  "processes. As the processes ﬂow around the system, the following scenario\nmay result. The CPU-bound process will get and hold the CPU. During this\ntime, all the other processes will ﬁnish their I/O and will move into the ready\nqueue, waiting for the CPU. While the processes wait in the ready queue, the\nI/O devices are idle. Eventually, the CPU-bound process ﬁnishes its CPU burst\nand moves to an I/O device. All the I/O-bound processes, which have short\nCPU bursts, execute quickly and move back to the I/O queues. At this point,\nthe CPU sits idle. The CPU-bound process will then move back to the ready\nqueue and be allocated the CPU. Again, all the I/O processes end up waiting in\nthe ready queue until the CPU-bound process is done. There is a convoy effect",
  "as all the other processes wait for the one big process to get off the CPU. This\neffect results in lower CPU and device utilization than might be possible if the\nshorter processes were allowed to go ﬁrst.\nNote also that the FCFS scheduling algorithm is nonpreemptive. Once the\nCPU has been allocated to a process, that process keeps the CPU until it releases\nthe CPU, either by terminating or by requesting I/O. The FCFS algorithm is thus\nparticularly troublesome for time-sharing systems, where it is important that\neach user get a share of the CPU at regular intervals. It would be disastrous to\nallow one process to keep the CPU for an extended period.\n6.3.2\nShortest-Job-First Scheduling\nA differentapproachto CPU schedulingisthe shortest-job-ﬁrst (SJF)scheduling",
  "algorithm. This algorithm associates with each process the length of the\nprocess’s next CPU burst. When the CPU is available, it is assigned to the 268\nChapter 6\nCPU Scheduling\nprocess that has the smallest next CPU burst. If the next CPU bursts of two\nprocesses are the same, FCFS scheduling is used to break the tie. Note that a\nmore appropriate term for this scheduling method would be the shortest-next-\nCPU-burst algorithm, because scheduling depends on the length of the next\nCPU burst of a process, rather than its total length. We use the term SJF because\nmost people and textbooks use this term to refer to this type of scheduling.\nAs an example of SJF scheduling, consider the following set of processes,\nwith the length of the CPU burst given in milliseconds:\nProcess\nBurst Time\nP1\n6\nP2\n8",
  "Process\nBurst Time\nP1\n6\nP2\n8\nP3\n7\nP4\n3\nUsing SJF scheduling, we would schedule these processes according to the\nfollowing Gantt chart:\nP3\nP2\nP4\nP1\n24\n16\n9\n0\n3\nThe waiting time is 3 milliseconds for process P1, 16 milliseconds for process\nP2, 9 milliseconds for process P3, and 0 milliseconds for process P4. Thus, the\naverage waiting time is (3 + 16 + 9 + 0)/4 = 7 milliseconds. By comparison, if\nwe were using the FCFS scheduling scheme, the average waiting time would\nbe 10.25 milliseconds.\nThe SJF scheduling algorithm is provably optimal, in that it gives the\nminimum average waiting time for a given set of processes. Moving a short\nprocess before a long one decreases the waiting time of the short process more\nthan it increases the waiting time of the long process. Consequently, the average",
  "waiting time decreases.\nThe real difﬁculty with the SJF algorithm is knowing the length of the next\nCPU request. For long-term (job) scheduling in a batch system, we can use\nthe process time limit that a user speciﬁes when he submits the job. In this\nsituation, users are motivated to estimate the process time limit accurately,\nsince a lower value may mean faster response but too low a value will cause\na time-limit-exceeded error and require resubmission. SJF scheduling is used\nfrequently in long-term scheduling.\nAlthough the SJF algorithm is optimal, it cannot be implemented at the\nlevel of short-term CPU scheduling. With short-term scheduling, there is no\nway to know the length of the next CPU burst. One approach to this problem",
  "is to try to approximate SJF scheduling. We may not know the length of the\nnext CPU burst, but we may be able to predict its value. We expect that the\nnext CPU burst will be similar in length to the previous ones. By computing\nan approximation of the length of the next CPU burst, we can pick the process\nwith the shortest predicted CPU burst.\nThe next CPU burst is generally predicted as an exponential average of\nthe measured lengths of previous CPU bursts. We can deﬁne the exponential 6.3\nScheduling Algorithms\n269\n6\n4\n6\n4\n13\n13\n13\n…\n8\n10\n6\n6\n5\n9\n11\n12\n…\nCPU burst (ti)\n\"guess\" (τi)\nti\nτi\n2\ntime\n4\n6\n8\n10\n12\nFigure 6.3\nPrediction of the length of the next CPU burst.\naverage with the following formula. Let tn be the length of the nth CPU burst,",
  "and let \"n+1 be our predicted value for the next CPU burst. Then, for #, 0 ≤# ≤\n1, deﬁne\n\"n+1 = # tn + (1 −#)\"n.\nThe value of tn contains our most recent information, while \"n stores the past\nhistory. The parameter # controls the relative weight of recent and past history\nin our prediction. If # = 0, then \"n+1 = \"n, and recent history has no effect (current\nconditions are assumed to be transient). If # = 1, then \"n+1 = tn, and only the most\nrecent CPU burst matters (history is assumed to be old and irrelevant). More\ncommonly, # = 1/2, so recent history and past history are equally weighted.\nThe initial \"0 can be deﬁned as a constant or as an overall system average.\nFigure 6.3 shows an exponential average with # = 1/2 and \"0 = 10.",
  "To understand the behavior of the exponential average, we can expand the\nformula for \"n+1 by substituting for \"n to ﬁnd\n\"n+1 = #tn + (1 −#)#tn−1 + · · · + (1 −#) j#tn−j + · · · + (1 −#)n+1\"0.\nTypically, # is less than 1. As a result, (1 −#) is also less than 1, and each\nsuccessive term has less weight than its predecessor.\nThe SJF algorithm can be either preemptive or nonpreemptive. The choice\narises when a new process arrives at the ready queue while a previous process is\nstill executing. The next CPU burst of the newly arrived process may be shorter\nthan what is left of the currently executing process. A preemptive SJF algorithm\nwill preempt the currently executing process, whereas a nonpreemptive SJF\nalgorithm will allow the currently running process to ﬁnish its CPU burst.",
  "Preemptive SJF scheduling is sometimes called shortest-remaining-time-ﬁrst\nscheduling. 270\nChapter 6\nCPU Scheduling\nAs an example, consider the following four processes, with the length of\nthe CPU burst given in milliseconds:\nProcess\nArrival Time\nBurst Time\nP1\n0\n8\nP2\n1\n4\nP3\n2\n9\nP4\n3\n5\nIf the processes arrive at the ready queue at the times shown and need the\nindicated burst times, then the resulting preemptive SJF schedule is as depicted\nin the following Gantt chart:\nP1\nP3\nP1\nP2\nP4\n26\n17\n10\n0\n1\n5\nProcess P1 is started at time 0, since it is the only process in the queue. Process\nP2 arrives at time 1. The remaining time for process P1 (7 milliseconds) is\nlarger than the time required by process P2 (4 milliseconds), so process P1 is",
  "preempted, and process P2 is scheduled. The average waiting time for this\nexample is [(10 −1) + (1 −1) + (17 −2) + (5 −3)]/4 = 26/4 = 6.5 milliseconds.\nNonpreemptive SJF scheduling would result in an average waiting time of 7.75\nmilliseconds.\n6.3.3\nPriority Scheduling\nThe SJF algorithm is a special case of the general priority-scheduling algorithm.\nA priority is associated with each process, and the CPUis allocated to the process\nwith the highest priority. Equal-priority processes are scheduled in FCFS order.\nAn SJF algorithm is simply a priority algorithm where the priority (p) is the\ninverse of the (predicted) next CPU burst. The larger the CPU burst, the lower\nthe priority, and vice versa.\nNote that we discuss scheduling in terms of high priority and low priority.",
  "Priorities are generally indicated by some ﬁxed range of numbers, such as 0\nto 7 or 0 to 4,095. However, there is no general agreement on whether 0 is the\nhighest or lowest priority. Some systems use low numbers to represent low\npriority; others use low numbers for high priority. This difference can lead to\nconfusion. In this text, we assume that low numbers represent high priority.\nAs an example, consider the following set of processes, assumed to have\narrived at time 0 in the order P1, P2, · · ·, P5, with the length of the CPU burst\ngiven in milliseconds:\nProcess\nBurst Time\nPriority\nP1\n10\n3\nP2\n1\n1\nP3\n2\n4\nP4\n1\n5\nP5\n5\n2 6.3\nScheduling Algorithms\n271\nUsing priority scheduling, we would schedule these processes according to the\nfollowing Gantt chart:\nP1\nP4\nP3\nP2\nP5\n19\n18\n16\n6\n0\n1",
  "P1\nP4\nP3\nP2\nP5\n19\n18\n16\n6\n0\n1\nThe average waiting time is 8.2 milliseconds.\nPriorities can be deﬁned either internally or externally. Internally deﬁned\npriorities use some measurable quantity or quantities to compute the priority\nof a process. For example, time limits, memory requirements, the number of\nopen ﬁles, and the ratio of average I/O burst to average CPU burst have been\nused in computing priorities. External priorities are set by criteria outside the\noperating system, such as the importance of the process, the type and amount\nof funds being paid for computer use, the department sponsoring the work,\nand other, often political, factors.\nPriority scheduling can be either preemptive or nonpreemptive. When a",
  "process arrives at the ready queue, its priority is compared with the priority\nof the currently running process. A preemptive priority scheduling algorithm\nwill preempt the CPU if the priority of the newly arrived process is higher\nthan the priority of the currently running process. A nonpreemptive priority\nscheduling algorithm will simply put the new process at the head of the ready\nqueue.\nA major problem with priority scheduling algorithms is indeﬁnite block-\ning, or starvation. A process that is ready to run but waiting for the CPU can\nbe considered blocked. A priority scheduling algorithm can leave some low-\npriority processes waiting indeﬁnitely. In a heavily loaded computer system, a\nsteady stream of higher-priority processes can prevent a low-priority process",
  "from ever getting the CPU. Generally, one of two things will happen. Either the\nprocess will eventually be run (at 2 A.M. Sunday, when the system is ﬁnally\nlightly loaded), or the computer system will eventually crash and lose all\nunﬁnished low-priority processes. (Rumor has it that when they shut down\nthe IBM 7094 at MIT in 1973, they found a low-priority process that had been\nsubmitted in 1967 and had not yet been run.)\nA solution to the problem of indeﬁnite blockage of low-priority processes is\naging. Aging involves gradually increasing the priority of processes that wait\nin the system for a long time. For example, if priorities range from 127 (low)\nto 0 (high), we could increase the priority of a waiting process by 1 every 15",
  "minutes. Eventually, even a process with an initial priority of 127 would have\nthe highest priority in the system and would be executed. In fact, it would take\nno more than 32 hours for a priority-127 process to age to a priority-0 process.\n6.3.4\nRound-Robin Scheduling\nThe round-robin (RR) scheduling algorithm is designed especially for time-\nsharing systems. It is similar to FCFS scheduling, but preemption is added to\nenable the system to switch between processes. A small unit of time, called a\ntime quantum or time slice, is deﬁned. A time quantum is generally from 10\nto 100 milliseconds in length. The ready queue is treated as a circular queue. 272\nChapter 6\nCPU Scheduling\nThe CPU scheduler goes around the ready queue, allocating the CPU to each",
  "process for a time interval of up to 1 time quantum.\nTo implement RR scheduling, we again treat the ready queue as a FIFO\nqueue of processes. New processes are added to the tail of the ready queue.\nThe CPU scheduler picks the ﬁrst process from the ready queue, sets a timer to\ninterrupt after 1 time quantum, and dispatches the process.\nOne of two things will then happen. The process may have a CPU burst of\nless than 1 time quantum. In this case, the process itself will release the CPU\nvoluntarily. The scheduler will then proceed to the next process in the ready\nqueue. If the CPU burst of the currently running process is longer than 1 time\nquantum, the timer will go off and will cause an interrupt to the operating\nsystem. A context switch will be executed, and the process will be put at the",
  "tail of the ready queue. The CPU scheduler will then select the next process in\nthe ready queue.\nThe average waiting time under the RR policy is often long. Consider the\nfollowing set of processes that arrive at time 0, with the length of the CPU burst\ngiven in milliseconds:\nProcess\nBurst Time\nP1\n24\nP2\n3\nP3\n3\nIf we use a time quantum of 4 milliseconds, then process P1 gets the ﬁrst 4\nmilliseconds. Since it requires another 20 milliseconds, it is preempted after\nthe ﬁrst time quantum, and the CPU is given to the next process in the queue,\nprocess P2. Process P2 does not need 4 milliseconds, so it quits before its time\nquantum expires. The CPU is then given to the next process, process P3. Once\neach process has received 1 time quantum, the CPU is returned to process P1",
  "for an additional time quantum. The resulting RR schedule is as follows:\nP1\nP1\nP1\nP1\nP1\nP1\nP2\n30\n18\n14\n26\n22\n10\n7\n0\n4\nP3\nLet’s calculate the average waiting time for this schedule. P1 waits for 6\nmilliseconds (10 - 4), P2 waits for 4 milliseconds, and P3 waits for 7 milliseconds.\nThus, the average waiting time is 17/3 = 5.66 milliseconds.\nIn the RR scheduling algorithm, no process is allocated the CPU for more\nthan 1 time quantum in a row (unless it is the only runnable process). If a\nprocess’s CPU burst exceeds 1 time quantum, that process is preempted and is\nput back in the ready queue. The RR scheduling algorithm is thus preemptive.\nIf there are n processes in the ready queue and the time quantum is q,\nthen each process gets 1/n of the CPU time in chunks of at most q time units.",
  "Each process must wait no longer than (n −1) × q time units until its\nnext time quantum. For example, with ﬁve processes and a time quantum of 20\nmilliseconds, each process will get up to 20 milliseconds every 100 milliseconds.\nThe performance ofthe RR algorithmdependsheavilyonthe size ofthe time\nquantum. At one extreme, if the time quantum is extremely large, the RR policy 6.3\nScheduling Algorithms\n273\nprocess time $ 10\nquantum\ncontext\nswitches\n12\n0\n6\n1\n1\n9\n0\n10\n0\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n6\nFigure 6.4\nHow a smaller time quantum increases context switches.\nis the same as the FCFS policy. In contrast, if the time quantum is extremely\nsmall (say, 1 millisecond), the RR approach can result in a large number of\ncontext switches. Assume, for example, that we have only one process of 10",
  "time units. If the quantum is 12 time units, the process ﬁnishes in less than 1\ntime quantum, with no overhead. If the quantum is 6 time units, however, the\nprocess requires 2 quanta, resulting in a context switch. If the time quantum is\n1 time unit, then nine context switches will occur, slowing the execution of the\nprocess accordingly (Figure 6.4).\nThus, we want the time quantum to be large with respect to the context-\nswitch time. If the context-switch time is approximately 10 percent of the\ntime quantum, then about 10 percent of the CPU time will be spent in context\nswitching. In practice, most modern systems have time quanta ranging from\n10 to 100 milliseconds. The time required for a context switch is typically less",
  "than 10 microseconds; thus, the context-switch time is a small fraction of the\ntime quantum.\nTurnaround time also depends on the size of the time quantum. As we\ncan see from Figure 6.5, the average turnaround time of a set of processes\ndoes not necessarily improve as the time-quantum size increases. In general,\nthe average turnaround time can be improved if most processes ﬁnish their\nnext CPU burst in a single time quantum. For example, given three processes\nof 10 time units each and a quantum of 1 time unit, the average turnaround\ntime is 29. If the time quantum is 10, however, the average turnaround time\ndrops to 20. If context-switch time is added in, the average turnaround time\nincreases even more for a smaller time quantum, since more context switches\nare required.",
  "are required.\nAlthough the time quantum should be large compared with the context-\nswitch time, it should not be too large. As we pointed out earlier, if the time\nquantum is too large, RR scheduling degenerates to an FCFS policy. A rule of\nthumb is that 80 percent of the CPU bursts should be shorter than the time\nquantum.\n6.3.5\nMultilevel Queue Scheduling\nAnother class of scheduling algorithms has been created for situations in\nwhich processes are easily classiﬁed into different groups. For example, a 274\nChapter 6\nCPU Scheduling\naverage turnaround time\n1\n12.5\n12.0\n11.5\n11.0\n10.5\n10.0\n9.5\n9.0\n2\n3\n4\ntime quantum\n5\n6\n7\nP1 \nP2 \nP3 \nP4\n6 \n3 \n1 \n7\nprocess\ntime\nFigure 6.5\nHow turnaround time varies with the time quantum.\ncommon division is made between foreground (interactive) processes and",
  "background (batch) processes. These two types of processes have different\nresponse-time requirements and so may have different scheduling needs. In\naddition, foreground processes may have priority (externally deﬁned) over\nbackground processes.\nA multilevel queue scheduling algorithm partitions the ready queue into\nseveral separate queues (Figure 6.6). The processes are permanently assigned to\none queue, generally based on some property of the process, such as memory\nsize, process priority, or process type. Each queue has its own scheduling\nalgorithm. For example, separate queues might be used for foreground and\nbackground processes. The foreground queue might be scheduled by an RR\nalgorithm, while the background queue is scheduled by an FCFS algorithm.",
  "In addition, there must be scheduling among the queues, which is com-\nmonly implemented as ﬁxed-priority preemptive scheduling. For example, the\nforeground queue may have absolute priority over the background queue.\nLet’s look at an example of a multilevel queue scheduling algorithm with\nﬁve queues, listed below in order of priority:\n1. System processes\n2. Interactive processes\n3. Interactive editing processes\n4. Batch processes\n5. Student processes 6.3\nScheduling Algorithms\n275\nsystem processes\nhighest priority\nlowest priority\ninteractive processes\ninteractive editing processes\nbatch processes\nstudent processes\nFigure 6.6\nMultilevel queue scheduling.\nEach queue has absolute priority over lower-priority queues. No process in the",
  "batch queue, for example, could run unless the queues for system processes,\ninteractive processes, and interactive editing processes were all empty. If an\ninteractive editing process entered the ready queue while a batch process was\nrunning, the batch process would be preempted.\nAnother possibility is to time-slice among the queues. Here, each queue gets\na certain portion of the CPU time, which it can then schedule among its various\nprocesses. For instance, in the foreground–background queue example, the\nforeground queue can be given 80 percent of the CPU time for RR scheduling\namong its processes, while the background queue receives 20 percent of the\nCPU to give to its processes on an FCFS basis.\n6.3.6\nMultilevel Feedback Queue Scheduling",
  "6.3.6\nMultilevel Feedback Queue Scheduling\nNormally, when the multilevel queue scheduling algorithm is used, processes\nare permanently assigned to a queue when they enter the system. If there\nare separate queues for foreground and background processes, for example,\nprocesses do not move from one queue to the other, since processes do not\nchange their foreground or background nature. This setup has the advantage\nof low scheduling overhead, but it is inﬂexible.\nThe multilevel feedback queue scheduling algorithm, in contrast, allows\na process to move between queues. The idea is to separate processes according\nto the characteristics of their CPU bursts. If a process uses too much CPU time,\nit will be moved to a lower-priority queue. This scheme leaves I/O-bound and",
  "interactive processes in the higher-priority queues. In addition, a process that\nwaits too long in a lower-priority queue may be moved to a higher-priority\nqueue. This form of aging prevents starvation.\nFor example, consider a multilevel feedback queue scheduler with three\nqueues, numbered from 0 to 2 (Figure 6.7). The scheduler ﬁrst executes all 276\nChapter 6\nCPU Scheduling\nquantum $ 8\nquantum $ 16\nFCFS\nFigure 6.7\nMultilevel feedback queues.\nprocesses in queue 0. Only when queue 0 is empty will it execute processes\nin queue 1. Similarly, processes in queue 2 will be executed only if queues 0\nand 1 are empty. A process that arrives for queue 1 will preempt a process in\nqueue 2. A process in queue 1 will in turn be preempted by a process arriving\nfor queue 0.",
  "for queue 0.\nA process entering the ready queue is put in queue 0. A process in queue 0\nis given a time quantum of 8 milliseconds. If it does not ﬁnish within this time,\nit is moved to the tail of queue 1. If queue 0 is empty, the process at the head\nof queue 1 is given a quantum of 16 milliseconds. If it does not complete, it is\npreempted and is put into queue 2. Processes in queue 2 are run on an FCFS\nbasis but are run only when queues 0 and 1 are empty.\nThis scheduling algorithm gives highest priority to any process with a CPU\nburst of 8 milliseconds or less. Such a process will quickly get the CPU, ﬁnish\nits CPU burst, and go off to its next I/O burst. Processes that need more than\n8 but less than 24 milliseconds are also served quickly, although with lower",
  "priority than shorter processes. Long processes automatically sink to queue\n2 and are served in FCFS order with any CPU cycles left over from queues 0\nand 1.\nIn general, a multilevel feedback queue scheduler is deﬁned by the\nfollowing parameters:\n• The number of queues\n• The scheduling algorithm for each queue\n• The method used to determine when to upgrade a process to a higher-\npriority queue\n• The method used to determine when to demote a process to a lower-\npriority queue\n• The method used to determine which queue a process will enter when that\nprocess needs service\nThe deﬁnition of a multilevel feedback queue scheduler makes it the most\ngeneral CPU-scheduling algorithm. It can be conﬁgured to match a speciﬁc",
  "system under design. Unfortunately, it is also the most complex algorithm, 6.4\nThread Scheduling\n277\nsince deﬁning the best scheduler requires some means by which to select\nvalues for all the parameters.\n6.4\nThread Scheduling\nIn Chapter 4, we introduced threads to the process model, distinguishing\nbetween user-level and kernel-level threads. On operating systems that support\nthem, it is kernel-level threads—not processes—that are being scheduled by\nthe operating system. User-level threads are managed by a thread library,\nand the kernel is unaware of them. To run on a CPU, user-level threads\nmust ultimately be mapped to an associated kernel-level thread, although\nthis mapping may be indirect and may use a lightweight process (LWP). In this",
  "section, we explore scheduling issues involving user-level and kernel-level\nthreads and offer speciﬁc examples of scheduling for Pthreads.\n6.4.1\nContention Scope\nOne distinction between user-level and kernel-level threads lies in how they\nare scheduled. On systems implementing the many-to-one (Section 4.3.1) and\nmany-to-many (Section 4.3.3) models, the thread library schedules user-level\nthreads to run on an available LWP. This scheme is known as process-\ncontention scope (PCS), since competition for the CPU takes place among\nthreads belonging to the same process. (When we say the thread library\nschedules user threads onto available LWPs, we do not mean that the threads\nare actually running on a CPU. That would require the operating system to",
  "schedule the kernel thread onto a physical CPU.) To decide which kernel-level\nthread to schedule onto a CPU, the kernel uses system-contention scope (SCS).\nCompetition for the CPU with SCS scheduling takes place among all threads\nin the system. Systems using the one-to-one model (Section 4.3.2), such as\nWindows, Linux, and Solaris, schedule threads using only SCS.\nTypically, PCS is done according to priority—the scheduler selects the\nrunnable thread with the highest priority to run. User-level thread priorities\nare set by the programmer and are not adjusted by the thread library, although\nsome thread libraries may allow the programmer to change the priority of\na thread. It is important to note that PCS will typically preempt the thread",
  "currently running in favor of a higher-priority thread; however, there is no\nguarantee of time slicing (Section 6.3.4) among threads of equal priority.\n6.4.2\nPthread Scheduling\nWe provided a sample POSIX Pthread program in Section 4.4.1, along with an\nintroduction to thread creation with Pthreads. Now, we highlight the POSIX\nPthread API that allows specifying PCS or SCS during thread creation. Pthreads\nidentiﬁes the following contention scope values:\n• PTHREAD SCOPE PROCESS schedules threads using PCS scheduling.\n• PTHREAD SCOPE SYSTEM schedules threads using SCS scheduling. 278\nChapter 6\nCPU Scheduling\nOn\nsystems\nimplementing\nthe\nmany-to-many\nmodel,\nthe\nPTHREAD SCOPE PROCESS policy schedules user-level threads onto available",
  "LWPs. The number of LWPs is maintained by the thread library, perhaps using\nscheduler activations (Section 4.6.5). The PTHREAD SCOPE SYSTEM scheduling\npolicy will create and bind an LWP for each user-level thread on many-to-many\nsystems, effectively mapping threads using the one-to-one policy.\nThe Pthread IPC provides two functions for getting—and setting—the\ncontention scope policy:\n• pthread attr setscope(pthread attr t *attr, int scope)\n• pthread attr getscope(pthread attr t *attr, int *scope)\nThe ﬁrst parameter for both functions contains a pointer to the attribute set for\nthe thread. The second parameter for the pthread attr setscope() function\nis passed either the PTHREAD SCOPE SYSTEM or the PTHREAD SCOPE PROCESS",
  "value, indicating how the contention scope is to be set. In the case of\npthread attr getscope(), this second parameter contains a pointer to an\nint value that is set to the current value of the contention scope. If an error\noccurs, each of these functions returns a nonzero value.\nIn Figure 6.8, we illustrate a Pthread scheduling\nAPI. The pro-\ngram\nﬁrst\ndetermines\nthe\nexisting\ncontention\nscope\nand\nsets\nit\nto\nPTHREAD SCOPE SYSTEM. It then creates ﬁve separate threads that will\nrun using the SCS scheduling policy. Note that on some systems, only certain\ncontention scope values are allowed. For example, Linux and Mac OS X\nsystems allow only PTHREAD SCOPE SYSTEM.\n6.5\nMultiple-Processor Scheduling\nOur discussion thus far has focused on the problems of scheduling the CPU in",
  "a system with a single processor. If multiple CPUs are available, load sharing\nbecomes possible—but scheduling problems become correspondingly more\ncomplex. Many possibilities have been tried; and as we saw with single-\nprocessor CPU scheduling, there is no one best solution.\nHere, we discuss several concerns in multiprocessor scheduling. We\nconcentrate on systems in which the processors are identical—homogeneous\n—in terms of their functionality. We can then use any available processor to\nrun any process in the queue. Note, however, that even with homogeneous\nmultiprocessors, there are sometimes limitations on scheduling. Consider a\nsystem with an I/O device attached to a private bus of one processor. Processes\nthat wish to use that device must be scheduled to run on that processor.\n6.5.1",
  "6.5.1\nApproaches to Multiple-Processor Scheduling\nOne approach to CPU scheduling in a multiprocessor system has all scheduling\ndecisions, I/O processing, and other system activities handled by a single\nprocessor—the master server. The other processors execute only user code.\nThis asymmetric multiprocessing is simple because only one processor\naccesses the system data structures, reducing the need for data sharing. 6.5\nMultiple-Processor Scheduling\n279\n#include <pthread.h>\n#include <stdio.h>\n#define NUM THREADS 5\nint main(int argc, char *argv[])\n{\nint i, scope;\npthread t tid[NUM THREADS];\npthread attr t attr;\n/* get the default attributes */\npthread attr init(&attr);\n/* first inquire on the current scope */\nif (pthread attr getscope(&attr, &scope) != 0)",
  "if (pthread attr getscope(&attr, &scope) != 0)\nfprintf(stderr, \"Unable to get scheduling scope\\n\");\nelse {\nif (scope == PTHREAD SCOPE PROCESS)\nprintf(\"PTHREAD SCOPE PROCESS\");\nelse if (scope == PTHREAD SCOPE SYSTEM)\nprintf(\"PTHREAD SCOPE SYSTEM\");\nelse\nfprintf(stderr, \"Illegal scope value.\\n\");\n}\n/* set the scheduling algorithm to PCS or SCS */\npthread attr setscope(&attr, PTHREAD SCOPE SYSTEM);\n/* create the threads */\nfor (i = 0; i < NUM THREADS; i++)\npthread create(&tid[i],&attr,runner,NULL);\n/* now join on each thread */\nfor (i = 0; i < NUM THREADS; i++)\npthread join(tid[i], NULL);\n}\n/* Each thread will begin control in this function */\nvoid *runner(void *param)\n{\n/* do some work ... */\npthread exit(0);\n}\nFigure 6.8\nPthread scheduling API.",
  "}\nFigure 6.8\nPthread scheduling API.\nA second approach uses symmetric multiprocessing (SMP), where each\nprocessor is self-scheduling. All processes may be in a common ready queue, or\neach processor may have its own private queue of ready processes. Regardless, 280\nChapter 6\nCPU Scheduling\nscheduling proceeds by having the scheduler for each processor examine the\nready queue and select a process to execute. As we saw in Chapter 5, if we have\nmultiple processors trying to access and update a common data structure, the\nscheduler must be programmed carefully. We must ensure that two separate\nprocessors do not choose to schedule the same process and that processes are\nnot lost from the queue. Virtually all modern operating systems support SMP,",
  "including Windows, Linux, and Mac OS X. In the remainder of this section, we\ndiscuss issues concerning SMP systems.\n6.5.2\nProcessor Afﬁnity\nConsider what happens to cache memory when a process has been running on\na speciﬁc processor. The data most recently accessed by the process populate\nthe cache for the processor. As a result, successive memory accesses by the\nprocess are often satisﬁed in cache memory. Now consider what happens\nif the process migrates to another processor. The contents of cache memory\nmust be invalidated for the ﬁrst processor, and the cache for the second\nprocessor must be repopulated. Because of the high cost of invalidating and\nrepopulating caches, most SMP systems try to avoid migration of processes",
  "from one processor to another and instead attempt to keep a process running\non the same processor. This is known as processor afﬁnity—that is, a process\nhas an afﬁnity for the processor on which it is currently running.\nProcessor afﬁnity takes several forms. When an operating system has a\npolicy of attempting to keep a process running on the same processor—but\nnot guaranteeing that it will do so—we have a situation known as soft afﬁnity.\nHere, the operating system will attempt to keep a process on a single processor,\nbut it is possible for a process to migrate between processors. In contrast, some\nsystems provide system calls that support hard afﬁnity, thereby allowing a\nprocess to specify a subset of processors on which it may run. Many systems",
  "provide both soft and hard afﬁnity. For example, Linux implements soft afﬁnity,\nbut it also provides the sched setaffinity() system call, which supports\nhard afﬁnity.\nThe main-memory architecture of a system can affect processor afﬁnity\nissues. Figure 6.9 illustrates an architecture featuring non-uniform memory\naccess (NUMA), in which a CPU has faster access to some parts of main memory\nthan to other parts. Typically, this occurs in systems containing combined CPU\nand memory boards. The CPUs on a board can access the memory on that\nboard faster than they can access memory on other boards in the system.\nIf the operating system’s CPU scheduler and memory-placement algorithms\nwork together, then a process that is assigned afﬁnity to a particular CPU",
  "can be allocated memory on the board where that CPU resides. This example\nalso shows that operating systems are frequently not as cleanly deﬁned and\nimplemented as described in operating-system textbooks. Rather, the “solid\nlines” between sections of an operating system are frequently only “dotted\nlines,” with algorithms creating connections in ways aimed at optimizing\nperformance and reliability.\n6.5.3\nLoad Balancing\nOn SMP systems, it is important to keep the workload balanced among all\nprocessors to fully utilize the beneﬁts of having more than one processor. 6.5\nMultiple-Processor Scheduling\n281\nCPU\nfast access\nmemory\nCPU\nfast access\nslow access\nmemory\ncomputer\nFigure 6.9\nNUMA and CPU scheduling.\nOtherwise, one or more processors may sit idle while other processors have",
  "high workloads, along with lists of processes awaiting the CPU. Load balancing\nattempts to keep the workload evenly distributed across all processors in an\nSMP system. It is important to note that load balancing is typically necessary\nonly on systems where each processor has its own private queue of eligible\nprocesses to execute. On systems with a common run queue, load balancing\nis often unnecessary, because once a processor becomes idle, it immediately\nextracts a runnable process from the common run queue. It is also important to\nnote, however, that in most contemporary operating systems supporting SMP,\neach processor does have a private queue of eligible processes.\nThere are two general approaches to load balancing: push migration and",
  "pull migration. With push migration, a speciﬁc task periodically checks the\nload on each processor and—if it ﬁnds an imbalance—evenly distributes the\nload by moving (or pushing) processes from overloaded to idle or less-busy\nprocessors. Pull migration occurs when an idle processor pulls a waiting task\nfrom a busy processor. Push and pull migration need not be mutually exclusive\nand are in fact often implemented in parallel on load-balancing systems. For\nexample, the Linux scheduler (described in Section 6.7.1) and the ULE scheduler\navailable for FreeBSD systems implement both techniques.\nInterestingly, load balancing often counteracts the beneﬁts of processor\nafﬁnity, discussed in Section 6.5.2. That is, the beneﬁt of keeping a process",
  "running on the same processor is that the process can take advantage of its data\nbeing in that processor’s cache memory. Either pulling or pushing a process\nfrom one processor to another removes this beneﬁt. As is often the case in\nsystems engineering, there is no absolute rule concerning what policy is best.\nThus, in some systems, an idle processor always pulls a process from a non-idle\nprocessor. In other systems, processes are moved only if the imbalance exceeds\na certain threshold.\n6.5.4\nMulticore Processors\nTraditionally, SMP systems have allowed several threads to run concurrently by\nproviding multiple physical processors. However, a recent practice in computer 282\nChapter 6\nCPU Scheduling\ntime\ncompute cycle\nmemory stall cycle\nthread\nC\nC\nM\nC\nM\nC\nM\nM\nC\nM\nFigure 6.10\nMemory stall.",
  "C\nC\nM\nC\nM\nC\nM\nM\nC\nM\nFigure 6.10\nMemory stall.\nhardware has been to place multiple processor cores on the same physical chip,\nresulting in a multicore processor. Each core maintains its architectural state\nand thus appears to the operating system to be a separate physical processor.\nSMP systems that use multicore processors are faster and consume less power\nthan systems in which each processor has its own physical chip.\nMulticore processors may complicate scheduling issues. Let’s consider how\nthis can happen. Researchers have discovered that when a processor accesses\nmemory, it spends a signiﬁcant amount of time waiting for the data to become\navailable. This situation, known as a memory stall, may occur for various",
  "reasons, such as a cache miss (accessing data that are not in cache memory).\nFigure 6.10 illustrates a memory stall. In this scenario, the processor can spend\nup to 50 percent of its time waiting for data to become available from memory.\nTo remedy this situation, many recent hardware designs have implemented\nmultithreaded processor cores in which two (or more) hardware threads are\nassigned to each core. That way, if one thread stalls while waiting for memory,\nthe core can switch to another thread. Figure 6.11 illustrates a dual-threaded\nprocessor core on which the execution of thread 0 and the execution of thread 1\nare interleaved. From an operating-system perspective, each hardware thread\nappears as a logical processor that is available to run a software thread. Thus,",
  "on a dual-threaded, dual-core system, four logical processors are presented to\nthe operating system. The UltraSPARC T3 CPU has sixteen cores per chip and\neight hardware threads per core. From the perspective of the operating system,\nthere appear to be 128 logical processors.\nIn general, there are two ways to multithread a processing core: coarse-\ngrained and ﬁne-grained multithreading. With coarse-grained multithreading,\na thread executes on a processor until a long-latency event such as a memory\nstall occurs. Because of the delay caused by the long-latency event, the\nprocessor must switch to another thread to begin execution. However, the\ncost of switching between threads is high, since the instruction pipeline must\ntime\nthread0\nthread1\nC\nM\nC\nM\nC\nM\nC\nC\nM\nC\nM\nC\nM\nC\nFigure 6.11",
  "thread1\nC\nM\nC\nM\nC\nM\nC\nC\nM\nC\nM\nC\nM\nC\nFigure 6.11\nMultithreaded multicore system. 6.6\nReal-Time CPU Scheduling\n283\nbe ﬂushed before the other thread can begin execution on the processor core.\nOnce this new thread begins execution, it begins ﬁlling the pipeline with its\ninstructions. Fine-grained (or interleaved) multithreading switches between\nthreads at a much ﬁner level of granularity—typically at the boundary of an\ninstruction cycle. However, the architectural design of ﬁne-grained systems\nincludes logic for thread switching. As a result, the cost of switching between\nthreads is small.\nNotice that a multithreaded multicore processor actually requires two\ndifferent levels of scheduling. On one level are the scheduling decisions that",
  "must be made by the operating system as it chooses which software thread to\nrun on each hardware thread (logical processor). For this level of scheduling,\nthe operating system may choose any scheduling algorithm, such as those\ndescribed in Section 6.3. A second level of scheduling speciﬁes how each core\ndecides which hardware thread to run. There are several strategies to adopt\nin this situation. The UltraSPARC T3, mentioned earlier, uses a simple round-\nrobin algorithm to schedule the eight hardware threads to each core. Another\nexample, the Intel Itanium, is a dual-core processor with two hardware-\nmanaged threads per core. Assigned to each hardware thread is a dynamic\nurgency value ranging from 0 to 7, with 0 representing the lowest urgency",
  "and 7 the highest. The Itanium identiﬁes ﬁve different events that may trigger\na thread switch. When one of these events occurs, the thread-switching logic\ncompares the urgency of the two threads and selects the thread with the highest\nurgency value to execute on the processor core.\n6.6\nReal-Time CPU Scheduling\nCPU scheduling for real-time operating systems involves special issues. In\ngeneral, we can distinguish between soft real-time systems and hard real-time\nsystems. Soft real-time systems provide no guarantee as to when a critical\nreal-time process will be scheduled. They guarantee only that the process will\nbe given preference over noncritical processes. Hard real-time systems have\nstricter requirements. A task must be serviced by its deadline; service after the",
  "deadline has expired is the same as no service at all. In this section, we explore\nseveral issues related to process scheduling in both soft and hard real-time\noperating systems.\n6.6.1\nMinimizing Latency\nConsider the event-driven nature of a real-time system. The system is typically\nwaiting for an event in real time to occur. Events may arise either in software\n—as when a timer expires—or in hardware—as when a remote-controlled\nvehicle detects that it is approaching an obstruction. When an event occurs, the\nsystem must respond to and service it as quickly as possible. We refer to event\nlatency as the amount of time that elapses from when an event occurs to when\nit is serviced (Figure 6.12).\nUsually, different events have different latency requirements. For example,",
  "the latency requirement for an antilock brake system might be 3 to 5 millisec-\nonds. That is, from the time a wheel ﬁrst detects that it is sliding, the system\ncontrolling the antilock brakes has 3 to 5 milliseconds to respond to and control 284\nChapter 6\nCPU Scheduling\nt1\nt0\nevent latency\nevent E first occurs\nreal-time system responds to E\nTime\nFigure 6.12\nEvent latency.\nthe situation. Any response that takes longer might result in the automobile’s\nveering out of control. In contrast, an embedded system controlling radar in\nan airliner might tolerate a latency period of several seconds.\nTwo types of latencies affect the performance of real-time systems:\n1. Interrupt latency\n2. Dispatch latency\nInterrupt latency refers to the period of time from the arrival of an interrupt",
  "at the CPU to the start of the routine that services the interrupt. When an\ninterrupt occurs, the operating system must ﬁrst complete the instruction it\nis executing and determine the type of interrupt that occurred. It must then\nsave the state of the current process before servicing the interrupt using the\nspeciﬁc interrupt service routine (ISR). The total time required to perform these\ntasks is the interrupt latency (Figure 6.13). Obviously, it is crucial for real-\ntask T running\nISR\ndetermine\ninterrupt\ntype\ninterrupt\ninterrupt\nlatency\ncontext\nswitch\ntime\nFigure 6.13\nInterrupt latency. 6.6\nReal-Time CPU Scheduling\n285\nresponse to event\nreal-time \nprocess \nexecution\nevent\nconflicts\ntime\ndispatch \nresponse interval\ndispatch latency\nprocess made \navailable\ninterrupt \nprocessing\nFigure 6.14",
  "available\ninterrupt \nprocessing\nFigure 6.14\nDispatch latency.\ntime operating systems to minimize interrupt latency to ensure that real-time\ntasks receive immediate attention. Indeed, for hard real-time systems, interrupt\nlatency must not simply be minimized, it must be bounded to meet the strict\nrequirements of these systems.\nOne important factor contributing to interrupt latency is the amount of time\ninterrupts may be disabled while kernel data structures are being updated.\nReal-time operating systems require that interrupts be disabled for only very\nshort periods of time.\nThe amount of time required for the scheduling dispatcher to stop one\nprocess and start another is known as dispatch latency. Providing real-time\ntasks with immediate access to the CPU mandates that real-time operating",
  "systems minimize this latency as well. The most effective technique for keeping\ndispatch latency low is to provide preemptive kernels.\nIn Figure 6.14, we diagram the makeup of dispatch latency. The conﬂict\nphase of dispatch latency has two components:\n1. Preemption of any process running in the kernel\n2. Release by low-priority processes of resources needed by a high-priority\nprocess\nAs an example, in Solaris, the dispatch latency with preemption disabled\nis over a hundred milliseconds. With preemption enabled, it is reduced to less\nthan a millisecond.\n6.6.2\nPriority-Based Scheduling\nThe most important feature of a real-time operating system is to respond\nimmediately to a real-time process as soon as that process requires the CPU. 286\nChapter 6\nCPU Scheduling",
  "Chapter 6\nCPU Scheduling\nAs a result, the scheduler for a real-time operating system must support a\npriority-based algorithm with preemption. Recall that priority-based schedul-\ning algorithms assign each process a priority based on its importance; more\nimportant tasks are assigned higher priorities than those deemed less impor-\ntant. If the scheduler also supports preemption, a process currently running\non the CPU will be preempted if a higher-priority process becomes available to\nrun.\nPreemptive, priority-based scheduling algorithms are discussed in detail in\nSection 6.3.3, and Section 6.7 presents examples of the soft real-time scheduling\nfeatures of the Linux, Windows, and Solaris operating systems. Each of\nthese systems assigns real-time processes the highest scheduling priority. For",
  "example, Windows has 32 different priority levels. The highest levels—priority\nvalues 16 to 31—are reserved for real-time processes. Solaris and Linux have\nsimilar prioritization schemes.\nNote that providing a preemptive, priority-based scheduler only guaran-\ntees soft real-time functionality. Hard real-time systems must further guarantee\nthat real-time tasks will be serviced in accord with their deadline requirements,\nand making such guarantees requires additional scheduling features. In the\nremainder of this section, we cover scheduling algorithms appropriate for\nhard real-time systems.\nBefore we proceed with the details of the individual schedulers, however,\nwe must deﬁne certain characteristics of the processes that are to be scheduled.",
  "First, the processes are considered periodic. That is, they require the CPU at\nconstant intervals (periods). Once a periodic process has acquired the CPU, it\nhas a ﬁxed processing time t, a deadline d by which it must be serviced by the\nCPU, and a period p. The relationship of the processing time, the deadline, and\nthe period can be expressed as 0 ≤t ≤d ≤p. The rate of a periodic task is 1/p.\nFigure 6.15 illustrates the execution of a periodic process over time. Schedulers\ncan take advantage of these characteristics and assign priorities according to a\nprocess’s deadline or rate requirements.\nWhat is unusual about this form of scheduling is that a process may have to\nannounce its deadline requirements to the scheduler. Then, using a technique",
  "known as an admission-control algorithm, the scheduler does one of two\nthings. It either admits the process, guaranteeing that the process will complete\non time, or rejects the request as impossible if it cannot guarantee that the task\nwill be serviced by its deadline.\nperiod1\nperiod2\nperiod3\nTime\np\np\np\nd\nd\nd\nt\nt\nt\nFigure 6.15\nPeriodic task. 6.6\nReal-Time CPU Scheduling\n287\n0\n10\n20\n30\n40\n50\n60\n70\n80\n120\n90\n100\n110\nP1\nP1\nP1, P2\nP2\ndeadlines\nFigure 6.16\nScheduling of tasks when P2 has a higher priority than P1.\n6.6.3\nRate-Monotonic Scheduling\nThe rate-monotonic scheduling algorithm schedules periodic tasks using a\nstatic priority policy with preemption. If a lower-priority process is running\nand a higher-priority process becomes available to run, it will preempt the",
  "lower-priorityprocess. Uponenteringthe system, eachperiodictaskisassigned\na priority inversely based on its period. The shorter the period, the higher the\npriority; the longer the period, the lower the priority. The rationale behind this\npolicy is to assign a higher priority to tasks that require the CPU more often.\nFurthermore, rate-monotonic scheduling assumes that the processing time of\na periodic process is the same for each CPU burst. That is, every time a process\nacquires the CPU, the duration of its CPU burst is the same.\nLet’s consider an example. We have two processes, P1 and P2. The periods\nfor P1 and P2 are 50 and 100, respectively—that is, p1 = 50 and p2 = 100. The\nprocessing times are t1 = 20 for P1 and t2 = 35 for P2. The deadline for each",
  "process requires that it complete its CPU burst by the start of its next period.\nWe must ﬁrst ask ourselves whether it is possible to schedule these tasks\nso that each meets its deadlines. If we measure the CPU utilization of a process\nPi as the ratio of its burst to its period—ti/pi —the CPU utilization of P1 is\n20/50 = 0.40 and that of P2 is 35/100 = 0.35, for a total CPU utilization of 75\npercent. Therefore, it seems we can schedule these tasks in such a way that\nboth meet their deadlines and still leave the CPU with available cycles.\nSuppose we assign P2 a higher priority than P1. The execution of P1 and P2\nin this situation is shown in Figure 6.16. As we can see, P2 starts execution ﬁrst\nand completes at time 35. At this point, P1 starts; it completes its CPU burst at",
  "time 55. However, the ﬁrst deadline for P1 was at time 50, so the scheduler has\ncaused P1 to miss its deadline.\nNow suppose we use rate-monotonic scheduling, in which we assign P1\na higher priority than P2 because the period of P1 is shorter than that of P2.\nThe execution of these processes in this situation is shown in Figure 6.17.\nP1 starts ﬁrst and completes its CPU burst at time 20, thereby meeting its ﬁrst\ndeadline. P2 starts running at this point and runs until time 50. At this time, it is\npreempted by P1, although it still has 5 milliseconds remaining in its CPU burst.\nP1 completes its CPU burst at time 70, at which point the scheduler resumes\n0\n10\n20\n30\n40\n50\n60\n70\n80\n120 130 140 150 160 170 180 190 200\n90 100 110\nP1\nP1\nP1, P2\nP1\nP2\ndeadlines\nP1, P2\nP1\nP2\nP1\nP2\nP1\nP2\nFigure 6.17",
  "P2\ndeadlines\nP1, P2\nP1\nP2\nP1\nP2\nP1\nP2\nFigure 6.17\nRate-monotonic scheduling. 288\nChapter 6\nCPU Scheduling\nP2. P2 completes its CPU burst at time 75, also meeting its ﬁrst deadline. The\nsystem is idle until time 100, when P1 is scheduled again.\nRate-monotonic scheduling is considered optimal in that if a set of\nprocesses cannot be scheduled by this algorithm, it cannot be scheduled by\nany other algorithm that assigns static priorities. Let’s next examine a set of\nprocesses that cannot be scheduled using the rate-monotonic algorithm.\nAssume that process P1 has a period of p1 = 50 and a CPU burst of t1 = 25.\nFor P2, the corresponding values are p2 = 80 and t2 = 35. Rate-monotonic\nscheduling would assign process P1 a higher priority, as it has the shorter",
  "period. The total CPU utilization of the two processes is (25/50)+(35/80) = 0.94,\nand it therefore seems logical that the two processes could be scheduled and still\nleave the CPU with 6 percent available time. Figure 6.18 shows the scheduling\nof processes P1 and P2. Initially, P1 runs until it completes its CPU burst at\ntime 25. Process P2 then begins running and runs until time 50, when it is\npreempted by P1. At this point, P2 still has 10 milliseconds remaining in its\nCPU burst. Process P1 runs until time 75; consequently, P2 misses the deadline\nfor completion of its CPU burst at time 80.\nDespite being optimal, then, rate-monotonic scheduling has a limitation:\nCPU utilization is bounded, and it is not always possible fully to maximize CPU",
  "resources. The worst-case CPU utilization for scheduling N processes is\nN(21/N −1).\nWith one process in the system, CPU utilization is 100 percent, but it falls\nto approximately 69 percent as the number of processes approaches inﬁnity.\nWith two processes, CPU utilization is bounded at about 83 percent. Combined\nCPU utilization for the two processes scheduled in Figure 6.16 and Figure\n6.17 is 75 percent; therefore, the rate-monotonic scheduling algorithm is\nguaranteed to schedule them so that they can meet their deadlines. For the two\nprocesses scheduled in Figure 6.18, combined CPU utilization is approximately\n94 percent; therefore, rate-monotonic scheduling cannot guarantee that they\ncan be scheduled so that they meet their deadlines.\n6.6.4\nEarliest-Deadline-First Scheduling",
  "6.6.4\nEarliest-Deadline-First Scheduling\nEarliest-deadline-ﬁrst (EDF) scheduling dynamically assigns priorities accord-\ning to deadline. The earlier the deadline, the higher the priority; the later the\ndeadline, the lower the priority. Under the EDF policy, when a process becomes\nrunnable, it must announce its deadline requirements to the system. Priorities\nmay have to be adjusted to reﬂect the deadline of the newly runnable process.\nNote how this differs from rate-monotonic scheduling, where priorities are\nﬁxed.\n0\n10\n20\n30\n40\n50\n60\n70\n80\n120 130 140 150 160\n90\n100 110\nP1\nP1\nP2\nP1\nP2\ndeadlines\nP1\nP2\nP1, P2\nFigure 6.18\nMissing deadlines with rate-monotonic scheduling. 6.6\nReal-Time CPU Scheduling\n289\n0\n10\n20\n30\n40\n50\n60\n70\n80\n120 130 140 150 160\n90\n100 110\nP1\nP1\nP1\nP2\nP1\nP2\ndeadlines\nP2\nP1",
  "90\n100 110\nP1\nP1\nP1\nP2\nP1\nP2\ndeadlines\nP2\nP1\nP1\nP2\nP2\nFigure 6.19\nEarliest-deadline-ﬁrst scheduling.\nTo illustrate EDF scheduling, we again schedule the processes shown in\nFigure 6.18, which failed to meet deadline requirements under rate-monotonic\nscheduling. Recall that P1 has values of p1 = 50 and t1 = 25 and that P2 has\nvalues of p2 = 80 and t2 = 35. The EDF scheduling of these processes is shown\nin Figure 6.19. Process P1 has the earliest deadline, so its initial priority is higher\nthan that of process P2. Process P2 begins running at the end of the CPU burst\nfor P1. However, whereas rate-monotonic scheduling allows P1 to preempt P2\nat the beginning of its next period at time 50, EDF scheduling allows process",
  "P2 to continue running. P2 now has a higher priority than P1 because its next\ndeadline (at time 80) is earlier than that of P1 (at time 100). Thus, both P1 and\nP2 meet their ﬁrst deadlines. Process P1 again begins running at time 60 and\ncompletes its second CPU burst at time 85, also meeting its second deadline at\ntime 100. P2 begins running at this point, only to be preempted by P1 at the\nstart of its next period at time 100. P2 is preempted because P1 has an earlier\ndeadline (time 150) than P2 (time 160). At time 125, P1 completes its CPU burst\nand P2 resumes execution, ﬁnishing at time 145 and meeting its deadline as\nwell. The system is idle until time 150, when P1 is scheduled to run once again.\nUnlike the rate-monotonic algorithm, EDF scheduling does not require that",
  "processes be periodic, nor must a process require a constant amount of CPU\ntime per burst. The only requirement is that a process announce its deadline\nto the scheduler when it becomes runnable. The appeal of EDF scheduling is\nthat it is theoretically optimal—theoretically, it can schedule processes so that\neach process can meet its deadline requirements and CPU utilization will be\n100 percent. In practice, however, it is impossible to achieve this level of CPU\nutilization due to the cost of context switching between processes and interrupt\nhandling.\n6.6.5\nProportional Share Scheduling\nProportional share schedulers operate by allocating T shares among all\napplications. An application can receive N shares of time, thus ensuring that",
  "the application will have N/T of the total processor time. As an example,\nassume that a total of T = 100 shares is to be divided among three processes,\nA, B, and C. Ais assigned 50 shares, B is assigned 15 shares, and C is assigned\n20 shares. This scheme ensures that A will have 50 percent of total processor\ntime, B will have 15 percent, and C will have 20 percent.\nProportional share schedulers must work in conjunction with an\nadmission-control policy to guarantee that an application receives its allocated\nshares of time. An admission-control policy will admit a client requesting\na particular number of shares only if sufﬁcient shares are available. In our\ncurrent example, we have allocated 50 + 15 + 20 = 85 shares of the total of 290\nChapter 6\nCPU Scheduling",
  "Chapter 6\nCPU Scheduling\n100 shares. If a new process D requested 30 shares, the admission controller\nwould deny D entry into the system.\n6.6.6\nPOSIX Real-Time Scheduling\nThe POSIX standard also provides extensions for real-time computing—\nPOSIX.1b. Here, we cover some of the POSIX API related to scheduling real-time\nthreads. POSIX deﬁnes two scheduling classes for real-time threads:\n• SCHED FIFO\n• SCHED RR\nSCHED FIFO schedules threads according to a ﬁrst-come, ﬁrst-served policy\nusing a FIFO queue as outlined in Section 6.3.1. However, there is no time slicing\namongthreadsofequal priority. Therefore, the highest-priorityreal-time thread\nat the front of the FIFO queue will be granted the CPU until it terminates or",
  "blocks. SCHED RR uses a round-robin policy. It is similar to SCHED FIFO except\nthat it provides time slicing among threads of equal priority. POSIX provides\nan additional scheduling class—SCHED OTHER—but its implementation is\nundeﬁned and system speciﬁc; it may behave differently on different systems.\nThe POSIX API speciﬁes the following two functions for getting and setting\nthe scheduling policy:\n• pthread attr getsched policy(pthread attr t *attr, int\n*policy)\n• pthread attr setsched policy(pthread attr t *attr, int\npolicy)\nThe ﬁrst parameter to both functions is a pointer to the set of attributes for\nthe thread. The second parameter is either (1) a pointer to an integer that is\nset to the current scheduling policy (for pthread attr getsched policy())",
  "or (2) an integer value (SCHED FIFO, SCHED RR, or SCHED OTHER) for the\npthread attr setsched policy() function. Both functions return nonzero\nvalues if an error occurs.\nIn Figure 6.20, we illustrate a POSIX Pthread program using this API. This\nprogram ﬁrst determines the current scheduling policy and then sets the\nscheduling algorithm to SCHED FIFO.\n6.7\nOperating-System Examples\nWe turn next to a description of the scheduling policies of the Linux, Windows,\nand Solaris operating systems. It is important to note that we use the term\nprocess scheduling in a general sense here. In fact, we are describing the\nscheduling of kernel threads with Solaris and Windows systems and of tasks\nwith the Linux scheduler.\n6.7.1\nExample: Linux Scheduling",
  "6.7.1\nExample: Linux Scheduling\nProcess scheduling in Linux has had an interesting history. Prior to Version 2.5,\nthe Linux kernel ran a variation of the traditional UNIX scheduling algorithm. 6.7\nOperating-System Examples\n291\n#include <pthread.h>\n#include <stdio.h>\n#define NUM THREADS 5\nint main(int argc, char *argv[])\n{\nint i, policy;\npthread t tid[NUM THREADS];\npthread attr t attr;\n/* get the default attributes */\npthread attr init(&attr);\n/* get the current scheduling policy */\nif (pthread attr getschedpolicy(&attr, &policy) != 0)\nfprintf(stderr, \"Unable to get policy.\\n\");\nelse {\nif (policy == SCHED OTHER)\nprintf(\"SCHED OTHER\\n\");\nelse if (policy == SCHED RR)\nprintf(\"SCHED RR\\n\");\nelse if (policy == SCHED FIFO)\nprintf(\"SCHED FIFO\\n\");\n}",
  "printf(\"SCHED FIFO\\n\");\n}\n/* set the scheduling policy - FIFO, RR, or OTHER */\nif (pthread attr setschedpolicy(&attr, SCHED FIFO) != 0)\nfprintf(stderr, \"Unable to set policy.\\n\");\n/* create the threads */\nfor (i = 0; i < NUM THREADS; i++)\npthread create(&tid[i],&attr,runner,NULL);\n/* now join on each thread */\nfor (i = 0; i < NUM THREADS; i++)\npthread join(tid[i], NULL);\n}\n/* Each thread will begin control in this function */\nvoid *runner(void *param)\n{\n/* do some work ... */\npthread exit(0);\n}\nFigure 6.20\nPOSIX real-time scheduling API. 292\nChapter 6\nCPU Scheduling\nHowever, as this algorithm was not designed with SMP systems in mind, it\ndid not adequately support systems with multiple processors. In addition, it\nresulted in poor performance for systems with a large number of runnable",
  "processes. With Version 2.5 of the kernel, the scheduler was overhauled to\ninclude a scheduling algorithm—known as O(1)—that ran in constant time\nregardless of the number of tasks in the system. The O(1) scheduler also\nprovided increased support for SMP systems, including processor afﬁnity and\nload balancing between processors. However, in practice, although the O(1)\nscheduler delivered excellent performance on SMP systems, it led to poor\nresponse times for the interactive processes that are common on many desktop\ncomputer systems. During development of the 2.6 kernel, the scheduler was\nagain revised; and in release 2.6.23 of the kernel, the Completely Fair Scheduler\n(CFS) became the default Linux scheduling algorithm.",
  "Scheduling in the Linux system is based on scheduling classes. Each class is\nassigned a speciﬁc priority. By using different scheduling classes, the kernel can\naccommodate different scheduling algorithms based on the needs of the system\nand its processes. The scheduling criteria for a Linux server, for example, may\nbe different from those for a mobile device running Linux. To decide which\ntask to run next, the scheduler selects the highest-priority task belonging to\nthe highest-priority scheduling class. Standard Linux kernels implement two\nscheduling classes: (1) a default scheduling class using the CFS scheduling\nalgorithm and (2) a real-time scheduling class. We discuss each of these classes\nhere. New scheduling classes can, of course, be added.",
  "Rather than using strict rules that associate a relative priority value with\nthe length of a time quantum, the CFS scheduler assigns a proportion of CPU\nprocessing time to each task. This proportion is calculated based on the nice\nvalue assigned to each task. Nice values range from −20 to +19, where a\nnumerically lower nice value indicates a higher relative priority. Tasks with\nlower nice values receive a higher proportion of CPU processing time than\ntasks with higher nice values. The default nice value is 0. (The term nice comes\nfrom the idea that if a task increases its nice value from, say, 0 to +10, it is being\nnice to other tasks in the system by lowering its relative priority.) CFS doesn’t\nuse discrete values of time slices and instead identiﬁes a targeted latency,",
  "which is an interval of time during which every runnable task should run at\nleast once. Proportions of CPU time are allocated from the value of targeted\nlatency. In addition to having default and minimum values, targeted latency\ncan increase if the number of active tasks in the system grows beyond a certain\nthreshold.\nThe CFS scheduler doesn’t directly assign priorities. Rather, it records how\nlong each task has run by maintaining the virtual run time of each task using\nthe per-task variable vruntime. The virtual run time is associated with a decay\nfactor based on the priority of a task: lower-priority tasks have higher rates\nof decay than higher-priority tasks. For tasks at normal priority (nice values\nof 0), virtual run time is identical to actual physical run time. Thus, if a task",
  "with default priority runs for 200 milliseconds, its vruntime will also be 200\nmilliseconds. However, if a lower-priority task runs for 200 milliseconds, its\nvruntime will be higher than 200 milliseconds. Similarly, if a higher-priority\ntask runs for 200 milliseconds, its vruntime will be less than 200 milliseconds.\nTo decide which task to run next, the scheduler simply selects the task that has\nthe smallest vruntime value. In addition, a higher-priority task that becomes\navailable to run can preempt a lower-priority task. 6.7\nOperating-System Examples\n293\nCFS PERFORMANCE\nThe Linux CFS scheduler provides an efﬁcient algorithm for selecting which\ntask to run next. Each runnable task is placed in a red-black tree—a balanced",
  "binary search tree whose key is based on the value of vruntime. This tree is\nshown below:\nT0\nT2\nT3\nT5\nT6\nT1\nT4\nT9\nT7\nT8\nsmaller\nlarger\nTask with the smallest\nvalue of vruntime\nValue of vruntime\nWhen a task becomes runnable, it is added to the tree. If a task on the\ntree is not runnable (for example, if it is blocked while waiting for I/O), it is\nremoved. Generally speaking, tasks that have been given less processing time\n(smaller values of vruntime) are toward the left side of the tree, and tasks\nthat have been given more processing time are on the right side. According\nto the properties of a binary search tree, the leftmost node has the smallest\nkey value, which for the sake of the CFS scheduler means that it is the task",
  "with the highest priority. Because the red-black tree is balanced, navigating\nit to discover the leftmost node will require O(lgN) operations (where N\nis the number of nodes in the tree). However, for efﬁciency reasons, the\nLinux scheduler caches this value in the variable rb leftmost, and thus\ndetermining which task to run next requires only retrieving the cached value.\nLet’s examine the CFS scheduler in action: Assume that two tasks have the\nsame nice values. One task is I/O-bound and the other is CPU-bound. Typically,\nthe I/O-bound task will run only for short periods before blocking for additional\nI/O, and the CPU-bound task will exhaust its time period whenever it has\nan opportunity to run on a processor. Therefore, the value of vruntime will",
  "eventually be lower for the I/O-bound task than for the CPU-bound task, giving\nthe I/O-bound task higher priority than the CPU-bound task. At that point, if\nthe CPU-bound task is executing when the I/O-bound task becomes eligible\nto run (for example, when I/O the task is waiting for becomes available), the\nI/O-bound task will preempt the CPU-bound task.\nLinux also implements real-time scheduling using the POSIX standard as\ndescribed in Section 6.6.6. Any task scheduled using either the SCHED FIFO or\nthe SCHED RR real-time policy runs at a higher priority than normal (non-real- 294\nChapter 6\nCPU Scheduling\n0\n100\n139\n99\nReal-Time\nNormal\nPriority\nHigher\nLower\nFigure 6.21\nScheduling priorities on a Linux system.\ntime) tasks. Linux uses two separate priority ranges, one for real-time tasks",
  "and a second for normal tasks. Real-time tasks are assigned static priorities\nwithin the range of 0 to 99, and normal (i.e. non real-time) tasks are assigned\npriorities from 100 to 139. These two ranges map into a global priority scheme\nwherein numerically lower values indicate higher relative priorities. Normal\ntasks are assigned a priority based on their nice values, where a value of –20\nmaps to priority 100 and a nice value of +19 maps to 139. This scheme is shown\nin Figure 6.21.\n6.7.2\nExample: Windows Scheduling\nWindows schedules threads using a priority-based, preemptive scheduling\nalgorithm. The Windows scheduler ensures that the highest-priority thread\nwill always run. The portion of the Windows kernel that handles scheduling",
  "is called the dispatcher. A thread selected to run by the dispatcher will run\nuntil it is preempted by a higher-priority thread, until it terminates, until its\ntime quantum ends, or until it calls a blocking system call, such as for I/O. If a\nhigher-priority real-time thread becomes ready while a lower-priority thread\nis running, the lower-priority thread will be preempted. This preemption gives\na real-time thread preferential access to the CPU when the thread needs such\naccess.\nThe dispatcher uses a 32-level priority scheme to determine the order of\nthread execution. Priorities are divided into two classes. The variable class\ncontains threads having priorities from 1 to 15, and the real-time class contains\nthreads with priorities ranging from 16 to 31. (There is also a thread running at",
  "priority 0 that is used for memory management.) The dispatcher uses a queue\nfor each scheduling priority and traverses the set of queues from highest to\nlowest until it ﬁnds a thread that is ready to run. If no ready thread is found,\nthe dispatcher will execute a special thread called the idle thread.\nThere is a relationship between the numeric priorities of the Windows\nkernel and the Windows API. The Windows API identiﬁes the following six\npriority classes to which a process can belong:\n• IDLE PRIORITY CLASS\n• BELOW NORMAL PRIORITY CLASS\n• NORMAL PRIORITY CLASS\n• ABOVE NORMAL PRIORITY CLASS 6.7\nOperating-System Examples\n295\n• HIGH PRIORITY CLASS\n• REALTIME PRIORITY CLASS\nProcesses are typically members of the NORMAL PRIORITY CLASS. A process",
  "belongs to this class unless the parent of the process was a member of the\nIDLE PRIORITY CLASS or unless another class was speciﬁed when the process\nwas created. Additionally, the priority class of a process can be altered with\nthe SetPriorityClass() function in the Windows API. Priorities in all classes\nexcept the REALTIME PRIORITY CLASS are variable, meaning that the priority of\na thread belonging to one of these classes can change.\nA thread within a given priority classes also has a relative priority. The\nvalues for relative priorities include:\n• IDLE\n• LOWEST\n• BELOW NORMAL\n• NORMAL\n• ABOVE NORMAL\n• HIGHEST\n• TIME CRITICAL\nThe priority of each thread is based on both the priority class it belongs to\nand its relative priority within that class. This relationship is shown in Figure",
  "6.22. The values of the priority classes appear in the top row. The left column\ncontains the values for the relative priorities. For example, if the relative priority\nof a thread in the ABOVE NORMAL PRIORITY CLASS is NORMAL, the numeric\npriority of that thread is 10.\nFurthermore, each thread has a base priority representing a value in the\npriority range for the class to which the thread belongs. By default, the base\nhigh\nabove\nnormal\nnormal\nbelow\nnormal\nidle\npriority\ntime-critical\nreal-\ntime\n31\n26\n25\n24\n23\n22\n16\n15\n15\n14\n13\n12\n11\n1\n15\n12\n11\n10\n9\n8\n1\n15\n10\n9\n8\n7\n6\n1\n15\n8\n7\n6\n5\n4\n1\n15\n6\n5\n4\n3\n2\n1\nhighest\nabove normal\nnormal\nlowest\nidle\nbelow normal\nFigure 6.22\nWindows thread priorities. 296\nChapter 6\nCPU Scheduling",
  "Chapter 6\nCPU Scheduling\npriority is the value of the NORMAL relative priority for that class. The base\npriorities for each priority class are as follows:\n• REALTIME PRIORITY CLASS—24\n• HIGH PRIORITY CLASS—13\n• ABOVE NORMAL PRIORITY CLASS—10\n• NORMAL PRIORITY CLASS—8\n• BELOW NORMAL PRIORITY CLASS—6\n• IDLE PRIORITY CLASS—4\nThe initial priority of a thread is typically the base priority of the process\nthe thread belongs to, although the SetThreadPriority() function in the\nWindows API can also be used to modify a thread’s the base priority.\nWhen a thread’s time quantum runs out, that thread is interrupted. If the\nthread is in the variable-priority class, its priority is lowered. The priority is\nnever lowered below the base priority, however. Lowering the priority tends",
  "to limit the CPU consumption of compute-bound threads. When a variable-\npriority thread is released from a wait operation, the dispatcher boosts the\npriority. The amount of the boost depends on what the thread was waiting for.\nFor example, a thread waiting for keyboard I/O would get a large increase,\nwhereas a thread waiting for a disk operation would get a moderate one.\nThis strategy tends to give good response times to interactive threads that\nare using the mouse and windows. It also enables I/O-bound threads to keep\nthe I/O devices busy while permitting compute-bound threads to use spare\nCPU cycles in the background. This strategy is used by several time-sharing\noperating systems, including UNIX. In addition, the window with which the",
  "user is currently interacting receives a priority boost to enhance its response\ntime.\nWhen a user is running an interactive program, the system needs to provide\nespecially good performance. For this reason, Windows has a special schedul-\ning rule for processes in the NORMAL PRIORITY CLASS. Windows distinguishes\nbetween the foreground process that is currently selected on the screen and\nthe background processes that are not currently selected. When a process\nmoves into the foreground, Windows increases the scheduling quantum by\nsome factor—typically by 3. This increase gives the foreground process three\ntimes longer to run before a time-sharing preemption occurs.\nWindows 7 introduced user-mode scheduling (UMS), which allows appli-",
  "cations to create and manage threads independently of the kernel. Thus,\nan application can create and schedule multiple threads without involving\nthe Windows kernel scheduler. For applications that create a large number\nof threads, scheduling threads in user mode is much more efﬁcient than\nkernel-mode thread scheduling, as no kernel intervention is necessary.\nEarlier versions of Windows provided a similar feature known as ﬁbers,\nwhich allowed several user-mode threads (ﬁbers) to be mapped to a single\nkernel thread. However, ﬁbers were of limited practical use. A ﬁber was\nunable to make calls to the Windows API because all ﬁbers had to share the\nthread environment block (TEB) of the thread on which they were running. This 6.7\nOperating-System Examples\n297",
  "Operating-System Examples\n297\npresented a problem if a Windows API function placed state information into\nthe TEB for one ﬁber, only to have the information overwritten by a different\nﬁber. UMS overcomes this obstacle by providing each user-mode thread with\nits own thread context.\nIn addition, unlike ﬁbers, UMS is not intended to be used directly by\nthe programmer. The details of writing user-mode schedulers can be very\nchallenging, and UMS does not include such a scheduler. Rather, the schedulers\ncome from programming language libraries that build on top of UMS. For\nexample, Microsoft provides Concurrency Runtime (ConcRT), a concurrent\nprogramming framework for C++ that is designed for task-based parallelism\n(Section 4.2) on multicore processors. ConcRT provides a user-mode scheduler",
  "together with facilities for decomposing programs into tasks, which can then\nbe scheduled on the available processing cores. Further details on UMS can be\nfound in Section 19.7.3.7.\n6.7.3\nExample: Solaris Scheduling\nSolaris uses priority-based thread scheduling. Each thread belongs to one of\nsix classes:\n1. Time sharing (TS)\n2. Interactive (IA)\n3. Real time (RT)\n4. System (SYS)\n5. Fair share (FSS)\n6. Fixed priority (FP)\nWithin each class there are different priorities and different scheduling algo-\nrithms.\nThe default scheduling class for a process is time sharing. The scheduling\npolicy for the time-sharing class dynamically alters priorities and assigns time\nslices of different lengths using a multilevel feedback queue. By default, there",
  "is an inverse relationship between priorities and time slices. The higher the\npriority, the smaller the time slice; and the lower the priority, the larger the\ntime slice. Interactive processes typically have a higher priority; CPU-bound\nprocesses, a lower priority. This scheduling policy gives good response time\nfor interactive processes and good throughput for CPU-bound processes. The\ninteractive class uses the same scheduling policy as the time-sharing class, but\nit gives windowing applications—such as those created by the KDE or GNOME\nwindow managers—a higher priority for better performance.\nFigure 6.23 shows the dispatch table for scheduling time-sharing and\ninteractive threads. These two scheduling classes include 60 priority levels,",
  "but for brevity, we display only a handful. The dispatch table shown in Figure\n6.23 contains the following ﬁelds:\n• Priority. The class-dependent priority for the time-sharing and interactive\nclasses. A higher number indicates a higher priority. 298\nChapter 6\nCPU Scheduling\ntime\nquantum\npriority\nreturn\nfrom \nsleep\ntime\nquantum\nexpired\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n55\n59\n200\n200\n160\n160\n120\n120\n80\n80\n40\n40\n40\n40\n20\n0\n0\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n49\n50\n50\n51\n51\n52\n52\n53\n54\n55\n56\n58\n58\n59\nFigure 6.23\nSolaris dispatch table for time-sharing and interactive threads.\n• Time quantum. The time quantum for the associated priority. This illus-\ntrates the inverse relationship between priorities and time quanta: the\nlowest priority (priority 0) has the highest time quantum (200 millisec-",
  "onds), and the highest priority (priority 59) has the lowest time quantum\n(20 milliseconds).\n• Time quantum expired. The new priority of a thread that has used\nits entire time quantum without blocking. Such threads are considered\nCPU-intensive. As shown in the table, these threads have their priorities\nlowered.\n• Return from sleep. The priority of a thread that is returning from sleeping\n(such as from waiting for I/O). As the table illustrates, when I/O is available\nfor a waiting thread, its priority is boosted to between 50 and 59, supporting\nthe scheduling policy of providing good response time for interactive\nprocesses.\nThreads in the real-time class are given the highest priority. A real-time\nprocess will run before a process in any other class. This assignment allows",
  "a real-time process to have a guaranteed response from the system within\na bounded period of time. In general, however, few processes belong to the\nreal-time class.\nSolaris uses the system class to run kernel threads, such as the scheduler\nand paging daemon. Once the priority of a system thread is established, it does\nnot change. The system class is reserved for kernel use (user processes running\nin kernel mode are not in the system class). 6.7\nOperating-System Examples\n299\nThe ﬁxed-priority and fair-share classes were introduced with Solaris 9.\nThreads in the ﬁxed-priority class have the same priority range as those in\nthe time-sharing class; however, their priorities are not dynamically adjusted.\nThe fair-share scheduling class uses CPU shares instead of priorities to",
  "make scheduling decisions. CPU shares indicate entitlement to available CPU\nresources and are allocated to a set of processes (known as a project).\nEach scheduling class includes a set of priorities. However, the scheduler\nconverts the class-speciﬁc priorities into global priorities and selects the thread\nwith the highest global priority to run. The selected thread runs on the CPU\nuntil it (1) blocks, (2) uses its time slice, or (3) is preempted by a higher-priority\nthread. If there are multiple threads with the same priority, the scheduler uses\na round-robin queue. Figure 6.24 illustrates how the six scheduling classes\nrelate to one another and how they map to global priorities. Notice that the\nkernel maintains ten threads for servicing interrupts. These threads do not",
  "belong to any scheduling class and execute at the highest priority (160–169).\nAs mentioned, Solaris has traditionally used the many-to-many model (Section\n4.3.3) but switched to the one-to-one model (Section 4.3.2) beginning with\nSolaris 9.\ninterrupt threads\n169\nhighest\nlowest\nfirst\nscheduling\norder\nglobal\npriority\nlast\n160\n159\n100\n60\n59\n0\n99\nrealtime (RT) threads\nsystem (SYS) threads\nfair share (FSS) threads\nfixed priority (FX) threads\ntimeshare (TS) threads\ninteractive (IA) threads\nFigure 6.24\nSolaris scheduling. 300\nChapter 6\nCPU Scheduling\n6.8\nAlgorithm Evaluation\nHow do we select a CPU-scheduling algorithm for a particular system? As we\nsaw in Section 6.3, there are many scheduling algorithms, each with its own\nparameters. As a result, selecting an algorithm can be difﬁcult.",
  "The ﬁrst problem is deﬁning the criteria to be used in selecting an algorithm.\nAs we saw in Section 6.2, criteria are often deﬁned in terms of CPU utilization,\nresponse time, or throughput. To select an algorithm, we must ﬁrst deﬁne\nthe relative importance of these elements. Our criteria may include several\nmeasures, such as these:\n• Maximizing CPU utilization under the constraint that the maximum\nresponse time is 1 second\n• Maximizing throughput such that turnaround time is (on average) linearly\nproportional to total execution time\nOnce the selection criteria have been deﬁned, we want to evaluate the\nalgorithms under consideration. We next describe the various evaluation\nmethods we can use.\n6.8.1\nDeterministic Modeling",
  "methods we can use.\n6.8.1\nDeterministic Modeling\nOne major class of evaluation methods is analytic evaluation. Analytic\nevaluation uses the given algorithm and the system workload to produce\na formula or number to evaluate the performance of the algorithm for that\nworkload.\nDeterministic modeling is one type of analytic evaluation. This method\ntakesaparticularpredetermined workload and deﬁnesthe performance ofeach\nalgorithm for that workload. For example, assume that we have the workload\nshown below. All ﬁve processes arrive at time 0, in the order given, with the\nlength of the CPU burst given in milliseconds:\nProcess\nBurst Time\nP1\n10\nP2\n29\nP3\n3\nP4\n7\nP5\n12\nConsider the FCFS, SJF, and RR (quantum = 10 milliseconds) scheduling",
  "algorithms for this set of processes. Which algorithm would give the minimum\naverage waiting time?\nFor the FCFS algorithm, we would execute the processes as\nP2\nP5\nP3\nP4\nP1\n61\n39\n49\n42\n0\n10 6.8\nAlgorithm Evaluation\n301\nThe waiting time is 0 milliseconds for process P1, 10 milliseconds for process\nP2, 39 milliseconds for process P3, 42 milliseconds for process P4, and 49\nmilliseconds for process P5. Thus, the average waiting time is (0 + 10 + 39\n+ 42 + 49)/5 = 28 milliseconds.\nWith nonpreemptive SJF scheduling, we execute the processes as\nP5\nP2\nP3\nP4\n61\n32\n20\n10\n0\n3\nP1\nThe waiting time is 10 milliseconds for process P1, 32 milliseconds for process\nP2, 0 milliseconds for process P3, 3 milliseconds for process P4, and 20",
  "milliseconds for process P5. Thus, the average waiting time is (10 + 32 + 0\n+ 3 + 20)/5 = 13 milliseconds.\nWith the RR algorithm, we execute the processes as\nP5\nP5\nP2\nP2\nP2\nP3\nP4\n61\n30\n40\n50 52\n20\n23\n10\n0\nP1\nThe waiting time is 0 milliseconds for process P1, 32 milliseconds for process\nP2, 20 milliseconds for process P3, 23 milliseconds for process P4, and 40\nmilliseconds for process P5. Thus, the average waiting time is (0 + 32 + 20\n+ 23 + 40)/5 = 23 milliseconds.\nWe can see that, in this case, the average waiting time obtained with the SJF\npolicy is less than half that obtained with FCFS scheduling; the RR algorithm\ngives us an intermediate value.\nDeterministic modeling is simple and fast. It gives us exact numbers,",
  "allowing us to compare the algorithms. However, it requires exact numbers for\ninput, and its answers apply only to those cases. The main uses of deterministic\nmodeling are in describing scheduling algorithms and providing examples. In\ncases where we are running the same program over and over again and can\nmeasure the program’s processing requirements exactly, we may be able to use\ndeterministic modeling to select a scheduling algorithm. Furthermore, over a\nset of examples, deterministic modeling may indicate trends that can then be\nanalyzed and proved separately. For example, it can be shown that, for the\nenvironment described (all processes and their times available at time 0), the\nSJF policy will always result in the minimum waiting time.\n6.8.2\nQueueing Models",
  "6.8.2\nQueueing Models\nOn many systems, the processes that are run vary from day to day, so there\nis no static set of processes (or times) to use for deterministic modeling. What\ncan be determined, however, is the distribution of CPU and I/O bursts. These\ndistributions can be measured and then approximated or simply estimated. The\nresult is a mathematical formula describing the probability of a particular CPU\nburst. Commonly, this distribution is exponential and is described by its mean.\nSimilarly, we can describe the distribution of times when processes arrive in\nthe system (the arrival-time distribution). From these two distributions, it is 302\nChapter 6\nCPU Scheduling\npossible to compute the average throughput, utilization, waiting time, and so\non for most algorithms.",
  "on for most algorithms.\nThe computer system is described as a network of servers. Each server has\na queue of waiting processes. The CPU is a server with its ready queue, as is\nthe I/O system with its device queues. Knowing arrival rates and service rates,\nwe can compute utilization, average queue length, average wait time, and so\non. This area of study is called queueing-network analysis.\nAs an example, let n be the average queue length (excluding the process\nbeing serviced), let W be the average waiting time in the queue, and let % be\nthe average arrival rate for new processes in the queue (such as three processes\nper second). We expect that during the time W that a process waits, % × W\nnew processes will arrive in the queue. If the system is in a steady state, then",
  "the number of processes leaving the queue must be equal to the number of\nprocesses that arrive. Thus,\nn = % × W.\nThis equation, known as Little’s formula, is particularly useful because it is\nvalid for any scheduling algorithm and arrival distribution.\nWe can use Little’s formula to compute one of the three variables if we\nknow the other two. For example, if we know that 7 processes arrive every\nsecond (on average) and that there are normally 14 processes in the queue,\nthen we can compute the average waiting time per process as 2 seconds.\nQueueing analysis can be useful in comparing scheduling algorithms,\nbut it also has limitations. At the moment, the classes of algorithms and\ndistributions that can be handled are fairly limited. The mathematics of",
  "complicated algorithms and distributions can be difﬁcult to work with. Thus,\narrival and service distributions are often deﬁned in mathematically tractable\n—but unrealistic—ways. It is also generally necessary to make a number of\nindependent assumptions, which may not be accurate. As a result of these\ndifﬁculties, queueing models are often only approximations of real systems,\nand the accuracy of the computed results may be questionable.\n6.8.3\nSimulations\nTo get a more accurate evaluation of scheduling algorithms, we can use\nsimulations. Running simulations involves programming a model of the\ncomputer system. Software data structures represent the major components\nof the system. The simulator has a variable representing a clock. As this",
  "variable’s value is increased, the simulator modiﬁes the system state to reﬂect\nthe activities of the devices, the processes, and the scheduler. As the simulation\nexecutes, statistics that indicate algorithm performance are gathered and\nprinted.\nThe data to drive the simulation can be generated in several ways. The\nmost common method uses a random-number generator that is programmed to\ngenerate processes, CPU burst times, arrivals, departures, and so on, according\nto probability distributions. The distributions can be deﬁned mathematically\n(uniform, exponential, Poisson) or empirically. If a distribution is to be deﬁned\nempirically, measurements of the actual system under study are taken. The\nresults deﬁne the distribution of events in the real system; this distribution can",
  "then be used to drive the simulation. 6.8\nAlgorithm Evaluation\n303\nactual\nprocess\nexecution\nperformance\nstatistics\nfor FCFS\nsimulation\nFCFS\nperformance\nstatistics\nfor SJF\nperformance\nstatistics\nfor RR (q $ 14)\ntrace tape\nsimulation\nSJF\nsimulation\nRR (q $ 14)\n• • •\nCPU   10\nI/O    213 \nCPU   12 \nI/O    112 \nCPU     2 \nI/O    147 \nCPU 173\n• • •\nFigure 6.25\nEvaluation of CPU schedulers by simulation.\nA distribution-driven simulation may be inaccurate, however, because of\nrelationships between successive events in the real system. The frequency\ndistribution indicates only how many instances of each event occur; it does not\nindicate anything about the order of their occurrence. To correct this problem,\nwe can use trace tapes. We create a trace tape by monitoring the real system and",
  "recording the sequence of actual events (Figure 6.25). We then use this sequence\nto drive the simulation. Trace tapes provide an excellent way to compare two\nalgorithms on exactly the same set of real inputs. This method can produce\naccurate results for its inputs.\nSimulations can be expensive, often requiring hours of computer time. A\nmore detailed simulation provides more accurate results, but it also takes more\ncomputer time. In addition, trace tapes can require large amounts of storage\nspace. Finally, the design, coding, and debugging of the simulator can be a\nmajor task.\n6.8.4\nImplementation\nEven a simulation is of limited accuracy. The only completely accurate way\nto evaluate a scheduling algorithm is to code it up, put it in the operating",
  "system, and see how it works. This approach puts the actual algorithm in the\nreal system for evaluation under real operating conditions.\nThe major difﬁculty with this approach is the high cost. The expense is\nincurred not only in coding the algorithm and modifying the operating system\nto support it (along with its required data structures) but also in the reaction\nof the users to a constantly changing operating system. Most users are not\ninterested in building a better operating system; they merely want to get their\nprocesses executed and use their results. A constantly changing operating\nsystem does not help the users to get their work done.\nAnother difﬁculty is that the environment in which the algorithm is used",
  "will change. The environment will change not only in the usual way, as new 304\nChapter 6\nCPU Scheduling\nprograms are written and the types of problems change, but also as a result\nof the performance of the scheduler. If short processes are given priority, then\nusers may break larger processes into sets of smaller processes. If interactive\nprocesses are given priority over noninteractive processes, then users may\nswitch to interactive use.\nFor example, researchers designed one system that classiﬁed interactive\nand noninteractive processes automatically by looking at the amount of\nterminal I/O. If a process did not input or output to the terminal in a 1-second\ninterval, the process was classiﬁed as noninteractive and was moved to a",
  "lower-priority queue. In response to this policy, one programmer modiﬁed his\nprograms to write an arbitrary character to the terminal at regular intervals of\nless than 1 second. The system gave his programs a high priority, even though\nthe terminal output was completely meaningless.\nThe most ﬂexible scheduling algorithms are those that can be altered\nby the system managers or by the users so that they can be tuned for\na speciﬁc application or set of applications. A workstation that performs\nhigh-end graphical applications, for instance, may have scheduling needs\ndifferent from those of a Web server or ﬁle server. Some operating systems—\nparticularly several versions of UNIX—allow the system manager to ﬁne-tune\nthe scheduling parameters for a particular system conﬁguration. For example,",
  "Solaris provides the dispadmin command to allow the system administrator\nto modify the parameters of the scheduling classes described in Section 6.7.3.\nAnother approach is to use APIs that can modify the priority of a process\nor thread. The Java, POSIX, and Windows API provide such functions. The\ndownfall of this approach is that performance-tuning a system or application\nmost often does not result in improved performance in more general situations.\n6.9\nSummary\nCPU scheduling is the task of selecting a waiting process from the ready queue\nand allocating the CPU to it. The CPU is allocated to the selected process by the\ndispatcher.\nFirst-come, ﬁrst-served (FCFS) scheduling is the simplest scheduling algo-\nrithm, but it can cause short processes to wait for very long processes. Shortest-",
  "job-ﬁrst (SJF) scheduling is provably optimal, providing the shortest average\nwaiting time. Implementing SJF scheduling is difﬁcult, however, because pre-\ndicting the length of the next CPU burst is difﬁcult. The SJF algorithm is a special\ncase of the general priority scheduling algorithm, which simply allocates the\nCPU to the highest-priority process. Both priority and SJF scheduling may suffer\nfrom starvation. Aging is a technique to prevent starvation.\nRound-robin (RR) scheduling is more appropriate for a time-shared (inter-\nactive) system. RR scheduling allocates the CPU to the ﬁrst process in the ready\nqueue for q time units, where q is the time quantum. After q time units, if\nthe process has not relinquished the CPU, it is preempted, and the process is",
  "put at the tail of the ready queue. The major problem is the selection of the\ntime quantum. If the quantum is too large, RR scheduling degenerates to FCFS\nscheduling. If the quantum is too small, scheduling overhead in the form of\ncontext-switch time becomes excessive. Practice Exercises\n305\nThe FCFS algorithm is nonpreemptive; the RR algorithm is preemptive. The\nSJF and priority algorithms may be either preemptive or nonpreemptive.\nMultilevel queue algorithms allow different algorithms to be used for\ndifferent classes of processes. The most common model includes a foreground\ninteractive queue that uses RR scheduling and a background batch queue that\nuses FCFS scheduling. Multilevel feedback queues allow processes to move\nfrom one queue to another.",
  "from one queue to another.\nMany contemporary computer systems support multiple processors and\nallow each processor to schedule itself independently. Typically, each processor\nmaintains its own private queue of processes (or threads), all of which are\navailable to run. Additional issues related to multiprocessor scheduling include\nprocessor afﬁnity, load balancing, and multicore processing.\nA real-time computer system requires that results arrive within a deadline\nperiod; results arriving after the deadline has passed are useless. Hard real-time\nsystems must guarantee that real-time tasks are serviced within their deadline\nperiods. Soft real-time systems are less restrictive, assigning real-time tasks\nhigher scheduling priority than other tasks.",
  "higher scheduling priority than other tasks.\nReal-time scheduling algorithms include rate-monotonic and earliest-\ndeadline-ﬁrst scheduling. Rate-monotonic scheduling assigns tasks that\nrequire the CPU more often a higher priority than tasks that require the\nCPU less often. Earliest-deadline-ﬁrst scheduling assigns priority according\nto upcoming deadlines—the earlier the deadline, the higher the priority.\nProportional share scheduling divides up processor time into shares and\nassigning each process a number of shares, thus guaranteeing each process\na proportional share of CPU time. The POSIX Pthread API provides various\nfeatures for scheduling real-time threads as well.\nOperating systems supporting threads at the kernel level must schedule",
  "threads—not processes—for execution. This is the case with Solaris and\nWindows. Both of these systems schedule threads using preemptive, priority-\nbased scheduling algorithms, including support for real-time threads. The\nLinux process scheduler uses a priority-based algorithm with real-time support\nas well. The scheduling algorithms for these three operating systems typically\nfavor interactive over CPU-bound processes.\nThe wide variety of scheduling algorithms demands that we have methods\nto select among algorithms. Analytic methods use mathematical analysis to\ndetermine the performance of an algorithm. Simulation methods determine\nperformance by imitating the scheduling algorithm on a “representative”\nsample of processes and computing the resulting performance. However,",
  "simulation can at best provide an approximation of actual system performance.\nThe only reliable technique for evaluating a scheduling algorithm is to\nimplement the algorithm on an actual system and monitor its performance\nin a “real-world” environment.\nPractice Exercises\n6.1\nA CPU-scheduling algorithm determines an order for the execution\nof its scheduled processes. Given n processes to be scheduled on one\nprocessor, how many different schedules are possible? Give a formula\nin terms of n. 306\nChapter 6\nCPU Scheduling\n6.2\nExplain the difference between preemptive and nonpreemptive schedul-\ning.\n6.3\nSuppose that the following processes arrive for execution at the times\nindicated. Each process will run for the amount of time listed. In",
  "answering the questions, use nonpreemptive scheduling, and base all\ndecisions on the information you have at the time the decision must be\nmade.\nProcess\nArrival Time\nBurst Time\nP1\n0.0\n8\nP2\n0.4\n4\nP3\n1.0\n1\na.\nWhat is the average turnaround time for these processes with the\nFCFS scheduling algorithm?\nb.\nWhat is the average turnaround time for these processes with the\nSJF scheduling algorithm?\nc.\nThe SJF algorithm is supposed to improve performance, but notice\nthat we chose to run process P1 at time 0 because we did not know\nthat two shorter processes would arrive soon. Compute what the\naverage turnaround time will be if the CPU is left idle for the ﬁrst\n1 unit and then SJF scheduling is used. Remember that processes\nP1 and P2 are waiting during this idle time, so their waiting time",
  "may increase. This algorithm could be called future-knowledge\nscheduling.\n6.4\nWhat advantage is there in having different time-quantum sizes at\ndifferent levels of a multilevel queueing system?\n6.5\nMany CPU-scheduling algorithms are parameterized. For example, the\nRR algorithm requires a parameter to indicate the time slice. Multilevel\nfeedback queues require parameters to deﬁne the number of queues, the\nscheduling algorithm for each queue, the criteria used to move processes\nbetween queues, and so on.\nThese algorithms are thus really sets of algorithms (for example, the\nset of RR algorithms for all time slices, and so on). One set of algorithms\nmay include another (for example, the FCFSalgorithm is the RR algorithm\nwith an inﬁnite time quantum). What (if any) relation holds between the",
  "following pairs of algorithm sets?\na.\nPriority and SJF\nb.\nMultilevel feedback queues and FCFS\nc.\nPriority and FCFS\nd.\nRR and SJF\n6.6\nSuppose that a scheduling algorithm (at the level of short-term CPU\nscheduling) favors those processes that have used the least processor Exercises\n307\ntime in the recent past. Why will this algorithm favor I/O-bound\nprograms and yet not permanently starve CPU-bound programs?\n6.7\nDistinguish between PCS and SCS scheduling.\n6.8\nAssume that an operating system maps user-level threads to the kernel\nusing the many-to-many model and that the mapping is done through\nthe use of LWPs. Furthermore, the system allows program developers to\ncreate real-time threads. Is it necessary to bind a real-time thread to an\nLWP?\n6.9",
  "LWP?\n6.9\nThe traditional UNIX scheduler enforces an inverse relationship between\npriority numbers and priorities: the higher the number, the lower the\npriority. The scheduler recalculates process priorities once per second\nusing the following function:\nPriority = (recent CPU usage / 2) + base\nwhere base = 60 and recent CPU usage refers to a value indicating how\noften a process has used the CPU since priorities were last recalculated.\nAssume that recent CPU usage is 40 for process P1, 18 for process P2,\nand 10 for process P3. What will be the new priorities for these three\nprocesses when priorities are recalculated? Based on this information,\ndoes the traditional UNIX scheduler raise or lower the relative priority\nof a CPU-bound process?\nExercises\n6.10",
  "of a CPU-bound process?\nExercises\n6.10\nWhy is it important for the scheduler to distinguish I/O-bound programs\nfrom CPU-bound programs?\n6.11\nDiscuss how the following pairs of scheduling criteria conﬂict in certain\nsettings.\na.\nCPU utilization and response time\nb.\nAverage turnaround time and maximum waiting time\nc.\nI/O device utilization and CPU utilization\n6.12\nOne technique for implementing lottery scheduling works by assigning\nprocesses lottery tickets, which are used for allocating CPU time. When-\never a scheduling decision has to be made, a lottery ticket is chosen\nat random, and the process holding that ticket gets the CPU. The BTV\noperating system implements lottery scheduling by holding a lottery\n50 times each second, with each lottery winner getting 20 milliseconds",
  "of CPU time (20 milliseconds × 50 = 1 second). Describe how the BTV\nscheduler can ensure that higher-priority threads receive more attention\nfrom the CPU than lower-priority threads.\n6.13\nIn Chapter 5, we discussed possible race conditions on various kernel\ndata structures. Most scheduling algorithms maintain a run queue,\nwhich lists processes eligible to run on a processor. On multicore systems,\nthere are two general options: (1) each processing core has its own run 308\nChapter 6\nCPU Scheduling\nqueue, or (2) a single run queue is shared by all processing cores. What\nare the advantages and disadvantages of each of these approaches?\n6.14\nConsider the exponential average formula used to predict the length of\nthe next CPU burst. What are the implications of assigning the following",
  "values to the parameters used by the algorithm?\na.\n# = 0 and \"0 = 100 milliseconds\nb.\n# = 0.99 and \"0 = 10 milliseconds\n6.15\nA variation of the round-robin scheduler is the regressive round-robin\nscheduler. This scheduler assigns each process a time quantum and a\npriority. The initial value of a time quantum is 50 milliseconds. However,\nevery time a process has been allocated the CPU and uses its entire time\nquantum (does not block for I/O), 10 milliseconds is added to its time\nquantum, and its priority level is boosted. (The time quantum for a\nprocess can be increased to a maximum of 100 milliseconds.) When a\nprocess blocks before using its entire time quantum, its time quantum is\nreduced by 5 milliseconds, but its priority remains the same. What type",
  "of process (CPU-bound or I/O-bound) does the regressive round-robin\nscheduler favor? Explain.\n6.16\nConsider the following set of processes, with the length of the CPU burst\ngiven in milliseconds:\nProcess\nBurst Time\nPriority\nP1\n2\n2\nP2\n1\n1\nP3\n8\n4\nP4\n4\n2\nP5\n5\n3\nThe processes are assumed to have arrived in the order P1, P2, P3, P4, P5,\nall at time 0.\na.\nDraw four Gantt charts that illustrate the execution of these\nprocesses using the following scheduling algorithms: FCFS, SJF,\nnonpreemptive priority (a larger priority number implies a higher\npriority), and RR (quantum = 2).\nb.\nWhat is the turnaround time of each process for each of the\nscheduling algorithms in part a?\nc.\nWhat is the waiting time of each process for each of these schedul-\ning algorithms?\nd.",
  "ing algorithms?\nd.\nWhich of the algorithms results in the minimum average waiting\ntime (over all processes)?\n6.17\nThe following processes are being scheduled using a preemptive, round-\nrobin scheduling algorithm. Each process is assigned a numerical\npriority, with a higher number indicating a higher relative priority.\nIn addition to the processes listed below, the system also has an idle Exercises\n309\ntask (which consumes no CPU resources and is identiﬁed as Pidle). This\ntask has priority 0 and is scheduled whenever the system has no other\navailable processes to run. The length of a time quantum is 10 units.\nIf a process is preempted by a higher-priority process, the preempted\nprocess is placed at the end of the queue.\nThread\nPriority\nBurst\nArrival\nP1\n40\n20\n0\nP2\n30\n25\n25\nP3\n30\n25\n30\nP4\n35",
  "Arrival\nP1\n40\n20\n0\nP2\n30\n25\n25\nP3\n30\n25\n30\nP4\n35\n15\n60\nP5\n5\n10\n100\nP6\n10\n10\n105\na.\nShow the scheduling order of the processes using a Gantt chart.\nb.\nWhat is the turnaround time for each process?\nc.\nWhat is the waiting time for each process?\nd.\nWhat is the CPU utilization rate?\n6.18\nThe nice command is used to set the nice value of a process on Linux,\nas well as on other UNIX systems. Explain why some systems may allow\nany user to assign a process a nice value >= 0 yet allow only the root\nuser to assign nice values < 0.\n6.19\nWhich of the following scheduling algorithms could result in starvation?\na.\nFirst-come, ﬁrst-served\nb.\nShortest job ﬁrst\nc.\nRound robin\nd.\nPriority\n6.20\nConsider a variant of the RR scheduling algorithm in which the entries\nin the ready queue are pointers to the PCBs.",
  "in the ready queue are pointers to the PCBs.\na.\nWhat would be the effect of putting two pointers to the same\nprocess in the ready queue?\nb.\nWhat would be two major advantages and two disadvantages of\nthis scheme?\nc.\nHow would you modify the basic RR algorithm to achieve the same\neffect without the duplicate pointers?\n6.21\nConsider a system running ten I/O-bound tasks and one CPU-bound\ntask. Assume that the I/O-bound tasks issue an I/O operation once for\nevery millisecond of CPU computing and that each I/O operation takes\n10 milliseconds to complete. Also assume that the context-switching\noverhead is 0.1 millisecond and that all processes are long-running tasks.\nDescribe the CPU utilization for a round-robin scheduler when: 310\nChapter 6\nCPU Scheduling\na.\nThe time quantum is 1 millisecond",
  "a.\nThe time quantum is 1 millisecond\nb.\nThe time quantum is 10 milliseconds\n6.22\nConsider a system implementing multilevel queue scheduling. What\nstrategy can a computer user employ to maximize the amount of CPU\ntime allocated to the user’s process?\n6.23\nConsider a preemptive priority scheduling algorithm based on dynami-\ncally changing priorities. Larger priority numbers imply higher priority.\nWhen a process is waiting for the CPU (in the ready queue, but not\nrunning), its priority changes at a rate #. When it is running, its priority\nchanges at a rate &. All processes are given a priority of 0 when they\nenter the ready queue. The parameters # and & can be set to give many\ndifferent scheduling algorithms.\na.\nWhat is the algorithm that results from & > # > 0?\nb.",
  "b.\nWhat is the algorithm that results from # < & < 0?\n6.24\nExplain the differences in how much the following scheduling algo-\nrithms discriminate in favor of short processes:\na.\nFCFS\nb.\nRR\nc.\nMultilevel feedback queues\n6.25\nUsing the Windows scheduling algorithm, determine the numeric\npriority of each of the following threads.\na.\nA thread in the REALTIME PRIORITY CLASS with a relative priority\nof NORMAL\nb.\nA thread in the ABOVE NORMAL PRIORITY CLASS with a relative\npriority of HIGHEST\nc.\nA thread in the BELOW NORMAL PRIORITY CLASS with a relative\npriority of ABOVE NORMAL\n6.26\nAssuming that no threads belong to the REALTIME PRIORITY CLASS and\nthat none may be assigned a TIME CRITICAL priority, what combination\nof priority class and priority corresponds to the highest possible relative",
  "priority in Windows scheduling?\n6.27\nConsider the scheduling algorithm in the Solaris operating system for\ntime-sharing threads.\na.\nWhat is the time quantum (in milliseconds) for a thread with\npriority 15? With priority 40?\nb.\nAssume that a thread with priority 50 has used its entire time\nquantum without blocking. What new priority will the scheduler\nassign this thread?\nc.\nAssume that a thread with priority 20 blocks for I/O before its time\nquantum has expired. What new priority will the scheduler assign\nthis thread? Bibliographical Notes\n311\n6.28\nAssume that two tasks Aand B are running on a Linux system. The nice\nvalues of Aand B are −5 and +5, respectively. Using the CFS scheduler as\na guide, describe how the respective values of vruntime vary between",
  "the two processes given each of the following scenarios:\n• Both A and B are CPU-bound.\n• A is I/O-bound, and B is CPU-bound.\n• A is CPU-bound, and B is I/O-bound.\n6.29\nDiscuss ways in which the priority inversion problem could be\naddressed in a real-time system. Also discuss whether the solutions\ncould be implemented within the context of a proportional share sched-\nuler.\n6.30\nUnder what circumstances is rate-monotonic scheduling inferior to\nearliest-deadline-ﬁrst scheduling in meeting the deadlines associated\nwith processes?\n6.31\nConsider two processes, P1 and P2, where p1 = 50, t1 = 25, p2 = 75, and\nt2 = 30.\na.\nCan these two processes be scheduled using rate-monotonic\nscheduling? Illustrate your answer using a Gantt chart such as\nthe ones in Figure 6.16–Figure 6.19.\nb.",
  "the ones in Figure 6.16–Figure 6.19.\nb.\nIllustrate the scheduling of these two processes using earliest-\ndeadline-ﬁrst (EDF) scheduling.\n6.32\nExplain why interrupt and dispatch latency times must be bounded in\na hard real-time system.\nBibliographical Notes\nFeedback queues were originally implemented on the CTSS system described in\n[Corbato et al. (1962)]. This feedback queue scheduling system was analyzed by\n[Schrage (1967)]. The preemptive priority scheduling algorithm of Exercise 6.23\nwas suggested by [Kleinrock (1975)]. The scheduling algorithms for hard real-\ntime systems, such as rate monotonic scheduling and earliest-deadline-ﬁrst\nscheduling, are presented in [Liu and Layland (1973)].\n[Anderson et al. (1989)], [Lewis and Berg (1998)], and [Philbin et al. (1996)]",
  "discuss thread scheduling. Multicore scheduling is examined in [McNairy and\nBhatia (2005)] and [Kongetira et al. (2005)].\n[Fisher (1981)], [Hall et al. (1996)], and [Lowney et al. (1993)] describe\nscheduling techniques that take into account information regarding process\nexecution times from previous runs.\nFair-share schedulers are covered by [Henry (1984)], [Woodside (1986)],\nand [Kay and Lauder (1988)].\nScheduling policies used in the UNIX V operating system are described\nby [Bach (1987)]; those for UNIX FreeBSD 5.2 are presented by [McKusick and\nNeville-Neil (2005)]; and those for the Mach operating system are discussed\nby [Black (1990)]. [Love (2010)] and [Mauerer (2008)] cover scheduling in 312\nChapter 6\nCPU Scheduling",
  "Chapter 6\nCPU Scheduling\nLinux. [Faggioli et al. (2009)] discuss adding an EDF scheduler to the Linux\nkernel. Details of the ULE scheduler can be found in [Roberson (2003)]. Solaris\nscheduling is described by [Mauro and McDougall (2007)]. [Russinovich and\nSolomon (2009)] discusses scheduling in Windows internals. [Butenhof (1997)]\nand [Lewis and Berg (1998)] describe scheduling in Pthreads systems. [Siddha\net al. (2007)] discuss scheduling challenges on multicore systems.\nBibliography\n[Anderson et al. (1989)]\nT. E. Anderson, E. D. Lazowska, and H. M. Levy,\n“The Performance Implications of Thread Management Alternatives for\nShared-Memory Multiprocessors”, IEEE Transactions on Computers, Volume 38,\nNumber 12 (1989), pages 1631–1644.\n[Bach (1987)]",
  "Number 12 (1989), pages 1631–1644.\n[Bach (1987)]\nM. J. Bach, The Design of the UNIX Operating System, Prentice Hall\n(1987).\n[Black (1990)]\nD. L. Black, “Scheduling Support for Concurrency and Parallelism\nin the Mach Operating System”, IEEE Computer, Volume 23, Number 5 (1990),\npages 35–43.\n[Butenhof (1997)]\nD. Butenhof, Programming with POSIX Threads, Addison-\nWesley (1997).\n[Corbato et al. (1962)]\nF. J. Corbato, M. Merwin-Daggett, and R. C. Daley, “An\nExperimental Time-Sharing System”, Proceedings of the AFIPS Fall Joint Computer\nConference (1962), pages 335–344.\n[Faggioli et al. (2009)]\nD. Faggioli, F. Checconi, M. Trimarchi, and C. Scordino,\n“An EDF scheduling class for the Linux kernel”, Proceedings of the 11th Real-Time\nLinux Workshop (2009).\n[Fisher (1981)]",
  "Linux Workshop (2009).\n[Fisher (1981)]\nJ. A. Fisher, “Trace Scheduling: A Technique for Global Microcode\nCompaction”, IEEE Transactions on Computers, Volume 30, Number 7 (1981),\npages 478–490.\n[Hall et al. (1996)]\nL. Hall, D. Shmoys, and J. Wein, “Scheduling To Minimize\nAverage Completion Time: Off-line and On-line Algorithms”, SODA: ACM-\nSIAM Symposium on Discrete Algorithms (1996).\n[Henry (1984)]\nG. Henry, “The Fair Share Scheduler”, AT&T Bell Laboratories\nTechnical Journal (1984).\n[Kay and Lauder (1988)]\nJ. Kay and P. Lauder, “A Fair Share Scheduler”, Com-\nmunications of the ACM, Volume 31, Number 1 (1988), pages 44–55.\n[Kleinrock (1975)]\nL. Kleinrock, Queueing Systems, Volume II: Computer Applica-\ntions, Wiley-Interscience (1975).\n[Kongetira et al. (2005)]",
  "[Kongetira et al. (2005)]\nP. Kongetira, K. Aingaran, and K. Olukotun, “Niagara:\nA 32-Way Multithreaded SPARC Processor”, IEEE Micro Magazine, Volume 25,\nNumber 2 (2005), pages 21–29. Bibliography\n313\n[Lewis and Berg (1998)]\nB. Lewis and D. Berg, Multithreaded Programming with\nPthreads, Sun Microsystems Press (1998).\n[Liu and Layland (1973)]\nC. L. Liu and J. W. Layland, “Scheduling Algorithms\nfor Multiprogramming in a Hard Real-Time Environment”, Communications of\nthe ACM, Volume 20, Number 1 (1973), pages 46–61.\n[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[Lowney et al. (1993)]\nP. G. Lowney, S. M. Freudenberger, T. J. Karzes, W. D.\nLichtenstein, R. P. Nix, J. S. O’Donnell, and J. C. Ruttenberg, “The Multiﬂow",
  "Trace Scheduling Compiler”, Journal of Supercomputing, Volume 7, Number 1-2\n(1993), pages 51–142.\n[Mauerer (2008)]\nW. Mauerer, Professional Linux Kernel Architecture, John Wiley\nand Sons (2008).\n[Mauro and McDougall (2007)]\nJ. Mauro and R. McDougall, Solaris Internals:\nCore Kernel Architecture, Prentice Hall (2007).\n[McKusick and Neville-Neil (2005)]\nM. K. McKusick and G. V. Neville-Neil,\nThe Design and Implementation of the FreeBSD UNIX Operating System, Addison\nWesley (2005).\n[McNairy and Bhatia (2005)]\nC. McNairy and R. Bhatia, “Montecito: A Dual–\nCore, Dual-Threaded Itanium Processor”, IEEE Micro Magazine, Volume 25,\nNumber 2 (2005), pages 10–20.\n[Philbin et al. (1996)]\nJ. Philbin, J. Edler, O. J. Anshus, C. C. Douglas, and K. Li,",
  "“Thread Scheduling for Cache Locality”, Architectural Support for Programming\nLanguages and Operating Systems (1996), pages 60–71.\n[Roberson (2003)]\nJ. Roberson, “ULE: A Modern Scheduler For FreeBSD”,\nProceedings of the USENIX BSDCon Conference (2003), pages 17–28.\n[Russinovich and Solomon (2009)]\nM. E. Russinovich and D. A. Solomon, Win-\ndows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition,\nMicrosoft Press (2009).\n[Schrage (1967)]\nL. E. Schrage, “The Queue M/G/I with Feedback to Lower\nPriority Queues”, Management Science, Volume 13, (1967), pages 466–474.\n[Siddha et al. (2007)]\nS. Siddha, V. Pallipadi, and A. Mallick, “Process Schedul-\ning Challenges in the Era of Multi-Core Processors”, Intel Technology Journal,\nVolume 11, Number 4 (2007).\n[Woodside (1986)]",
  "Volume 11, Number 4 (2007).\n[Woodside (1986)]\nC. Woodside, “Controllability of Computer Performance\nTradeoffs Obtained Using Controlled-Share Queue Schedulers”, IEEE Transac-\ntions on Software Engineering, Volume SE-12, Number 10 (1986), pages 1041–1048.  7\nC H A P T E R\nDeadlocks\nIn a multiprogramming environment, several processes may compete for a\nﬁnite number of resources. A process requests resources; if the resources are\nnot available at that time, the process enters a waiting state. Sometimes, a\nwaiting process is never again able to change state, because the resources it\nhas requested are held by other waiting processes. This situation is called\na deadlock. We discussed this issue brieﬂy in Chapter 5 in connection with\nsemaphores.",
  "semaphores.\nPerhaps the best illustration of a deadlock can be drawn from a law passed\nby the Kansas legislature early in the 20th century. It said, in part: “When two\ntrains approach each other at a crossing, both shall come to a full stop and\nneither shall start up again until the other has gone.”\nIn this chapter, we describe methods that an operating system can use\nto prevent or deal with deadlocks. Although some applications can identify\nprograms that may deadlock, operating systems typically do not provide\ndeadlock-prevention facilities, and it remains the responsibility of program-\nmers to ensure that they design deadlock-free programs. Deadlock problems\ncan only become more common, given current trends, including larger num-",
  "bers of processes, multithreaded programs, many more resources within a\nsystem, and an emphasis on long-lived ﬁle and database servers rather than\nbatch systems.\nCHAPTER OBJECTIVES\n• To develop a description of deadlocks, which prevent sets of concurrent\nprocesses from completing their tasks.\n• To present a number of different methods for preventing or avoiding\ndeadlocks in a computer system.\n7.1\nSystem Model\nA system consists of a ﬁnite number of resources to be distributed among a\nnumber of competing processes. The resources may be partitioned into several\n315 316\nChapter 7\nDeadlocks\ntypes (or classes), each consisting of some number of identical instances. CPU\ncycles, ﬁles, and I/O devices (such as printers and DVD drives) are examples of",
  "resource types. If a system has two CPUs, then the resource type CPU has two\ninstances. Similarly, the resource type printer may have ﬁve instances.\nIf a process requests an instance of a resource type, the allocation of any\ninstance of the type should satisfy the request. If it does not, then the instances\nare not identical, and the resource type classes have not been deﬁned properly.\nFor example, a system may have two printers. These two printers may be\ndeﬁned to be in the same resource class if no one cares which printer prints\nwhich output. However, if one printer is on the ninth ﬂoor and the other is\nin the basement, then people on the ninth ﬂoor may not see both printers\nas equivalent, and separate resource classes may need to be deﬁned for each\nprinter.",
  "printer.\nChapter 5 discussed various synchronization tools, such as mutex locks\nand semaphores. These tools are also considered system resources, and they\nare a common source of deadlock. However, a lock is typically associated with\nprotecting a speciﬁc data structure—that is, one lock may be used to protect\naccess to a queue, another to protect access to a linked list, and so forth. For that\nreason, each lock is typically assigned its own resource class, and deﬁnition is\nnot a problem.\nA process must request a resource before using it and must release the\nresource after using it. A process may request as many resources as it requires\nto carry out its designated task. Obviously, the number of resources requested",
  "may not exceed the total number of resources available in the system. In other\nwords, a process cannot request three printers if the system has only two.\nUnder the normal mode of operation, a process may utilize a resource in\nonly the following sequence:\n1. Request. The process requests the resource. If the request cannot be\ngranted immediately (for example, if the resource is being used by another\nprocess), then the requesting process must wait until it can acquire the\nresource.\n2. Use. The process can operate on the resource (for example, if the resource\nis a printer, the process can print on the printer).\n3. Release. The process releases the resource.\nThe request and release of resources may be system calls, as explained in",
  "Chapter 2. Examples are the request() and release() device, open() and\nclose() ﬁle, and allocate() and free() memory system calls. Similarly,\nas we saw in Chapter 5, the request and release of semaphores can be\naccomplished through the wait() and signal() operations on semaphores\nor through acquire() and release() of a mutex lock. For each use of a\nkernel-managed resource by a process or thread, the operating system checks\nto make sure that the process has requested and has been allocated the resource.\nA system table records whether each resource is free or allocated. For each\nresource that is allocated, the table also records the process to which it is\nallocated. If a process requests a resource that is currently allocated to another",
  "process, it can be added to a queue of processes waiting for this resource.\nA set of processes is in a deadlocked state when every process in the set is\nwaiting for an event that can be caused only by another process in the set. The 7.2\nDeadlock Characterization\n317\nevents with which we are mainly concerned here are resource acquisition and\nrelease. The resources may be either physical resources (for example, printers,\ntape drives, memory space, and CPU cycles) or logical resources (for example,\nsemaphores, mutex locks, and ﬁles). However, other types of events may result\nin deadlocks (for example, the IPC facilities discussed in Chapter 3).\nTo illustrate a deadlocked state, consider a system with three CD RW drives.",
  "Suppose each of three processes holds one of these CDRW drives. If each process\nnow requests another drive, the three processes will be in a deadlocked state.\nEach is waiting for the event “CD RW is released,” which can be caused only\nby one of the other waiting processes. This example illustrates a deadlock\ninvolving the same resource type.\nDeadlocks may also involve different resource types. For example, consider\na system with one printer and one DVDdrive. Suppose that process Pi is holding\nthe DVD and process Pj is holding the printer. If Pi requests the printer and Pj\nrequests the DVD drive, a deadlock occurs.\nDevelopers of multithreaded applications must remain aware of the\npossibility of deadlocks. The locking tools presented in Chapter 5 are designed",
  "to avoid race conditions. However, in using these tools, developers must pay\ncareful attention to how locks are acquired and released. Otherwise, deadlock\ncan occur, as illustrated in the dining-philosophers problem in Section 5.7.3.\n7.2\nDeadlock Characterization\nIn a deadlock, processes never ﬁnish executing, and system resources are tied\nup, preventing other jobs from starting. Before we discuss the various methods\nfor dealing with the deadlock problem, we look more closely at features that\ncharacterize deadlocks.\nDEADLOCK WITH MUTEX LOCKS\nLet’s see how deadlock can occur in a multithreaded Pthread program\nusing\nmutex\nlocks.\nThe pthread mutex init()\nfunction\ninitializes\nan unlocked mutex. Mutex locks\nare acquired and released using\npthread mutex lock()\nand\npthread mutex unlock(),",
  "pthread mutex lock()\nand\npthread mutex unlock(),\nrespec-\ntively. If a thread attempts to acquire a locked mutex, the call to\npthread mutex lock() blocks the thread until the owner of the mutex\nlock invokes pthread mutex unlock().\nTwo mutex locks are created in the following code example:\n/* Create and initialize the mutex locks */\npthread mutex t first mutex;\npthread mutex t second mutex;\npthread mutex init(&first mutex,NULL);\npthread mutex init(&second mutex,NULL);\nNext, two threads—thread one and thread two—are created, and both\nthese threads have access to both mutex locks. thread one and thread two 318\nChapter 7\nDeadlocks\nDEADLOCK WITH MUTEX LOCKS (Continued)\nrun in the functions do work one() and do work two(), respectively, as\nshown below:\n/* thread one runs in this function */",
  "/* thread one runs in this function */\nvoid *do work one(void *param)\n{\npthread mutex lock(&first mutex);\npthread mutex lock(&second mutex);\n/**\n* Do some work\n*/\npthread mutex unlock(&second mutex);\npthread mutex unlock(&first mutex);\npthread exit(0);\n}\n/* thread two runs in this function */\nvoid *do work two(void *param)\n{\npthread mutex lock(&second mutex);\npthread mutex lock(&first mutex);\n/**\n* Do some work\n*/\npthread mutex unlock(&first mutex);\npthread mutex unlock(&second mutex);\npthread exit(0);\n}\nIn this example, thread one attempts to acquire the mutex locks in the order\n(1) first mutex, (2) second mutex, while thread two attempts to acquire\nthe mutex locks in the order (1) second mutex, (2) first mutex. Deadlock",
  "is possible if thread one acquires first mutex while thread two acquires\nsecond mutex.\nNote that, even though deadlock is possible, it will not occur if thread one\ncan acquire and release the mutex locks for first mutex and second mutex\nbefore thread two attempts to acquire the locks. And, of course, the order\nin which the threads run depends on how they are scheduled by the CPU\nscheduler. This example illustrates a problem with handling deadlocks: it is\ndifﬁcult to identify and test for deadlocks that may occur only under certain\nscheduling circumstances.\n7.2.1\nNecessary Conditions\nA deadlock situation can arise if the following four conditions hold simultane-\nously in a system: 7.2\nDeadlock Characterization\n319\n1. Mutual exclusion. At least one resource must be held in a nonsharable",
  "mode; that is, only one process at a time can use the resource. If another\nprocess requests that resource, the requesting process must be delayed\nuntil the resource has been released.\n2. Hold and wait. A process must be holding at least one resource and\nwaiting to acquire additional resources that are currently being held by\nother processes.\n3. No preemption. Resources cannot be preempted; that is, a resource can\nbe released only voluntarily by the process holding it, after that process\nhas completed its task.\n4. Circular wait. A set {P0, P1, ..., Pn} of waiting processes must exist such\nthat P0 is waiting for a resource held by P1, P1 is waiting for a resource\nheld by P2, ..., Pn−1 is waiting for a resource held by Pn, and Pn is waiting\nfor a resource held by P0.",
  "for a resource held by P0.\nWe emphasize that all four conditions must hold for a deadlock to\noccur. The circular-wait condition implies the hold-and-wait condition, so the\nfour conditions are not completely independent. We shall see in Section 7.4,\nhowever, that it is useful to consider each condition separately.\n7.2.2\nResource-Allocation Graph\nDeadlocks can be described more precisely in terms of a directed graph called\na system resource-allocation graph. This graph consists of a set of vertices V\nand a set of edges E. The set of vertices V is partitioned into two different types\nof nodes: P = {P1, P2, ..., Pn}, the set consisting of all the active processes in the\nsystem, and R = {R1, R2, ..., Rm}, the set consisting of all resource types in the\nsystem.",
  "system.\nA directed edge from process Pi to resource type Rj is denoted by Pi →Rj;\nit signiﬁes that process Pi has requested an instance of resource type Rj and\nis currently waiting for that resource. A directed edge from resource type Rj\nto process Pi is denoted by Rj →Pi; it signiﬁes that an instance of resource\ntype Rj has been allocated to process Pi. A directed edge Pi →Rj is called a\nrequest edge; a directed edge Rj →Pi is called an assignment edge.\nPictorially, we represent each process Pi as a circle and each resource type\nRj as a rectangle. Since resource type Rj may have more than one instance, we\nrepresent each such instance as a dot within the rectangle. Note that a request\nedge points to only the rectangle Rj, whereas an assignment edge must also",
  "designate one of the dots in the rectangle.\nWhen process Pi requests an instance of resource type Rj, a request edge\nis inserted in the resource-allocation graph. When this request can be fulﬁlled,\nthe request edge is instantaneously transformed to an assignment edge. When\nthe process no longer needs access to the resource, it releases the resource. As\na result, the assignment edge is deleted.\nThe resource-allocation graph shown in Figure 7.1 depicts the following\nsituation.\n• The sets P, R, and E:\n◦P = {P1, P2, P3} 320\nChapter 7\nDeadlocks\nR1\nR3\nR2\nR4\nP3\nP2\nP1\nFigure 7.1\nResource-allocation graph.\n◦R = {R1, R2, R3, R4}\n◦E = {P1 →R1, P2 →R3, R1 →P2, R2 →P2, R2 →P1, R3 →P3}\n• Resource instances:\n◦One instance of resource type R1\n◦Two instances of resource type R2",
  "◦Two instances of resource type R2\n◦One instance of resource type R3\n◦Three instances of resource type R4\n• Process states:\n◦Process P1 is holding an instance of resource type R2 and is waiting for\nan instance of resource type R1.\n◦Process P2 is holding an instance of R1 and an instance of R2 and is\nwaiting for an instance of R3.\n◦Process P3 is holding an instance of R3.\nGiven the deﬁnition of a resource-allocation graph, it can be shown that, if\nthe graph contains no cycles, then no process in the system is deadlocked. If\nthe graph does contain a cycle, then a deadlock may exist.\nIf each resource type has exactly one instance, then a cycle implies that a\ndeadlock has occurred. If the cycle involves only a set of resource types, each",
  "of which has only a single instance, then a deadlock has occurred. Each process\ninvolved in the cycle is deadlocked. In this case, a cycle in the graph is both a\nnecessary and a sufﬁcient condition for the existence of deadlock.\nIf each resource type has several instances, then a cycle does not necessarily\nimply that a deadlock has occurred. In this case, a cycle in the graph is a\nnecessary but not a sufﬁcient condition for the existence of deadlock.\nTo illustrate this concept, we return to the resource-allocation graph\ndepicted in Figure 7.1. Suppose that process P3 requests an instance of resource 7.2\nDeadlock Characterization\n321\nR1\nR3\nR2\nR4\nP3\nP2\nP1\nFigure 7.2\nResource-allocation graph with a deadlock.\ntype R2. Since no resource instance is currently available, we add a request edge",
  "P3 →R2 to the graph (Figure 7.2). At this point, two minimal cycles exist in the\nsystem:\nP1 →R1 →P2 →R3 →P3 →R2 →P1\nP2 →R3 →P3 →R2 →P2\nProcesses P1, P2, and P3 are deadlocked. Process P2 is waiting for the resource\nR3, which is held by process P3. Process P3 is waiting for either process P1 or\nprocess P2 to release resource R2. In addition, process P1 is waiting for process\nP2 to release resource R1.\nNow consider the resource-allocation graph in Figure 7.3. In this example,\nwe also have a cycle:\nP1 →R1 →P3 →R2 →P1\nR2\nR1\nP3\nP4\nP2\nP1\nFigure 7.3\nResource-allocation graph with a cycle but no deadlock. 322\nChapter 7\nDeadlocks\nHowever, there is no deadlock. Observe that process P4 may release its instance\nof resource type R2. That resource can then be allocated to P3, breaking the cycle.",
  "In summary, if a resource-allocation graph does not have a cycle, then the\nsystem is not in a deadlocked state. If there is a cycle, then the system may or\nmay not be in a deadlocked state. This observation is important when we deal\nwith the deadlock problem.\n7.3\nMethods for Handling Deadlocks\nGenerally speaking, we can deal with the deadlock problem in one of three\nways:\n• We can use a protocol to prevent or avoid deadlocks, ensuring that the\nsystem will never enter a deadlocked state.\n• We can allow the system to enter a deadlocked state, detect it, and recover.\n• We can ignore the problem altogether and pretend that deadlocks never\noccur in the system.\nThe third solution is the one used by most operating systems, including Linux",
  "and Windows. It is then up to the application developer to write programs that\nhandle deadlocks.\nNext, we elaborate brieﬂy on each of the three methods for handling\ndeadlocks. Then, in Sections 7.4 through 7.7, we present detailed algorithms.\nBefore proceeding, we should mention that some researchers have argued that\nnone of the basic approaches alone is appropriate for the entire spectrum of\nresource-allocation problems in operating systems. The basic approaches can\nbe combined, however, allowing us to select an optimal approach for each class\nof resources in a system.\nTo ensure that deadlocks never occur, the system can use either a deadlock-\nprevention or a deadlock-avoidance scheme. Deadlock prevention provides a",
  "set of methods to ensure that at least one of the necessary conditions (Section\n7.2.1) cannot hold. These methods prevent deadlocks by constraining how\nrequests for resources can be made. We discuss these methods in Section 7.4.\nDeadlock avoidance requires that the operating system be given additional\ninformation in advance concerning which resources a process will request\nand use during its lifetime. With this additional knowledge, the operating\nsystem can decide for each request whether or not the process should wait.\nTo decide whether the current request can be satisﬁed or must be delayed, the\nsystem must consider the resources currently available, the resources currently\nallocated to each process, and the future requests and releases of each process.",
  "We discuss these schemes in Section 7.5.\nIf a system does not employ either a deadlock-prevention or a deadlock-\navoidance algorithm, then a deadlock situation may arise. In this environment,\nthe system can provide an algorithm that examines the state of the system to\ndetermine whether a deadlock has occurred and an algorithm to recover from\nthe deadlock (if a deadlock has indeed occurred). We discuss these issues in\nSection 7.6 and Section 7.7. 7.4\nDeadlock Prevention\n323\nIn the absence of algorithms to detect and recover from deadlocks, we may\narrive at a situation in which the system is in a deadlocked state yet has no\nway of recognizing what has happened. In this case, the undetected deadlock\nwill cause the system’s performance to deteriorate, because resources are being",
  "held by processes that cannot run and because more and more processes, as\nthey make requests for resources, will enter a deadlocked state. Eventually, the\nsystem will stop functioning and will need to be restarted manually.\nAlthough this method may not seem to be a viable approach to the deadlock\nproblem, it is nevertheless used in most operating systems, as mentioned\nearlier. Expense is one important consideration. Ignoring the possibility of\ndeadlocks is cheaper than the other approaches. Since in many systems,\ndeadlocks occur infrequently (say, once per year), the extra expense of the\nother methods may not seem worthwhile. In addition, methods used to recover\nfrom other conditions may be put to use to recover from deadlock. In some",
  "circumstances, a system is in a frozen state but not in a deadlocked state.\nWe see this situation, for example, with a real-time process running at the\nhighest priority (or any process running on a nonpreemptive scheduler) and\nnever returning control to the operating system. The system must have manual\nrecovery methods for such conditions and may simply use those techniques\nfor deadlock recovery.\n7.4\nDeadlock Prevention\nAs we noted in Section 7.2.1, for a deadlock to occur, each of the four necessary\nconditions must hold. By ensuring that at least one of these conditions cannot\nhold, we can prevent the occurrence of a deadlock. We elaborate on this\napproach by examining each of the four necessary conditions separately.\n7.4.1\nMutual Exclusion",
  "7.4.1\nMutual Exclusion\nThe mutual exclusion condition must hold. That is, at least one resource must be\nnonsharable. Sharable resources, in contrast, do not require mutually exclusive\naccess and thus cannot be involved in a deadlock. Read-only ﬁles are a good\nexample of a sharable resource. If several processes attempt to open a read-only\nﬁle at the same time, they can be granted simultaneous access to the ﬁle. A\nprocess never needs to wait for a sharable resource. In general, however, we\ncannot prevent deadlocks by denying the mutual-exclusion condition, because\nsome resources are intrinsically nonsharable. For example, a mutex lock cannot\nbe simultaneously shared by several processes.\n7.4.2\nHold and Wait\nTo ensure that the hold-and-wait condition never occurs in the system, we must",
  "guarantee that, whenever a process requests a resource, it does not hold any\nother resources. One protocol that we can use requires each process to request\nand be allocated all its resources before it begins execution. We can implement\nthis provision by requiring that system calls requesting resources for a process\nprecede all other system calls. 324\nChapter 7\nDeadlocks\nAn alternative protocol allows a process to request resources only when\nit has none. A process may request some resources and use them. Before it\ncan request any additional resources, it must release all the resources that it is\ncurrently allocated.\nTo illustrate the difference between these two protocols, we consider a\nprocess that copies data from a DVD drive to a ﬁle on disk, sorts the ﬁle, and",
  "then prints the results to a printer. If all resources must be requested at the\nbeginning of the process, then the process must initially request the DVD drive,\ndisk ﬁle, and printer. It will hold the printer for its entire execution, even though\nit needs the printer only at the end.\nThe second method allows the process to request initially only the DVD\ndrive and disk ﬁle. It copies from the DVD drive to the disk and then releases\nboth the DVD drive and the disk ﬁle. The process must then request the disk\nﬁle and the printer. After copying the disk ﬁle to the printer, it releases these\ntwo resources and terminates.\nBoth these protocols have two main disadvantages. First, resource utiliza-\ntion may be low, since resources may be allocated but unused for a long period.",
  "In the example given, for instance, we can release the DVD drive and disk ﬁle,\nand then request the disk ﬁle and printer, only if we can be sure that our data\nwill remain on the disk ﬁle. Otherwise, we must request all resources at the\nbeginning for both protocols.\nSecond, starvation is possible. A process that needs several popular\nresources may have to wait indeﬁnitely, because at least one of the resources\nthat it needs is always allocated to some other process.\n7.4.3\nNo Preemption\nThe third necessary condition for deadlocks is that there be no preemption\nof resources that have already been allocated. To ensure that this condition\ndoes not hold, we can use the following protocol. If a process is holding\nsome resources and requests another resource that cannot be immediately",
  "allocated to it (that is, the process must wait), then all resources the process is\ncurrently holding are preempted. In other words, these resources are implicitly\nreleased. The preempted resources are added to the list of resources for which\nthe process is waiting. The process will be restarted only when it can regain its\nold resources, as well as the new ones that it is requesting.\nAlternatively, if a process requests some resources, we ﬁrst check whether\nthey are available. If they are, we allocate them. If they are not, we check\nwhether they are allocated to some other process that is waiting for additional\nresources. If so, we preempt the desired resources from the waiting process and\nallocate them to the requesting process. If the resources are neither available",
  "nor held by a waiting process, the requesting process must wait. While it is\nwaiting, some of its resources may be preempted, but only if another process\nrequests them. A process can be restarted only when it is allocated the new\nresources it is requesting and recovers any resources that were preempted\nwhile it was waiting.\nThis protocol is often applied to resources whose state can be easily saved\nand restored later, such as CPU registers and memory space. It cannot generally\nbe applied to such resources as mutex locks and semaphores. 7.4\nDeadlock Prevention\n325\n7.4.4\nCircular Wait\nThe fourth and ﬁnal condition for deadlocks is the circular-wait condition. One\nway to ensure that this condition never holds is to impose a total ordering of",
  "all resource types and to require that each process requests resources in an\nincreasing order of enumeration.\nTo illustrate, we let R = {R1, R2, ..., Rm} be the set of resource types. We\nassign to each resource type a unique integer number, which allows us to\ncompare two resources and to determine whether one precedes another in our\nordering. Formally, we deﬁne a one-to-one function F: R →N, where N is the\nset of natural numbers. For example, if the set of resource types R includes\ntape drives, disk drives, and printers, then the function F might be deﬁned as\nfollows:\nF(tape drive) = 1\nF(disk drive) = 5\nF(printer) = 12\nWe can now consider the following protocol to prevent deadlocks: Each\nprocess can request resources only in an increasing order of enumeration. That",
  "is, a process can initially request any number of instances of a resource type\n—say, Ri. After that, the process can request instances of resource type Rj if\nand only if F(Rj) > F(Ri). For example, using the function deﬁned previously,\na process that wants to use the tape drive and printer at the same time must\nﬁrst request the tape drive and then request the printer. Alternatively, we can\nrequire that a process requesting an instance of resource type Rj must have\nreleased any resources Ri such that F(Ri) ≥F(Rj). Note also that if several\ninstances of the same resource type are needed, a single request for all of them\nmust be issued.\nIf these two protocols are used, then the circular-wait condition cannot\nhold. We can demonstrate this fact by assuming that a circular wait exists",
  "(proof by contradiction). Let the set of processes involved in the circular wait be\n{P0, P1, ..., Pn}, where Pi is waiting for a resource Ri, which is held by process\nPi+1. (Modulo arithmetic is used on the indexes, so that Pn is waiting for\na resource Rn held by P0.) Then, since process Pi+1 is holding resource Ri\nwhile requesting resource Ri+1, we must have F(Ri) < F(Ri+1) for all i. But\nthis condition means that F(R0) < F(R1) < ... < F(Rn) < F(R0). By transitivity,\nF(R0) < F(R0), which is impossible. Therefore, there can be no circular wait.\nWe can accomplish this scheme in an application program by developing\nan ordering among all synchronization objects in the system. All requests for\nsynchronization objects must be made in increasing order. For example, if the",
  "lock ordering in the Pthread program shown in Figure 7.4 was\nF(first mutex) = 1\nF(second mutex) = 5\nthen thread two could not request the locks out of order.\nKeep in mind that developing an ordering, or hierarchy, does not in itself\nprevent deadlock. It is up to application developers to write programs that\nfollow the ordering. Also note that the function F should be deﬁned according\nto the normal order of usage of the resources in a system. For example, because 326\nChapter 7\nDeadlocks\n/* thread one runs in this function */\nvoid *do work one(void *param)\n{\npthread mutex lock(&first mutex);\npthread mutex lock(&second mutex);\n/**\n* Do some work\n*/\npthread mutex unlock(&second mutex);\npthread mutex unlock(&first mutex);\npthread exit(0);\n}\n/* thread two runs in this function */",
  "}\n/* thread two runs in this function */\nvoid *do work two(void *param)\n{\npthread mutex lock(&second mutex);\npthread mutex lock(&first mutex);\n/**\n* Do some work\n*/\npthread mutex unlock(&first mutex);\npthread mutex unlock(&second mutex);\npthread exit(0);\n}\nFigure 7.4\nDeadlock example.\nthe tape drive is usually needed before the printer, it would be reasonable to\ndeﬁne F(tape drive) < F(printer).\nAlthough ensuring that resources are acquired in the proper order is the\nresponsibility of application developers, certain software can be used to verify\nthat locks are acquired in the proper order and to give appropriate warnings\nwhen locks are acquired out of order and deadlock is possible. One lock-order\nveriﬁer, which works on BSD versions of UNIX such as FreeBSD, is known as",
  "witness. Witness uses mutual-exclusion locks to protect critical sections, as\ndescribed in Chapter 5. It works by dynamically maintaining the relationship\nof lock orders in a system. Let’s use the program shown in Figure 7.4 as an\nexample. Assume that thread one is the ﬁrst to acquire the locks and does so in\nthe order (1) first mutex, (2) second mutex. Witness records the relationship\nthat first mutex must be acquired before second mutex. If thread two later\nacquires the locks out of order, witness generates a warning message on the\nsystem console.\nIt is also important to note that imposing a lock ordering does not guarantee\ndeadlock prevention if locks can be acquired dynamically. For example, assume\nwe have a function that transfers funds between two accounts. To prevent a",
  "race condition, each account has an associated mutex lock that is obtained from\na get lock() function such as shown in Figure 7.5: 7.5\nDeadlock Avoidance\n327\nvoid transaction(Account from, Account to, double amount)\n{\nmutex lock1, lock2;\nlock1 = get lock(from);\nlock2 = get lock(to);\nacquire(lock1);\nacquire(lock2);\nwithdraw(from, amount);\ndeposit(to, amount);\nrelease(lock2);\nrelease(lock1);\n}\nFigure 7.5\nDeadlock example with lock ordering.\nDeadlock is possible if two threads simultaneously invoke the transaction()\nfunction, transposing different accounts. That is, one thread might invoke\ntransaction(checking account, savings account, 25);\nand another might invoke\ntransaction(savings account, checking account, 50);\nWe leave it as an exercise for students to ﬁx this situation.\n7.5",
  "7.5\nDeadlock Avoidance\nDeadlock-preventionalgorithms, asdiscussed inSection7.4, preventdeadlocks\nby limiting how requests can be made. The limits ensure that at least one of\nthe necessary conditions for deadlock cannot occur. Possible side effects of\npreventing deadlocks by this method, however, are low device utilization and\nreduced system throughput.\nAn alternative method for avoiding deadlocks is to require additional\ninformation about how resources are to be requested. For example, in a system\nwith one tape drive and one printer, the system might need to know that\nprocess P will request ﬁrst the tape drive and then the printer before releasing\nboth resources, whereas process Q will request ﬁrst the printer and then the",
  "tape drive. With this knowledge of the complete sequence of requests and\nreleases for each process, the system can decide for each request whether or\nnot the process should wait in order to avoid a possible future deadlock. Each\nrequest requires that in making this decision the system consider the resources\ncurrently available, the resources currently allocated to each process, and the\nfuture requests and releases of each process.\nThe various algorithms that use this approach differ in the amount and\ntype of information required. The simplest and most useful model requires\nthat each process declare the maximum number of resources of each type that\nit may need. Given this a priori information, it is possible to construct an 328\nChapter 7\nDeadlocks",
  "Chapter 7\nDeadlocks\nalgorithm that ensures that the system will never enter a deadlocked state. A\ndeadlock-avoidance algorithm dynamically examines the resource-allocation\nstate to ensure that a circular-wait condition can never exist. The resource-\nallocation state is deﬁned by the number of available and allocated resources\nand the maximum demands of the processes. In the following sections, we\nexplore two deadlock-avoidance algorithms.\n7.5.1\nSafe State\nA state is safe if the system can allocate resources to each process (up to its\nmaximum) in some order and still avoid a deadlock. More formally, a system\nis in a safe state only if there exists a safe sequence. A sequence of processes\n<P1, P2, ..., Pn> is a safe sequence for the current allocation state if, for each",
  "Pi, the resource requests that Pi can still make can be satisﬁed by the currently\navailable resources plus the resources held by all Pj, with j < i. In this situation,\nif the resources that Pi needs are not immediately available, then Pi can wait\nuntil all Pj have ﬁnished. When they have ﬁnished, Pi can obtain all of its\nneeded resources, complete its designated task, return its allocated resources,\nand terminate. When Pi terminates, Pi+1 can obtain its needed resources, and\nso on. If no such sequence exists, then the system state is said to be unsafe.\nA safe state is not a deadlocked state. Conversely, a deadlocked state is\nan unsafe state. Not all unsafe states are deadlocks, however (Figure 7.6).\nAn unsafe state may lead to a deadlock. As long as the state is safe, the",
  "operating system can avoid unsafe (and deadlocked) states. In an unsafe state,\nthe operating system cannot prevent processes from requesting resources in\nsuch a way that a deadlock occurs. The behavior of the processes controls\nunsafe states.\nTo illustrate, we consider a system with twelve magnetic tape drives and\nthree processes: P0, P1, and P2. Process P0 requires ten tape drives, process P1\nmay need as many as four tape drives, and process P2 may need up to nine tape\ndrives. Suppose that, at time t0, process P0 is holding ﬁve tape drives, process\nP1 is holding two tape drives, and process P2 is holding two tape drives. (Thus,\nthere are three free tape drives.)\ndeadlock\nunsafe\nsafe\nFigure 7.6\nSafe, unsafe, and deadlocked state spaces. 7.5\nDeadlock Avoidance\n329\nMaximum Needs",
  "Deadlock Avoidance\n329\nMaximum Needs\nCurrent Needs\nP0\n10\n5\nP1\n4\n2\nP2\n9\n2\nAt time t0, the system is in a safe state. The sequence <P1, P0, P2> satisﬁes\nthe safety condition. Process P1 can immediately be allocated all its tape drives\nand then return them (the system will then have ﬁve available tape drives);\nthen process P0 can get all its tape drives and return them (the system will then\nhave ten available tape drives); and ﬁnally process P2 can get all its tape drives\nand return them (the system will then have all twelve tape drives available).\nA system can go from a safe state to an unsafe state. Suppose that, at time\nt1, process P2 requests and is allocated one more tape drive. The system is no\nlonger in a safe state. At this point, only process P1 can be allocated all its tape",
  "drives. When it returns them, the system will have only four available tape\ndrives. Since process P0 is allocated ﬁve tape drives but has a maximum of ten,\nit may request ﬁve more tape drives. If it does so, it will have to wait, because\nthey are unavailable. Similarly, process P2 may request six additional tape\ndrives and have to wait, resulting in a deadlock. Our mistake was in granting\nthe request from process P2 for one more tape drive. If we had made P2 wait\nuntil either of the other processes had ﬁnished and released its resources, then\nwe could have avoided the deadlock.\nGiven the concept of a safe state, we can deﬁne avoidance algorithms that\nensure that the system will never deadlock. The idea is simply to ensure that the",
  "system will always remain in a safe state. Initially, the system is in a safe state.\nWhenever a process requests a resource that is currently available, the system\nmust decide whether the resource can be allocated immediately or whether\nthe process must wait. The request is granted only if the allocation leaves the\nsystem in a safe state.\nIn this scheme, if a process requests a resource that is currently available,\nit may still have to wait. Thus, resource utilization may be lower than it would\notherwise be.\n7.5.2\nResource-Allocation-Graph Algorithm\nIf we have a resource-allocation system with only one instance of each resource\ntype, we can use a variant of the resource-allocation graph deﬁned in Section\n7.2.2 for deadlock avoidance. In addition to the request and assignment edges",
  "already described, we introduce a new type of edge, called a claim edge.\nA claim edge Pi →Rj indicates that process Pi may request resource Rj at\nsome time in the future. This edge resembles a request edge in direction but is\nrepresented in the graph by a dashed line. When process Pi requests resource\nRj, the claim edge Pi →Rj is converted to a request edge. Similarly, when a\nresource Rj is released by Pi, the assignment edge Rj →Pi is reconverted to a\nclaim edge Pi →Rj.\nNote that the resources must be claimed a priori in the system. That is,\nbefore process Pi starts executing, all its claim edges must already appear in\nthe resource-allocation graph. We can relax this condition by allowing a claim\nedge Pi →Rj to be added to the graph only if all the edges associated with",
  "process Pi are claim edges. 330\nChapter 7\nDeadlocks\nR1\nR2\nP2\nP1\nFigure 7.7\nResource-allocation graph for deadlock avoidance.\nNow suppose that process Pi requests resource Rj. The request can be\ngranted only if converting the request edge Pi →Rj to an assignment edge\nRj →Pi does not result in the formation of a cycle in the resource-allocation\ngraph. We check for safety by using a cycle-detection algorithm. An algorithm\nfor detecting a cycle in this graph requires an order of n2 operations, where n\nis the number of processes in the system.\nIf no cycle exists, then the allocation of the resource will leave the system\nin a safe state. If a cycle is found, then the allocation will put the system in\nan unsafe state. In that case, process Pi will have to wait for its requests to be\nsatisﬁed.",
  "satisﬁed.\nTo illustrate this algorithm, we consider the resource-allocation graph of\nFigure 7.7. Suppose that P2 requests R2. Although R2 is currently free, we\ncannot allocate it to P2, since this action will create a cycle in the graph (Figure\n7.8). A cycle, as mentioned, indicates that the system is in an unsafe state. If P1\nrequests R2, and P2 requests R1, then a deadlock will occur.\n7.5.3\nBanker’s Algorithm\nThe resource-allocation-graph algorithm is not applicable to a resource-\nallocation system with multiple instances of each resource type. The deadlock-\navoidance algorithm that we describe next is applicable to such a system but\nis less efﬁcient than the resource-allocation graph scheme. This algorithm is\ncommonly known as the banker’s algorithm. The name was chosen because",
  "the algorithm could be used in a banking system to ensure that the bank never\nR1\nR2\nP2\nP1\nFigure 7.8\nAn unsafe state in a resource-allocation graph. 7.5\nDeadlock Avoidance\n331\nallocated its available cash in such a way that it could no longer satisfy the\nneeds of all its customers.\nWhen a new process enters the system, it must declare the maximum\nnumber of instances of each resource type that it may need. This number may\nnot exceed the total number of resources in the system. When a user requests\na set of resources, the system must determine whether the allocation of these\nresources will leave the system in a safe state. If it will, the resources are\nallocated; otherwise, the process must wait until some other process releases\nenough resources.",
  "enough resources.\nSeveral data structures must be maintained to implement the banker’s\nalgorithm. These data structures encode the state of the resource-allocation\nsystem. We need the following data structures, where n is the number of\nprocesses in the system and m is the number of resource types:\n• Available. A vector of length m indicates the number of available resources\nof each type. If Available[j] equals k, then k instances of resource type Rj\nare available.\n• Max. An n × m matrix deﬁnes the maximum demand of each process.\nIf Max[i][j] equals k, then process Pi may request at most k instances of\nresource type Rj.\n• Allocation. An n × m matrix deﬁnes the number of resources of each type\ncurrently allocated to each process. If Allocation[i][j] equals k, then process",
  "Pi is currently allocated k instances of resource type Rj.\n• Need. An n × m matrix indicates the remaining resource need of each\nprocess. If Need[i][j] equals k, then process Pi may need k more instances\nof resource type Rj to complete its task. Note that Need[i][j] equals Max[i][j]\n−Allocation[i][j].\nThese data structures vary over time in both size and value.\nTo simplify the presentation of the banker’s algorithm, we next establish\nsome notation. Let X and Y be vectors of length n. We say that X ≤Y if and\nonly if X[i] ≤Y[i] for all i = 1, 2, ..., n. For example, if X = (1,7,3,2) and Y =\n(0,3,2,1), then Y ≤X. In addition, Y < X if Y ≤X and Y ̸= X.\nWe can treat each row in the matrices Allocation and Need as vectors",
  "and refer to them as Allocationi and Needi. The vector Allocationi speciﬁes\nthe resources currently allocated to process Pi; the vector Needi speciﬁes the\nadditional resources that process Pi may still request to complete its task.\n7.5.3.1\nSafety Algorithm\nWe can now present the algorithm for ﬁnding out whether or not a system is\nin a safe state. This algorithm can be described as follows:\n1. Let Work and Finish be vectors of length m and n, respectively. Initialize\nWork = Available and Finish[i] = false for i = 0, 1, ..., n −1.\n2. Find an index i such that both\na. Finish[i] == false\nb. Needi ≤Work 332\nChapter 7\nDeadlocks\nIf no such i exists, go to step 4.\n3. Work = Work + Allocationi\nFinish[i] = true\nGo to step 2.\n4. If Finish[i] == true for all i, then the system is in a safe state.",
  "This algorithm may require an order of m × n2 operations to determine whether\na state is safe.\n7.5.3.2\nResource-Request Algorithm\nNext, we describe the algorithm for determining whether requests can be safely\ngranted.\nLet Requesti be the request vector for process Pi. If Requesti [ j] == k, then\nprocess Pi wants k instances of resource type Rj. When a request for resources\nis made by process Pi, the following actions are taken:\n1. If Requesti ≤Needi, go to step 2. Otherwise, raise an error condition, since\nthe process has exceeded its maximum claim.\n2. If Requesti ≤Available, go to step 3. Otherwise, Pi must wait, since the\nresources are not available.\n3. Have the system pretend to have allocated the requested resources to\nprocess Pi by modifying the state as follows:",
  "process Pi by modifying the state as follows:\nAvailable = Available–Requesti;\nAllocationi = Allocationi + Requesti;\nNeedi = Needi –Requesti;\nIf the resulting resource-allocation state is safe, the transaction is com-\npleted, and process Pi is allocated its resources. However, if the new state\nis unsafe, then Pi must wait for Requesti, and the old resource-allocation\nstate is restored.\n7.5.3.3\nAn Illustrative Example\nTo illustrate the use of the banker’s algorithm, consider a system with ﬁve\nprocesses P0 through P4 and three resource types A, B, and C. Resource type A\nhas ten instances, resource type B has ﬁve instances, and resource type C has\nseven instances. Suppose that, at time T0, the following snapshot of the system\nhas been taken:\nAllocation\nMax\nAvailable\nA B C\nA B C\nA B C\nP0\n0 1 0",
  "Max\nAvailable\nA B C\nA B C\nA B C\nP0\n0 1 0\n7 5 3\n3 3 2\nP1\n2 0 0\n3 2 2\nP2\n3 0 2\n9 0 2\nP3\n2 1 1\n2 2 2\nP4\n0 0 2\n4 3 3 7.6\nDeadlock Detection\n333\nThe content of the matrix Need is deﬁned to be Max −Allocation and is as\nfollows:\nNeed\nA B C\nP0\n7 4 3\nP1\n1 2 2\nP2\n6 0 0\nP3\n0 1 1\nP4\n4 3 1\nWe claim that the system is currently in a safe state. Indeed, the sequence\n<P1, P3, P4, P2, P0> satisﬁes the safety criteria. Suppose now that process\nP1 requests one additional instance of resource type A and two instances of\nresource type C, so Request1 = (1,0,2). To decide whether this request can be\nimmediately granted, we ﬁrst check that Request1 ≤Available—that is, that\n(1,0,2) ≤(3,3,2), which is true. We then pretend that this request has been\nfulﬁlled, and we arrive at the following new state:\nAllocation",
  "Allocation\nNeed\nAvailable\nA B C\nA B C\nA B C\nP0\n0 1 0\n7 4 3\n2 3 0\nP1\n3 0 2\n0 2 0\nP2\n3 0 2\n6 0 0\nP3\n2 1 1\n0 1 1\nP4\n0 0 2\n4 3 1\nWe must determine whether this new system state is safe. To do so, we\nexecute our safety algorithm and ﬁnd that the sequence <P1, P3, P4, P0, P2>\nsatisﬁes the safety requirement. Hence, we can immediately grant the request\nof process P1.\nYou should be able to see, however, that when the system is in this state, a\nrequest for (3,3,0) by P4 cannot be granted, since the resources are not available.\nFurthermore, a request for (0,2,0) by P0 cannot be granted, even though the\nresources are available, since the resulting state is unsafe.\nWe leave it as a programming exercise for students to implement the\nbanker’s algorithm.\n7.6\nDeadlock Detection",
  "banker’s algorithm.\n7.6\nDeadlock Detection\nIf a system does not employ either a deadlock-prevention or a deadlock-\navoidance algorithm, then a deadlock situation may occur. In this environment,\nthe system may provide:\n• An algorithm that examines the state of the system to determine whether\na deadlock has occurred\n• An algorithm to recover from the deadlock 334\nChapter 7\nDeadlocks\nP3\nP5\nP4\nP2\nP1\nR2\nR1\nR3\nR4\nR5\nP3\nP5\nP4\nP2\nP1\n(b)\n(a)\nFigure 7.9\n(a) Resource-allocation graph. (b) Corresponding wait-for graph.\nIn the following discussion, we elaborate on these two requirements as they\npertain to systems with only a single instance of each resource type, as well as to\nsystems with several instances of each resource type. At this point, however, we",
  "note that a detection-and-recovery scheme requires overhead that includes not\nonly the run-time costs of maintaining the necessary information and executing\nthe detection algorithm but also the potential losses inherent in recovering from\na deadlock.\n7.6.1\nSingle Instance of Each Resource Type\nIf all resources have only a single instance, then we can deﬁne a deadlock-\ndetection algorithm that uses a variant of the resource-allocation graph, called\na wait-for graph. We obtain this graph from the resource-allocation graph by\nremoving the resource nodes and collapsing the appropriate edges.\nMore precisely, an edge from Pi to Pj in a wait-for graph implies that\nprocess Pi is waiting for process Pj to release a resource that Pi needs. An edge",
  "Pi →Pj exists in a wait-for graph if and only if the corresponding resource-\nallocation graph contains two edges Pi →Rq and Rq →Pj for some resource\nRq. In Figure 7.9, we present a resource-allocation graph and the corresponding\nwait-for graph.\nAs before, a deadlock exists in the system if and only if the wait-for graph\ncontains a cycle. To detect deadlocks, the system needs to maintain the wait-\nfor graph and periodically invoke an algorithm that searches for a cycle in\nthe graph. An algorithm to detect a cycle in a graph requires an order of n2\noperations, where n is the number of vertices in the graph.\n7.6.2\nSeveral Instances of a Resource Type\nThe wait-for graph scheme is not applicable to a resource-allocation system",
  "with multiple instances of each resource type. We turn now to a deadlock- 7.6\nDeadlock Detection\n335\ndetection algorithm that is applicable to such a system. The algorithm employs\nseveral time-varying data structures that are similar to those used in the\nbanker’s algorithm (Section 7.5.3):\n• Available. A vector of length m indicates the number of available resources\nof each type.\n• Allocation. An n × m matrix deﬁnes the number of resources of each type\ncurrently allocated to each process.\n• Request. An n × m matrix indicates the current request of each process.\nIf Request[i][j] equals k, then process Pi is requesting k more instances of\nresource type Rj.\nThe ≤relation between two vectors is deﬁned as in Section 7.5.3. To simplify",
  "notation, we again treat the rows in the matrices Allocation and Request as\nvectors; we refer to them as Allocationi and Requesti. The detection algorithm\ndescribed here simply investigates every possible allocation sequence for the\nprocesses that remain to be completed. Compare this algorithm with the\nbanker’s algorithm of Section 7.5.3.\n1. Let Work and Finish be vectors of length m and n, respectively. Initialize\nWork = Available. For i = 0, 1, ..., n–1, if Allocationi ̸= 0, then Finish[i] =\nfalse. Otherwise, Finish[i] = true.\n2. Find an index i such that both\na. Finish[i] == false\nb. Requesti ≤Work\nIf no such i exists, go to step 4.\n3. Work = Work + Allocationi\nFinish[i] = true\nGo to step 2.\n4. If Finish[i] == false for some i, 0 ≤i < n, then the system is in a deadlocked",
  "state. Moreover, if Finish[i] == false, then process Pi is deadlocked.\nThis algorithm requires an order of m × n2 operations to detect whether the\nsystem is in a deadlocked state.\nYou may wonder why we reclaim the resources of process Pi (in step 3) as\nsoon as we determine that Requesti ≤Work (in step 2b). We know that Pi is\ncurrently not involved in a deadlock (since Requesti ≤Work). Thus, we take\nan optimistic attitude and assume that Pi will require no more resources to\ncomplete its task; it will thus soon return all currently allocated resources to\nthe system. If our assumption is incorrect, a deadlock may occur later. That\ndeadlock will be detected the next time the deadlock-detection algorithm is\ninvoked.\nTo illustrate this algorithm, we consider a system with ﬁve processes P0",
  "through P4 and three resource types A, B, and C. Resource type A has seven\ninstances, resource type B has two instances, and resource type C has six 336\nChapter 7\nDeadlocks\ninstances. Suppose that, at time T0, we have the following resource-allocation\nstate:\nAllocation\nRequest\nAvailable\nA B C\nA B C\nA B C\nP0\n0 1 0\n0 0 0\n0 0 0\nP1\n2 0 0\n2 0 2\nP2\n3 0 3\n0 0 0\nP3\n2 1 1\n1 0 0\nP4\n0 0 2\n0 0 2\nWe claim that the system is not in a deadlocked state. Indeed, if we execute\nour algorithm, we will ﬁnd that the sequence <P0, P2, P3, P1, P4> results in\nFinish[i] == true for all i.\nSuppose now that process P2 makes one additional request for an instance\nof type C. The Request matrix is modiﬁed as follows:\nRequest\nA B C\nP0\n0 0 0\nP1\n2 0 2\nP2\n0 0 1\nP3\n1 0 0\nP4\n0 0 2",
  "P0\n0 0 0\nP1\n2 0 2\nP2\n0 0 1\nP3\n1 0 0\nP4\n0 0 2\nWe claim that the system is now deadlocked. Although we can reclaim the\nresources held by process P0, the number of available resources is not sufﬁcient\nto fulﬁll the requests of the other processes. Thus, a deadlock exists, consisting\nof processes P1, P2, P3, and P4.\n7.6.3\nDetection-Algorithm Usage\nWhen should we invoke the detection algorithm? The answer depends on two\nfactors:\n1. How often is a deadlock likely to occur?\n2. How many processes will be affected by deadlock when it happens?\nIf deadlocks occur frequently, then the detection algorithm should be invoked\nfrequently. Resources allocated to deadlocked processes will be idle until the\ndeadlock can be broken. In addition, the number of processes involved in the\ndeadlock cycle may grow.",
  "deadlock cycle may grow.\nDeadlocks occur only when some process makes a request that cannot be\ngranted immediately. This request may be the ﬁnal request that completes a\nchain of waiting processes. In the extreme, then, we can invoke the deadlock-\ndetection algorithm every time a request for allocation cannot be granted\nimmediately. In this case, we can identify not only the deadlocked set of 7.7\nRecovery from Deadlock\n337\nprocesses but also the speciﬁc process that “caused” the deadlock. (In reality,\neach of the deadlocked processes is a link in the cycle in the resource graph, so\nall of them, jointly, caused the deadlock.) If there are many different resource\ntypes, one request may create many cycles in the resource graph, each cycle",
  "completed by the most recent request and “caused” by the one identiﬁable\nprocess.\nOf course, invoking the deadlock-detection algorithm for every resource\nrequest will incur considerable overhead in computation time. A less expensive\nalternative is simply to invoke the algorithm at deﬁned intervals—for example,\nonce per hour or whenever CPU utilization drops below 40 percent. (A deadlock\neventually cripples system throughput and causes CPU utilization to drop.) If\nthe detection algorithm is invoked at arbitrary points in time, the resource\ngraph may contain many cycles. In this case, we generally cannot tell which of\nthe many deadlocked processes “caused” the deadlock.\n7.7\nRecovery from Deadlock\nWhen a detection algorithm determines that a deadlock exists, several alter-",
  "natives are available. One possibility is to inform the operator that a deadlock\nhas occurred and to let the operator deal with the deadlock manually. Another\npossibility is to let the system recover from the deadlock automatically. There\nare two options for breaking a deadlock. One is simply to abort one or more\nprocesses to break the circular wait. The other is to preempt some resources\nfrom one or more of the deadlocked processes.\n7.7.1\nProcess Termination\nTo eliminate deadlocks by aborting a process, we use one of two methods. In\nboth methods, the system reclaims all resources allocated to the terminated\nprocesses.\n• Abort all deadlocked processes. This method clearly will break the\ndeadlock cycle, but at great expense. The deadlocked processes may have",
  "computed for a long time, and the results of these partial computations\nmust be discarded and probably will have to be recomputed later.\n• Abort one process at a time until the deadlock cycle is eliminated. This\nmethod incurs considerable overhead, since after each process is aborted, a\ndeadlock-detection algorithm must be invoked to determine whether any\nprocesses are still deadlocked.\nAborting a process may not be easy. If the process was in the midst of\nupdating a ﬁle, terminating it will leave that ﬁle in an incorrect state. Similarly,\nif the process was in the midst of printing data on a printer, the system must\nreset the printer to a correct state before printing the next job.\nIf the partial termination method is used, then we must determine which",
  "deadlocked process (or processes) should be terminated. This determination is\na policy decision, similar to CPU-scheduling decisions. The question is basically\nan economic one; we should abort those processes whose termination will incur 338\nChapter 7\nDeadlocks\nthe minimum cost. Unfortunately, the term minimum cost is not a precise one.\nMany factors may affect which process is chosen, including:\n1. What the priority of the process is\n2. How long the process has computed and how much longer the process\nwill compute before completing its designated task\n3. How many and what types of resources the process has used (for example,\nwhether the resources are simple to preempt)\n4. How many more resources the process needs in order to complete\n5. How many processes will need to be terminated",
  "5. How many processes will need to be terminated\n6. Whether the process is interactive or batch\n7.7.2\nResource Preemption\nTo eliminate deadlocks using resource preemption, we successively preempt\nsome resources from processes and give these resources to other processes until\nthe deadlock cycle is broken.\nIf preemption is required to deal with deadlocks, then three issues need to\nbe addressed:\n1. Selecting a victim. Which resources and which processes are to be\npreempted? As in process termination, we must determine the order of\npreemption to minimize cost. Cost factors may include such parameters\nas the number of resources a deadlocked process is holding and the\namount of time the process has thus far consumed.\n2. Rollback. If we preempt a resource from a process, what should be done",
  "with that process? Clearly, it cannot continue with its normal execution; it\nis missing some needed resource. We must roll back the process to some\nsafe state and restart it from that state.\nSince, in general, it is difﬁcult to determine what a safe state is, the\nsimplest solution is a total rollback: abort the process and then restart\nit. Although it is more effective to roll back the process only as far as\nnecessary to break the deadlock, this method requires the system to keep\nmore information about the state of all running processes.\n3. Starvation. How do we ensure that starvation will not occur? That is,\nhow can we guarantee that resources will not always be preempted from\nthe same process?\nIn a system where victim selection is based primarily on cost factors,",
  "it may happen that the same process is always picked as a victim. As\na result, this process never completes its designated task, a starvation\nsituation any practical system must address. Clearly, we must ensure\nthat a process can be picked as a victim only a (small) ﬁnite number of\ntimes. The most common solution is to include the number of rollbacks\nin the cost factor. Practice Exercises\n339\n7.8\nSummary\nA deadlocked state occurs when two or more processes are waiting indeﬁnitely\nfor an event that can be caused only by one of the waiting processes. There are\nthree principal methods for dealing with deadlocks:\n• Use some protocol to prevent or avoid deadlocks, ensuring that the system\nwill never enter a deadlocked state.",
  "will never enter a deadlocked state.\n• Allow the system to enter a deadlocked state, detect it, and then recover.\n• Ignore the problem altogether and pretend that deadlocks never occur in\nthe system.\nThe third solution is the one used by most operating systems, including Linux\nand Windows.\nA deadlock can occur only if four necessary conditions hold simultaneously\nin the system: mutual exclusion, hold and wait, no preemption, and circular\nwait. To prevent deadlocks, we can ensure that at least one of the necessary\nconditions never holds.\nA method for avoiding deadlocks, rather than preventing them, requires\nthat the operating system have a priori information about how each process\nwill utilize system resources. The banker’s algorithm, for example, requires",
  "a priori information about the maximum number of each resource class that\neach process may request. Using this information, we can deﬁne a deadlock-\navoidance algorithm.\nIf a system does not employ a protocol to ensure that deadlocks will never\noccur, then a detection-and-recovery scheme may be employed. A deadlock-\ndetection algorithm must be invoked to determine whether a deadlock\nhas occurred. If a deadlock is detected, the system must recover either by\nterminating some of the deadlocked processes or by preempting resources\nfrom some of the deadlocked processes.\nWhere preemption is used to deal with deadlocks, three issues must be\naddressed: selecting a victim, rollback, and starvation. In a system that selects",
  "victims for rollback primarily on the basis of cost factors, starvation may occur,\nand the selected process can never complete its designated task.\nResearchers have argued that none of the basic approaches alone is appro-\npriate for the entire spectrum of resource-allocation problems in operating\nsystems. The basic approaches can be combined, however, allowing us to select\nan optimal approach for each class of resources in a system.\nPractice Exercises\n7.1\nList three examples of deadlocks that are not related to a computer-\nsystem environment.\n7.2\nSuppose that a system is in an unsafe state. Show that it is possible for\nthe processes to complete their execution without entering a deadlocked\nstate. 340\nChapter 7\nDeadlocks\n7.3\nConsider the following snapshot of a system:\nAllocation\nMax",
  "Allocation\nMax\nAvailable\nA B C D\nA B C D\nA B C D\nP0\n0 0 1 2\n0 0 1 2\n1 5 2 0\nP1\n1 0 0 0\n1 7 5 0\nP2\n1 3 5 4\n2 3 5 6\nP3\n0 6 3 2\n0 6 5 2\nP4\n0 0 1 4\n0 6 5 6\nAnswer the following questions using the banker’s algorithm:\na.\nWhat is the content of the matrix Need?\nb.\nIs the system in a safe state?\nc.\nIf a request from process P1 arrives for (0,4,2,0), can the request be\ngranted immediately?\n7.4\nA possible method for preventing deadlocks is to have a single, higher-\norder resource that must be requested before any other resource. For\nexample, if multiple threads attempt to access the synchronization\nobjects A· · · E, deadlock is possible. (Such synchronization objects may\ninclude mutexes, semaphores, condition variables, and the like.) We can",
  "prevent the deadlock by adding a sixth object F. Whenever a thread\nwants to acquire the synchronization lock for any object A· · · E, it must\nﬁrst acquire the lock for object F. This solution is known as containment:\nthe locks for objects A · · · E are contained within the lock for object F.\nCompare this scheme with the circular-wait scheme of Section 7.4.4.\n7.5\nProve that the safety algorithm presented in Section 7.5.3 requires an\norder of m × n2 operations.\n7.6\nConsider a computer system that runs 5,000 jobs per month and has no\ndeadlock-prevention or deadlock-avoidance scheme. Deadlocks occur\nabout twice per month, and the operator must terminate and rerun\nabout ten jobs per deadlock. Each job is worth about two dollars (in CPU",
  "time), and the jobs terminated tend to be about half done when they are\naborted.\nA systems programmer has estimated that a deadlock-avoidance\nalgorithm (like the banker’s algorithm) could be installed in the system\nwith an increase of about 10 percent in the average execution time per\njob. Since the machine currently has 30 percent idle time, all 5,000 jobs\nper month could still be run, although turnaround time would increase\nby about 20 percent on average.\na.\nWhat are the arguments for installing the deadlock-avoidance\nalgorithm?\nb.\nWhat are the arguments against installing the deadlock-avoidance\nalgorithm? Exercises\n341\n7.7\nCan a system detect that some of its processes are starving? If you answer\n“yes,” explain how it can. If you answer “no,” explain how the system",
  "can deal with the starvation problem.\n7.8\nConsider the following resource-allocation policy. Requests for and\nreleases of resources are allowed at any time. If a request for resources\ncannot be satisﬁed because the resources are not available, then we check\nany processes that are blocked waiting for resources. If a blocked process\nhas the desired resources, then these resources are taken away from it\nand are given to the requesting process. The vector of resources for which\nthe blocked process is waiting is increased to include the resources that\nwere taken away.\nFor example, a system has three resource types, and the vector\nAvailable is initialized to (4,2,2). If process P0 asks for (2,2,1), it gets\nthem. If P1 asks for (1,0,1), it gets them. Then, if P0 asks for (0,0,1), it",
  "is blocked (resource not available). If P2 now asks for (2,0,0), it gets the\navailable one (1,0,0), as well as one that was allocated to P0 (since P0 is\nblocked). P0’s Allocation vector goes down to (1,2,1), and its Need vector\ngoes up to (1,0,1).\na.\nCan deadlock occur? If you answer “yes,” give an example. If you\nanswer “no,” specify which necessary condition cannot occur.\nb.\nCan indeﬁnite blocking occur? Explain your answer.\n7.9\nSuppose that you have coded the deadlock-avoidance safety algorithm\nand now have been asked to implement the deadlock-detection algo-\nrithm. Can you do so by simply using the safety algorithm code and\nredeﬁning Maxi = Waitingi + Allocationi, where Waitingi is a vector\nspecifying the resources for which process i is waiting and Allocationi",
  "is as deﬁned in Section 7.5? Explain your answer.\n7.10\nIs it possible to have a deadlock involving only one single-threaded\nprocess? Explain your answer.\nExercises\n7.11\nConsider the trafﬁc deadlock depicted in Figure 7.10.\na.\nShow that the four necessary conditions for deadlock hold in this\nexample.\nb.\nState a simple rule for avoiding deadlocks in this system.\n7.12\nAssume a multithreaded application uses only reader–writer locks for\nsynchronization. Applying the four necessary conditions for deadlock,\nis deadlock still possible if multiple reader–writer locks are used?\n7.13\nThe program example shown in Figure 7.4 doesn’t always lead to\ndeadlock. Describe what role the CPU scheduler plays and how it can\ncontribute to deadlock in this program. 342\nChapter 7\nDeadlocks\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•",
  "Chapter 7\nDeadlocks\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\n•\nFigure 7.10\nTrafﬁc deadlock for Exercise 7.11.\n7.14\nIn Section 7.4.4, we describe a situation in which we prevent deadlock\nby ensuring that all locks are acquired in a certain order. However,\nwe also point out that deadlock is possible in this situation if two\nthreads simultaneously invoke the transaction() function. Fix the\ntransaction() function to prevent deadlocks.\n7.15\nCompare the circular-wait scheme with the various deadlock-avoidance\nschemes (like the banker’s algorithm) with respect to the following\nissues:\na.\nRuntime overheads\nb.\nSystem throughput\n7.16\nIn a real computer system, neither the resources available nor the\ndemands of processes for resources are consistent over long periods",
  "(months). Resources break or are replaced, new processes come and go,\nand new resources are bought and added to the system. If deadlock is\ncontrolled by the banker’s algorithm, which of the following changes\ncan be made safely (without introducing the possibility of deadlock),\nand under what circumstances?\na.\nIncrease Available (new resources added).\nb.\nDecrease Available (resource permanently removed from system).\nc.\nIncrease Max for one process (the process needs or wants more\nresources than allowed).\nd.\nDecrease Max for one process (the process decides it does not need\nthat many resources). Exercises\n343\ne.\nIncrease the number of processes.\nf.\nDecrease the number of processes.\n7.17\nConsider a system consisting of four resources of the same type that are",
  "shared by three processes, each of which needs at most two resources.\nShow that the system is deadlock free.\n7.18\nConsider a system consisting of m resources of the same type being\nshared by n processes. A process can request or release only one resource\nat a time. Show that the system is deadlock free if the following two\nconditions hold:\na.\nThe maximum need of each process is between one resource and\nm resources.\nb.\nThe sum of all maximum needs is less than m + n.\n7.19\nConsider the version of the dining-philosophers problem in which the\nchopsticks are placed at the center of the table and any two of them\ncan be used by a philosopher. Assume that requests for chopsticks are\nmade one at a time. Describe a simple rule for determining whether a",
  "particular request can be satisﬁed without causing deadlock given the\ncurrent allocation of chopsticks to philosophers.\n7.20\nConsider again the setting in the preceding question. Assume now that\neach philosopher requires three chopsticks to eat. Resource requests are\nstill issued one at a time. Describe some simple rules for determining\nwhether a particular request can be satisﬁed without causing deadlock\ngiven the current allocation of chopsticks to philosophers.\n7.21\nWe can obtain the banker’s algorithm for a single resource type from\nthe general banker’s algorithm simply by reducing the dimensionality\nof the various arrays by 1. Show through an example that we cannot\nimplement the multiple-resource-type banker’s scheme by applying the",
  "single-resource-type scheme to each resource type individually.\n7.22\nConsider the following snapshot of a system:\nAllocation\nMax\nA B C D\nA B C D\nP0\n3 0 1 4\n5 1 1 7\nP1\n2 2 1 0\n3 2 1 1\nP2\n3 1 2 1\n3 3 2 1\nP3\n0 5 1 0\n4 6 1 2\nP4\n4 2 1 2\n6 3 2 5\nUsing the banker’s algorithm, determine whether or not each of the\nfollowing states is unsafe. If the state is safe, illustrate the order in which\nthe processes may complete. Otherwise, illustrate why the state is unsafe.\na.\nAvailable = (0, 3, 0, 1)\nb.\nAvailable = (1, 0, 0, 2) 344\nChapter 7\nDeadlocks\n7.23\nConsider the following snapshot of a system:\nAllocation\nMax\nAvailable\nA B C D\nA B C D\nA B C D\nP0\n2 0 0 1\n4 2 1 2\n3 3 2 1\nP1\n3 1 2 1\n5 2 5 2\nP2\n2 1 0 3\n2 3 1 6\nP3\n1 3 1 2\n1 4 2 4\nP4\n1 4 3 2\n3 6 6 5",
  "2 3 1 6\nP3\n1 3 1 2\n1 4 2 4\nP4\n1 4 3 2\n3 6 6 5\nAnswer the following questions using the banker’s algorithm:\na.\nIllustrate that the system is in a safe state by demonstrating an\norder in which the processes may complete.\nb.\nIf a request from process P1 arrives for (1, 1, 0, 0), can the request\nbe granted immediately?\nc.\nIf a request from process P4 arrives for (0, 0, 2, 0), can the request\nbe granted immediately?\n7.24\nWhat is the optimistic assumption made in the deadlock-detection\nalgorithm? How can this assumption be violated?\n7.25\nA single-lane bridge connects the two Vermont villages of North\nTunbridge and South Tunbridge. Farmers in the two villages use this\nbridge to deliver their produce to the neighboring town. The bridge",
  "can become deadlocked if a northbound and a southbound farmer get\non the bridge at the same time. (Vermont farmers are stubborn and are\nunable to back up.) Using semaphores and/or mutex locks, design an\nalgorithm in pseudocode that prevents deadlock. Initially, do not be\nconcerned about starvation (the situation in which northbound farmers\nprevent southbound farmers from using the bridge, or vice versa).\n7.26\nModify your solution to Exercise 7.25 so that it is starvation-free.\nProgramming Problems\n7.27\nImplement your solution to Exercise 7.25 using POSIX synchronization.\nIn particular, represent northbound and southbound farmers as separate\nthreads. Once a farmer is on the bridge, the associated thread will sleep\nfor a random period of time, representing traveling across the bridge.",
  "Design your program so that you can create several threads representing\nthe northbound and southbound farmers. Programming Projects\n345\nProgramming Projects\nBanker’s Algorithm\nFor this project, you will write a multithreaded program that implements the\nbanker’s algorithm discussed in Section 7.5.3. Several customers request and\nrelease resources from the bank. The banker will grant a request only if it leaves\nthe system in a safe state. A request that leaves the system in an unsafe state\nwill be denied. This programming assignment combines three separate topics:\n(1) multithreading, (2) preventing race conditions, and (3) deadlock avoidance.\nThe Banker\nThe banker will consider requests from n customers for m resources types. as",
  "outlined in Section 7.5.3. The banker will keep track of the resources using the\nfollowing data structures:\n/* these may be any values >= 0 */\n#define NUMBER OF CUSTOMERS 5\n#define NUMBER OF RESOURCES 3\n/* the available amount of each resource */\nint available[NUMBER OF RESOURCES];\n/*the maximum demand of each customer */\nint maximum[NUMBER OF CUSTOMERS][NUMBER OF RESOURCES];\n/* the amount currently allocated to each customer */\nint allocation[NUMBER OF CUSTOMERS][NUMBER OF RESOURCES];\n/* the remaining need of each customer */\nint need[NUMBER OF CUSTOMERS][NUMBER OF RESOURCES];\nThe Customers\nCreate n customer threads that request and release resources from the bank.\nThe customers will continually loop, requesting and then releasing random",
  "numbers of resources. The customers’ requests for resources will be bounded\nby their respective values in the need array. The banker will grant a request if\nit satisﬁes the safety algorithm outlined in Section 7.5.3.1. If a request does not\nleave the system in a safe state, the banker will deny it. Function prototypes\nfor requesting and releasing resources are as follows:\nint request resources(int customer num, int request[]);\nint release resources(int customer num, int release[]);\nThese two functions should return 0 if successful (the request has been\ngranted) and –1 if unsuccessful. Multiple threads (customers) will concurrently 346\nChapter 7\nDeadlocks\naccess shared data through these two functions. Therefore, access must be",
  "controlled through mutex locks to prevent race conditions. Both the Pthreads\nand Windows APIs provide mutex locks. The use of Pthreads mutex locks is\ncovered in Section 5.9.4; mutex locks for Windows systems are described in the\nproject entitled “Producer–Consumer Problem” at the end of Chapter 5.\nImplementation\nYou should invoke your program by passing the number of resources of each\ntype on the command line. For example, if there were three resource types,\nwith ten instances of the ﬁrst type, ﬁve of the second type, and seven of the\nthird type, you would invoke your program follows:\n./a.out 10 5 7\nThe available array would be initialized to these values. You may initialize\nthe maximum array (which holds the maximum demand of each customer) using\nany method you ﬁnd convenient.",
  "any method you ﬁnd convenient.\nBibliographical Notes\nMost research involving deadlock was conducted many years ago. [Dijkstra\n(1965)] was one of the ﬁrst and most inﬂuential contributors in the deadlock\narea. [Holt (1972)] was the ﬁrst person to formalize the notion of deadlocks in\nterms of an allocation-graph model similar to the one presented in this chapter.\nStarvation was also covered by [Holt (1972)]. [Hyman (1985)] provided the\ndeadlock example from the Kansas legislature. A study of deadlock handling\nis provided in [Levine (2003)].\nThe various prevention algorithms were suggested by [Havender (1968)],\nwho devised the resource-ordering scheme for the IBM OS/360 system. The\nbanker’s algorithm for avoiding deadlocks was developed for a single resource",
  "type by [Dijkstra (1965)] and was extended to multiple resource types by\n[Habermann (1969)].\nThe deadlock-detection algorithm for multiple instances of a resource type,\nwhich is described in Section 7.6.2, was presented by [Coffman et al. (1971)].\n[Bach (1987)] describes how many of the algorithms in the traditional\nUNIX kernel handle deadlock. Solutions to deadlock problems in networks are\ndiscussed in works such as [Culler et al. (1998)] and [Rodeheffer and Schroeder\n(1991)].\nThe witness lock-order veriﬁer is presented in [Baldwin (2002)].\nBibliography\n[Bach (1987)]\nM. J. Bach, The Design of the UNIX Operating System, Prentice Hall\n(1987).\n[Baldwin (2002)]\nJ. Baldwin, “Locking in the Multithreaded FreeBSD Kernel”,\nUSENIX BSD (2002). Bibliography\n347\n[Coffman et al. (1971)]",
  "347\n[Coffman et al. (1971)]\nE. G. Coffman, M. J. Elphick, and A. Shoshani, “System\nDeadlocks”, Computing Surveys, Volume 3, Number 2 (1971), pages 67–78.\n[Culler et al. (1998)]\nD. E. Culler, J. P. Singh, and A. Gupta, Parallel Computer\nArchitecture: A Hardware/Software Approach, Morgan Kaufmann Publishers Inc.\n(1998).\n[Dijkstra (1965)]\nE. W. Dijkstra, “Cooperating Sequential Processes”, Technical\nreport, Technological University, Eindhoven, the Netherlands (1965).\n[Habermann (1969)]\nA. N. Habermann, “Prevention of System Deadlocks”,\nCommunications of the ACM, Volume 12, Number 7 (1969), pages 373–377, 385.\n[Havender (1968)]\nJ. W. Havender, “Avoiding Deadlock in Multitasking Sys-\ntems”, IBM Systems Journal, Volume 7, Number 2 (1968), pages 74–84.\n[Holt (1972)]",
  "[Holt (1972)]\nR. C. Holt, “Some Deadlock Properties of Computer Systems”,\nComputing Surveys, Volume 4, Number 3 (1972), pages 179–196.\n[Hyman (1985)]\nD. Hyman, The Columbus Chicken Statute and More Bonehead\nLegislation, S. Greene Press (1985).\n[Levine (2003)]\nG. Levine, “Deﬁning Deadlock”, Operating Systems Review, Vol-\nume 37, Number 1 (2003).\n[Rodeheffer and Schroeder (1991)]\nT. L. Rodeheffer and M. D. Schroeder,\n“Automatic Reconﬁguration in Autonet”, Proceedings of the ACM Symposium\non Operating Systems Principles (1991), pages 183–97.  Part Three\nMemory\nManagement\nThe main purpose of a computer system is to execute programs. These\nprograms, together with the data they access, must be at least partially\nin main memory during execution.",
  "in main memory during execution.\nTo improve both the utilization of the CPU and the speed of its\nresponse to users, a general-purpose computer must keep several pro-\ncesses in memory. Many memory-management schemes exist, reﬂect-\ning various approaches, and the effectiveness of each algorithm depends\non the situation. Selection of a memory-management scheme for a sys-\ntem depends on many factors, especially on the hardware design of the\nsystem. Most algorithms require hardware support.  8\nC H A P T E R\nMain Memory\nIn Chapter 6, we showed how the CPU can be shared by a set of processes. As\na result of CPU scheduling, we can improve both the utilization of the CPU and\nthe speed of the computer’s response to its users. To realize this increase in",
  "performance, however, we must keep several processes in memory—that is,\nwe must share memory.\nIn this chapter, we discuss various ways to manage memory. The memory-\nmanagement algorithms vary from a primitive bare-machine approach to\npaging and segmentation strategies. Each approach has its own advantages\nand disadvantages. Selection of a memory-management method for a speciﬁc\nsystem depends on many factors, especially on the hardware design of the\nsystem. As we shall see, many algorithms require hardware support, leading\nmany systems to have closely integrated hardware and operating-system\nmemory management.\nCHAPTER OBJECTIVES\n• To provide a detailed description of various ways of organizing memory\nhardware.\n• To explore various techniques of allocating memory to processes.",
  "• To discuss in detail how paging works in contemporary computer systems.\n8.1\nBackground\nAs we saw in Chapter 1, memory is central to the operation of a modern\ncomputer system. Memory consists of a large array of bytes, each with its own\naddress. The CPU fetches instructions from memory according to the value of\nthe program counter. These instructions may cause additional loading from\nand storing to speciﬁc memory addresses.\nA typical instruction-execution cycle, for example, ﬁrst fetches an instruc-\ntion from memory. The instruction is then decoded and may cause operands\nto be fetched from memory. After the instruction has been executed on the\noperands, results may be stored back in memory. The memory unit sees only\n351 352\nChapter 8\nMain Memory",
  "351 352\nChapter 8\nMain Memory\na stream of memory addresses; it does not know how they are generated (by\nthe instruction counter, indexing, indirection, literal addresses, and so on) or\nwhat they are for (instructions or data). Accordingly, we can ignore how a\nprogram generates a memory address. We are interested only in the sequence\nof memory addresses generated by the running program.\nWe begin our discussion by covering several issues that are pertinent\nto managing memory: basic hardware, the binding of symbolic memory\naddresses to actual physical addresses, and the distinction between logical\nand physical addresses. We conclude the section with a discussion of dynamic\nlinking and shared libraries.\n8.1.1\nBasic Hardware",
  "8.1.1\nBasic Hardware\nMain memory and the registers built into the processor itself are the only\ngeneral-purpose storage that the CPU can access directly. There are machine\ninstructions that take memory addresses as arguments, but none that take disk\naddresses. Therefore, any instructions in execution, and any data being used\nby the instructions, must be in one of these direct-access storage devices. If the\ndata are not in memory, they must be moved there before the CPU can operate\non them.\nRegisters that are built into the CPU are generally accessible within one\ncycle of the CPU clock. Most CPUs can decode instructions and perform simple\noperations on register contents at the rate of one or more operations per\nclock tick. The same cannot be said of main memory, which is accessed via",
  "a transaction on the memory bus. Completing a memory access may take\nmany cycles of the CPU clock. In such cases, the processor normally needs to\nstall, since it does not have the data required to complete the instruction that it\nis executing. This situation is intolerable because of the frequency of memory\naccesses. The remedy is to add fast memory between the CPUand main memory,\ntypically on the CPU chip for fast access. Such a cache was described in Section\n1.8.3. To manage a cache built into the CPU, the hardware automatically speeds\nup memory access without any operating-system control.\nNot only are we concerned with the relative speed of accessing physical\nmemory, but we also must ensure correct operation. For proper system",
  "operation we must protect the operating system from access by user processes.\nOn multiuser systems, we must additionally protect user processes from\none another. This protection must be provided by the hardware because the\noperating system doesn’t usually intervene between the CPU and its memory\naccesses (because of the resulting performance penalty). Hardware implements\nthis production in several different ways, as we show throughout the chapter.\nHere, we outline one possible implementation.\nWe ﬁrst need to make sure that each process has a separate memory space.\nSeparate per-process memory space protects the processes from each other and\nis fundamental to having multiple processes loaded in memory for concurrent\nexecution. To separate memory spaces, we need the ability to determine the",
  "range of legal addresses that the process may access and to ensure that the\nprocess can access only these legal addresses. We can provide this protection\nby using two registers, usually a base and a limit, as illustrated in Figure 8.1.\nThe base register holds the smallest legal physical memory address; the limit\nregister speciﬁes the size of the range. For example, if the base register holds 8.1\nBackground\n353\noperating\nsystem\n0\n256000\n300040\n300040\nbase\n120900\nlimit\n420940\n880000\n1024000\nprocess\nprocess\nprocess\nFigure 8.1\nA base and a limit register deﬁne a logical address space.\n300040 and the limit register is 120900, then the program can legally access all\naddresses from 300040 through 420939 (inclusive).\nProtection of memory space is accomplished by having the CPU hardware",
  "compare every address generated in user mode with the registers. Any attempt\nby a program executing in user mode to access operating-system memory or\nother users’ memory results in a trap to the operating system, which treats the\nattempt as a fatal error (Figure 8.2). This scheme prevents a user program from\n(accidentally or deliberately) modifying the code or data structures of either\nthe operating system or other users.\nThe base and limit registers can be loaded only by the operating system,\nwhich uses a special privileged instruction. Since privileged instructions can\nbe executed only in kernel mode, and since only the operating system executes\nin kernel mode, only the operating system can load the base and limit registers.\nbase\nmemory\ntrap to operating system\nmonitor—addressing error",
  "trap to operating system\nmonitor—addressing error\naddress\nyes\nyes\nno\nno\nCPU\nbase ' limit\n≥\n<\nFigure 8.2\nHardware address protection with base and limit registers. 354\nChapter 8\nMain Memory\nThis scheme allows the operating system to change the value of the registers\nbut prevents user programs from changing the registers’ contents.\nThe operating system, executing in kernel mode, is given unrestricted\naccess to both operating-system memory and users’ memory. This provision\nallows the operating system to load users’ programs into users’ memory, to\ndump out those programs in case of errors, to access and modify parameters\nof system calls, to perform I/O to and from user memory, and to provide\nmany other services. Consider, for example, that an operating system for a",
  "multiprocessing system must execute context switches, storing the state of one\nprocess from the registers into main memory before loading the next process’s\ncontext from main memory into the registers.\n8.1.2\nAddress Binding\nUsually, a program resides on a disk as a binary executable ﬁle. To be executed,\nthe program must be brought into memory and placed within a process.\nDepending on the memory management in use, the process may be moved\nbetween disk and memory during its execution. The processes on the disk that\nare waiting to be brought into memory for execution form the input queue.\nThe normal single-tasking procedure is to select one of the processes\nin the input queue and to load that process into memory. As the process",
  "is executed, it accesses instructions and data from memory. Eventually, the\nprocess terminates, and its memory space is declared available.\nMost systems allow a user process to reside in any part of the physical\nmemory. Thus, although the address space of the computer may start at 00000,\nthe ﬁrst address of the user process need not be 00000. You will see later how\na user program actually places a process in physical memory.\nIn most cases, a user program goes through several steps—some of which\nmay be optional—before being executed (Figure 8.3). Addresses may be\nrepresented in different ways during these steps. Addresses in the source\nprogram are generally symbolic (such as the variable count). A compiler\ntypically binds these symbolic addresses to relocatable addresses (such as",
  "“14 bytes from the beginning of this module”). The linkage editor or loader\nin turn binds the relocatable addresses to absolute addresses (such as 74014).\nEach binding is a mapping from one address space to another.\nClassically, the binding of instructions and data to memory addresses can\nbe done at any step along the way:\n• Compile time. If you know at compile time where the process will reside\nin memory, then absolute code can be generated. For example, if you know\nthat a user process will reside starting at location R, then the generated\ncompiler code will start at that location and extend up from there. If, at\nsome later time, the starting location changes, then it will be necessary\nto recompile this code. The MS-DOS .COM-format programs are bound at\ncompile time.",
  "compile time.\n• Load time. If it is not known at compile time where the process will reside\nin memory, then the compiler must generate relocatable code. In this case,\nﬁnal binding is delayed until load time. If the starting address changes, we\nneed only reload the user code to incorporate this changed value. 8.1\nBackground\n355\ndynamic\nlinking\nsource\nprogram\nobject\nmodule\nlinkage\neditor\nload\nmodule\nloader\nin-memory\nbinary\nmemory\nimage\nother\nobject\nmodules\ncompile\ntime\nload\ntime\nexecution\ntime (run\ntime)\ncompiler or\nassembler\nsystem\nlibrary\ndynamically\nloaded\nsystem\nlibrary\nFigure 8.3\nMultistep processing of a user program.\n• Execution time. If the process can be moved during its execution from\none memory segment to another, then binding must be delayed until run",
  "time. Special hardware must be available for this scheme to work, as will\nbe discussed in Section 8.1.3. Most general-purpose operating systems use\nthis method.\nA major portion of this chapter is devoted to showing how these various bind-\nings can be implemented effectively in a computer system and to discussing\nappropriate hardware support.\n8.1.3\nLogical Versus Physical Address Space\nAn address generated by the CPU is commonly referred to as a logical address,\nwhereas an address seen by the memory unit—that is, the one loaded into\nthe memory-address register of the memory—is commonly referred to as a\nphysical address.\nThe compile-time and load-time address-binding methods generate iden-\ntical logical and physical addresses. However, the execution-time address- 356\nChapter 8\nMain Memory\n'",
  "Chapter 8\nMain Memory\n'\nMMU\nCPU\nmemory\n14346\n14000\nrelocation\nregister\n346\nlogical\naddress\nphysical\naddress\nFigure 8.4\nDynamic relocation using a relocation register.\nbinding scheme results in differing logical and physical addresses. In this\ncase, we usually refer to the logical address as a virtual address. We use\nlogical address and virtual address interchangeably in this text. The set of all\nlogical addresses generated by a program is a logical address space. The set\nof all physical addresses corresponding to these logical addresses is a physical\naddress space. Thus, in the execution-time address-binding scheme, the logical\nand physical address spaces differ.\nThe run-time mapping from virtual to physical addresses is done by a",
  "hardware device called the memory-management unit (MMU). We can choose\nfrom many different methods to accomplish such mapping, as we discuss in\nSection 8.3 through Section 8.5. For the time being, we illustrate this mapping\nwith a simple MMU scheme that is a generalization of the base-register scheme\ndescribed in Section 8.1.1. The base register is now called a relocation register.\nThe value in the relocation register is added to every address generated by a\nuser process at the time the address is sent to memory (see Figure 8.4). For\nexample, if the base is at 14000, then an attempt by the user to address location\n0 is dynamically relocated to location 14000; an access to location 346 is mapped\nto location 14346.\nThe user program never sees the real physical addresses. The program can",
  "create a pointer to location 346, store it in memory, manipulate it, and compare it\nwith other addresses—all as the number 346. Only when it is used as a memory\naddress (in an indirect load or store, perhaps) is it relocated relative to the base\nregister. The user program deals with logical addresses. The memory-mapping\nhardware converts logical addresses into physical addresses. This form of\nexecution-time binding was discussed in Section 8.1.2. The ﬁnal location of\na referenced memory address is not determined until the reference is made.\nWe now have two different types of addresses: logical addresses (in the\nrange 0 to max) and physical addresses (in the range R + 0 to R + max for a base\nvalue R). The user program generates only logical addresses and thinks that",
  "the process runs in locations 0 to max. However, these logical addresses must\nbe mapped to physical addresses before they are used. The concept of a logical 8.1\nBackground\n357\naddress space that is bound to a separate physical address space is central to\nproper memory management.\n8.1.4\nDynamic Loading\nIn our discussion so far, it has been necessary for the entire program and all\ndata of a process to be in physical memory for the process to execute. The size\nof a process has thus been limited to the size of physical memory. To obtain\nbetter memory-space utilization, we can use dynamic loading. With dynamic\nloading, a routine is not loaded until it is called. All routines are kept on disk\nin a relocatable load format. The main program is loaded into memory and",
  "is executed. When a routine needs to call another routine, the calling routine\nﬁrst checks to see whether the other routine has been loaded. If it has not, the\nrelocatable linking loader is called to load the desired routine into memory and\nto update the program’s address tables to reﬂect this change. Then control is\npassed to the newly loaded routine.\nThe advantage of dynamic loading is that a routine is loaded only when it\nis needed. This method is particularly useful when large amounts of code are\nneeded to handle infrequently occurring cases, such as error routines. In this\ncase, although the total program size may be large, the portion that is used\n(and hence loaded) may be much smaller.\nDynamic loading does not require special support from the operating",
  "system. It is the responsibility of the users to design their programs to take\nadvantage of such a method. Operating systems may help the programmer,\nhowever, by providing library routines to implement dynamic loading.\n8.1.5\nDynamic Linking and Shared Libraries\nDynamically linked libraries are system libraries that are linked to user\nprograms when the programs are run (refer back to Figure 8.3). Some operating\nsystems support only static linking, in which system libraries are treated\nlike any other object module and are combined by the loader into the binary\nprogram image. Dynamic linking, in contrast, is similar to dynamic loading.\nHere, though, linking, rather than loading, is postponed until execution time.\nThis feature is usually used with system libraries, such as language subroutine",
  "libraries. Without this facility, each program on a system must include a copy\nof its language library (or at least the routines referenced by the program) in the\nexecutable image. This requirement wastes both disk space and main memory.\nWith dynamic linking, a stub is included in the image for each library-\nroutine reference. The stub is a small piece of code that indicates how to locate\nthe appropriate memory-resident library routine or how to load the library if\nthe routine is not already present. When the stub is executed, it checks to see\nwhether the needed routine is already in memory. If it is not, the program loads\nthe routine into memory. Either way, the stub replaces itself with the address\nof the routine and executes the routine. Thus, the next time that particular",
  "code segment is reached, the library routine is executed directly, incurring no\ncost for dynamic linking. Under this scheme, all processes that use a language\nlibrary execute only one copy of the library code.\nThis feature can be extended to library updates (such as bug ﬁxes). A library\nmay be replaced by a new version, and all programs that reference the library\nwill automatically use the new version. Without dynamic linking, all such 358\nChapter 8\nMain Memory\nprograms would need to be relinked to gain access to the new library. So that\nprograms will not accidentally execute new, incompatible versions of libraries,\nversion information is included in both the program and the library. More than\none version of a library may be loaded into memory, and each program uses its",
  "version information to decide which copy of the library to use. Versions with\nminor changes retain the same version number, whereas versions with major\nchanges increment the number. Thus, only programs that are compiled with\nthe new library version are affected by any incompatible changes incorporated\nin it. Other programs linked before the new library was installed will continue\nusing the older library. This system is also known as shared libraries.\nUnlike dynamic loading, dynamic linking and shared libraries generally\nrequire help from the operating system. If the processes in memory are\nprotected from one another, then the operating system is the only entity that can\ncheck to see whether the needed routine is in another process’s memory space",
  "or that can allow multiple processes to access the same memory addresses. We\nelaborate on this concept when we discuss paging in Section 8.5.4.\n8.2\nSwapping\nA process must be in memory to be executed. A process, however, can be\nswapped temporarily out of memory to a backing store and then brought back\ninto memory for continued execution (Figure 8.5). Swapping makes it possible\nfor the total physical address space of all processes to exceed the real physical\nmemory of the system, thus increasing the degree of multiprogramming in a\nsystem.\n8.2.1\nStandard Swapping\nStandard swapping involves moving processes between main memory and\na backing store. The backing store is commonly a fast disk. It must be large\noperating\nsystem\nswap out\nswap in\nuser\nspace\nmain memory\nbacking store\nprocess P2",
  "user\nspace\nmain memory\nbacking store\nprocess P2\nprocess P1\n1\n2\nFigure 8.5\nSwapping of two processes using a disk as a backing store. 8.2\nSwapping\n359\nenough to accommodate copies of all memory images for all users, and it must\nprovide direct access to these memory images. The system maintains a ready\nqueue consisting of all processes whose memory images are on the backing\nstore or in memory and are ready to run. Whenever the CPU scheduler decides\nto execute a process, it calls the dispatcher. The dispatcher checks to see whether\nthe next process in the queue is in memory. If it is not, and if there is no free\nmemory region, the dispatcher swaps out a process currently in memory and\nswaps in the desired process. It then reloads registers and transfers control to\nthe selected process.",
  "the selected process.\nThe context-switch time in such a swapping system is fairly high. To get an\nidea of the context-switch time, let’s assume that the user process is 100 MB in\nsize and the backing store is a standard hard disk with a transfer rate of 50 MB\nper second. The actual transfer of the 100-MB process to or from main memory\ntakes\n100 MB/50 MB per second = 2 seconds\nThe swap time is 200 milliseconds. Since we must swap both out and in, the\ntotal swap time is about 4,000 milliseconds. (Here, we are ignoring other disk\nperformance aspects, which we cover in Chapter 10.)\nNotice that the major part of the swap time is transfer time. The total\ntransfer time is directly proportional to the amount of memory swapped.\nIf we have a computer system with 4 GB of main memory and a resident",
  "operating system taking 1 GB, the maximum size of the user process is 3\nGB. However, many user processes may be much smaller than this—say, 100\nMB. A 100-MB process could be swapped out in 2 seconds, compared with\nthe 60 seconds required for swapping 3 GB. Clearly, it would be useful to\nknow exactly how much memory a user process is using, not simply how\nmuch it might be using. Then we would need to swap only what is actually\nused, reducing swap time. For this method to be effective, the user must\nkeep the system informed of any changes in memory requirements. Thus,\na process with dynamic memory requirements will need to issue system calls\n(request memory() and release memory()) to inform the operating system\nof its changing memory needs.",
  "of its changing memory needs.\nSwapping is constrained by other factors as well. If we want to swap\na process, we must be sure that it is completely idle. Of particular concern\nis any pending I/O. A process may be waiting for an I/O operation when\nwe want to swap that process to free up memory. However, if the I/O is\nasynchronously accessing the user memory for I/O buffers, then the process\ncannot be swapped. Assume that the I/O operation is queued because the\ndevice is busy. If we were to swap out process P1 and swap in process P2, the\nI/O operation might then attempt to use memory that now belongs to process\nP2. There are two main solutions to this problem: never swap a process with\npending I/O, or execute I/O operations only into operating-system buffers.",
  "Transfers between operating-system buffers and process memory then occur\nonly when the process is swapped in. Note that this double buffering itself\nadds overhead. We now need to copy the data again, from kernel memory to\nuser memory, before the user process can access it.\nStandard swapping is not used in modern operating systems. It requires too\nmuch swapping time and provides too little execution time to be a reasonable 360\nChapter 8\nMain Memory\nmemory-management solution. Modiﬁed versions of swapping, however, are\nfound on many systems, including UNIX, Linux, and Windows. In one common\nvariation, swapping is normally disabled but will start if the amount of free\nmemory (unused memory available for the operating system or processes to",
  "use) falls below a threshold amount. Swapping is halted when the amount\nof free memory increases. Another variation involves swapping portions of\nprocesses—rather than entire processes—to decrease swap time. Typically,\nthese modiﬁed forms of swapping work in conjunction with virtual memory,\nwhich we cover in Chapter 9.\n8.2.2\nSwapping on Mobile Systems\nAlthough most operating systems for PCs and servers support some modiﬁed\nversion of swapping, mobile systems typically do not support swapping in any\nform. Mobile devices generally use ﬂash memory rather than more spacious\nhard disks as their persistent storage. The resulting space constraint is one\nreason why mobile operating-system designers avoid swapping. Other reasons",
  "include the limited number of writes that ﬂash memory can tolerate before it\nbecomes unreliable and the poor throughput between main memory and ﬂash\nmemory in these devices.\nInstead of using swapping, when free memory falls below a certain\nthreshold, Apple’s iOS asks applications to voluntarily relinquish allocated\nmemory. Read-only data (such as code) are removed from the system and later\nreloaded from ﬂash memory if necessary. Data that have been modiﬁed (such\nas the stack) are never removed. However, any applications that fail to free up\nsufﬁcient memory may be terminated by the operating system.\nAndroid does not support swapping and adopts a strategy similar to that\nused by iOS. It may terminate a process if insufﬁcient free memory is available.",
  "However, before terminating a process, Android writes its application state to\nﬂash memory so that it can be quickly restarted.\nBecause of these restrictions, developers for mobile systems must carefully\nallocate and release memory to ensure that their applications do not use too\nmuch memory or suffer from memory leaks. Note that both iOS and Android\nsupport paging, so they do have memory-management abilities. We discuss\npaging later in this chapter.\n8.3\nContiguous Memory Allocation\nThe main memory must accommodate both the operating system and the\nvarious user processes. We therefore need to allocate main memory in the most\nefﬁcient way possible. This section explains one early method, contiguous\nmemory allocation.\nThe memory is usually divided into two partitions: one for the resident",
  "operating system and one for the user processes. We can place the operating\nsystem in either low memory or high memory. The major factor affecting this\ndecision is the location of the interrupt vector. Since the interrupt vector is\noften in low memory, programmers usually place the operating system in low\nmemory as well. Thus, in this text, we discuss only the situation in which 8.3\nContiguous Memory Allocation\n361\nthe operating system resides in low memory. The development of the other\nsituation is similar.\nWe usually want several user processes to reside in memory at the same\ntime. We therefore need to consider how to allocate available memory to the\nprocesses that are in the input queue waiting to be brought into memory. In",
  "contiguous memory allocation, each process is contained in a single section of\nmemory that is contiguous to the section containing the next process.\n8.3.1\nMemory Protection\nBefore discussing memory allocation further, we must discuss the issue of\nmemory protection. We can prevent a process from accessing memory it does\nnot own by combining two ideas previously discussed. If we have a system\nwith a relocation register (Section 8.1.3), together with a limit register (Section\n8.1.1), we accomplish our goal. The relocation register contains the value of\nthe smallest physical address; the limit register contains the range of logical\naddresses (for example, relocation = 100040 and limit = 74600). Each logical\naddress must fall within the range speciﬁed by the limit register. The MMU",
  "maps the logical address dynamically by adding the value in the relocation\nregister. This mapped address is sent to memory (Figure 8.6).\nWhen the CPU scheduler selects a process for execution, the dispatcher\nloads the relocation and limit registers with the correct values as part of the\ncontext switch. Because every address generated by a CPU is checked against\nthese registers, we can protect both the operating system and the other users’\nprograms and data from being modiﬁed by this running process.\nThe relocation-register scheme provides an effective way to allow the\noperating system’s size to change dynamically. This ﬂexibility is desirable in\nmany situations. For example, the operating system contains code and buffer",
  "space for device drivers. If a device driver (or other operating-system service)\nis not commonly used, we do not want to keep the code and data in memory, as\nwe might be able to use that space for other purposes. Such code is sometimes\ncalled transient operating-system code; it comes and goes as needed. Thus,\nusing this code changes the size of the operating system during program\nexecution.\nCPU\nmemory\nlogical\naddress\ntrap: addressing error\nno\nyes\nphysical\naddress\nrelocation\nregister\n'\n(\nlimit\nregister\nFigure 8.6\nHardware support for relocation and limit registers. 362\nChapter 8\nMain Memory\n8.3.2\nMemory Allocation\nNow we are ready to turn to memory allocation. One of the simplest\nmethods for allocating memory is to divide memory into several ﬁxed-sized",
  "partitions. Each partition may contain exactly one process. Thus, the degree\nof multiprogramming is bound by the number of partitions. In this multiple-\npartition method, when a partition is free, a process is selected from the input\nqueue and is loaded into the free partition. When the process terminates, the\npartition becomes available for another process. This method was originally\nused by the IBM OS/360 operating system (called MFT)but is no longer in use.\nThe method described next is a generalization of the ﬁxed-partition scheme\n(called MVT); it is used primarily in batch environments. Many of the ideas\npresented here are also applicable to a time-sharing environment in which\npure segmentation is used for memory management (Section 8.4).",
  "In the variable-partition scheme, the operating system keeps a table\nindicating which parts of memory are available and which are occupied.\nInitially, all memory is available for user processes and is considered one\nlarge block of available memory, a hole. Eventually, as you will see, memory\ncontains a set of holes of various sizes.\nAs processes enter the system, they are put into an input queue. The\noperating system takes into account the memory requirements of each process\nand the amount of available memory space in determining which processes are\nallocated memory. When a process is allocated space, it is loaded into memory,\nand it can then compete for CPU time. When a process terminates, it releases its\nmemory, which the operating system may then ﬁll with another process from",
  "the input queue.\nAt any given time, then, we have a list of available block sizes and an\ninput queue. The operating system can order the input queue according to\na scheduling algorithm. Memory is allocated to processes until, ﬁnally, the\nmemory requirements of the next process cannot be satisﬁed—that is, no\navailable block of memory (or hole) is large enough to hold that process. The\noperating system can then wait until a large enough block is available, or it can\nskip down the input queue to see whether the smaller memory requirements\nof some other process can be met.\nIn general, as mentioned, the memory blocks available comprise a set of\nholes of various sizes scattered throughout memory. When a process arrives",
  "and needs memory, the system searches the set for a hole that is large enough\nfor this process. If the hole is too large, it is split into two parts. One part is\nallocated to the arriving process; the other is returned to the set of holes. When\na process terminates, it releases its block of memory, which is then placed back\nin the set of holes. If the new hole is adjacent to other holes, these adjacent holes\nare merged to form one larger hole. At this point, the system may need to check\nwhether there are processes waiting for memory and whether this newly freed\nand recombined memory could satisfy the demands of any of these waiting\nprocesses.\nThis procedure is a particular instance of the general dynamic storage-\nallocation problem, which concerns how to satisfy a request of size n from a",
  "list of free holes. There are many solutions to this problem. The ﬁrst-ﬁt, best-ﬁt,\nand worst-ﬁt strategies are the ones most commonly used to select a free hole\nfrom the set of available holes. 8.3\nContiguous Memory Allocation\n363\n• First ﬁt. Allocate the ﬁrst hole that is big enough. Searching can start either\nat the beginning of the set of holes or at the location where the previous\nﬁrst-ﬁt search ended. We can stop searching as soon as we ﬁnd a free hole\nthat is large enough.\n• Best ﬁt. Allocate the smallest hole that is big enough. We must search the\nentire list, unless the list is ordered by size. This strategy produces the\nsmallest leftover hole.\n• Worst ﬁt. Allocate the largest hole. Again, we must search the entire list,",
  "unless it is sorted by size. This strategy produces the largest leftover hole,\nwhich may be more useful than the smaller leftover hole from a best-ﬁt\napproach.\nSimulations have shown that both ﬁrst ﬁt and best ﬁt are better than worst\nﬁt in terms of decreasing time and storage utilization. Neither ﬁrst ﬁt nor best\nﬁt is clearly better than the other in terms of storage utilization, but ﬁrst ﬁt is\ngenerally faster.\n8.3.3\nFragmentation\nBoth the ﬁrst-ﬁt and best-ﬁt strategies for memory allocation suffer from\nexternal fragmentation. As processes are loaded and removed from memory,\nthe free memory space is broken into little pieces. External fragmentation exists\nwhen there is enough total memory space to satisfy a request but the available",
  "spaces are not contiguous: storage is fragmented into a large number of small\nholes. This fragmentation problem can be severe. In the worst case, we could\nhave a block of free (or wasted) memory between every two processes. If all\nthese small pieces of memory were in one big free block instead, we might be\nable to run several more processes.\nWhether we are using the ﬁrst-ﬁt or best-ﬁt strategy can affect the amount\nof fragmentation. (First ﬁt is better for some systems, whereas best ﬁt is better\nfor others.) Another factor is which end of a free block is allocated. (Which is\nthe leftover piece—the one on the top or the one on the bottom?) No matter\nwhich algorithm is used, however, external fragmentation will be a problem.",
  "Depending on the total amount of memory storage and the average process\nsize, external fragmentation may be a minor or a major problem. Statistical\nanalysis of ﬁrst ﬁt, for instance, reveals that, even with some optimization,\ngiven N allocated blocks, another 0.5 N blocks will be lost to fragmentation.\nThat is, one-third of memory may be unusable! This property is known as the\n50-percent rule.\nMemory fragmentation can be internal as well as external. Consider a\nmultiple-partition allocation scheme with a hole of 18,464 bytes. Suppose that\nthe next process requests 18,462 bytes. If we allocate exactly the requested block,\nwe are left with a hole of 2 bytes. The overhead to keep track of this hole will be\nsubstantially larger than the hole itself. The general approach to avoiding this",
  "problem is to break the physical memory into ﬁxed-sized blocks and allocate\nmemory in units based on block size. With this approach, the memory allocated\nto a process may be slightly larger than the requested memory. The difference\nbetween these two numbers is internal fragmentation—unused memory that\nis internal to a partition. 364\nChapter 8\nMain Memory\nOne solution to the problem of external fragmentation is compaction. The\ngoal is to shufﬂe the memory contents so as to place all free memory together\nin one large block. Compaction is not always possible, however. If relocation\nis static and is done at assembly or load time, compaction cannot be done. It is\npossible only if relocation is dynamic and is done at execution time. If addresses",
  "are relocated dynamically, relocation requires only moving the program and\ndata and then changing the base register to reﬂect the new base address. When\ncompaction is possible, we must determine its cost. The simplest compaction\nalgorithm is to move all processes toward one end of memory; all holes move in\nthe other direction, producing one large hole of available memory. This scheme\ncan be expensive.\nAnother possible solution to the external-fragmentation problem is to\npermit the logical address space of the processes to be noncontiguous, thus\nallowing a process to be allocated physical memory wherever such memory is\navailable. Two complementary techniques achieve this solution: segmentation\n(Section 8.4) and paging (Section 8.5). These techniques can also be combined.",
  "Fragmentation is a general problem in computing that can occur wherever\nwe must manage blocks of data. We discuss the topic further in the storage\nmanagement chapters (Chapters 10 through and 12).\n8.4\nSegmentation\nAs we’ve already seen, the user’s view of memory is not the same as the actual\nphysical memory. This is equally true of the programmer’s view of memory.\nIndeed, dealing with memory in terms of its physical properties is inconvenient\nto both the operating system and the programmer. What if the hardware could\nprovide a memory mechanism that mapped the programmer’s view to the\nactual physical memory? The system would have more freedom to manage\nmemory, while the programmer would have a more natural programming\nenvironment. Segmentation provides such a mechanism.\n8.4.1\nBasic Method",
  "8.4.1\nBasic Method\nDo programmers think of memory as a linear array of bytes, some containing\ninstructions and others containing data? Most programmers would say “no.”\nRather, they prefer to view memory as a collection of variable-sized segments,\nwith no necessary ordering among the segments (Figure 8.7).\nWhen writing a program, a programmer thinks of it as a main program\nwith a set of methods, procedures, or functions. It may also include various data\nstructures: objects, arrays, stacks, variables, and so on. Each of these modules or\ndata elements is referred to by name. The programmer talks about “the stack,”\n“the math library,” and “the main program” without caring what addresses\nin memory these elements occupy. She is not concerned with whether the",
  "stack is stored before or after the Sqrt() function. Segments vary in length,\nand the length of each is intrinsically deﬁned by its purpose in the program.\nElements within a segment are identiﬁed by their offset from the beginning of\nthe segment: the ﬁrst statement of the program, the seventh stack frame entry\nin the stack, the ﬁfth instruction of the Sqrt(), and so on.\nSegmentation is a memory-management scheme that supports this pro-\ngrammer view of memory. A logical address space is a collection of segments. 8.4\nSegmentation\n365\nlogical address \nsubroutine\nstack\nsymbol \ntable\nmain \nprogram\nSqrt\nFigure 8.7\nProgrammer’s view of a program.\nEach segment has a name and a length. The addresses specify both the segment\nname and the offset within the segment. The programmer therefore speciﬁes",
  "each address by two quantities: a segment name and an offset.\nFor simplicity of implementation, segments are numbered and are referred\nto by a segment number, rather than by a segment name. Thus, a logical address\nconsists of a two tuple:\n<segment-number, offset>.\nNormally, when a program is compiled, the compiler automatically constructs\nsegments reﬂecting the input program.\nA C compiler might create separate segments for the following:\n1. The code\n2. Global variables\n3. The heap, from which memory is allocated\n4. The stacks used by each thread\n5. The standard C library\nLibraries that are linked in during compile time might be assigned separate\nsegments. The loader would take all these segments and assign them segment\nnumbers.\n8.4.2\nSegmentation Hardware",
  "numbers.\n8.4.2\nSegmentation Hardware\nAlthough the programmer can now refer to objects in the program by a\ntwo-dimensional address, the actual physical memory is still, of course, a one-\ndimensional sequence of bytes. Thus, we must deﬁne an implementation to\nmap two-dimensional user-deﬁned addresses into one-dimensional physical 366\nChapter 8\nMain Memory\nCPU\nphysical memory\ns\nd\n<\n+\ntrap: addressing error\nno\nyes\nsegment  \ntable\nlimit base\ns\nFigure 8.8\nSegmentation hardware.\naddresses. This mapping is effected by a segment table. Each entry in the\nsegment table has a segment base and a segment limit. The segment base\ncontains the starting physical address where the segment resides in memory,\nand the segment limit speciﬁes the length of the segment.",
  "The use of a segment table is illustrated in Figure 8.8. A logical address\nconsists of two parts: a segment number, s, and an offset into that segment, d.\nThe segment number is used as an index to the segment table. The offset d of\nthe logical address must be between 0 and the segment limit. If it is not, we trap\nto the operating system (logical addressing attempt beyond end of segment).\nWhen an offset is legal, it is added to the segment base to produce the address\nin physical memory of the desired byte. The segment table is thus essentially\nan array of base–limit register pairs.\nAs an example, consider the situation shown in Figure 8.9. We have ﬁve\nsegments numbered from 0 through 4. The segments are stored in physical",
  "memory as shown. The segment table has a separate entry for each segment,\ngiving the beginning address of the segment in physical memory (or base) and\nthe length of that segment (or limit). For example, segment 2 is 400 bytes long\nand begins at location 4300. Thus, a reference to byte 53 of segment 2 is mapped\nonto location 4300 + 53 = 4353. A reference to segment 3, byte 852, is mapped to\n3200 (the base of segment 3) + 852 = 4052. A reference to byte 1222 of segment\n0 would result in a trap to the operating system, as this segment is only 1,000\nbytes long.\n8.5\nPaging\nSegmentation permits the physical address space of a process to be non-\ncontiguous. Paging is another memory-management scheme that offers this\nadvantage. However, paging avoids external fragmentation and the need for 8.5",
  "Paging\n367\nlogical address space\nsubroutine\nstack\nsymbol \ntable\nmain \nprogram\nSqrt\n1400\nphysical memory\n2400\n3200\nsegment 2\n4300\n4700\n5700\n6300\n6700\nsegment table\nlimit\n0 \n1 \n2 \n3 \n4\n1000 \n400 \n400 \n1100 \n1000\nbase\n1400 \n6300 \n4300 \n3200 \n4700\nsegment 0\nsegment 3\nsegment 4\nsegment 2\nsegment 1\nsegment 0\nsegment 3\nsegment 4\nsegment 1\nFigure 8.9\nExample of segmentation.\ncompaction, whereas segmentation does not. It also solves the considerable\nproblem of ﬁtting memory chunks of varying sizes onto the backing store.\nMost memory-management schemes used before the introduction of paging\nsuffered from this problem. The problem arises because, when code fragments\nor data residing in main memory need to be swapped out, space must be found",
  "on the backing store. The backing store has the same fragmentation problems\ndiscussed in connection with main memory, but access is much slower, so\ncompaction is impossible. Because of its advantages over earlier methods,\npaging in its various forms is used in most operating systems, from those for\nmainframes through those for smartphones. Paging is implemented through\ncooperation between the operating system and the computer hardware.\n8.5.1\nBasic Method\nThe basic method for implementing paging involves breaking physical mem-\nory into ﬁxed-sized blocks called frames and breaking logical memory into\nblocks of the same size called pages. When a process is to be executed, its\npages are loaded into any available memory frames from their source (a ﬁle",
  "system or the backing store). The backing store is divided into ﬁxed-sized\nblocks that are the same size as the memory frames or clusters of multiple\nframes. This rather simple idea has great functionality and wide ramiﬁcations.\nFor example, the logical address space is now totally separate from the physical\naddress space, so a process can have a logical 64-bit address space even though\nthe system has less than 264 bytes of physical memory.\nThe hardware support for paging is illustrated in Figure 8.10. Every address\ngenerated by the CPU is divided into two parts: a page number (p) and a page 368\nChapter 8\nMain Memory\nphysical\nmemory\nf\nlogical\naddress\npage table\nphysical\naddress\nCPU\np\np\nf\nd\nd\nf\nf0000 … 0000\nf1111 … 1111\nFigure 8.10\nPaging hardware.",
  "f1111 … 1111\nFigure 8.10\nPaging hardware.\noffset (d). The page number is used as an index into a page table. The page table\ncontains the base address of each page in physical memory. This base address\nis combined with the page offset to deﬁne the physical memory address that\nis sent to the memory unit. The paging model of memory is shown in Figure\n8.11.\npage 0\npage 1\npage 2\npage 3\nlogical\nmemory\npage 1\npage 3\npage 0\npage 2\nphysical\nmemory\npage table\nframe\nnumber\n1\n4\n3\n7\n0\n1\n2\n3\n0\n1\n2\n3\n4\n5\n6\n7\nFigure 8.11\nPaging model of logical and physical memory. 8.5\nPaging\n369\nThe page size (like the frame size) is deﬁned by the hardware. The size of a\npage is a power of 2, varying between 512 bytes and 1 GB per page, depending\non the computer architecture. The selection of a power of 2 as a page size",
  "makes the translation of a logical address into a page number and page offset\nparticularly easy. If the size of the logical address space is 2m, and a page size is\n2n bytes, then the high-order m −n bits of a logical address designate the page\nnumber, and the n low-order bits designate the page offset. Thus, the logical\naddress is as follows:\np\nd\npage number\npage offset\nm – n\nn\nwhere p is an index into the page table and d is the displacement within the\npage.\nAs a concrete (although minuscule) example, consider the memory in\nFigure 8.12. Here, in the logical address, n= 2 and m = 4. Using a page size\nof 4 bytes and a physical memory of 32 bytes (8 pages), we show how the\nprogrammer’s view of memory can be mapped into physical memory. Logical",
  "address 0 is page 0, offset 0. Indexing into the page table, we ﬁnd that page 0\nlogical memory\nphysical memory\npage table\ni\nj\nk\nl\nm\nn\no\np\na\nb\nc\nd\ne\nf\ng\nh\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\nl\nm\nn\no\np\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n0\n0\n4\n8\n12\n16\n20\n24\n28\n1\n2\n3\n5\n6\n1\n2\nFigure 8.12\nPaging example for a 32-byte memory with 4-byte pages. 370\nChapter 8\nMain Memory\nOBTAINING THE PAGE SIZE ON LINUX SYSTEMS\nOn a Linux system, the page size varies according to architecture, and\nthere are several ways of obtaining the page size. One approach is to use\nthe getpagesize() system call. Another strategy is to enter the following\ncommand on the command line:\ngetconf PAGESIZE\nEach of these techniques returns the page size as a number of bytes.",
  "is in frame 5. Thus, logical address 0 maps to physical address 20 [= (5 × 4) +\n0]. Logical address 3 (page 0, offset 3) maps to physical address 23 [= (5 × 4) +\n3]. Logical address 4 is page 1, offset 0; according to the page table, page 1 is\nmapped to frame 6. Thus, logical address 4 maps to physical address 24 [= (6\n× 4) + 0]. Logical address 13 maps to physical address 9.\nYou may have noticed that paging itself is a form of dynamic relocation.\nEvery logical address is bound by the paging hardware to some physical\naddress. Using paging is similar to using a table of base (or relocation) registers,\none for each frame of memory.\nWhen we use a paging scheme, we have no external fragmentation: any free\nframe can be allocated to a process that needs it. However, we may have some",
  "internal fragmentation. Notice that frames are allocated as units. If the memory\nrequirements of a process do not happen to coincide with page boundaries,\nthe last frame allocated may not be completely full. For example, if page size\nis 2,048 bytes, a process of 72,766 bytes will need 35 pages plus 1,086 bytes. It\nwill be allocated 36 frames, resulting in internal fragmentation of 2,048 −1,086\n= 962 bytes. In the worst case, a process would need n pages plus 1 byte. It\nwould be allocated n + 1 frames, resulting in internal fragmentation of almost\nan entire frame.\nIf process size is independent of page size, we expect internal fragmentation\nto average one-half page per process. This consideration suggests that small\npage sizes are desirable. However, overhead is involved in each page-table",
  "entry, and this overhead is reduced as the size of the pages increases. Also,\ndisk I/O is more efﬁcient when the amount data being transferred is larger\n(Chapter 10). Generally, page sizes have grown over time as processes, data\nsets, and main memory have become larger. Today, pages typically are between\n4 KB and 8 KB in size, and some systems support even larger page sizes. Some\nCPUs and kernels even support multiple page sizes. For instance, Solaris uses\npage sizes of 8 KB and 4 MB, depending on the data stored by the pages.\nResearchers are now developing support for variable on-the-ﬂy page size.\nFrequently, on a 32-bit CPU, each page-table entry is 4 bytes long, but that\nsize can vary as well. A 32-bit entry can point to one of 232 physical page frames.",
  "If frame size is 4 KB (212), then a system with 4-byte entries can address 244 bytes\n(or 16 TB) of physical memory. We should note here that the size of physical\nmemory in a paged memory system is different from the maximum logical size\nof a process. As we further explore paging, we introduce other information that\nmust be kept in the page-table entries. That information reduces the number 8.5\nPaging\n371\n(a)\nfree-frame list\n14\n13\n18\n20\n15\n13\n14\n15\n16\n17\n18\n19\n20\n21\npage 0 \npage 1 \npage 2 \npage 3\nnew process\n(b)\nfree-frame list\n15\n13 page 1\npage 0\npage 2\npage 3\n14\n15\n16\n17\n18\n19\n20\n21\npage 0 \npage 1 \npage 2 \npage 3\nnew process\nnew-process page table\n14\n0\n1\n2\n3\n13\n18\n20\nFigure 8.13\nFree frames (a) before allocation and (b) after allocation.",
  "of bits available to address page frames. Thus, a system with 32-bit page-table\nentries may address less physical memory than the possible maximum. A 32-bit\nCPU uses 32-bit addresses, meaning that a given process space can only be 232\nbytes (4 TB). Therefore, paging lets us use physical memory that is larger than\nwhat can be addressed by the CPU’s address pointer length.\nWhen a process arrives in the system to be executed, its size, expressed\nin pages, is examined. Each page of the process needs one frame. Thus, if the\nprocess requires n pages, at least n frames must be available in memory. If n\nframes are available, they are allocated to this arriving process. The ﬁrst page\nof the process is loaded into one of the allocated frames, and the frame number",
  "is put in the page table for this process. The next page is loaded into another\nframe, its frame number is put into the page table, and so on (Figure 8.13).\nAn important aspect of paging is the clear separation between the program-\nmer’s view of memory and the actual physical memory. The programmer views\nmemory as one single space, containing only this one program. In fact, the user\nprogram is scattered throughout physical memory, which also holds other\nprograms. The difference between the programmer’s view of memory and\nthe actual physical memory is reconciled by the address-translation hardware.\nThe logical addresses are translated into physical addresses. This mapping is\nhidden from the programmer and is controlled by the operating system. Notice",
  "that the user process by deﬁnition is unable to access memory it does not own.\nIt has no way of addressing memory outside of its page table, and the table\nincludes only those pages that the process owns.\nSince the operating system is managing physical memory, it must be aware\nof the allocation details of physical memory—which frames are allocated,\nwhich frames are available, how many total frames there are, and so on. This\ninformation is generally kept in a data structure called a frame table. The frame\ntable has one entry for each physical page frame, indicating whether the latter 372\nChapter 8\nMain Memory\nis free or allocated and, if it is allocated, to which page of which process or\nprocesses.\nIn addition, the operating system must be aware that user processes operate",
  "in user space, and all logical addresses must be mapped to produce physical\naddresses. If a user makes a system call (to do I/O, for example) and provides\nan address as a parameter (a buffer, for instance), that address must be mapped\nto produce the correct physical address. The operating system maintains a copy\nof the page table for each process, just as it maintains a copy of the instruction\ncounter and register contents. This copy is used to translate logical addresses to\nphysical addresses whenever the operating system must map a logical address\nto a physical address manually. It is also used by the CPU dispatcher to deﬁne\nthe hardware page table when a process is to be allocated the CPU. Paging\ntherefore increases the context-switch time.\n8.5.2\nHardware Support",
  "8.5.2\nHardware Support\nEach operating system has its own methods for storing page tables. Some\nallocate a page table for each process. A pointer to the page table is stored with\nthe other register values (like the instruction counter) in the process control\nblock. When the dispatcher is told to start a process, it must reload the user\nregisters and deﬁne the correct hardware page-table values from the stored user\npage table. Other operating systems provide one or at most a few page tables,\nwhich decreases the overhead involved when processes are context-switched.\nThe hardware implementation of the page table can be done in several\nways. In the simplest case, the page table is implemented as a set of dedicated",
  "registers. These registers should be built with very high-speed logic to make the\npaging-address translation efﬁcient. Every access to memory must go through\nthe paging map, so efﬁciency is a major consideration. The CPU dispatcher\nreloads these registers, just as it reloads the other registers. Instructions to load\nor modify the page-table registers are, of course, privileged, so that only the\noperating system can change the memory map. The DEC PDP-11 is an example\nof such an architecture. The address consists of 16 bits, and the page size is 8\nKB. The page table thus consists of eight entries that are kept in fast registers.\nThe use of registers for the page table is satisfactory if the page table is\nreasonably small (for example, 256 entries). Most contemporary computers,",
  "however, allow the page table to be very large (for example, 1 million entries).\nFor these machines, the use of fast registers to implement the page table is\nnot feasible. Rather, the page table is kept in main memory, and a page-table\nbase register (PTBR) points to the page table. Changing page tables requires\nchanging only this one register, substantially reducing context-switch time.\nThe problem with this approach is the time required to access a user\nmemory location. If we want to access location i, we must ﬁrst index into\nthe page table, using the value in the PTBR offset by the page number for i. This\ntask requires a memory access. It provides us with the frame number, which\nis combined with the page offset to produce the actual address. We can then",
  "access the desired place in memory. With this scheme, two memory accesses\nare needed to access a byte (one for the page-table entry, one for the byte). Thus,\nmemory access is slowed by a factor of 2. This delay would be intolerable under\nmost circumstances. We might as well resort to swapping! 8.5\nPaging\n373\nThe standard solution to this problem is to use a special, small, fast-\nlookup hardware cache called a translation look-aside buffer (TLB). The TLB\nis associative, high-speed memory. Each entry in the TLB consists of two parts:\na key (or tag) and a value. When the associative memory is presented with an\nitem, the item is compared with all keys simultaneously. If the item is found,\nthe corresponding value ﬁeld is returned. The search is fast; a TLB lookup in",
  "modern hardware is part of the instruction pipeline, essentially adding no\nperformance penalty. To be able to execute the search within a pipeline step,\nhowever, the TLB must be kept small. It is typically between 32 and 1,024 entries\nin size. Some CPUs implement separate instruction and data address TLBs. That\ncan double the number of TLB entries available, because those lookups occur\nin different pipeline steps. We can see in this development an example of the\nevolution of CPU technology: systems have evolved from having no TLBs to\nhaving multiple levels of TLBs, just as they have multiple levels of caches.\nThe TLB is used with page tables in the following way. The TLB contains\nonly a few of the page-table entries. When a logical address is generated by the",
  "CPU, its page number is presented to the TLB. If the page number is found, its\nframe number is immediately available and is used to access memory. As just\nmentioned, these steps are executed as part of the instruction pipeline within\nthe CPU, adding no performance penalty compared with a system that does\nnot implement paging.\nIf the page number is not in the TLB (known as a TLB miss), a memory\nreference to the page table must be made. Depending on the CPU, this may be\ndone automatically in hardware or via an interrupt to the operating system.\nWhen the frame number is obtained, we can use it to access memory (Figure\n8.14). In addition, we add the page number and frame number to the TLB, so\npage table\nf\nCPU\nlogical\naddress\np\nd\nf\nd\nphysical\naddress\nphysical\nmemory\np\nTLB miss\npage\nnumber",
  "address\nphysical\nmemory\np\nTLB miss\npage\nnumber\nframe\nnumber\nTLB hit\nTLB\nFigure 8.14\nPaging hardware with TLB. 374\nChapter 8\nMain Memory\nthat they will be found quickly on the next reference. If the TLB is already full\nof entries, an existing entry must be selected for replacement. Replacement\npolicies range from least recently used (LRU) through round-robin to random.\nSome CPUs allow the operating system to participate in LRU entry replacement,\nwhile others handle the matter themselves. Furthermore, some TLBs allow\ncertain entries to be wired down, meaning that they cannot be removed from\nthe TLB. Typically, TLB entries for key kernel code are wired down.\nSome TLBs store address-space identiﬁers (ASIDs) in each TLB entry. An",
  "ASID uniquely identiﬁes each process and is used to provide address-space\nprotection for that process. When the TLB attempts to resolve virtual page\nnumbers, it ensures that the ASID for the currently running process matches the\nASID associated with the virtual page. If the ASIDs do not match, the attempt is\ntreated asa TLBmiss. Inadditiontoprovidingaddress-space protection, an ASID\nallows the TLB to contain entries for several different processes simultaneously.\nIf the TLB does not support separate ASIDs, then every time a new page table\nis selected (for instance, with each context switch), the TLB must be ﬂushed\n(or erased) to ensure that the next executing process does not use the wrong\ntranslation information. Otherwise, the TLB could include old entries that",
  "contain valid virtual addresses but have incorrect or invalid physical addresses\nleft over from the previous process.\nThe percentage of times that the page number of interest is found in the\nTLB is called the hit ratio. An 80-percent hit ratio, for example, means that\nwe ﬁnd the desired page number in the TLB 80 percent of the time. If it takes\n100 nanoseconds to access memory, then a mapped-memory access takes 100\nnanoseconds when the page number is in the TLB. If we fail to ﬁnd the page\nnumber in the TLB then we must ﬁrst access memory for the page table and\nframe number (100 nanoseconds) and then access the desired byte in memory\n(100 nanoseconds), for a total of 200 nanoseconds. (We are assuming that a\npage-table lookup takes only one memory access, but it can take more, as we",
  "shall see.) To ﬁnd the effective memory-access time, we weight the case by its\nprobability:\neffective access time = 0.80 × 100 + 0.20 × 200\n= 120 nanoseconds\nIn this example, we suffer a 20-percent slowdown in average memory-access\ntime (from 100 to 120 nanoseconds).\nFor a 99-percent hit ratio, which is much more realistic, we have\neffective access time = 0.99 × 100 + 0.01 × 200\n= 101 nanoseconds\nThis increased hit rate produces only a 1 percent slowdown in access time.\nAs we noted earlier, CPUs today may provide multiple levels of TLBs.\nCalculating memory access times in modern CPUs is therefore much more\ncomplicated than shown in the example above. For instance, the Intel Core\ni7 CPU has a 128-entry L1 instruction TLB and a 64-entry L1 data TLB. In the",
  "case of a miss at L1, it takes the CPU six cycles to check for the entry in the L2\n512-entry TLB. A miss in L2 means that the CPU must either walk through the 8.5\nPaging\n375\npage-table entries in memory to ﬁnd the associated frame address, which can\ntake hundreds of cycles, or interrupt to the operating system to have it do the\nwork.\nA complete performance analysis of paging overhead in such a system\nwould require miss-rate information about each TLB tier. We can see from the\ngeneral information above, however, that hardware features can have a signif-\nicant effect on memory performance and that operating-system improvements\n(such as paging) can result in and, in turn, be affected by hardware changes\n(such as TLBs). We will further explore the impact of the hit ratio on the TLB in",
  "Chapter 9.\nTLBs are a hardware feature and therefore would seem to be of little concern\nto operating systems and their designers. But the designer needs to understand\nthe function and features of TLBs, which vary by hardware platform. For\noptimal operation, an operating-system design for a given platform must\nimplement paging according to the platform’s TLBdesign. Likewise, a change in\nthe TLB design (for example, between generations of Intel CPUs) may necessitate\na change in the paging implementation of the operating systems that use it.\n8.5.3\nProtection\nMemory protection in a paged environment is accomplished by protection bits\nassociated with each frame. Normally, these bits are kept in the page table.\nOne bit can deﬁne a page to be read–write or read-only. Every reference",
  "to memory goes through the page table to ﬁnd the correct frame number. At\nthe same time that the physical address is being computed, the protection bits\ncan be checked to verify that no writes are being made to a read-only page. An\nattempt to write to a read-only page causes a hardware trap to the operating\nsystem (or memory-protection violation).\nWe can easily expand this approach to provide a ﬁner level of protection.\nWe can create hardware to provide read-only, read–write, or execute-only\nprotection; or, by providing separate protection bits for each kind of access, we\ncan allow any combination of these accesses. Illegal attempts will be trapped\nto the operating system.\nOne additional bit is generally attached to each entry in the page table: a",
  "valid–invalid bit. When this bit is set to valid, the associated page is in the\nprocess’s logical address space and is thus a legal (or valid) page. When the\nbit is set toinvalid, the page is not in the process’s logical address space. Illegal\naddresses are trapped by use of the valid–invalid bit. The operating system\nsets this bit for each page to allow or disallow access to the page.\nSuppose, for example, that in a system with a 14-bit address space (0 to\n16383), we have a program that should use only addresses 0 to 10468. Given\na page size of 2 KB, we have the situation shown in Figure 8.15. Addresses in\npages 0, 1, 2, 3, 4, and 5 are mapped normally through the page table. Any\nattempt to generate an address in pages 6 or 7, however, will ﬁnd that the",
  "valid–invalid bit is set to invalid, and the computer will trap to the operating\nsystem (invalid page reference).\nNotice that this scheme has created a problem. Because the program\nextends only to address 10468, any reference beyond that address is illegal.\nHowever, references to page 5 are classiﬁed as valid, so accesses to addresses\nup to 12287 are valid. Only the addresses from 12288 to 16383 are invalid. This 376\nChapter 8\nMain Memory\npage 0\npage 0\npage 1\npage 2\npage 3\npage 4\npage 5\npage n\n•••\n00000\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nframe number\n0\n1\n2\n3\n4\n5\n6\n7\n2\n3\n4\n7\n8\n9\n0\n0\nv\nv\nv\nv\nv\nv\ni\ni\npage table\nvalid–invalid bit\n10,468\n12,287\npage 1\npage 2\npage 3\npage 4\npage 5\nFigure 8.15\nValid (v) or invalid (i) bit in a page table.",
  "Valid (v) or invalid (i) bit in a page table.\nproblem is a result of the 2-KB page size and reﬂects the internal fragmentation\nof paging.\nRarely does a process use all its address range. In fact, many processes\nuse only a small fraction of the address space available to them. It would be\nwasteful in these cases to create a page table with entries for every page in the\naddress range. Most of this table would be unused but would take up valuable\nmemory space. Some systems provide hardware, in the form of a page-table\nlength register (PTLR), to indicate the size of the page table. This value is\nchecked against every logical address to verify that the address is in the valid\nrange for the process. Failure of this test causes an error trap to the operating\nsystem.\n8.5.4\nShared Pages",
  "system.\n8.5.4\nShared Pages\nAn advantage of paging is the possibility of sharing common code. This con-\nsideration is particularly important in a time-sharing environment. Consider a\nsystem that supports 40 users, each of whom executes a text editor. If the text\neditor consists of 150 KB of code and 50 KB of data space, we need 8,000 KB to\nsupport the 40 users. If the code is reentrant code (or pure code), however, it\ncan be shared, as shown in Figure 8.16. Here, we see three processes sharing\na three-page editor—each page 50 KB in size (the large page size is used to\nsimplify the ﬁgure). Each process has its own data page.\nReentrant code is non-self-modifying code: it never changes during execu-\ntion. Thus, two or more processes can execute the same code at the same time. 8.5\nPaging\n377\n7",
  "Paging\n377\n7\n6\n5\ned 2\n4\ned 1\n3\n2\ndata 1\n1\n0\n3\n4\n6\n1\npage table\nfor P1\nprocess P1\ndata 1\ned 2\ned 3\ned 1\n3\n4\n6\n2\npage table\nfor P3\nprocess P3\ndata 3\ned 2\ned 3\ned 1\n3\n4\n6\n7\npage table\nfor P2\nprocess P2\ndata 2\ned 2\ned 3\ned 1\n8\n9\n10\n11\ndata 3\n2\ndata \ned 3\nFigure 8.16\nSharing of code in a paging environment.\nEach process has its own copy of registers and data storage to hold the data for\nthe process’s execution. The data for two different processes will, of course, be\ndifferent.\nOnly one copy of the editor need be kept in physical memory. Each user’s\npage table maps onto the same physical copy of the editor, but data pages are\nmapped onto different frames. Thus, to support 40 users, we need only one\ncopy of the editor (150 KB), plus 40 copies of the 50 KB of data space per user.",
  "The total space required is now 2,150 KB instead of 8,000 KB—a signiﬁcant\nsavings.\nOther heavily used programs can also be shared—compilers, window\nsystems, run-time libraries, database systems, and so on. To be sharable, the\ncode must be reentrant. The read-only nature of shared code should not be\nleft to the correctness of the code; the operating system should enforce this\nproperty.\nThe sharing of memory among processes on a system is similar to the\nsharing of the address space of a task by threads, described in Chapter 4.\nFurthermore, recall that in Chapter 3 we described shared memory as a method\nof interprocess communication. Some operating systems implement shared\nmemory using shared pages.\nOrganizing memory according to pages provides numerous beneﬁts in",
  "addition to allowing several processes to share the same physical pages. We\ncover several other beneﬁts in Chapter 9. 378\nChapter 8\nMain Memory\n8.6\nStructure of the Page Table\nIn this section, we explore some of the most common techniques for structuring\nthe page table, including hierarchical paging, hashed page tables, and inverted\npage tables.\n8.6.1\nHierarchical Paging\nMost modern computer systems support a large logical address space\n(232 to 264). In such an environment, the page table itself becomes excessively\nlarge. For example, consider a system with a 32-bit logical address space. If\nthe page size in such a system is 4 KB (212), then a page table may consist of\nup to 1 million entries (232/212). Assuming that each entry consists of 4 bytes,",
  "each process may need up to 4 MB of physical address space for the page table\nalone. Clearly, we would not want to allocate the page table contiguously in\nmain memory. One simple solution to this problem is to divide the page table\ninto smaller pieces. We can accomplish this division in several ways.\nOne way is to use a two-level paging algorithm, in which the page table\nitself is also paged (Figure 8.17). For example, consider again the system with\na 32-bit logical address space and a page size of 4 KB. A logical address is\ndivided into a page number consisting of 20 bits and a page offset consisting\nof 12 bits. Because we page the page table, the page number is further divided\n•••\n•••\nouter page\ntable\npage of\npage table\npage table\nmemory\n929\n900\n929\n900\n708\n500\n100\n1\n0\n•••\n100\n708\n•••",
  "929\n900\n929\n900\n708\n500\n100\n1\n0\n•••\n100\n708\n•••\n•••\n•••\n•••\n•••\n•••\n•••\n•••\n•••\n1\n500\nFigure 8.17\nA two-level page-table scheme. 8.6\nStructure of the Page Table\n379\nlogical address\nouter page\ntable\np1\np2\np1\npage of\npage table\np2\nd\nd\nFigure 8.18\nAddress translation for a two-level 32-bit paging architecture.\ninto a 10-bit page number and a 10-bit page offset. Thus, a logical address is as\nfollows:\np1\np2\nd\npage number\npage offset\n10\n10\n12\nwhere p1 is an index into the outer page table and p2 is the displacement\nwithin the page of the inner page table. The address-translation method for this\narchitecture is shown in Figure 8.18. Because address translation works from\nthe outer page table inward, this scheme is also known as a forward-mapped\npage table.",
  "page table.\nConsider the memory management of one of the classic systems, the VAX\nminicomputer from Digital Equipment Corporation (DEC). The VAX was the\nmost popular minicomputer of its time and was sold from 1977 through 2000.\nThe VAX architecture supported a variation of two-level paging. The VAX is a 32-\nbit machine with a page size of 512 bytes. The logical address space of a process\nis divided into four equal sections, each of which consists of 230 bytes. Each\nsection represents a different part of the logical address space of a process. The\nﬁrst 2 high-order bits of the logical address designate the appropriate section.\nThe next 21 bits represent the logical page number of that section, and the ﬁnal\n9 bits represent an offset in the desired page. By partitioning the page table in",
  "this manner, the operating system can leave partitions unused until a process\nneeds them. Entire sections of virtual address space are frequently unused, and\nmultilevel page tables have no entries for these spaces, greatly decreasing the\namount of memory needed to store virtual memory data structures.\nAn address on the VAX architecture is as follows:\ns\np\nd\nsection\npage\noffset\n2\n21\n9\nwhere s designates the section number, p is an index into the page table, and d\nis the displacement within the page. Even when this scheme is used, the size\nof a one-level page table for a VAX process using one section is 221 bits ∗4 380\nChapter 8\nMain Memory\nbytes per entry = 8 MB. To further reduce main-memory use, the VAX pages the\nuser-process page tables.",
  "user-process page tables.\nFor a system with a 64-bit logical address space, a two-level paging scheme\nis no longer appropriate. To illustrate this point, let’s suppose that the page\nsize in such a system is 4 KB (212). In this case, the page table consists of up\nto 252 entries. If we use a two-level paging scheme, then the inner page tables\ncan conveniently be one page long, or contain 210 4-byte entries. The addresses\nlook like this:\np1\np2\nd\nouter page\ninner page\noffset\n42\n10\n12\nThe outer page table consists of 242 entries, or 244 bytes. The obvious way to\navoid such a large table is to divide the outer page table into smaller pieces.\n(This approach is also used on some 32-bit processors for added ﬂexibility and\nefﬁciency.)",
  "efﬁciency.)\nWe can divide the outer page table in various ways. For example, we can\npage the outer page table, giving us a three-level paging scheme. Suppose that\nthe outer page table is made up of standard-size pages (210 entries, or 212 bytes).\nIn this case, a 64-bit address space is still daunting:\np1\np2\np3\n2nd outer page\nouter page\ninner page\n32\n10\n10\nd\noffset\n12\nThe outer page table is still 234 bytes (16 GB) in size.\nThe next step would be a four-level paging scheme, where the second-level\nouter page table itself is also paged, and so forth. The 64-bit UltraSPARC would\nrequire seven levels of paging—a prohibitive number of memory accesses—\nto translate each logical address. You can see from this example why, for 64-bit",
  "architectures, hierarchical page tables are generally considered inappropriate.\n8.6.2\nHashed Page Tables\nA common approach for handling address spaces larger than 32 bits is to use\na hashed page table, with the hash value being the virtual page number. Each\nentry in the hash table contains a linked list of elements that hash to the same\nlocation (to handle collisions). Each element consists of three ﬁelds: (1) the\nvirtual page number, (2) the value of the mapped page frame, and (3) a pointer\nto the next element in the linked list.\nThe algorithm works as follows: The virtual page number in the virtual\naddress is hashed into the hash table. The virtual page number is compared\nwith ﬁeld 1 in the ﬁrst element in the linked list. If there is a match, the",
  "corresponding page frame (ﬁeld 2) is used to form the desired physical address.\nIf there is no match, subsequent entries in the linked list are searched for a\nmatching virtual page number. This scheme is shown in Figure 8.19.\nA variation of this scheme that is useful for 64-bit address spaces has\nbeen proposed. This variation uses clustered page tables, which are similar to 8.6\nStructure of the Page Table\n381\nhash table\nq\ns\nlogical address\nphysical\naddress\nphysical\nmemory\np\nd\nr\nd\np\nr\nhash\nfunction\n• • •\nFigure 8.19\nHashed page table.\nhashed page tables except that each entry in the hash table refers to several\npages (such as 16) rather than a single page. Therefore, a single page-table\nentry can store the mappings for multiple physical-page frames. Clustered",
  "page tables are particularly useful for sparse address spaces, where memory\nreferences are noncontiguous and scattered throughout the address space.\n8.6.3\nInverted Page Tables\nUsually, each process has an associated page table. The page table has one\nentry for each page that the process is using (or one slot for each virtual\naddress, regardless of the latter’s validity). This table representation is a natural\none, since processes reference pages through the pages’ virtual addresses. The\noperating system must then translate this reference into a physical memory\naddress. Since the table is sorted by virtual address, the operating system is\nable to calculate where in the table the associated physical address entry is\nlocated and to use that value directly. One of the drawbacks of this method",
  "is that each page table may consist of millions of entries. These tables may\nconsume large amounts of physical memory just to keep track of how other\nphysical memory is being used.\nTo solve this problem, we can use an inverted page table. An inverted\npage table has one entry for each real page (or frame) of memory. Each entry\nconsists of the virtual address of the page stored in that real memory location,\nwith information about the process that owns the page. Thus, only one page\ntable is in the system, and it has only one entry for each page of physical\nmemory. Figure 8.20 shows the operation of an inverted page table. Compare\nit with Figure 8.10, which depicts a standard page table in operation. Inverted\npage tables often require that an address-space identiﬁer (Section 8.5.2) be",
  "stored in each entry of the page table, since the table usually contains several\ndifferent address spaces mapping physical memory. Storing the address-space\nidentiﬁer ensures that a logical page for a particular process is mapped to the\ncorresponding physical page frame. Examples of systems using inverted page\ntables include the 64-bit UltraSPARC and PowerPC. 382\nChapter 8\nMain Memory\npage table\nCPU\nlogical\naddress\nphysical\naddress\nphysical\nmemory\ni\npid\np\npid\nsearch\np\nd\ni\nd\nFigure 8.20\nInverted page table.\nTo illustrate this method, we describe a simpliﬁed version of the inverted\npage table used in the IBM RT. IBM was the ﬁrst major company to use inverted\npage tables, starting with the IBM System 38 and continuing through the",
  "RS/6000 and the current IBM Power CPUs. For the IBM RT, each virtual address\nin the system consists of a triple:\n<process-id, page-number, offset>.\nEach inverted page-table entry is a pair <process-id, page-number> where the\nprocess-id assumes the role of the address-space identiﬁer. When a memory\nreference occurs, part of the virtual address, consisting of <process-id, page-\nnumber>, is presented to the memory subsystem. The inverted page table\nis then searched for a match. If a match is found—say, at entry i—then the\nphysical address <i, offset> is generated. If no match is found, then an illegal\naddress access has been attempted.\nAlthough this scheme decreases the amount of memory needed to store\neach page table, it increases the amount of time needed to search the table when",
  "a page reference occurs. Because the inverted page table is sorted by physical\naddress, but lookups occur on virtual addresses, the whole table might need\nto be searched before a match is found. This search would take far too long.\nTo alleviate this problem, we use a hash table, as described in Section 8.6.2,\nto limit the search to one—or at most a few—page-table entries. Of course,\neach access to the hash table adds a memory reference to the procedure, so one\nvirtual memory reference requires at least two real memory reads—one for the\nhash-table entry and one for the page table. (Recall that the TLB is searched ﬁrst,\nbefore the hash table is consulted, offering some performance improvement.)\nSystems that use inverted page tables have difﬁculty implementing shared",
  "memory. Shared memory is usually implemented as multiple virtual addresses\n(one for each process sharing the memory) that are mapped to one physical\naddress. This standard method cannot be used with inverted page tables;\nbecause there is only one virtual page entry for every physical page, one 8.7\nExample: Intel 32 and 64-bit Architectures\n383\nphysical page cannot have two (or more) shared virtual addresses. A simple\ntechnique for addressing this issue is to allow the page table to contain only\none mapping of a virtual address to the shared physical address. This means\nthat references to virtual addresses that are not mapped result in page faults.\n8.6.4\nOracle SPARC Solaris\nConsider as a ﬁnal example a modern 64-bit CPU and operating system that are",
  "tightly integrated to provide low-overhead virtual memory. Solaris running\non the SPARC CPU is a fully 64-bit operating system and as such has to solve\nthe problem of virtual memory without using up all of its physical memory\nby keeping multiple levels of page tables. Its approach is a bit complex but\nsolves the problem efﬁciently using hashed page tables. There are two hash\ntables—one for the kernel and one for all user processes. Each maps memory\naddresses from virtual to physical memory. Each hash-table entry represents a\ncontiguous area of mapped virtual memory, which is more efﬁcient than having\na separate hash-table entry for each page. Each entry has a base address and a\nspan indicating the number of pages the entry represents.",
  "Virtual-to-physical translation would take too long if each address required\nsearching through a hash table, so the CPU implements a TLB that holds\ntranslation table entries (TTEs) for fast hardware lookups. A cache of these TTEs\nreside in a translation storage buffer (TSB), which includes an entry per recently\naccessed page. When a virtual address reference occurs, the hardware searches\nthe TLB for a translation. If none is found, the hardware walks through the\nin-memory TSB looking for the TTE that corresponds to the virtual address that\ncaused the lookup. This TLB walk functionality is found on many modern CPUs.\nIf a match is found in the TSB, the CPU copies the TSB entry into the TLB, and\nthe memory translation completes. If no match is found in the TSB, the kernel",
  "is interrupted to search the hash table. The kernel then creates a TTE from the\nappropriate hash table and stores it in the TSB for automatic loading into the TLB\nby the CPU memory-management unit. Finally, the interrupt handler returns\ncontrol to the MMU, which completes the address translation and retrieves the\nrequested byte or word from main memory.\n8.7\nExample: Intel 32 and 64-bit Architectures\nThe architecture of Intel chips has dominated the personal computer landscape\nfor several years. The 16-bit Intel 8086 appeared in the late 1970s and was soon\nfollowed by another 16-bit chip—the Intel 8088—which was notable for being\nthe chip used in the original IBM PC. Both the 8086 chip and the 8088 chip were\nbased on a segmented architecture. Intel later produced a series of 32-bit chips",
  "—the IA-32—which included the family of 32-bit Pentium processors. The\nIA-32 architecture supported both paging and segmentation. More recently,\nIntel has produced a series of 64-bit chips based on the x86-64 architecture.\nCurrently, all the most popular PC operating systems run on Intel chips,\nincluding Windows, Mac OS X, and Linux (although Linux, of course, runs\non several other architectures as well). Notably, however, Intel’s dominance\nhas not spread to mobile systems, where the ARM architecture currently enjoys\nconsiderable success (see Section 8.8). 384\nChapter 8\nMain Memory\nCPU\nlogical\naddress\nsegmentation\nunit\nlinear\naddress\npaging\nunit\nphysical\naddress\nphysical\nmemory\nFigure 8.21\nLogical to physical address translation in IA-32.",
  "Logical to physical address translation in IA-32.\nIn this section, we examine address translation for both IA-32 and x86-64\narchitectures. Before we proceed, however, it is important to note that because\nIntel has released several versions—as well as variations—of its architectures\nover the years, we cannot provide a complete description of the memory-\nmanagement structure of all its chips. Nor can we provide all of the CPU details,\nas that information is best left to books on computer architecture. Rather, we\npresent the major memory-management concepts of these Intel CPUs.\n8.7.1\nIA-32 Architecture\nMemory management in IA-32 systems is divided into two components—\nsegmentation and paging—and works as follows: The CPU generates logical",
  "addresses, which are given to the segmentation unit. The segmentation unit\nproduces a linear address for each logical address. The linear address is then\ngiven to the paging unit, which in turn generates the physical address in main\nmemory. Thus, the segmentation and paging units form the equivalent of the\nmemory-management unit (MMU). This scheme is shown in Figure 8.21.\n8.7.1.1\nIA-32 Segmentation\nThe IA-32 architecture allows a segment to be as large as 4 GB, and the maximum\nnumber of segments per process is 16 K. The logical address space of a process is\ndivided into two partitions. The ﬁrst partition consists of up to 8 Ksegments that\nare private to that process. The second partition consists of up to 8 K segments",
  "that are shared among all the processes. Information about the ﬁrst partition is\nkept in the local descriptor table (LDT); information about the second partition\nis kept in the global descriptor table (GDT). Each entry in the LDT and GDT\nconsists of an 8-byte segment descriptor with detailed information about a\nparticular segment, including the base location and limit of that segment.\nThe logical address is a pair (selector, offset), where the selector is a 16-bit\nnumber:\np\n2\ng\n1\ns\n13\nin which s designates the segment number, g indicates whether the segment is\nin the GDT or LDT, and p deals with protection. The offset is a 32-bit number\nspecifying the location of the byte within the segment in question.\nThe machine has six segment registers, allowing six segments to be",
  "addressed at any one time by a process. It also has six 8-byte microprogram\nregisters to hold the corresponding descriptors from either the LDT or GDT.\nThis cache lets the Pentium avoid having to read the descriptor from memory\nfor every memory reference. 8.7\nExample: Intel 32 and 64-bit Architectures\n385\nlogical address\nselector\ndescriptor table\nsegment descriptor\n+\n32-bit linear address\noffset\nFigure 8.22\nIA-32 segmentation.\nThe linear address on the IA-32 is 32 bits long and is formed as follows.\nThe segment register points to the appropriate entry in the LDT or GDT. The\nbase and limit information about the segment in question is used to generate\na linear address. First, the limit is used to check for address validity. If the",
  "address is not valid, a memory fault is generated, resulting in a trap to the\noperating system. If it is valid, then the value of the offset is added to the value\nof the base, resulting in a 32-bit linear address. This is shown in Figure 8.22. In\nthe following section, we discuss how the paging unit turns this linear address\ninto a physical address.\n8.7.1.2\nIA-32 Paging\nThe IA-32 architecture allows a page size of either 4 KB or 4 MB. For 4-KB pages,\nIA-32 uses a two-level paging scheme in which the division of the 32-bit linear\naddress is as follows:\np1\np2\nd\npage number\npage offset\n10\n10\n12\nThe address-translation scheme for this architecture is similar to the scheme\nshown in Figure 8.18. The IA-32 address translation is shown in more detail in",
  "Figure 8.23. The 10 high-order bits reference an entry in the outermost page\ntable, which IA-32 terms the page directory. (The CR3 register points to the\npage directory for the current process.) The page directory entry points to an\ninner page table that is indexed by the contents of the innermost 10 bits in the\nlinear address. Finally, the low-order bits 0–11 refer to the offset in the 4-KB\npage pointed to in the page table.\nOne entry in the page directory is the Page Size ﬂag, which—if set—\nindicates that the size of the page frame is 4 MB and not the standard 4 KB.\nIf this ﬂag is set, the page directory points directly to the 4-MB page frame,\nbypassing the inner page table; and the 22 low-order bits in the linear address\nrefer to the offset in the 4-MB page frame. 386\nChapter 8",
  "Chapter 8\nMain Memory\npage directory\npage directory\nCR3\nregister\npage\ndirectory\npage\ntable\n4-KB\npage\n4-MB\npage\npage table\noffset\noffset\n(linear address)\n31\n22 21\n12 11\n0\n21\n31\n22\n0\nFigure 8.23\nPaging in the IA-32 architecture.\nTo improve the efﬁciency of physical memory use, IA-32 page tables can\nbe swapped to disk. In this case, an invalid bit is used in the page directory\nentry to indicate whether the table to which the entry is pointing is in memory\nor on disk. If the table is on disk, the operating system can use the other 31\nbits to specify the disk location of the table. The table can then be brought into\nmemory on demand.\nAs software developers began to discover the 4-GB memory limitations\nof 32-bit architectures, Intel adopted a page address extension (PAE), which",
  "allows 32-bit processors to access a physical address space larger than 4 GB. The\nfundamental difference introduced by PAE support was that paging went from\na two-level scheme (as shown in Figure 8.23) to a three-level scheme, where\nthe top two bits refer to a page directory pointer table. Figure 8.24 illustrates\na PAE system with 4-KB pages. (PAE also supports 2-MB pages.)\n31 30 29\n21 20\n12 11\n0\npage table\noffset\npage directory\n4-KB\npage\npage\ntable\npage directory\npointer table\nCR3\nregister\npage\ndirectory\nFigure 8.24\nPage address extensions. 8.7\nExample: Intel 32 and 64-bit Architectures\n387\nunused\npage map\nlevel 4\npage directory\npointer table\npage\ndirectory\npage\ntable\noffset\n63\n63\n47\n48\n39 38\n30 29\n21 20\n12 11\n0\nFigure 8.25\nx86-64 linear address.",
  "21 20\n12 11\n0\nFigure 8.25\nx86-64 linear address.\nPAE also increased the page-directory and page-table entries from 32 to 64\nbits in size, which allowed the base address of page tables and page frames to\nextend from 20 to 24 bits. Combined with the 12-bit offset, adding PAE support\nto IA-32 increased the address space to 36 bits, which supports up to 64 GB\nof physical memory. It is important to note that operating system support is\nrequired to use PAE. Both Linux and Intel Mac OS X support PAE. However,\n32-bit versions of Windows desktop operating systems still provide support\nfor only 4 GB of physical memory, even if PAE is enabled.\n8.7.2\nx86-64\nIntel has had an interesting history of developing 64-bit architectures. Its initial",
  "entry was the IA-64 (later named Itanium) architecture, but that architecture\nwas not widely adopted. Meanwhile, another chip manufacturer— AMD —\nbegan developing a 64-bit architecture known as x86-64 that was based on\nextending the existing IA-32 instruction set. The x86-64 supported much larger\nlogical and physical address spaces, as well as several other architectural\nadvances. Historically, AMD had often developed chips based on Intel’s\narchitecture, but now the roles were reversed as Intel adopted AMD’s x86-64\narchitecture. In discussing this architecture, rather than using the commercial\nnames AMD64 and Intel 64, we will use the more general term x86-64.\nSupport for a 64-bit address space yields an astonishing 264 bytes of",
  "addressable memory—a number greater than 16 quintillion (or 16 exabytes).\nHowever, even though 64-bit systems can potentially address this much\nmemory, in practice far fewer than 64 bits are used for address representation\nin current designs. The x86-64 architecture currently provides a 48-bit virtual\naddress with support for page sizes of 4 KB, 2 MB, or 1 GB using four levels of\npaging hierarchy. The representation of the linear address appears in Figure\n8.25. Because this addressing scheme can use PAE, virtual addresses are 48 bits\nin size but support 52-bit physical addresses (4096 terabytes).\n64-BIT COMPUTING\nHistory has taught us that even though memory capacities, CPU speeds,\nand similar computer capabilities seem large enough to satisfy demand for",
  "the foreseeable future, the growth of technology ultimately absorbs available\ncapacities, and we ﬁnd ourselves in need of additional memory or processing\npower, often sooner than we think. What might the future of technology bring\nthat would make a 64-bit address space seem too small? 388\nChapter 8\nMain Memory\n8.8\nExample: ARM Architecture\nAlthough Intel chips have dominated the personal computer market for over 30\nyears, chips for mobile devices such as smartphones and tablet computers often\ninstead run on 32-bit ARM processors. Interestingly, whereas Intel both designs\nand manufactures chips, ARM only designs them. It then licenses its designs to\nchip manufacturers. Apple has licensed the ARM design for its iPhone and iPad",
  "mobile devices, and several Android-based smartphones use ARM processors\nas well.\nThe 32-bit ARM architecture supports the following page sizes:\n1. 4-KB and 16-KB pages\n2. 1-MB and 16-MB pages (termed sections)\nThe paging system in use depends on whether a page or a section is being\nreferenced. One-level paging is used for 1-MB and 16-MB sections; two-level\npaging is used for 4-KB and 16-KB pages. Address translation with the ARM\nMMU is shown in Figure 8.26.\nThe ARM architecture also supports two levels of TLBs. At the outer level\nare two micro TLBs—a separate TLB for data and another for instructions.\nThe micro TLB supports ASIDs as well. At the inner level is a single main TLB.\nAddress translation begins at the micro TLB level. In the case of a miss, the",
  "main TLB is then checked. If both TLBs yield misses, a page table walk must be\nperformed in hardware.\nouter page\ninner page\noffset\n4-KB\nor\n16-KB\npage\n1-MB\nor\n16-MB \nsection\n32 bits\nFigure 8.26\nLogical address translation in ARM. 8.9\nSummary\n389\n8.9\nSummary\nMemory-management algorithms for multiprogrammed operating systems\nrange from the simple single-user system approach to segmentation and\npaging. The most important determinant of the method used in a particular\nsystem is the hardware provided. Every memory address generated by the\nCPU must be checked for legality and possibly mapped to a physical address.\nThe checking cannot be implemented (efﬁciently) in software. Hence, we are\nconstrained by the hardware available.\nThe various memory-management algorithms (contiguous allocation, pag-",
  "ing, segmentation, and combinations of paging and segmentation) differ in\nmany aspects. In comparing different memory-management strategies, we use\nthe following considerations:\n• Hardware support. A simple base register or a base–limit register pair is\nsufﬁcient for the single- and multiple-partition schemes, whereas paging\nand segmentation need mapping tables to deﬁne the address map.\n• Performance. As the memory-management algorithm becomes more\ncomplex, the time required to map a logical address to a physical address\nincreases. For the simple systems, we need only compare or add to the\nlogical address—operations that are fast. Paging and segmentation can be\nas fast if the mapping table is implemented in fast registers. If the table is",
  "in memory, however, user memory accesses can be degraded substantially.\nA TLB can reduce the performance degradation to an acceptable level.\n• Fragmentation. A multiprogrammed system will generally perform more\nefﬁciently if it has a higher level of multiprogramming. For a given\nset of processes, we can increase the multiprogramming level only by\npacking more processes into memory. To accomplish this task, we must\nreduce memory waste, or fragmentation. Systems with ﬁxed-sized allo-\ncation units, such as the single-partition scheme and paging, suffer from\ninternal fragmentation. Systems with variable-sized allocation units, such\nas the multiple-partition scheme and segmentation, suffer from external\nfragmentation.\n• Relocation. One solution to the external-fragmentation problem is com-",
  "paction. Compaction involves shifting a program in memory in such a\nway that the program does not notice the change. This consideration\nrequires that logical addresses be relocated dynamically, at execution time.\nIf addresses are relocated only at load time, we cannot compact storage.\n• Swapping. Swapping can be added to any algorithm. At intervals deter-\nmined by the operating system, usually dictated by CPU-scheduling poli-\ncies, processes are copied from main memory to a backing store and later\nare copied back to main memory. This scheme allows more processes to\nbe run than can be ﬁt into memory at one time. In general, PC operating\nsystems support paging, and operating systems for mobile devices do not.\n• Sharing. Another means of increasing the multiprogramming level is to",
  "share code and data among different processes. Sharing generally requires\nthat either paging or segmentation be used to provide small packets of 390\nChapter 8\nMain Memory\ninformation (pages or segments) that can be shared. Sharing is a means\nof running many processes with a limited amount of memory, but shared\nprograms and data must be designed carefully.\n• Protection. If paging or segmentation is provided, different sections of a\nuser program can be declared execute-only, read-only, or read–write. This\nrestriction is necessary with shared code or data and is generally useful\nin any case to provide simple run-time checks for common programming\nerrors.\nPractice Exercises\n8.1\nName two differences between logical and physical addresses.\n8.2",
  "8.2\nConsider a system in which a program can be separated into two\nparts: code and data. The CPU knows whether it wants an instruction\n(instruction fetch) or data (data fetch or store). Therefore, two base–\nlimit register pairs are provided: one for instructions and one for data.\nThe instruction base–limit register pair is automatically read-only, so\nprograms can be shared among different users. Discuss the advantages\nand disadvantages of this scheme.\n8.3\nWhy are page sizes always powers of 2?\n8.4\nConsider a logical address space of 64 pages of 1,024 words each, mapped\nonto a physical memory of 32 frames.\na.\nHow many bits are there in the logical address?\nb.\nHow many bits are there in the physical address?\n8.5\nWhat is the effect of allowing two entries in a page table to point to the",
  "same page frame in memory? Explain how this effect could be used to\ndecrease the amount of time needed to copy a large amount of memory\nfrom one place to another. What effect would updating some byte on the\none page have on the other page?\n8.6\nDescribe a mechanism by which one segment could belong to the address\nspace of two different processes.\n8.7\nSharing segments among processes without requiring that they have the\nsame segment number is possible in a dynamically linked segmentation\nsystem.\na.\nDeﬁne a system that allows static linking and sharing of segments\nwithout requiring that the segment numbers be the same.\nb.\nDescribe a paging scheme that allows pages to be shared without\nrequiring that the page numbers be the same.\n8.8",
  "requiring that the page numbers be the same.\n8.8\nIn the IBM/370, memory protection is provided through the use of keys.\nA key is a 4-bit quantity. Each 2-K block of memory has a key (the\nstorage key) associated with it. The CPU also has a key (the protection\nkey) associated with it. A store operation is allowed only if both keys Exercises\n391\nare equal or if either is 0. Which of the following memory-management\nschemes could be used successfully with this hardware?\na.\nBare machine\nb.\nSingle-user system\nc.\nMultiprogramming with a ﬁxed number of processes\nd.\nMultiprogramming with a variable number of processes\ne.\nPaging\nf.\nSegmentation\nExercises\n8.9\nExplain the difference between internal and external fragmentation.\n8.10\nConsider the following process for generating binaries. A compiler is",
  "used to generate the object code for individual modules, and a linkage\neditor is used to combine multiple object modules into a single program\nbinary. How does the linkage editor change the binding of instructions\nand data to memory addresses? What information needs to be passed\nfrom the compiler to the linkage editor to facilitate the memory-binding\ntasks of the linkage editor?\n8.11\nGiven six memory partitions of 300 KB, 600 KB, 350 KB, 200 KB, 750 KB,\nand 125 KB (in order), how would the ﬁrst-ﬁt, best-ﬁt, and worst-ﬁt\nalgorithms place processes of size 115 KB, 500 KB, 358 KB, 200 KB, and\n375 KB (in order)? Rank the algorithms in terms of how efﬁciently they\nuse memory.\n8.12\nMost systems allow a program to allocate more memory to its address",
  "space during execution. Allocation of data in the heap segments of\nprograms is an example of such allocated memory. What is required\nto support dynamic memory allocation in the following schemes?\na.\nContiguous memory allocation\nb.\nPure segmentation\nc.\nPure paging\n8.13\nCompare the memory organization schemes of contiguous memory\nallocation, pure segmentation, and pure paging with respect to the\nfollowing issues:\na.\nExternal fragmentation\nb.\nInternal fragmentation\nc.\nAbility to share code across processes\n8.14\nOn a system with paging, a process cannot access memory that it does\nnot own. Why? How could the operating system allow access to other\nmemory? Why should it or should it not? 392\nChapter 8\nMain Memory\n8.15\nExplain why mobile operating systems such as iOS and Android do not",
  "support swapping.\n8.16\nAlthough Android does not support swapping on its boot disk, it is\npossible to set up a swap space using a separate SD nonvolatile memory\ncard. Why would Android disallow swapping on its boot disk yet allow\nit on a secondary disk?\n8.17\nCompare paging with segmentation with respect to how much memory\nthe address translation structures require to convert virtual addresses to\nphysical addresses.\n8.18\nExplain why address space identiﬁers (ASIDs) are used.\n8.19\nProgram binaries in many systems are typically structured as follows.\nCode is stored starting with a small, ﬁxed virtual address, such as 0. The\ncode segment is followed by the data segment that is used for storing\nthe program variables. When the program starts executing, the stack is",
  "allocated at the other end of the virtual address space and is allowed\nto grow toward lower virtual addresses. What is the signiﬁcance of this\nstructure for the following schemes?\na.\nContiguous memory allocation\nb.\nPure segmentation\nc.\nPure paging\n8.20\nAssuming a 1-KB page size, what are the page numbers and offsets for\nthe following address references (provided as decimal numbers):\na.\n3085\nb.\n42095\nc.\n215201\nd.\n650000\ne.\n2000001\n8.21\nThe BTV operating system has a 21-bit virtual address, yet on certain\nembedded devices, it has only a 16-bit physical address. It also has a\n2-KB page size. How many entries are there in each of the following?\na.\nA conventional, single-level page table\nb.\nAn inverted page table\n8.22\nWhat is the maximum amount of physical memory?\n8.23",
  "8.23\nConsider a logical address space of 256 pages with a 4-KB page size,\nmapped onto a physical memory of 64 frames.\na.\nHow many bits are required in the logical address?\nb.\nHow many bits are required in the physical address? Exercises\n393\n8.24\nConsider a computer system with a 32-bit logical address and 4-KB page\nsize. The system supports up to 512 MB of physical memory. How many\nentries are there in each of the following?\n8.25\nConsider a paging system with the page table stored in memory.\na.\nIf a memory reference takes 50 nanoseconds, how long does a\npaged memory reference take?\nb.\nIf we add TLBs, and 75 percent of all page-table references are found\nin the TLBs, what is the effective memory reference time? (Assume\nthat ﬁnding a page-table entry in the TLBs takes 2 nanoseconds, if",
  "the entry is present.)\n8.26\nWhy are segmentation and paging sometimes combined into one\nscheme?\n8.27\nExplain why sharing a reentrant module is easier when segmentation is\nused than when pure paging is used.\n8.28\nConsider the following segment table:\nSegment\nBase\nLength\n0\n219\n600\n1\n2300\n14\n2\n90\n100\n3\n1327\n580\n4\n1952\n96\nWhat are the physical addresses for the following logical addresses?\na.\n0,430\nb.\n1,10\nc.\n2,500\nd.\n3,400\ne.\n4,112\n8.29\nWhat is the purpose of paging the page tables?\n8.30\nConsider the hierarchical paging scheme used by the VAX architecture.\nHow many memory operations are performed when a user program\nexecutes a memory-load operation?\n8.31\nCompare the segmented paging scheme with the hashed page table\nscheme for handling large address spaces. Under what circumstances is",
  "one scheme preferable to the other?\n8.32\nConsider the Intel address-translation scheme shown in Figure 8.22.\na.\nDescribe all the steps taken by the Intel Pentium in translating a\nlogical address into a physical address.\nb.\nWhat are the advantages to the operating system of hardware that\nprovides such complicated memory translation? 394\nChapter 8\nMain Memory\nc.\nAre there any disadvantages to this address-translation system? If\nso, what are they? If not, why is this scheme not used by every\nmanufacturer?\nProgramming Problems\n8.33\nAssume that a system has a 32-bit virtual address with a 4-KB page size.\nWrite a C program that is passed a virtual address (in decimal) on the\ncommand line and have it output the page number and offset for the",
  "given address. As an example, your program would run as follows:\n./a.out 19986\nYour program would output:\nThe address 19986 contains:\npage number = 4\noffset = 3602\nWriting this program will require using the appropriate data type to\nstore 32 bits. We encourage you to use unsigned data types as well.\nBibliographical Notes\nDynamic storage allocation was discussed by [Knuth (1973)] (Section 2.5), who\nfound through simulation that ﬁrst ﬁt is generally superior to best ﬁt. [Knuth\n(1973)] also discussed the 50-percent rule.\nThe concept of paging can be credited to the designers of the Atlas system,\nwhich has been described by [Kilburn et al. (1961)] and by [Howarth et al.\n(1961)]. The concept of segmentation was ﬁrst discussed by [Dennis (1965)].",
  "Paged segmentation was ﬁrst supported in the GE 645, on which MULTICS was\noriginally implemented ([Organick (1972)] and [Daley and Dennis (1967)]).\nInverted page tables are discussed in an article about the IBM RT storage\nmanager by [Chang and Mergen (1988)].\n[Hennessy and Patterson (2012)] explains the hardware aspects of TLBs,\ncaches, and MMUs. [Talluri et al. (1995)] discusses page tables for 64-bit address\nspaces. [Jacob and Mudge (2001)] describes techniques for managing the TLB.\n[Fang et al. (2001)] evaluates support for large pages.\nhttp://msdn.microsoft.com/en-us/library/windows/hardware/gg487512.\naspx discusses PAE support for Windows systems.\nhttp://www.intel.com/content/www/us/en/processors/architectures-sof-",
  "tware-developer-manuals.html provides various manuals for Intel 64 and\nIA-32 architectures.\nhttp://www.arm.com/products/processors/cortex-a/cortex-a9.php pro-\nvides an overview of the ARM architecture.\nBibliography\n[Chang and Mergen (1988)]\nA. Chang and M. F. Mergen, “801 Storage: Archi-\ntecture and Programming”, ACM Transactions on Computer Systems, Volume 6,\nNumber 1 (1988), pages 28–50. Bibliography\n395\n[Daley and Dennis (1967)]\nR. C. Daley and J. B. Dennis, “Virtual Memory,\nProcesses, and Sharing in Multics”, Proceedings of the ACM Symposium on\nOperating Systems Principles (1967), pages 121–128.\n[Dennis (1965)]\nJ. B. Dennis, “Segmentation and the Design of Multipro-\ngrammed Computer Systems”, Communications of the ACM, Volume 8, Number\n4 (1965), pages 589–602.\n[Fang et al. (2001)]",
  "4 (1965), pages 589–602.\n[Fang et al. (2001)]\nZ. Fang, L. Zhang, J. B. Carter, W. C. Hsieh, and S. A. McKee,\n“Reevaluating Online Superpage Promotion with Hardware Support”, Proceed-\nings of the International Symposium on High-Performance Computer Architecture,\nVolume 50, Number 5 (2001).\n[Hennessy and Patterson (2012)]\nJ. Hennessy and D. Patterson, Computer Archi-\ntecture: A Quantitative Approach, Fifth Edition, Morgan Kaufmann (2012).\n[Howarth et al. (1961)]\nD. J. Howarth, R. B. Payne, and F. H. Sumner, “The\nManchester University Atlas Operating System, Part II: User’s Description”,\nComputer Journal, Volume 4, Number 3 (1961), pages 226–229.\n[Jacob and Mudge (2001)]\nB. Jacob and T. Mudge, “Uniprocessor Virtual Mem-\nory Without TLBs”, IEEE Transactions on Computers, Volume 50, Number 5",
  "(2001).\n[Kilburn et al. (1961)]\nT. Kilburn, D. J. Howarth, R. B. Payne, and F. H. Sumner,\n“The Manchester University Atlas Operating System, Part I: Internal Organiza-\ntion”, Computer Journal, Volume 4, Number 3 (1961), pages 222–225.\n[Knuth (1973)]\nD. E. Knuth, The Art of Computer Programming, Volume 1: Funda-\nmental Algorithms, Second Edition, Addison-Wesley (1973).\n[Organick (1972)]\nE. I. Organick, The Multics System: An Examination of Its\nStructure, MIT Press (1972).\n[Talluri et al. (1995)]\nM. Talluri, M. D. Hill, and Y. A. Khalidi, “A New Page\nTable for 64-bit Address Spaces”, Proceedings of the ACM Symposium on Operating\nSystems Principles (1995), pages 184–200.  9\nC H A P T E R\nVirtual\nMemory\nIn Chapter 8, we discussed various memory-management strategies used in",
  "computer systems. All these strategies have the same goal: to keep many\nprocesses in memory simultaneously to allow multiprogramming. However,\nthey tend to require that an entire process be in memory before it can execute.\nVirtual memory is a technique that allows the execution of processes\nthat are not completely in memory. One major advantage of this scheme is\nthat programs can be larger than physical memory. Further, virtual memory\nabstracts main memory into an extremely large, uniform array of storage,\nseparating logical memory as viewed by the user from physical memory.\nThis technique frees programmers from the concerns of memory-storage\nlimitations. Virtual memory also allows processes to share ﬁles easily and",
  "to implement shared memory. In addition, it provides an efﬁcient mechanism\nfor process creation. Virtual memory is not easy to implement, however, and\nmay substantially decrease performance if it is used carelessly. In this chapter,\nwe discuss virtual memory in the form of demand paging and examine its\ncomplexity and cost.\nCHAPTER OBJECTIVES\n• To describe the beneﬁts of a virtual memory system.\n• To explain the concepts of demand paging, page-replacement algorithms,\nand allocation of page frames.\n• To discuss the principles of the working-set model.\n• To examine the relationship between shared memory and memory-mapped\nﬁles.\n• To explore how kernel memory is managed.\n9.1\nBackground\nThe memory-management algorithms outlined in Chapter 8 are necessary",
  "because of one basic requirement: The instructions being executed must be\n397 398\nChapter 9\nVirtual Memory\nin physical memory. The ﬁrst approach to meeting this requirement is to place\nthe entire logical address space in physical memory. Dynamic loading can help\nto ease this restriction, but it generally requires special precautions and extra\nwork by the programmer.\nThe requirement that instructions must be in physical memory to be\nexecuted seems both necessary and reasonable; but it is also unfortunate, since\nit limits the size of a program to the size of physical memory. In fact, an\nexamination of real programs shows us that, in many cases, the entire program\nis not needed. For instance, consider the following:\n• Programs often have code to handle unusual error conditions. Since these",
  "errors seldom, if ever, occur in practice, this code is almost never executed.\n• Arrays, lists, and tables are often allocated more memory than they actually\nneed. An array may be declared 100 by 100 elements, even though it is\nseldom larger than 10 by 10 elements. An assembler symbol table may\nhave room for 3,000 symbols, although the average program has less than\n200 symbols.\n• Certain options and features of a program may be used rarely. For instance,\nthe routines on U.S. government computers that balance the budget have\nnot been used in many years.\nEven in those cases where the entire program is needed, it may not all be\nneeded at the same time.\nThe ability to execute a program that is only partially in memory would\nconfer many beneﬁts:",
  "confer many beneﬁts:\n• A program would no longer be constrained by the amount of physical\nmemory that is available. Users would be able to write programs for an\nextremely large virtual address space, simplifying the programming task.\n• Because each user program could take less physical memory, more\nprograms could be run at the same time, with a corresponding increase in\nCPU utilization and throughput but with no increase in response time or\nturnaround time.\n• Less I/O would be needed to load or swap user programs into memory, so\neach user program would run faster.\nThus, running a program that is not entirely in memory would beneﬁt both\nthe system and the user.\nVirtual memory involves the separation of logical memory as perceived",
  "by users from physical memory. This separation allows an extremely large\nvirtual memory to be provided for programmers when only a smaller physical\nmemory is available (Figure 9.1). Virtual memory makes the task of program-\nming much easier, because the programmer no longer needs to worry about\nthe amount of physical memory available; she can concentrate instead on the\nproblem to be programmed.\nThe virtual address space of a process refers to the logical (or virtual) view\nof how a process is stored in memory. Typically, this view is that a process\nbegins at a certain logical address—say, address 0—and exists in contiguous\nmemory, as shown in Figure 9.2. Recall from Chapter 8, though, that in fact 9.1\nBackground\n399\nvirtual\nmemory\nmemory\nmap\nphysical\nmemory\n•\n•\n•\npage 0\npage 1\npage 2",
  "map\nphysical\nmemory\n•\n•\n•\npage 0\npage 1\npage 2\npage v\nFigure 9.1\nDiagram showing virtual memory that is larger than physical memory.\nphysical memory may be organized in page frames and that the physical page\nframes assigned to a process may not be contiguous. It is up to the memory-\nmanagement unit (MMU) to map logical pages to physical page frames in\nmemory.\nNote in Figure 9.2 that we allow the heap to grow upward in memory as\nit is used for dynamic memory allocation. Similarly, we allow for the stack to\ncode\n0\nMax\ndata\nheap\nstack\nFigure 9.2\nVirtual address space. 400\nChapter 9\nVirtual Memory\nshared library\nstack\nshared  \npages\ncode\ndata\nheap\ncode\ndata\nheap\nshared library\nstack\nFigure 9.3\nShared library using virtual memory.",
  "Figure 9.3\nShared library using virtual memory.\ngrow downward in memory through successive function calls. The large blank\nspace (or hole) between the heap and the stack is part of the virtual address\nspace but will require actual physical pages only if the heap or stack grows.\nVirtual address spaces that include holes are known as sparse address spaces.\nUsing a sparse address space is beneﬁcial because the holes can be ﬁlled as the\nstack or heap segments grow or if we wish to dynamically link libraries (or\npossibly other shared objects) during program execution.\nIn addition to separating logical memory from physical memory, virtual\nmemory allows ﬁles and memory to be shared by two or more processes\nthrough page sharing (Section 8.5.4). This leads to the following beneﬁts:",
  "• System libraries can be shared by several processes through mapping of the\nshared object into a virtual address space. Although each process considers\nthe libraries to be part of its virtual address space, the actual pages where\nthe libraries reside in physical memory are shared by all the processes\n(Figure 9.3). Typically, a library is mapped read-only into the space of each\nprocess that is linked with it.\n• Similarly, processes can share memory. Recall from Chapter 3 that two\nor more processes can communicate through the use of shared memory.\nVirtual memory allows one process to create a region of memory that it can\nshare with another process. Processes sharing this region consider it part\nof their virtual address space, yet the actual physical pages of memory are",
  "shared, much as is illustrated in Figure 9.3.\n• Pages can be shared during process creation with the fork() system call,\nthus speeding up process creation.\nWe further explore these—and other—beneﬁts of virtual memory later in\nthis chapter. First, though, we discuss implementing virtual memory through\ndemand paging. 9.2\nDemand Paging\n401\n9.2\nDemand Paging\nConsider how an executable program might be loaded from disk into memory.\nOne option is to load the entire program in physical memory at program\nexecution time. However, a problem with this approach is that we may not\ninitially need the entire program in memory. Suppose a program starts with\na list of available options from which the user is to select. Loading the entire",
  "program into memory results in loading the executable code for all options,\nregardless of whether or not an option is ultimately selected by the user. An\nalternative strategy is to load pages only as they are needed. This technique is\nknown as demand paging and is commonly used in virtual memory systems.\nWith demand-paged virtual memory, pages are loaded only when they are\ndemanded during program execution. Pages that are never accessed are thus\nnever loaded into physical memory.\nA demand-paging system is similar to a paging system with swapping\n(Figure 9.4) where processes reside in secondary memory (usually a disk).\nWhen we want to execute a process, we swap it into memory. Rather than\nswapping the entire process into memory, though, we use a lazy swapper.",
  "A lazy swapper never swaps a page into memory unless that page will be\nneeded. In the context of a demand-paging system, use of the term “swapper”\nis technically incorrect. A swapper manipulates entire processes, whereas a\npager is concerned with the individual pages of a process. We thus use “pager,”\nrather than “swapper,” in connection with demand paging.\nprogram\nA\nswap out\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nswap in\nprogram\nB\nmain\nmemory\nFigure 9.4\nTransfer of a paged memory to contiguous disk space. 402\nChapter 9\nVirtual Memory\n9.2.1\nBasic Concepts\nWhen a process is to be swapped in, the pager guesses which pages will be\nused before the process is swapped out again. Instead of swapping in a whole",
  "process, the pager brings only those pages into memory. Thus, it avoids reading\ninto memory pages that will not be used anyway, decreasing the swap time\nand the amount of physical memory needed.\nWith this scheme, we need some form of hardware support to distinguish\nbetween the pages that are in memory and the pages that are on the disk.\nThe valid–invalid bit scheme described in Section 8.5.3 can be used for this\npurpose. This time, however, when this bit is set to “valid,” the associated page\nis both legal and in memory. If the bit is set to “invalid,” the page either is not\nvalid (that is, not in the logical address space of the process) or is valid but\nis currently on the disk. The page-table entry for a page that is brought into",
  "memory is set as usual, but the page-table entry for a page that is not currently\nin memory is either simply marked invalid or contains the address of the page\non disk. This situation is depicted in Figure 9.5.\nNotice that marking a page invalid will have no effect if the process never\nattempts to access that page. Hence, if we guess right and page in all pages\nthat are actually needed and only those pages, the process will run exactly as\nthough we had brought in all pages. While the process executes and accesses\npages that are memory resident, execution proceeds normally.\nB\nD\nD\nE\nF\nH\nlogical\nmemory\nvalid–invalid\nbit\nframe\npage table\n1\n0\n4\n6\n2\n3\n4\n5\n9\n6\n7\n1\n0\n2\n3\n4\n5\n6\n7\ni\nv\nv\ni\ni\nv\ni\ni\nphysical memory\nA\nA\nB\nC\nC\nF\nG\nH\nF\n1\n0\n2\n3\n4\n5\n6\n7\n9\n8\n10\n11\n12\n13\n14\n15\nA\nC\nE\nG\nFigure 9.5",
  "4\n5\n6\n7\n9\n8\n10\n11\n12\n13\n14\n15\nA\nC\nE\nG\nFigure 9.5\nPage table when some pages are not in main memory. 9.2\nDemand Paging\n403\nload M\nreference\ntrap\ni\npage is on\nbacking store\noperating\nsystem\nrestart\ninstruction\nreset page\ntable\npage table\nphysical\nmemory\nbring in\nmissing page\nfree frame\n1\n2\n3\n6\n5\n4\nFigure 9.6\nSteps in handling a page fault.\nBut what happens if the process tries to access a page that was not brought\ninto memory? Access to a page marked invalid causes a page fault. The paging\nhardware, in translating the address through the page table, will notice that\nthe invalid bit is set, causing a trap to the operating system. This trap is the\nresult of the operating system’s failure to bring the desired page into memory.",
  "The procedure for handling this page fault is straightforward (Figure 9.6):\n1. We check an internal table (usually kept with the process control block)\nfor this process to determine whether the reference was a valid or an\ninvalid memory access.\n2. If the reference was invalid, we terminate the process. If it was valid but\nwe have not yet brought in that page, we now page it in.\n3. We ﬁnd a free frame (by taking one from the free-frame list, for example).\n4. We schedule a disk operation to read the desired page into the newly\nallocated frame.\n5. When the disk read is complete, we modify the internal table kept with\nthe process and the page table to indicate that the page is now in memory.\n6. We restart the instruction that was interrupted by the trap. The process",
  "can now access the page as though it had always been in memory.\nIn the extreme case, we can start executing a process with no pages in\nmemory. When the operating system sets the instruction pointer to the ﬁrst 404\nChapter 9\nVirtual Memory\ninstruction of the process, which is on a non-memory-resident page, the process\nimmediately faults for the page. After this page is brought into memory, the\nprocess continues to execute, faulting as necessary until every page that it\nneeds is in memory. At that point, it can execute with no more faults. This\nscheme is pure demand paging: never bring a page into memory until it is\nrequired.\nTheoretically, some programs could access several new pages of memory\nwith each instruction execution (one page for the instruction and many for",
  "data), possibly causing multiple page faults per instruction. This situation\nwould result in unacceptable system performance. Fortunately, analysis of\nrunning processes shows that this behavior is exceedingly unlikely. Programs\ntend to have locality of reference, described in Section 9.6.1, which results in\nreasonable performance from demand paging.\nThe hardware to support demand paging is the same as the hardware for\npaging and swapping:\n• Page table. This table has the ability to mark an entry invalid through a\nvalid–invalid bit or a special value of protection bits.\n• Secondary memory. This memory holds those pages that are not present\nin main memory. The secondary memory is usually a high-speed disk. It is\nknown as the swap device, and the section of disk used for this purpose is",
  "known as swap space. Swap-space allocation is discussed in Chapter 10.\nA crucial requirement for demand paging is the ability to restart any\ninstruction after a page fault. Because we save the state (registers, condition\ncode, instruction counter) of the interrupted process when the page fault\noccurs, we must be able to restart the process in exactly the same place and\nstate, except that the desired page is now in memory and is accessible. In most\ncases, this requirement is easy to meet. A page fault may occur at any memory\nreference. If the page fault occurs on the instruction fetch, we can restart by\nfetching the instruction again. If a page fault occurs while we are fetching an\noperand, we must fetch and decode the instruction again and then fetch the\noperand.",
  "operand.\nAs a worst-case example, consider a three-address instruction such as ADD\nthe content of A to B, placing the result in C. These are the steps to execute this\ninstruction:\n1. Fetch and decode the instruction (ADD).\n2. Fetch A.\n3. Fetch B.\n4. Add A and B.\n5. Store the sum in C.\nIf we fault when we try to store in C (because C is in a page not currently\nin memory), we will have to get the desired page, bring it in, correct the\npage table, and restart the instruction. The restart will require fetching the\ninstruction again, decoding it again, fetching the two operands again, and\nthen adding again. However, there is not much repeated work (less than one 9.2\nDemand Paging\n405\ncomplete instruction), and the repetition is necessary only when a page fault\noccurs.",
  "occurs.\nThe major difﬁculty arises when one instruction may modify several\ndifferent locations. For example, consider the IBM System 360/370 MVC (move\ncharacter) instruction, which can move up to 256 bytes from one location to\nanother (possibly overlapping) location. If either block (source or destination)\nstraddles a page boundary, a page fault might occur after the move is partially\ndone. In addition, if the source and destination blocks overlap, the source\nblock may have been modiﬁed, in which case we cannot simply restart the\ninstruction.\nThis problem can be solved in two different ways. In one solution, the\nmicrocode computes and attempts to access both ends of both blocks. If a page\nfault is going to occur, it will happen at this step, before anything is modiﬁed.",
  "The move can then take place; we know that no page fault can occur, since all\nthe relevant pages are in memory. The other solution uses temporary registers\nto hold the values of overwritten locations. If there is a page fault, all the old\nvalues are written back into memory before the trap occurs. This action restores\nmemory to its state before the instruction was started, so that the instruction\ncan be repeated.\nThis is by no means the only architectural problem resulting from adding\npaging to an existing architecture to allow demand paging, but it illustrates\nsome of the difﬁculties involved. Paging is added between the CPU and the\nmemory in a computer system. It should be entirely transparent to the user\nprocess. Thus, people often assume that paging can be added to any system.",
  "Although this assumption is true for a non-demand-paging environment,\nwhere a page fault represents a fatal error, it is not true where a page fault\nmeans only that an additional page must be brought into memory and the\nprocess restarted.\n9.2.2\nPerformance of Demand Paging\nDemand paging can signiﬁcantly affect the performance of a computer system.\nTo see why, let’s compute the effective access time for a demand-paged\nmemory. For most computer systems, the memory-access time, denoted ma,\nranges from 10 to 200 nanoseconds. As long as we have no page faults, the\neffective access time is equal to the memory access time. If, however, a page\nfault occurs, we must ﬁrst read the relevant page from disk and then access the\ndesired word.",
  "desired word.\nLet p be the probability of a page fault (0 ≤p ≤1). We would expect p to\nbe close to zero—that is, we would expect to have only a few page faults. The\neffective access time is then\neffective access time = (1 −p) × ma + p × page fault time.\nTo compute the effective access time, we must know how much time is\nneeded to service a page fault. A page fault causes the following sequence to\noccur:\n1. Trap to the operating system.\n2. Save the user registers and process state. 406\nChapter 9\nVirtual Memory\n3. Determine that the interrupt was a page fault.\n4. Check that the page reference was legal and determine the location of the\npage on the disk.\n5. Issue a read from the disk to a free frame:\na. Wait in a queue for this device until the read request is serviced.",
  "b. Wait for the device seek and/or latency time.\nc. Begin the transfer of the page to a free frame.\n6. While waiting, allocate the CPU to some other user (CPU scheduling,\noptional).\n7. Receive an interrupt from the disk I/O subsystem (I/O completed).\n8. Save the registers and process state for the other user (if step 6 is executed).\n9. Determine that the interrupt was from the disk.\n10. Correct the page table and other tables to show that the desired page is\nnow in memory.\n11. Wait for the CPU to be allocated to this process again.\n12. Restore the user registers, process state, and new page table, and then\nresume the interrupted instruction.\nNot all of these steps are necessary in every case. For example, we are assuming",
  "that, in step 6, the CPU is allocated to another process while the I/O occurs.\nThis arrangement allows multiprogramming to maintain CPU utilization but\nrequires additional time to resume the page-fault service routine when the I/O\ntransfer is complete.\nIn any case, we are faced with three major components of the page-fault\nservice time:\n1. Service the page-fault interrupt.\n2. Read in the page.\n3. Restart the process.\nThe ﬁrst and third tasks can be reduced, with careful coding, to several\nhundred instructions. These tasks may take from 1 to 100 microseconds each.\nThe page-switch time, however, will probably be close to 8 milliseconds.\n(A typical hard disk has an average latency of 3 milliseconds, a seek of\n5 milliseconds, and a transfer time of 0.05 milliseconds. Thus, the total",
  "paging time is about 8 milliseconds, including hardware and software time.)\nRemember also that we are looking at only the device-service time. If a queue\nof processes is waiting for the device, we have to add device-queueing time as\nwe wait for the paging device to be free to service our request, increasing even\nmore the time to swap. 9.2\nDemand Paging\n407\nWith an average page-fault service time of 8 milliseconds and a memory-\naccess time of 200 nanoseconds, the effective access time in nanoseconds\nis\neffective access time = (1 −p) × (200) + p (8 milliseconds)\n= (1 −p) × 200 + p × 8,000,000\n= 200 + 7,999,800 × p.\nWe see, then, that the effective access time is directly proportional to the\npage-fault rate. If one access out of 1,000 causes a page fault, the effective access",
  "time is 8.2 microseconds. The computer will be slowed down by a factor of 40\nbecause of demand paging! If we want performance degradation to be less\nthan 10 percent, we need to keep the probability of page faults at the following\nlevel:\n220 > 200 + 7,999,800 × p,\n20 > 7,999,800 × p,\np < 0.0000025.\nThat is, to keep the slowdown due to paging at a reasonable level, we can\nallow fewer than one memory access out of 399,990 to page-fault. In sum,\nit is important to keep the page-fault rate low in a demand-paging system.\nOtherwise, the effective access time increases, slowing process execution\ndramatically.\nAn additional aspect of demand paging is the handling and overall use\nof swap space. Disk I/O to swap space is generally faster than that to the ﬁle",
  "system. It is a faster ﬁle system because swap space is allocated in much larger\nblocks, and ﬁle lookups and indirect allocation methods are not used (Chapter\n10). The system can therefore gain better paging throughput by copying an\nentire ﬁle image into the swap space at process startup and then performing\ndemand paging from the swap space. Another option is to demand pages\nfrom the ﬁle system initially but to write the pages to swap space as they are\nreplaced. This approach will ensure that only needed pages are read from the\nﬁle system but that all subsequent paging is done from swap space.\nSome systems attempt to limit the amount of swap space used through\ndemand paging of binary ﬁles. Demand pages for such ﬁles are brought directly",
  "from the ﬁle system. However, when page replacement is called for, these\nframes can simply be overwritten (because they are never modiﬁed), and the\npages can be read in from the ﬁle system again if needed. Using this approach,\nthe ﬁle system itself serves as the backing store. However, swap space must still\nbe used for pages not associated with a ﬁle (known as anonymous memory);\nthese pages include the stack and heap for a process. This method appears to\nbe a good compromise and is used in several systems, including Solaris and\nBSD UNIX.\nMobile operating systems typically do not support swapping. Instead,\nthese systems demand-page from the ﬁle system and reclaim read-only pages\n(such as code) from applications if memory becomes constrained. Such data",
  "can be demand-paged from the ﬁle system if it is later needed. Under iOS,\nanonymous memory pages are never reclaimed from an application unless the\napplication is terminated or explicitly releases the memory. 408\nChapter 9\nVirtual Memory\n9.3\nCopy-on-Write\nIn Section 9.2, we illustrated how a process can start quickly by demand-paging\nin the page containing the ﬁrst instruction. However, process creation using the\nfork() system call may initially bypass the need for demand paging by using\na technique similar to page sharing (covered in Section 8.5.4). This technique\nprovides rapid process creation and minimizes the number of new pages that\nmust be allocated to the newly created process.\nRecall that the fork() system call creates a child process that is a duplicate",
  "of its parent. Traditionally, fork() worked by creating a copy of the parent’s\naddress space for the child, duplicating the pages belonging to the parent.\nHowever, considering that many child processes invoke the exec() system\ncall immediately after creation, the copying of the parent’s address space may\nbe unnecessary. Instead, we can use a technique known as copy-on-write,\nwhich works by allowing the parent and child processes initially to share the\nsame pages. These shared pages are marked as copy-on-write pages, meaning\nthat if either process writes to a shared page, a copy of the shared page is\ncreated. Copy-on-write is illustrated in Figures 9.7 and 9.8, which show the\ncontents of the physical memory before and after process 1 modiﬁes page C.",
  "For example, assume that the child process attempts to modify a page\ncontaining portions of the stack, with the pages set to be copy-on-write. The\noperating system will create a copy of this page, mapping it to the address space\nof the child process. The child process will then modify its copied page and not\nthe page belonging to the parent process. Obviously, when the copy-on-write\ntechnique is used, only the pages that are modiﬁed by either process are copied;\nall unmodiﬁed pages can be shared by the parent and child processes. Note, too,\nthat only pages that can be modiﬁed need be marked as copy-on-write. Pages\nthat cannot be modiﬁed (pages containing executable code) can be shared by\nthe parent and child. Copy-on-write is a common technique used by several",
  "operating systems, including Windows XP, Linux, and Solaris.\nWhen it is determined that a page is going to be duplicated using copy-\non-write, it is important to note the location from which the free page will\nbe allocated. Many operating systems provide a pool of free pages for such\nrequests. These free pages are typically allocated when the stack or heap for a\nprocess must expand or when there are copy-on-write pages to be managed.\nprocess1\nphysical\nmemory\npage A\npage B\npage C\nprocess2\nFigure 9.7\nBefore process 1 modiﬁes page C. 9.4\nPage Replacement\n409\nprocess1\nphysical\nmemory\npage A\npage B\npage C\nCopy of page C\nprocess2\nFigure 9.8\nAfter process 1 modiﬁes page C.\nOperating systems typically allocate these pages using a technique known as",
  "zero-ﬁll-on-demand. Zero-ﬁll-on-demand pages have been zeroed-out before\nbeing allocated, thus erasing the previous contents.\nSeveral versions of UNIX (including Solaris and Linux) provide a variation\nof the fork() system call—vfork() (for virtual memory fork)—that operates\ndifferently from fork() with copy-on-write. With vfork(), the parent process\nis suspended, and the child process uses the address space of the parent.\nBecause vfork() does not use copy-on-write, if the child process changes\nany pages of the parent’s address space, the altered pages will be visible to the\nparent once it resumes. Therefore, vfork() must be used with caution to ensure\nthat the child process does not modify the address space of the parent. vfork()",
  "is intended to be used when the child process calls exec() immediately after\ncreation. Because no copying of pages takes place, vfork() is an extremely\nefﬁcient method of process creation and is sometimes used to implement UNIX\ncommand-line shell interfaces.\n9.4\nPage Replacement\nIn our earlier discussion of the page-fault rate, we assumed that each page\nfaults at most once, when it is ﬁrst referenced. This representation is not strictly\naccurate, however. If a process of ten pages actually uses only half of them, then\ndemand paging saves the I/O necessary to load the ﬁve pages that are never\nused. We could also increase our degree of multiprogramming by running\ntwice as many processes. Thus, if we had forty frames, we could run eight",
  "processes, rather than the four that could run if each required ten frames (ﬁve\nof which were never used).\nIf we increase our degree of multiprogramming, we are over-allocating\nmemory. If we run six processes, each of which is ten pages in size but actually\nuses only ﬁve pages, we have higher CPU utilization and throughput, with\nten frames to spare. It is possible, however, that each of these processes, for a\nparticular data set, may suddenly try to use all ten of its pages, resulting in a\nneed for sixty frames when only forty are available.\nFurther, consider that system memory is not used only for holding program\npages. Buffers for I/O also consume a considerable amount of memory. This use 410\nChapter 9\nVirtual Memory\nmonitor\nload M\nphysical\nmemory\n1\n0\n2\n3\n4\n5\n6\n7\nH\nload M\nJ\nM",
  "physical\nmemory\n1\n0\n2\n3\n4\n5\n6\n7\nH\nload M\nJ\nM\nlogical memory\nfor user 1\n0\nPC\n1\n2\n3\nB\nM\nvalid–invalid\nbit\nframe\npage table\nfor user 1\ni\nA\nB\nD\nE\nlogical memory\nfor user 2\n0\n1\n2\n3\nvalid–invalid\nbit\nframe\npage table\nfor user 2\ni\n4\n3\n5\nv\nv\nv\n7\n2\nv\nv\n6\nv\nD\nH\nJ\nA\nE\nFigure 9.9\nNeed for page replacement.\ncan increase the strain on memory-placement algorithms. Deciding how much\nmemory to allocate to I/O and how much to program pages is a signiﬁcant\nchallenge. Some systems allocate a ﬁxed percentage of memory for I/O buffers,\nwhereas others allow both user processes and the I/O subsystem to compete\nfor all system memory.\nOver-allocation of memory manifests itself as follows. While a user process\nis executing, a page fault occurs. The operating system determines where the",
  "desired page is residing on the disk but then ﬁnds that there are no free frames\non the free-frame list; all memory is in use (Figure 9.9).\nThe operating system has several options at this point. It could terminate\nthe user process. However, demand paging is the operating system’s attempt to\nimprove the computer system’s utilization and throughput. Users should not\nbe aware that their processes are running on a paged system—paging should\nbe logically transparent to the user. So this option is not the best choice.\nThe operating system could instead swap out a process, freeing all its\nframes and reducing the level of multiprogramming. This option is a good one\nin certain circumstances, and we consider it further in Section 9.6. Here, we\ndiscuss the most common solution: page replacement.",
  "9.4.1\nBasic Page Replacement\nPage replacement takes the following approach. If no frame is free, we ﬁnd\none that is not currently being used and free it. We can free a frame by writing\nits contents to swap space and changing the page table (and all other tables) to\nindicate that the page is no longer in memory (Figure 9.10). We can now use\nthe freed frame to hold the page for which the process faulted. We modify the\npage-fault service routine to include page replacement: 9.4\nPage Replacement\n411\nvalid–invalid bit\nframe\nf\npage table\nvictim\nchange\nto invalid\npage out\nvictim\npage\npage in\ndesired\npage\nreset page\ntable for\nnew page\nphysical\nmemory\n2\n4\n1\n3\nf\n0\ni\nv\nFigure 9.10\nPage replacement.\n1. Find the location of the desired page on the disk.\n2. Find a free frame:",
  "2. Find a free frame:\na. If there is a free frame, use it.\nb. If there is no free frame, use a page-replacement algorithm to select\na victim frame.\nc. Write the victim frame to the disk; change the page and frame tables\naccordingly.\n3. Read the desired page into the newly freed frame; change the page and\nframe tables.\n4. Continue the user process from where the page fault occurred.\nNotice that, if no frames are free, two page transfers (one out and one in)\nare required. This situation effectively doubles the page-fault service time and\nincreases the effective access time accordingly.\nWe can reduce this overhead by using a modify bit (or dirty bit). When\nthis scheme is used, each page or frame has a modify bit associated with it in",
  "the hardware. The modify bit for a page is set by the hardware whenever any\nbyte in the page is written into, indicating that the page has been modiﬁed.\nWhen we select a page for replacement, we examine its modify bit. If the bit\nis set, we know that the page has been modiﬁed since it was read in from the\ndisk. In this case, we must write the page to the disk. If the modify bit is not set,\nhowever, the page has not been modiﬁed since it was read into memory. In this\ncase, we need not write the memory page to the disk: it is already there. This\ntechnique also applies to read-only pages (for example, pages of binary code). 412\nChapter 9\nVirtual Memory\nSuch pages cannot be modiﬁed; thus, they may be discarded when desired.",
  "This scheme can signiﬁcantly reduce the time required to service a page fault,\nsince it reduces I/O time by one-half if the page has not been modiﬁed.\nPage replacement is basic to demand paging. It completes the separation\nbetween logical memory and physical memory. With this mechanism, an\nenormous virtual memory can be provided for programmers on a smaller\nphysical memory. With no demand paging, user addresses are mapped into\nphysical addresses, and the two sets of addresses can be different. All the\npages of a process still must be in physical memory, however. With demand\npaging, the size of the logical address space is no longer constrained by physical\nmemory. If we have a user process of twenty pages, we can execute it in ten",
  "frames simply by using demand paging and using a replacement algorithm to\nﬁnd a free frame whenever necessary. If a page that has been modiﬁed is to be\nreplaced, its contents are copied to the disk. A later reference to that page will\ncause a page fault. At that time, the page will be brought back into memory,\nperhaps replacing some other page in the process.\nWe must solve two major problems to implement demand paging: we must\ndevelop a frame-allocation algorithm and a page-replacement algorithm.\nThat is, if we have multiple processes in memory, we must decide how many\nframes to allocate to each process; and when page replacement is required,\nwe must select the frames that are to be replaced. Designing appropriate\nalgorithms to solve these problems is an important task, because disk I/O",
  "is so expensive. Even slight improvements in demand-paging methods yield\nlarge gains in system performance.\nThere are many different page-replacement algorithms. Every operating\nsystem probably has its own replacement scheme. How do we select a\nparticular replacement algorithm? In general, we want the one with the lowest\npage-fault rate.\nWe evaluate an algorithm by running it on a particular string of memory\nreferences and computing the number of page faults. The string of memory\nreferences is called a reference string. We can generate reference strings\nartiﬁcially (by using a random-number generator, for example), or we can trace\na given system and record the address of each memory reference. The latter\nchoice produces a large number of data (on the order of 1 million addresses",
  "per second). To reduce the number of data, we use two facts.\nFirst, for a given page size (and the page size is generally ﬁxed by the\nhardware or system), we need to consider only the page number, rather than\nthe entire address. Second, if we have a reference to a page p, then any references\nto page p that immediately follow will never cause a page fault. Page p will\nbe in memory after the ﬁrst reference, so the immediately following references\nwill not fault.\nFor example, if we trace a particular process, we might record the following\naddress sequence:\n0100, 0432, 0101, 0612, 0102, 0103, 0104, 0101, 0611, 0102, 0103,\n0104, 0101, 0610, 0102, 0103, 0104, 0101, 0609, 0102, 0105\nAt 100 bytes per page, this sequence is reduced to the following reference\nstring:",
  "string:\n1, 4, 1, 6, 1, 6, 1, 6, 1, 6, 1 9.4\nPage Replacement\n413\nnumber of page faults\n16\n14\n12\n10\n8\n6\n4\n2\n1\n2\n3\nnumber of frames\n4\n5\n6\nFigure 9.11\nGraph of page faults versus number of frames.\nTo determine the number of page faults for a particular reference string and\npage-replacement algorithm, we also need to know the number of page frames\navailable. Obviously, as the number of frames available increases, the number\nof page faults decreases. For the reference string considered previously, for\nexample, if we had three or more frames, we would have only three faults—\none fault for the ﬁrst reference to each page. In contrast, with only one frame\navailable, we would have a replacement with every reference, resulting in",
  "eleven faults. In general, we expect a curve such as that in Figure 9.11. As the\nnumber of frames increases, the number of page faults drops to some minimal\nlevel. Of course, adding physical memory increases the number of frames.\nWe next illustrate several page-replacement algorithms. In doing so, we\nuse the reference string\n7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 1, 2, 0, 1, 7, 0, 1\nfor a memory with three frames.\n9.4.2\nFIFO Page Replacement\nThe simplest page-replacement algorithm is a ﬁrst-in, ﬁrst-out (FIFO) algorithm.\nA FIFO replacement algorithm associates with each page the time when that\npage was brought into memory. When a page must be replaced, the oldest\npage is chosen. Notice that it is not strictly necessary to record the time when",
  "a page is brought in. We can create a FIFO queue to hold all pages in memory.\nWe replace the page at the head of the queue. When a page is brought into\nmemory, we insert it at the tail of the queue.\nFor our example reference string, our three frames are initially empty. The\nﬁrst three references (7, 0, 1) cause page faults and are brought into these empty\nframes. The next reference (2) replaces page 7, because page 7 was brought in\nﬁrst. Since 0 is the next reference and 0 is already in memory, we have no fault\nfor this reference. The ﬁrst reference to 3 results in replacement of page 0, since\nit is now ﬁrst in line. Because of this replacement, the next reference, to 0, will 414\nChapter 9\nVirtual Memory\n7\n7\n0\n7\n0\n1\npage frames\nreference string\n2\n0\n1\n2\n3\n1\n2\n3\n0\n4\n3\n0\n4\n2\n0\n4\n2\n3\n0\n2\n3\n7",
  "2\n0\n1\n2\n3\n1\n2\n3\n0\n4\n3\n0\n4\n2\n0\n4\n2\n3\n0\n2\n3\n7\n1\n2\n7\n0\n2\n7\n0\n1\n0\n1\n3\n0\n7\n0\n1\n2\n0\n3\n0\n4\n2\n3\n0\n7\n1\n1\n0\n2\n1\n2\n0\n3\n1\n2\nFigure 9.12\nFIFO page-replacement algorithm.\nfault. Page 1 is then replaced by page 0. This process continues as shown in\nFigure 9.12. Every time a fault occurs, we show which pages are in our three\nframes. There are ﬁfteen faults altogether.\nThe FIFO page-replacement algorithm is easy to understand and program.\nHowever, its performance is not always good. On the one hand, the page\nreplaced may be an initialization module that was used a long time ago and is\nno longer needed. On the other hand, it could contain a heavily used variable\nthat was initialized early and is in constant use.\nNotice that, even if we select for replacement a page that is in active use,",
  "everything still works correctly. After we replace an active page with a new\none, a fault occurs almost immediately to retrieve the active page. Some other\npage must be replaced to bring the active page back into memory. Thus, a bad\nreplacement choice increases the page-fault rate and slows process execution.\nIt does not, however, cause incorrect execution.\nTo illustrate the problems that are possible with a FIFO page-replacement\nalgorithm, consider the following reference string:\n1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5\nFigure 9.13 shows the curve of page faults for this reference string versus the\nnumber of available frames. Notice that the number of faults for four frames\n(ten) is greater than the number of faults for three frames (nine)! This most",
  "unexpected result is known as Belady’s anomaly: for some page-replacement\nalgorithms, the page-fault rate may increase as the number of allocated frames\nincreases. We would expect that giving more memory to a process would\nimprove its performance. In some early research, investigators noticed that\nthis assumption was not always true. Belady’s anomaly was discovered as a\nresult.\n9.4.3\nOptimal Page Replacement\nOne result of the discovery of Belady’s anomaly was the search for an optimal\npage-replacement algorithm—the algorithm that has the lowest page-fault\nrate of all algorithms and will never suffer from Belady’s anomaly. Such an\nalgorithm does exist and has been called OPT or MIN. It is simply this:\nReplace the page that will not be used for the longest period of time.",
  "Use of this page-replacement algorithm guarantees the lowest possible page-\nfault rate for a ﬁxed number of frames. 9.4\nPage Replacement\n415\nnumber of page faults\n16\n14\n12\n10\n8\n6\n4\n2\n1\n2\n3\nnumber of frames\n4\n5\n6\n7\nFigure 9.13\nPage-fault curve for FIFO replacement on a reference string.\nFor example, on our sample reference string, the optimal page-replacement\nalgorithm would yield nine page faults, as shown in Figure 9.14. The ﬁrst three\nreferences cause faults that ﬁll the three empty frames. The reference to page\n2 replaces page 7, because page 7 will not be used until reference 18, whereas\npage 0 will be used at 5, and page 1 at 14. The reference to page 3 replaces\npage 1, as page 1 will be the last of the three pages in memory to be referenced",
  "again. With only nine page faults, optimal replacement is much better than\na FIFO algorithm, which results in ﬁfteen faults. (If we ignore the ﬁrst three,\nwhich all algorithms must suffer, then optimal replacement is twice as good as\nFIFO replacement.) In fact, no replacement algorithm can process this reference\nstring in three frames with fewer than nine faults.\nUnfortunately, the optimal page-replacement algorithm is difﬁcult to\nimplement, because it requires future knowledge of the reference string. (We\nencountered a similar situation with the SJF CPU-scheduling algorithm in\nSection 6.3.2.) As a result, the optimal algorithm is used mainly for comparison\nstudies. For instance, it may be useful to know that, although a new algorithm",
  "is not optimal, it is within 12.3 percent of optimal at worst and within 4.7\npercent on average.\npage frames\nreference string\n7\n7\n0\n7\n0\n1\n2\n0\n1\n2\n0\n3\n2\n4\n3\n2\n0\n3\n7\n0\n1\n2\n0\n1\n7\n0\n1\n2\n0\n3\n0\n4\n2\n3\n0\n7\n1\n1\n0\n2\n1\n2\n0\n3\nFigure 9.14\nOptimal page-replacement algorithm. 416\nChapter 9\nVirtual Memory\n9.4.4\nLRU Page Replacement\nIf the optimal algorithm is not feasible, perhaps an approximation of the\noptimal algorithm is possible. The key distinction between the FIFO and OPT\nalgorithms (other than looking backward versus forward in time) is that the\nFIFO algorithm uses the time when a page was brought into memory, whereas\nthe OPT algorithm uses the time when a page is to be used. If we use the recent\npast as an approximation of the near future, then we can replace the page that",
  "has not been used for the longest period of time. This approach is the least\nrecently used (LRU) algorithm.\nLRU replacement associates with each page the time of that page’s last use.\nWhen a page must be replaced, LRU chooses the page that has not been used\nfor the longest period of time. We can think of this strategy as the optimal\npage-replacement algorithm looking backward in time, rather than forward.\n(Strangely, if we let SR be the reverse of a reference string S, then the page-fault\nrate for the OPT algorithm on S is the same as the page-fault rate for the OPT\nalgorithm on SR. Similarly, the page-fault rate for the LRU algorithm on S is the\nsame as the page-fault rate for the LRU algorithm on SR.)\nThe result of applying LRU replacement to our example reference string is",
  "shown in Figure 9.15. The LRU algorithm produces twelve faults. Notice that\nthe ﬁrst ﬁve faults are the same as those for optimal replacement. When the\nreference to page 4 occurs, however, LRU replacement sees that, of the three\nframes in memory, page 2 was used least recently. Thus, the LRU algorithm\nreplaces page 2, not knowing that page 2 is about to be used. When it then faults\nfor page 2, the LRU algorithm replaces page 3, since it is now the least recently\nused of the three pages in memory. Despite these problems, LRU replacement\nwith twelve faults is much better than FIFO replacement with ﬁfteen.\nThe LRU policy is often used as a page-replacement algorithm and\nis considered to be good. The major problem is how to implement LRU",
  "replacement. An LRU page-replacement algorithm may require substantial\nhardware assistance. The problem is to determine an order for the frames\ndeﬁned by the time of last use. Two implementations are feasible:\n• Counters. In the simplest case, we associate with each page-table entry a\ntime-of-use ﬁeld and add to the CPU a logical clock or counter. The clock is\nincremented for every memory reference. Whenever a reference to a page\nis made, the contents of the clock register are copied to the time-of-use\nﬁeld in the page-table entry for that page. In this way, we always have\npage frames\nreference string\n7\n7\n0\n7\n0\n1\n2\n0\n1\n2\n0\n3\n4\n0\n3\n4\n0\n2\n4\n3\n2\n0\n3\n2\n1\n3\n2\n1\n0\n2\n1\n0\n7\n7\n0\n1\n2\n0\n3\n0\n4\n2\n3\n0\n7\n1\n1\n0\n2\n1\n2\n0\n3\nFigure 9.15\nLRU page-replacement algorithm. 9.4\nPage Replacement\n417",
  "Page Replacement\n417\nthe “time” of the last reference to each page. We replace the page with the\nsmallest time value. This scheme requires a search of the page table to ﬁnd\nthe LRU page and a write to memory (to the time-of-use ﬁeld in the page\ntable) for each memory access. The times must also be maintained when\npage tables are changed (due to CPU scheduling). Overﬂow of the clock\nmust be considered.\n• Stack. Another approach to implementing LRU replacement is to keep\na stack of page numbers. Whenever a page is referenced, it is removed\nfrom the stack and put on the top. In this way, the most recently used\npage is always at the top of the stack and the least recently used page is\nalways at the bottom (Figure 9.16). Because entries must be removed from",
  "the middle of the stack, it is best to implement this approach by using a\ndoubly linked list with a head pointer and a tail pointer. Removing a page\nand putting it on the top of the stack then requires changing six pointers\nat worst. Each update is a little more expensive, but there is no search for\na replacement; the tail pointer points to the bottom of the stack, which is\nthe LRU page. This approach is particularly appropriate for software or\nmicrocode implementations of LRU replacement.\nLike optimal replacement, LRU replacement does not suffer from Belady’s\nanomaly. Both belong to a class of page-replacement algorithms, called stack\nalgorithms, that can never exhibit Belady’s anomaly. A stack algorithm is an\nalgorithm for which it can be shown that the set of pages in memory for n",
  "frames is always a subset of the set of pages that would be in memory with n\n+ 1 frames. For LRU replacement, the set of pages in memory would be the n\nmost recently referenced pages. If the number of frames is increased, these n\npages will still be the most recently referenced and so will still be in memory.\nNote that neither implementation of LRU would be conceivable without\nhardware assistance beyond the standard TLB registers. The updating of the\nclock ﬁelds or stack must be done for every memory reference. If we were\nto use an interrupt for every reference to allow software to update such data\nstructures, it would slow every memory reference by a factor of at least ten,\n2\n1\n0\n4\n7\nstack\nbefore\na\n7\n2\n1\n4\n0\nstack\nafter\nb\nreference string\n4\n7\n0\n7\n1\n0\n1\n2\n1\n2\n2\n7\na\nb\n1\nFigure 9.16",
  "4\n7\n0\n7\n1\n0\n1\n2\n1\n2\n2\n7\na\nb\n1\nFigure 9.16\nUse of a stack to record the most recent page references. 418\nChapter 9\nVirtual Memory\nhence slowing every user process by a factor of ten. Few systems could tolerate\nthat level of overhead for memory management.\n9.4.5\nLRU-Approximation Page Replacement\nFew computer systems provide sufﬁcient hardware support for true LRU page\nreplacement. In fact, some systems provide no hardware support, and other\npage-replacement algorithms (such as a FIFO algorithm) must be used. Many\nsystems provide some help, however, in the form of a reference bit. The\nreference bit for a page is set by the hardware whenever that page is referenced\n(either a read or a write to any byte in the page). Reference bits are associated\nwith each entry in the page table.",
  "with each entry in the page table.\nInitially, all bits are cleared (to 0) by the operating system. As a user process\nexecutes, the bit associated with each page referenced is set (to 1) by the\nhardware. After some time, we can determine which pages have been used and\nwhich have not been used by examining the reference bits, although we do not\nknow the order of use. This information is the basis for many page-replacement\nalgorithms that approximate LRU replacement.\n9.4.5.1\nAdditional-Reference-Bits Algorithm\nWe can gain additional ordering information by recording the reference bits at\nregular intervals. We can keep an 8-bit byte for each page in a table in memory.\nAt regular intervals (say, every 100 milliseconds), a timer interrupt transfers",
  "control to the operating system. The operating system shifts the reference bit\nfor each page into the high-order bit of its 8-bit byte, shifting the other bits right\nby 1 bit and discarding the low-order bit. These 8-bit shift registers contain the\nhistory of page use for the last eight time periods. If the shift register contains\n00000000, for example, then the page has not been used for eight time periods.\nA page that is used at least once in each period has a shift register value of\n11111111. A page with a history register value of 11000100 has been used more\nrecently than one with a value of 01110111. If we interpret these 8-bit bytes\nas unsigned integers, the page with the lowest number is the LRU page, and\nit can be replaced. Notice that the numbers are not guaranteed to be unique,",
  "however. We can either replace (swap out) all pages with the smallest value or\nuse the FIFO method to choose among them.\nThe number of bits of history included in the shift register can be varied,\nof course, and is selected (depending on the hardware available) to make\nthe updating as fast as possible. In the extreme case, the number can be\nreduced to zero, leaving only the reference bit itself. This algorithm is called\nthe second-chance page-replacement algorithm.\n9.4.5.2\nSecond-Chance Algorithm\nThe basic algorithm of second-chance replacement is a FIFO replacement\nalgorithm. When a page has been selected, however, we inspect its reference\nbit. If the value is 0, we proceed to replace this page; but if the reference bit",
  "is set to 1, we give the page a second chance and move on to select the next\nFIFO page. When a page gets a second chance, its reference bit is cleared, and\nits arrival time is reset to the current time. Thus, a page that is given a second\nchance will not be replaced until all other pages have been replaced (or given 9.4\nPage Replacement\n419\ncircular queue of pages\n(a)\nnext\nvictim\n0\nreference\nbits\npages\n0\n1\n1\n0\n1\n1\n…\n…\ncircular queue of pages\n(b)\n0\nreference\nbits\npages\n0\n0\n0\n0\n1\n1\n…\n…\nFigure 9.17\nSecond-chance (clock) page-replacement algorithm.\nsecond chances). In addition, if a page is used often enough to keep its reference\nbit set, it will never be replaced.\nOne way to implement the second-chance algorithm (sometimes referred",
  "to as the clock algorithm) is as a circular queue. A pointer (that is, a hand on\nthe clock) indicates which page is to be replaced next. When a frame is needed,\nthe pointer advances until it ﬁnds a page with a 0 reference bit. As it advances,\nit clears the reference bits (Figure 9.17). Once a victim page is found, the page\nis replaced, and the new page is inserted in the circular queue in that position.\nNotice that, in the worst case, when all bits are set, the pointer cycles through\nthe whole queue, giving each page a second chance. It clears all the reference\nbits before selecting the next page for replacement. Second-chance replacement\ndegenerates to FIFO replacement if all bits are set.\n9.4.5.3\nEnhanced Second-Chance Algorithm",
  "9.4.5.3\nEnhanced Second-Chance Algorithm\nWe can enhance the second-chance algorithm by considering the reference bit\nand the modify bit (described in Section 9.4.1) as an ordered pair. With these\ntwo bits, we have the following four possible classes:\n1. (0, 0) neither recently used nor modiﬁed—best page to replace\n2. (0, 1) not recently used but modiﬁed—not quite as good, because the\npage will need to be written out before replacement 420\nChapter 9\nVirtual Memory\n3. (1, 0) recently used but clean—probably will be used again soon\n4. (1, 1) recently used and modiﬁed—probably will be used again soon, and\nthe page will be need to be written out to disk before it can be replaced\nEach page is in one of these four classes. When page replacement is called for,",
  "we use the same scheme as in the clock algorithm; but instead of examining\nwhether the page to which we are pointing has the reference bit set to 1,\nwe examine the class to which that page belongs. We replace the ﬁrst page\nencountered in the lowest nonempty class. Notice that we may have to scan\nthe circular queue several times before we ﬁnd a page to be replaced.\nThe major difference between this algorithm and the simpler clock algo-\nrithm is that here we give preference to those pages that have been modiﬁed\nin order to reduce the number of I/Os required.\n9.4.6\nCounting-Based Page Replacement\nThere are many other algorithms that can be used for page replacement. For\nexample, we can keep a counter of the number of references that have been",
  "made to each page and develop the following two schemes.\n• The least frequently used (LFU) page-replacement algorithm requires that\nthe page with the smallest count be replaced. The reason for this selection is\nthat an actively used page should have a large reference count. A problem\narises, however, when a page is used heavily during the initial phase of\na process but then is never used again. Since it was used heavily, it has a\nlarge count and remains in memory even though it is no longer needed.\nOne solution is to shift the counts right by 1 bit at regular intervals, forming\nan exponentially decaying average usage count.\n• The most frequently used (MFU) page-replacement algorithm is based\non the argument that the page with the smallest count was probably just",
  "brought in and has yet to be used.\nAs you might expect, neither MFU nor LFU replacement is common. The\nimplementation of these algorithms is expensive, and they do not approximate\nOPT replacement well.\n9.4.7\nPage-Buffering Algorithms\nOther procedures are often used in addition to a speciﬁc page-replacement\nalgorithm. For example, systems commonly keep a pool of free frames. When\na page fault occurs, a victim frame is chosen as before. However, the desired\npage is read into a free frame from the pool before the victim is written out. This\nprocedure allows the process to restart as soon as possible, without waiting\nfor the victim page to be written out. When the victim is later written out, its\nframe is added to the free-frame pool.",
  "frame is added to the free-frame pool.\nAn expansion of this idea is to maintain a list of modiﬁed pages. Whenever\nthe paging device is idle, a modiﬁed page is selected and is written to the disk.\nIts modify bit is then reset. This scheme increases the probability that a page\nwill be clean when it is selected for replacement and will not need to be written\nout. 9.5\nAllocation of Frames\n421\nAnother modiﬁcation is to keep a pool of free frames but to remember\nwhich page was in each frame. Since the frame contents are not modiﬁed when\na frame is written to the disk, the old page can be reused directly from the\nfree-frame pool if it is needed before that frame is reused. No I/O is needed in\nthis case. When a page fault occurs, we ﬁrst check whether the desired page is",
  "in the free-frame pool. If it is not, we must select a free frame and read into it.\nThis technique is used in the VAX/VMS system along with a FIFO replace-\nment algorithm. When the FIFO replacement algorithm mistakenly replaces a\npage that is still in active use, that page is quickly retrieved from the free-frame\npool, and no I/O is necessary. The free-frame buffer provides protection against\nthe relatively poor, but simple, FIFO replacement algorithm. This method is\nnecessary because the early versions of VAX did not implement the reference\nbit correctly.\nSome versions of the UNIX system use this method in conjunction with\nthe second-chance algorithm. It can be a useful augmentation to any page-\nreplacement algorithm, to reduce the penalty incurred if the wrong victim\npage is selected.",
  "page is selected.\n9.4.8\nApplications and Page Replacement\nIn certain cases, applications accessing data through the operating system’s\nvirtual memory perform worse than if the operating system provided no\nbuffering at all. A typical example is a database, which provides its own\nmemory management and I/O buffering. Applications like this understand\ntheir memory use and disk use better than does an operating system that is\nimplementing algorithms for general-purpose use. If the operating system is\nbuffering I/O and the application is doing so as well, however, then twice the\nmemory is being used for a set of I/O.\nIn another example, data warehouses frequently perform massive sequen-\ntial disk reads, followed by computations and writes. The LRU algorithm would",
  "be removing old pages and preserving new ones, while the application would\nmore likely be reading older pages than newer ones (as it starts its sequential\nreads again). Here, MFU would actually be more efﬁcient than LRU.\nBecause of such problems, some operating systems give special programs\nthe ability to use a disk partition as a large sequential array of logical blocks,\nwithout any ﬁle-system data structures. This array is sometimes called the raw\ndisk, and I/O to this array is termed raw I/O. Raw I/O bypasses all the ﬁle-\nsystem services, such as ﬁle I/O demand paging, ﬁle locking, prefetching, space\nallocation, ﬁle names, and directories. Note that although certain applications\nare more efﬁcient when implementing their own special-purpose storage",
  "services on a raw partition, most applications perform better when they use\nthe regular ﬁle-system services.\n9.5\nAllocation of Frames\nWe turn next to the issue of allocation. How do we allocate the ﬁxed amount\nof free memory among the various processes? If we have 93 free frames and\ntwo processes, how many frames does each process get?\nThe simplest case is the single-user system. Consider a single-user system\nwith 128 KB of memory composed of pages 1 KB in size. This system has 128 422\nChapter 9\nVirtual Memory\nframes. The operating system may take 35 KB, leaving 93 frames for the user\nprocess. Under pure demand paging, all 93 frames would initially be put on\nthe free-frame list. When a user process started execution, it would generate a",
  "sequence of page faults. The ﬁrst 93 page faults would all get free frames from\nthe free-frame list. When the free-frame list was exhausted, a page-replacement\nalgorithm would be used to select one of the 93 in-memory pages to be replaced\nwith the 94th, and so on. When the process terminated, the 93 frames would\nonce again be placed on the free-frame list.\nThere are many variations on this simple strategy. We can require that the\noperating system allocate all its buffer and table space from the free-frame list.\nWhen this space is not in use by the operating system, it can be used to support\nuser paging. We can try to keep three free frames reserved on the free-frame list\nat all times. Thus, when a page fault occurs, there is a free frame available to",
  "page into. While the page swap is taking place, a replacement can be selected,\nwhich is then written to the disk as the user process continues to execute.\nOther variants are also possible, but the basic strategy is clear: the user process\nis allocated any free frame.\n9.5.1\nMinimum Number of Frames\nOur strategies for the allocation of frames are constrained in various ways. We\ncannot, for example, allocate more than the total number of available frames\n(unless there is page sharing). We must also allocate at least a minimum number\nof frames. Here, we look more closely at the latter requirement.\nOne reason for allocating at least a minimum number of frames involves\nperformance. Obviously, as the number of frames allocated to each process",
  "decreases, the page-fault rate increases, slowing process execution. In addition,\nremember that, when a page fault occurs before an executing instruction\nis complete, the instruction must be restarted. Consequently, we must have\nenough frames to hold all the different pages that any single instruction can\nreference.\nFor example, consider a machine in which all memory-reference instruc-\ntions may reference only one memory address. In this case, we need at least one\nframe for the instruction and one frame for the memory reference. In addition,\nif one-level indirect addressing is allowed (for example, a load instruction on\npage 16 can refer to an address on page 0, which is an indirect reference to page\n23), then paging requires at least three frames per process. Think about what",
  "might happen if a process had only two frames.\nThe minimum number of frames is deﬁned by the computer architecture.\nFor example, the move instruction for the PDP-11 includes more than one word\nfor some addressing modes, and thus the instruction itself may straddle two\npages. In addition, each of its two operands may be indirect references, for a\ntotal of six frames. Another example is the IBM 370 MVC instruction. Since the\ninstruction is from storage location to storage location, it takes 6 bytes and can\nstraddle two pages. The block of characters to move and the area to which it\nis to be moved can each also straddle two pages. This situation would require\nsix frames. The worst case occurs when the MVC instruction is the operand of",
  "an EXECUTE instruction that straddles a page boundary; in this case, we need\neight frames. 9.5\nAllocation of Frames\n423\nThe worst-case scenario occurs in computer architectures that allow\nmultiple levels of indirection (for example, each 16-bit word could contain\na 15-bit address plus a 1-bit indirect indicator). Theoretically, a simple load\ninstruction could reference an indirect address that could reference an indirect\naddress (on another page) that could also reference an indirect address (on yet\nanother page), and so on, until every page in virtual memory had been touched.\nThus, in the worst case, the entire virtual memory must be in physical memory.\nTo overcome this difﬁculty, we must place a limit on the levels of indirection (for",
  "example, limit an instruction to at most 16 levels of indirection). When the ﬁrst\nindirection occurs, a counter is set to 16; the counter is then decremented for\neach successive indirection for this instruction. If the counter is decremented to\n0, a trap occurs (excessive indirection). This limitation reduces the maximum\nnumber of memory references per instruction to 17, requiring the same number\nof frames.\nWhereas the minimum number of frames per process is deﬁned by the\narchitecture, the maximum number is deﬁned by the amount of available\nphysical memory. In between, we are still left with signiﬁcant choice in frame\nallocation.\n9.5.2\nAllocation Algorithms\nThe easiest way to split m frames among n processes is to give everyone an",
  "equal share, m/n frames (ignoring frames needed by the operating system\nfor the moment). For instance, if there are 93 frames and ﬁve processes, each\nprocess will get 18 frames. The three leftover frames can be used as a free-frame\nbuffer pool. This scheme is called equal allocation.\nAn alternative is to recognize that various processes will need differing\namounts of memory. Consider a system with a 1-KB frame size. If a small\nstudent process of 10 KB and an interactive database of 127 KB are the only\ntwo processes running in a system with 62 free frames, it does not make much\nsense to give each process 31 frames. The student process does not need more\nthan 10 frames, so the other 21 are, strictly speaking, wasted.\nTo solve this problem, we can use proportional allocation, in which we",
  "allocate available memory to each process according to its size. Let the size of\nthe virtual memory for process pi be si, and deﬁne\nS = # si.\nThen, if the total number of available frames is m, we allocate ai frames to\nprocess pi, where ai is approximately\nai = si/S × m.\nOf course, we must adjust each ai to be an integer that is greater than the\nminimum number of frames required by the instruction set, with a sum not\nexceeding m.\nWith proportional allocation, we would split 62 frames between two\nprocesses, one of 10 pages and one of 127 pages, by allocating 4 frames and 57\nframes, respectively, since\n10/137 × 62 ≈4, and\n127/137 × 62 ≈57. 424\nChapter 9\nVirtual Memory\nIn this way, both processes share the available frames according to their\n“needs,” rather than equally.",
  "“needs,” rather than equally.\nIn both equal and proportional allocation, of course, the allocation may\nvary according to the multiprogramming level. If the multiprogramming level\nis increased, each process will lose some frames to provide the memory needed\nfor the new process. Conversely, if the multiprogramming level decreases, the\nframes that were allocated to the departed process can be spread over the\nremaining processes.\nNotice that, with either equal or proportional allocation, a high-priority\nprocess is treated the same as a low-priority process. By its deﬁnition, however,\nwe may want to give the high-priority process more memory to speed its\nexecution, to the detriment of low-priority processes. One solution is to use",
  "a proportional allocation scheme wherein the ratio of frames depends not on\nthe relative sizes of processes but rather on the priorities of processes or on a\ncombination of size and priority.\n9.5.3\nGlobal versus Local Allocation\nAnother important factor in the way frames are allocated to the various\nprocesses is page replacement. With multiple processes competing for frames,\nwe can classify page-replacement algorithms into two broad categories: global\nreplacement and local replacement. Global replacement allows a process to\nselect a replacement frame from the set of all frames, even if that frame is\ncurrently allocated to some other process; that is, one process can take a frame\nfrom another. Local replacement requires that each process select from only its\nown set of allocated frames.",
  "own set of allocated frames.\nFor example, consider an allocation scheme wherein we allow high-priority\nprocesses to select frames from low-priority processes for replacement. A\nprocess can select a replacement from among its own frames or the frames\nof any lower-priority process. This approach allows a high-priority process to\nincrease its frame allocation at the expense of a low-priority process. With a\nlocal replacement strategy, the number of frames allocated to a process does not\nchange. With global replacement, a process may happen to select only frames\nallocated to other processes, thus increasing the number of frames allocated to\nit (assuming that other processes do not choose its frames for replacement).\nOne problem with a global replacement algorithm is that a process cannot",
  "control its own page-fault rate. The set of pages in memory for a process\ndepends not only on the paging behavior of that process but also on the paging\nbehavior of other processes. Therefore, the same process may perform quite\ndifferently (for example, taking 0.5 seconds for one execution and 10.3 seconds\nfor the next execution) because of totally external circumstances. Such is not\nthe case with a local replacement algorithm. Under local replacement, the\nset of pages in memory for a process is affected by the paging behavior of\nonly that process. Local replacement might hinder a process, however, by\nnot making available to it other, less used pages of memory. Thus, global\nreplacement generally results in greater system throughput and is therefore\nthe more commonly used method.\n9.5.4",
  "the more commonly used method.\n9.5.4\nNon-Uniform Memory Access\nThus far in our coverage of virtual memory, we have assumed that all main\nmemory is created equal—or at least that it is accessed equally. On many 9.6\nThrashing\n425\ncomputer systems, that is not the case. Often, in systems with multiple CPUs\n(Section 1.3.2), a given CPU can access some sections of main memory faster\nthan it can access others. These performance differences are caused by how\nCPUs and memory are interconnected in the system. Frequently, such a system\nis made up of several system boards, each containing multiple CPUs and some\nmemory. The system boards are interconnected in various ways, ranging from\nsystem buses to high-speed network connections like InﬁniBand. As you might",
  "expect, the CPUs on a particular board can access the memory on that board with\nless delay than they can access memory on other boards in the system. Systems\nin which memory access times vary signiﬁcantly are known collectively as\nnon-uniform memory access (NUMA) systems, and without exception, they\nare slower than systems in which memory and CPUs are located on the same\nmotherboard.\nManaging which page frames are stored at which locations can signiﬁcantly\naffect performance in NUMA systems. If we treat memory as uniform in such\na system, CPUs may wait signiﬁcantly longer for memory access than if we\nmodify memory allocation algorithms to take NUMA into account. Similar\nchanges must be made to the scheduling system. The goal of these changes is",
  "to have memory frames allocated “as close as possible” to the CPU on which\nthe process is running. The deﬁnition of “close” is “with minimum latency,”\nwhich typically means on the same system board as the CPU.\nThe algorithmic changes consist of having the scheduler track the last CPU\non which each process ran. If the scheduler tries to schedule each process onto\nits previous CPU, and the memory-management system tries to allocate frames\nfor the process close to the CPU on which it is being scheduled, then improved\ncache hits and decreased memory access times will result.\nThe picture is more complicated once threads are added. For example, a\nprocess with many running threads may end up with those threads scheduled\non many different system boards. How is the memory to be allocated in this",
  "case? Solaris solves the problem by creating lgroups (for “latency groups”) in\nthe kernel. Each lgroup gathers together close CPUs and memory. In fact, there\nis a hierarchy of lgroups based on the amount of latency between the groups.\nSolaris tries to schedule all threads of a process and allocate all memory of a\nprocess within an lgroup. If that is not possible, it picks nearby lgroups for the\nrest of the resources needed. This practice minimizes overall memory latency\nand maximizes CPU cache hit rates.\n9.6\nThrashing\nIf the number of frames allocated to a low-priority process falls below the\nminimum number required by the computer architecture, we must suspend\nthat process’s execution. We should then page out its remaining pages, freeing",
  "all its allocated frames. This provision introduces a swap-in, swap-out level of\nintermediate CPU scheduling.\nIn fact, look at any process that does not have “enough” frames. If the\nprocess does not have the number of frames it needs to support pages in\nactive use, it will quickly page-fault. At this point, it must replace some page.\nHowever, since all its pages are in active use, it must replace a page that will\nbe needed again right away. Consequently, it quickly faults again, and again,\nand again, replacing pages that it must bring back in immediately. 426\nChapter 9\nVirtual Memory\nThis high paging activity is called thrashing. A process is thrashing if it is\nspending more time paging than executing.\n9.6.1\nCause of Thrashing",
  "9.6.1\nCause of Thrashing\nThrashing results in severe performance problems. Consider the following\nscenario, which is based on the actual behavior of early paging systems.\nThe operating system monitors CPU utilization. If CPU utilization is too low,\nwe increase the degree of multiprogramming by introducing a new process\nto the system. A global page-replacement algorithm is used; it replaces pages\nwithout regard to the process to which they belong. Now suppose that a process\nenters a new phase in its execution and needs more frames. It starts faulting and\ntaking frames away from other processes. These processes need those pages,\nhowever, and so they also fault, taking frames from other processes. These\nfaulting processes must use the paging device to swap pages in and out. As",
  "they queue up for the paging device, the ready queue empties. As processes\nwait for the paging device, CPU utilization decreases.\nThe CPU scheduler sees the decreasing CPU utilization and increases the\ndegree of multiprogramming as a result. The new process tries to get started by\ntaking frames from running processes, causing more page faults and a longer\nqueue for the paging device. As a result, CPU utilization drops even further,\nand the CPU scheduler tries to increase the degree of multiprogramming even\nmore. Thrashing has occurred, and system throughput plunges. The page-\nfault rate increases tremendously. As a result, the effective memory-access\ntime increases. No work is getting done, because the processes are spending\nall their time paging.",
  "all their time paging.\nThis phenomenon is illustrated in Figure 9.18, in which CPU utilization\nis plotted against the degree of multiprogramming. As the degree of multi-\nprogramming increases, CPU utilization also increases, although more slowly,\nuntil a maximum is reached. If the degree of multiprogramming is increased\neven further, thrashing sets in, and CPU utilization drops sharply. At this point,\nto increase CPU utilization and stop thrashing, we must decrease the degree of\nmultiprogramming.\nthrashing\ndegree of multiprogramming\nCPU utilization\nFigure 9.18\nThrashing. 9.6\nThrashing\n427\nWe can limit the effects of thrashing by using a local replacement algorithm\n(or priority replacement algorithm). With local replacement, if one process",
  "starts thrashing, it cannot steal frames from another process and cause the latter\nto thrash as well. However, the problem is not entirely solved. If processes are\nthrashing, they will be in the queue for the paging device most of the time. The\naverage service time for a page fault will increase because of the longer average\nqueue for the paging device. Thus, the effective access time will increase even\nfor a process that is not thrashing.\nTo prevent thrashing, we must provide a process with as many frames as\nit needs. But how do we know how many frames it “needs”? There are several\ntechniques. The working-set strategy (Section 9.6.2) starts by looking at how\nmany frames a process is actually using. This approach deﬁnes the locality\nmodel of process execution.",
  "model of process execution.\nThe locality model states that, as a process executes, it moves from locality\nto locality. A locality is a set of pages that are actively used together (Figure\n9.19). A program is generally composed of several different localities, which\nmay overlap.\nFor example, when a function is called, it deﬁnes a new locality. In this\nlocality, memory references are made to the instructions of the function call, its\nlocal variables, and a subset of the global variables. When we exit the function,\nthe process leaves this locality, since the local variables and instructions of the\nfunction are no longer in active use. We may return to this locality later.\nThus, we see that localities are deﬁned by the program structure and its",
  "data structures. The locality model states that all programs will exhibit this\nbasic memory reference structure. Note that the locality model is the unstated\nprinciple behind the caching discussions so far in this book. If accesses to any\ntypes of data were random rather than patterned, caching would be useless.\nSuppose we allocate enough frames to a process to accommodate its current\nlocality. It will fault for the pages in its locality until all these pages are in\nmemory; then, it will not fault again until it changes localities. If we do not\nallocate enough frames to accommodate the size of the current locality, the\nprocess will thrash, since it cannot keep in memory all the pages that it is\nactively using.\n9.6.2\nWorking-Set Model",
  "actively using.\n9.6.2\nWorking-Set Model\nAs mentioned, the working-set model is based on the assumption of locality.\nThis model uses a parameter, !, to deﬁne the working-set window. The idea\nis to examine the most recent ! page references. The set of pages in the most\nrecent ! page references is the working set (Figure 9.20). If a page is in active\nuse, it will be in the working set. If it is no longer being used, it will drop from\nthe working set ! time units after its last reference. Thus, the working set is an\napproximation of the program’s locality.\nFor example, given the sequence of memory references shown in Figure\n9.20, if ! = 10 memory references, then the working set at time t1 is {1, 2, 5,\n6, 7}. By time t2, the working set has changed to {3, 4}.",
  "The accuracy of the working set depends on the selection of !. If ! is too\nsmall, it will not encompass the entire locality; if ! is too large, it may overlap\nseveral localities. In the extreme, if ! is inﬁnite, the working set is the set of\npages touched during the process execution. 428\nChapter 9\nVirtual Memory\n18\n20\n22\n24\n26\n28\n30\n32\n34\npage numbers\nmemory address\nexecution time\nFigure 9.19\nLocality in a memory-reference pattern.\nThe most important property of the working set, then, is its size. If we\ncompute the working-set size, WSSi, for each process in the system, we can\nthen consider that\nD = # WSSi,\nwhere D is the total demand for frames. Each process is actively using the pages\nin its working set. Thus, process i needs WSSi frames. If the total demand is",
  "greater than the total number of available frames (D > m), thrashing will occur,\nbecause some processes will not have enough frames.\nOnce ! has been selected, use of the working-set model is simple. The\noperating system monitors the working set of each process and allocates to 9.6\nThrashing\n429\npage reference table\n. . . 2 6 1 5 7 7 7 7 5 1 6 2 3 4 1 2 3 4 4 4 3 4 3 4 4 4 1 3 2 3 4 4 4 3 4 4 4 . . . \n∆\nt1\nWS(t1) = {1,2,5,6,7}\n∆\nt2\nWS(t2) = {3,4}\nFigure 9.20\nWorking-set model.\nthat working set enough frames to provide it with its working-set size. If there\nare enough extra frames, another process can be initiated. If the sum of the\nworking-set sizes increases, exceeding the total number of available frames,\nthe operating system selects a process to suspend. The process’s pages are",
  "written out (swapped), and its frames are reallocated to other processes. The\nsuspended process can be restarted later.\nThis working-set strategy prevents thrashing while keeping the degree of\nmultiprogramming as high as possible. Thus, it optimizes CPU utilization. The\ndifﬁculty with the working-set model is keeping track of the working set. The\nworking-set window is a moving window. At each memory reference, a new\nreference appears at one end, and the oldest reference drops off the other end.\nA page is in the working set if it is referenced anywhere in the working-set\nwindow.\nWe can approximate the working-set model with a ﬁxed-interval timer\ninterrupt and a reference bit. For example, assume that ! equals 10,000\nreferences and that we can cause a timer interrupt every 5,000 references.",
  "When we get a timer interrupt, we copy and clear the reference-bit values for\neach page. Thus, if a page fault occurs, we can examine the current reference\nbit and two in-memory bits to determine whether a page was used within the\nlast 10,000 to 15,000 references. If it was used, at least one of these bits will be\non. If it has not been used, these bits will be off. Pages with at least one bit on\nwill be considered to be in the working set.\nNote that this arrangement is not entirely accurate, because we cannot\ntell where, within an interval of 5,000, a reference occurred. We can reduce the\nuncertainty by increasing the number of history bits and the frequency of inter-\nrupts (for example, 10 bits and interrupts every 1,000 references). However, the",
  "cost to service these more frequent interrupts will be correspondingly higher.\n9.6.3\nPage-Fault Frequency\nThe working-set model is successful, and knowledge of the working set can\nbe useful for prepaging (Section 9.9.1), but it seems a clumsy way to control\nthrashing. A strategy that uses the page-fault frequency (PFF) takes a more\ndirect approach.\nThe speciﬁc problem is how to prevent thrashing. Thrashing has a high\npage-fault rate. Thus, we want to control the page-fault rate. When it is too\nhigh, we know that the process needs more frames. Conversely, if the page-fault\nrate is too low, then the process may have too many frames. We can establish\nupper and lower bounds on the desired page-fault rate (Figure 9.21). If the",
  "actual page-fault rate exceeds the upper limit, we allocate the process another 430\nChapter 9\nVirtual Memory\nnumber of frames\nincrease number\nof frames\nupper bound\nlower bound\ndecrease number\nof frames\npage-fault rate\nFigure 9.21\nPage-fault frequency.\nframe. If the page-fault rate falls below the lower limit, we remove a frame\nfrom the process. Thus, we can directly measure and control the page-fault\nrate to prevent thrashing.\nAs with the working-set strategy, we may have to swap out a process. If the\npage-fault rate increases and no free frames are available, we must select some\nprocess and swap it out to backing store. The freed frames are then distributed\nto processes with high page-fault rates.\n9.6.4\nConcluding Remarks",
  "9.6.4\nConcluding Remarks\nPractically speaking, thrashing and the resulting swapping have a disagreeably\nlarge impact on performance. The current best practice in implementing a\ncomputer facility is to include enough physical memory, whenever possible,\nto avoid thrashing and swapping. From smartphones through mainframes,\nproviding enough memory to keep all working sets in memory concurrently,\nexcept under extreme conditions, gives the best user experience.\n9.7\nMemory-Mapped Files\nConsider a sequential read of a ﬁle on disk using the standard system calls\nopen(), read(), and write(). Each ﬁle access requires a system call and disk\naccess. Alternatively, we can use the virtual memory techniques discussed\nso far to treat ﬁle I/O as routine memory accesses. This approach, known as",
  "memory mapping a ﬁle, allows a part of the virtual address space to be logically\nassociated with the ﬁle. As we shall see, this can lead to signiﬁcant performance\nincreases.\n9.7.1\nBasic Mechanism\nMemory mapping a ﬁle is accomplished by mapping a disk block to a page (or\npages) in memory. Initial access to the ﬁle proceeds through ordinary demand\npaging, resulting in a page fault. However, a page-sized portion of the ﬁle is\nread from the ﬁle system into a physical page (some systems may opt to read 9.7\nMemory-Mapped Files\n431\nWORKING SETS AND PAGE-FAULT RATES\nThere is a direct relationship between the working set of a process and its\npage-fault rate. Typically, as shown in Figure 9.20, the working set of a process\nchanges over time as references to data and code sections move from one",
  "locality to another. Assuming there is sufﬁcient memory to store the working\nset of a process (that is, the process is not thrashing), the page-fault rate of\nthe process will transition between peaks and valleys over time. This general\nbehavior is shown below:\n1\n0\ntime\nworking set\npage \nfault \nrate\nA peak in the page-fault rate occurs when we begin demand-paging a new\nlocality. However, once the working set of this new locality is in memory,\nthe page-fault rate falls. When the process moves to a new working set, the\npage-fault rate rises toward a peak once again, returning to a lower rate once\nthe new working set is loaded into memory. The span of time between the\nstart of one peak and the start of the next peak represents the transition from\none working set to another.",
  "one working set to another.\nin more than a page-sized chunk of memory at a time). Subsequent reads and\nwrites to the ﬁle are handled as routine memory accesses. Manipulating ﬁles\nthrough memory rather than incurring the overhead of using the read() and\nwrite() system calls simpliﬁes and speeds up ﬁle access and usage.\nNote that writes to the ﬁle mapped in memory are not necessarily\nimmediate (synchronous) writes to the ﬁle on disk. Some systems may choose\nto update the physical ﬁle when the operating system periodically checks\nwhether the page in memory has been modiﬁed. When the ﬁle is closed, all the\nmemory-mapped data are written back to disk and removed from the virtual\nmemory of the process.\nSome operating systems provide memory mapping only through a speciﬁc",
  "system call and use the standard system calls to perform all other ﬁle I/O.\nHowever, some systems choose to memory-map a ﬁle regardless of whether\nthe ﬁle was speciﬁed as memory-mapped. Let’s take Solaris as an example. If\na ﬁle is speciﬁed as memory-mapped (using the mmap() system call), Solaris\nmaps the ﬁle into the address space of the process. If a ﬁle is opened and\naccessed using ordinary system calls, such as open(), read(), and write(), 432\nChapter 9\nVirtual Memory\nprocess A\nvirtual memory\n1\n1\n1\n2\n3\n4\n5\n6\n2\n3\n3\n4\n5\n5\n4\n2\n6\n6\n1\n2\n3\n4\n5\n6\nprocess B\nvirtual memory\nphysical memory\ndisk file\nFigure 9.22\nMemory-mapped ﬁles.\nSolaris still memory-maps the ﬁle; however, the ﬁle is mapped to the kernel\naddress space. Regardless of how the ﬁle is opened, then, Solaris treats all",
  "ﬁle I/O as memory-mapped, allowing ﬁle access to take place via the efﬁcient\nmemory subsystem.\nMultiple processes may be allowed to map the same ﬁle concurrently,\nto allow sharing of data. Writes by any of the processes modify the data in\nvirtual memory and can be seen by all others that map the same section of\nthe ﬁle. Given our earlier discussions of virtual memory, it should be clear\nhow the sharing of memory-mapped sections of memory is implemented:\nthe virtual memory map of each sharing process points to the same page of\nphysical memory—the page that holds a copy of the disk block. This memory\nsharing is illustrated in Figure 9.22. The memory-mapping system calls can\nalso support copy-on-write functionality, allowing processes to share a ﬁle in",
  "read-only mode but to have their own copies of any data they modify. So that\naccess to the shared data is coordinated, the processes involved might use one\nof the mechanisms for achieving mutual exclusion described in Chapter 5.\nQuite often, shared memory is in fact implemented by memory mapping\nﬁles. Under this scenario, processes can communicate using shared memory\nby having the communicating processes memory-map the same ﬁle into their\nvirtual address spaces. The memory-mapped ﬁle serves as the region of shared\nmemory between the communicating processes (Figure 9.23). We have already\nseen this in Section 3.4.1, where a POSIX shared memory object is created and\neach communicating process memory-maps the object into its address space.",
  "In the following section, we illustrate support in the Windows API for shared\nmemory using memory-mapped ﬁles. 9.7\nMemory-Mapped Files\n433\nprocess1\nmemory-mapped\nfile\nshared\nmemory\nshared\nmemory\nshared\nmemory\nprocess2\nFigure 9.23\nShared memory using memory-mapped I/O.\n9.7.2\nShared Memory in the Windows API\nThe general outline for creating a region of shared memory using memory-\nmapped ﬁles in the Windows API involves ﬁrst creating a ﬁle mapping for the\nﬁle to be mapped and then establishing a view of the mapped ﬁle in a process’s\nvirtual address space. A second process can then open and create a view of\nthe mapped ﬁle in its virtual address space. The mapped ﬁle represents the\nshared-memory object that will enable communication to take place between\nthe processes.",
  "the processes.\nWe next illustrate these steps in more detail. In this example, a producer\nprocess ﬁrst creates a shared-memory object using the memory-mapping\nfeatures available in the Windows API. The producer then writes a message\nto shared memory. After that, a consumer process opens a mapping to the\nshared-memory object and reads the message written by the consumer.\nTo establish a memory-mapped ﬁle, a process ﬁrst opens the ﬁle to be\nmapped with the CreateFile() function, which returns a HANDLE to the\nopened ﬁle. The process then creates a mapping of this ﬁle HANDLE using\nthe CreateFileMapping() function. Once the ﬁle mapping is established, the\nprocess then establishes a view of the mapped ﬁle in its virtual address space",
  "with the MapViewOfFile() function. The view of the mapped ﬁle represents\nthe portion of the ﬁle being mapped in the virtual address space of the process\n—the entire ﬁle or only a portion of it may be mapped. We illustrate this\nsequence in the program shown in Figure 9.24. (We eliminate much of the error\nchecking for code brevity.)\nThe call to CreateFileMapping() creates a named shared-memory object\ncalled SharedObject. The consumer process will communicate using this\nshared-memory segment by creating a mapping to the same named object.\nThe producer then creates a view of the memory-mapped ﬁle in its virtual\naddress space. By passing the last three parameters the value 0, it indicates\nthat the mapped view is the entire ﬁle. It could instead have passed values",
  "specifying an offset and size, thus creating a view containing only a subsection\nof the ﬁle. (It is important to note that the entire mapping may not be loaded\ninto memory when the mapping is established. Rather, the mapped ﬁle may be\ndemand-paged, thus bringing pages into memory only as they are accessed.)\nThe MapViewOfFile() function returns a pointer to the shared-memory object;\nany accesses to this memory location are thus accesses to the memory-mapped 434\nChapter 9\nVirtual Memory\n#include <windows.h>\n#include <stdio.h>\nint main(int argc, char *argv[])\n{\nHANDLE hFile, hMapFile;\nLPVOID lpMapAddress;\nhFile = CreateFile(\"temp.txt\", /* file name */\nGENERIC READ | GENERIC WRITE, /* read/write access */\n0, /* no sharing of the file */\nNULL, /* default security */",
  "NULL, /* default security */\nOPEN ALWAYS, /* open new or existing file */\nFILE ATTRIBUTE NORMAL, /* routine file attributes */\nNULL); /* no file template */\nhMapFile = CreateFileMapping(hFile, /* file handle */\nNULL, /* default security */\nPAGE READWRITE, /* read/write access to mapped pages */\n0, /* map entire file */\n0,\nTEXT(\"SharedObject\")); /* named shared memory object */\nlpMapAddress = MapViewOfFile(hMapFile, /* mapped object handle */\nFILE MAP ALL ACCESS, /* read/write access */\n0, /* mapped view of entire file */\n0,\n0);\n/* write to shared memory */\nsprintf(lpMapAddress,\"Shared memory message\");\nUnmapViewOfFile(lpMapAddress);\nCloseHandle(hFile);\nCloseHandle(hMapFile);\n}\nFigure 9.24\nProducer writing to shared memory using the Windows API.",
  "ﬁle. In this instance, the producer process writes the message “Shared memory\nmessage” to shared memory.\nA program illustrating how the consumer process establishes a view of\nthe named shared-memory object is shown in Figure 9.25. This program is\nsomewhat simpler than the one shown in Figure 9.24, as all that is necessary\nis for the process to create a mapping to the existing named shared-memory\nobject. The consumer process must also create a view of the mapped ﬁle, just\nas the producer process did in the program in Figure 9.24. The consumer then\nreads from shared memory the message “Shared memory message” that was\nwritten by the producer process. 9.7\nMemory-Mapped Files\n435\n#include <windows.h>\n#include <stdio.h>\nint main(int argc, char *argv[])\n{\nHANDLE hMapFile;\nLPVOID lpMapAddress;",
  "{\nHANDLE hMapFile;\nLPVOID lpMapAddress;\nhMapFile = OpenFileMapping(FILE MAP ALL ACCESS, /* R/W access */\nFALSE, /* no inheritance */\nTEXT(\"SharedObject\")); /* name of mapped file object */\nlpMapAddress = MapViewOfFile(hMapFile, /* mapped object handle */\nFILE MAP ALL ACCESS, /* read/write access */\n0, /* mapped view of entire file */\n0,\n0);\n/* read from shared memory */\nprintf(\"Read message %s\", lpMapAddress);\nUnmapViewOfFile(lpMapAddress);\nCloseHandle(hMapFile);\n}\nFigure 9.25\nConsumer reading from shared memory using the Windows API.\nFinally, both processes remove the view of the mapped ﬁle with a call to\nUnmapViewOfFile(). We provide a programming exercise at the end of this\nchapter using shared memory with memory mapping in the Windows API.\n9.7.3\nMemory-Mapped I/O",
  "9.7.3\nMemory-Mapped I/O\nIn the case of I/O, as mentioned in Section 1.2.1, each I/O controller includes\nregisters to hold commands and the data being transferred. Usually, special I/O\ninstructions allow data transfers between these registers and system memory.\nTo allow more convenient access to I/O devices, many computer architectures\nprovide memory-mapped I/O. In this case, ranges of memory addresses are\nset aside and are mapped to the device registers. Reads and writes to these\nmemory addresses cause the data to be transferred to and from the device\nregisters. This method is appropriate for devices that have fast response times,\nsuch as video controllers. In the IBM PC, each location on the screen is mapped\nto a memory location. Displaying text on the screen is almost as easy as writing",
  "the text into the appropriate memory-mapped locations.\nMemory-mapped I/O is also convenient for other devices, such as the serial\nand parallel ports used to connect modems and printers to a computer. The\nCPU transfers data through these kinds of devices by reading and writing a few\ndevice registers, called an I/O port. To send out a long string of bytes through a\nmemory-mapped serial port, the CPU writes one data byte to the data register\nand sets a bit in the control register to signal that the byte is available. The device 436\nChapter 9\nVirtual Memory\ntakes the data byte and then clears the bit in the control register to signal that\nit is ready for the next byte. Then the CPU can transfer the next byte. If the\nCPU uses polling to watch the control bit, constantly looping to see whether",
  "the device is ready, this method of operation is called programmed I/O (PIO).\nIf the CPU does not poll the control bit, but instead receives an interrupt when\nthe device is ready for the next byte, the data transfer is said to be interrupt\ndriven.\n9.8\nAllocating Kernel Memory\nWhen a process running in user mode requests additional memory, pages\nare allocated from the list of free page frames maintained by the kernel.\nThis list is typically populated using a page-replacement algorithm such as\nthose discussed in Section 9.4 and most likely contains free pages scattered\nthroughout physical memory, as explained earlier. Remember, too, that if a\nuser process requests a single byte of memory, internal fragmentation will\nresult, as the process will be granted an entire page frame.",
  "Kernel memory is often allocated from a free-memory pool different from\nthe list used to satisfy ordinary user-mode processes. There are two primary\nreasons for this:\n1. The kernel requests memory for data structures of varying sizes, some of\nwhich are less than a page in size. As a result, the kernel must use memory\nconservatively and attempt to minimize waste due to fragmentation. This\nis especially important because many operating systems do not subject\nkernel code or data to the paging system.\n2. Pages allocated to user-mode processes do not necessarily have to be in\ncontiguous physical memory. However, certain hardware devices interact\ndirectly with physical memory—without the beneﬁt of a virtual memory\ninterface—and consequently may require memory residing in physically",
  "contiguous pages.\nIn the following sections, we examine two strategies for managing free memory\nthat is assigned to kernel processes: the “buddy system” and slab allocation.\n9.8.1\nBuddy System\nThe buddy system allocates memory from a ﬁxed-size segment consisting of\nphysically contiguous pages. Memory is allocated from this segment using a\npower-of-2 allocator, which satisﬁes requests in units sized as a power of 2\n(4 KB, 8 KB, 16 KB, and so forth). A request in units not appropriately sized is\nrounded up to the next highest power of 2. For example, a request for 11 KB is\nsatisﬁed with a 16-KB segment.\nLet’s consider a simple example. Assume the size of a memory segment\nis initially 256 KB and the kernel requests 21 KB of memory. The segment is",
  "initially divided into two buddies—which we will call AL and AR—each 128\nKB in size. One of these buddies is further divided into two 64-KB buddies—\nBL and BR. However, the next-highest power of 2 from 21 KB is 32 KB so either\nBL or BR is again divided into two 32-KB buddies, CL and CR. One of these 9.8\nAllocating Kernel Memory\n437\nphysically contiguous pages \n256 KB\n128 KB \nAL\n64 KB \nBR\n64 KB \nBL\n32 KB \nCL\n32 KB \nCR\n128 KB \nAR\nFigure 9.26\nBuddy system allocation.\nbuddies is used to satisfy the 21-KB request. This scheme is illustrated in Figure\n9.26, where CL is the segment allocated to the 21-KB request.\nAn advantage of the buddy system is how quickly adjacent buddies can be\ncombined to form larger segments using a technique known as coalescing. In",
  "Figure 9.26, for example, when the kernel releases the CL unit it was allocated,\nthe system can coalesce CL and CR into a 64-KB segment. This segment, BL, can\nin turn be coalesced with its buddy BR to form a 128-KB segment. Ultimately,\nwe can end up with the original 256-KB segment.\nThe obvious drawback to the buddy system is that rounding up to the\nnext highest power of 2 is very likely to cause fragmentation within allocated\nsegments. For example, a 33-KB request can only be satisﬁed with a 64-\nKB segment. In fact, we cannot guarantee that less than 50 percent of the\nallocated unit will be wasted due to internal fragmentation. In the following\nsection, we explore a memory allocation scheme where no space is lost due to\nfragmentation.\n9.8.2\nSlab Allocation",
  "fragmentation.\n9.8.2\nSlab Allocation\nA second strategy for allocating kernel memory is known as slab allocation. A\nslab is made up of one or more physically contiguous pages. A cache consists of\none or more slabs. There is a single cache for each unique kernel data structure\n—for example, a separate cache for the data structure representing process\ndescriptors, a separate cache for ﬁle objects, a separate cache for semaphores,\nand so forth. Each cache is populated with objects that are instantiations of the\nkernel data structure the cache represents. For example, the cache representing\nsemaphores stores instances of semaphore objects, the cache representing\nprocess descriptors stores instances of process descriptor objects, and so forth.",
  "The relationship among slabs, caches, and objects is shown in Figure 9.27. The\nﬁgure shows two kernel objects 3 KB in size and three objects 7 KB in size, each\nstored in a separate cache. 438\nChapter 9\nVirtual Memory\n3-KB\nobjects\n7-KB\nobjects\nkernel objects\ncaches\nslabs\nphysically\ncontiguous\npages\nFigure 9.27\nSlab allocation.\nThe slab-allocation algorithm uses caches to store kernel objects. When a\ncache is created, a number of objects—which are initially marked as free—are\nallocated to the cache. The number of objects in the cache depends on the size\nof the associated slab. For example, a 12-KB slab (made up of three continguous\n4-KB pages) could store six 2-KB objects. Initially, all objects in the cache are\nmarked as free. When a new object for a kernel data structure is needed, the",
  "allocator can assign any free object from the cache to satisfy the request. The\nobject assigned from the cache is marked as used.\nLet’s consider a scenario in which the kernel requests memory from the\nslab allocator for an object representing a process descriptor. In Linux systems,\na process descriptor is of the type struct task struct, which requires\napproximately 1.7 KB of memory. When the Linux kernel creates a new task,\nit requests the necessary memory for the struct task struct object from its\ncache. The cache will fulﬁll the request using a struct task struct object\nthat has already been allocated in a slab and is marked as free.\nIn Linux, a slab may be in one of three possible states:\n1. Full. All objects in the slab are marked as used.",
  "2. Empty. All objects in the slab are marked as free.\n3. Partial. The slab consists of both used and free objects.\nThe slab allocator ﬁrst attempts to satisfy the request with a free object in a\npartial slab. If none exists, a free object is assigned from an empty slab. If no\nempty slabs are available, a new slab is allocated from contiguous physical\npages and assigned to a cache; memory for the object is allocated from this\nslab.\nThe slab allocator provides two main beneﬁts:\n1. No memory is wasted due to fragmentation. Fragmentation is not an\nissue because each unique kernel data structure has an associated cache,\nand each cache is made up of one or more slabs that are divided into 9.9\nOther Considerations\n439\nchunks the size of the objects being represented. Thus, when the kernel",
  "requests memory for an object, the slab allocator returns the exact amount\nof memory required to represent the object.\n2. Memory requests can be satisﬁed quickly. The slab allocation scheme\nis thus particularly effective for managing memory when objects are\nfrequently allocated and deallocated, as is often the case with requests\nfrom the kernel. The act of allocating—and releasing—memory can be\na time-consuming process. However, objects are created in advance and\nthus can be quickly allocated from the cache. Furthermore, when the\nkernel has ﬁnished with an object and releases it, it is marked as free and\nreturned to its cache, thus making it immediately available for subsequent\nrequests from the kernel.\nThe slab allocator ﬁrst appeared in the Solaris 2.4 kernel. Because of its",
  "general-purpose nature, this allocator is now also used for certain user-mode\nmemory requests in Solaris. Linux originally used the buddy system; however,\nbeginning with Version 2.2, the Linux kernel adopted the slab allocator.\nRecent distributions of Linux now include two other kernel memory allo-\ncators—the SLOB and SLUB allocators. (Linux refers to its slab implementation\nas SLAB.)\nThe SLOB allocator is designed for systems with a limited amount of\nmemory, such as embedded systems. SLOB (which stands for Simple List of\nBlocks) works by maintaining three lists of objects: small (for objects less than\n256 bytes), medium (for objects less than 1,024 bytes), and large (for objects\nless than 1,024 bytes). Memory requests are allocated from an object on an",
  "appropriately sized list using a ﬁrst-ﬁt policy.\nBeginning with Version 2.6.24, the SLUB allocator replaced SLAB as the\ndefault allocator for the Linux kernel. SLUB addresses performance issues\nwith slab allocation by reducing much of the overhead required by the\nSLAB allocator. One change is to move the metadata that is stored with\neach slab under SLAB allocation to the page structure the Linux kernel\nuses for each page. Additionally, SLUB removes the per-CPU queues that the\nSLAB allocator maintains for objects in each cache. For systems with a large\nnumber of processors, the amount of memory allocated to these queues was\nnot insigniﬁcant. Thus, SLUB provides better performance as the number of\nprocessors on a system increases.\n9.9\nOther Considerations",
  "9.9\nOther Considerations\nThe major decisions that we make for a paging system are the selections of\na replacement algorithm and an allocation policy, which we discussed earlier\nin this chapter. There are many other considerations as well, and we discuss\nseveral of them here.\n9.9.1\nPrepaging\nAn obvious property of pure demand paging is the large number of page faults\nthat occur when a process is started. This situation results from trying to get the\ninitial locality into memory. The same situation may arise at other times. For 440\nChapter 9\nVirtual Memory\ninstance, when a swapped-out process is restarted, all its pages are on the disk,\nand each must be brought in by its own page fault. Prepaging is an attempt to",
  "prevent this high level of initial paging. The strategy is to bring into memory at\none time all the pages that will be needed. Some operating systems—notably\nSolaris—prepage the page frames for small ﬁles.\nIn a system using the working-set model, for example, we could keep with\neach process a list of the pages in its working set. If we must suspend a process\n(due to an I/O wait or a lack of free frames), we remember the working set for\nthat process. When the process is to be resumed (because I/O has ﬁnished or\nenough free frames have become available), we automatically bring back into\nmemory its entire working set before restarting the process.\nPrepaging may offer an advantage in some cases. The question is simply\nwhether the cost of using prepaging is less than the cost of servicing the",
  "corresponding page faults. It may well be the case that many of the pages\nbrought back into memory by prepaging will not be used.\nAssume that s pages are prepaged and a fraction # of these s pages is\nactually used (0 ≤# ≤1). The question is whether the cost of the s * # saved\npage faults is greater or less than the cost of prepaging s * (1 −#) unnecessary\npages. If # is close to 0, prepaging loses; if # is close to 1, prepaging wins.\n9.9.2\nPage Size\nThe designers of an operating system for an existing machine seldom have\na choice concerning the page size. However, when new machines are being\ndesigned, a decision regarding the best page size must be made. As you might\nexpect, there is no single best page size. Rather, there is a set of factors that",
  "support various sizes. Page sizes are invariably powers of 2, generally ranging\nfrom 4,096 (212) to 4,194,304 (222) bytes.\nHow do we select a page size? One concern is the size of the page table. For\na given virtual memory space, decreasing the page size increases the number\nof pages and hence the size of the page table. For a virtual memory of 4 MB\n(222), for example, there would be 4,096 pages of 1,024 bytes but only 512 pages\nof 8,192 bytes. Because each active process must have its own copy of the page\ntable, a large page size is desirable.\nMemory is better utilized with smaller pages, however. If a process is\nallocated memory starting at location 00000 and continuing until it has as much\nas it needs, it probably will not end exactly on a page boundary. Thus, a part",
  "of the ﬁnal page must be allocated (because pages are the units of allocation)\nbut will be unused (creating internal fragmentation). Assuming independence\nof process size and page size, we can expect that, on the average, half of the\nﬁnal page of each process will be wasted. This loss is only 256 bytes for a page\nof 512 bytes but is 4,096 bytes for a page of 8,192 bytes. To minimize internal\nfragmentation, then, we need a small page size.\nAnother problem is the time required to read or write a page. I/O time is\ncomposed of seek, latency, and transfer times. Transfer time is proportional to\nthe amount transferred (that is, the page size)—a fact that would seem to argue\nfor a small page size. However, as we shall see in Section 10.1.1, latency and",
  "seek time normally dwarf transfer time. At a transfer rate of 2 MB per second,\nit takes only 0.2 milliseconds to transfer 512 bytes. Latency time, though, is\nperhaps 8 milliseconds, and seek time 20 milliseconds. Of the total I/O time 9.9\nOther Considerations\n441\n(28.2 milliseconds), therefore, only 1 percent is attributable to the actual transfer.\nDoubling the page size increases I/O time to only 28.4 milliseconds. It takes 28.4\nmilliseconds to read a single page of 1,024 bytes but 56.4 milliseconds to read\nthe same amount as two pages of 512 bytes each. Thus, a desire to minimize\nI/O time argues for a larger page size.\nWith a smaller page size, though, total I/O should be reduced, since locality\nwill be improved. A smaller page size allows each page to match program",
  "locality more accurately. For example, consider a process 200 KB in size, of\nwhich only half (100 KB) is actually used in an execution. If we have only one\nlarge page, we must bring in the entire page, a total of 200 KB transferred and\nallocated. If instead we had pages of only 1 byte, then we could bring in only\nthe 100 KB that are actually used, resulting in only 100 KB transferred and\nallocated. With a smaller page size, then, we have better resolution, allowing\nus to isolate only the memory that is actually needed. With a larger page size,\nwe must allocate and transfer not only what is needed but also anything else\nthat happens to be in the page, whether it is needed or not. Thus, a smaller\npage size should result in less I/O and less total allocated memory.",
  "But did you notice that with a page size of 1 byte, we would have a page\nfault for each byte? A process of 200 KB that used only half of that memory\nwould generate only one page fault with a page size of 200 KB but 102,400 page\nfaults with a page size of 1 byte. Each page fault generates the large amount\nof overhead needed for processing the interrupt, saving registers, replacing a\npage, queueing for the paging device, and updating tables. To minimize the\nnumber of page faults, we need to have a large page size.\nOther factors must be considered as well (such as the relationship between\npage size and sector size on the paging device). The problem has no best\nanswer. As we have seen, some factors (internal fragmentation, locality) argue",
  "for a small page size, whereas others (table size, I/O time) argue for a large\npage size. Nevertheless, the historical trend is toward larger page sizes, even\nfor mobile systems. Indeed, the ﬁrst edition of Operating System Concepts (1983)\nused 4,096 bytes as the upper bound on page sizes, and this value was the most\ncommon page size in 1990. Modern systems may now use much larger page\nsizes, as we will see in the following section.\n9.9.3\nTLB Reach\nIn Chapter 8, we introduced the hit ratio of the TLB. Recall that the hit ratio\nfor the TLB refers to the percentage of virtual address translations that are\nresolved in the TLB rather than the page table. Clearly, the hit ratio is related\nto the number of entries in the TLB, and the way to increase the hit ratio is",
  "by increasing the number of entries in the TLB. This, however, does not come\ncheaply, as the associative memory used to construct the TLB is both expensive\nand power hungry.\nRelated to the hit ratio is a similar metric: the TLBreach. The TLB reach refers\nto the amount of memory accessible from the TLB and is simply the number\nof entries multiplied by the page size. Ideally, the working set for a process is\nstored in the TLB. If it is not, the process will spend a considerable amount of\ntime resolving memory references in the page table rather than the TLB. If we\ndouble the number of entries in the TLB, we double the TLB reach. However, 442\nChapter 9\nVirtual Memory\nfor some memory-intensive applications, this may still prove insufﬁcient for\nstoring the working set.",
  "storing the working set.\nAnother approach for increasing the TLB reach is to either increase the\nsize of the page or provide multiple page sizes. If we increase the page size\n—say, from 8 KB to 32 KB—we quadruple the TLB reach. However, this may\nlead to an increase in fragmentation for some applications that do not require\nsuch a large page size. Alternatively, an operating system may provide several\ndifferent page sizes. For example, the UltraSPARC supports page sizes of 8 KB,\n64 KB, 512 KB, and 4 MB. Of these available pages sizes, Solaris uses both 8-KB\nand 4-MB page sizes. And with a 64-entry TLB, the TLB reach for Solaris ranges\nfrom 512 KB with 8-KB pages to 256 MB with 4-MB pages. For the majority of\napplications, the 8-KB page size is sufﬁcient, although Solaris maps the ﬁrst 4 MB",
  "of kernel code and data with two 4-MB pages. Solaris also allows applications\n—such as databases—to take advantage of the large 4-MB page size.\nProviding support for multiple page sizes requires the operating system\n—not hardware—to manage the TLB. For example, one of the ﬁelds in a TLB\nentry must indicate the size of the page frame corresponding to the TLB entry.\nManaging the TLB in software and not hardware comes at a cost in performance.\nHowever, the increased hit ratio and TLB reach offset the performance costs.\nIndeed, recent trends indicate a move toward software-managed TLBs and\noperating-system support for multiple page sizes.\n9.9.4\nInverted Page Tables\nSection 8.6.3 introduced the concept of the inverted page table. The purpose",
  "of this form of page management is to reduce the amount of physical memory\nneeded to track virtual-to-physical address translations. We accomplish this\nsavings by creating a table that has one entry per page of physical memory,\nindexed by the pair <process-id, page-number>.\nBecause they keep information about which virtual memory page is stored\nin each physical frame, inverted page tables reduce the amount of physical\nmemory needed to store this information. However, the inverted page table\nno longer contains complete information about the logical address space of a\nprocess, and that information is required if a referenced page is not currently\nin memory. Demand paging requires this information to process page faults.",
  "For the information to be available, an external page table (one per process)\nmust be kept. Each such table looks like the traditional per-process page table\nand contains information on where each virtual page is located.\nBut do external page tables negate the utility of inverted page tables? Since\nthese tables are referenced only when a page fault occurs, they do not need to\nbe available quickly. Instead, they are themselves paged in and out of memory\nas necessary. Unfortunately, a page fault may now cause the virtual memory\nmanager to generate another page fault as it pages in the external page table it\nneeds to locate the virtual page on the backing store. This special case requires\ncareful handling in the kernel and a delay in the page-lookup processing.\n9.9.5\nProgram Structure",
  "9.9.5\nProgram Structure\nDemand paging is designed to be transparent to the user program. In many\ncases, the user is completely unaware of the paged nature of memory. In other 9.9\nOther Considerations\n443\ncases, however, system performance can be improved if the user (or compiler)\nhas an awareness of the underlying demand paging.\nLet’s look at a contrived but informative example. Assume that pages are\n128 words in size. Consider a C program whose function is to initialize to 0\neach element of a 128-by-128 array. The following code is typical:\nint i, j;\nint[128][128] data;\nfor (j = 0; j < 128; j++)\nfor (i = 0; i < 128; i++)\ndata[i][j] = 0;\nNotice that the array is stored row major; that is, the array is stored\ndata[0][0], data[0][1], · · ·, data[0][127], data[1][0], data[1][1], · · ·,",
  "data[127][127]. For pages of 128 words, each row takes one page. Thus,\nthe preceding code zeros one word in each page, then another word in each\npage, and so on. If the operating system allocates fewer than 128 frames to the\nentire program, then its execution will result in 128 × 128 = 16,384 page faults.\nIn contrast, suppose we change the code to\nint i, j;\nint[128][128] data;\nfor (i = 0; i < 128; i++)\nfor (j = 0; j < 128; j++)\ndata[i][j] = 0;\nThis code zeros all the words on one page before starting the next page,\nreducing the number of page faults to 128.\nCareful selection of data structures and programming structures can\nincrease locality and hence lower the page-fault rate and the number of pages in\nthe working set. For example, a stack has good locality, since access is always",
  "made to the top. A hash table, in contrast, is designed to scatter references,\nproducing bad locality. Of course, locality of reference is just one measure of\nthe efﬁciency of the use of a data structure. Other heavily weighted factors\ninclude search speed, total number of memory references, and total number of\npages touched.\nAt a later stage, the compiler and loader can have a signiﬁcant effect on\npaging. Separating code and data and generating reentrant code means that\ncode pages can be read-only and hence will never be modiﬁed. Clean pages\ndo not have to be paged out to be replaced. The loader can avoid placing\nroutines across page boundaries, keeping each routine completely in one page.\nRoutines that call each other many times can be packed into the same page.",
  "This packaging is a variant of the bin-packing problem of operations research:\ntry to pack the variable-sized load segments into the ﬁxed-sized pages so that\ninterpage references are minimized. Such an approach is particularly useful\nfor large page sizes. 444\nChapter 9\nVirtual Memory\n9.9.6\nI/O Interlock and Page Locking\nWhen demand paging is used, we sometimes need to allow some of the pages\nto be locked in memory. One such situation occurs when I/O is done to or from\nuser (virtual) memory. I/O is often implemented by a separate I/O processor.\nFor example, a controller for a USB storage device is generally given the number\nof bytes to transfer and a memory address for the buffer (Figure 9.28). When\nthe transfer is complete, the CPU is interrupted.",
  "the transfer is complete, the CPU is interrupted.\nWe must be sure the following sequence of events does not occur: A process\nissues an I/O request and is put in a queue for that I/O device. Meanwhile, the\nCPU is given to other processes. These processes cause page faults, and one of\nthem, using a global replacement algorithm, replaces the page containing the\nmemory buffer for the waiting process. The pages are paged out. Some time\nlater, when the I/O request advances to the head of the device queue, the I/O\noccurs to the speciﬁed address. However, this frame is now being used for a\ndifferent page belonging to another process.\nThere are two common solutions to this problem. One solution is never to\nexecute I/O to user memory. Instead, data are always copied between system",
  "memory and user memory. I/O takes place only between system memory\nand the I/O device. To write a block on tape, we ﬁrst copy the block to system\nmemory and then write it to tape. This extra copying may result in unacceptably\nhigh overhead.\nAnother solution is to allow pages to be locked into memory. Here, a lock\nbit is associated with every frame. If the frame is locked, it cannot be selected\nfor replacement. Under this approach, to write a block on tape, we lock into\nmemory the pages containing the block. The system can then continue as\nusual. Locked pages cannot be replaced. When the I/O is complete, the pages\nare unlocked.\nbuffer\ndisk drive\nFigure 9.28\nThe reason why frames used for I/O must be in memory. 9.10\nOperating-System Examples\n445",
  "Operating-System Examples\n445\nLock bits are used in various situations. Frequently, some or all of the\noperating-system kernel is locked into memory. Many operating systems\ncannot tolerate a page fault caused by the kernel or by a speciﬁc kernel module,\nincluding the one performing memory management. User processes may also\nneed to lock pages into memory. A database process may want to manage\na chunk of memory, for example, moving blocks between disk and memory\nitself because it has the best knowledge of how it is going to use its data. Such\npinning of pages in memory is fairly common, and most operating systems\nhave a system call allowing an application to request that a region of its logical\naddress space be pinned. Note that this feature could be abused and could",
  "cause stress on the memory-management algorithms. Therefore, an application\nfrequently requires special privileges to make such a request.\nAnother use for a lock bit involves normal page replacement. Consider\nthe following sequence of events: A low-priority process faults. Selecting a\nreplacement frame, the paging system reads the necessary page into memory.\nReady to continue, the low-priority process enters the ready queue and waits\nfor the CPU. Since it is a low-priority process, it may not be selected by the\nCPU scheduler for a time. While the low-priority process waits, a high-priority\nprocess faults. Looking for a replacement, the paging system sees a page that\nis in memory but has not been referenced or modiﬁed: it is the page that the",
  "low-priority process just brought in. This page looks like a perfect replacement:\nit is clean and will not need to be written out, and it apparently has not been\nused for a long time.\nWhether the high-priority process should be able to replace the low-priority\nprocess is a policy decision. After all, we are simply delaying the low-priority\nprocess for the beneﬁt of the high-priority process. However, we are wasting\nthe effort spent to bring in the page for the low-priority process. If we decide\nto prevent replacement of a newly brought-in page until it can be used at least\nonce, then we can use the lock bit to implement this mechanism. When a page\nis selected for replacement, its lock bit is turned on. It remains on until the\nfaulting process is again dispatched.",
  "faulting process is again dispatched.\nUsing a lock bit can be dangerous: the lock bit may get turned on but\nnever turned off. Should this situation occur (because of a bug in the operating\nsystem, for example), the locked frame becomes unusable. On a single-user\nsystem, the overuse of locking would hurt only the user doing the locking.\nMultiuser systems must be less trusting of users. For instance, Solaris allows\nlocking “hints,” but it is free to disregard these hints if the free-frame pool\nbecomes too small or if an individual process requests that too many pages be\nlocked in memory.\n9.10 Operating-System Examples\nIn this section, we describe how Windows and Solaris implement virtual\nmemory.\n9.10.1\nWindows\nWindows implements virtual memory using demand paging with clustering.",
  "Clustering handles page faults by bringing in not only the faulting page but also 446\nChapter 9\nVirtual Memory\nseveral pages following the faulting page. When a process is ﬁrst created, it is\nassigned a working-set minimum and maximum. The working-set minimum\nis the minimum number of pages the process is guaranteed to have in memory.\nIf sufﬁcient memory is available, a process may be assigned as many pages as\nits working-set maximum. (In some circumstances, a process may be allowed\nto exceed its working-set maximum.) The virtual memory manager maintains a\nlist of free page frames. Associated with this list is a threshold value that is used\nto indicate whether sufﬁcient free memory is available. If a page fault occurs for",
  "a process that is below its working-set maximum, the virtual memory manager\nallocates a page from this list of free pages. If a process that is at its working-set\nmaximum incurs a page fault, it must select a page for replacement using a\nlocal LRU page-replacement policy.\nWhen the amount of free memory falls below the threshold, the virtual\nmemory manager uses a tactic known as automatic working-set trimming to\nrestore the value above the threshold. Automatic working-set trimming works\nby evaluating the number of pages allocated to processes. If a process has\nbeen allocated more pages than its working-set minimum, the virtual memory\nmanager removes pages until the process reaches its working-set minimum.\nA process that is at its working-set minimum may be allocated pages from",
  "the free-page-frame list once sufﬁcient free memory is available. Windows\nperforms working-set trimming on both user mode and system processes.\nVirtual memory is discussed in great detail in the Windows case study in\nChapter 19.\n9.10.2\nSolaris\nIn Solaris, when a thread incurs a page fault, the kernel assigns a page to the\nfaulting thread from the list of free pages it maintains. Therefore, it is imperative\nthat the kernel keep a sufﬁcient amount of free memory available. Associated\nwith this list of free pages is a parameter—lotsfree—that represents a\nthreshold to begin paging. The lotsfree parameter is typically set to 1/64\nthe size of the physical memory. Four times per second, the kernel checks\nwhether the amount of free memory is less than lotsfree. If the number of",
  "free pages falls below lotsfree, a process known as a pageout starts up. The\npageout process is similar to the second-chance algorithm described in Section\n9.4.5.2, except that it uses two hands while scanning pages, rather than one.\nThe pageout process works as follows: The front hand of the clock scans\nall pages in memory, setting the reference bit to 0. Later, the back hand of the\nclock examines the reference bit for the pages in memory, appending each page\nwhose reference bit is still set to 0 to the free list and writing to disk its contents\nif modiﬁed. Solaris maintains a cache list of pages that have been “freed” but\nhave not yet been overwritten. The free list contains frames that have invalid\ncontents. Pages can be reclaimed from the cache list if they are accessed before",
  "being moved to the free list.\nThe pageout algorithm uses several parameters to control the rate at which\npages are scanned (known as the scanrate). The scanrate is expressed in\npages per second and ranges from slowscan to fastscan. When free memory\nfalls below lotsfree, scanning occurs at slowscan pages per second and\nprogresses to fastscan, depending on the amount of free memory available.\nThe default value of slowscan is 100 pages per second. Fastscan is typically 9.10\nOperating-System Examples\n447\nset to the value (total physical pages)/2 pages per second, with a maximum of\n8,192 pages per second. This is shown in Figure 9.29 (with fastscan set to the\nmaximum).\nThe distance (in pages) between the hands of the clock is determined",
  "by a system parameter, handspread. The amount of time between the front\nhand’s clearing a bit and the back hand’s investigating its value depends on\nthe scanrate and the handspread. If scanrate is 100 pages per second and\nhandspread is 1,024 pages, 10 seconds can pass between the time a bit is set by\nthe front hand and the time it is checked by the back hand. However, because\nof the demands placed on the memory system, a scanrate of several thousand\nis not uncommon. This means that the amount of time between clearing and\ninvestigating a bit is often a few seconds.\nAs mentioned above, the pageout process checks memory four times per\nsecond. However, if free memory falls below the value of desfree (Figure 9.29),\npageout will run a hundred times per second with the intention of keeping at",
  "least desfree free memory available. If the pageout process is unable to keep\nthe amount of free memory at desfree for a 30-second average, the kernel\nbegins swapping processes, thereby freeing all pages allocated to swapped\nprocesses. In general, the kernel looks for processes that have been idle for\nlong periods of time. If the system is unable to maintain the amount of free\nmemory at minfree, the pageout process is called for every request for a new\npage.\nRecent releases of the Solaris kernel have provided enhancements of\nthe paging algorithm. One such enhancement involves recognizing pages\nfrom shared libraries. Pages belonging to libraries that are being shared by\nseveral processes—even if they are eligible to be claimed by the scanner—",
  "are skipped during the page-scanning process. Another enhancement concerns\nminfree\nscan rate\n100 \nslowscan\n8192 \nfastscan\ndesfree\namount of free memory\nlotsfree\nFigure 9.29\nSolaris page scanner. 448\nChapter 9\nVirtual Memory\ndistinguishing pages that have been allocated to processes from pages allocated\ntoregular ﬁles. Thisisknownasprioritypagingand iscovered inSection12.6.2.\n9.11\nSummary\nIt is desirable to be able to execute a process whose logical address space is\nlarger than the available physical address space. Virtual memory is a technique\nthat enables us to map a large logical address space onto a smaller physical\nmemory. Virtual memory allows us to run extremely large processes and to\nraise the degree of multiprogramming, increasing CPU utilization. Further, it",
  "frees application programmers from worrying about memory availability. In\naddition, with virtual memory, several processes can share system libraries\nand memory. With virtual memory, we can also use an efﬁcient type of process\ncreation known as copy-on-write, wherein parent and child processes share\nactual pages of memory.\nVirtual memory is commonly implemented by demand paging. Pure\ndemand paging never brings in a page until that page is referenced. The ﬁrst\nreference causes a page fault to the operating system. The operating-system\nkernel consults an internal table to determine where the page is located on the\nbacking store. It then ﬁnds a free frame and reads the page in from the backing\nstore. The page table is updated to reﬂect this change, and the instruction that",
  "caused the page fault is restarted. This approach allows a process to run even\nthough its entire memory image is not in main memory at once. As long as the\npage-fault rate is reasonably low, performance is acceptable.\nWe can use demand paging to reduce the number of frames allocated to\na process. This arrangement can increase the degree of multiprogramming\n(allowing more processes to be available for execution at one time) and—in\ntheory, at least—the CPU utilization of the system. It also allows processes\nto be run even though their memory requirements exceed the total available\nphysical memory. Such processes run in virtual memory.\nIf total memory requirements exceed the capacity of physical memory,\nthen it may be necessary to replace pages from memory to free frames for",
  "new pages. Various page-replacement algorithms are used. FIFO page replace-\nment is easy to program but suffers from Belady’s anomaly. Optimal page\nreplacement requires future knowledge. LRU replacement is an approxima-\ntion of optimal page replacement, but even it may be difﬁcult to implement.\nMost page-replacement algorithms, such as the second-chance algorithm, are\napproximations of LRU replacement.\nIn addition to a page-replacement algorithm, a frame-allocation policy\nis needed. Allocation can be ﬁxed, suggesting local page replacement, or\ndynamic, suggesting global replacement. The working-set model assumes that\nprocesses execute in localities. The working set is the set of pages in the current\nlocality. Accordingly, each process should be allocated enough frames for its",
  "current working set. If a process does not have enough memory for its working\nset, it will thrash. Providing enough frames to each process to avoid thrashing\nmay require process swapping and scheduling.\nMost operating systems provide features for memory mapping ﬁles, thus\nallowing ﬁle I/O to be treated as routine memory access. The Win32 API\nimplements shared memory through memory mapping of ﬁles. Practice Exercises\n449\nKernel processes typically require memory to be allocated using pages\nthat are physically contiguous. The buddy system allocates memory to kernel\nprocesses in units sized according to a power of 2, which often results in\nfragmentation. Slab allocators assign kernel data structures to caches associated",
  "with slabs, which are made up of one or more physically contiguous pages.\nWith slab allocation, no memory is wasted due to fragmentation, and memory\nrequests can be satisﬁed quickly.\nIn addition to requiring us to solve the major problems of page replacement\nand frame allocation, the proper design of a paging system requires that\nwe consider prepaging, page size, TLB reach, inverted page tables, program\nstructure, I/O interlock and page locking, and other issues.\nPractice Exercises\n9.1\nUnder what circumstances do page faults occur? Describe the actions\ntaken by the operating system when a page fault occurs.\n9.2\nAssume that you have a page-reference string for a process with m\nframes (initially all empty). The page-reference string has length p, and",
  "n distinct page numbers occur in it. Answer these questions for any\npage-replacement algorithms:\na.\nWhat is a lower bound on the number of page faults?\nb.\nWhat is an upper bound on the number of page faults?\n9.3\nConsider the page table shown in Figure 9.30 for a system with 12-bit\nvirtual and physical addresses and with 256-byte pages. The list of free\npage frames is D, E, F (that is, D is at the head of the list, E is second,\nand F is last).\nPage\nPage Frame\n0\n1\n2\n3\n4\n5\n–\n2\nC\nA\n–\n4\n6\n3\n7\n–\n8\nB\n9\n0\nFigure 9.30\nPage table for Exercise 9.3. 450\nChapter 9\nVirtual Memory\nConvert the following virtual addresses to their equivalent physical\naddresses in hexadecimal. All numbers are given in hexadecimal. (A\ndash for a page frame indicates that the page is not in memory.)\n• 9EF\n• 111\n• 700\n• 0FF",
  "• 9EF\n• 111\n• 700\n• 0FF\n9.4\nConsider the following page-replacement algorithms. Rank these algo-\nrithms on a ﬁve-point scale from “bad” to “perfect” according to their\npage-fault rate. Separate those algorithms that suffer from Belady’s\nanomaly from those that do not.\na.\nLRU replacement\nb.\nFIFO replacement\nc.\nOptimal replacement\nd.\nSecond-chance replacement\n9.5\nDiscuss the hardware support required to support demand paging.\n9.6\nAn operating system supports a paged virtual memory. The central\nprocessor has a cycle time of 1 microsecond. It costs an additional 1\nmicrosecond to access a page other than the current one. Pages have 1,000\nwords, and the paging device is a drum that rotates at 3,000 revolutions\nper minute and transfers 1 million words per second. The following",
  "statistical measurements were obtained from the system:\n• One percent of all instructions executed accessed a page other than\nthe current page.\n• Of the instructions that accessed another page, 80 percent accessed\na page already in memory.\n• When a new page was required, the replaced page was modiﬁed 50\npercent of the time.\nCalculate the effective instruction time on this system, assuming that the\nsystem is running one process only and that the processor is idle during\ndrum transfers.\n9.7\nConsider the two-dimensional array A:\nint A[][] = new int[100][100];\nwhere A[0][0] is at location 200 in a paged memory system with pages\nof size 200. A small process that manipulates the matrix resides in page\n0 (locations 0 to 199). Thus, every instruction fetch will be from page 0.",
  "For three page frames, how many page faults are generated by the\nfollowing array-initialization loops? Use LRU replacement, and assume Practice Exercises\n451\nthat page frame 1 contains the process and the other two are initially\nempty.\na.\nfor (int j = 0; j < 100; j++)\nfor (int i = 0; i < 100; i++)\nA[i][j] = 0;\nb.\nfor (int i = 0; i < 100; i++)\nfor (int j = 0; j < 100; j++)\nA[i][j] = 0;\n9.8\nConsider the following page reference string:\n1, 2, 3, 4, 2, 1, 5, 6, 2, 1, 2, 3, 7, 6, 3, 2, 1, 2, 3, 6.\nHow many page faults would occur for the following replacement\nalgorithms, assuming one, two, three, four, ﬁve, six, and seven frames?\nRemember that all frames are initially empty, so your ﬁrst unique pages\nwill cost one fault each.\n• LRU replacement\n• FIFO replacement\n• Optimal replacement\n9.9",
  "• FIFO replacement\n• Optimal replacement\n9.9\nSuppose that youwant touse apagingalgorithmthat requiresareference\nbit (such as second-chance replacement or working-set model), but\nthe hardware does not provide one. Sketch how you could simulate a\nreference bit even if one were not provided by the hardware, or explain\nwhy it is not possible to do so. If it is possible, calculate what the cost\nwould be.\n9.10\nYou have devised a new page-replacement algorithm that you think may\nbe optimal. In some contorted test cases, Belady’s anomaly occurs. Is the\nnew algorithm optimal? Explain your answer.\n9.11\nSegmentation is similar to paging but uses variable-sized “pages.” Deﬁne\ntwo segment-replacement algorithms, one based on the FIFO page-",
  "replacement scheme and the other on the LRU page-replacement scheme.\nRemember that since segments are not the same size, the segment that\nis chosen for replacement may be too small to leave enough consecutive\nlocations for the needed segment. Consider strategies for systems where\nsegments cannot be relocated and strategies for systems where they can.\n9.12\nConsider a demand-paged computer system where the degree of mul-\ntiprogramming is currently ﬁxed at four. The system was recently\nmeasured to determine utilization of the CPU and the paging disk. Three\nalternative results are shown below. For each case, what is happening?\nCan the degree of multiprogramming be increased to increase the CPU\nutilization? Is the paging helping?\na.\nCPU utilization 13 percent; disk utilization 97 percent\nb.",
  "b.\nCPU utilization 87 percent; disk utilization 3 percent\nc.\nCPU utilization 13 percent; disk utilization 3 percent 452\nChapter 9\nVirtual Memory\n9.13\nWe have an operating system for a machine that uses base and limit\nregisters, but we have modiﬁed the machine to provide a page table.\nCan the page tables be set up to simulate base and limit registers? How\ncan they be, or why can they not be?\nExercises\n9.14\nAssume that a program has just referenced an address in virtual memory.\nDescribe a scenario in which each of the following can occur. (If no such\nscenario can occur, explain why.)\n• TLB miss with no page fault\n• TLB miss and page fault\n• TLB hit and no page fault\n• TLB hit and page fault\n9.15\nA simpliﬁed view of thread states isReady, Running, and Blocked, where",
  "a thread is either ready and waiting to be scheduled, is running on the\nprocessor, or is blocked (for example, waiting for I/O). This is illustrated\nin Figure 9.31. Assuming a thread is in the Running state, answer the\nfollowing questions, and explain your answer:\na.\nWill the thread change state if it incurs a page fault? If so, to what\nstate will it change?\nb.\nWill the thread change state if it generates a TLBmiss that is resolved\nin the page table? If so, to what state will it change?\nc.\nWill the thread change state if an address reference is resolved in\nthe page table? If so, to what state will it change?\n9.16\nConsider a system that uses pure demand paging.\na.\nWhen a process ﬁrst starts execution, how would you characterize\nthe page-fault rate?\nb.",
  "the page-fault rate?\nb.\nOnce the working set for a process is loaded into memory, how\nwould you characterize the page-fault rate?\nReady\nBlocked\nRunning\nFigure 9.31\nThread state diagram for Exercise 9.15. Exercises\n453\nc.\nAssume that a process changes its locality and the size of the new\nworking set is too large to be stored in available free memory.\nIdentify some options system designers could choose from to\nhandle this situation.\n9.17\nWhat is the copy-on-write feature, and under what circumstances is its\nuse beneﬁcial? What hardware support is required to implement this\nfeature?\n9.18\nA certain computer provides its users with a virtual memory space of\n232 bytes. The computer has 222 bytes of physical memory. The virtual\nmemory is implemented by paging, and the page size is 4,096 bytes.",
  "A user process generates the virtual address 11123456. Explain how\nthe system establishes the corresponding physical location. Distinguish\nbetween software and hardware operations.\n9.19\nAssume that we have a demand-paged memory. The page table is held in\nregisters. It takes 8 milliseconds to service a page fault if an empty frame\nis available or if the replaced page is not modiﬁed and 20 milliseconds if\nthe replaced page is modiﬁed. Memory-access time is 100 nanoseconds.\nAssume that the page to be replaced is modiﬁed 70 percent of the\ntime. What is the maximum acceptable page-fault rate for an effective\naccess time of no more than 200 nanoseconds?\n9.20\nWhen a page fault occurs, the process requesting the page must block",
  "while waiting for the page to be brought from disk into physical memory.\nAssume that there exists a process with ﬁve user-level threads and that\nthe mapping of user threads to kernel threads is one to one. If one user\nthread incurs a page fault while accessing its stack, would the other\nuser threads belonging to the same process also be affected by the page\nfault—that is, would they also have to wait for the faulting page to be\nbrought into memory? Explain.\n9.21\nConsider the following page reference string:\n7, 2, 3, 1, 2, 5, 3, 4, 6, 7, 7, 1, 0, 5, 4, 6, 2, 3, 0 , 1.\nAssuming demand paging with three frames, how many page faults\nwould occur for the following replacement algorithms?\n• LRU replacement\n• FIFO replacement\n• Optimal replacement\n9.22",
  "• FIFO replacement\n• Optimal replacement\n9.22\nThe page table shown in Figure 9.32 is for a system with 16-bit virtual\nand physical addresses and with 4,096-byte pages. The reference bit is\nset to 1 when the page has been referenced. Periodically, a thread zeroes\nout all values of the reference bit. A dash for a page frame indicates\nthe page is not in memory. The page-replacement algorithm is localized\nLRU, and all numbers are provided in decimal.\na.\nConvert the following virtual addresses (in hexadecimal) to the\nequivalent physical addresses. You may provide answers in either 454\nChapter 9\nVirtual Memory\nPage\nPage Frame\nReference Bit\n0\n9\n0\n1\n1\n0\n2\n14\n0\n3\n10\n0\n4\n–\n0\n5\n13\n0\n6\n8\n0\n7\n15\n0\n8\n–\n0\n9\n0\n0\n10\n5\n0\n11\n4\n0\n12\n–\n0\n13\n–\n0\n14\n3\n0\n15\n2\n0\nFigure 9.32\nPage table for Exercise 9.22.",
  "15\n2\n0\nFigure 9.32\nPage table for Exercise 9.22.\nhexadecimal or decimal. Also set the reference bit for the appro-\npriate entry in the page table.\n• 0xE12C\n• 0x3A9D\n• 0xA9D9\n• 0x7001\n• 0xACA1\nb.\nUsing the above addresses as a guide, provide an example of a\nlogical address (in hexadecimal) that results in a page fault.\nc.\nFrom what set of page frames will the LRU page-replacement\nalgorithm choose in resolving a page fault?\n9.23\nAssume that you are monitoring the rate at which the pointer in the\nclock algorithm moves. (The pointer indicates the candidate page for\nreplacement.) What can you say about the system if you notice the\nfollowing behavior:\na.\nPointer is moving fast.\nb.\nPointer is moving slow.\n9.24\nDiscuss situations in which the least frequently used (LFU) page-",
  "replacementalgorithmgeneratesfewer page faults thanthe least recently\nused (LRU) page-replacement algorithm. Also discuss under what cir-\ncumstances the opposite holds.\n9.25\nDiscuss situations in which the most frequently used (MFU) page-\nreplacementalgorithmgeneratesfewer page faults thanthe least recently\nused (LRU) page-replacement algorithm. Also discuss under what cir-\ncumstances the opposite holds. Exercises\n455\n9.26\nThe VAX/VMS system uses a FIFO replacement algorithm for resident\npages and a free-frame pool of recently used pages. Assume that the\nfree-frame pool is managed using the LRU replacement policy. Answer\nthe following questions:\na.\nIf a page fault occurs and the page does not exist in the free-frame\npool, how is free space generated for the newly requested page?\nb.",
  "b.\nIf a page fault occurs and the page exists in the free-frame pool,\nhow is the resident page set and the free-frame pool managed to\nmake space for the requested page?\nc.\nWhat does the system degenerate to if the number of resident pages\nis set to one?\nd.\nWhat does the system degenerate to if the number of pages in the\nfree-frame pool is zero?\n9.27\nConsider a demand-paging system with the following time-measured\nutilizations:\nCPU utilization\n20%\nPaging disk\n97.7%\nOther I/O devices\n5%\nFor each of the following, indicate whether it will (or is likely to) improve\nCPU utilization. Explain your answers.\na.\nInstall a faster CPU.\nb.\nInstall a bigger paging disk.\nc.\nIncrease the degree of multiprogramming.\nd.\nDecrease the degree of multiprogramming.\ne.\nInstall more main memory.\nf.",
  "e.\nInstall more main memory.\nf.\nInstall a faster hard disk or multiple controllers with multiple hard\ndisks.\ng.\nAdd prepaging to the page-fetch algorithms.\nh.\nIncrease the page size.\n9.28\nSuppose that a machine provides instructions that can access memory\nlocations using the one-level indirect addressing scheme. What sequence\nof page faults is incurred when all of the pages of a program are\ncurrently nonresident and the ﬁrst instruction of the program is an\nindirect memory-load operation? What happens when the operating\nsystem is using a per-process frame allocation technique and only two\npages are allocated to this process?\n9.29\nSuppose that your replacement policy (in a paged system) is to examine\neach page regularly and to discard that page if it has not been used since",
  "the last examination. What would you gain and what would you lose\nby using this policy rather than LRU or second-chance replacement? 456\nChapter 9\nVirtual Memory\n9.30\nA page-replacement algorithm should minimize the number of page\nfaults. We can achieve this minimization by distributing heavily used\npages evenly over all of memory, rather than having them compete for\na small number of page frames. We can associate with each page frame\na counter of the number of pages associated with that frame. Then,\nto replace a page, we can search for the page frame with the smallest\ncounter.\na.\nDeﬁne a page-replacement algorithm using this basic idea. Specif-\nically address these problems:\ni.\nWhat is the initial value of the counters?\nii.\nWhen are counters increased?\niii.\nWhen are counters decreased?",
  "iii.\nWhen are counters decreased?\niv.\nHow is the page to be replaced selected?\nb.\nHow many page faults occur for your algorithm for the following\nreference string with four page frames?\n1, 2, 3, 4, 5, 3, 4, 1, 6, 7, 8, 7, 8, 9, 7, 8, 9, 5, 4, 5, 4, 2.\nc.\nWhat is the minimum number of page faults for an optimal page-\nreplacement strategy for the reference string in part b with four\npage frames?\n9.31\nConsider a demand-paging system with a paging disk that has an\naverage access and transfer time of 20 milliseconds. Addresses are\ntranslated through a page table in main memory, with an access time of 1\nmicrosecond per memory access. Thus, each memory reference through\nthe page table takes two accesses. To improve this time, we have added",
  "an associative memory that reduces access time to one memory reference\nif the page-table entry is in the associative memory.\nAssume that 80 percent of the accesses are in the associative memory\nand that, of those remaining, 10 percent (or 2 percent of the total) cause\npage faults. What is the effective memory access time?\n9.32\nWhat is the cause of thrashing? How does the system detect thrashing?\nOnce it detects thrashing, what can the system do to eliminate this\nproblem?\n9.33\nIs it possible for a process to have two working sets, one representing\ndata and another representing code? Explain.\n9.34\nConsider the parameter ! used to deﬁne the working-set window in the\nworking-set model. When ! is set to a small value, what is the effect",
  "on the page-fault frequency and the number of active (nonsuspended)\nprocesses currently executing in the system? What is the effect when !\nis set to a very high value?\n9.35\nIn a 1,024-KB segment, memory is allocated using the buddy system.\nUsing Figure 9.26 as a guide, draw a tree illustrating how the following\nmemory requests are allocated:\n• Request 6-KB Programming Problems\n457\n• Request 250 bytes\n• Request 900 bytes\n• Request 1,500 bytes\n• Request 7-KB\nNext, modify the tree for the following releases of memory. Perform\ncoalescing whenever possible:\n• Release 250 bytes\n• Release 900 bytes\n• Release 1,500 bytes\n9.36\nA system provides support for user-level and kernel-level threads. The\nmapping in this system is one to one (there is a corresponding kernel",
  "thread for each user thread). Does a multithreaded process consist of (a)\na working set for the entire process or (b) a working set for each thread?\nExplain\n9.37\nThe slab-allocation algorithm uses a separate cache for each different\nobject type. Assuming there is one cache per object type, explain why\nthis scheme doesn’t scale well with multiple CPUs. What could be done\nto address this scalability issue?\n9.38\nConsider a system that allocates pages of different sizes to its processes.\nWhat are the advantages of such a paging scheme? What modiﬁcations\nto the virtual memory system provide this functionality?\nProgramming Problems\n9.39\nWrite a program that implements the FIFO, LRU, and optimal page-\nreplacement algorithms presented in this chapter. First, generate a",
  "random page-reference string where page numbers range from 0 to 9.\nApply the random page-reference string to each algorithm, and record\nthe number of page faults incurred by each algorithm. Implement the\nreplacement algorithms so that the number of page frames can vary from\n1 to 7. Assume that demand paging is used.\n9.40\nRepeat Exercise 3.22, this time using Windows shared memory. In partic-\nular, using the producer—consumer strategy, design two programs that\ncommunicate with shared memory using the Windows API as outlined\nin Section 9.7.2. The producer will generate the numbers speciﬁed in\nthe Collatz conjecture and write them to a shared memory object. The\nconsumer will then read and output the sequence of numbers from\nshared memory.",
  "shared memory.\nIn this instance, the producer will be passed an integer parameter\non the command line specifying how many numbers to produce (for\nexample, providing 5 on the command line means the producer process\nwill generate the ﬁrst ﬁve numbers). 458\nChapter 9\nVirtual Memory\nProgramming Projects\nDesigning a Virtual Memory Manager\nThis project consists of writing a program that translates logical to physical\naddresses for a virtual address space of size 216 = 65,536 bytes. Your program\nwill read from a ﬁle containing logical addresses and, using a TLB as well as\na page table, will translate each logical address to its corresponding physical\naddress and output the value of the byte stored at the translated physical",
  "address. The goal behind this project is to simulate the steps involved in\ntranslating logical to physical addresses.\nSpeciﬁcs\nYour program will read a ﬁle containing several 32-bit integer numbers that\nrepresent logical addresses. However, you need only be concerned with 16-bit\naddresses, so you must mask the rightmost 16 bits of each logical address.\nThese 16 bits are divided into (1) an 8-bit page number and (2) 8-bit page offset.\nHence, the addresses are structured as shown in Figure 9.33.\nOther speciﬁcs include the following:\n• 28 entries in the page table\n• Page size of 28 bytes\n• 16 entries in the TLB\n• Frame size of 28 bytes\n• 256 frames\n• Physical memory of 65,536 bytes (256 frames × 256-byte frame size)\nAdditionally, your program need only be concerned with reading logical",
  "addresses and translating them to their corresponding physical addresses. You\ndo not need to support writing to the logical address space.\nAddress Translation\nYour program will translate logical to physical addresses using a TLB and page\ntable as outlined in Section 8.5. First, the page number is extracted from the\nlogical address, and the TLB is consulted. In the case of a TLB-hit, the frame\nnumber is obtained from the TLB. In the case of a TLB-miss, the page table\nmust be consulted. In the latter case, either the frame number is obtained\noffset\n0\n7\n8\n15\n16\n31\npage \nnumber\nFigure 9.33\nAddress structure. Programming Projects\n459\npage\nnumber\n0\n1\n2\n15\n0\n1\n2\n255\nTLB\npage\ntable\nTLB hit\nTLB miss\npage 0\npage 255\npage 1\npage 2\nframe\nnumber\n....\n....\n0\n1\n2\n255\nphysical\nmemory\nframe 0\nframe 255",
  "....\n0\n1\n2\n255\nphysical\nmemory\nframe 0\nframe 255\nframe 1\nframe 2\n....\npage\nnumber\noffset\nframe\nnumber\noffset\nFigure 9.34\nA representation of the address-translation process.\nfrom the page table or a page fault occurs. A visual representation of the\naddress-translation process appears in Figure 9.34.\nHandling Page Faults\nYour program will implement demand paging as described in Section 9.2. The\nbacking store is represented by the ﬁle BACKING STORE.bin, a binary ﬁle of size\n65,536 bytes. When a page fault occurs, you will read in a 256-byte page from the\nﬁle BACKING STORE and store it in an available page frame in physical memory.\nFor example, if a logical address with page number 15 resulted in a page fault,\nyour program would read in page 15 from BACKING STORE (remember that",
  "pages begin at 0 and are 256 bytes in size) and store it in a page frame in\nphysical memory. Once this frame is stored (and the page table and TLB are\nupdated), subsequent accesses to page 15 will be resolved by either the TLB or\nthe page table.\nYou will need to treat BACKING STORE.bin as a random-access ﬁle so that\nyou can randomly seek to certain positions of the ﬁle for reading. We suggest\nusing the standard C library functions for performing I/O, including fopen(),\nfread(), fseek(), and fclose().\nThe size of physical memory is the same as the size of the virtual\naddress space—65,536 bytes—so you do not need to be concerned about\npage replacements during a page fault. Later, we describe a modiﬁcation\nto this project using a smaller amount of physical memory; at that point, a",
  "page-replacement strategy will be required. 460\nChapter 9\nVirtual Memory\nTest File\nWe provide the ﬁle addresses.txt, which contains integer values represent-\ning logical addresses ranging from 0 −65535 (the size of the virtual address\nspace). Your program will open this ﬁle, read each logical address and translate\nit to its corresponding physical address, and output the value of the signed byte\nat the physical address.\nHow to Begin\nFirst, write a simple program that extracts the page number and offset (based\non Figure 9.33) from the following integer numbers:\n1, 256, 32768, 32769, 128, 65534, 33153\nPerhaps the easiest way to do this is by using the operators for bit-masking\nand bit-shifting. Once you can correctly establish the page number and offset",
  "from an integer number, you are ready to begin.\nInitially, we suggest that you bypass the TLB and use only a page table. You\ncan integrate the TLB once your page table is working properly. Remember,\naddress translation can work without a TLB; the TLB just makes it faster. When\nyou are ready to implement the TLB, recall that it has only 16 entries, so you\nwill need to use a replacement strategy when you update a full TLB. You may\nuse either a FIFO or an LRU policy for updating your TLB.\nHow to Run Your Program\nYour program should run as follows:\n./a.out addresses.txt\nYour program will read in the ﬁle addresses.txt, which contains 1,000 logical\naddresses ranging from 0 to 65535. Your program is to translate each logical",
  "address to a physical address and determine the contents of the signed byte\nstored at the correct physical address. (Recall that in the C language, the char\ndata type occupies a byte of storage, so we suggest using char values.)\nYour program is to output the following values:\n1. The logical address being translated (the integer value being read from\naddresses.txt).\n2. The corresponding physical address (what your program translates the\nlogical address to).\n3. The signed byte value stored at the translated physical address.\nWe also provide the ﬁle correct.txt, which contains the correct output\nvalues for the ﬁle addresses.txt. You should use this ﬁle to determine if your\nprogram is correctly translating logical to physical addresses.\nStatistics",
  "Statistics\nAfter completion, your program is to report the following statistics: Bibliographical Notes\n461\n1. Page-fault rate—The percentage of address references that resulted in\npage faults.\n2.\nTLB hit rate—The percentage of address references that were resolved in\nthe TLB.\nSince the logical addresses in addresses.txt were generated randomly\nand do not reﬂect any memory access locality, do not expect to have a high TLB\nhit rate.\nModiﬁcations\nThis project assumes that physical memory is the same size as the virtual\naddress space. In practice, physical memory is typically much smaller than a\nvirtual address space. A suggested modiﬁcation is to use a smaller physical\naddress space. We recommend using 128 page frames rather than 256. This",
  "change will require modifying your program so that it keeps track of free page\nframes as well as implementing a page-replacement policy using either FIFO\nor LRU (Section 9.4).\nBibliographical Notes\nDemand paging was ﬁrst used in the Atlas system, implemented on the\nManchester University MUSE computer around 1960 ([Kilburn et al. (1961)]).\nAnother early demand-paging system was MULTICS, implemented on the GE\n645 system ([Organick (1972)]). Virtual memory was added to Unix in 1979\n[Babaoglu and Joy (1981)]\n[Belady et al. (1969)] were the ﬁrst researchers to observe that the FIFO\nreplacement strategy may produce the anomaly that bears Belady’s name.\n[Mattson et al. (1970)] demonstrated that stack algorithms are not subject to\nBelady’s anomaly.",
  "Belady’s anomaly.\nThe optimal replacement algorithm was presented by [Belady (1966)]\nand was proved to be optimal by [Mattson et al. (1970)]. Belady’s optimal\nalgorithm is for a ﬁxed allocation; [Prieve and Fabry (1976)] presented an\noptimal algorithm for situations in which the allocation can vary.\nThe enhanced clock algorithm was discussed by [Carr and Hennessy\n(1981)].\nThe working-set model was developed by [Denning (1968)]. Discussions\nconcerning the working-set model were presented by [Denning (1980)].\nThe scheme for monitoring the page-fault rate was developed by [Wulf\n(1969)], who successfully applied this technique to the Burroughs B5500\ncomputer system.\nBuddy system memory allocators were described in [Knowlton (1965)],",
  "[Peterson and Norman (1977)], and [Purdom, Jr. and Stigler (1970)]. [Bonwick\n(1994)] discussed the slab allocator, and [Bonwick and Adams (2001)] extended\nthe discussion to multiple processors. Other memory-ﬁtting algorithms can be\nfound in [Stephenson (1983)], [Bays (1977)], and [Brent (1989)]. A survey of\nmemory-allocation strategies can be found in [Wilson et al. (1995)].\n[Solomon and Russinovich (2000)] and [Russinovich and Solomon (2005)]\ndescribed how Windows implements virtual memory. [McDougall and Mauro 462\nChapter 9\nVirtual Memory\n(2007)] discussed virtual memory in Solaris. Virtual memory techniques in\nLinux and FreeBSD were described by [Love (2010)] and [McKusick and\nNeville-Neil (2005)], respectively. [Ganapathy and Schimmel (1998)] and",
  "[Navarro et al. (2002)] discussed operating system support for multiple page\nsizes.\nBibliography\n[Babaoglu and Joy (1981)]\nO. Babaoglu and W. Joy, “Converting a Swap-Based\nSystem to Do Paging in an Architecture Lacking Page-Reference Bits”, Pro-\nceedings of the ACM Symposium on Operating Systems Principles (1981), pages\n78–86.\n[Bays (1977)]\nC. Bays, “A Comparison of Next-Fit, First-Fit and Best-Fit”, Com-\nmunications of the ACM, Volume 20, Number 3 (1977), pages 191–192.\n[Belady (1966)]\nL. A. Belady, “A Study of Replacement Algorithms for a Virtu-\nal-Storage Computer”, IBM Systems Journal, Volume 5, Number 2 (1966), pages\n78–101.\n[Belady et al. (1969)]\nL. A. Belady, R. A. Nelson, and G. S. Shedler, “An Anomaly\nin Space-Time Characteristics of Certain Programs Running in a Paging",
  "Machine”, Communications of the ACM, Volume 12, Number 6 (1969), pages\n349–353.\n[Bonwick (1994)]\nJ. Bonwick, “The Slab Allocator: An Object-Caching Kernel\nMemory Allocator”, USENIX Summer (1994), pages 87–98.\n[Bonwick and Adams (2001)]\nJ. Bonwick and J. Adams, “Magazines and Vmem:\nExtending the Slab Allocator to Many CPUs and Arbitrary Resources”, Proceed-\nings of the 2001 USENIX Annual Technical Conference (2001).\n[Brent (1989)]\nR. Brent, “Efﬁcient Implementation of the First-Fit Strategy for\nDynamic Storage Allocation”, ACM Transactions on Programming Languages and\nSystems, Volume 11, Number 3 (1989), pages 388–403.\n[Carr and Hennessy (1981)]\nW. R. Carr and J. L. Hennessy, “WSClock—A\nSimple and Effective Algorithm for Virtual Memory Management”, Proceedings",
  "of the ACM Symposium on Operating Systems Principles (1981), pages 87–95.\n[Denning (1968)]\nP. J. Denning, “The Working Set Model for Program Behavior”,\nCommunications of the ACM, Volume 11, Number 5 (1968), pages 323–333.\n[Denning (1980)]\nP. J. Denning, “Working Sets Past and Present”, IEEE Transac-\ntions on Software Engineering, Volume SE-6, Number 1 (1980), pages 64–84.\n[Ganapathy and Schimmel (1998)]\nN. Ganapathy and C. Schimmel, “General\nPurpose Operating System Support for Multiple Page Sizes”, Proceedings of the\nUSENIX Technical Conference (1998).\n[Kilburn et al. (1961)]\nT. Kilburn, D. J. Howarth, R. B. Payne, and F. H. Sumner,\n“The Manchester University Atlas Operating System, Part I: Internal Organiza-\ntion”, Computer Journal, Volume 4, Number 3 (1961), pages 222–225. Bibliography",
  "463\n[Knowlton (1965)]\nK. C. Knowlton, “A Fast Storage Allocator”, Communications\nof the ACM, Volume 8, Number 10 (1965), pages 623–624.\n[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[Mattson et al. (1970)]\nR. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger,\n“Evaluation Techniques for Storage Hierarchies”, IBM Systems Journal, Volume\n9, Number 2 (1970), pages 78–117.\n[McDougall and Mauro (2007)]\nR. McDougall and J. Mauro, Solaris Internals,\nSecond Edition, Prentice Hall (2007).\n[McKusick and Neville-Neil (2005)]\nM. K. McKusick and G. V. Neville-Neil,\nThe Design and Implementation of the FreeBSD UNIX Operating System, Addison\nWesley (2005).\n[Navarro et al. (2002)]\nJ. Navarro, S. Lyer, P. Druschel, and A. Cox, “Practical,",
  "Transparent Operating System Support for Superpages”, Proceedings of the\nUSENIX Symposium on Operating Systems Design and Implementation (2002).\n[Organick (1972)]\nE. I. Organick, The Multics System: An Examination of Its\nStructure, MIT Press (1972).\n[Peterson and Norman (1977)]\nJ. L. Peterson and T. A. Norman, “Buddy Sys-\ntems”, Communications of the ACM, Volume 20, Number 6 (1977), pages 421–431.\n[Prieve and Fabry (1976)]\nB. G. Prieve and R. S. Fabry, “VMIN—An Optimal\nVariable Space Page-Replacement Algorithm”, Communications of the ACM,\nVolume 19, Number 5 (1976), pages 295–297.\n[Purdom, Jr. and Stigler (1970)]\nP. W. Purdom, Jr. and S. M. Stigler, “Statistical\nProperties of the Buddy System”, J. ACM, Volume 17, Number 4 (1970), pages\n683–697.\n[Russinovich and Solomon (2005)]",
  "683–697.\n[Russinovich and Solomon (2005)]\nM. E. Russinovich and D. A. Solomon,\nMicrosoft Windows Internals, Fourth Edition, Microsoft Press (2005).\n[Solomon and Russinovich (2000)]\nD. A. Solomon and M. E. Russinovich, Inside\nMicrosoft Windows 2000, Third Edition, Microsoft Press (2000).\n[Stephenson (1983)]\nC. J. Stephenson, “Fast Fits: A New Method for Dynamic\nStorage Allocation”, Proceedings of the Ninth Symposium on Operating Systems\nPrinciples (1983), pages 30–32.\n[Wilson et al. (1995)]\nP. R. Wilson, M. S. Johnstone, M. Neely, and D. Boles,\n“Dynamic Storage Allocation: A Survey and Critical Review”, Proceedings of the\nInternational Workshop on Memory Management (1995), pages 1–116.\n[Wulf (1969)]\nW. A. Wulf, “Performance Monitors for Multiprogramming Sys-",
  "tems”, Proceedings of the ACM Symposium on Operating Systems Principles (1969),\npages 175–181.  Part Four\nStorage\nManagement\nSince main memory is usually too small to accommodate all the data and\nprograms permanently, the computer system must provide secondary\nstorage to back up main memory. Modern computer systems use disks\nas the primary on-line storage medium for information (both programs\nand data). The ﬁle system provides the mechanism for on-line storage\nof and access to both data and programs residing on the disks. A ﬁle\nis a collection of related information deﬁned by its creator. The ﬁles are\nmapped by the operating system onto physical devices. Files are normally\norganized into directories for ease of use.\nThe devices that attach to a computer vary in many aspects. Some",
  "devices transfer a character or a block of characters at a time. Some\ncan be accessed only sequentially, others randomly. Some transfer\ndata synchronously, others asynchronously. Some are dedicated, some\nshared. They can be read-only or read–write. They vary greatly in speed.\nIn many ways, they are also the slowest major component of the\ncomputer.\nBecause of all this device variation, the operating system needs to\nprovide a wide range of functionality to applications, to allow them to\ncontrol all aspects of the devices. One key goal of an operating system’s\nI/O subsystem is to provide the simplest interface possible to the rest of\nthe system. Because devices are a performance bottleneck, another key\nis to optimize I/O for maximum concurrency.  10\nC H A P T E R\nMass-Storage\nStructure",
  "C H A P T E R\nMass-Storage\nStructure\nThe ﬁle system can be viewed logically as consisting of three parts. In Chapter\n11, we examine the user and programmer interface to the ﬁle system. In\nChapter 12, we describe the internal data structures and algorithms used by\nthe operating system to implement this interface. In this chapter, we begin a\ndiscussion of ﬁle systems at the lowest level: the structure of secondary storage.\nWe ﬁrst describe the physical structure of magnetic disks and magnetic tapes.\nWe then describe disk-scheduling algorithms, which schedule the order of\ndisk I/Os to maximize performance. Next, we discuss disk formatting and\nmanagement of boot blocks, damaged blocks, and swap space. We conclude\nwith an examination of the structure of RAID systems.\nCHAPTER OBJECTIVES",
  "CHAPTER OBJECTIVES\n• To describe the physical structure of secondary storage devices and its\neffects on the uses of the devices.\n• To explain the performance characteristics of mass-storage devices.\n• To evaluate disk scheduling algorithms.\n• To discuss operating-system services provided for mass storage, including\nRAID.\n10.1\nOverview of Mass-Storage Structure\nIn this section, we present a general overview of the physical structure of\nsecondary and tertiary storage devices.\n10.1.1\nMagnetic Disks\nMagnetic disks provide the bulk of secondary storage for modern computer\nsystems. Conceptually, disks are relatively simple (Figure 10.1). Each disk\nplatter has a ﬂat circular shape, like a CD. Common platter diameters range",
  "from 1.8 to 3.5 inches. The two surfaces of a platter are covered with a magnetic\nmaterial. We store information by recording it magnetically on the platters.\n467 468\nChapter 10\nMass-Storage Structure\ntrack t\nsector s\nspindle\ncylinder c\nplatter\narm\nread-write\nhead\narm assembly\nrotation\nFigure 10.1\nMoving-head disk mechanism.\nA read–write head “ﬂies” just above each surface of every platter. The\nheads are attached to a disk arm that moves all the heads as a unit. The surface\nof a platter is logically divided into circular tracks, which are subdivided into\nsectors. The set of tracks that are at one arm position makes up a cylinder.\nThere may be thousands of concentric cylinders in a disk drive, and each track\nmay contain hundreds of sectors. The storage capacity of common disk drives",
  "is measured in gigabytes.\nWhen the disk is in use, a drive motor spins it at high speed. Most drives\nrotate 60 to 250 times per second, speciﬁed in terms of rotations per minute\n(RPM). Common drives spin at 5,400, 7,200, 10,000, and 15,000 RPM. Disk speed\nhas two parts. The transfer rate is the rate at which data ﬂow between the drive\nand the computer. The positioning time, or random-access time, consists of\ntwo parts: the time necessary to move the disk arm to the desired cylinder,\ncalled the seek time, and the time necessary for the desired sector to rotate to\nthe disk head, called the rotational latency. Typical disks can transfer several\nmegabytes of data per second, and they have seek times and rotational latencies\nof several milliseconds.",
  "of several milliseconds.\nBecause the disk head ﬂies on an extremely thin cushion of air (measured\nin microns), there is a danger that the head will make contact with the disk\nsurface. Although the disk platters are coated with a thin protective layer, the\nhead will sometimes damage the magnetic surface. This accident is called a\nhead crash. A head crash normally cannot be repaired; the entire disk must be\nreplaced.\nA disk can be removable, allowing different disks to be mounted as needed.\nRemovable magnetic disks generally consist of one platter, held in a plastic\ncase to prevent damage while not in the disk drive. Other forms of removable\ndisks include CDs, DVDs, and Blu-ray discs as well as removable ﬂash-memory\ndevices known as ﬂash drives (which are a type of solid-state drive). 10.1",
  "Overview of Mass-Storage Structure\n469\nA disk drive is attached to a computer by a set of wires called an I/O\nbus. Several kinds of buses are available, including advanced technology\nattachment (ATA), serial ATA (SATA), eSATA, universal serial bus (USB), and\nﬁbre channel (FC). The data transfers on a bus are carried out by special\nelectronic processors called controllers. The host controller is the controller at\nthe computer end of the bus. A disk controller is built into each disk drive. To\nperform a disk I/O operation, the computer places a command into the host\ncontroller, typically using memory-mapped I/O ports, as described in Section\n9.7.3. The host controller then sends the command via messages to the disk",
  "controller, and the disk controller operates the disk-drive hardware to carry\nout the command. Disk controllers usually have a built-in cache. Data transfer\nat the disk drive happens between the cache and the disk surface, and data\ntransfer to the host, at fast electronic speeds, occurs between the cache and the\nhost controller.\n10.1.2\nSolid-State Disks\nSometimes old technologies are used in new ways as economics change or\nthe technologies evolve. An example is the growing importance of solid-state\ndisks, or SSDs. Simply described, an SSD is nonvolatile memory that is used like\na hard drive. There are many variations of this technology, from DRAM with a\nbattery to allow it to maintain its state in a power failure through ﬂash-memory",
  "technologies like single-level cell (SLC) and multilevel cell (MLC) chips.\nSSDs have the same characteristics as traditional hard disks but can be more\nreliable because they have no moving parts and faster because they have no\nseek time or latency. In addition, they consume less power. However, they are\nmore expensive per megabyte than traditional hard disks, have less capacity\nthan the larger hard disks, and may have shorter life spans than hard disks,\nso their uses are somewhat limited. One use for SSDs is in storage arrays,\nwhere they hold ﬁle-system metadata that require high performance. SSDs are\nalso used in some laptop computers to make them smaller, faster, and more\nenergy-efﬁcient.\nBecause SSDs can be much faster than magnetic disk drives, standard bus",
  "interfaces can cause a major limit on throughput. Some SSDs are designed to\nconnect directly to the system bus (PCI, for example). SSDs are changing other\ntraditional aspects of computer design as well. Some systems use them as\na direct replacement for disk drives, while others use them as a new cache\ntier, moving data between magnetic disks, SSDs, and memory to optimize\nperformance.\nIn the remainder of this chapter, some sections pertain to SSDs, while\nothers do not. For example, because SSDs have no disk head, disk-scheduling\nalgorithms largely do not apply. Throughput and formatting, however, do\napply.\n10.1.3\nMagnetic Tapes\nMagnetic tape was used as an early secondary-storage medium. Although it\nis relatively permanent and can hold large quantities of data, its access time",
  "is slow compared with that of main memory and magnetic disk. In addition,\nrandom access to magnetic tape is about a thousand times slower than random\naccess to magnetic disk, so tapes are not very useful for secondary storage. 470\nChapter 10\nMass-Storage Structure\nDISK TRANSFER RATES\nAs with many aspects of computing, published performance numbers for\ndisks are not the same as real-world performance numbers. Stated transfer\nrates are always lower than effective transfer rates, for example. The transfer\nrate may be the rate at which bits can be read from the magnetic media by\nthe disk head, but that is different from the rate at which blocks are delivered\nto the operating system.\nTapes are used mainly for backup, for storage of infrequently used information,",
  "and as a medium for transferring information from one system to another.\nA tape is kept in a spool and is wound or rewound past a read–write head.\nMoving to the correct spot on a tape can take minutes, but once positioned, tape\ndrives can write data at speeds comparable to disk drives. Tape capacities vary\ngreatly, depending on the particular kind of tape drive, with current capacities\nexceeding several terabytes. Some tapes have built-in compression that can\nmore than double the effective storage. Tapes and their drivers are usually\ncategorized by width, including 4, 8, and 19 millimeters and 1/4 and 1/2 inch.\nSome are named according to technology, such as LTO-5 and SDLT.\n10.2 Disk Structure\nModern magnetic disk drives are addressed as large one-dimensional arrays of",
  "logical blocks, where the logical block is the smallest unit of transfer. The size\nof a logical block is usually 512 bytes, although some disks can be low-level\nformatted to have a different logical block size, such as 1,024 bytes. This option\nis described in Section 10.5.1. The one-dimensional array of logical blocks is\nmapped onto the sectors of the disk sequentially. Sector 0 is the ﬁrst sector\nof the ﬁrst track on the outermost cylinder. The mapping proceeds in order\nthrough that track, then through the rest of the tracks in that cylinder, and then\nthrough the rest of the cylinders from outermost to innermost.\nBy using this mapping, we can—at least in theory—convert a logical block\nnumber into an old-style disk address that consists of a cylinder number, a track",
  "number within that cylinder, and a sector number within that track. In practice,\nit is difﬁcult to perform this translation, for two reasons. First, most disks have\nsome defective sectors, but the mapping hides this by substituting spare sectors\nfrom elsewhere on the disk. Second, the number of sectors per track is not a\nconstant on some drives.\nLet’s look more closely at the second reason. On media that use constant\nlinear velocity (CLV), the density of bits per track is uniform. The farther a\ntrack is from the center of the disk, the greater its length, so the more sectors it\ncan hold. As we move from outer zones to inner zones, the number of sectors\nper track decreases. Tracks in the outermost zone typically hold 40 percent",
  "more sectors than do tracks in the innermost zone. The drive increases its\nrotation speed as the head moves from the outer to the inner tracks to keep\nthe same rate of data moving under the head. This method is used in CD-ROM 10.3\nDisk Attachment\n471\nand DVD-ROM drives. Alternatively, the disk rotation speed can stay constant;\nin this case, the density of bits decreases from inner tracks to outer tracks to\nkeep the data rate constant. This method is used in hard disks and is known as\nconstant angular velocity (CAV).\nThe number of sectors per track has been increasing as disk technology\nimproves, and the outer zone of a disk usually has several hundred sectors per\ntrack. Similarly, the number of cylinders per disk has been increasing; large\ndisks have tens of thousands of cylinders.",
  "disks have tens of thousands of cylinders.\n10.3 Disk Attachment\nComputers access disk storage in two ways. One way is via I/O ports (or\nhost-attached storage); this is common on small systems. The other way is via\na remote host in a distributed ﬁle system; this is referred to as network-attached\nstorage.\n10.3.1\nHost-Attached Storage\nHost-attached storage is storage accessed through local I/O ports. These ports\nuse several technologies. The typical desktop PC uses an I/O bus architecture\ncalled IDE or ATA. This architecture supports a maximum of two drives per I/O\nbus. A newer, similar protocol that has simpliﬁed cabling is SATA.\nHigh-end workstations and servers generally use more sophisticated I/O\narchitectures such as ﬁbre channel (FC), a high-speed serial architecture that",
  "can operate over optical ﬁber or over a four-conductor copper cable. It has\ntwo variants. One is a large switched fabric having a 24-bit address space. This\nvariant is expected to dominate in the future and is the basis of storage-area\nnetworks (SANs), discussed in Section 10.3.3. Because of the large address space\nand the switched nature of the communication, multiple hosts and storage\ndevices can attach to the fabric, allowing great ﬂexibility in I/O communication.\nThe other FC variant is an arbitrated loop (FC-AL) that can address 126 devices\n(drives and controllers).\nA wide variety of storage devices are suitable for use as host-attached\nstorage. Among these are hard disk drives, RAID arrays, and CD, DVD, and",
  "tape drives. The I/O commands that initiate data transfers to a host-attached\nstorage device are reads and writes of logical data blocks directed to speciﬁcally\nidentiﬁed storage units (such as bus ID or target logical unit).\n10.3.2\nNetwork-Attached Storage\nA network-attached storage (NAS) device is a special-purpose storage system\nthat is accessed remotely over a data network (Figure 10.2). Clients access\nnetwork-attached storage via a remote-procedure-call interface such as NFS\nfor UNIX systems or CIFS for Windows machines. The remote procedure calls\n(RPCs) are carried via TCP or UDP over an IP network—usually the same local-\narea network (LAN) that carries all data trafﬁc to the clients. Thus, it may be\neasiest to think of NAS as simply another storage-access protocol. The network-",
  "attached storage unit is usually implemented as a RAID array with software\nthat implements the RPC interface. 472\nChapter 10\nMass-Storage Structure\nNAS\nclient\nNAS\nclient\nclient\nLAN/WAN\nFigure 10.2\nNetwork-attached storage.\nNetwork-attached storage provides a convenient way for all the computers\non a LAN to share a pool of storage with the same ease of naming and access\nenjoyed with local host-attached storage. However, it tends to be less efﬁcient\nand have lower performance than some direct-attached storage options.\niSCSI is the latest network-attached storage protocol. In essence, it uses the\nIP network protocol to carry the SCSI protocol. Thus, networks—rather than\nSCSI cables—can be used as the interconnects between hosts and their storage.",
  "As a result, hosts can treat their storage as if it were directly attached, even if\nthe storage is distant from the host.\n10.3.3\nStorage-Area Network\nOne drawback of network-attached storage systems is that the storage I/O\noperations consume bandwidth on the data network, thereby increasing the\nlatency of network communication. This problem can be particularly acute\nin large client–server installations—the communication between servers and\nclients competes for bandwidth with the communication among servers and\nstorage devices.\nA storage-area network (SAN) is a private network (using storage protocols\nrather than networking protocols) connecting servers and storage units, as\nshown in Figure 10.3. The power of a SAN lies in its ﬂexibility. Multiple hosts",
  "and multiple storage arrays can attach to the same SAN, and storage can\nbe dynamically allocated to hosts. A SAN switch allows or prohibits access\nbetween the hosts and the storage. As one example, if a host is running low\non disk space, the SAN can be conﬁgured to allocate more storage to that host.\nSANs make it possible for clusters of servers to share the same storage and for\nstorage arrays to include multiple direct host connections. SANs typically have\nmore ports—as well as more expensive ports—than storage arrays.\nFC is the most common SAN interconnect, although the simplicity of iSCSI is\nincreasing its use. Another SAN interconnect is InﬁniBand — a special-purpose\nbus architecture that provides hardware and software support for high-speed",
  "interconnection networks for servers and storage units.\n10.4 Disk Scheduling\nOne of the responsibilities of the operating system is to use the hardware\nefﬁciently. For the disk drives, meeting this responsibility entails having fast 10.4\nDisk Scheduling\n473\nLAN/WAN\nstorage\narray\nstorage\narray\ndata-processing\ncenter\nweb content\nprovider\nserver\nclient\nclient\nclient\nserver\ntape\nlibrary\nSAN\nFigure 10.3\nStorage-area network.\naccess time and large disk bandwidth. For magnetic disks, the access time has\ntwo major components, as mentioned in Section 10.1.1. The seek time is the\ntime for the disk arm to move the heads to the cylinder containing the desired\nsector. The rotational latency is the additional time for the disk to rotate the",
  "desired sector to the disk head. The disk bandwidth is the total number of bytes\ntransferred, divided by the total time between the ﬁrst request for service and\nthe completion of the last transfer. We can improve both the access time and\nthe bandwidth by managing the order in which disk I/O requests are serviced.\nWhenever a process needs I/O to or from the disk, it issues a system call to\nthe operating system. The request speciﬁes several pieces of information:\n• Whether this operation is input or output\n• What the disk address for the transfer is\n• What the memory address for the transfer is\n• What the number of sectors to be transferred is\nIf the desired disk drive and controller are available, the request can be\nserviced immediately. If the drive or controller is busy, any new requests",
  "for service will be placed in the queue of pending requests for that drive.\nFor a multiprogramming system with many processes, the disk queue may\noften have several pending requests. Thus, when one request is completed, the\noperating system chooses which pending request to service next. How does\nthe operating system make this choice? Any one of several disk-scheduling\nalgorithms can be used, and we discuss them next.\n10.4.1\nFCFS Scheduling\nThe simplest form of disk scheduling is, of course, the ﬁrst-come, ﬁrst-served\n(FCFS) algorithm. This algorithm is intrinsically fair, but it generally does not\nprovide the fastest service. Consider, for example, a disk queue with requests\nfor I/O to blocks on cylinders\n98, 183, 37, 122, 14, 124, 65, 67, 474\nChapter 10\nMass-Storage Structure\n0\n14\n37",
  "Chapter 10\nMass-Storage Structure\n0\n14\n37\n536567\n98\n122124\n183199\nqueue $ 98, 183, 37, 122, 14, 124, 65, 67\nhead starts at 53\nFigure 10.4\nFCFS disk scheduling.\nin that order. If the disk head is initially at cylinder 53, it will ﬁrst move from\n53 to 98, then to 183, 37, 122, 14, 124, 65, and ﬁnally to 67, for a total head\nmovement of 640 cylinders. This schedule is diagrammed in Figure 10.4.\nThe wild swing from 122 to 14 and then back to 124 illustrates the problem\nwith this schedule. If the requests for cylinders 37 and 14 could be serviced\ntogether, before or after the requests for 122 and 124, the total head movement\ncould be decreased substantially, and performance could be thereby improved.\n10.4.2\nSSTF Scheduling",
  "10.4.2\nSSTF Scheduling\nIt seems reasonable to service all the requests close to the current head position\nbefore moving the head far away to service other requests. This assumption is\nthe basis for the shortest-seek-time-ﬁrst (SSTF) algorithm. The SSTF algorithm\nselects the request with the least seek time from the current head position.\nIn other words, SSTF chooses the pending request closest to the current head\nposition.\nFor our example request queue, the closest request to the initial head\nposition (53) is at cylinder 65. Once we are at cylinder 65, the next closest\nrequest is at cylinder 67. From there, the request at cylinder 37 is closer than the\none at 98, so 37 is served next. Continuing, we service the request at cylinder 14,",
  "then 98, 122, 124, and ﬁnally 183 (Figure 10.5). This scheduling method results\nin a total head movement of only 236 cylinders—little more than one-third\nof the distance needed for FCFS scheduling of this request queue. Clearly, this\nalgorithm gives a substantial improvement in performance.\nSSTF scheduling is essentially a form of shortest-job-ﬁrst (SJF) scheduling;\nand like SJF scheduling, it may cause starvation of some requests. Remember\nthat requests may arrive at any time. Suppose that we have two requests in\nthe queue, for cylinders 14 and 186, and while the request from 14 is being\nserviced, a new request near 14 arrives. This new request will be serviced\nnext, making the request at 186 wait. While this request is being serviced,",
  "another request close to 14 could arrive. In theory, a continual stream of requests\nnear one another could cause the request for cylinder 186 to wait indeﬁnitely. 10.4\nDisk Scheduling\n475\n0\n14\n37\n536567\n98\n122124\n183199\nqueue $ 98, 183, 37, 122, 14, 124, 65, 67\nhead starts at 53\nFigure 10.5\nSSTF disk scheduling.\nThis scenario becomes increasingly likely as the pending-request queue grows\nlonger.\nAlthough the SSTF algorithm is a substantial improvement over the FCFS\nalgorithm, it is not optimal. In the example, we can do better by moving the\nhead from 53 to 37, even though the latter is not closest, and then to 14, before\nturning around to service 65, 67, 98, 122, 124, and 183. This strategy reduces\nthe total head movement to 208 cylinders.\n10.4.3\nSCAN Scheduling",
  "10.4.3\nSCAN Scheduling\nIn the SCAN algorithm, the disk arm starts at one end of the disk and moves\ntoward the other end, servicing requests as it reaches each cylinder, until it gets\nto the other end of the disk. At the other end, the direction of head movement\nis reversed, and servicing continues. The head continuously scans back and\nforth across the disk. The SCAN algorithm is sometimes called the elevator\nalgorithm, since the disk arm behaves just like an elevator in a building, ﬁrst\nservicing all the requests going up and then reversing to service requests the\nother way.\nLet’s return to our example to illustrate. Before applying SCAN to schedule\nthe requests on cylinders 98, 183, 37, 122, 14, 124, 65, and 67, we need to know",
  "the direction of head movement in addition to the head’s current position.\nAssuming that the disk arm is moving toward 0 and that the initial head\nposition is again 53, the head will next service 37 and then 14. At cylinder 0,\nthe arm will reverse and will move toward the other end of the disk, servicing\nthe requests at 65, 67, 98, 122, 124, and 183 (Figure 10.6). If a request arrives in\nthe queue just in front of the head, it will be serviced almost immediately; a\nrequest arriving just behind the head will have to wait until the arm moves to\nthe end of the disk, reverses direction, and comes back.\nAssuming a uniform distribution of requests for cylinders, consider the\ndensity of requests when the head reaches one end and reverses direction. At",
  "this point, relatively few requests are immediately in front of the head, since\nthese cylinders have recently been serviced. The heaviest density of requests 476\nChapter 10\nMass-Storage Structure\n0\n14\n37\n536567\n98\n122124\n183199\nqueue $ 98, 183, 37, 122, 14, 124, 65, 67\nhead starts at 53\nFigure 10.6\nSCAN disk scheduling.\nis at the other end of the disk. These requests have also waited the longest, so\nwhy not go there ﬁrst? That is the idea of the next algorithm.\n10.4.4\nC-SCAN Scheduling\nCircular SCAN (C-SCAN) scheduling is a variant of SCAN designed to provide\na more uniform wait time. Like SCAN, C-SCAN moves the head from one end\nof the disk to the other, servicing requests along the way. When the head\nreaches the other end, however, it immediately returns to the beginning of",
  "the disk without servicing any requests on the return trip (Figure 10.7). The\nC-SCAN scheduling algorithm essentially treats the cylinders as a circular list\nthat wraps around from the ﬁnal cylinder to the ﬁrst one.\n0\n14\n37\n53 65 67\n98\n122124\n183199\nqueue = 98, 183, 37, 122, 14, 124, 65, 67\nhead starts at 53\nFigure 10.7\nC-SCAN disk scheduling. 10.4\nDisk Scheduling\n477\n10.4.5\nLOOK Scheduling\nAs we described them, both SCAN and C-SCAN move the disk arm across the\nfull width of the disk. In practice, neither algorithm is often implemented this\nway. More commonly, the arm goes only as far as the ﬁnal request in each\ndirection. Then, it reverses direction immediately, without going all the way to\nthe end of the disk. Versions of SCAN and C-SCAN that follow this pattern are",
  "called LOOK and C-LOOK scheduling, because they look for a request before\ncontinuing to move in a given direction (Figure 10.8).\n10.4.6\nSelection of a Disk-Scheduling Algorithm\nGiven so many disk-scheduling algorithms, how do we choose the best one?\nSSTF is common and has a natural appeal because it increases performance over\nFCFS. SCAN and C-SCAN perform better for systems that place a heavy load on\nthe disk, because they are less likely to cause a starvation problem. For any\nparticular list of requests, we can deﬁne an optimal order of retrieval, but the\ncomputation needed to ﬁnd an optimal schedule may not justify the savings\nover SSTF or SCAN. With any scheduling algorithm, however, performance\ndepends heavily on the number and types of requests. For instance, suppose",
  "that the queue usually has just one outstanding request. Then, all scheduling\nalgorithms behave the same, because they have only one choice of where to\nmove the disk head: they all behave like FCFS scheduling.\nRequests for disk service can be greatly inﬂuenced by the ﬁle-allocation\nmethod. A program reading a contiguously allocated ﬁle will generate several\nrequests that are close together on the disk, resulting in limited head movement.\nA linked or indexed ﬁle, in contrast, may include blocks that are widely\nscattered on the disk, resulting in greater head movement.\nThe location of directories and index blocks is also important. Since every\nﬁle must be opened to be used, and opening a ﬁle requires searching the",
  "directory structure, the directories will be accessed frequently. Suppose that a\ndirectory entry is on the ﬁrst cylinder and a ﬁle’s data are on the ﬁnal cylinder. In\nthis case, the disk head has to move the entire width of the disk. If the directory\n0\n14\n37\n536567\n98\n122124\n183199\nqueue = 98, 183, 37, 122, 14, 124, 65, 67\nhead starts at 53\nFigure 10.8\nC-LOOK disk scheduling. 478\nChapter 10\nMass-Storage Structure\nDISK SCHEDULING and SSDs\nThe disk-scheduling algorithms discussed in this section focus primarily on\nminimizing the amount of disk head movement in magnetic disk drives.\nSSDs—which do not contain moving disk heads—commonly use a simple\nFCFS policy. For example, the Linux Noop scheduler uses an FCFS policy\nbut modiﬁes it to merge adjacent requests. The observed behavior of SSDs",
  "indicates that the time required to service reads is uniform but that, because\nof the properties of ﬂash memory, write service time is not uniform. Some\nSSD schedulers have exploited this property and merge only adjacent write\nrequests, servicing all read requests in FCFS order.\nentry were on the middle cylinder, the head would have to move only one-half\nthe width. Caching the directories and index blocks in main memory can also\nhelp to reduce disk-arm movement, particularly for read requests.\nBecause of these complexities, the disk-scheduling algorithm should be\nwritten as a separate module of the operating system, so that it can be replaced\nwith a different algorithm if necessary. Either SSTF or LOOK is a reasonable\nchoice for the default algorithm.",
  "choice for the default algorithm.\nThe scheduling algorithms described here consider only the seek distances.\nFor modern disks, the rotational latency can be nearly as large as the\naverage seek time. It is difﬁcult for the operating system to schedule for\nimproved rotational latency, though, because modern disks do not disclose the\nphysical location of logical blocks. Disk manufacturers have been alleviating\nthis problem by implementing disk-scheduling algorithms in the controller\nhardware built into the disk drive. If the operating system sends a batch of\nrequests to the controller, the controller can queue them and then schedule\nthem to improve both the seek time and the rotational latency.\nIf I/O performance were the only consideration, the operating system",
  "would gladly turn over the responsibility of disk scheduling to the disk hard-\nware. In practice, however, the operating system may have other constraints on\nthe service order for requests. For instance, demand paging may take priority\nover application I/O, and writes are more urgent than reads if the cache is\nrunning out of free pages. Also, it may be desirable to guarantee the order\nof a set of disk writes to make the ﬁle system robust in the face of system\ncrashes. Consider what could happen if the operating system allocated a\ndisk page to a ﬁle and the application wrote data into that page before the\noperating system had a chance to ﬂush the ﬁle system metadata back to disk.\nTo accommodate such requirements, an operating system may choose to do its",
  "own disk scheduling and to spoon-feed the requests to the disk controller, one\nby one, for some types of I/O.\n10.5 Disk Management\nThe operating system is responsible for several other aspects of disk manage-\nment, too. Here we discuss disk initialization, booting from disk, and bad-block\nrecovery. 10.5\nDisk Management\n479\n10.5.1\nDisk Formatting\nA new magnetic disk is a blank slate: it is just a platter of a magnetic recording\nmaterial. Before a disk can store data, it must be divided into sectors that the\ndisk controller can read and write. This process is called low-level formatting,\nor physical formatting. Low-level formatting ﬁlls the disk with a special data\nstructure for each sector. The data structure for a sector typically consists of a",
  "header, a data area (usually 512 bytes in size), and a trailer. The header and\ntrailer contain information used by the disk controller, such as a sector number\nand an error-correcting code (ECC). When the controller writes a sector of data\nduring normal I/O, the ECC is updated with a value calculated from all the bytes\nin the data area. When the sector is read, the ECC is recalculated and compared\nwith the stored value. If the stored and calculated numbers are different, this\nmismatch indicates that the data area of the sector has become corrupted and\nthat the disk sector may be bad (Section 10.5.3). The ECC is an error-correcting\ncode because it contains enough information, if only a few bits of data have\nbeen corrupted, to enable the controller to identify which bits have changed",
  "and calculate what their correct values should be. It then reports a recoverable\nsoft error. The controller automatically does the ECC processing whenever a\nsector is read or written.\nMost hard disks are low-level-formatted at the factory as a part of the\nmanufacturing process. This formatting enables the manufacturer to test the\ndisk and to initialize the mapping from logical block numbers to defect-free\nsectors on the disk. For many hard disks, when the disk controller is instructed\nto low-level-format the disk, it can also be told how many bytes of data space\nto leave between the header and trailer of all sectors. It is usually possible to\nchoose among a few sizes, such as 256, 512, and 1,024 bytes. Formatting a disk",
  "with a larger sector size means that fewer sectors can ﬁt on each track; but it\nalso means that fewer headers and trailers are written on each track and more\nspace is available for user data. Some operating systems can handle only a\nsector size of 512 bytes.\nBefore it can use a disk to hold ﬁles, the operating system still needs to\nrecord its own data structures on the disk. It does so in two steps. The ﬁrst step\nis to partition the disk into one or more groups of cylinders. The operating\nsystem can treat each partition as though it were a separate disk. For instance,\none partition can hold a copy of the operating system’s executable code, while\nanother holds user ﬁles. The second step is logical formatting, or creation of a",
  "ﬁle system. In this step, the operating system stores the initial ﬁle-system data\nstructures onto the disk. These data structures may include maps of free and\nallocated space and an initial empty directory.\nTo increase efﬁciency, most ﬁle systems group blocks together into larger\nchunks, frequently called clusters. Disk I/O is done via blocks, but ﬁle system\nI/O is done via clusters, effectively assuring that I/O has more sequential-access\nand fewer random-access characteristics.\nSome operating systems give special programs the ability to use a disk\npartition as a large sequential array of logical blocks, without any ﬁle-system\ndata structures. This array is sometimes called the raw disk, and I/O to this\narray is termed raw I/O. For example, some database systems prefer raw",
  "I/O because it enables them to control the exact disk location where each\ndatabase record is stored. Raw I/O bypasses all the ﬁle-system services, such 480\nChapter 10\nMass-Storage Structure\nas the buffer cache, ﬁle locking, prefetching, space allocation, ﬁle names, and\ndirectories. We can make certain applications more efﬁcient by allowing them\nto implement their own special-purpose storage services on a raw partition,\nbut most applications perform better when they use the regular ﬁle-system\nservices.\n10.5.2\nBoot Block\nFor a computer to start running—for instance, when it is powered up or\nrebooted—it must have an initial program to run. This initial bootstrap\nprogram tends to be simple. It initializes all aspects of the system, from CPU",
  "registers to device controllers and the contents of main memory, and then\nstarts the operating system. To do its job, the bootstrap program ﬁnds the\noperating-system kernel on disk, loads that kernel into memory, and jumps to\nan initial address to begin the operating-system execution.\nFor most computers, the bootstrap is stored in read-only memory (ROM).\nThis location is convenient, because ROM needs no initialization and is at a ﬁxed\nlocation that the processor can start executing when powered up or reset. And,\nsince ROM is read only, it cannot be infected by a computer virus. The problem is\nthat changing this bootstrap code requires changing the ROM hardware chips.\nFor this reason, most systems store a tiny bootstrap loader program in the boot",
  "ROM whose only job is to bring in a full bootstrap program from disk. The full\nbootstrap program can be changed easily: a new version is simply written onto\nthe disk. The full bootstrap program is stored in the “boot blocks” at a ﬁxed\nlocation on the disk. A disk that has a boot partition is called a boot disk or\nsystem disk.\nThe code in the boot ROM instructs the disk controller to read the boot\nblocks into memory (no device drivers are loaded at this point) and then starts\nexecuting that code. The full bootstrap program is more sophisticated than the\nbootstrap loader in the boot ROM. It is able to load the entire operating system\nfrom a non-ﬁxed location on disk and to start the operating system running.\nEven so, the full bootstrap code may be small.",
  "Even so, the full bootstrap code may be small.\nLet’s consider as an example the boot process in Windows. First, note that\nWindows allows a hard disk to be divided into partitions, and one partition\n—identiﬁed as the boot partition—contains the operating system and device\ndrivers. The Windows system places its boot code in the ﬁrst sector on the hard\ndisk, which it terms the master boot record, or MBR. Booting begins by running\ncode that is resident in the system’s ROM memory. This code directs the system\nto read the boot code from the MBR. In addition to containing boot code, the\nMBR contains a table listing the partitions for the hard disk and a ﬂag indicating\nwhich partition the system is to be booted from, as illustrated in Figure 10.9.",
  "Once the system identiﬁes the boot partition, it reads the ﬁrst sector from that\npartition (which is called the boot sector) and continues with the remainder of\nthe boot process, which includes loading the various subsystems and system\nservices.\n10.5.3\nBad Blocks\nBecause disks have moving parts and small tolerances (recall that the disk\nhead ﬂies just above the disk surface), they are prone to failure. Sometimes the\nfailure is complete; in this case, the disk needs to be replaced and its contents 10.5\nDisk Management\n481\nMBR\npartition 1\npartition 2\npartition 3\npartition 4\nboot\ncode\npartition\ntable\nboot partition\nFigure 10.9\nBooting from disk in Windows.\nrestored from backup media to the new disk. More frequently, one or more",
  "sectors become defective. Most disks even come from the factory with bad\nblocks. Depending on the disk and controller in use, these blocks are handled\nin a variety of ways.\nOn simple disks, such as some disks with IDE controllers, bad blocks are\nhandled manually. One strategy is to scan the disk to ﬁnd bad blocks while\nthe disk is being formatted. Any bad blocks that are discovered are ﬂagged as\nunusable so that the ﬁle system does not allocate them. If blocks go bad during\nnormal operation, a special program (such as the Linux badblocks command)\nmust be run manually to search for the bad blocks and to lock them away. Data\nthat resided on the bad blocks usually are lost.\nMore sophisticated disks are smarter about bad-block recovery. The con-",
  "troller maintains a list of bad blocks on the disk. The list is initialized during\nthe low-level formatting at the factory and is updated over the life of the disk.\nLow-level formatting also sets aside spare sectors not visible to the operating\nsystem. The controller can be told to replace each bad sector logically with one\nof the spare sectors. This scheme is known as sector sparing or forwarding.\nA typical bad-sector transaction might be as follows:\n• The operating system tries to read logical block 87.\n• The controller calculates the ECC and ﬁnds that the sector is bad. It reports\nthis ﬁnding to the operating system.\n• The next time the system is rebooted, a special command is run to tell the\ncontroller to replace the bad sector with a spare.",
  "• After that, whenever the system requests logical block 87, the request is\ntranslated into the replacement sector’s address by the controller.\nNote that such a redirection by the controller could invalidate any opti-\nmization by the operating system’s disk-scheduling algorithm! For this reason,\nmost disks are formatted to provide a few spare sectors in each cylinder and\na spare cylinder as well. When a bad block is remapped, the controller uses a\nspare sector from the same cylinder, if possible.\nAs an alternative to sector sparing, some controllers can be instructed to\nreplace a bad block by sector slipping. Here is an example: Suppose that 482\nChapter 10\nMass-Storage Structure\nlogical block 17 becomes defective and the ﬁrst available spare follows sector",
  "202. Sector slipping then remaps all the sectors from 17 to 202, moving them\nall down one spot. That is, sector 202 is copied into the spare, then sector 201\ninto 202, then 200 into 201, and so on, until sector 18 is copied into sector 19.\nSlipping the sectors in this way frees up the space of sector 18 so that sector 17\ncan be mapped to it.\nThe replacement of a bad block generally is not totally automatic, because\nthe data in the bad block are usually lost. Soft errors may trigger a process in\nwhich a copy of the block data is made and the block is spared or slipped.\nAn unrecoverable hard error, however, results in lost data. Whatever ﬁle was\nusing that block must be repaired (for instance, by restoration from a backup\ntape), and that requires manual intervention.",
  "tape), and that requires manual intervention.\n10.6 Swap-Space Management\nSwapping was ﬁrst presented in Section 8.2, where we discussed moving\nentire processes between disk and main memory. Swapping in that setting\noccurs when the amount of physical memory reaches a critically low point and\nprocesses are moved from memory to swap space to free available memory.\nIn practice, very few modern operating systems implement swapping in\nthis fashion. Rather, systems now combine swapping with virtual memory\ntechniques (Chapter 9) and swap pages, not necessarily entire processes. In fact,\nsome systems now use the terms “swapping” and “paging” interchangeably,\nreﬂecting the merging of these two concepts.\nSwap-space management is another low-level task of the operating",
  "system. Virtual memory uses disk space as an extension of main memory.\nSince disk access is much slower than memory access, using swap space\nsigniﬁcantly decreases system performance. The main goal for the design and\nimplementation of swap space is to provide the best throughput for the virtual\nmemory system. In this section, we discuss how swap space is used, where\nswap space is located on disk, and how swap space is managed.\n10.6.1\nSwap-Space Use\nSwap space is used in various ways by different operating systems, depending\non the memory-management algorithms in use. For instance, systems that\nimplement swapping may use swap space to hold an entire process image,\nincluding the code and data segments. Paging systems may simply store pages",
  "that have been pushed out of main memory. The amount of swap space needed\non a system can therefore vary from a few megabytes of disk space to gigabytes,\ndepending on the amount of physical memory, the amount of virtual memory\nit is backing, and the way in which the virtual memory is used.\nNote that it may be safer to overestimate than to underestimate the amount\nof swap space required, because if a system runs out of swap space it may be\nforced to abort processes or may crash entirely. Overestimation wastes disk\nspace that could otherwise be used for ﬁles, but it does no other harm. Some\nsystems recommend the amount to be set aside for swap space. Solaris, for\nexample, suggests setting swap space equal to the amount by which virtual",
  "memory exceeds pageable physical memory. In the past, Linux has suggested 10.6\nSwap-Space Management\n483\nsetting swap space to double the amount of physical memory. Today, that\nlimitation is gone, and most Linux systems use considerably less swap space.\nSome operating systems—including Linux—allow the use of multiple\nswap spaces, including both ﬁles and dedicated swap partitions. These swap\nspaces are usually placed on separate disks so that the load placed on the\nI/O system by paging and swapping can be spread over the system’s I/O\nbandwidth.\n10.6.2\nSwap-Space Location\nA swap space can reside in one of two places: it can be carved out of the\nnormal ﬁle system, or it can be in a separate disk partition. If the swap space",
  "is simply a large ﬁle within the ﬁle system, normal ﬁle-system routines can be\nused to create it, name it, and allocate its space. This approach, though easy\nto implement, is inefﬁcient. Navigating the directory structure and the disk-\nallocation data structures takes time and (possibly) extra disk accesses. External\nfragmentation can greatly increase swapping times by forcing multiple seeks\nduring reading or writing of a process image. We can improve performance\nby caching the block location information in physical memory and by using\nspecial tools to allocate physically contiguous blocks for the swap ﬁle, but the\ncost of traversing the ﬁle-system data structures remains.\nAlternatively, swap space can be created in a separate raw partition. No",
  "ﬁle system or directory structure is placed in this space. Rather, a separate\nswap-space storage manager is used to allocate and deallocate the blocks\nfrom the raw partition. This manager uses algorithms optimized for speed\nrather than for storage efﬁciency, because swap space is accessed much more\nfrequently than ﬁle systems (when it is used). Internal fragmentation may\nincrease, but this trade-off is acceptable because the life of data in the swap\nspace generally is much shorter than that of ﬁles in the ﬁle system. Since\nswap space is reinitialized at boot time, any fragmentation is short-lived. The\nraw-partition approach creates a ﬁxed amount of swap space during disk\npartitioning. Adding more swap space requires either repartitioning the disk",
  "(which involves moving the other ﬁle-system partitions or destroying them\nand restoring them from backup) or adding another swap space elsewhere.\nSome operating systems are ﬂexible and can swap both in raw partitions\nand in ﬁle-system space. Linux is an example: the policy and implementation\nare separate, allowing the machine’s administrator to decide which type of\nswapping to use. The trade-off is between the convenience of allocation and\nmanagement in the ﬁle system and the performance of swapping in raw\npartitions.\n10.6.3\nSwap-Space Management: An Example\nWe can illustrate how swap space is used by following the evolution of\nswapping and paging in various UNIX systems. The traditional UNIX kernel\nstarted with an implementation of swapping that copied entire processes",
  "between contiguous disk regions and memory. UNIX later evolved to a\ncombination of swapping and paging as paging hardware became available.\nIn Solaris 1 (SunOS), the designers changed standard UNIX methods to\nimprove efﬁciency and reﬂect technological developments. When a process\nexecutes, text-segment pages containing code are brought in from the ﬁle 484\nChapter 10\nMass-Storage Structure\nswap area\npage\nslot\nswap partition\nor swap file \nswap map\n1\n0\n3\n0\n1\nFigure 10.10\nThe data structures for swapping on Linux systems.\nsystem, accessed in main memory, and thrown away if selected for pageout. It\nis more efﬁcient to reread a page from the ﬁle system than to write it to swap\nspace and then reread it from there. Swap space is only used as a backing store",
  "for pages of anonymous memory, which includes memory allocated for the\nstack, heap, and uninitialized data of a process.\nMore changes were made in later versions of Solaris. The biggest change\nis that Solaris now allocates swap space only when a page is forced out of\nphysical memory, rather than when the virtual memory page is ﬁrst created.\nThis scheme gives better performance on modern computers, which have more\nphysical memory than older systems and tend to page less.\nLinux is similar to Solaris in that swap space is used only for anonymous\nmemory—that is, memory not backed by any ﬁle. Linux allows one or more\nswap areas to be established. A swap area may be in either a swap ﬁle on a\nregular ﬁle system or a dedicated swap partition. Each swap area consists of a",
  "series of 4-KB page slots, which are used to hold swapped pages. Associated\nwith each swap area is a swap map—an array of integer counters, each\ncorresponding to a page slot in the swap area. If the value of a counter is 0,\nthe corresponding page slot is available. Values greater than 0 indicate that the\npage slot is occupied by a swapped page. The value of the counter indicates the\nnumber of mappings to the swapped page. For example, a value of 3 indicates\nthat the swapped page is mapped to three different processes (which can occur\nif the swapped page is storing a region of memory shared by three processes).\nThe data structures for swapping on Linux systems are shown in Figure 10.10.\n10.7 RAID Structure\nDisk drives have continued to get smaller and cheaper, so it is now econom-",
  "ically feasible to attach many disks to a computer system. Having a large\nnumber of disks in a system presents opportunities for improving the rate\nat which data can be read or written, if the disks are operated in parallel.\nFurthermore, this setup offers the potential for improving the reliability of data\nstorage, because redundant information can be stored on multiple disks. Thus,\nfailure of one disk does not lead to loss of data. A variety of disk-organization\ntechniques, collectively called redundant arrays of independent disks (RAID),\nare commonly used to address the performance and reliability issues.\nIn the past, RAIDs composed of small, cheap disks were viewed as a\ncost-effective alternative to large, expensive disks. Today, RAIDs are used for 10.7\nRAID Structure\n485",
  "RAID Structure\n485\nSTRUCTURING RAID\nRAID storage can be structured in a variety of ways. For example, a system\ncan have disks directly attached to its buses. In this case, the operating\nsystem or system software can implement RAID functionality. Alternatively,\nan intelligent host controller can control multiple attached disks and can\nimplement RAID on those disks in hardware. Finally, a storage array, or RAID\narray, can be used. A RAID array is a standalone unit with its own controller,\ncache (usually), and disks. It is attached to the host via one or more standard\ncontrollers (for example, FC). This common setup allows an operating system\nor software without RAID functionality to have RAID-protected disks. It is\neven used on systems that do have RAID software layers because of its",
  "simplicity and ﬂexibility.\ntheir higher reliability and higher data-transfer rate, rather than for economic\nreasons. Hence, the I in RAID, which once stood for “inexpensive,” now stands\nfor “independent.”\n10.7.1\nImprovement of Reliability via Redundancy\nLet’s ﬁrst consider the reliability of RAIDs. The chance that some disk out of\na set of N disks will fail is much higher than the chance that a speciﬁc single\ndisk will fail. Suppose that the mean time to failure of a single disk is 100,000\nhours. Then the mean time to failure of some disk in an array of 100 disks\nwill be 100,000/100 = 1,000 hours, or 41.66 days, which is not long at all! If we\nstore only one copy of the data, then each disk failure will result in loss of a",
  "signiﬁcant amount of data—and such a high rate of data loss is unacceptable.\nThe solution to the problem of reliability is to introduce redundancy; we\nstore extra information that is not normally needed but that can be used in the\nevent of failure of a disk to rebuild the lost information. Thus, even if a disk\nfails, data are not lost.\nThe simplest (but most expensive) approach to introducing redundancy is\nto duplicate every disk. This technique is called mirroring. With mirroring, a\nlogical disk consists of two physical disks, and every write is carried out on\nboth disks. The result is called a mirrored volume. If one of the disks in the\nvolume fails, the data can be read from the other. Data will be lost only if the\nsecond disk fails before the ﬁrst failed disk is replaced.",
  "The mean time to failure of a mirrored volume—where failure is the loss of\ndata—depends on two factors. One is the mean time to failure of the individual\ndisks. The other is the mean time to repair, which is the time it takes (on\naverage) to replace a failed disk and to restore the data on it. Suppose that the\nfailures of the two disks are independent; that is, the failure of one disk is not\nconnected to the failure of the other. Then, if the mean time to failure of a single\ndisk is 100,000 hours and the mean time to repair is 10 hours, the mean time\nto data loss of a mirrored disk system is 100, 0002/(2 ∗10) = 500 ∗106 hours,\nor 57,000 years! 486\nChapter 10\nMass-Storage Structure\nYou should be aware that we cannot really assume that disk failures will",
  "be independent. Power failures and natural disasters, such as earthquakes,\nﬁres, and ﬂoods, may result in damage to both disks at the same time.\nAlso, manufacturing defects in a batch of disks can cause correlated failures.\nAs disks age, the probability of failure grows, increasing the chance that a\nsecond disk will fail while the ﬁrst is being repaired. In spite of all these\nconsiderations, however, mirrored-disk systems offer much higher reliability\nthan do single-disk systems.\nPower failures are a particular source of concern, since they occur far more\nfrequently than do natural disasters. Even with mirroring of disks, if writes are\nin progress to the same block in both disks, and power fails before both blocks",
  "are fully written, the two blocks can be in an inconsistent state. One solution\nto this problem is to write one copy ﬁrst, then the next. Another is to add a\nsolid-state nonvolatile RAM (NVRAM) cache to the RAID array. This write-back\ncache is protected from data loss during power failures, so the write can be\nconsidered complete at that point, assuming the NVRAM has some kind of error\nprotection and correction, such as ECC or mirroring.\n10.7.2\nImprovement in Performance via Parallelism\nNow let’s consider how parallel access to multiple disks improves perfor-\nmance. With disk mirroring, the rate at which read requests can be handled is\ndoubled, since read requests can be sent to either disk (as long as both disks",
  "in a pair are functional, as is almost always the case). The transfer rate of each\nread is the same as in a single-disk system, but the number of reads per unit\ntime has doubled.\nWith multiple disks, we can improve the transfer rate as well (or instead)\nby striping data across the disks. In its simplest form, data striping consists\nof splitting the bits of each byte across multiple disks; such striping is called\nbit-level striping. For example, if we have an array of eight disks, we write\nbit i of each byte to disk i. The array of eight disks can be treated as a single\ndisk with sectors that are eight times the normal size and, more important, that\nhave eight times the access rate. Every disk participates in every access (read",
  "or write); so the number of accesses that can be processed per second is about\nthe same as on a single disk, but each access can read eight times as many data\nin the same time as on a single disk.\nBit-level striping can be generalized to include a number of disks that either\nis a multiple of 8 or divides 8. For example, if we use an array of four disks,\nbits i and 4 + i of each byte go to disk i. Further, striping need not occur at\nthe bit level. In block-level striping, for instance, blocks of a ﬁle are striped\nacross multiple disks; with n disks, block i of a ﬁle goes to disk (i mod n) + 1.\nOther levels of striping, such as bytes of a sector or sectors of a block, also are\npossible. Block-level striping is the most common.",
  "Parallelism in a disk system, as achieved through striping, has two main\ngoals:\n1. Increase the throughput of multiple small accesses (that is, page accesses)\nby load balancing.\n2. Reduce the response time of large accesses. 10.7\nRAID Structure\n487\n10.7.3\nRAID Levels\nMirroring provides high reliability, but it is expensive. Striping provides high\ndata-transfer rates, but it does not improve reliability. Numerous schemes\nto provide redundancy at lower cost by using disk striping combined with\n“parity” bits (which we describe shortly) have been proposed. These schemes\nhave different cost–performance trade-offs and are classiﬁed according to\nlevels called RAID levels. We describe the various levels here; Figure 10.11",
  "shows them pictorially (in the ﬁgure, P indicates error-correcting bits and C\nindicates a second copy of the data). In all cases depicted in the ﬁgure, four\ndisks’ worth of data are stored, and the extra disks are used to store redundant\ninformation for failure recovery.\n(a) RAID 0: non-redundant striping.\n(b) RAID 1: mirrored disks.\nC\nC\nC\nC\n(c) RAID 2: memory-style error-correcting codes.\n(d) RAID 3: bit-interleaved parity.\n(e) RAID 4: block-interleaved parity.\n(f) RAID 5: block-interleaved distributed parity.\nP\nP\nP\nP\nP\nP\nP\n(g) RAID 6: P ' Q redundancy.\nP\nP\nP\nP\nP\nP\nP\nP\nP\nP\nP\nP\nP\nFigure 10.11\nRAID levels. 488\nChapter 10\nMass-Storage Structure\n• RAID level 0. RAID level 0 refers to disk arrays with striping at the level of",
  "blocks but without any redundancy (such as mirroring or parity bits), as\nshown in Figure 10.11(a).\n• RAID level 1. RAID level 1 refers to disk mirroring. Figure 10.11(b) shows\na mirrored organization.\n• RAID level 2. RAID level 2 is also known as memory-style error-correcting-\ncode (ECC) organization. Memory systems have long detected certain\nerrors by using parity bits. Each byte in a memory system may have a\nparity bit associated with it that records whether the number of bits in the\nbyte set to 1 is even (parity = 0) or odd (parity = 1). If one of the bits in the\nbyte is damaged (either a 1 becomes a 0, or a 0 becomes a 1), the parity of\nthe byte changes and thus does not match the stored parity. Similarly, if the",
  "stored parity bit is damaged, it does not match the computed parity. Thus,\nall single-bit errors are detected by the memory system. Error-correcting\nschemes store two or more extra bits and can reconstruct the data if a single\nbit is damaged.\nThe idea of ECC can be used directly in disk arrays via striping of\nbytes across disks. For example, the ﬁrst bit of each byte can be stored in\ndisk 1, the second bit in disk 2, and so on until the eighth bit is stored in\ndisk 8; the error-correction bits are stored in further disks. This scheme\nis shown in Figure 10.11(c), where the disks labeled P store the error-\ncorrection bits. If one of the disks fails, the remaining bits of the byte and\nthe associated error-correction bits can be read from other disks and used",
  "to reconstruct the damaged data. Note that RAID level 2 requires only three\ndisks’ overhead for four disks of data, unlike RAID level 1, which requires\nfour disks’ overhead.\n• RAID level 3. RAID level 3, or bit-interleaved parity organization, improves\non level 2 by taking into account the fact that, unlike memory systems, disk\ncontrollers can detect whether a sector has been read correctly, so a single\nparity bit can be used for error correction as well as for detection. The idea\nis as follows: If one of the sectors is damaged, we know exactly which\nsector it is, and we can ﬁgure out whether any bit in the sector is a 1 or\na 0 by computing the parity of the corresponding bits from sectors in the\nother disks. If the parity of the remaining bits is equal to the stored parity,",
  "the missing bit is 0; otherwise, it is 1. RAID level 3 is as good as level 2 but is\nless expensive in the number of extra disks required (it has only a one-disk\noverhead), so level 2 is not used in practice. Level 3 is shown pictorially in\nFigure 10.11(d).\nRAID level 3 has two advantages over level 1. First, the storage over-\nhead is reduced because only one parity disk is needed for several regular\ndisks, whereas one mirror disk is needed for every disk in level 1. Second,\nsince reads and writes of a byte are spread out over multiple disks with\nN-way striping of data, the transfer rate for reading or writing a single\nblock is N times as fast as with RAID level 1. On the negative side, RAID\nlevel 3 supports fewer I/Os per second, since every disk has to participate\nin every I/O request.",
  "in every I/O request.\nA further performance problem with RAID 3—and with all parity-\nbased RAID levels—is the expense of computing and writing the parity. 10.7\nRAID Structure\n489\nThis overhead results in signiﬁcantly slower writes than with non-parity\nRAID arrays. To moderate this performance penalty, many RAID storage\narrays include a hardware controller with dedicated parity hardware. This\ncontroller ofﬂoads the parity computation from the CPU to the array. The\narray has an NVRAM cache as well, to store the blocks while the parity is\ncomputed and to buffer the writes from the controller to the spindles. This\ncombination can make parity RAID almost as fast as non-parity. In fact, a\ncaching array doing parity RAID can outperform a non-caching non-parity\nRAID.",
  "RAID.\n• RAID level 4. RAID level 4, or block-interleaved parity organization, uses\nblock-level striping, as in RAID 0, and in addition keeps a parity block on\na separate disk for corresponding blocks from N other disks. This scheme\nis diagrammed in Figure 10.11(e). If one of the disks fails, the parity block\ncan be used with the corresponding blocks from the other disks to restore\nthe blocks of the failed disk.\nA block read accesses only one disk, allowing other requests to be\nprocessed by the other disks. Thus, the data-transfer rate for each access\nis slower, but multiple read accesses can proceed in parallel, leading to a\nhigher overall I/O rate. The transfer rates for large reads are high, since all\nthe disks can be read in parallel. Large writes also have high transfer rates,",
  "since the data and parity can be written in parallel.\nSmall independent writes cannot be performed in parallel. An operating-\nsystem write of data smaller than a block requires that the block be read,\nmodiﬁed with the new data, and written back. The parity block has to be\nupdated as well. This is known as the read-modify-write cycle. Thus, a\nsingle write requires four disk accesses: two to read the two old blocks and\ntwo to write the two new blocks.\nWAFL (which we cover in Chapter 12) uses RAID level 4 because this RAID\nlevel allows disks to be added to a RAID set seamlessly. If the added disks\nare initialized with blocks containing only zeros, then the parity value does\nnot change, and the RAID set is still correct.",
  "not change, and the RAID set is still correct.\n• RAID level 5. RAID level 5, or block-interleaved distributed parity, differs\nfrom level 4 in that it spreads data and parity among all N+1 disks, rather\nthan storing data in N disks and parity in one disk. For each block, one of\nthe disks stores the parity and the others store data. For example, with an\narray of ﬁve disks, the parity for the nth block is stored in disk (n mod 5)+1.\nThe nth blocks of the other four disks store actual data for that block. This\nsetup is shown in Figure 10.11(f), where the Ps are distributed across all\nthe disks. A parity block cannot store parity for blocks in the same disk,\nbecause a disk failure would result in loss of data as well as of parity, and",
  "hence the loss would not be recoverable. By spreading the parity across\nall the disks in the set, RAID 5 avoids potential overuse of a single parity\ndisk, which can occur with RAID 4. RAID 5 is the most common parity RAID\nsystem.\n• RAID level 6. RAID level 6, also called the P + Q redundancy scheme, is\nmuch like RAID level 5 but stores extra redundant information to guard\nagainst multiple disk failures. Instead of parity, error-correcting codes such\nas the Reed–Solomon codes are used. In the scheme shown in Figure 490\nChapter 10\nMass-Storage Structure\n10.11(g), 2 bits of redundant data are stored for every 4 bits of data—\ncompared with 1 parity bit in level 5—and the system can tolerate two\ndisk failures.\n• RAID levels 0 + 1 and 1 + 0. RAID level 0 + 1 refers to a combination of RAID",
  "levels 0 and 1. RAID 0 provides the performance, while RAID 1 provides\nthe reliability. Generally, this level provides better performance than RAID\n5. It is common in environments where both performance and reliability\nare important. Unfortunately, like RAID 1, it doubles the number of disks\nneeded for storage, so it is also relatively expensive. In RAID 0 + 1, a set\nof disks are striped, and then the stripe is mirrored to another, equivalent\nstripe.\nAnother RAID option that is becoming available commercially is RAID\nlevel 1 + 0, in which disks are mirrored in pairs and then the resulting\nmirrored pairs are striped. This scheme has some theoretical advantages\nover RAID 0 + 1. For example, if a single disk fails in RAID 0 + 1, an entire",
  "stripe is inaccessible, leaving only the other stripe. With a failure in RAID 1\n+ 0, a single disk is unavailable, but the disk that mirrors it is still available,\nas are all the rest of the disks (Figure 10.12).\nNumerous variations have been proposed to the basic RAID schemes described\nhere. As a result, some confusion may exist about the exact deﬁnitions of the\ndifferent RAID levels.\nx\nx\nmirror\na) RAID 0 ' 1 with a single disk failure.\nstripe\nstripe\nmirror\nb) RAID 1 ' 0 with a single disk failure.\nstripe\nmirror\nmirror\nmirror\nFigure 10.12\nRAID 0 + 1 and 1 + 0. 10.7\nRAID Structure\n491\nThe implementation of RAID is another area of variation. Consider the\nfollowing layers at which RAID can be implemented.\n• Volume-management software can implement RAID within the kernel or",
  "at the system software layer. In this case, the storage hardware can provide\nminimal features and still be part of a full RAID solution. Parity RAID is\nfairly slow when implemented in software, so typically RAID 0, 1, or 0 + 1\nis used.\n• RAID can be implemented in the host bus-adapter (HBA) hardware. Only\nthe disks directly connected to the HBA can be part of a given RAID set.\nThis solution is low in cost but not very ﬂexible.\n• RAID can be implemented in the hardware of the storage array. The storage\narray can create RAID sets of various levels and can even slice these sets\ninto smaller volumes, which are then presented to the operating system.\nThe operating system need only implement the ﬁle system on each of the\nvolumes. Arrays can have multiple connections available or can be part of",
  "a SAN, allowing multiple hosts to take advantage of the array’s features.\n• RAID can be implemented in the SAN interconnect layer by disk virtualiza-\ntion devices. In this case, a device sits between the hosts and the storage.\nIt accepts commands from the servers and manages access to the storage.\nIt could provide mirroring, for example, by writing each block to two\nseparate storage devices.\nOther features, such as snapshots and replication, can be implemented\nat each of these levels as well. A snapshot is a view of the ﬁle system\nbefore the last update took place. (Snapshots are covered more fully in\nChapter 12.) Replication involves the automatic duplication of writes between\nseparate sites for redundancy and disaster recovery. Replication can be",
  "synchronous or asynchronous. In synchronous replication, each block must be\nwritten locally and remotely before the write is considered complete, whereas\nin asynchronous replication, the writes are grouped together and written\nperiodically. Asynchronous replication can result in data loss if the primary\nsite fails, but it is faster and has no distance limitations.\nThe implementation of these features differs depending on the layer at\nwhich RAID is implemented. For example, if RAID is implemented in software,\nthen each host may need to carry out and manage its own replication. If\nreplication is implemented in the storage array or in the SAN interconnect,\nhowever, then whatever the host operating system or its features, the host’s\ndata can be replicated.",
  "data can be replicated.\nOne other aspect of most RAID implementations is a hot spare disk or disks.\nA hot spare is not used for data but is conﬁgured to be used as a replacement in\ncase of disk failure. For instance, a hot spare can be used to rebuild a mirrored\npair should one of the disks in the pair fail. In this way, the RAID level can be\nreestablished automatically, without waiting for the failed disk to be replaced.\nAllocating more than one hot spare allows more than one failure to be repaired\nwithout human intervention. 492\nChapter 10\nMass-Storage Structure\n10.7.4\nSelecting a RAID Level\nGiven the many choices they have, how do system designers choose a RAID\nlevel? One consideration is rebuild performance. If a disk fails, the time needed",
  "to rebuild its data can be signiﬁcant. This may be an important factor if a\ncontinuous supply of data is required, as it is in high-performance or interactive\ndatabase systems. Furthermore, rebuild performance inﬂuences the mean time\nto failure.\nRebuild performance varies with the RAID level used. Rebuilding is easiest\nfor RAID level 1, since data can be copied from another disk. For the other\nlevels, we need to access all the other disks in the array to rebuild data in a\nfailed disk. Rebuild times can be hours for RAID 5 rebuilds of large disk sets.\nRAID level 0 is used in high-performance applications where data loss is\nnot critical. RAID level 1 is popular for applications that require high reliability\nwith fast recovery. RAID 0 + 1 and 1 + 0 are used where both performance and",
  "reliability are important—for example, for small databases. Due to RAID 1’s\nhigh space overhead, RAID 5 is often preferred for storing large volumes of\ndata. Level 6 is not supported currently by many RAID implementations, but it\nshould offer better reliability than level 5.\nRAID system designers and administrators of storage have to make several\nother decisions as well. For example, how many disks should be in a given\nRAID set? How many bits should be protected by each parity bit? If more disks\nare in an array, data-transfer rates are higher, but the system is more expensive.\nIf more bits are protected by a parity bit, the space overhead due to parity bits\nis lower, but the chance that a second disk will fail before the ﬁrst failed disk is",
  "repaired is greater, and that will result in data loss.\n10.7.5\nExtensions\nThe concepts of RAID have been generalized to other storage devices, including\narrays of tapes, and even to the broadcast of data over wireless systems. When\napplied to arrays of tapes, RAID structures are able to recover data even if one\nof the tapes in an array is damaged. When applied to broadcast of data, a block\nof data is split into short units and is broadcast along with a parity unit. If one\nof the units is not received for any reason, it can be reconstructed from the\nother units. Commonly, tape-drive robots containing multiple tape drives will\nstripe data across all the drives to increase throughput and decrease backup\ntime.\n10.7.6\nProblems with RAID",
  "time.\n10.7.6\nProblems with RAID\nUnfortunately, RAID does not always assure that data are available for the\noperating system and its users. A pointer to a ﬁle could be wrong, for example,\nor pointers within the ﬁle structure could be wrong. Incomplete writes, if not\nproperly recovered, could result in corrupt data. Some other process could\naccidentally write over a ﬁle system’s structures, too. RAID protects against\nphysical media errors, but not other hardware and software errors. As large as\nis the landscape of software and hardware bugs, that is how numerous are the\npotential perils for data on a system.\nThe Solaris ZFS ﬁle system takes an innovative approach to solving these\nproblems through the use of checksums—a technique used to verify the 10.7\nRAID Structure\n493",
  "RAID Structure\n493\nTHE InServ STORAGE ARRAY\nInnovation, in an effort to provide better, faster, and less expensive solutions,\nfrequently blurs the lines that separated previous technologies. Consider the\nInServ storage array from 3Par. Unlike most other storage arrays, InServ\ndoes not require that a set of disks be conﬁgured at a speciﬁc RAID level.\nRather, each disk is broken into 256-MB “chunklets.” RAID is then applied at\nthe chunklet level. A disk can thus participate in multiple and various RAID\nlevels as its chunklets are used for multiple volumes.\nInServ also provides snapshots similar to those created by the WAFL ﬁle\nsystem. The format of InServ snapshots can be read–write as well as read-\nonly, allowing multiple hosts to mount copies of a given ﬁle system without",
  "needing their own copies of the entire ﬁle system. Any changes a host makes\nin its own copy are copy-on-write and so are not reﬂected in the other copies.\nA further innovation is utility storage. Some ﬁle systems do not expand\nor shrink. On these systems, the original size is the only size, and any change\nrequires copying data. An administrator can conﬁgure InServ to provide a\nhost with a large amount of logical storage that initially occupies only a small\namount of physical storage. As the host starts using the storage, unused disks\nare allocated to the host, up to the original logical level. The host thus can\nbelieve that it has a large ﬁxed storage space, create its ﬁle systems there, and\nso on. Disks can be added or removed from the ﬁle system by InServ without",
  "the ﬁle system’s noticing the change. This feature can reduce the number of\ndrives needed by hosts, or at least delay the purchase of disks until they are\nreally needed.\nintegrity of data. ZFS maintains internal checksums of all blocks, including\ndata and metadata. These checksums are not kept with the block that is being\nchecksummed. Rather, they are stored with the pointer to that block. (See Figure\n10.13.) Consider an inode — a data structure for storing ﬁle system metadata\n— with pointers to its data. Within the inode is the checksum of each block\nof data. If there is a problem with the data, the checksum will be incorrect,\nand the ﬁle system will know about it. If the data are mirrored, and there is a\nblock with a correct checksum and one with an incorrect checksum, ZFS will",
  "automatically update the bad block with the good one. Similarly, the directory\nentry that points to the inode has a checksum for the inode. Any problem\nin the inode is detected when the directory is accessed. This checksumming\ntakes places throughout all ZFS structures, providing a much higher level of\nconsistency, error detection, and error correction than is found in RAID disk sets\nor standard ﬁle systems. The extra overhead that is created by the checksum\ncalculation and extra block read-modify-write cycles is not noticeable because\nthe overall performance of ZFS is very fast.\nAnother issue with most RAID implementations is lack of ﬂexibility.\nConsider a storage array with twenty disks divided into four sets of ﬁve disks.",
  "Each set of ﬁve disks is a RAID level 5 set. As a result, there are four separate\nvolumes, each holding a ﬁle system. But what if one ﬁle system is too large to ﬁt\non a ﬁve-disk RAID level 5 set? And what if another ﬁle system needs very little\nspace? If such factors are known ahead of time, then the disks and volumes 494\nChapter 10\nMass-Storage Structure\nmetadata block 1\naddress 1\nchecksum MB2\nchecksum\naddress 2\nmetadata block 2\naddress\nchecksum D1\nchecksum D2\ndata 1\ndata 2\naddress\nFigure 10.13\nZFS checksums all metadata and data.\ncan be properly allocated. Very frequently, however, disk use and requirements\nchange over time.\nEven if the storage array allowed the entire set of twenty disks to be\ncreated as one large RAID set, other issues could arise. Several volumes of",
  "various sizes could be built on the set. But some volume managers do not\nallow us to change a volume’s size. In that case, we would be left with the same\nissue described above—mismatched ﬁle-system sizes. Some volume managers\nallow size changes, but some ﬁle systems do not allow for ﬁle-system growth\nor shrinkage. The volumes could change sizes, but the ﬁle systems would need\nto be recreated to take advantage of those changes.\nZFS combines ﬁle-system management and volume management into a\nunit providing greater functionality than the traditional separation of those\nfunctions allows. Disks, or partitions of disks, are gathered together via RAID\nsets into pools of storage. A pool can hold one or more ZFS ﬁle systems. The",
  "entire pool’s free space is available to all ﬁle systems within that pool. ZFS uses\nthe memory model of malloc() and free() to allocate and release storage for\neach ﬁle system as blocks are used and freed within the ﬁle system. As a result,\nthere are no artiﬁcial limits on storage use and no need to relocate ﬁle systems\nbetween volumes or resize volumes. ZFS provides quotas to limit the size of a\nﬁle system and reservations to assure that a ﬁle system can grow by a speciﬁed\namount, but those variables can be changed by the ﬁle-system owner at any\ntime. Figure 10.14(a) depicts traditional volumes and ﬁle systems, and Figure\n10.14(b) shows the ZFS model.\n10.8 Stable-Storage Implementation\nIn Chapter 5, we introduced the write-ahead log, which requires the availability",
  "of stable storage. By deﬁnition, information residing in stable storage is never\nlost. To implement such storage, we need to replicate the required information 10.8\nStable-Storage Implementation\n495\nFS\nvolume\nZFS\nZFS\nstorage pool\nZFS\nvolume\nvolume\nFS\nFS\n(a) Traditional volumes and file systems.\n(b) ZFS and pooled storage.\nFigure 10.14\n(a) Traditional volumes and ﬁle systems. (b) A ZFS pool and ﬁle systems.\non multiple storage devices (usually disks) with independent failure modes.\nWe also need to coordinate the writing of updates in a way that guarantees\nthat a failure during an update will not leave all the copies in a damaged state\nand that, when we are recovering from a failure, we can force all copies to a",
  "consistent and correct value, even if another failure occurs during the recovery.\nIn this section, we discuss how to meet these needs.\nA disk write results in one of three outcomes:\n1. Successful completion. The data were written correctly on disk.\n2. Partial failure. A failure occurred in the midst of transfer, so only some of\nthe sectors were written with the new data, and the sector being written\nduring the failure may have been corrupted.\n3. Total failure. The failure occurred before the disk write started, so the\nprevious data values on the disk remain intact.\nWhenever a failure occurs during writing of a block, the system needs to\ndetect it and invoke a recovery procedure to restore the block to a consistent",
  "state. To do that, the system must maintain two physical blocks for each logical\nblock. An output operation is executed as follows:\n1. Write the information onto the ﬁrst physical block.\n2. When the ﬁrst write completes successfully, write the same information\nonto the second physical block.\n3. Declare the operation complete only after the second write completes\nsuccessfully. 496\nChapter 10\nMass-Storage Structure\nDuring recovery from a failure, each pair of physical blocks is examined.\nIf both are the same and no detectable error exists, then no further action is\nnecessary. If one block contains a detectable error then we replace its contents\nwith the value of the other block. If neither block contains a detectable error,",
  "but the blocks differ in content, then we replace the content of the ﬁrst block\nwith that of the second. This recovery procedure ensures that a write to stable\nstorage either succeeds completely or results in no change.\nWe can extend this procedure easily to allow the use of an arbitrarily large\nnumber of copies of each block of stable storage. Although having a large\nnumber of copies further reduces the probability of a failure, it is usually\nreasonable to simulate stable storage with only two copies. The data in stable\nstorage are guaranteed to be safe unless a failure destroys all the copies.\nBecause waiting for disk writes to complete (synchronous I/O) is time\nconsuming, many storage arrays add NVRAM as a cache. Since the memory is",
  "nonvolatile (it usually has battery power to back up the unit’s power), it can\nbe trusted to store the data en route to the disks. It is thus considered part of\nthe stable storage. Writes to it are much faster than to disk, so performance is\ngreatly improved.\n10.9\nSummary\nDisk drives are the major secondary storage I/O devices on most computers.\nMost secondary storage devices are either magnetic disks or magnetic tapes,\nalthough solid-state disks are growing in importance. Modern disk drives are\nstructured as large one-dimensional arrays of logical disk blocks. Generally,\nthese logical blocks are 512 bytes in size. Disks may be attached to a computer\nsystem in one of two ways: (1) through the local I/O ports on the host computer\nor (2) through a network connection.",
  "or (2) through a network connection.\nRequests for disk I/O are generated by the ﬁle system and by the virtual\nmemory system. Each request speciﬁes the address on the disk to be referenced,\nin the form of a logical block number. Disk-scheduling algorithms can improve\nthe effective bandwidth, the average response time, and the variance in\nresponse time. Algorithms such as SSTF, SCAN, C-SCAN, LOOK, and C-LOOK\nare designed to make such improvements through strategies for disk-queue\nordering. Performance of disk-scheduling algorithms can vary greatly on\nmagnetic disks. In contrast, because solid-state disks have no moving parts,\nperformance varies little among algorithms, and quite often a simple FCFS\nstrategy is used.\nPerformance can be harmed by external fragmentation. Some systems",
  "have utilities that scan the ﬁle system to identify fragmented ﬁles; they then\nmove blocks around to decrease the fragmentation. Defragmenting a badly\nfragmented ﬁle system can signiﬁcantly improve performance, but the system\nmay have reduced performance while the defragmentation is in progress.\nSophisticated ﬁle systems, such as the UNIX Fast File System, incorporate\nmany strategies to control fragmentation during space allocation so that disk\nreorganization is not needed.\nThe operating system manages the disk blocks. First, a disk must be low-\nlevel-formatted to create the sectors on the raw hardware—new disks usually\ncome preformatted. Then, the disk is partitioned, ﬁle systems are created, and Practice Exercises\n497",
  "497\nboot blocks are allocated to store the system’s bootstrap program. Finally, when\na block is corrupted, the system must have a way to lock out that block or to\nreplace it logically with a spare.\nBecause an efﬁcient swap space is a key to good performance, systems\nusually bypass the ﬁle system and use raw-disk access for paging I/O. Some\nsystems dedicate a raw-disk partition to swap space, and others use a ﬁle\nwithin the ﬁle system instead. Still other systems allow the user or system\nadministrator to make the decision by providing both options.\nBecause of the amount of storage required on large systems, disks are\nfrequently made redundant via RAID algorithms. These algorithms allow more\nthan one disk to be used for a given operation and allow continued operation",
  "and even automatic recovery in the face of a disk failure. RAID algorithms\nare organized into different levels; each level provides some combination of\nreliability and high transfer rates.\nPractice Exercises\n10.1\nIs disk scheduling, other than FCFS scheduling, useful in a single-user\nenvironment? Explain your answer.\n10.2\nExplain why SSTF scheduling tends to favor middle cylinders over the\ninnermost and outermost cylinders.\n10.3\nWhy is rotational latency usually not considered in disk scheduling?\nHow would you modify SSTF, SCAN, and C-SCAN to include latency\noptimization?\n10.4\nWhy is it important to balance ﬁle-system I/O among the disks and\ncontrollers on a system in a multitasking environment?\n10.5\nWhat are the tradeoffs involved in rereading code pages from the ﬁle",
  "system versus using swap space to store them?\n10.6\nIs there any way to implement truly stable storage? Explain your\nanswer.\n10.7\nIt is sometimes said that tape is a sequential-access medium, whereas\na magnetic disk is a random-access medium. In fact, the suitability\nof a storage device for random access depends on the transfer size.\nThe term “streaming transfer rate” denotes the rate for a data transfer\nthat is underway, excluding the effect of access latency. In contrast,\nthe “effective transfer rate” is the ratio of total bytes per total seconds,\nincluding overhead time such as access latency.\nSuppose we have a computer with the following characteristics: the\nlevel-2 cache has an access latency of 8 nanoseconds and a streaming",
  "transfer rate of 800 megabytes per second, the main memory has an\naccess latency of 60 nanoseconds and a streaming transfer rate of 80\nmegabytes per second, the magnetic disk has an access latency of 15\nmilliseconds and a streaming transfer rate of 5 megabytes per second,\nand a tape drive has an access latency of 60 seconds and a streaming\ntransfer rate of 2 megabytes per second. 498\nChapter 10\nMass-Storage Structure\na.\nRandom access causes the effective transfer rate of a device to\ndecrease, because no data are transferred during the access time.\nFor the disk described, what is the effective transfer rate if an\naverage access is followed by a streaming transfer of (1) 512 bytes,\n(2) 8 kilobytes, (3) 1 megabyte, and (4) 16 megabytes?\nb.",
  "b.\nThe utilization of a device is the ratio of effective transfer rate to\nstreaming transfer rate. Calculate the utilization of the disk drive\nfor each of the four transfer sizes given in part a.\nc.\nSuppose that a utilization of 25 percent (or higher) is considered\nacceptable. Using the performance ﬁgures given, compute the\nsmallest transfer size for disk that gives acceptable utilization.\nd.\nComplete the following sentence: A disk is a random-access\ndevice for transfers larger than\nbytes and is a sequential-\naccess device for smaller transfers.\ne.\nCompute the minimum transfer sizes that give acceptable utiliza-\ntion for cache, memory, and tape.\nf.\nWhen is a tape a random-access device, and when is it a\nsequential-access device?\n10.8",
  "sequential-access device?\n10.8\nCould a RAID level 1 organization achieve better performance for read\nrequests than a RAID level 0 organization (with nonredundant striping\nof data)? If so, how?\nExercises\n10.9\nNone of the disk-scheduling disciplines, except FCFS, is truly fair\n(starvation may occur).\na.\nExplain why this assertion is true.\nb.\nDescribe a way to modify algorithms such as SCAN to ensure\nfairness.\nc.\nExplain why fairness is an important goal in a time-sharing\nsystem.\nd.\nGive three or more examples of circumstances in which it is\nimportant that the operating system be unfair in serving I/O\nrequests.\n10.10\nExplain why SSDs often use an FCFS disk-scheduling algorithm.\n10.11\nSuppose that a disk drive has 5,000 cylinders, numbered 0 to 4,999. The",
  "drive is currently serving a request at cylinder 2,150, and the previous\nrequest was at cylinder 1,805. The queue of pending requests, in FIFO\norder, is:\n2,069, 1,212, 2,296, 2,800, 544, 1,618, 356, 1,523, 4,965, 3681 Exercises\n499\nStarting from the current head position, what is the total distance (in\ncylinders) that the disk arm moves to satisfy all the pending requests\nfor each of the following disk-scheduling algorithms?\na.\nFCFS\nb.\nSSTF\nc.\nSCAN\nd.\nLOOK\ne.\nC-SCAN\nf.\nC-LOOK\n10.12\nElementary physics states that when an object is subjected to a constant\nacceleration a, the relationship between distance d and time t is given\nby d = 1\n2at2. Suppose that, during a seek, the disk in Exercise 10.11\naccelerates the disk arm at a constant rate for the ﬁrst half of the seek,",
  "then decelerates the disk arm at the same rate for the second half of the\nseek. Assume that the disk can perform a seek to an adjacent cylinder\nin 1 millisecond and a full-stroke seek over all 5,000 cylinders in 18\nmilliseconds.\na.\nThe distance of a seek is the number of cylinders over which the\nhead moves. Explain why the seek time is proportional to the\nsquare root of the seek distance.\nb.\nWrite an equation for the seek time as a function of the seek\ndistance. This equation should be of the form t = x + y\n√\nL, where\nt is the time in milliseconds and L is the seek distance in cylinders.\nc.\nCalculate the total seek time for each of the schedules in Exercise\n10.11. Determine which schedule is the fastest (has the smallest\ntotal seek time).\nd.",
  "total seek time).\nd.\nThe percentage speedup is the time saved divided by the original\ntime. What is the percentage speedup of the fastest schedule over\nFCFS?\n10.13\nSuppose that the disk in Exercise 10.12 rotates at 7,200 RPM.\na.\nWhat is the average rotational latency of this disk drive?\nb.\nWhat seek distance can be covered in the time that you found for\npart a?\n10.14\nDescribe some advantages and disadvantages of using SSDs as a\ncaching tier and as a disk-drive replacement compared with using only\nmagnetic disks.\n10.15\nCompare the performance of C-SCAN and SCAN scheduling, assuming\na uniform distribution of requests. Consider the average response time\n(the time between the arrival of a request and the completion of that",
  "request’s service), the variation in response time, and the effective 500\nChapter 10\nMass-Storage Structure\nbandwidth. How does performance depend on the relative sizes of\nseek time and rotational latency?\n10.16\nRequests are not usually uniformly distributed. For example, we can\nexpect a cylinder containing the ﬁle-system metadata to be accessed\nmore frequently than a cylinder containing only ﬁles. Suppose you\nknow that 50 percent of the requests are for a small, ﬁxed number of\ncylinders.\na.\nWould any of the scheduling algorithms discussed in this chapter\nbe particularly good for this case? Explain your answer.\nb.\nPropose a disk-scheduling algorithm that gives even better per-\nformance by taking advantage of this “hot spot” on the disk.\n10.17",
  "10.17\nConsider a RAID level 5 organization comprising ﬁve disks, with the\nparity for sets of four blocks on four disks stored on the ﬁfth disk. How\nmany blocks are accessed in order to perform the following?\na.\nA write of one block of data\nb.\nA write of seven continuous blocks of data\n10.18\nCompare the throughput achieved by a RAID level 5 organization with\nthat achieved by a RAID level 1 organization for the following:\na.\nRead operations on single blocks\nb.\nRead operations on multiple contiguous blocks\n10.19\nCompare the performance of write operations achieved by a RAID level\n5 organization with that achieved by a RAID level 1 organization.\n10.20\nAssume that you have a mixed conﬁguration comprising disks orga-\nnized as RAID level 1 and RAID level 5 disks. Assume that the system",
  "has ﬂexibility in deciding which disk organization to use for storing a\nparticular ﬁle. Which ﬁles should be stored in the RAID level 1 disks\nand which in the RAID level 5 disks in order to optimize performance?\n10.21\nThe reliability of a hard-disk drive is typically described in terms of\na quantity called mean time between failures (MTBF). Although this\nquantity is called a “time,” the MTBF actually is measured in drive-hours\nper failure.\na.\nIf a system contains 1,000 disk drives, each of which has a 750,000-\nhour MTBF, which of the following best describes how often a\ndrive failure will occur in that disk farm: once per thousand years,\nonce per century, once per decade, once per year, once per month,\nonce per week, once per day, once per hour, once per minute, or\nonce per second?\nb.",
  "once per second?\nb.\nMortality statistics indicate that, on the average, a U.S. resident\nhas about 1 chance in 1,000 of dying between the ages of 20 and 21.\nDeduce the MTBF hours for 20-year-olds. Convert this ﬁgure from\nhours to years. What does this MTBF tell you about the expected\nlifetime of a 20-year-old? Bibliographical Notes\n501\nc.\nThe manufacturer guarantees a 1-million-hour MTBF for a certain\nmodel of disk drive. What can you conclude about the number of\nyears for which one of these drives is under warranty?\n10.22\nDiscuss the relative advantages and disadvantages of sector sparing\nand sector slipping.\n10.23\nDiscuss the reasons why the operating system might require accurate\ninformation on how blocks are stored on a disk. How could the oper-",
  "ating system improve ﬁle-system performance with this knowledge?\nProgramming Problems\n10.24\nWrite a program that implements the following disk-scheduling algo-\nrithms:\na.\nFCFS\nb.\nSSTF\nc.\nSCAN\nd.\nC-SCAN\ne.\nLOOK\nf.\nC-LOOK\nYour program will service a disk with 5,000 cylinders numbered 0 to\n4,999. The program will generate a random series of 1,000 cylinder\nrequests and service them according to each of the algorithms listed\nabove. The program will be passed the initial position of the disk head\n(as a parameter on the command line) and report the total amount of\nhead movement required by each algorithm.\nBibliographical Notes\n[Services (2012)] provides an overview of data storage in a variety of modern\ncomputing environments. [Teorey and Pinkerton (1972)] present an early",
  "comparative analysis of disk-scheduling algorithms using simulations that\nmodel a disk for which seek time is linear in the number of cylinders crossed.\nScheduling optimizations that exploit disk idle times are discussed in [Lumb\net al. (2000)]. [Kim et al. (2009)] discusses disk-scheduling algorithms for SSDs.\nDiscussions of redundant arrays of independent disks (RAIDs) are pre-\nsented by [Patterson et al. (1988)].\n[Russinovich and Solomon (2009)], [McDougall and Mauro (2007)], and\n[Love (2010)] discuss ﬁle system details in Windows, Solaris, and Linux,\nrespectively.\nThe I/O size and randomness of the workload inﬂuence disk performance\nconsiderably. [Ousterhout et al. (1985)] and [Ruemmler and Wilkes (1993)]\nreport numerous interesting workload characteristics—for example, most ﬁles",
  "are small, most newly created ﬁles are deleted soon thereafter, most ﬁles that 502\nChapter 10\nMass-Storage Structure\nare opened for reading are read sequentially in their entirety, and most seeks\nare short.\nThe concept of a storage hierarchy has been studied for more than forty\nyears. For instance, a 1970 paper by [Mattson et al. (1970)] describes a\nmathematical approach to predicting the performance of a storage hierarchy.\nBibliography\n[Kim et al. (2009)]\nJ. Kim, Y. Oh, E. Kim, J. C. D. Lee, andS. Noh,“Diskschedulers\nfor solid state drivers” (2009), pages 295–304.\n[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[Lumb et al. (2000)]\nC. Lumb, J. Schindler, G. R. Ganger, D. F. Nagle, and",
  "E. Riedel, “Towards Higher Disk Head Utilization: Extracting Free Bandwidth\nFrom Busy Disk Drives”, Symposium on Operating Systems Design and Implemen-\ntation (2000).\n[Mattson et al. (1970)]\nR. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger,\n“Evaluation Techniques for Storage Hierarchies”, IBM Systems Journal, Volume\n9, Number 2 (1970), pages 78–117.\n[McDougall and Mauro (2007)]\nR. McDougall and J. Mauro, Solaris Internals,\nSecond Edition, Prentice Hall (2007).\n[Ousterhout et al. (1985)]\nJ. K. Ousterhout, H. D. Costa, D. Harrison, J. A. Kunze,\nM. Kupfer, and J. G. Thompson, “A Trace-Driven Analysis of the UNIX 4.2 BSD\nFile System”, Proceedings of the ACM Symposium on Operating Systems Principles\n(1985), pages 15–24.\n[Patterson et al. (1988)]",
  "(1985), pages 15–24.\n[Patterson et al. (1988)]\nD. A. Patterson, G. Gibson, and R. H. Katz, “A Case\nfor Redundant Arrays of Inexpensive Disks (RAID)”, Proceedings of the ACM\nSIGMOD International Conference on the Management of Data (1988), pages 109–\n116.\n[Ruemmler and Wilkes (1993)]\nC. Ruemmler and J. Wilkes, “Unix Disk Access\nPatterns”, Proceedings of the Winter USENIX Conference (1993), pages 405–420.\n[Russinovich and Solomon (2009)]\nM. E. Russinovich and D. A. Solomon, Win-\ndows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition,\nMicrosoft Press (2009).\n[Services (2012)]\nE. E. Services, Information Storage and Management: Storing,\nManaging, and Protecting Digital Information in Classic, Virtualized, and Cloud\nEnvironments, Wiley (2012).",
  "Environments, Wiley (2012).\n[Teorey and Pinkerton (1972)]\nT. J. Teorey and T. B. Pinkerton, “A Comparative\nAnalysis of Disk Scheduling Policies”, Communications of the ACM, Volume 15,\nNumber 3 (1972), pages 177–184. 11\nC H A P T E R\nFile-System\nInterface\nFor most users, the ﬁle system is the most visible aspect of an operating\nsystem. It provides the mechanism for on-line storage of and access to both\ndata and programs of the operating system and all the users of the computer\nsystem. The ﬁle system consists of two distinct parts: a collection of ﬁles, each\nstoring related data, and a directory structure, which organizes and provides\ninformation about all the ﬁles in the system. File systems live on devices,\nwhich we described in the preceding chapter and will continue to discuss in",
  "the following one. In this chapter, we consider the various aspects of ﬁles and\nthe major directory structures. We also discuss the semantics of sharing ﬁles\namong multiple processes, users, and computers. Finally, we discuss ways to\nhandle ﬁle protection, necessary when we have multiple users and we want to\ncontrol who may access ﬁles and how ﬁles may be accessed.\nCHAPTER OBJECTIVES\n• To explain the function of ﬁle systems.\n• To describe the interfaces to ﬁle systems.\n• To discuss ﬁle-system design tradeoffs, including access methods, ﬁle\nsharing, ﬁle locking, and directory structures.\n• To explore ﬁle-system protection.\n11.1\nFile Concept\nComputers can store information on various storage media, such as magnetic\ndisks, magnetic tapes, and optical disks. So that the computer system will",
  "be convenient to use, the operating system provides a uniform logical view\nof stored information. The operating system abstracts from the physical\nproperties of its storage devices to deﬁne a logical storage unit, the ﬁle. Files are\nmapped by the operating system onto physical devices. These storage devices\nare usually nonvolatile, so the contents are persistent between system reboots.\n503 504\nChapter 11\nFile-System Interface\nA ﬁle is a named collection of related information that is recorded on\nsecondary storage. From a user’s perspective, a ﬁle is the smallest allotment\nof logical secondary storage; that is, data cannot be written to secondary\nstorage unless they are within a ﬁle. Commonly, ﬁles represent programs (both",
  "source and object forms) and data. Data ﬁles may be numeric, alphabetic,\nalphanumeric, or binary. Files may be free form, such as text ﬁles, or may be\nformatted rigidly. In general, a ﬁle is a sequence of bits, bytes, lines, or records,\nthe meaning of which is deﬁned by the ﬁle’s creator and user. The concept of\na ﬁle is thus extremely general.\nThe information in a ﬁle is deﬁned by its creator. Many different types of\ninformation may be stored in a ﬁle—source or executable programs, numeric or\ntext data, photos, music, video, and so on. A ﬁle has a certain deﬁned structure,\nwhich depends on its type. A text ﬁle is a sequence of characters organized\ninto lines (and possibly pages). A source ﬁle is a sequence of functions, each of",
  "which is further organized as declarations followed by executable statements.\nAn executable ﬁle is a series of code sections that the loader can bring into\nmemory and execute.\n11.1.1\nFile Attributes\nA ﬁle is named, for the convenience of its human users, and is referred to by\nits name. A name is usually a string of characters, such as example.c. Some\nsystems differentiate between uppercase and lowercase characters in names,\nwhereas other systems do not. When a ﬁle is named, it becomes independent\nof the process, the user, and even the system that created it. For instance, one\nuser might create the ﬁle example.c, and another user might edit that ﬁle by\nspecifying its name. The ﬁle’s owner might write the ﬁle to a USB disk, send it",
  "as an e-mail attachment, or copy it across a network, and it could still be called\nexample.c on the destination system.\nA ﬁle’s attributes vary from one operating system to another but typically\nconsist of these:\n• Name. The symbolic ﬁle name is the only information kept in human-\nreadable form.\n• Identiﬁer. This unique tag, usually a number, identiﬁes the ﬁle within the\nﬁle system; it is the non-human-readable name for the ﬁle.\n• Type. This information is needed for systems that support different types\nof ﬁles.\n• Location. This information is a pointer to a device and to the location of\nthe ﬁle on that device.\n• Size. The current size of the ﬁle (in bytes, words, or blocks) and possibly\nthe maximum allowed size are included in this attribute.",
  "• Protection. Access-control information determines who can do reading,\nwriting, executing, and so on.\n• Time, date, and user identiﬁcation. This information may be kept for\ncreation, last modiﬁcation, and last use. These data can be useful for\nprotection, security, and usage monitoring. 11.1\nFile Concept\n505\nFigure 11.1\nA ﬁle info window on Mac OS X.\nSome newer ﬁle systems also support extended ﬁle attributes, including\ncharacter encoding of the ﬁle and security features such as a ﬁle checksum.\nFigure 11.1 illustrates a ﬁle info window on Mac OS X, which displays a ﬁle’s\nattributes.\nThe information about all ﬁles is kept in the directory structure, which\nalso resides on secondary storage. Typically, a directory entry consists of the",
  "ﬁle’s name and its unique identiﬁer. The identiﬁer in turn locates the other 506\nChapter 11\nFile-System Interface\nﬁle attributes. It may take more than a kilobyte to record this information for\neach ﬁle. In a system with many ﬁles, the size of the directory itself may be\nmegabytes. Because directories, like ﬁles, must be nonvolatile, they must be\nstored on the device and brought into memory piecemeal, as needed.\n11.1.2\nFile Operations\nA ﬁle is an abstract data type. To deﬁne a ﬁle properly, we need to consider the\noperations that can be performed on ﬁles. The operating system can provide\nsystem calls to create, write, read, reposition, delete, and truncate ﬁles. Let’s\nexamine what the operating system must do to perform each of these six basic",
  "ﬁle operations. It should then be easy to see how other similar operations, such\nas renaming a ﬁle, can be implemented.\n• Creating a ﬁle. Two steps are necessary to create a ﬁle. First, space in the\nﬁle system must be found for the ﬁle. We discuss how to allocate space for\nthe ﬁle in Chapter 12. Second, an entry for the new ﬁle must be made in\nthe directory.\n• Writing a ﬁle. To write a ﬁle, we make a system call specifying both the\nname of the ﬁle and the information to be written to the ﬁle. Given the\nname of the ﬁle, the system searches the directory to ﬁnd the ﬁle’s location.\nThe system must keep a write pointer to the location in the ﬁle where the\nnext write is to take place. The write pointer must be updated whenever a\nwrite occurs.",
  "write occurs.\n• Reading a ﬁle. To read from a ﬁle, we use a system call that speciﬁes the\nname of the ﬁle and where (in memory) the next block of the ﬁle should\nbe put. Again, the directory is searched for the associated entry, and the\nsystem needs to keep a read pointer to the location in the ﬁle where the\nnext read is to take place. Once the read has taken place, the read pointer\nis updated. Because a process is usually either reading from or writing to\na ﬁle, the current operation location can be kept as a per-process current-\nﬁle-position pointer. Both the read and write operations use this same\npointer, saving space and reducing system complexity.\n• Repositioning within a ﬁle. The directory is searched for the appropriate",
  "entry, and the current-ﬁle-position pointer is repositioned to a given value.\nRepositioning within a ﬁle need not involve any actual I/O. This ﬁle\noperation is also known as a ﬁle seek.\n• Deleting a ﬁle. To delete a ﬁle, we search the directory for the named ﬁle.\nHaving found the associated directory entry, we release all ﬁle space, so\nthat it can be reused by other ﬁles, and erase the directory entry.\n• Truncating a ﬁle. The user may want to erase the contents of a ﬁle but\nkeep its attributes. Rather than forcing the user to delete the ﬁle and then\nrecreate it, this function allows all attributes to remain unchanged—except\nfor ﬁle length—but lets the ﬁle be reset to length zero and its ﬁle space\nreleased.\nThese six basic operations comprise the minimal set of required ﬁle",
  "operations. Other common operations include appending new information 11.1\nFile Concept\n507\nto the end of an existing ﬁle and renaming an existing ﬁle. These primitive\noperations can then be combined to perform other ﬁle operations. For instance,\nwe can create a copy of a ﬁle—or copy the ﬁle to another I/O device, such as\na printer or a display—by creating a new ﬁle and then reading from the old\nand writing to the new. We also want to have operations that allow a user to\nget and set the various attributes of a ﬁle. For example, we may want to have\noperations that allow a user to determine the status of a ﬁle, such as the ﬁle’s\nlength, and to set ﬁle attributes, such as the ﬁle’s owner.\nMost of the ﬁle operations mentioned involve searching the directory for",
  "the entry associated with the named ﬁle. To avoid this constant searching,\nmany systems require that an open() system call be made before a ﬁle is ﬁrst\nused. The operating system keeps a table, called the open-ﬁle table, containing\ninformation about all open ﬁles. When a ﬁle operation is requested, the ﬁle is\nspeciﬁed via an index into this table, so no searching is required. When the ﬁle\nis no longer being actively used, it is closed by the process, and the operating\nsystem removes its entry from the open-ﬁle table. create() and delete() are\nsystem calls that work with closed rather than open ﬁles.\nSome systems implicitly open a ﬁle when the ﬁrst reference to it is made.\nThe ﬁle is automatically closed when the job or program that opened the",
  "ﬁle terminates. Most systems, however, require that the programmer open a\nﬁle explicitly with the open() system call before that ﬁle can be used. The\nopen() operation takes a ﬁle name and searches the directory, copying the\ndirectory entry into the open-ﬁle table. The open() call can also accept access-\nmode information—create, read-only, read–write, append-only, and so on.\nThis mode is checked against the ﬁle’s permissions. If the request mode is\nallowed, the ﬁle is opened for the process. The open() system call typically\nreturns a pointer to the entry in the open-ﬁle table. This pointer, not the actual\nﬁle name, is used in all I/O operations, avoiding any further searching and\nsimplifying the system-call interface.\nThe implementation of the open() and close() operations is more",
  "complicated in an environment where several processes may open the ﬁle\nsimultaneously. This may occur in a system where several different applications\nopen the same ﬁle at the same time. Typically, the operating system uses two\nlevels of internal tables: a per-process table and a system-wide table. The per-\nprocess table tracks all ﬁles that a process has open. Stored in this table is\ninformation regarding the process’s use of the ﬁle. For instance, the current\nﬁle pointer for each ﬁle is found here. Access rights to the ﬁle and accounting\ninformation can also be included.\nEach entry in the per-process table in turn points to a system-wide open-ﬁle\ntable. The system-wide table contains process-independent information, such",
  "as the location of the ﬁle on disk, access dates, and ﬁle size. Once a ﬁle has\nbeen opened by one process, the system-wide table includes an entry for the\nﬁle. When another process executes an open() call, a new entry is simply\nadded to the process’s open-ﬁle table pointing to the appropriate entry in\nthe system-wide table. Typically, the open-ﬁle table also has an open count\nassociated with each ﬁle to indicate how many processes have the ﬁle open.\nEach close() decreases this open count, and when the open count reaches\nzero, the ﬁle is no longer in use, and the ﬁle’s entry is removed from the\nopen-ﬁle table. 508\nChapter 11\nFile-System Interface\nIn summary, several pieces of information are associated with an open ﬁle.",
  "• File pointer. On systems that do not include a ﬁle offset as part of the\nread() and write() system calls, the system must track the last read–\nwrite location as a current-ﬁle-position pointer. This pointer is unique to\neach process operating on the ﬁle and therefore must be kept separate from\nthe on-disk ﬁle attributes.\n• File-open count. As ﬁles are closed, the operating system must reuse its\nopen-ﬁle table entries, or it could run out of space in the table. Multiple\nprocesses may have opened a ﬁle, and the system must wait for the last\nﬁle to close before removing the open-ﬁle table entry. The ﬁle-open count\ntracks the number of opens and closes and reaches zero on the last close.\nThe system can then remove the entry.",
  "The system can then remove the entry.\n• Disk location of the ﬁle. Most ﬁle operations require the system to modify\ndata within the ﬁle. The information needed to locate the ﬁle on disk is\nkept in memory so that the system does not have to read it from disk for\neach operation.\n• Access rights. Each process opens a ﬁle in an access mode. This information\nis stored on the per-process table so the operating system can allow or deny\nsubsequent I/O requests.\nSome operating systems provide facilities for locking an open ﬁle (or\nsections of a ﬁle). File locks allow one process to lock a ﬁle and prevent other\nprocesses from gaining access to it. File locks are useful for ﬁles that are shared\nby several processes—for example, a system log ﬁle that can be accessed and",
  "modiﬁed by a number of processes in the system.\nFile locks provide functionality similar to reader–writer locks, covered in\nSection 5.7.2. A shared lock is akin to a reader lock in that several processes\ncan acquire the lock concurrently. An exclusive lock behaves like a writer lock;\nonly one process at a time can acquire such a lock. It is important to note\nthat not all operating systems provide both types of locks: some systems only\nprovide exclusive ﬁle locking.\nFILE LOCKING IN JAVA\nIn the Java API, acquiring a lock requires ﬁrst obtaining the FileChannel\nfor the ﬁle to be locked. The lock() method of the FileChannel is used to\nacquire the lock. The API of the lock() method is\nFileLock lock(long begin, long end, boolean shared)",
  "where begin and end are the beginning and ending positions of the region\nbeing locked. Setting shared to true is for shared locks; setting shared\nto false acquires the lock exclusively. The lock is released by invoking the\nrelease() of the FileLock returned by the lock() operation.\nThe program in Figure 11.2 illustrates ﬁle locking in Java. This program\nacquires two locks on the ﬁle file.txt. The ﬁrst half of the ﬁle is acquired\nas an exclusive lock; the lock for the second half is a shared lock. 11.1\nFile Concept\n509\nFILE LOCKING IN JAVA (Continued)\nimport java.io.*;\nimport java.nio.channels.*;\npublic class LockingExample {\npublic static final boolean EXCLUSIVE = false;\npublic static final boolean SHARED = true;\npublic static void main(String args[]) throws IOException {",
  "FileLock sharedLock = null;\nFileLock exclusiveLock = null;\ntry {\nRandomAccessFile raf = new RandomAccessFile(\"file.txt\",\"rw\");\n// get the channel for the file\nFileChannel ch = raf.getChannel();\n// this locks the first half of the file - exclusive\nexclusiveLock = ch.lock(0, raf.length()/2, EXCLUSIVE);\n/** Now modify the data . . . */\n// release the lock\nexclusiveLock.release();\n// this locks the second half of the file - shared\nsharedLock = ch.lock(raf.length()/2+1,raf.length(),SHARED);\n/** Now read the data . . . */\n// release the lock\nsharedLock.release();\n} catch (java.io.IOException ioe) {\nSystem.err.println(ioe);\n}\nfinally {\nif (exclusiveLock != null)\nexclusiveLock.release();\nif (sharedLock != null)\nsharedLock.release();\n}\n}\n}\nFigure 11.2\nFile-locking example in Java.",
  "}\n}\n}\nFigure 11.2\nFile-locking example in Java.\nFurthermore, operating systems may provide either mandatory or advi-\nsory ﬁle-locking mechanisms. If a lock is mandatory, then once a process\nacquires an exclusive lock, the operating system will prevent any other process 510\nChapter 11\nFile-System Interface\nfrom accessing the locked ﬁle. For example, assume a process acquires an\nexclusive lock on the ﬁle system.log. If we attempt to open system.log\nfrom another process—for example, a text editor—the operating system will\nprevent access until the exclusive lock is released. This occurs even if the text\neditor is not written explicitly to acquire the lock. Alternatively, if the lock\nis advisory, then the operating system will not prevent the text editor from",
  "acquiring access to system.log. Rather, the text editor must be written so that\nit manually acquires the lock before accessing the ﬁle. In other words, if the\nlocking scheme is mandatory, the operating system ensures locking integrity.\nFor advisory locking, it is up to software developers to ensure that locks are\nappropriately acquired and released. As a general rule, Windows operating\nsystems adopt mandatory locking, and UNIX systems employ advisory locks.\nThe use of ﬁle locks requires the same precautions as ordinary process\nsynchronization. For example, programmers developing on systems with\nmandatory locking must be careful to hold exclusive ﬁle locks only while\nthey are accessing the ﬁle. Otherwise, they will prevent other processes from",
  "accessing the ﬁle as well. Furthermore, some measures must be taken to ensure\nthat two or more processes do not become involved in a deadlock while trying\nto acquire ﬁle locks.\n11.1.3\nFile Types\nWhen we design a ﬁle system—indeed, an entire operating system—we\nalways consider whether the operating system should recognize and support\nﬁle types. If an operating system recognizes the type of a ﬁle, it can then operate\non the ﬁle in reasonable ways. For example, a common mistake occurs when a\nuser tries to output the binary-object form of a program. This attempt normally\nproduces garbage; however, the attempt can succeed if the operating system\nhas been told that the ﬁle is a binary-object program.\nA common technique for implementing ﬁle types is to include the type",
  "as part of the ﬁle name. The name is split into two parts—a name and an\nextension, usually separated by a period (Figure 11.3). In this way, the user\nand the operating system can tell from the name alone what the type of a ﬁle\nis. Most operating systems allow users to specify a ﬁle name as a sequence\nof characters followed by a period and terminated by an extension made\nup of additional characters. Examples include resume.docx, server.c, and\nReaderThread.cpp.\nThe system uses the extension to indicate the type of the ﬁle and the type\nof operations that can be done on that ﬁle. Only a ﬁle with a .com, .exe, or .sh\nextension can be executed, for instance. The .com and .exe ﬁles are two forms\nof binary executable ﬁles, whereas the .sh ﬁle is a shell script containing, in",
  "ASCII format, commands to the operating system. Application programs also\nuse extensions to indicate ﬁle types in which they are interested. For example,\nJava compilers expect source ﬁles to have a .java extension, and the Microsoft\nWord word processor expects its ﬁles to end with a .doc or .docx extension.\nThese extensions are not always required, so a user may specify a ﬁle without\nthe extension (to save typing), and the application will look for a ﬁle with\nthe given name and the extension it expects. Because these extensions are\nnot supported by the operating system, they can be considered “hints” to the\napplications that operate on them. 11.1\nFile Concept\n511\nfile type\nusual extension\nfunction\nready-to-run machine-\nlanguage program \nexecutable\nexe, com, bin\nor none \ncompiled, machine",
  "exe, com, bin\nor none \ncompiled, machine\nlanguage, not linked \nobject\nobj, o\nbinary file containing\naudio or A/V information   \nmultimedia\nmpeg, mov, mp3,\nmp4, avi\nrelated files grouped into\none file, sometimes com-\npressed, for archiving\nor storage\narchive\nrar, zip, tar\nASCII or binary file in a\nformat for printing or\nviewing\nprint or view\ngif, pdf, jpg\nlibraries of routines for\nprogrammers\nlibrary\nlib, a, so, dll\nvarious word-processor\nformats\nword processor\ndocx\ncommands to the command\ninterpreter\nbatch\nbat, sh\ntextual data, documents\nmarkup\nxml, html, tex\nsource code in various\nlanguages\nsource code\nc, cc, java, perl,\nasm\nxml, rtf,\nFigure 11.3\nCommon ﬁle types.\nConsider, too, the Mac OS X operating system. In this system, each ﬁle has",
  "a type, such as .app (for application). Each ﬁle also has a creator attribute\ncontaining the name of the program that created it. This attribute is set by\nthe operating system during the create() call, so its use is enforced and\nsupported by the system. For instance, a ﬁle produced by a word processor\nhas the word processor’s name as its creator. When the user opens that ﬁle, by\ndouble-clicking the mouse on the icon representing the ﬁle, the word processor\nis invoked automatically and the ﬁle is loaded, ready to be edited.\nThe UNIX system uses a crude magic number stored at the beginning of\nsome ﬁles to indicate roughly the type of the ﬁle—executable program, shell\nscript, PDF ﬁle, and so on. Not all ﬁles have magic numbers, so system features",
  "cannot be based solely on this information. UNIX does not record the name of\nthe creating program, either. UNIX does allow ﬁle-name-extension hints, but\nthese extensions are neither enforced nor depended on by the operating system;\nthey are meant mostly to aid users in determining what type of contents the\nﬁle contains. Extensions can be used or ignored by a given application, but that\nis up to the application’s programmer.\n11.1.4\nFile Structure\nFile types also can be used to indicate the internal structure of the ﬁle. As\nmentioned in Section 11.1.3, source and object ﬁles have structures that match\nthe expectations of the programs that read them. Further, certain ﬁles must 512\nChapter 11\nFile-System Interface\nconform to a required structure that is understood by the operating system. For",
  "example, the operating system requires that an executable ﬁle have a speciﬁc\nstructure so that it can determine where in memory to load the ﬁle and what\nthe location of the ﬁrst instruction is. Some operating systems extend this idea\ninto a set of system-supported ﬁle structures, with sets of special operations\nfor manipulating ﬁles with those structures.\nThis point brings us to one of the disadvantages of having the operating\nsystem support multiple ﬁle structures: the resulting size of the operating\nsystem is cumbersome. If the operating system deﬁnes ﬁve different ﬁle\nstructures, it needs to contain the code to support these ﬁle structures.\nIn addition, it may be necessary to deﬁne every ﬁle as one of the ﬁle\ntypes supported by the operating system. When new applications require",
  "information structured in ways not supported by the operating system, severe\nproblems may result.\nFor example, assume that a system supports two types of ﬁles: text ﬁles\n(composed of ASCII characters separated by a carriage return and line feed)\nand executable binary ﬁles. Now, if we (as users) want to deﬁne an encrypted\nﬁle to protect the contents from being read by unauthorized people, we may\nﬁnd neither ﬁle type to be appropriate. The encrypted ﬁle is not ASCII text lines\nbut rather is (apparently) random bits. Although it may appear to be a binary\nﬁle, it is not executable. As a result, we may have to circumvent or misuse the\noperating system’s ﬁle-type mechanism or abandon our encryption scheme.\nSome operating systems impose (and support) a minimal number of ﬁle",
  "structures. This approach has been adopted in UNIX, Windows, and others.\nUNIX considers each ﬁle to be a sequence of 8-bit bytes; no interpretation of\nthese bits is made by the operating system. This scheme provides maximum\nﬂexibility but little support. Each application program must include its own\ncode to interpret an input ﬁle as to the appropriate structure. However, all\noperating systems must support at least one structure—that of an executable\nﬁle—so that the system is able to load and run programs.\n11.1.5\nInternal File Structure\nInternally, locating an offset within a ﬁle can be complicated for the operating\nsystem. Disk systems typically have a well-deﬁned block size determined by\nthe size of a sector. All disk I/O is performed in units of one block (physical",
  "record), and all blocks are the same size. It is unlikely that the physical record\nsize will exactly match the length of the desired logical record. Logical records\nmay even vary in length. Packing a number of logical records into physical\nblocks is a common solution to this problem.\nFor example, the UNIX operating system deﬁnes all ﬁles to be simply\nstreams of bytes. Each byte is individually addressable by its offset from the\nbeginning (or end) of the ﬁle. In this case, the logical record size is 1 byte. The\nﬁle system automatically packs and unpacks bytes into physical disk blocks—\nsay, 512 bytes per block—as necessary.\nThe logical record size, physical block size, and packing technique deter-\nmine how many logical records are in each physical block. The packing can be",
  "done either by the user’s application program or by the operating system. In\neither case, the ﬁle may be considered a sequence of blocks. All the basic I/O 11.2\nAccess Methods\n513\nbeginning\nend\ncurrent position\nrewind\nread or write\nFigure 11.4\nSequential-access ﬁle.\nfunctions operate in terms of blocks. The conversion from logical records to\nphysical blocks is a relatively simple software problem.\nBecause disk space is always allocated in blocks, some portion of the last\nblock of each ﬁle is generally wasted. If each block were 512 bytes, for example,\nthen a ﬁle of 1,949 bytes would be allocated four blocks (2,048 bytes); the last\n99 bytes would be wasted. The waste incurred to keep everything in units\nof blocks (instead of bytes) is internal fragmentation. All ﬁle systems suffer",
  "from internal fragmentation; the larger the block size, the greater the internal\nfragmentation.\n11.2\nAccess Methods\nFiles store information. When it is used, this information must be accessed\nand read into computer memory. The information in the ﬁle can be accessed\nin several ways. Some systems provide only one access method for ﬁles.\nwhile others support many access methods, and choosing the right one for\na particular application is a major design problem.\n11.2.1\nSequential Access\nThe simplest access method is sequential access. Information in the ﬁle is\nprocessed in order, one record after the other. This mode of access is by far the\nmost common; for example, editors and compilers usually access ﬁles in this\nfashion.\nReads and writes make up the bulk of the operations on a ﬁle. A read",
  "operation—read next()—reads the next portion of the ﬁle and automatically\nadvances a ﬁle pointer, which tracks the I/O location. Similarly, the write\noperation—write next()—appends to the end of the ﬁle and advances to the\nend of the newly written material (the new end of ﬁle). Such a ﬁle can be reset\nto the beginning, and on some systems, a program may be able to skip forward\nor backward n records for some integer n—perhaps only for n = 1. Sequential\naccess, which is depicted in Figure 11.4, is based on a tape model of a ﬁle and\nworks as well on sequential-access devices as it does on random-access ones.\n11.2.2\nDirect Access\nAnother method is direct access (or relative access). Here, a ﬁle is made up\nof ﬁxed-length logical records that allow programs to read and write records",
  "rapidly in no particular order. The direct-access method is based on a disk\nmodel of a ﬁle, since disks allow random access to any ﬁle block. For direct 514\nChapter 11\nFile-System Interface\naccess, the ﬁle is viewed as a numbered sequence of blocks or records. Thus,\nwe may read block 14, then read block 53, and then write block 7. There are no\nrestrictions on the order of reading or writing for a direct-access ﬁle.\nDirect-access ﬁles are of great use for immediate access to large amounts\nof information. Databases are often of this type. When a query concerning a\nparticular subject arrives, we compute which block contains the answer and\nthen read that block directly to provide the desired information.\nAs a simple example, on an airline-reservation system, we might store all",
  "the information about a particular ﬂight (for example, ﬂight 713) in the block\nidentiﬁed by the ﬂight number. Thus, the number of available seats for ﬂight\n713 is stored in block 713 of the reservation ﬁle. To store information about a\nlarger set, such as people, we might compute a hash function on the people’s\nnames or search a small in-memory index to determine a block to read and\nsearch.\nFor the direct-access method, the ﬁle operations must be modiﬁed to\ninclude the block number as a parameter. Thus, we have read(n), where\nn is the block number, rather than read next(), and write(n) rather\nthan write next(). An alternative approach is to retain read next() and\nwrite next(), as with sequential access, and to add an operation posi-",
  "tion file(n) where n is the block number. Then, to effect a read(n), we\nwould position file(n) and then read next().\nThe block number provided by the user to the operating system is normally\na relative block number. A relative block number is an index relative to the\nbeginning of the ﬁle. Thus, the ﬁrst relative block of the ﬁle is 0, the next is\n1, and so on, even though the absolute disk address may be 14703 for the\nﬁrst block and 3192 for the second. The use of relative block numbers allows\nthe operating system to decide where the ﬁle should be placed (called the\nallocation problem, as we discuss in Chapter 12) and helps to prevent the user\nfrom accessing portions of the ﬁle system that may not be part of her ﬁle. Some\nsystems start their relative block numbers at 0; others start at 1.",
  "How, then, does the system satisfy a request for record N in a ﬁle? Assuming\nwe have a logical record length L, the request for record N is turned into an\nI/O request for L bytes starting at location L ∗(N) within the ﬁle (assuming the\nﬁrst record is N = 0). Since logical records are of a ﬁxed size, it is also easy to\nread, write, or delete a record.\nNot all operating systems support both sequential and direct access for\nﬁles. Some systems allow only sequential ﬁle access; others allow only direct\naccess. Some systems require that a ﬁle be deﬁned as sequential or direct when\nit is created. Such a ﬁle can be accessed only in a manner consistent with its\ndeclaration. We can easily simulate sequential access on a direct-access ﬁle by",
  "simply keeping a variable cp that deﬁnes our current position, as shown in\nFigure 11.5. Simulating a direct-access ﬁle on a sequential-access ﬁle, however,\nis extremely inefﬁcient and clumsy.\n11.2.3\nOther Access Methods\nOther access methods can be built on top of a direct-access method. These\nmethods generally involve the construction of an index for the ﬁle. The index,\nlike an index in the back of a book, contains pointers to the various blocks. To 11.3\nDirectory and Disk Structure\n515\nsequential access\nreset\nread_next\nwrite_next\ncp \n 0;\nread cp ;\ncp \n cp \n 1;\nwrite cp;\ncp \n cp \n 1;\nimplementation for direct access\nFigure 11.5\nSimulation of sequential access on a direct-access ﬁle.\nﬁnd a record in the ﬁle, we ﬁrst search the index and then use the pointer to",
  "access the ﬁle directly and to ﬁnd the desired record.\nFor example, a retail-price ﬁle might list the universal product codes (UPCs)\nfor items, with the associated prices. Each record consists of a 10-digit UPC and\na 6-digit price, for a 16-byte record. If our disk has 1,024 bytes per block, we\ncan store 64 records per block. A ﬁle of 120,000 records would occupy about\n2,000 blocks (2 million bytes). By keeping the ﬁle sorted by UPC, we can deﬁne\nan index consisting of the ﬁrst UPC in each block. This index would have 2,000\nentries of 10 digits each, or 20,000 bytes, and thus could be kept in memory. To\nﬁnd the price of a particular item, we can make a binary search of the index.\nFrom this search, we learn exactly which block contains the desired record and",
  "access that block. This structure allows us to search a large ﬁle doing little I/O.\nWith large ﬁles, the index ﬁle itself may become too large to be kept in\nmemory. One solution is to create an index for the index ﬁle. The primary\nindex ﬁle contains pointers to secondary index ﬁles, which point to the actual\ndata items.\nFor example, IBM’s indexed sequential-access method (ISAM) uses a small\nmaster index that points to disk blocks of a secondary index. The secondary\nindex blocks point to the actual ﬁle blocks. The ﬁle is kept sorted on a deﬁned\nkey. To ﬁnd a particular item, we ﬁrst make a binary search of the master index,\nwhich provides the block number of the secondary index. This block is read\nin, and again a binary search is used to ﬁnd the block containing the desired",
  "record. Finally, this block is searched sequentially. In this way, any record can\nbe located from its key by at most two direct-access reads. Figure 11.6 shows a\nsimilar situation as implemented by VMS index and relative ﬁles.\n11.3\nDirectory and Disk Structure\nNext, we consider how to store ﬁles. Certainly, no general-purpose computer\nstores just one ﬁle. There are typically thousands, millions, even billions of\nﬁles within a computer. Files are stored on random-access storage devices,\nincluding hard disks, optical disks, and solid-state (memory-based) disks.\nA storage device can be used in its entirety for a ﬁle system. It can also be\nsubdivided for ﬁner-grained control. For example, a disk can be partitioned\ninto quarters, and each quarter can hold a separate ﬁle system. Storage devices",
  "can also be collected together into RAID sets that provide protection from the\nfailure of a single disk (as described in Section 10.7). Sometimes, disks are\nsubdivided and also collected into RAID sets. 516\nChapter 11\nFile-System Interface\nindex file\nrelative file\nSmith\nlast name\nsmith, john social-security age\nlogical record\nnumber\nAdams\nArthur\nAsher\n•\n•\n•\nFigure 11.6\nExample of index and relative ﬁles.\nPartitioning is useful for limiting the sizes of individual ﬁle systems,\nputting multiple ﬁle-system types on the same device, or leaving part of the\ndevice available for other uses, such as swap space or unformatted (raw) disk\nspace. A ﬁle system can be created on each of these parts of the disk. Any entity\ncontaining a ﬁle system is generally known as a volume. The volume may be",
  "a subset of a device, a whole device, or multiple devices linked together into\na RAID set. Each volume can be thought of as a virtual disk. Volumes can also\nstore multiple operating systems, allowing a system to boot and run more than\none operating system.\nEach volume that contains a ﬁle system must also contain information\nabout the ﬁles in the system. This information is kept in entries in a device\ndirectory or volume table of contents. The device directory (more commonly\nknown simply as the directory) records information—such as name, location,\nsize, and type—for all ﬁles on that volume. Figure 11.7 shows a typical\nﬁle-system organization.\ndirectory\ndirectory\ndirectory\nfiles\npartition A\npartition B\npartition C\nfiles\ndisk 1\ndisk 2\ndisk 3\nfiles\nFigure 11.7",
  "files\ndisk 1\ndisk 2\ndisk 3\nfiles\nFigure 11.7\nA typical ﬁle-system organization. 11.3\nDirectory and Disk Structure\n517\n/\nufs\n/devices\ndevfs\n/dev\ndev\n/system/contract\nctfs\n/proc\nproc\n/etc/mnttab\nmntfs\n/etc/svc/volatile\ntmpfs\n/system/object\nobjfs\n/lib/libc.so.1\nlofs\n/dev/fd\nfd\n/var\nufs\n/tmp\ntmpfs\n/var/run\ntmpfs\n/opt\nufs\n/zpbge\nzfs\n/zpbge/backup\nzfs\n/export/home\nzfs\n/var/mail\nzfs\n/var/spool/mqueue\nzfs\n/zpbg\nzfs\n/zpbg/zones\nzfs\nFigure 11.8\nSolaris ﬁle systems.\n11.3.1\nStorage Structure\nAs we have just seen, a general-purpose computer system has multiple storage\ndevices, and those devices can be sliced up into volumes that hold ﬁle systems.\nComputer systems may have zero or more ﬁle systems, and the ﬁle systems\nmay be of varying types. For example, a typical Solaris system may have dozens",
  "of ﬁle systems of a dozen different types, as shown in the ﬁle system list in\nFigure 11.8.\nIn this book, we consider only general-purpose ﬁle systems. It is worth\nnoting, though, that there are many special-purpose ﬁle systems. Consider the\ntypes of ﬁle systems in the Solaris example mentioned above:\n• tmpfs—a “temporary” ﬁle system that is created in volatile main memory\nand has its contents erased if the system reboots or crashes\n• objfs—a “virtual” ﬁle system (essentially an interface to the kernel that\nlooks like a ﬁle system) that gives debuggers access to kernel symbols\n• ctfs—a virtual ﬁle system that maintains “contract” information to manage\nwhich processes start when the system boots and must continue to run\nduring operation",
  "during operation\n• lofs—a “loop back” ﬁle system that allows one ﬁle system to be accessed\nin place of another one\n• procfs—a virtual ﬁle system that presents information on all processes as\na ﬁle system\n• ufs, zfs—general-purpose ﬁle systems 518\nChapter 11\nFile-System Interface\nThe ﬁle systems of computers, then, can be extensive. Even within a ﬁle\nsystem, it is useful to segregate ﬁles into groups and manage and act on those\ngroups. This organization involves the use of directories. In the remainder of\nthis section, we explore the topic of directory structure.\n11.3.2\nDirectory Overview\nThe directory can be viewed as a symbol table that translates ﬁle names into\ntheir directory entries. If we take such a view, we see that the directory itself can",
  "be organized in many ways. The organization must allow us to insert entries,\nto delete entries, to search for a named entry, and to list all the entries in the\ndirectory. In this section, we examine several schemes for deﬁning the logical\nstructure of the directory system.\nWhen considering a particular directory structure, we need to keep in mind\nthe operations that are to be performed on a directory:\n• Search for a ﬁle. We need to be able to search a directory structure to ﬁnd\nthe entry for a particular ﬁle. Since ﬁles have symbolic names, and similar\nnames may indicate a relationship among ﬁles, we may want to be able to\nﬁnd all ﬁles whose names match a particular pattern.\n• Create a ﬁle. New ﬁles need to be created and added to the directory.",
  "• Delete a ﬁle. When a ﬁle is no longer needed, we want to be able to remove\nit from the directory.\n• List a directory. We need to be able to list the ﬁles in a directory and the\ncontents of the directory entry for each ﬁle in the list.\n• Rename a ﬁle. Because the name of a ﬁle represents its contents to its users,\nwe must be able to change the name when the contents or use of the ﬁle\nchanges. Renaming a ﬁle may also allow its position within the directory\nstructure to be changed.\n• Traverse the ﬁle system. We may wish to access every directory and every\nﬁle within a directory structure. For reliability, it is a good idea to save the\ncontents and structure of the entire ﬁle system at regular intervals. Often,\nwe do this by copying all ﬁles to magnetic tape. This technique provides a",
  "backup copy in case of system failure. In addition, if a ﬁle is no longer in\nuse, the ﬁle can be copied to tape and the disk space of that ﬁle released\nfor reuse by another ﬁle.\nIn the following sections, we describe the most common schemes for deﬁning\nthe logical structure of a directory.\n11.3.3\nSingle-Level Directory\nThe simplest directory structure is the single-level directory. All ﬁles are\ncontained in the same directory, which is easy to support and understand\n(Figure 11.9).\nA single-level directory has signiﬁcant limitations, however, when the\nnumber of ﬁles increases or when the system has more than one user. Since all\nﬁles are in the same directory, they must have unique names. If two users call 11.3\nDirectory and Disk Structure\n519\ncat\nfiles\ndirectory\nbo\na\ntest\ndata\nmail\ncont",
  "519\ncat\nfiles\ndirectory\nbo\na\ntest\ndata\nmail\ncont\nhex\nrecords\nFigure 11.9\nSingle-level directory.\ntheir data ﬁle test.txt, then the unique-name rule is violated. For example,\nin one programming class, 23 students called the program for their second\nassignment prog2.c; another 11 called it assign2.c. Fortunately, most ﬁle\nsystems support ﬁle names of up to 255 characters, so it is relatively easy to\nselect unique ﬁle names.\nEven a single user on a single-level directory may ﬁnd it difﬁcult to\nremember the names of all the ﬁles as the number of ﬁles increases. It is not\nuncommon for a user to have hundreds of ﬁles on one computer system and an\nequal number of additional ﬁles on another system. Keeping track of so many\nﬁles is a daunting task.\n11.3.4\nTwo-Level Directory",
  "11.3.4\nTwo-Level Directory\nAs we have seen, a single-level directory often leads to confusion of ﬁle names\namong different users. The standard solution is to create a separate directory\nfor each user.\nIn the two-level directory structure, each user has his own user ﬁle\ndirectory (UFD). The UFDs have similar structures, but each lists only the\nﬁles of a single user. When a user job starts or a user logs in, the system’s\nmaster ﬁle directory (MFD) is searched. The MFD is indexed by user name or\naccount number, and each entry points to the UFD for that user (Figure 11.10).\nWhen a user refers to a particular ﬁle, only his own UFD is searched. Thus,\ndifferent users may have ﬁles with the same name, as long as all the ﬁle names",
  "within each UFD are unique. To create a ﬁle for a user, the operating system\nsearches only that user’s UFD to ascertain whether another ﬁle of that name\nexists. To delete a ﬁle, the operating system conﬁnes its search to the local UFD;\nthus, it cannot accidentally delete another user’s ﬁle that has the same name.\ncat\nbo\na\ntest\nx\ndata\na\na\nuser 1 user 2 user 3 user 4\ndata\na\ntest\nuser file\ndirectory\nmaster file\ndirectory\nFigure 11.10\nTwo-level directory structure. 520\nChapter 11\nFile-System Interface\nThe user directories themselves must be created and deleted as necessary.\nA special system program is run with the appropriate user name and account\ninformation. The program creates a new UFD and adds an entry for it to the MFD.",
  "The execution of this program might be restricted to system administrators. The\nallocation of disk space for user directories can be handled with the techniques\ndiscussed in Chapter 12 for ﬁles themselves.\nAlthough the two-level directory structure solves the name-collision prob-\nlem, it still has disadvantages. This structure effectively isolates one user from\nanother. Isolation is an advantage when the users are completely independent\nbut is a disadvantage when the users want to cooperate on some task and to\naccess one another’s ﬁles. Some systems simply do not allow local user ﬁles to\nbe accessed by other users.\nIf access is to be permitted, one user must have the ability to name a ﬁle\nin another user’s directory. To name a particular ﬁle uniquely in a two-level",
  "directory, we must give both the user name and the ﬁle name. A two-level\ndirectory can be thought of as a tree, or an inverted tree, of height 2. The root\nof the tree is the MFD. Its direct descendants are the UFDs. The descendants of\nthe UFDs are the ﬁles themselves. The ﬁles are the leaves of the tree. Specifying\na user name and a ﬁle name deﬁnes a path in the tree from the root (the MFD)\nto a leaf (the speciﬁed ﬁle). Thus, a user name and a ﬁle name deﬁne a path\nname. Every ﬁle in the system has a path name. To name a ﬁle uniquely, a user\nmust know the path name of the ﬁle desired.\nFor example, if user A wishes to access her own test ﬁle named test.txt,\nshe can simply refer to test.txt. To access the ﬁle named test.txt of",
  "user B (with directory-entry name userb), however, she might have to refer\nto /userb/test.txt. Every system has its own syntax for naming ﬁles in\ndirectories other than the user’s own.\nAdditional syntax is needed to specify the volume of a ﬁle. For instance,\nin Windows a volume is speciﬁed by a letter followed by a colon. Thus,\na ﬁle speciﬁcation might be C:\\userb\\test. Some systems go even fur-\nther and separate the volume, directory name, and ﬁle name parts of the\nspeciﬁcation. In VMS, for instance, the ﬁle login.com might be speciﬁed as:\nu:[sst.jdeck]login.com;1, where u is the name of the volume, sst is the\nname of the directory, jdeck is the name of the subdirectory, and 1 is the\nversion number. Other systems—such as UNIX and Linux—simply treat the",
  "volume name as part of the directory name. The ﬁrst name given is that of the\nvolume, and the rest is the directory and ﬁle. For instance, /u/pbg/test might\nspecify volume u, directory pbg, and ﬁle test.\nA special instance of this situation occurs with the system ﬁles. Programs\nprovided as part of the system—loaders, assemblers, compilers, utility rou-\ntines, libraries, and so on—are generally deﬁned as ﬁles. When the appropriate\ncommands are given to the operating system, these ﬁles are read by the loader\nand executed. Many command interpreters simply treat such a command as\nthe name of a ﬁle to load and execute. In the directory system as we deﬁned it\nabove, this ﬁle name would be searched for in the current UFD. One solution",
  "would be to copy the system ﬁles into each UFD. However, copying all the\nsystem ﬁles would waste an enormous amount of space. (If the system ﬁles\nrequire 5 MB, then supporting 12 users would require 5 × 12 = 60 MB just for\ncopies of the system ﬁles.) 11.3\nDirectory and Disk Structure\n521\nThe standard solution is to complicate the search procedure slightly. A\nspecial user directory is deﬁned to contain the system ﬁles (for example, user\n0). Whenever a ﬁle name is given to be loaded, the operating system ﬁrst\nsearches the local UFD. If the ﬁle is found, it is used. If it is not found, the system\nautomatically searches the special user directory that contains the system ﬁles.\nThe sequence of directories searched when a ﬁle is named is called the search",
  "path. The search path can be extended to contain an unlimited list of directories\nto search when a command name is given. This method is the one most used\nin UNIX and Windows. Systems can also be designed so that each user has his\nown search path.\n11.3.5\nTree-Structured Directories\nOnce we have seen how to view a two-level directory as a two-level tree,\nthe natural generalization is to extend the directory structure to a tree of\narbitrary height (Figure 11.11). This generalization allows users to create their\nown subdirectories and to organize their ﬁles accordingly. A tree is the most\ncommon directory structure. The tree has a root directory, and every ﬁle in the\nsystem has a unique path name.\nA directory (or subdirectory) contains a set of ﬁles or subdirectories. A",
  "directory is simply another ﬁle, but it is treated in a special way. All directories\nhave the same internal format. One bit in each directory entry deﬁnes the entry\nas a ﬁle (0) or as a subdirectory (1). Special system calls are used to create and\ndelete directories.\nIn normal use, each process has a current directory. The current directory\nshould contain most of the ﬁles that are of current interest to the process.\nWhen reference is made to a ﬁle, the current directory is searched. If a ﬁle\nis needed that is not in the current directory, then the user usually must\nlist\nobj\nspell\nfind\ncount\nhex\nreorder\nstat\nmail\ndist\nroot\nspell\nbin\nprograms\np\ne\nmail\nreorder\nlist\nfind\nprog\ncopy\nprt\nexp\nlast\nfirst\nhex\ncount\nall\nFigure 11.11\nTree-structured directory structure. 522\nChapter 11",
  "Chapter 11\nFile-System Interface\neither specify a path name or change the current directory to be the directory\nholding that ﬁle. To change directories, a system call is provided that takes a\ndirectory name as a parameter and uses it to redeﬁne the current directory.\nThus, the user can change her current directory whenever she wants. From one\nchange directory() system call to the next, all open() system calls search\nthe current directory for the speciﬁed ﬁle. Note that the search path may or\nmay not contain a special entry that stands for “the current directory.”\nThe initial current directory of a user’s login shell is designated when\nthe user job starts or the user logs in. The operating system searches the\naccounting ﬁle (or some other predeﬁned location) to ﬁnd an entry for this",
  "user (for accounting purposes). In the accounting ﬁle is a pointer to (or the\nname of) the user’s initial directory. This pointer is copied to a local variable\nfor this user that speciﬁes the user’s initial current directory. From that shell,\nother processes can be spawned. The current directory of any subprocess is\nusually the current directory of the parent when it was spawned.\nPath names can be of two types: absolute and relative. An absolute path\nname begins at the root and follows a path down to the speciﬁed ﬁle, giving\nthe directory names on the path. A relative path name deﬁnes a path from the\ncurrent directory. For example, in the tree-structured ﬁle system of Figure\n11.11, if the current directory is root/spell/mail, then the relative path",
  "name prt/first refers to the same ﬁle as does the absolute path name\nroot/spell/mail/prt/first.\nAllowing a user to deﬁne her own subdirectories permits her to impose\na structure on her ﬁles. This structure might result in separate directories for\nﬁles associated with different topics (for example, a subdirectory was created\nto hold the text of this book) or different forms of information (for example,\nthe directory programs may contain source programs; the directory bin may\nstore all the binaries).\nAn interesting policy decision in a tree-structured directory concerns how\nto handle the deletion of a directory. If a directory is empty, its entry in the\ndirectory that contains it can simply be deleted. However, suppose the directory",
  "to be deleted is not empty but contains several ﬁles or subdirectories. One of\ntwo approaches can be taken. Some systems will not delete a directory unless\nit is empty. Thus, to delete a directory, the user must ﬁrst delete all the ﬁles\nin that directory. If any subdirectories exist, this procedure must be applied\nrecursively to them, so that they can be deleted also. This approach can result\nin a substantial amount of work. An alternative approach, such as that taken\nby the UNIX rm command, is to provide an option: when a request is made\nto delete a directory, all that directory’s ﬁles and subdirectories are also to be\ndeleted. Either approach is fairly easy to implement; the choice is one of policy.\nThe latter policy is more convenient, but it is also more dangerous, because an",
  "entire directory structure can be removed with one command. If that command\nis issued in error, a large number of ﬁles and directories will need to be restored\n(assuming a backup exists).\nWith a tree-structured directory system, users can be allowed to access, in\naddition to their ﬁles, the ﬁles of other users. For example, user B can access a\nﬁle of user A by specifying its path names. User B can specify either an absolute\nor a relative path name. Alternatively, user B can change her current directory\nto be user A’s directory and access the ﬁle by its ﬁle names. 11.3\nDirectory and Disk Structure\n523\n11.3.6\nAcyclic-Graph Directories\nConsider two programmers who are working on a joint project. The ﬁles asso-\nciated with that project can be stored in a subdirectory, separating them from",
  "other projects and ﬁles of the two programmers. But since both programmers\nare equally responsible for the project, both want the subdirectory to be in their\nown directories. In this situation, the common subdirectory should be shared.\nA shared directory or ﬁle exists in the ﬁle system in two (or more) places at\nonce.\nA tree structure prohibits the sharing of ﬁles or directories. An acyclic graph\n—that is, a graph with no cycles—allows directories to share subdirectories\nand ﬁles (Figure 11.12). The same ﬁle or subdirectory may be in two different\ndirectories. The acyclic graph is a natural generalization of the tree-structured\ndirectory scheme.\nIt is important to note that a shared ﬁle (or directory) is not the same as two",
  "copies of the ﬁle. With two copies, each programmer can view the copy rather\nthan the original, but if one programmer changes the ﬁle, the changes will not\nappear in the other’s copy. With a shared ﬁle, only one actual ﬁle exists, so any\nchanges made by one person are immediately visible to the other. Sharing is\nparticularly important for subdirectories; a new ﬁle created by one person will\nautomatically appear in all the shared subdirectories.\nWhen people are working as a team, all the ﬁles they want to share can be\nput into one directory. The UFD of each team member will contain this directory\nof shared ﬁles as a subdirectory. Even in the case of a single user, the user’s ﬁle\norganization may require that some ﬁle be placed in different subdirectories.",
  "For example, a program written for a particular project should be both in the\ndirectory of all programs and in the directory for that project.\nShared ﬁles and subdirectories can be implemented in several ways. A\ncommon way, exempliﬁed by many of the UNIX systems, is to create a new\ndirectory entry called a link. A link is effectively a pointer to another ﬁle\nlist\nall\nw\ncount words\nlist\nlist\nrade\nw7\ncount\nroot\ndict\nspell\nFigure 11.12\nAcyclic-graph directory structure. 524\nChapter 11\nFile-System Interface\nor subdirectory. For example, a link may be implemented as an absolute or a\nrelative path name. When a reference to a ﬁle is made, we search the directory. If\nthe directory entry is marked as a link, then the name of the real ﬁle is included",
  "in the link information. We resolve the link by using that path name to locate\nthe real ﬁle. Links are easily identiﬁed by their format in the directory entry\n(or by having a special type on systems that support types) and are effectively\nindirect pointers. The operating system ignores these links when traversing\ndirectory trees to preserve the acyclic structure of the system.\nAnother common approach to implementing shared ﬁles is simply to\nduplicate all information about them in both sharing directories. Thus, both\nentries are identical and equal. Consider the difference between this approach\nand the creation of a link. The link is clearly different from the original directory\nentry; thus, the two are not equal. Duplicate directory entries, however, make",
  "the original and the copy indistinguishable. A major problem with duplicate\ndirectory entries is maintaining consistency when a ﬁle is modiﬁed.\nAn acyclic-graph directory structure is more ﬂexible than a simple tree\nstructure, but it is also more complex. Several problems must be considered\ncarefully. A ﬁle may now have multiple absolute path names. Consequently,\ndistinct ﬁle names may refer to the same ﬁle. This situation is similar to the\naliasing problem for programming languages. If we are trying to traverse the\nentire ﬁle system—to ﬁnd a ﬁle, to accumulate statistics on all ﬁles, or to copy\nall ﬁles to backup storage—this problem becomes signiﬁcant, since we do not\nwant to traverse shared structures more than once.\nAnother problem involves deletion. When can the space allocated to a",
  "shared ﬁle be deallocated and reused? One possibility is to remove the ﬁle\nwhenever anyone deletes it, but this action may leave dangling pointers to the\nnow-nonexistent ﬁle. Worse, if the remaining ﬁle pointers contain actual disk\naddresses, and the space is subsequently reused for other ﬁles, these dangling\npointers may point into the middle of other ﬁles.\nIn a system where sharing is implemented by symbolic links, this situation\nis somewhat easier to handle. The deletion of a link need not affect the original\nﬁle; only the link is removed. If the ﬁle entry itself is deleted, the space for\nthe ﬁle is deallocated, leaving the links dangling. We can search for these links\nand remove them as well, but unless a list of the associated links is kept with",
  "each ﬁle, this search can be expensive. Alternatively, we can leave the links\nuntil an attempt is made to use them. At that time, we can determine that the\nﬁle of the name given by the link does not exist and can fail to resolve the\nlink name; the access is treated just as with any other illegal ﬁle name. (In this\ncase, the system designer should consider carefully what to do when a ﬁle is\ndeleted and another ﬁle of the same name is created, before a symbolic link to\nthe original ﬁle is used.) In the case of UNIX, symbolic links are left when a ﬁle\nis deleted, and it is up to the user to realize that the original ﬁle is gone or has\nbeen replaced. Microsoft Windows uses the same approach.\nAnother approach to deletion is to preserve the ﬁle until all references to",
  "it are deleted. To implement this approach, we must have some mechanism\nfor determining that the last reference to the ﬁle has been deleted. We could\nkeep a list of all references to a ﬁle (directory entries or symbolic links). When\na link or a copy of the directory entry is established, a new entry is added to\nthe ﬁle-reference list. When a link or directory entry is deleted, we remove its\nentry on the list. The ﬁle is deleted when its ﬁle-reference list is empty. 11.3\nDirectory and Disk Structure\n525\nThe trouble with this approach is the variable and potentially large size\nof the ﬁle-reference list. However, we really do not need to keep the entire\nlist—we need to keep only a count of the number of references. Adding a",
  "new link or directory entry increments the reference count. Deleting a link\nor entry decrements the count. When the count is 0, the ﬁle can be deleted;\nthere are no remaining references to it. The UNIX operating system uses this\napproach for nonsymbolic links (or hard links), keeping a reference count in the\nﬁle information block (or inode; see Section A.7.2). By effectively prohibiting\nmultiple references to directories, we maintain an acyclic-graph structure.\nTo avoid problems such as the ones just discussed, some systems simply\ndo not allow shared directories or links.\n11.3.7\nGeneral Graph Directory\nA serious problem with using an acyclic-graph structure is ensuring that there\nare no cycles. If we start with a two-level directory and allow users to create",
  "subdirectories, a tree-structured directory results. It should be fairly easy to see\nthat simply adding new ﬁles and subdirectories to an existing tree-structured\ndirectory preserves the tree-structured nature. However, when we add links,\nthe tree structure is destroyed, resulting in a simple graph structure (Figure\n11.13).\nThe primary advantage of an acyclic graph is the relative simplicity of the\nalgorithms to traverse the graph and to determine when there are no more\nreferences to a ﬁle. We want to avoid traversing shared sections of an acyclic\ngraph twice, mainly for performance reasons. If we have just searched a major\nshared subdirectory for a particular ﬁle without ﬁnding it, we want to avoid\nsearching that subdirectory again; the second search would be a waste of time.",
  "If cycles are allowed to exist in the directory, we likewise want to\navoid searching any component twice, for reasons of correctness as well as\nperformance. A poorly designed algorithm might result in an inﬁnite loop\ncontinually searching through the cycle and never terminating. One solution\ntext\nmail\navi\ncount\nunhex\nhex\ncount book\nbook\nmail unhex\nhyp\nroot\navi\ntc\njim\nFigure 11.13\nGeneral graph directory. 526\nChapter 11\nFile-System Interface\nis to limit arbitrarily the number of directories that will be accessed during a\nsearch.\nA similar problem exists when we are trying to determine when a ﬁle\ncan be deleted. With acyclic-graph directory structures, a value of 0 in the\nreference count means that there are no more references to the ﬁle or directory,",
  "and the ﬁle can be deleted. However, when cycles exist, the reference count\nmay not be 0 even when it is no longer possible to refer to a directory or ﬁle.\nThis anomaly results from the possibility of self-referencing (or a cycle) in the\ndirectory structure. In this case, we generally need to use a garbage collection\nscheme to determine when the last reference has been deleted and the disk\nspace can be reallocated. Garbage collection involves traversing the entire ﬁle\nsystem, marking everything that can be accessed. Then, a second pass collects\neverything that is not marked onto a list of free space. (A similar marking\nprocedure can be used to ensure that a traversal or search will cover everything\nin the ﬁle system once and only once.) Garbage collection for a disk-based ﬁle",
  "system, however, is extremely time consuming and is thus seldom attempted.\nGarbage collection is necessary only because of possible cycles in the graph.\nThus, an acyclic-graph structure is much easier to work with. The difﬁculty\nis to avoid cycles as new links are added to the structure. How do we know\nwhen a new link will complete a cycle? There are algorithms to detect cycles\nin graphs; however, they are computationally expensive, especially when the\ngraph is on disk storage. A simpler algorithm in the special case of directories\nand links is to bypass links during directory traversal. Cycles are avoided, and\nno extra overhead is incurred.\n11.4\nFile-System Mounting\nJust as a ﬁle must be opened before it is used, a ﬁle system must be mounted",
  "before it can be available to processes on the system. More speciﬁcally, the\ndirectory structure may be built out of multiple volumes, which must be\nmounted to make them available within the ﬁle-system name space.\nThe mount procedure is straightforward. The operating system is given the\nname of the device and the mount point—the location within the ﬁle structure\nwhere the ﬁle system is to be attached. Some operating systems require that a\nﬁle system type be provided, while others inspect the structures of the device\nand determine the type of ﬁle system. Typically, a mount point is an empty\ndirectory. For instance, on a UNIX system, a ﬁle system containing a user’s home\ndirectories might be mounted as /home; then, to access the directory structure",
  "within that ﬁle system, we could precede the directory names with /home, as\nin /home/jane. Mounting that ﬁle system under /users would result in the\npath name /users/jane, which we could use to reach the same directory.\nNext, the operating system veriﬁes that the device contains a valid ﬁle\nsystem. It does so by asking the device driver to read the device directory\nand verifying that the directory has the expected format. Finally, the operating\nsystem notes in its directory structure that a ﬁle system is mounted at the\nspeciﬁed mount point. This scheme enables the operating system to traverse\nits directory structure, switching among ﬁle systems, and even ﬁle systems of\nvarying types, as appropriate. 11.4\nFile-System Mounting\n527\nusers\n/\nbill\nfred\nhelp\nsue\njane\nprog\ndoc\n(a)\n(b)",
  "users\n/\nbill\nfred\nhelp\nsue\njane\nprog\ndoc\n(a)\n(b)\nFigure 11.14\nFile system. (a) Existing system. (b) Unmounted volume.\nTo illustrate ﬁle mounting, consider the ﬁle system depicted in Figure\n11.14, where the triangles represent subtrees of directories that are of interest.\nFigure 11.14(a) shows an existing ﬁle system, while Figure 11.14(b) shows an\nunmounted volume residing on /device/dsk. At this point, only the ﬁles\non the existing ﬁle system can be accessed. Figure 11.15 shows the effects of\nmounting the volume residing on /device/dsk over /users. If the volume is\nunmounted, the ﬁle system is restored to the situation depicted in Figure 11.14.\nSystems impose semantics to clarify functionality. For example, a system",
  "may disallow a mount over a directory that contains ﬁles; or it may make the\nmounted ﬁle system available at that directory and obscure the directory’s\nexisting ﬁles until the ﬁle system is unmounted, terminating the use of the ﬁle\nsystem and allowing access to the original ﬁles in that directory. As another\nexample, a system may allow the same ﬁle system to be mounted repeatedly,\nat different mount points; or it may only allow one mount per ﬁle system.\n/\nusers\nsue\njane\nprog\ndoc\nFigure 11.15\nMount point. 528\nChapter 11\nFile-System Interface\nConsider the actions of the Mac OS X operating system. Whenever the\nsystem encounters a disk for the ﬁrst time (either at boot time or while the\nsystem is running), the Mac OS X operating system searches for a ﬁle system",
  "on the device. If it ﬁnds one, it automatically mounts the ﬁle system under\nthe /Volumes directory, adding a folder icon labeled with the name of the ﬁle\nsystem (as stored in the device directory). The user is then able to click on the\nicon and thus display the newly mounted ﬁle system.\nThe Microsoft Windowsfamilyofoperatingsystemsmaintainsanextended\ntwo-level directory structure, with devices and volumes assigned drive letters.\nVolumes have a general graph directory structure associated with the drive let-\nter. The path to a speciﬁc ﬁle takes the form of drive-letter:\\path\\to\\file.\nThe more recent versions of Windows allow a ﬁle system to be mounted\nanywhere in the directory tree, just as UNIX does. Windows operating systems",
  "automatically discover all devices and mount all located ﬁle systems at boot\ntime. In some systems, like UNIX, the mount commands are explicit. A system\nconﬁguration ﬁle contains a list of devices and mount points for automatic\nmounting at boot time, but other mounts may be executed manually.\nIssues concerning ﬁle system mounting are further discussed in Section\n12.2.2 and in Section A.7.5.\n11.5\nFile Sharing\nIn the previous sections, we explored the motivation for ﬁle sharing and some of\nthe difﬁculties involved in allowing users to share ﬁles. Such ﬁle sharing is very\ndesirable for users who want to collaborate and to reduce the effort required\nto achieve a computing goal. Therefore, user-oriented operating systems must",
  "accommodate the need to share ﬁles in spite of the inherent difﬁculties.\nIn this section, we examine more aspects of ﬁle sharing. We begin by\ndiscussing general issues that arise when multiple users share ﬁles. Once\nmultiple users are allowed to share ﬁles, the challenge is to extend sharing to\nmultiple ﬁle systems, including remote ﬁle systems; we discuss that challenge\nas well. Finally, we consider what to do about conﬂicting actions occurring on\nshared ﬁles. For instance, if multiple users are writing to a ﬁle, should all the\nwrites be allowed to occur, or should the operating system protect the users’\nactions from one another?\n11.5.1\nMultiple Users\nWhen an operating system accommodates multiple users, the issues of ﬁle",
  "sharing, ﬁle naming, and ﬁle protection become preeminent. Given a directory\nstructure that allows ﬁles to be shared by users, the system must mediate the\nﬁle sharing. The system can either allow a user to access the ﬁles of other users\nby default or require that a user speciﬁcally grant access to the ﬁles. These are\nthe issues of access control and protection, which are covered in Section 11.6.\nTo implement sharing and protection, the system must maintain more\nﬁle and directory attributes than are needed on a single-user system. Although\nmany approaches have been taken to meet this requirement, most systems have\nevolved to use the concepts of ﬁle (or directory) owner (or user) and group.\nThe owner is the user who can change attributes and grant access and who has 11.5\nFile Sharing\n529",
  "File Sharing\n529\nthe most control over the ﬁle. The group attribute deﬁnes a subset of users who\ncan share access to the ﬁle. For example, the owner of a ﬁle on a UNIX system\ncan issue all operations on a ﬁle, while members of the ﬁle’s group can execute\none subset of those operations, and all other users can execute another subset\nof operations. Exactly which operations can be executed by group members\nand other users is deﬁnable by the ﬁle’s owner. More details on permission\nattributes are included in the next section.\nThe owner and group IDs of a given ﬁle (or directory) are stored with the\nother ﬁle attributes. When a user requests an operation on a ﬁle, the user ID can\nbe compared with the owner attribute to determine if the requesting user is the",
  "owner of the ﬁle. Likewise, the group IDs can be compared. The result indicates\nwhich permissions are applicable. The system then applies those permissions\nto the requested operation and allows or denies it.\nMany systems have multiple local ﬁle systems, including volumes of a\nsingle disk or multiple volumes on multiple attached disks. In these cases,\nthe ID checking and permission matching are straightforward, once the ﬁle\nsystems are mounted.\n11.5.2\nRemote File Systems\nWith the advent of networks (Chapter 17), communication among remote\ncomputers became possible. Networking allows the sharing ofresources spread\nacross a campus or even around the world. One obvious resource to share is\ndata in the form of ﬁles.\nThrough the evolution of network and ﬁle technology, remote ﬁle-sharing",
  "methods have changed. The ﬁrst implemented method involves manually\ntransferring ﬁles between machines via programs like ftp. The second major\nmethod uses a distributed ﬁle system (DFS) in which remote directories are\nvisible from a local machine. In some ways, the third method, the World Wide\nWeb, is a reversion to the ﬁrst. A browser is needed to gain access to the\nremote ﬁles, and separate operations (essentially a wrapper for ftp) are used\nto transfer ﬁles. Increasingly, cloud computing (Section 1.11.7) is being used\nfor ﬁle sharing as well.\nftp is used for both anonymous and authenticated access. Anonymous\naccess allows a user to transfer ﬁles without having an account on the remote\nsystem. The World Wide Web uses anonymous ﬁle exchange almost exclusively.",
  "DFS involves a much tighter integration between the machine that is accessing\nthe remote ﬁles and the machine providing the ﬁles. This integration adds\ncomplexity, as we describe in this section.\n11.5.2.1\nThe Client–Server Model\nRemote ﬁle systems allow a computer to mount one or more ﬁle systems from\none or more remote machines. In this case, the machine containing the ﬁles\nis the server, and the machine seeking access to the ﬁles is the client. The\nclient–server relationship is common with networked machines. Generally,\nthe server declares that a resource is available to clients and speciﬁes exactly\nwhich resource (in this case, which ﬁles) and exactly which clients. A server\ncan serve multiple clients, and a client can use multiple servers, depending on",
  "the implementation details of a given client–server facility. 530\nChapter 11\nFile-System Interface\nThe server usually speciﬁes the available ﬁles on a volume or directory\nlevel. Client identiﬁcation is more difﬁcult. A client can be speciﬁed by\na network name or other identiﬁer, such as an IP address, but these can\nbe spoofed, or imitated. As a result of spooﬁng, an unauthorized client\ncould be allowed access to the server. More secure solutions include secure\nauthentication of the client via encrypted keys. Unfortunately, with security\ncome many challenges, including ensuring compatibility of the client and\nserver (they must use the same encryption algorithms) and security of key\nexchanges (intercepted keys could again allow unauthorized access). Because",
  "of the difﬁculty of solving these problems, unsecure authentication methods\nare most commonly used.\nIn the case of UNIX and its network ﬁle system (NFS), authentication takes\nplace via the client networking information, by default. In this scheme, the\nuser’s IDs on the client and server must match. If they do not, the server will\nbe unable to determine access rights to ﬁles. Consider the example of a user\nwho has an ID of 1000 on the client and 2000 on the server. A request from\nthe client to the server for a speciﬁc ﬁle will not be handled appropriately, as\nthe server will determine if user 1000 has access to the ﬁle rather than basing\nthe determination on the real user ID of 2000. Access is thus granted or denied",
  "based on incorrect authentication information. The server must trust the client\nto present the correct user ID. Note that the NFS protocols allow many-to-many\nrelationships. That is, many servers can provide ﬁles to many clients. In fact,\na given machine can be both a server to some NFS clients and a client of other\nNFS servers.\nOnce the remote ﬁle system is mounted, ﬁle operation requests are sent\non behalf of the user across the network to the server via the DFS protocol.\nTypically, a ﬁle-open request is sent along with the ID of the requesting user.\nThe server then applies the standard access checks to determine if the user has\ncredentials to access the ﬁle in the mode requested. The request is either allowed",
  "or denied. If it is allowed, a ﬁle handle is returned to the client application,\nand the application then can perform read, write, and other operations on the\nﬁle. The client closes the ﬁle when access is completed. The operating system\nmay apply semantics similar to those for a local ﬁle-system mount or may use\ndifferent semantics.\n11.5.2.2\nDistributed Information Systems\nTo make client–server systems easier to manage, distributed information\nsystems, also known as distributed naming services, provide uniﬁed access\nto the information needed for remote computing. The domain name system\n(DNS) provides host-name-to-network-address translations for the entire Inter-\nnet. Before DNS became widespread, ﬁles containing the same information",
  "were sent via e-mail or ftp between all networked hosts. Obviously, this\nmethodology was not scalable! DNS is further discussed in Section 17.4.1.\nOther distributed information systems provide user name/password/user\nID/group ID space for a distributed facility. UNIX systems have employed a\nwide variety of distributed information methods. Sun Microsystems (now\npart of Oracle Corporation) introduced yellow pages (since renamed network\ninformation service, or NIS), and most of the industry adopted its use. It\ncentralizes storage of user names, host names, printer information, and the like. 11.5\nFile Sharing\n531\nUnfortunately, it uses unsecure authentication methods, including sending\nuser passwords unencrypted (in clear text) and identifying hosts by IP address.",
  "Sun’s NIS+ was a much more secure replacement for NIS but was much more\ncomplicated and was not widely adopted.\nIn the case of Microsoft’s common Internet ﬁle system (CIFS), network\ninformation is used in conjunction with user authentication (user name and\npassword) to create a network login that the server uses to decide whether\nto allow or deny access to a requested ﬁle system. For this authentication to\nbe valid, the user names must match from machine to machine (as with NFS).\nMicrosoft uses active directory as a distributed naming structure to provide a\nsingle name space for users. Once established, the distributed naming facility\nis used by all clients and servers to authenticate users.\nThe industry is moving toward use of the lightweight directory-access",
  "protocol (LDAP) as a secure distributed naming mechanism. In fact, active\ndirectory is based on LDAP. Oracle Solaris and most other major operating\nsystems include LDAP and allow it to be employed for user authentication as\nwell as system-wide retrieval of information, such as availability of printers.\nConceivably, one distributed LDAP directory could be used by an organization\nto store all user and resource information for all the organization’s computers.\nThe result would be secure single sign-on for users, who would enter\ntheir authentication information once for access to all computers within the\norganization. It would also ease system-administration efforts by combining,\nin one location, information that is currently scattered in various ﬁles on each",
  "system or in different distributed information services.\n11.5.2.3\nFailure Modes\nLocal ﬁle systems can fail for a variety of reasons, including failure of the\ndisk containing the ﬁle system, corruption of the directory structure or other\ndisk-management information (collectively called metadata), disk-controller\nfailure, cable failure, and host-adapter failure. User or system-administrator\nfailure can also cause ﬁles to be lost or entire directories or volumes to be\ndeleted. Many of these failures will cause a host to crash and an error condition\nto be displayed, and human intervention will be required to repair the damage.\nRemote ﬁle systems have even more failure modes. Because of the\ncomplexity of network systems and the required interactions between remote",
  "machines, many more problems can interfere with the proper operation of\nremote ﬁle systems. In the case of networks, the network can be interrupted\nbetween two hosts. Such interruptions can result from hardware failure, poor\nhardware conﬁguration, or networking implementation issues. Although some\nnetworks have built-in resiliency, including multiple paths between hosts,\nmany do not. Any single failure can thus interrupt the ﬂow of DFS commands.\nConsider a client in the midst of using a remote ﬁle system. It has ﬁles open\nfrom the remote host; among other activities, it may be performing directory\nlookups to open ﬁles, reading or writing data to ﬁles, and closing ﬁles. Now\nconsider a partitioning of the network, a crash of the server, or even a scheduled",
  "shutdown of the server. Suddenly, the remote ﬁle system is no longer reachable.\nThis scenario is rather common, so it would not be appropriate for the client\nsystem to act as it would if a local ﬁle system were lost. Rather, the system can\neither terminate all operations to the lost server or delay operations until the 532\nChapter 11\nFile-System Interface\nserver is again reachable. These failure semantics are deﬁned and implemented\nas part of the remote-ﬁle-system protocol. Termination of all operations can\nresult in users’ losing data—and patience. Thus, most DFS protocols either\nenforce or allow delaying of ﬁle-system operations to remote hosts, with the\nhope that the remote host will become available again.\nTo implement this kind of recovery from failure, some kind of state",
  "information may be maintained on both the client and the server. If both server\nand client maintain knowledge of their current activities and open ﬁles, then\nthey can seamlessly recover from a failure. In the situation where the server\ncrashes but must recognize that it has remotely mounted exported ﬁle systems\nand opened ﬁles, NFS takes a simple approach, implementing a stateless DFS.\nIn essence, it assumes that a client request for a ﬁle read or write would not\nhave occurred unless the ﬁle system had been remotely mounted and the ﬁle\nhad been previously open. The NFS protocol carries all the information needed\nto locate the appropriate ﬁle and perform the requested operation. Similarly,\nit does not track which clients have the exported volumes mounted, again",
  "assuming that if a request comes in, it must be legitimate. While this stateless\napproach makes NFS resilient and rather easy to implement, it also makes it\nunsecure. For example, forged read or write requests could be allowed by an\nNFS server. These issues are addressed in the industry standard NFS Version\n4, in which NFS is made stateful to improve its security, performance, and\nfunctionality.\n11.5.3\nConsistency Semantics\nConsistency semantics represent an important criterion for evaluating any\nﬁle system that supports ﬁle sharing. These semantics specify how multiple\nusers of a system are to access a shared ﬁle simultaneously. In particular, they\nspecify when modiﬁcations of data by one user will be observable by other",
  "users. These semantics are typically implemented as code with the ﬁle system.\nConsistency semantics are directly related to the process synchronization\nalgorithms of Chapter 5. However, the complex algorithms of that chapter tend\nnot to be implemented in the case of ﬁle I/O because of the great latencies and\nslow transfer rates of disks and networks. For example, performing an atomic\ntransaction to a remote disk could involve several network communications,\nseveral disk reads and writes, or both. Systems that attempt such a full set of\nfunctionalities tend to perform poorly. A successful implementation of complex\nsharing semantics can be found in the Andrew ﬁle system.\nFor the following discussion, we assume that a series of ﬁle accesses (that",
  "is, reads and writes) attempted by a user to the same ﬁle is always enclosed\nbetween the open() and close() operations. The series of accesses between\nthe open() and close() operations makes up a ﬁle session. To illustrate the\nconcept, we sketch several prominent examples of consistency semantics.\n11.5.3.1\nUNIX Semantics\nThe UNIX ﬁle system (Chapter 17) uses the following consistency semantics:\n• Writes to an open ﬁle by a user are visible immediately to other users who\nhave this ﬁle open. 11.6\nProtection\n533\n• One mode of sharing allows users to share the pointer of current location\ninto the ﬁle. Thus, the advancing of the pointer by one user affects all\nsharing users. Here, a ﬁle has a single image that interleaves all accesses,\nregardless of their origin.",
  "regardless of their origin.\nIn the UNIX semantics, a ﬁle is associated with a single physical image that\nis accessed as an exclusive resource. Contention for this single image causes\ndelays in user processes.\n11.5.3.2\nSession Semantics\nThe Andrew ﬁle system (OpenAFS) uses the following consistency semantics:\n• Writes to an open ﬁle by a user are not visible immediately to other users\nthat have the same ﬁle open.\n• Once a ﬁle is closed, the changes made to it are visible only in sessions\nstarting later. Already open instances of the ﬁle do not reﬂect these changes.\nAccording to these semantics, a ﬁle may be associated temporarily with several\n(possibly different) images at the same time. Consequently, multiple users are",
  "allowed to perform both read and write accesses concurrently on their images\nof the ﬁle, without delay. Almost no constraints are enforced on scheduling\naccesses.\n11.5.3.3\nImmutable-Shared-Files Semantics\nA unique approach is that of immutable shared ﬁles. Once a ﬁle is declared\nas shared by its creator, it cannot be modiﬁed. An immutable ﬁle has two key\nproperties: its name may not be reused, and its contents may not be altered.\nThus, the name of an immutable ﬁle signiﬁes that the contents of the ﬁle are\nﬁxed. The implementation of these semantics in a distributed system (Chapter\n17) is simple, because the sharing is disciplined (read-only).\n11.6\nProtection\nWhen information is stored in a computer system, we want to keep it safe",
  "from physical damage (the issue of reliability) and improper access (the issue\nof protection).\nReliability is generally provided by duplicate copies of ﬁles. Many comput-\ners have systems programs that automatically (or through computer-operator\nintervention) copy disk ﬁles to tape at regular intervals (once per day or week\nor month) to maintain a copy should a ﬁle system be accidentally destroyed.\nFile systems can be damaged by hardware problems (such as errors in reading\nor writing), power surges or failures, head crashes, dirt, temperature extremes,\nand vandalism. Files may be deleted accidentally. Bugs in the ﬁle-system soft-\nware can also cause ﬁle contents to be lost. Reliability is covered in more detail\nin Chapter 10. 534\nChapter 11\nFile-System Interface",
  "Chapter 11\nFile-System Interface\nProtection can be provided in many ways. For a single-user laptop system,\nwe might provide protection by locking the computer in a desk drawer or ﬁle\ncabinet. In a larger multiuser system, however, other mechanisms are needed.\n11.6.1\nTypes of Access\nThe need to protect ﬁles is a direct result of the ability to access ﬁles. Systems\nthat do not permit access to the ﬁles of other users do not need protection. Thus,\nwe could provide complete protection by prohibiting access. Alternatively, we\ncould provide free access with no protection. Both approaches are too extreme\nfor general use. What is needed is controlled access.\nProtection mechanisms provide controlled access by limiting the types of",
  "ﬁle access that can be made. Access is permitted or denied depending on\nseveral factors, one of which is the type of access requested. Several different\ntypes of operations may be controlled:\n• Read. Read from the ﬁle.\n• Write. Write or rewrite the ﬁle.\n• Execute. Load the ﬁle into memory and execute it.\n• Append. Write new information at the end of the ﬁle.\n• Delete. Delete the ﬁle and free its space for possible reuse.\n• List. List the name and attributes of the ﬁle.\nOther operations, such as renaming, copying, and editing the ﬁle, may also\nbe controlled. For many systems, however, these higher-level functions may\nbe implemented by a system program that makes lower-level system calls.\nProtection is provided at only the lower level. For instance, copying a ﬁle may",
  "be implemented simply by a sequence of read requests. In this case, a user with\nread access can also cause the ﬁle to be copied, printed, and so on.\nMany protection mechanisms have been proposed. Each has advantages\nand disadvantages and must be appropriate for its intended application. A\nsmall computer system that is used by only a few members of a research group,\nfor example, may not need the same types of protection as a large corporate\ncomputer that is used for research, ﬁnance, and personnel operations. We\ndiscuss some approaches to protection in the following sections and present a\nmore complete treatment in Chapter 14.\n11.6.2\nAccess Control\nThe most common approach to the protection problem is to make access",
  "dependent on the identity of the user. Different users may need different types\nof access to a ﬁle or directory. The most general scheme to implement identity-\ndependent access is to associate with each ﬁle and directory an access-control\nlist (ACL) specifying user names and the types of access allowed for each user.\nWhen a user requests access to a particular ﬁle, the operating system checks\nthe access list associated with that ﬁle. If that user is listed for the requested\naccess, the access is allowed. Otherwise, a protection violation occurs, and the\nuser job is denied access to the ﬁle. 11.6\nProtection\n535\nThis approach has the advantage of enabling complex access methodolo-\ngies. The main problem with access lists is their length. If we want to allow",
  "everyone to read a ﬁle, we must list all users with read access. This technique\nhas two undesirable consequences:\n• Constructing such a list may be a tedious and unrewarding task, especially\nif we do not know in advance the list of users in the system.\n• The directory entry, previously of ﬁxed size, now must be of variable size,\nresulting in more complicated space management.\nThese problems can be resolved by use of a condensed version of the access\nlist.\nTo condense the length of the access-control list, many systems recognize\nthree classiﬁcations of users in connection with each ﬁle:\n• Owner. The user who created the ﬁle is the owner.\n• Group. A set of users who are sharing the ﬁle and need similar access is a\ngroup, or work group.",
  "group, or work group.\n• Universe. All other users in the system constitute the universe.\nThe most common recent approach is to combine access-control lists with\nthe more general (and easier to implement) owner, group, and universe access-\ncontrol scheme just described. For example, Solaris uses the three categories\nof access by default but allows access-control lists to be added to speciﬁc ﬁles\nand directories when more ﬁne-grained access control is desired.\nTo illustrate, consider a person, Sara, who is writing a new book. She has\nhired three graduate students (Jim, Dawn, and Jill) to help with the project. The\ntext of the book is kept in a ﬁle named book.tex. The protection associated\nwith this ﬁle is as follows:\n• Sara should be able to invoke all operations on the ﬁle.",
  "• Jim, Dawn, and Jill should be able only to read and write the ﬁle; they\nshould not be allowed to delete the ﬁle.\n• All other users should be able to read, but not write, the ﬁle. (Sara is\ninterested in letting as many people as possible read the text so that she\ncan obtain feedback.)\nTo achieve such protection, we must create a new group—say, text—\nwith members Jim, Dawn, and Jill. The name of the group, text, must then\nbe associated with the ﬁle book.tex, and the access rights must be set in\naccordance with the policy we have outlined.\nNow consider a visitor to whom Sara would like to grant temporary access\nto Chapter 1. The visitor cannot be added to the text group because that would\ngive him access to all chapters. Because a ﬁle can be in only one group, Sara",
  "cannot add another group to Chapter 1. With the addition of access-control-list\nfunctionality, though, the visitor can be added to the access control list of\nChapter 1. 536\nChapter 11\nFile-System Interface\nPERMISSIONS IN A UNIX SYSTEM\nIn the UNIX system, directory protection and ﬁle protection are handled\nsimilarly. Associated with each subdirectory are three ﬁelds—owner, group,\nand universe—each consisting of the three bits rwx. Thus, a user can list\nthe content of a subdirectory only if the r bit is set in the appropriate ﬁeld.\nSimilarly, a user can change his current directory to another current directory\n(say, foo) only if the x bit associated with the foo subdirectory is set in the\nappropriate ﬁeld.\nA sample directory listing from a UNIX environment is shown in below:\n-rw-rw-r--",
  "-rw-rw-r--\ndrwx------\ndrwxrwxr-x\ndrwxrwx---\n-rw-r--r--\n-rwxr-xr-x\ndrwx--x--x\ndrwx------\ndrwxrwxrwx\n1 pbg\n5 pbg\n2 pbg \n2 jwg \n1 pbg \n1 pbg \n4 tag \n3 pbg \n3 pbg\nstaff\nstaff\nstaff\nstudent\nstaff\nstaff\nfaculty\nstaff\nstaff\nintro.ps\nprivate/\ndoc/\nstudent-proj/\nprogram.c\nprogram\nlib/\nmail/\ntest/\nSep 3 08:30\nJul 8 09.33\nJul 8 09:35\nAug 3 14:13\nFeb 24 2012\nFeb 24 2012\nJul 31 10:31\nAug 29 06:52\nJul 8 09:35\n31200\n512\n512\n512 \n9423\n20471 \n512 \n1024 \n512\nThe ﬁrst ﬁeld describes the protection of the ﬁle or directory. A d as the ﬁrst\ncharacter indicates a subdirectory. Also shown are the number of links to the\nﬁle, the owner’s name, the group’s name, the size of the ﬁle in bytes, the date\nof last modiﬁcation, and ﬁnally the ﬁle’s name (with optional extension).",
  "For this scheme to work properly, permissions and access lists must be\ncontrolled tightly. This control can be accomplished in several ways. For\nexample, in the UNIX system, groups can be created and modiﬁed only by\nthe manager of the facility (or by any superuser). Thus, control is achieved\nthrough human interaction. Access lists are discussed further in Section 14.5.2.\nWith the more limited protection classiﬁcation, only three ﬁelds are needed\nto deﬁne protection. Often, each ﬁeld is a collection of bits, and each bit either\nallows or prevents the access associated with it. For example, the UNIX system\ndeﬁnes three ﬁelds of 3 bits each—rwx, where r controls read access, w controls\nwrite access, and x controls execution. A separate ﬁeld is kept for the ﬁle owner,",
  "for the ﬁle’s group, and for all other users. In this scheme, 9 bits per ﬁle are\nneeded to record protection information. Thus, for our example, the protection\nﬁelds for the ﬁle book.tex are as follows: for the owner Sara, all bits are set;\nfor the group text, the r and w bits are set; and for the universe, only the r bit\nis set.\nOne difﬁculty in combining approaches comes in the user interface. Users\nmust be able to tell when the optional ACL permissions are set on a ﬁle. In the\nSolaris example, a “+” is appended to the regular permissions, as in:\n19 -rw-r--r--+ 1 jim staff 130 May 25 22:13 file1\nA separate set of commands, setfacl and getfacl, is used to manage the\nACLs. 11.6\nProtection\n537\nFigure 11.16\nWindows 7 access-control list management.",
  "Windows 7 access-control list management.\nWindows users typically manage access-control lists via the GUI. Figure\n11.16 shows a ﬁle-permission window on Windows 7 NTFS ﬁle system. In this\nexample, user “guest” is speciﬁcally denied access to the ﬁle ListPanel.java.\nAnother difﬁculty is assigning precedence when permission and ACLs\nconﬂict. For example, if Joe is in a ﬁle’s group, which has read permission,\nbut the ﬁle has an ACL granting Joe read and write permission, should a write\nby Joe be granted or denied? Solaris gives ACLs precedence (as they are more\nﬁne-grained and are not assigned by default). This follows the general rule that\nspeciﬁcity should have priority.\n11.6.3\nOther Protection Approaches\nAnother approach to the protection problem is to associate a password with",
  "each ﬁle. Just as access to the computer system is often controlled by a 538\nChapter 11\nFile-System Interface\npassword, access to each ﬁle can be controlled in the same way. If the passwords\nare chosen randomly and changed often, this scheme may be effective in\nlimiting access to a ﬁle. The use of passwords has a few disadvantages,\nhowever. First, the number of passwords that a user needs to remember may\nbecome large, making the scheme impractical. Second, if only one password is\nused for all the ﬁles, then once it is discovered, all ﬁles are accessible; protection\nis on an all-or-none basis. Some systems allow a user to associate a password\nwith a subdirectory, rather than with an individual ﬁle, to address this problem.",
  "In a multilevel directory structure, we need to protect not only individual\nﬁles but also collections of ﬁles in subdirectories; that is, we need to provide\na mechanism for directory protection. The directory operations that must be\nprotected are somewhat different from the ﬁle operations. We want to control\nthe creation and deletion of ﬁles in a directory. In addition, we probably want\nto control whether a user can determine the existence of a ﬁle in a directory.\nSometimes, knowledge of the existence and name of a ﬁle is signiﬁcant in itself.\nThus, listing the contents of a directory must be a protected operation. Similarly,\nif a path name refers to a ﬁle in a directory, the user must be allowed access to\nboth the directory and the ﬁle. In systems where ﬁles may have numerous path",
  "names (such as acyclic and general graphs), a given user may have different\naccess rights to a particular ﬁle, depending on the path name used.\n11.7\nSummary\nA ﬁle is an abstract data type deﬁned and implemented by the operating\nsystem. It is a sequence of logical records. A logical record may be a byte, a line\n(of ﬁxed or variable length), or a more complex data item. The operating system\nmay speciﬁcally support various record types or may leave that support to the\napplication program.\nThe major task for the operating system is to map the logical ﬁle concept\nonto physical storage devices such as magnetic disk or tape. Since the physical\nrecord size of the device may not be the same as the logical record size, it may",
  "be necessary to order logical records into physical records. Again, this task may\nbe supported by the operating system or left for the application program.\nEach device in a ﬁle system keeps a volume table of contents or a device\ndirectory listing the location of the ﬁles on the device. In addition, it is useful\nto create directories to allow ﬁles to be organized. A single-level directory\nin a multiuser system causes naming problems, since each ﬁle must have a\nunique name. A two-level directory solves this problem by creating a separate\ndirectory for each user’s ﬁles. The directory lists the ﬁles by name and includes\nthe ﬁle’s location on the disk, length, type, owner, time of creation, time of last\nuse, and so on.\nThe natural generalization of a two-level directory is a tree-structured",
  "directory. A tree-structured directory allows a user to create subdirectories\nto organize ﬁles. Acyclic-graph directory structures enable users to share\nsubdirectories and ﬁles but complicate searching and deletion. A general graph\nstructure allows complete ﬂexibility in the sharing of ﬁles and directories but\nsometimes requires garbage collection to recover unused disk space.\nDisks are segmented into one or more volumes, each containing a ﬁle\nsystem or left “raw.” File systems may be mounted into the system’s naming Practice Exercises\n539\nstructures to make them available. The naming scheme varies by operating\nsystem. Once mounted, the ﬁles within the volume are available for use. File\nsystems may be unmounted to disable access or for maintenance.",
  "File sharing depends on the semantics provided by the system. Files may\nhave multiple readers, multiple writers, or limits on sharing. Distributed ﬁle\nsystems allow client hosts to mount volumes or directories from servers, as long\nas they can access each other across a network. Remote ﬁle systems present\nchallenges in reliability, performance, and security. Distributed information\nsystems maintain user, host, and access information so that clients and servers\ncan share state information to manage use and access.\nSince ﬁles are the main information-storage mechanism in most computer\nsystems, ﬁle protection is needed. Access to ﬁles can be controlled separately\nfor each type of access—read, write, execute, append, delete, list directory,",
  "and so on. File protection can be provided by access lists, passwords, or other\ntechniques.\nPractice Exercises\n11.1\nSome systems automatically delete all user ﬁles when a user logs off or\na job terminates, unless the user explicitly requests that they be kept.\nOther systems keep all ﬁles unless the user explicitly deletes them.\nDiscuss the relative merits of each approach.\n11.2\nWhy do some systems keep track of the type of a ﬁle, while others leave\nit to the user and others simply do not implement multiple ﬁle types?\nWhich system is “better”?\n11.3\nSimilarly, some systems support many types of structures for a ﬁle’s\ndata, while others simply support a stream of bytes. What are the\nadvantages and disadvantages of each approach?\n11.4",
  "11.4\nCould you simulate a multilevel directory structure with a single-level\ndirectory structure in which arbitrarily long names can be used? If your\nanswer is yes, explain how you can do so, and contrast this scheme with\nthe multilevel directory scheme. If your answer is no, explain what\nprevents your simulation’s success. How would your answer change\nif ﬁle names were limited to seven characters?\n11.5\nExplain the purpose of the open() and close() operations.\n11.6\nIn some systems, a subdirectory can be read and written by an\nauthorized user, just as ordinary ﬁles can be.\na.\nDescribe the protection problems that could arise.\nb.\nSuggest a scheme for dealing with each of these protection\nproblems.\n11.7\nConsider a system that supports 5,000 users. Suppose that you want to",
  "allow 4,990 of these users to be able to access one ﬁle.\na.\nHow would you specify this protection scheme in UNIX? 540\nChapter 11\nFile-System Interface\nb.\nCan you suggest another protection scheme that can be used more\neffectively for this purpose than the scheme provided by UNIX?\n11.8\nResearchers have suggested that, instead of having an access list\nassociated with each ﬁle (specifying which users can access the ﬁle,\nand how), we should have a user control list associated with each user\n(specifying which ﬁles a user can access, and how). Discuss the relative\nmerits of these two schemes.\nExercises\n11.9\nConsider a ﬁle system in which a ﬁle can be deleted and its disk space\nreclaimed while links to that ﬁle still exist. What problems may occur if",
  "a new ﬁle is created in the same storage area or with the same absolute\npath name? How can these problems be avoided?\n11.10\nThe open-ﬁle table is used to maintain information about ﬁles that are\ncurrently open. Should the operating system maintain a separate table\nfor each user or maintain just one table that contains references to ﬁles\nthat are currently being accessed by all users? If the same ﬁle is being\naccessed by two different programs or users, should there be separate\nentries in the open-ﬁle table? Explain.\n11.11\nWhat are the advantages and disadvantages of providing mandatory\nlocks instead of advisory locks whose use is left to users’ discretion?\n11.12\nProvide examples of applications that typically access ﬁles according\nto the following methods:\n• Sequential\n• Random\n11.13",
  "• Sequential\n• Random\n11.13\nSome systems automatically open a ﬁle when it is referenced for the ﬁrst\ntime and close the ﬁle when the job terminates. Discuss the advantages\nand disadvantages of this scheme compared with the more traditional\none, where the user has to open and close the ﬁle explicitly.\n11.14\nIf the operating system knew that a certain application was going\nto access ﬁle data in a sequential manner, how could it exploit this\ninformation to improve performance?\n11.15\nGive an example of an application that could beneﬁt from operating-\nsystem support for random access to indexed ﬁles.\n11.16\nDiscuss the advantages and disadvantages of supporting links to ﬁles\nthat cross mount points (that is, the ﬁle link refers to a ﬁle that is stored\nin a different volume).\n11.17",
  "in a different volume).\n11.17\nSome systems provide ﬁle sharing by maintaining a single copy of a\nﬁle. Other systems maintain several copies, one for each of the users\nsharing the ﬁle. Discuss the relative merits of each approach. Bibliography\n541\n11.18\nDiscuss the advantages and disadvantages of associating with remote\nﬁle systems (stored on ﬁle servers) a set of failure semantics different\nfrom that associated with local ﬁle systems.\n11.19\nWhat are the implications of supporting UNIX consistency semantics\nfor shared access to ﬁles stored on remote ﬁle systems?\nBibliographical Notes\nDatabase systems and their ﬁle structures are described in full in [Silberschatz\net al. (2010)].\nA multilevel directory structure was ﬁrst implemented on the MULTICS",
  "system ([Organick (1972)]). Most operating systems now implement multilevel\ndirectory structures. These include Linux ([Love (2010)]), Mac OS X ([Singh\n(2007)]), Solaris ([McDougall and Mauro (2007)]), and all versions of Windows\n([Russinovich and Solomon (2005)]).\nThe network ﬁle system (NFS), designed by Sun Microsystems, allows\ndirectory structures to be spread across networked computer systems. NFS\nVersion 4 is described in RFC3505 (http://www.ietf.org/rfc/rfc3530.txt). A gen-\neral discussion of Solaris ﬁle systems is found in the Sun System Administration\nGuide: Devices and File Systems (http://docs.sun.com/app/docs/doc/817-5093).\nDNS was ﬁrst proposed by [Su (1982)] and has gone through several\nrevisions since. LDAP, also known as X.509, is a derivative subset of the X.500",
  "distributed directory protocol. It was deﬁned by [Yeong et al. (1995)] and has\nbeen implemented on many operating systems.\nBibliography\n[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[McDougall and Mauro (2007)]\nR. McDougall and J. Mauro, Solaris Internals,\nSecond Edition, Prentice Hall (2007).\n[Organick (1972)]\nE. I. Organick, The Multics System: An Examination of Its\nStructure, MIT Press (1972).\n[Russinovich and Solomon (2005)]\nM. E. Russinovich and D. A. Solomon,\nMicrosoft Windows Internals, Fourth Edition, Microsoft Press (2005).\n[Silberschatz et al. (2010)]\nA. Silberschatz, H. F. Korth, and S. Sudarshan,\nDatabase System Concepts, Sixth Edition, McGraw-Hill (2010).\n[Singh (2007)]\nA. Singh, Mac OS X Internals: A Systems Approach, Addison-",
  "Wesley (2007).\n[Su (1982)]\nZ. Su, “A Distributed System for Internet Name Service”, Network\nWorking Group, Request for Comments: 830 (1982).\n[Yeong et al. (1995)]\nW. Yeong, T. Howes, and S. Kille, “Lightweight Directory\nAccess Protocol”, Network Working Group, Request for Comments: 1777 (1995).  12\nC H A P T E R\nFile-System\nImplementation\nAs we saw in Chapter 11, the ﬁle system provides the mechanism for on-line\nstorage and access to ﬁle contents, including data and programs. The ﬁle system\nresides permanently on secondary storage, which is designed to hold a large\namount of data permanently. This chapter is primarily concerned with issues\nsurrounding ﬁle storage and access on the most common secondary-storage\nmedium, the disk. We explore ways to structure ﬁle use, to allocate disk space,",
  "to recover freed space, to track the locations of data, and to interface other\nparts of the operating system to secondary storage. Performance issues are\nconsidered throughout the chapter.\nCHAPTER OBJECTIVES\n• To describe the details of implementing local ﬁle systems and directory\nstructures.\n• To describe the implementation of remote ﬁle systems.\n• To discuss block allocation and free-block algorithms and trade-offs.\n12.1\nFile-System Structure\nDisks provide most of the secondary storage on which ﬁle systems are\nmaintained. Two characteristics make them convenient for this purpose:\n1. A disk can be rewritten in place; it is possible to read a block from the\ndisk, modify the block, and write it back into the same place.",
  "2. A disk can access directly any block of information it contains. Thus, it is\nsimple to access any ﬁle either sequentially or randomly, and switching\nfrom one ﬁle to another requires only moving the read–write heads and\nwaiting for the disk to rotate.\nWe discuss disk structure in great detail in Chapter 10.\nTo improve I/O efﬁciency, I/O transfers between memory and disk are\nperformed in units of blocks. Each block has one or more sectors. Depending\n543 544\nChapter 12\nFile-System Implementation\non the disk drive, sector size varies from 32 bytes to 4,096 bytes; the usual size\nis 512 bytes.\nFile systems provide efﬁcient and convenient access to the disk by allowing\ndata to be stored, located, and retrieved easily. A ﬁle system poses two quite",
  "different design problems. The ﬁrst problem is deﬁning how the ﬁle system\nshould look to the user. This task involves deﬁning a ﬁle and its attributes,\nthe operations allowed on a ﬁle, and the directory structure for organizing\nﬁles. The second problem is creating algorithms and data structures to map the\nlogical ﬁle system onto the physical secondary-storage devices.\nThe ﬁle system itself is generally composed of many different levels. The\nstructure shown in Figure 12.1 is an example of a layered design. Each level in\nthe design uses the features of lower levels to create new features for use by\nhigher levels.\nThe I/O control level consists of device drivers and interrupt handlers\nto transfer information between the main memory and the disk system. A",
  "device driver can be thought of as a translator. Its input consists of high-\nlevel commands such as “retrieve block 123.” Its output consists of low-level,\nhardware-speciﬁc instructions that are used by the hardware controller, which\ninterfaces the I/O device to the rest of the system. The device driver usually\nwrites speciﬁc bit patterns to special locations in the I/O controller’s memory\nto tell the controller which device location to act on and what actions to take.\nThe details of device drivers and the I/O infrastructure are covered in Chapter\n13.\nThe basic ﬁle system needs only to issue generic commands to the\nappropriate device driver to read and write physical blocks on the disk. Each\nphysical block is identiﬁed by its numeric disk address (for example, drive 1,",
  "cylinder 73, track 2, sector 10). This layer also manages the memory buffers\nand caches that hold various ﬁle-system, directory, and data blocks. A block\nin the buffer is allocated before the transfer of a disk block can occur. When\nthe buffer is full, the buffer manager must ﬁnd more buffer memory or free\napplication programs\nfile-organization module\nbasic file system\nI/O control\ndevices\nlogical file system\nFigure 12.1\nLayered ﬁle system. 12.1\nFile-System Structure\n545\nup buffer space to allow a requested I/O to complete. Caches are used to hold\nfrequently used ﬁle-system metadata to improve performance, so managing\ntheir contents is critical for optimum system performance.\nThe ﬁle-organization module knows about ﬁles and their logical blocks,",
  "as well as physical blocks. By knowing the type of ﬁle allocation used and\nthe location of the ﬁle, the ﬁle-organization module can translate logical block\naddresses to physical block addresses for the basic ﬁle system to transfer.\nEach ﬁle’s logical blocks are numbered from 0 (or 1) through N. Since the\nphysical blocks containing the data usually do not match the logical numbers,\na translation is needed to locate each block. The ﬁle-organization module also\nincludes the free-space manager, which tracks unallocated blocks and provides\nthese blocks to the ﬁle-organization module when requested.\nFinally, the logical ﬁle system manages metadata information. Metadata\nincludes all of the ﬁle-system structure except the actual data (or contents of",
  "the ﬁles). The logical ﬁle system manages the directory structure to provide\nthe ﬁle-organization module with the information the latter needs, given a\nsymbolic ﬁle name. It maintains ﬁle structure via ﬁle-control blocks. A ﬁle-\ncontrol block (FCB) (an inode in UNIX ﬁle systems) contains information about\nthe ﬁle, including ownership, permissions, and location of the ﬁle contents. The\nlogical ﬁle system is also responsible for protection, as discussed in Chaptrers\n11 and 14.\nWhen a layered structure is used for ﬁle-system implementation, duplica-\ntion of code is minimized. The I/O control and sometimes the basic ﬁle-system\ncode can be used by multiple ﬁle systems. Each ﬁle system can then have its\nown logical ﬁle-system and ﬁle-organization modules. Unfortunately, layering",
  "can introduce more operating-system overhead, which may result in decreased\nperformance. The use of layering, including the decision about how many\nlayers to use and what each layer should do, is a major challenge in designing\nnew systems.\nMany ﬁle systems are in use today, and most operating systems support\nmore than one. For example, most CD-ROMs are written in the ISO 9660\nformat, a standard format agreed on by CD-ROM manufacturers. In addition\nto removable-media ﬁle systems, each operating system has one or more disk-\nbased ﬁle systems. UNIX uses the UNIX ﬁle system (UFS), which is based on the\nBerkeley Fast File System (FFS). Windows supports disk ﬁle-system formats of\nFAT, FAT32, and NTFS (or Windows NT File System), as well as CD-ROM and DVD",
  "ﬁle-system formats. Although Linux supports over forty different ﬁle systems,\nthe standard Linux ﬁle system is known as the extended ﬁle system, with\nthe most common versions being ext3 and ext4. There are also distributed ﬁle\nsystems in which a ﬁle system on a server is mounted by one or more client\ncomputers across a network.\nFile-system research continues to be an active area of operating-system\ndesign and implementation. Google created its own ﬁle system to meet\nthe company’s speciﬁc storage and retrieval needs, which include high-\nperformance access from many clients across a very large number of disks.\nAnother interesting project is the FUSE ﬁle system, which provides ﬂexibility in\nﬁle-system development and use by implementing and executing ﬁle systems",
  "as user-level rather than kernel-level code. Using FUSE, a user can add a new\nﬁle system to a variety of operating systems and can use that ﬁle system to\nmanage her ﬁles. 546\nChapter 12\nFile-System Implementation\n12.2 File-System Implementation\nAs was described in Section 11.1.2, operating systems implement open()\nand close() systems calls for processes to request access to ﬁle contents.\nIn this section, we delve into the structures and operations used to implement\nﬁle-system operations.\n12.2.1\nOverview\nSeveral on-disk and in-memory structures are used to implement a ﬁle system.\nThese structures vary depending on the operating system and the ﬁle system,\nbut some general principles apply.\nOn disk, the ﬁle system may contain information about how to boot an",
  "operating system stored there, the total number of blocks, the number and\nlocation of free blocks, the directory structure, and individual ﬁles. Many of\nthese structures are detailed throughout the remainder of this chapter. Here,\nwe describe them brieﬂy:\n• A boot control block (per volume) can contain information needed by the\nsystem to boot an operating system from that volume. If the disk does not\ncontain an operating system, this block can be empty. It is typically the\nﬁrst block of a volume. In UFS, it is called the boot block. In NTFS, it is the\npartition boot sector.\n• A volume control block (per volume) contains volume (or partition)\ndetails, such as the number of blocks in the partition, the size of the blocks,",
  "a free-block count and free-block pointers, and a free-FCB count and FCB\npointers. In UFS, this is called a superblock. In NTFS, it is stored in the\nmaster ﬁle table.\n• A directory structure (per ﬁle system) is used to organize the ﬁles. In UFS,\nthis includes ﬁle names and associated inode numbers. In NTFS, it is stored\nin the master ﬁle table.\n• A per-ﬁle FCB contains many details about the ﬁle. It has a unique\nidentiﬁer number to allow association with a directory entry. In NTFS,\nthis information is actually stored within the master ﬁle table, which uses\na relational database structure, with a row per ﬁle.\nThe in-memory information is used for both ﬁle-system management and\nperformance improvement via caching. The data are loaded at mount time,",
  "updated during ﬁle-system operations, and discarded at dismount. Several\ntypes of structures may be included.\n• An in-memory mount table contains information about each mounted\nvolume.\n• An in-memory directory-structure cache holds the directory information\nof recently accessed directories. (For directories at which volumes are\nmounted, it can contain a pointer to the volume table.)\n• The system-wide open-ﬁle table contains a copy of the FCB of each open\nﬁle, as well as other information. 12.2\nFile-System Implementation\n547\nfile permissions\nfile dates (create, access, write)\nfile owner, group, ACL\nfile size\nfile data blocks or pointers to file data blocks\nFigure 12.2\nA typical ﬁle-control block.\n• The per-process open-ﬁle table contains a pointer to the appropriate entry",
  "in the system-wide open-ﬁle table, as well as other information.\n• Buffers hold ﬁle-system blocks when they are being read from disk or\nwritten to disk.\nTo create a new ﬁle, an application program calls the logical ﬁle system.\nThe logical ﬁle system knows the format of the directory structures. To create a\nnew ﬁle, it allocates a new FCB. (Alternatively, if the ﬁle-system implementation\ncreates all FCBs at ﬁle-system creation time, an FCB is allocated from the set\nof free FCBs.) The system then reads the appropriate directory into memory,\nupdates it with the new ﬁle name and FCB, and writes it back to the disk. A\ntypical FCB is shown in Figure 12.2.\nSome operating systems, including UNIX, treat a directory exactly the same",
  "as a ﬁle—one with a “type” ﬁeld indicating that it is a directory. Other operating\nsystems, including Windows, implement separate system calls for ﬁles and\ndirectories and treat directories as entities separate from ﬁles. Whatever the\nlarger structural issues, the logical ﬁle system can call the ﬁle-organization\nmodule to map the directory I/O into disk-block numbers, which are passed\non to the basic ﬁle system and I/O control system.\nNow that a ﬁle has been created, it can be used for I/O. First, though, it\nmust be opened. The open() call passes a ﬁle name to the logical ﬁle system.\nThe open() system call ﬁrst searches the system-wide open-ﬁle table to see\nif the ﬁle is already in use by another process. If it is, a per-process open-ﬁle",
  "table entry is created pointing to the existing system-wide open-ﬁle table. This\nalgorithm can save substantial overhead. If the ﬁle is not already open, the\ndirectory structure is searched for the given ﬁle name. Parts of the directory\nstructure are usually cached in memory to speed directory operations. Once\nthe ﬁle is found, the FCB is copied into a system-wide open-ﬁle table in memory.\nThis table not only stores the FCB but also tracks the number of processes that\nhave the ﬁle open.\nNext, an entry is made in the per-process open-ﬁle table, with a pointer\nto the entry in the system-wide open-ﬁle table and some other ﬁelds. These\nother ﬁelds may include a pointer to the current location in the ﬁle (for the next\nread() or write() operation) and the access mode in which the ﬁle is open.",
  "The open() call returns a pointer to the appropriate entry in the per-process 548\nChapter 12\nFile-System Implementation\ndirectory structure\ndirectory structure\nopen (file name)\nkernel memory\nuser space\nindex\n(a)\nfile-control block\nsecondary storage\ndata blocks\nper-process\nopen-file table\nsystem-wide\nopen-file table\nread (index)\nkernel memory\nuser space\n(b)\nfile-control block\nsecondary storage\nFigure 12.3\nIn-memory ﬁle-system structures. (a) File open. (b) File read.\nﬁle-system table. All ﬁle operations are then performed via this pointer. The\nﬁle name may not be part of the open-ﬁle table, as the system has no use for\nit once the appropriate FCB is located on disk. It could be cached, though, to\nsave time on subsequent opens of the same ﬁle. The name given to the entry",
  "varies. UNIX systems refer to it as a ﬁle descriptor; Windows refers to it as a\nﬁle handle.\nWhen a process closes the ﬁle, the per-process table entry is removed, and\nthe system-wide entry’s open count is decremented. When all users that have\nopened the ﬁle close it, any updated metadata is copied back to the disk-based\ndirectory structure, and the system-wide open-ﬁle table entry is removed.\nSome systems complicate this scheme further by using the ﬁle system as an\ninterface to other system aspects, such as networking. For example, in UFS, the\nsystem-wide open-ﬁle table holds the inodes and other information for ﬁles\nand directories. It also holds similar information for network connections and\ndevices. In this way, one mechanism can be used for multiple purposes.",
  "The caching aspects of ﬁle-system structures should not be overlooked.\nMost systems keep all information about an open ﬁle, except for its actual data\nblocks, in memory. The BSD UNIX system is typical in its use of caches wherever\ndisk I/O can be saved. Its average cache hit rate of 85 percent shows that these\ntechniques are well worth implementing. The BSD UNIX system is described\nfully in Appendix A.\nThe operating structures of a ﬁle-system implementation are summarized\nin Figure 12.3. 12.2\nFile-System Implementation\n549\n12.2.2\nPartitions and Mounting\nThe layout of a disk can have many variations, depending on the operating\nsystem. A disk can be sliced into multiple partitions, or a volume can span\nmultiple partitions on multiple disks. The former layout is discussed here,",
  "while the latter, which is more appropriately considered a form of RAID, is\ncovered in Section 10.7.\nEach partition can be either “raw,” containing no ﬁle system, or “cooked,”\ncontaining a ﬁle system. Raw disk is used where no ﬁle system is appropriate.\nUNIX swap space can use a raw partition, for example, since it uses its own\nformat on disk and does not use a ﬁle system. Likewise, some databases use raw\ndisk and format the data to suit their needs. Raw disk can also hold information\nneeded by disk RAID systems, such as bit maps indicating which blocks are\nmirrored and which have changed and need to be mirrored. Similarly, raw\ndisk can contain a miniature database holding RAID conﬁguration information,\nsuch as which disks are members of each RAID set. Raw disk use is discussed",
  "in Section 10.5.1.\nBoot information can be stored in a separate partition, as described in\nSection 10.5.2. Again, it has its own format, because at boot time the system\ndoes not have the ﬁle-system code loaded and therefore cannot interpret the\nﬁle-system format. Rather, boot information is usually a sequential series of\nblocks, loaded as an image into memory. Execution of the image starts at a\npredeﬁned location, such as the ﬁrst byte. This boot loader in turn knows\nenough about the ﬁle-system structure to be able to ﬁnd and load the kernel\nand start it executing. It can contain more than the instructions for how to boot\na speciﬁc operating system. For instance, many systems can be dual-booted,\nallowing us to install multiple operating systems on a single system. How does",
  "the system know which one to boot? A boot loader that understands multiple\nﬁle systems and multiple operating systems can occupy the boot space. Once\nloaded, it can boot one of the operating systems available on the disk. The disk\ncan have multiple partitions, each containing a different type of ﬁle system and\na different operating system.\nThe root partition, which contains the operating-system kernel and some-\ntimes other system ﬁles, is mounted at boot time. Other volumes can be\nautomatically mounted at boot or manually mounted later, depending on\nthe operating system. As part of a successful mount operation, the operating\nsystem veriﬁes that the device contains a valid ﬁle system. It does so by asking\nthe device driver to read the device directory and verifying that the directory",
  "has the expected format. If the format is invalid, the partition must have\nits consistency checked and possibly corrected, either with or without user\nintervention. Finally, the operating system notes in its in-memory mount table\nthat a ﬁle system is mounted, along with the type of the ﬁle system. The details\nof this function depend on the operating system.\nMicrosoft Windows–based systems mount each volume in a separate name\nspace, denoted by a letter and a colon. To record that a ﬁle system is mounted\nat F:, for example, the operating system places a pointer to the ﬁle system in\na ﬁeld of the device structure corresponding to F:. When a process speciﬁes\nthe driver letter, the operating system ﬁnds the appropriate ﬁle-system pointer",
  "and traverses the directory structures on that device to ﬁnd the speciﬁed ﬁle 550\nChapter 12\nFile-System Implementation\nor directory. Later versions of Windows can mount a ﬁle system at any point\nwithin the existing directory structure.\nOn UNIX, ﬁle systems can be mounted at any directory. Mounting is\nimplemented by setting a ﬂag in the in-memory copy of the inode for that\ndirectory. The ﬂag indicates that the directory is a mount point. A ﬁeld then\npoints to an entry in the mount table, indicating which device is mounted there.\nThe mount table entry contains a pointer to the superblock of the ﬁle system on\nthat device. This scheme enables the operating system to traverse its directory\nstructure, switching seamlessly among ﬁle systems of varying types.\n12.2.3\nVirtual File Systems",
  "12.2.3\nVirtual File Systems\nThe previous section makes it clear that modern operating systems must\nconcurrently support multiple types of ﬁle systems. But how does an operating\nsystem allow multiple types of ﬁle systems to be integrated into a directory\nstructure? And how can users seamlessly move between ﬁle-system types\nas they navigate the ﬁle-system space? We now discuss some of these\nimplementation details.\nAn obvious but suboptimal method of implementing multiple types of ﬁle\nsystems is to write directory and ﬁle routines for each type. Instead, however,\nmost operating systems, including UNIX, use object-oriented techniques to\nsimplify, organize, and modularize the implementation. The use of these\nmethods allows very dissimilar ﬁle-system types to be implemented within",
  "the same structure, including network ﬁle systems, such as NFS. Users can\naccess ﬁles contained within multiple ﬁle systems on the local disk or even on\nﬁle systems available across the network.\nData structures and procedures are used to isolate the basic system-\ncall functionality from the implementation details. Thus, the ﬁle-system\nimplementation consists of three major layers, as depicted schematically in\nFigure 12.4. The ﬁrst layer is the ﬁle-system interface, based on the open(),\nread(), write(), and close() calls and on ﬁle descriptors.\nThe second layer is called the virtual ﬁle system (VFS) layer. The VFS layer\nserves two important functions:\n1. It separates ﬁle-system-generic operations from their implementation",
  "by deﬁning a clean VFS interface. Several implementations for the VFS\ninterface may coexist on the same machine, allowing transparent access\nto different types of ﬁle systems mounted locally.\n2. It provides a mechanism for uniquely representing a ﬁle throughout a\nnetwork. The VFS is based on a ﬁle-representation structure, called a\nvnode, that contains a numerical designator for a network-wide unique\nﬁle. (UNIX inodes are unique within only a single ﬁle system.) This\nnetwork-wide uniqueness is required for support of network ﬁle systems.\nThe kernel maintains one vnode structure for each active node (ﬁle or\ndirectory).\nThus, the VFS distinguishes local ﬁles from remote ones, and local ﬁles are\nfurther distinguished according to their ﬁle-system types.",
  "The VFS activates ﬁle-system-speciﬁc operations to handle local requests\naccording to their ﬁle-system types and calls the NFS protocol procedures for 12.2\nFile-System Implementation\n551\nlocal file system\ntype 1\ndisk\nlocal file system\ntype 2\ndisk\nremote file system\ntype 1\nnetwork\nfile-system interface\nVFS interface\nFigure 12.4\nSchematic view of a virtual ﬁle system.\nremote requests. File handles are constructed from the relevant vnodes and\nare passed as arguments to these procedures. The layer implementing the\nﬁle-system type or the remote-ﬁle-system protocol is the third layer of the\narchitecture.\nLet’s brieﬂy examine the VFS architecture in Linux. The four main object\ntypes deﬁned by the Linux VFS are:\n• The inode object, which represents an individual ﬁle",
  "• The ﬁle object, which represents an open ﬁle\n• The superblock object, which represents an entire ﬁle system\n• The dentry object, which represents an individual directory entry\nFor each of these four object types, the VFS deﬁnes a set of operations that\nmay be implemented. Every object of one of these types contains a pointer to\na function table. The function table lists the addresses of the actual functions\nthat implement the deﬁned operations for that particular object. For example,\nan abbreviated API for some of the operations for the ﬁle object includes:\n• int open(. . .)—Open a ﬁle.\n• int close(...)—Close an already-open ﬁle.\n• ssize t read(. . .)—Read from a ﬁle.\n• ssize t write(. . .)—Write to a ﬁle.\n• int mmap(. . .)—Memory-map a ﬁle. 552\nChapter 12\nFile-System Implementation",
  "Chapter 12\nFile-System Implementation\nAn implementation of the ﬁle object for a speciﬁc ﬁle type is required to imple-\nment each function speciﬁed in the deﬁnition of the ﬁle object. (The complete\ndeﬁnition of the ﬁle object is speciﬁed in the struct file operations, which\nis located in the ﬁle /usr/include/linux/fs.h.)\nThus, the VFS software layer can perform an operation on one of these\nobjects by calling the appropriate function from the object’s function table,\nwithout having to know in advance exactly what kind of object it is dealing\nwith. The VFS does not know, or care, whether an inode represents a disk ﬁle,\na directory ﬁle, or a remote ﬁle. The appropriate function for that ﬁle’s read()\noperation will always be at the same place in its function table, and the VFS",
  "software layer will call that function without caring how the data are actually\nread.\n12.3 Directory Implementation\nThe selection of directory-allocation and directory-management algorithms\nsigniﬁcantly affects the efﬁciency, performance, and reliability of the ﬁle\nsystem. In this section, we discuss the trade-offs involved in choosing one\nof these algorithms.\n12.3.1\nLinear List\nThe simplest method of implementing a directory is to use a linear list of ﬁle\nnames with pointers to the data blocks. This method is simple to program\nbut time-consuming to execute. To create a new ﬁle, we must ﬁrst search the\ndirectory to be sure that no existing ﬁle has the same name. Then, we add a\nnew entry at the end of the directory. To delete a ﬁle, we search the directory for",
  "the named ﬁle and then release the space allocated to it. To reuse the directory\nentry, we can do one of several things. We can mark the entry as unused (by\nassigning it a special name, such as an all-blank name, or by including a used–\nunused bit in each entry), or we can attach it to a list of free directory entries. A\nthird alternative is to copy the last entry in the directory into the freed location\nand to decrease the length of the directory. A linked list can also be used to\ndecrease the time required to delete a ﬁle.\nThe real disadvantage of a linear list of directory entries is that ﬁnding a\nﬁle requires a linear search. Directory information is used frequently, and users\nwill notice if access to it is slow. In fact, many operating systems implement a",
  "software cache to store the most recently used directory information. A cache\nhit avoids the need to constantly reread the information from disk. A sorted\nlist allows a binary search and decreases the average search time. However, the\nrequirement that the list be kept sorted may complicate creating and deleting\nﬁles, since we may have to move substantial amounts of directory information\nto maintain a sorted directory. A more sophisticated tree data structure, such\nas a balanced tree, might help here. An advantage of the sorted list is that a\nsorted directory listing can be produced without a separate sort step.\n12.3.2\nHash Table\nAnother data structure used for a ﬁle directory is a hash table. Here, a linear",
  "list stores the directory entries, but a hash data structure is also used. The hash\ntable takes a value computed from the ﬁle name and returns a pointer to the ﬁle 12.4\nAllocation Methods\n553\nname in the linear list. Therefore, it can greatly decrease the directory search\ntime. Insertion and deletion are also fairly straightforward, although some\nprovision must be made for collisions—situations in which two ﬁle names\nhash to the same location.\nThe major difﬁculties with a hash table are its generally ﬁxed size and the\ndependence of the hash function on that size. For example, assume that we\nmake a linear-probing hash table that holds 64 entries. The hash function\nconverts ﬁle names into integers from 0 to 63 (for instance, by using the",
  "remainder of a division by 64). If we later try to create a 65th ﬁle, we must\nenlarge the directory hash table—say, to 128 entries. As a result, we need\na new hash function that must map ﬁle names to the range 0 to 127, and we\nmust reorganize the existing directory entries to reﬂect their new hash-function\nvalues.\nAlternatively, we can use a chained-overﬂow hash table. Each hash entry\ncan be a linked list instead of an individual value, and we can resolve collisions\nby adding the new entry to the linked list. Lookups may be somewhat slowed,\nbecause searching for a name might require stepping through a linked list of\ncolliding table entries. Still, this method is likely to be much faster than a linear\nsearch through the entire directory.\n12.4 Allocation Methods",
  "12.4 Allocation Methods\nThe direct-access nature of disks gives us ﬂexibility in the implementation of\nﬁles. In almost every case, many ﬁles are stored on the same disk. The main\nproblem is how to allocate space to these ﬁles so that disk space is utilized\neffectively and ﬁles can be accessed quickly. Three major methods of allocating\ndisk space are in wide use: contiguous, linked, and indexed. Each method has\nadvantages and disadvantages. Although some systems support all three, it is\nmore common for a system to use one method for all ﬁles within a ﬁle-system\ntype.\n12.4.1\nContiguous Allocation\nContiguous allocation requires that each ﬁle occupy a set of contiguous blocks\non the disk. Disk addresses deﬁne a linear ordering on the disk. With this",
  "ordering, assuming that only one job is accessing the disk, accessing block b +\n1 after block b normally requires no head movement. When head movement\nis needed (from the last sector of one cylinder to the ﬁrst sector of the next\ncylinder), the head need only move from one track to the next. Thus, the\nnumber of disk seeks required for accessing contiguously allocated ﬁles is\nminimal, as is seek time when a seek is ﬁnally needed.\nContiguous allocation of a ﬁle is deﬁned by the disk address and length (in\nblock units) of the ﬁrst block. If the ﬁle is n blocks long and starts at location\nb, then it occupies blocks b, b + 1, b + 2, ..., b + n −1. The directory entry for\neach ﬁle indicates the address of the starting block and the length of the area\nallocated for this ﬁle (Figure 12.5).",
  "allocated for this ﬁle (Figure 12.5).\nAccessing a ﬁle that has been allocated contiguously is easy. For sequential\naccess, the ﬁle system remembers the disk address of the last block referenced\nand, when necessary, reads the next block. For direct access to block i of a 554\nChapter 12\nFile-System Implementation\ndirectory\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\ncount\nf\ntr\nmail\nlist\nstart\n0 \n14 \n19 \n28 \n6\nlength\n2 \n3 \n6 \n4 \n2\nfile\ncount \ntr \nmail \nlist \nf\nFigure 12.5\nContiguous allocation of disk space.\nﬁle that starts at block b, we can immediately access block b + i. Thus, both\nsequential and direct access can be supported by contiguous allocation.\nContiguous allocation has some problems, however. One difﬁculty is",
  "ﬁnding space for a new ﬁle. The system chosen to manage free space determines\nhow this task is accomplished; these management systems are discussed in\nSection 12.5. Any management system can be used, but some are slower than\nothers.\nThe contiguous-allocation problem can be seen as a particular application\nof the general dynamic storage-allocation problem discussed in Section 8.3,\nwhich involves how to satisfy a request of size n from a list of free holes. First\nﬁt and best ﬁt are the most common strategies used to select a free hole from\nthe set of available holes. Simulations have shown that both ﬁrst ﬁt and best ﬁt\nare more efﬁcient than worst ﬁt in terms of both time and storage utilization.\nNeither ﬁrst ﬁt nor best ﬁt is clearly best in terms of storage utilization, but",
  "ﬁrst ﬁt is generally faster.\nAll these algorithms suffer from the problem of external fragmentation.\nAs ﬁles are allocated and deleted, the free disk space is broken into little pieces.\nExternal fragmentation exists whenever free space is broken into chunks. It\nbecomes a problem when the largest contiguous chunk is insufﬁcient for a\nrequest; storage is fragmented into a number of holes, none of which is large\nenough to store the data. Depending on the total amount of disk storage and the\naverage ﬁle size, external fragmentation may be a minor or a major problem.\nOne strategy for preventing loss of signiﬁcant amounts of disk space to\nexternal fragmentation is to copy an entire ﬁle system onto another disk. The",
  "original disk is then freed completely, creating one large contiguous free space.\nWe then copy the ﬁles back onto the original disk by allocating contiguous\nspace from this one large hole. This scheme effectively compacts all free space\ninto one contiguous space, solving the fragmentation problem. The cost of this 12.4\nAllocation Methods\n555\ncompaction is time, however, and the cost can be particularly high for large\nhard disks. Compacting these disks may take hours and may be necessary on\na weekly basis. Some systems require that this function be done off-line, with\nthe ﬁle system unmounted. During this down time, normal system operation\ngenerally cannot be permitted, so such compaction is avoided at all costs on\nproduction machines. Most modern systems that need defragmentation can",
  "perform it on-line during normal system operations, but the performance\npenalty can be substantial.\nAnother problem with contiguous allocation is determining how much\nspace is needed for a ﬁle. When the ﬁle is created, the total amount of space\nit will need must be found and allocated. How does the creator (program or\nperson) know the size of the ﬁle to be created? In some cases, this determination\nmay be fairly simple (copying an existing ﬁle, for example). In general,\nhowever, the size of an output ﬁle may be difﬁcult to estimate.\nIf we allocate too little space to a ﬁle, we may ﬁnd that the ﬁle cannot\nbe extended. Especially with a best-ﬁt allocation strategy, the space on both\nsides of the ﬁle may be in use. Hence, we cannot make the ﬁle larger in place.",
  "Two possibilities then exist. First, the user program can be terminated, with\nan appropriate error message. The user must then allocate more space and\nrun the program again. These repeated runs may be costly. To prevent them,\nthe user will normally overestimate the amount of space needed, resulting\nin considerable wasted space. The other possibility is to ﬁnd a larger hole,\ncopy the contents of the ﬁle to the new space, and release the previous space.\nThis series of actions can be repeated as long as space exists, although it can\nbe time consuming. The user need never be informed explicitly about what\nis happening, however; the system continues despite the problem, although\nmore and more slowly.\nEven if the total amount of space needed for a ﬁle is known in advance,",
  "preallocation may be inefﬁcient. A ﬁle that will grow slowly over a long period\n(months or years) must be allocated enough space for its ﬁnal size, even though\nmuch of that space will be unused for a long time. The ﬁle therefore has a large\namount of internal fragmentation.\nTo minimize these drawbacks, some operating systems use a modiﬁed\ncontiguous-allocation scheme. Here, a contiguous chunk of space is allocated\ninitially. Then, if that amount proves not to be large enough, another chunk of\ncontiguous space, known as an extent, is added. The location of a ﬁle’s blocks\nis then recorded as a location and a block count, plus a link to the ﬁrst block\nof the next extent. On some systems, the owner of the ﬁle can set the extent",
  "size, but this setting results in inefﬁciencies if the owner is incorrect. Internal\nfragmentation can still be a problem if the extents are too large, and external\nfragmentation can become a problem as extents of varying sizes are allocated\nand deallocated. The commercial Veritas ﬁle system uses extents to optimize\nperformance. Veritas is a high-performance replacement for the standard UNIX\nUFS.\n12.4.2\nLinked Allocation\nLinked allocation solves all problems of contiguous allocation. With linked\nallocation, each ﬁle is a linked list of disk blocks; the disk blocks may be\nscattered anywhere on the disk. The directory contains a pointer to the ﬁrst 556\nChapter 12\nFile-System Implementation\n0\n1\n2\n3\n4\n5\n7\n8\n9\n10\n11\n12\n13\n14\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n15\n6\nfile\njeep\nstart\n9",
  "23\n24\n25\n26\n27\n28\n29\n30\n31\n15\n6\nfile\njeep\nstart\n9\ndirectory\nend\n25\n1\n1\n-1\n2\nFigure 12.6\nLinked allocation of disk space.\nand last blocks of the ﬁle. For example, a ﬁle of ﬁve blocks might start at block\n9 and continue at block 16, then block 1, then block 10, and ﬁnally block 25\n(Figure 12.6). Each block contains a pointer to the next block. These pointers\nare not made available to the user. Thus, if each block is 512 bytes in size, and\na disk address (the pointer) requires 4 bytes, then the user sees blocks of 508\nbytes.\nTo create a new ﬁle, we simply create a new entry in the directory. With\nlinked allocation, each directory entry has a pointer to the ﬁrst disk block of\nthe ﬁle. This pointer is initialized to null (the end-of-list pointer value) to",
  "signify an empty ﬁle. The size ﬁeld is also set to 0. A write to the ﬁle causes\nthe free-space management system to ﬁnd a free block, and this new block\nis written to and is linked to the end of the ﬁle. To read a ﬁle, we simply\nread blocks by following the pointers from block to block. There is no external\nfragmentation with linked allocation, and any free block on the free-space list\ncan be used to satisfy a request. The size of a ﬁle need not be declared when the\nﬁle is created. A ﬁle can continue to grow as long as free blocks are available.\nConsequently, it is never necessary to compact disk space.\nLinked allocation does have disadvantages, however. The major problem\nis that it can be used effectively only for sequential-access ﬁles. To ﬁnd the",
  "ith block of a ﬁle, we must start at the beginning of that ﬁle and follow the\npointers until we get to the ith block. Each access to a pointer requires a disk\nread, and some require a disk seek. Consequently, it is inefﬁcient to support a\ndirect-access capability for linked-allocation ﬁles.\nAnother disadvantage is the space required for the pointers. If a pointer\nrequires 4 bytes out of a 512-byte block, then 0.78 percent of the disk is being\nused for pointers, rather than for information. Each ﬁle requires slightly more\nspace than it would otherwise.\nThe usual solution to this problem is to collect blocks into multiples, called\nclusters, and to allocate clusters rather than blocks. For instance, the ﬁle system 12.4\nAllocation Methods\n557",
  "Allocation Methods\n557\nmay deﬁne a cluster as four blocks and operate on the disk only in cluster\nunits. Pointers then use a much smaller percentage of the ﬁle’s disk space.\nThis method allows the logical-to-physical block mapping to remain simple\nbut improves disk throughput (because fewer disk-head seeks are required)\nand decreases the space needed for block allocation and free-list management.\nThe cost of this approach is an increase in internal fragmentation, because\nmore space is wasted when a cluster is partially full than when a block is\npartially full. Clusters can be used to improve the disk-access time for many\nother algorithms as well, so they are used in most ﬁle systems.\nYet another problem of linked allocation is reliability. Recall that the ﬁles",
  "are linked together by pointers scattered all over the disk, and consider what\nwould happen if a pointer were lost or damaged. A bug in the operating-system\nsoftware or a disk hardware failure might result in picking up the wrong\npointer. This error could in turn result in linking into the free-space list or into\nanother ﬁle. One partial solution is to use doubly linked lists, and another is\nto store the ﬁle name and relative block number in each block. However, these\nschemes require even more overhead for each ﬁle.\nAn important variation on linked allocation is the use of a ﬁle-allocation\ntable (FAT). This simple but efﬁcient method of disk-space allocation was used\nby the MS-DOS operating system. A section of disk at the beginning of each",
  "volume is set aside to contain the table. The table has one entry for each disk\nblock and is indexed by block number. The FAT is used in much the same\nway as a linked list. The directory entry contains the block number of the\nﬁrst block of the ﬁle. The table entry indexed by that block number contains\nthe block number of the next block in the ﬁle. This chain continues until it\nreaches the last block, which has a special end-of-ﬁle value as the table entry.\nAn unused block is indicated by a table value of 0. Allocating a new block to\na ﬁle is a simple matter of ﬁnding the ﬁrst 0-valued table entry and replacing\nthe previous end-of-ﬁle value with the address of the new block. The 0 is then\nreplaced with the end-of-ﬁle value. An illustrative example is the FAT structure",
  "shown in Figure 12.7 for a ﬁle consisting of disk blocks 217, 618, and 339.\nThe FAT allocation scheme can result in a signiﬁcant number of disk head\nseeks, unless the FAT is cached. The disk head must move to the start of the\nvolume to read the FAT and ﬁnd the location of the block in question, then\nmove to the location of the block itself. In the worst case, both moves occur for\neach of the blocks. A beneﬁt is that random-access time is improved, because\nthe disk head can ﬁnd the location of any block by reading the information in\nthe FAT.\n12.4.3\nIndexed Allocation\nLinked allocation solves the external-fragmentation and size-declaration prob-\nlems of contiguous allocation. However, in the absence of a FAT, linked",
  "allocation cannot support efﬁcient direct access, since the pointers to the blocks\nare scattered with the blocks themselves all over the disk and must be retrieved\nin order. Indexed allocation solves this problem by bringing all the pointers\ntogether into one location: the index block.\nEach ﬁle has its own index block, which is an array of disk-block addresses.\nThe ith entry in the index block points to the ith block of the ﬁle. The directory 558\nChapter 12\nFile-System Implementation\n• • •\ndirectory entry\ntest\n217\nstart block\nname\n0\n217\n618\n339\n618\n339\nnumber of disk blocks    –1\nFAT\nFigure 12.7\nFile-allocation table.\ncontains the address of the index block (Figure 12.8). To ﬁnd and read the ith\nblock, we use the pointer in the ith index-block entry. This scheme is similar to",
  "the paging scheme described in Section 8.5.\nWhen the ﬁle is created, all pointers in the index block are set to null.\nWhen the ith block is ﬁrst written, a block is obtained from the free-space\nmanager, and its address is put in the ith index-block entry.\nIndexed allocation supports direct access, without suffering from external\nfragmentation, because any free block on the disk can satisfy a request for more\nspace. Indexed allocation does suffer from wasted space, however. The pointer\ndirectory\n0\n1\n2\n3\n4\n5\n7\n8\n9\n10\n11\n12\n13\n14\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n15\n6\n9 \n16\n1\n10 \n25 \n–1 \n–1\n–1\nfile\njeep\nindex block\n19\n19\nFigure 12.8\nIndexed allocation of disk space. 12.4\nAllocation Methods\n559\noverhead of the index block is generally greater than the pointer overhead of",
  "linked allocation. Consider a common case in which we have a ﬁle of only one\nor two blocks. With linked allocation, we lose the space of only one pointer per\nblock. With indexed allocation, an entire index block must be allocated, even\nif only one or two pointers will be non-null.\nThis point raises the question of how large the index block should be. Every\nﬁle must have an index block, so we want the index block to be as small as\npossible. If the index block is too small, however, it will not be able to hold\nenough pointers for a large ﬁle, and a mechanism will have to be available to\ndeal with this issue. Mechanisms for this purpose include the following:\n• Linked scheme. An index block is normally one disk block. Thus, it can",
  "be read and written directly by itself. To allow for large ﬁles, we can link\ntogether several index blocks. For example, an index block might contain a\nsmall header giving the name of the ﬁle and a set of the ﬁrst 100 disk-block\naddresses. The next address (the last word in the index block) is null (for\na small ﬁle) or is a pointer to another index block (for a large ﬁle).\n• Multilevel index. A variant of linked representation uses a ﬁrst-level index\nblock to point to a set of second-level index blocks, which in turn point to\nthe ﬁle blocks. To access a block, the operating system uses the ﬁrst-level\nindex to ﬁnd a second-level index block and then uses that block to ﬁnd the\ndesired data block. This approach could be continued to a third or fourth",
  "level, depending on the desired maximum ﬁle size. With 4,096-byte blocks,\nwe could store 1,024 four-byte pointers in an index block. Two levels of\nindexes allow 1,048,576 data blocks and a ﬁle size of up to 4 GB.\n• Combined scheme. Another alternative, used in UNIX-based ﬁle systems,\nis to keep the ﬁrst, say, 15 pointers of the index block in the ﬁle’s inode.\nThe ﬁrst 12 of these pointers point to direct blocks; that is, they contain\naddresses of blocks that contain data of the ﬁle. Thus, the data for small\nﬁles (of no more than 12 blocks) do not need a separate index block. If the\nblock size is 4 KB, then up to 48 KB of data can be accessed directly. The next\nthree pointers point to indirect blocks. The ﬁrst points to a single indirect",
  "block, which is an index block containing not data but the addresses of\nblocks that do contain data. The second points to a double indirect block,\nwhich contains the address of a block that contains the addresses of blocks\nthat contain pointers to the actual data blocks. The last pointer contains\nthe address of a triple indirect block. (A UNIX inode is shown in Figure\n12.9.)\nUnder this method, the number of blocks that can be allocated to a ﬁle\nexceeds the amount of space addressable by the 4-byte ﬁle pointers used\nby many operating systems. A 32-bit ﬁle pointer reaches only 232 bytes,\nor 4 GB. Many UNIX and Linux implementations now support 64-bit ﬁle\npointers, which allows ﬁles and ﬁle systems to be several exbibytes in size.\nThe ZFS ﬁle system supports 128-bit ﬁle pointers.",
  "The ZFS ﬁle system supports 128-bit ﬁle pointers.\nIndexed-allocation schemes suffer from some of the same performance\nproblems as does linked allocation. Speciﬁcally, the index blocks can be cached\nin memory, but the data blocks may be spread all over a volume. 560\nChapter 12\nFile-System Implementation\ndirect blocks\ndata\ndata\ndata\ndata\ndata\ndata\ndata\ndata\ndata\ndata\n• \n• \n•\n• \n• \n•\n• • •\n• • •\n• • •\n• • •\nmode\nowners (2)\ntimestamps (3)\nsize block count \nsingle indirect\ndouble indirect\ntriple indirect\nFigure 12.9\nThe UNIX inode.\n12.4.4\nPerformance\nThe allocation methods that we have discussed vary in their storage efﬁciency\nand data-block access times. Both are important criteria in selecting the proper\nmethod or methods for an operating system to implement.",
  "Before selecting an allocation method, we need to determine how the\nsystems will be used. A system with mostly sequential access should not use\nthe same method as a system with mostly random access.\nFor any type of access, contiguous allocation requires only one access to get\na disk block. Since we can easily keep the initial address of the ﬁle in memory,\nwe can calculate immediately the disk address of the ith block (or the next\nblock) and read it directly.\nFor linked allocation, we can also keep the address of the next block in\nmemory and read it directly. This method is ﬁne for sequential access; for\ndirect access, however, an access to the ith block might require i disk reads. This\nproblem indicates why linked allocation should not be used for an application\nrequiring direct access.",
  "requiring direct access.\nAs a result, some systems support direct-access ﬁles by using contiguous\nallocation and sequential-access ﬁles by using linked allocation. For these\nsystems, the type of access to be made must be declared when the ﬁle is created.\nA ﬁle created for sequential access will be linked and cannot be used for direct\naccess. A ﬁle created for direct access will be contiguous and can support both\ndirect access and sequential access, but its maximum length must be declared\nwhen it is created. In this case, the operating system must have appropriate\ndata structures and algorithms to support both allocation methods. Files can be\nconverted from one type to another by the creation of a new ﬁle of the desired",
  "type, into which the contents of the old ﬁle are copied. The old ﬁle may then\nbe deleted and the new ﬁle renamed. 12.5\nFree-Space Management\n561\nIndexed allocation is more complex. If the index block is already in memory,\nthen the access can be made directly. However, keeping the index block in\nmemory requires considerable space. If this memory space is not available,\nthen we may have to read ﬁrst the index block and then the desired data\nblock. For a two-level index, two index-block reads might be necessary. For an\nextremely large ﬁle, accessing a block near the end of the ﬁle would require\nreading in all the index blocks before the needed data block ﬁnally could\nbe read. Thus, the performance of indexed allocation depends on the index",
  "structure, on the size of the ﬁle, and on the position of the block desired.\nSome systems combine contiguous allocation with indexed allocation by\nusing contiguous allocation for small ﬁles (up to three or four blocks) and\nautomatically switching to an indexed allocation if the ﬁle grows large. Since\nmost ﬁles are small, and contiguous allocation is efﬁcient for small ﬁles, average\nperformance can be quite good.\nMany other optimizations are in use. Given the disparity between CPU\nspeed and disk speed, it is not unreasonable to add thousands of extra\ninstructions to the operating system to save just a few disk-head movements.\nFurthermore, this disparity is increasing over time, to the point where hundreds\nof thousands of instructions could reasonably be used to optimize head\nmovements.",
  "movements.\n12.5 Free-Space Management\nSince disk space is limited, we need to reuse the space from deleted ﬁles for\nnew ﬁles, if possible. (Write-once optical disks allow only one write to any\ngiven sector, and thus reuse is not physically possible.) To keep track of free\ndisk space, the system maintains a free-space list. The free-space list records all\nfree disk blocks—those not allocated to some ﬁle or directory. To create a ﬁle,\nwe search the free-space list for the required amount of space and allocate that\nspace to the new ﬁle. This space is then removed from the free-space list. When\na ﬁle is deleted, its disk space is added to the free-space list. The free-space list,\ndespite its name, may not be implemented as a list, as we discuss next.\n12.5.1\nBit Vector",
  "12.5.1\nBit Vector\nFrequently, the free-space list is implemented as a bit map or bit vector. Each\nblock is represented by 1 bit. If the block is free, the bit is 1; if the block is\nallocated, the bit is 0.\nFor example, consider a disk where blocks 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 17,\n18, 25, 26, and 27 are free and the rest of the blocks are allocated. The free-space\nbit map would be\n001111001111110001100000011100000 ...\nThe main advantage of this approach is its relative simplicity and its\nefﬁciency in ﬁnding the ﬁrst free block or n consecutive free blocks on the\ndisk. Indeed, many computers supply bit-manipulation instructions that can\nbe used effectively for that purpose. One technique for ﬁnding the ﬁrst free",
  "block on a system that uses a bit-vector to allocate disk space is to sequentially\ncheck each word in the bit map to see whether that value is not 0, since a 562\nChapter 12\nFile-System Implementation\n0-valued word contains only 0 bits and represents a set of allocated blocks. The\nﬁrst non-0 word is scanned for the ﬁrst 1 bit, which is the location of the ﬁrst\nfree block. The calculation of the block number is\n(number of bits per word) × (number of 0-value words) + offset of ﬁrst 1 bit.\nAgain, we see hardware features driving software functionality. Unfor-\ntunately, bit vectors are inefﬁcient unless the entire vector is kept in main\nmemory (and is written to disk occasionally for recovery needs). Keeping it in\nmain memory is possible for smaller disks but not necessarily for larger ones.",
  "A 1.3-GB disk with 512-byte blocks would need a bit map of over 332 KB to\ntrack its free blocks, although clustering the blocks in groups of four reduces\nthis number to around 83 KB per disk. A 1-TB disk with 4-KB blocks requires 256\nMB to store its bit map. Given that disk size constantly increases, the problem\nwith bit vectors will continue to escalate as well.\n12.5.2\nLinked List\nAnother approach to free-space management is to link together all the free\ndisk blocks, keeping a pointer to the ﬁrst free block in a special location on the\ndisk and caching it in memory. This ﬁrst block contains a pointer to the next\nfree disk block, and so on. Recall our earlier example (Section 12.5.1), in which\nblocks 2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 17, 18, 25, 26, and 27 were free and the",
  "rest of the blocks were allocated. In this situation, we would keep a pointer to\nblock 2 as the ﬁrst free block. Block 2 would contain a pointer to block 3, which\nwould point to block 4, which would point to block 5, which would point to\nblock 8, and so on (Figure 12.10). This scheme is not efﬁcient; to traverse the\nlist, we must read each block, which requires substantial I/O time. Fortunately,\n0\n1\n2\n3\n4\n5\n7\n8\n9\n10\n11\n12\n13\n14\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n15\n6\nfree-space list head\nFigure 12.10\nLinked free-space list on disk. 12.5\nFree-Space Management\n563\nhowever, traversing the free list is not a frequent action. Usually, the operating\nsystem simply needs a free block so that it can allocate that block to a ﬁle, so",
  "the ﬁrst block in the free list is used. The FAT method incorporates free-block\naccounting into the allocation data structure. No separate method is needed.\n12.5.3\nGrouping\nA modiﬁcation of the free-list approach stores the addresses of n free blocks\nin the ﬁrst free block. The ﬁrst n−1 of these blocks are actually free. The last\nblock contains the addresses of another n free blocks, and so on. The addresses\nof a large number of free blocks can now be found quickly, unlike the situation\nwhen the standard linked-list approach is used.\n12.5.4\nCounting\nAnother approach takes advantage of the fact that, generally, several contigu-\nous blocks may be allocated or freed simultaneously, particularly when space\nis allocated with the contiguous-allocation algorithm or through clustering.",
  "Thus, rather than keeping a list of n free disk addresses, we can keep the\naddress of the ﬁrst free block and the number (n) of free contiguous blocks that\nfollow the ﬁrst block. Each entry in the free-space list then consists of a disk\naddress and a count. Although each entry requires more space than would a\nsimple disk address, the overall list is shorter, as long as the count is generally\ngreater than 1. Note that this method of tracking free space is similar to the\nextent method of allocating blocks. These entries can be stored in a balanced\ntree, rather than a linked list, for efﬁcient lookup, insertion, and deletion.\n12.5.5\nSpace Maps\nOracle’s ZFS ﬁle system (found in Solaris and other operating systems) was",
  "designed to encompass huge numbers of ﬁles, directories, and even ﬁle systems\n(in ZFS, we can create ﬁle-system hierarchies). On these scales, metadata I/O can\nhave a large performance impact. Consider, for example, that if the free-space\nlist is implemented as a bit map, bit maps must be modiﬁed both when blocks\nare allocated and when they are freed. Freeing 1 GB of data on a 1-TB disk could\ncause thousands of blocks of bit maps to be updated, because those data blocks\ncould be scattered over the entire disk. Clearly, the data structures for such a\nsystem could be large and inefﬁcient.\nIn its management of free space, ZFS uses a combination of techniques to\ncontrol the size of data structures and minimize the I/O needed to manage",
  "those structures. First, ZFS creates metaslabs to divide the space on the device\ninto chunks of manageable size. A given volume may contain hundreds of\nmetaslabs. Each metaslab has an associated space map. ZFS uses the counting\nalgorithm to store information about free blocks. Rather than write counting\nstructures to disk, it uses log-structured ﬁle-system techniques to record them.\nThe space map is a log of all block activity (allocating and freeing), in time\norder, in counting format. When ZFS decides to allocate or free space from a\nmetaslab, it loads the associated space map into memory in a balanced-tree\nstructure (for very efﬁcient operation), indexed by offset, and replays the log\ninto that structure. The in-memory space map is then an accurate representation",
  "of the allocated and free space in the metaslab. ZFS also condenses the map as 564\nChapter 12\nFile-System Implementation\nmuch as possible by combining contiguous free blocks into a single entry.\nFinally, the free-space list is updated on disk as part of the transaction-oriented\noperations of ZFS. During the collection and sorting phase, block requests can\nstill occur, and ZFS satisﬁes these requests from the log. In essence, the log plus\nthe balanced tree is the free list.\n12.6 Efﬁciency and Performance\nNow that we have discussed various block-allocation and directory-\nmanagement options, we can further consider their effect on performance\nand efﬁcient disk use. Disks tend to represent a major bottleneck in system\nperformance, since they are the slowest main computer component. In this",
  "section, we discuss a variety of techniques used to improve the efﬁciency and\nperformance of secondary storage.\n12.6.1\nEfﬁciency\nThe efﬁcient use of disk space depends heavily on the disk-allocation and\ndirectory algorithms in use. For instance, UNIX inodes are preallocated on\na volume. Even an empty disk has a percentage of its space lost to inodes.\nHowever, by preallocating the inodes and spreading them across the volume,\nwe improve the ﬁle system’s performance. This improved performance results\nfrom the UNIX allocation and free-space algorithms, which try to keep a ﬁle’s\ndata blocks near that ﬁle’s inode block to reduce seek time.\nAs another example, let’s reconsider the clustering scheme discussed in\nSection 12.4, which improves ﬁle-seek and ﬁle-transfer performance at the cost",
  "of internal fragmentation. To reduce this fragmentation, BSD UNIX varies the\ncluster size as a ﬁle grows. Large clusters are used where they can be ﬁlled, and\nsmall clusters are used for small ﬁles and the last cluster of a ﬁle. This system\nis described in Appendix A.\nThe types of data normally kept in a ﬁle’s directory (or inode) entry also\nrequire consideration. Commonly, a “last write date” is recorded to supply\ninformation to the user and to determine whether the ﬁle needs to be backed\nup. Some systems also keep a “last access date,” so that a user can determine\nwhen the ﬁle was last read. The result of keeping this information is that,\nwhenever the ﬁle is read, a ﬁeld in the directory structure must be written\nto. That means the block must be read into memory, a section changed, and",
  "the block written back out to disk, because operations on disks occur only in\nblock (or cluster) chunks. So any time a ﬁle is opened for reading, its directory\nentry must be read and written as well. This requirement can be inefﬁcient for\nfrequently accessed ﬁles, so we must weigh its beneﬁt against its performance\ncost when designing a ﬁle system. Generally, every data item associated with\na ﬁle needs to be considered for its effect on efﬁciency and performance.\nConsider, for instance, how efﬁciency is affected by the size of the pointers\nused to access data. Most systems use either 32-bit or 64-bit pointers throughout\nthe operating system. Using 32-bit pointers limits the size of a ﬁle to 232, or 4\nGB. Using 64-bit pointers allows very large ﬁle sizes, but 64-bit pointers require 12.6",
  "Efﬁciency and Performance\n565\nmore space to store. As a result, the allocation and free-space-management\nmethods (linked lists, indexes, and so on) use more disk space.\nOne of the difﬁculties in choosing a pointer size—or, indeed, any ﬁxed\nallocation size within an operating system—is planning for the effects of\nchanging technology. Consider that the IBM PC XT had a 10-MB hard drive\nand an MS-DOS ﬁle system that could support only 32 MB. (Each FAT entry\nwas 12 bits, pointing to an 8-KB cluster.) As disk capacities increased, larger\ndisks had to be split into 32-MB partitions, because the ﬁle system could not\ntrack blocks beyond 32 MB. As hard disks with capacities of over 100 MB became\ncommon, the disk data structures and algorithms in MS-DOS had to be modiﬁed",
  "to allow larger ﬁle systems. (Each FAT entry was expanded to 16 bits and later\nto 32 bits.) The initial ﬁle-system decisions were made for efﬁciency reasons;\nhowever, with the advent of MS-DOS Version 4, millions of computer users were\ninconvenienced when they had to switch to the new, larger ﬁle system. Solaris’\nZFS ﬁle system uses 128-bit pointers, which theoretically should never need\nto be extended. (The minimum mass of a device capable of storing 2128 bytes\nusing atomic-level storage would be about 272 trillion kilograms.)\nAs another example, consider the evolution of the Solaris operating system.\nOriginally, many data structures were of ﬁxed length, allocated at system\nstartup. These structures included the process table and the open-ﬁle table.",
  "When the process table became full, no more processes could be created. When\nthe ﬁle table became full, no more ﬁles could be opened. The system would fail\nto provide services to users. Table sizes could be increased only by recompiling\nthe kernel and rebooting the system. With later releases of Solaris, almost all\nkernel structures were allocated dynamically, eliminating these artiﬁcial limits\non system performance. Of course, the algorithms that manipulate these tables\nare more complicated, and the operating system is a little slower because it\nmust dynamically allocate and deallocate table entries; but that price is the\nusual one for more general functionality.\n12.6.2\nPerformance\nEven after the basic ﬁle-system algorithms have been selected, we can still",
  "improve performance in several ways. As will be discussed in Chapter 13,\nmost disk controllers include local memory to form an on-board cache that is\nlarge enough to store entire tracks at a time. Once a seek is performed, the\ntrack is read into the disk cache starting at the sector under the disk head\n(reducing latency time). The disk controller then transfers any sector requests\nto the operating system. Once blocks make it from the disk controller into main\nmemory, the operating system may cache the blocks there.\nSome systems maintain a separate section of main memory for a buffer\ncache, where blocks are kept under the assumption that they will be used\nagain shortly. Other systems cache ﬁle data using a page cache. The page",
  "cache uses virtual memory techniques to cache ﬁle data as pages rather than\nas ﬁle-system-oriented blocks. Caching ﬁle data using virtual addresses is far\nmore efﬁcient than caching through physical disk blocks, as accesses interface\nwith virtual memory rather than the ﬁle system. Several systems—including\nSolaris, Linux, and Windows —use page caching to cache both process pages\nand ﬁle data. This is known as uniﬁed virtual memory. 566\nChapter 12\nFile-System Implementation\nmemory-mapped I/O\nI/O using\nread( ) and write( )\npage cache\nbuffer cache\nfile system\nFigure 12.11\nI/O without a uniﬁed buffer cache.\nSome versions of UNIX and Linux provide a uniﬁed buffer cache. To\nillustrate the beneﬁts of the uniﬁed buffer cache, consider the two alternatives",
  "for opening and accessing a ﬁle. One approach is to use memory mapping\n(Section 9.7); the second is to use the standard system calls read() and\nwrite(). Without a uniﬁed buffer cache, we have a situation similar to Figure\n12.11. Here, the read() and write() system calls go through the buffer cache.\nThe memory-mapping call, however, requires using two caches—the page\ncache and the buffer cache. A memory mapping proceeds by reading in disk\nblocks from the ﬁle system and storing them in the buffer cache. Because the\nvirtual memory system does not interface with the buffer cache, the contents\nof the ﬁle in the buffer cache must be copied into the page cache. This situation,\nknown as double caching, requires caching ﬁle-system data twice. Not only",
  "does it waste memory but it also wastes signiﬁcant CPU and I/O cycles due to\nthe extra data movement within system memory. In addition, inconsistencies\nbetween the two caches can result in corrupt ﬁles. In contrast, when a uniﬁed\nbuffer cache is provided, both memory mapping and the read() and write()\nsystem calls use the same page cache. This has the beneﬁt of avoiding double\ncaching, and it allows the virtual memory system to manage ﬁle-system data.\nThe uniﬁed buffer cache is shown in Figure 12.12.\nRegardless of whether we are caching disk blocks or pages (or both), LRU\n(Section 9.4.4) seems a reasonable general-purpose algorithm for block or page\nreplacement. However, the evolution of the Solaris page-caching algorithms",
  "reveals the difﬁculty in choosing an algorithm. Solaris allows processes and the\npage cache to share unused memory. Versions earlier than Solaris 2.5.1 made\nno distinction between allocating pages to a process and allocating them to\nthe page cache. As a result, a system performing many I/O operations used\nmost of the available memory for caching pages. Because of the high rates of\nI/O, the page scanner (Section 9.10.2) reclaimed pages from processes—rather\nthan from the page cache—when free memory ran low. Solaris 2.6 and Solaris\n7 optionally implemented priority paging, in which the page scanner gives 12.6\nEfﬁciency and Performance\n567\nmemory-mapped I/O\nI/O using\nread( ) and write( )\nbuffer cache\nfile system\nFigure 12.12\nI/O using a uniﬁed buffer cache.",
  "Figure 12.12\nI/O using a uniﬁed buffer cache.\npriority to process pages over the page cache. Solaris 8 applied a ﬁxed limit to\nprocess pages and the ﬁle-system page cache, preventing either from forcing\nthe other out of memory. Solaris 9 and 10 again changed the algorithms to\nmaximize memory use and minimize thrashing.\nAnother issue that can affect the performance of I/O is whether writes to\nthe ﬁle system occur synchronously or asynchronously. Synchronous writes\noccur in the order in which the disk subsystem receives them, and the writes are\nnot buffered. Thus, the calling routine must wait for the data to reach the disk\ndrive before it can proceed. In an asynchronous write, the data are stored in\nthe cache, and control returns to the caller. Most writes are asynchronous.",
  "However, metadata writes, among others, can be synchronous. Operating\nsystems frequently include a ﬂag in the open system call to allow a process to\nrequest that writes be performed synchronously. For example, databases use\nthis feature for atomic transactions, to assure that data reach stable storage in\nthe required order.\nSome systems optimize their page cache by using different replacement\nalgorithms, depending on the access type of the ﬁle. A ﬁle being read or\nwritten sequentially should not have its pages replaced in LRU order, because\nthe most recently used page will be used last, or perhaps never again. Instead,\nsequential access can be optimized by techniques known as free-behind and\nread-ahead. Free-behind removes a page from the buffer as soon as the next",
  "page is requested. The previous pages are not likely to be used again and\nwaste buffer space. With read-ahead, a requested page and several subsequent\npages are read and cached. These pages are likely to be requested after the\ncurrent page is processed. Retrieving these data from the disk in one transfer\nand caching them saves a considerable amount of time. One might think that\na track cache on the controller would eliminate the need for read-ahead on a\nmultiprogrammed system. However, because of the high latency and overhead\ninvolved in making many small transfers from the track cache to main memory,\nperforming a read-ahead remains beneﬁcial.\nThe page cache, the ﬁle system, and the disk drivers have some interesting",
  "interactions. When data are written to a disk ﬁle, the pages are buffered in the\ncache, and the disk driver sorts its output queue according to disk address.\nThese two actions allow the disk driver to minimize disk-head seeks and to 568\nChapter 12\nFile-System Implementation\nwrite data at times optimized for disk rotation. Unless synchronous writes are\nrequired, a process writing to disk simply writes into the cache, and the system\nasynchronously writes the data to disk when convenient. The user process sees\nvery fast writes. When data are read from a disk ﬁle, the block I/O system does\nsome read-ahead; however, writes are much more nearly asynchronous than\nare reads. Thus, output to the disk through the ﬁle system is often faster than\nis input for large transfers, counter to intuition.",
  "12.7 Recovery\nFiles and directories are kept both in main memory and on disk, and care must\nbe taken to ensure that a system failure does not result in loss of data or in data\ninconsistency. We deal with these issues in this section. We also consider how\na system can recover from such a failure.\nA system crash can cause inconsistencies among on-disk ﬁle-system data\nstructures, such as directory structures, free-block pointers, and free FCB\npointers. Many ﬁle systems apply changes to these structures in place. A\ntypical operation, such as creating a ﬁle, can involve many structural changes\nwithin the ﬁle system on the disk. Directory structures are modiﬁed, FCBs are\nallocated, data blocks are allocated, and the free counts for all of these blocks",
  "are decreased. These changes can be interrupted by a crash, and inconsistencies\namong the structures can result. For example, the free FCB count might indicate\nthat an FCB had been allocated, but the directory structure might not point to\nthe FCB. Compounding this problem is the caching that operating systems do\nto optimize I/O performance. Some changes may go directly to disk, while\nothers may be cached. If the cached changes do not reach disk before a crash\noccurs, more corruption is possible.\nIn addition to crashes, bugs in ﬁle-system implementation, disk controllers,\nand even user applications can corrupt a ﬁle system. File systems have varying\nmethods to deal with corruption, depending on the ﬁle-system data structures\nand algorithms. We deal with these issues next.\n12.7.1",
  "12.7.1\nConsistency Checking\nWhatever the cause of corruption, a ﬁle system must ﬁrst detect the problems\nand then correct them. For detection, a scan of all the metadata on each ﬁle\nsystem can conﬁrm or deny the consistency of the system. Unfortunately, this\nscan can take minutes or hours and should occur every time the system boots.\nAlternatively, a ﬁle system can record its state within the ﬁle-system metadata.\nAt the start of any metadata change, a status bit is set to indicate that the\nmetadata is in ﬂux. If all updates to the metadata complete successfully, the ﬁle\nsystem can clear that bit. If, however, the status bit remains set, a consistency\nchecker is run.\nThe consistency checker—a systems program such as fsck in UNIX—",
  "compares the data in the directory structure with the data blocks on disk\nand tries to ﬁx any inconsistencies it ﬁnds. The allocation and free-space-\nmanagement algorithms dictate what types of problems the checker can ﬁnd\nand how successful it will be in ﬁxing them. For instance, if linked allocation is\nused and there is a link from any block to its next block, then the entire ﬁle can be 12.7\nRecovery\n569\nreconstructed from the data blocks, and the directory structure can be recreated.\nIn contrast, the loss of a directory entry on an indexed allocation system can\nbe disastrous, because the data blocks have no knowledge of one another. For\nthis reason, UNIX caches directory entries for reads; but any write that results",
  "in space allocation, or other metadata changes, is done synchronously, before\nthe corresponding data blocks are written. Of course, problems can still occur\nif a synchronous write is interrupted by a crash.\n12.7.2\nLog-Structured File Systems\nComputer scientists often ﬁnd that algorithms and technologies originally\nused in one area are equally useful in other areas. Such is the case with the\ndatabase log-based recovery algorithms. These logging algorithms have been\napplied successfully to the problem of consistency checking. The resulting\nimplementations are known as log-based transaction-oriented (or journaling)\nﬁle systems.\nNote that with the consistency-checking approach discussed in the pre-\nceding section, we essentially allow structures to break and repair them on",
  "recovery. However, there are several problems with this approach. One is that\nthe inconsistency may be irreparable. The consistency check may not be able to\nrecover the structures, resulting in loss of ﬁles and even entire directories.\nConsistency checking can require human intervention to resolve conﬂicts,\nand that is inconvenient if no human is available. The system can remain\nunavailable until the human tells it how to proceed. Consistency checking also\ntakes system and clock time. To check terabytes of data, hours of clock time\nmay be required.\nThe solution to this problem is to apply log-based recovery techniques to\nﬁle-system metadata updates. Both NTFS and the Veritas ﬁle system use this\nmethod, and it is included in recent versions of UFS on Solaris. In fact, it is",
  "becoming common on many operating systems.\nFundamentally, all metadata changes are written sequentially to a log.\nEach set of operations for performing a speciﬁc task is a transaction. Once\nthe changes are written to this log, they are considered to be committed,\nand the system call can return to the user process, allowing it to continue\nexecution. Meanwhile, these log entries are replayed across the actual ﬁle-\nsystem structures. As the changes are made, a pointer is updated to indicate\nwhich actions have completed and which are still incomplete. When an entire\ncommitted transaction is completed, it is removed from the log ﬁle, which is\nactually a circular buffer. A circular buffer writes to the end of its space and",
  "then continues at the beginning, overwriting older values as it goes. We would\nnot want the buffer to write over data that had not yet been saved, so that\nscenario is avoided. The log may be in a separate section of the ﬁle system or\neven on a separate disk spindle. It is more efﬁcient, but more complex, to have\nit under separate read and write heads, thereby decreasing head contention\nand seek times.\nIf the system crashes, the log ﬁle will contain zero or more transactions.\nAny transactions it contains were not completed to the ﬁle system, even though\nthey were committed by the operating system, so they must now be completed.\nThe transactions can be executed from the pointer until the work is complete 570\nChapter 12\nFile-System Implementation",
  "Chapter 12\nFile-System Implementation\nso that the ﬁle-system structures remain consistent. The only problem occurs\nwhen a transaction was aborted—that is, was not committed before the system\ncrashed. Any changes from such a transaction that were applied to the ﬁle\nsystem must be undone, again preserving the consistency of the ﬁle system.\nThis recovery is all that is needed after a crash, eliminating any problems with\nconsistency checking.\nA side beneﬁt of using logging on disk metadata updates is that those\nupdates proceed much faster than when they are applied directly to the on-\ndisk data structures. The reason is found in the performance advantage of\nsequential I/O over random I/O. The costly synchronous random metadata",
  "writes are turned into much less costly synchronous sequential writes to the\nlog-structured ﬁle system’s logging area. Those changes, in turn, are replayed\nasynchronously via random writes to the appropriate structures. The overall\nresult is a signiﬁcant gain in performance of metadata-oriented operations,\nsuch as ﬁle creation and deletion.\n12.7.3\nOther Solutions\nAnother alternative to consistency checking is employed by Network Appli-\nance’s WAFL ﬁle system and the Solaris ZFS ﬁle system. These systems never\noverwrite blocks with new data. Rather, a transaction writes all data and meta-\ndata changes to new blocks. When the transaction is complete, the metadata\nstructures that pointed to the old versions of these blocks are updated to point",
  "to the new blocks. The ﬁle system can then remove the old pointers and the old\nblocks and make them available for reuse. If the old pointers and blocks are\nkept, a snapshot is created; the snapshot is a view of the ﬁle system before the\nlast update took place. This solution should require no consistency checking if\nthe pointer update is done atomically. WAFL does have a consistency checker,\nhowever, so some failure scenarios can still cause metadata corruption. (See\nSection 12.9 for details of the WAFL ﬁle system.)\nZFS takes an even more innovative approach to disk consistency. It never\noverwrites blocks, just like WAFL. However, ZFS goes further and provides\nchecksumming of all metadata and data blocks. This solution (when combined",
  "with RAID) assures that data are always correct. ZFStherefore has no consistency\nchecker. (More details on ZFS are found in Section 10.7.6.)\n12.7.4\nBackup and Restore\nMagnetic disks sometimes fail, and care must be taken to ensure that the data\nlost in such a failure are not lost forever. To this end, system programs can be\nused to back up data from disk to another storage device, such as a magnetic\ntape or other hard disk. Recovery from the loss of an individual ﬁle, or of an\nentire disk, may then be a matter of restoring the data from backup.\nTo minimize the copying needed, we can use information from each ﬁle’s\ndirectory entry. For instance, if the backup program knows when the last\nbackup of a ﬁle was done, and the ﬁle’s last write date in the directory indicates",
  "that the ﬁle has not changed since that date, then the ﬁle does not need to be\ncopied again. A typical backup schedule may then be as follows:\n• Day 1. Copy to a backup medium all ﬁles from the disk. This is called a\nfull backup. 12.8\nNFS\n571\n• Day 2. Copy to another medium all ﬁles changed since day 1. This is an\nincremental backup.\n• Day 3. Copy to another medium all ﬁles changed since day 2.\n.\n.\n.\n• Day N. Copy to another medium all ﬁles changed since day N−1. Then\ngo back to day 1.\nThe new cycle can have its backup written over the previous set or onto a\nnew set of backup media.\nUsing this method, we can restore an entire disk by starting restores with\nthe full backup and continuing through each of the incremental backups. Of",
  "course, the larger the value of N, the greater the number of media that must be\nread for a complete restore. An added advantage of this backup cycle is that\nwe can restore any ﬁle accidentally deleted during the cycle by retrieving the\ndeleted ﬁle from the backup of the previous day.\nThe length of the cycle is a compromise between the amount of backup\nmedium needed and the number of days covered by a restore. To decrease the\nnumber of tapes that must be read to do a restore, an option is to perform a\nfull backup and then each day back up all ﬁles that have changed since the\nfull backup. In this way, a restore can be done via the most recent incremental\nbackup and the full backup, with no other incremental backups needed. The",
  "trade-off is that more ﬁles will be modiﬁed each day, so each successive\nincremental backup involves more ﬁles and more backup media.\nA user may notice that a particular ﬁle is missing or corrupted long after\nthe damage was done. For this reason, we usually plan to take a full backup\nfrom time to time that will be saved “forever.” It is a good idea to store these\npermanent backups far away from the regular backups to protect against\nhazard, such as a ﬁre that destroys the computer and all the backups too.\nAnd if the backup cycle reuses media, we must take care not to reuse the\nmedia too many times—if the media wear out, it might not be possible to\nrestore any data from the backups.\n12.8 NFS\nNetwork ﬁle systems are commonplace. They are typically integrated with",
  "the overall directory structure and interface of the client system. NFS is a good\nexample of a widely used, well implemented client–server network ﬁle system.\nHere, we use it as an example to explore the implementation details of network\nﬁle systems.\nNFS is both an implementation and a speciﬁcation of a software system for\naccessing remote ﬁles across LANs (or even WANs). NFS is part of ONC+, which\nmost UNIX vendors and some PC operating systems support. The implementa-\ntion described here is part of the Solaris operating system, which is a modiﬁed\nversion of UNIX SVR4. It uses either the TCP or UDP/IP protocol (depending on 572\nChapter 12\nFile-System Implementation\nlocal\nusr\nshared\ndir1\nusr\nU:\nS1:\nS2:\ndir2\nusr\nFigure 12.13\nThree independent ﬁle systems.",
  "usr\nFigure 12.13\nThree independent ﬁle systems.\nthe interconnecting network). The speciﬁcation and the implementation are\nintertwined in our description of NFS. Whenever detail is needed, we refer to\nthe Solaris implementation; whenever the description is general, it applies to\nthe speciﬁcation also.\nThere are multiple versions of NFS, with the latest being Version 4. Here,\nwe describe Version 3, as that is the one most commonly deployed.\n12.8.1\nOverview\nNFS views aset ofinterconnected workstations as aset ofindependent machines\nwith independent ﬁle systems. The goal is to allow some degree of sharing\namong these ﬁle systems (on explicit request) in a transparent manner. Sharing\nis based on a client–server relationship. A machine may be, and often is, both a",
  "client and a server. Sharing is allowed between any pair of machines. To ensure\nmachine independence, sharing of a remote ﬁle system affects only the client\nmachine and no other machine.\nSo that a remote directory will be accessible in a transparent manner\nfrom a particular machine—say, from M1—a client of that machine must\nﬁrst carry out a mount operation. The semantics of the operation involve\nmounting a remote directory over a directory of a local ﬁle system. Once the\nmount operation is completed, the mounted directory looks like an integral\nsubtree of the local ﬁle system, replacing the subtree descending from the\nlocal directory. The local directory becomes the name of the root of the newly\nmounted directory. Speciﬁcation of the remote directory as an argument for the",
  "mount operation is not done transparently; the location (or host name) of the\nremote directory has to be provided. However, from then on, users on machine\nM1 can access ﬁles in the remote directory in a totally transparent manner.\nTo illustrate ﬁle mounting, consider the ﬁle system depicted in Figure 12.13,\nwhere the triangles represent subtrees of directories that are of interest. The\nﬁgure shows three independent ﬁle systems of machines named U, S1, and\nS2. At this point, on each machine, only the local ﬁles can be accessed. Figure\n12.14(a) shows the effects of mounting S1:/usr/shared over U:/usr/local.\nThis ﬁgure depicts the view users on U have of their ﬁle system. After the\nmount is complete, they can access any ﬁle within the dir1 directory using the 12.8\nNFS\n573\nlocal\ndir1\ndir1\nusr",
  "NFS\n573\nlocal\ndir1\ndir1\nusr\nU:\nU:\n(a)\n(b)\nlocal\nusr\nFigure 12.14\nMounting in NFS. (a) Mounts. (b) Cascading mounts.\npreﬁx /usr/local/dir1. The original directory /usr/local on that machine\nis no longer visible.\nSubject to access-rights accreditation, any ﬁle system, or any directory\nwithin a ﬁle system, can be mounted remotely on top of any local directory.\nDiskless workstations can even mount their own roots from servers. Cascading\nmounts are also permitted in some NFS implementations. That is, a ﬁle system\ncan be mounted over another ﬁle system that is remotely mounted, not local. A\nmachine is affected by only those mounts that it has itself invoked. Mounting a\nremote ﬁle system does not give the client access to other ﬁle systems that were,",
  "by chance, mounted over the former ﬁle system. Thus, the mount mechanism\ndoes not exhibit a transitivity property.\nIn Figure 12.14(b), we illustrate cascading mounts. The ﬁgure shows the\nresult of mounting S2:/usr/dir2 over U:/usr/local/dir1, which is already\nremotely mounted from S1. Users can access ﬁles within dir2 on U using the\npreﬁx /usr/local/dir1. If a shared ﬁle system is mounted over a user’s home\ndirectories on all machines in a network, the user can log into any workstation\nand get their home environment. This property permits user mobility.\nOne of the design goals of NFS was to operate in a heterogeneous envi-\nronment of different machines, operating systems, and network architectures.\nThe NFS speciﬁcation is independent of these media. This independence is",
  "achieved through the use of RPC primitives built on top of an external data\nrepresentation (XDR) protocol used between two implementation-independent\ninterfaces. Hence, if the system’s heterogeneous machines and ﬁle systems are\nproperly interfaced to NFS, ﬁle systems of different types can be mounted both\nlocally and remotely.\nThe NFS speciﬁcation distinguishes between the services provided by a\nmount mechanism and the actual remote-ﬁle-access services. Accordingly, two\nseparate protocols are speciﬁed for these services: a mount protocol and a\nprotocol for remote ﬁle accesses, the NFS protocol. The protocols are speciﬁed as\nsets of RPCs. These RPCs are the building blocks used to implement transparent\nremote ﬁle access. 574\nChapter 12\nFile-System Implementation\n12.8.2\nThe Mount Protocol",
  "12.8.2\nThe Mount Protocol\nThe mount protocol establishes the initial logical connection between a server\nand a client. In Solaris, each machine has a server process, outside the kernel,\nperforming the protocol functions.\nA mount operation includes the name of the remote directory to be\nmounted and the name of the server machine storing it. The mount request\nis mapped to the corresponding RPC and is forwarded to the mount server\nrunning on the speciﬁc server machine. The server maintains an export list\nthat speciﬁes local ﬁle systems that it exports for mounting, along with names\nof machines that are permitted to mount them. (In Solaris, this list is the\n/etc/dfs/dfstab, which can be edited only by a superuser.) The speciﬁcation",
  "can also include access rights, such as read only. To simplify the maintenance\nof export lists and mount tables, a distributed naming scheme can be used to\nhold this information and make it available to appropriate clients.\nRecall that any directory within an exported ﬁle system can be mounted\nremotely by an accredited machine. A component unit is such a directory. When\nthe server receives a mount request that conforms to its export list, it returns to\nthe client a ﬁle handle that serves as the key for further accesses to ﬁles within\nthe mounted ﬁle system. The ﬁle handle contains all the information that the\nserver needs to distinguish an individual ﬁle it stores. In UNIX terms, the ﬁle\nhandle consists of a ﬁle-system identiﬁer and an inode number to identify the",
  "exact mounted directory within the exported ﬁle system.\nThe serveralsomaintainsalistofthe clientmachinesand the corresponding\ncurrently mounted directories. This list is used mainly for administrative\npurposes—for instance, for notifying all clients that the server is going down.\nOnly through addition and deletion of entries in this list can the server state\nbe affected by the mount protocol.\nUsually, a system has a static mounting preconﬁguration that is established\nat boot time (/etc/vfstab in Solaris); however, this layout can be modiﬁed. In\naddition to the actual mount procedure, the mount protocol includes several\nother procedures, such as unmount and return export list.\n12.8.3\nThe NFS Protocol\nThe NFS protocol provides a set of RPCs for remote ﬁle operations. The",
  "procedures support the following operations:\n• Searching for a ﬁle within a directory\n• Reading a set of directory entries\n• Manipulating links and directories\n• Accessing ﬁle attributes\n• Reading and writing ﬁles\nThese procedures can be invoked only after a ﬁle handle for the remotely\nmounted directory has been established.\nThe omission of open and close operations is intentional. A prominent\nfeature of NFS servers is that they are stateless. Servers do not maintain\ninformation about their clients from one access to another. No parallels to 12.8\nNFS\n575\nUNIX’s open-ﬁles table or ﬁle structures exist on the server side. Consequently,\neach request has to provide a full set of arguments, including a unique ﬁle\nidentiﬁer and an absolute offset inside the ﬁle for the appropriate operations.",
  "The resulting design is robust; no special measures need be taken to recover\na server after a crash. File operations must be idempotent for this purpose,\nthat is, the same operation performed multiple times has the same effect as\nif it were only performed once. To achieve idempotence, every NFS request\nhas a sequence number, allowing the server to determine if a request has been\nduplicated or if any are missing.\nMaintaining the list of clients that we mentioned seems to violate the\nstatelessness of the server. However, this list is not essential for the correct\noperation of the client or the server, and hence it does not need to be restored\nafter a server crash. Consequently, it may include inconsistent data and is\ntreated as only a hint.",
  "treated as only a hint.\nA further implication of the stateless-server philosophy and a result of the\nsynchrony of an RPC is that modiﬁed data (including indirection and status\nblocks) must be committed to the server’s disk before results are returned to\nthe client. That is, a client can cache write blocks, but when it ﬂushes them to the\nserver, it assumes that they have reached the server’s disks. The server must\nwrite all NFS data synchronously. Thus, a server crash and recovery will be\ninvisible to a client; all blocks that the server is managing for the client will be\nintact. The resulting performance penalty can be large, because the advantages\nof caching are lost. Performance can be increased by using storage with its own",
  "nonvolatile cache (usually battery-backed-up memory). The disk controller\nacknowledges the disk write when the write is stored in the nonvolatile cache.\nIn essence, the host sees a very fast synchronous write. These blocks remain\nintact even after a system crash and are written from this stable storage to disk\nperiodically.\nA single NFS write procedure call is guaranteed to be atomic and is not\nintermixed with other write calls to the same ﬁle. The NFS protocol, however,\ndoes not provide concurrency-control mechanisms. A write() system call may\nbe broken down into several RPC writes, because each NFS write or read call\ncan contain up to 8 KB of data and UDP packets are limited to 1,500 bytes. As a\nresult, two users writing to the same remote ﬁle may get their data intermixed.",
  "The claim is that, because lock management is inherently stateful, a service\noutside the NFS should provide locking (and Solaris does). Users are advised\nto coordinate access to shared ﬁles using mechanisms outside the scope of NFS.\nNFS is integrated into the operating system via a VFS. As an illustration\nof the architecture, let’s trace how an operation on an already-open remote\nﬁle is handled (follow the example in Figure 12.15). The client initiates the\noperation with a regular system call. The operating-system layer maps this\ncall to a VFS operation on the appropriate vnode. The VFS layer identiﬁes the\nﬁle as a remote one and invokes the appropriate NFS procedure. An RPC call\nis made to the NFS service layer at the remote server. This call is reinjected to",
  "the VFS layer on the remote system, which ﬁnds that it is local and invokes\nthe appropriate ﬁle-system operation. This path is retraced to return the result.\nAn advantage of this architecture is that the client and the server are identical;\nthus, a machine may be a client, or a server, or both. The actual service on each\nserver is performed by kernel threads. 576\nChapter 12\nFile-System Implementation\ndisk\ndisk\nsystem-calls interface\nclient\nserver\nother types of\nfile systems\nUNIX file\nsystem\nUNIX file\nsystem\nNFS\nclient\nRPC/XDR\nnetwork\nRPC/XDR\nNFS\nserver\nVFS interface\nVFS interface\nFigure 12.15\nSchematic view of the NFS architecture.\n12.8.4\nPath-Name Translation\nPath-name translation in NFS involves the parsing of a path name such as",
  "/usr/local/dir1/file.txt into separate directory entries, or components:\n(1) usr, (2) local, and (3) dir1. Path-name translation is done by breaking the\npath into component names and performing a separate NFS lookup call for\nevery pair of component name and directory vnode. Once a mount point is\ncrossed, every component lookup causes a separate RPC to the server. This\nexpensive path-name-traversal scheme is needed, since the layout of each\nclient’s logical name space is unique, dictated by the mounts the client has\nperformed. It would be much more efﬁcient to hand a server a path name\nand receive a target vnode once a mount point is encountered. At any point,\nhowever, there might be another mount point for the particular client of which\nthe stateless server is unaware.",
  "the stateless server is unaware.\nSo that lookup is fast, a directory-name-lookup cache on the client side\nholds the vnodes for remote directory names. This cache speeds up references\nto ﬁles with the same initial path name. The directory cache is discarded when\nattributes returned from the server do not match the attributes of the cached\nvnode.\nRecall that some implementations of NFS allow mounting a remote ﬁle\nsystem on top of another already-mounted remote ﬁle system (a cascading\nmount). When a client has a cascading mount, more than one server can be\ninvolved in a path-name traversal. However, when a client does a lookup on\na directory on which the server has mounted a ﬁle system, the client sees the\nunderlying directory instead of the mounted directory. 12.9",
  "Example: The WAFL File System\n577\n12.8.5\nRemote Operations\nWith the exception of opening and closing ﬁles, there is an almost one-to-one\ncorrespondence between the regular UNIX system calls for ﬁle operations and\nthe NFS protocol RPCs. Thus, a remote ﬁle operation can be translated directly\nto the corresponding RPC. Conceptually, NFS adheres to the remote-service\nparadigm; but in practice, buffering and caching techniques are employed for\nthe sake of performance. No direct correspondence exists between a remote\noperation and an RPC. Instead, ﬁle blocks and ﬁle attributes are fetched by the\nRPCs and are cached locally. Future remote operations use the cached data,\nsubject to consistency constraints.\nThere are two caches: the ﬁle-attribute (inode-information) cache and the",
  "ﬁle-blocks cache. When a ﬁle is opened, the kernel checks with the remote\nserver to determine whether to fetch or revalidate the cached attributes. The\ncached ﬁle blocks are used only if the corresponding cached attributes are up\nto date. The attribute cache is updated whenever new attributes arrive from\nthe server. Cached attributes are, by default, discarded after 60 seconds. Both\nread-ahead and delayed-write techniques are used between the server and the\nclient. Clients do not free delayed-write blocks until the server conﬁrms that\nthe data have been written to disk. Delayed-write is retained even when a ﬁle\nis opened concurrently, in conﬂicting modes. Hence, UNIX semantics (Section\n11.5.3.1) are not preserved.\nTuning the system for performance makes it difﬁcult to characterize the",
  "consistency semantics of NFS. New ﬁles created on a machine may not be\nvisible elsewhere for 30 seconds. Furthermore, writes to a ﬁle at one site may\nor may not be visible at other sites that have this ﬁle open for reading. New\nopens of a ﬁle observe only the changes that have already been ﬂushed to the\nserver. Thus, NFS provides neither strict emulation of UNIX semantics nor the\nsession semantics of Andrew (Section 11.5.3.2). In spite of these drawbacks, the\nutility and good performance of the mechanism make it the most widely used\nmulti-vendor-distributed system in operation.\n12.9 Example: The WAFL File System\nBecause disk I/O has such a huge impact on system performance, ﬁle-system\ndesign and implementation command quite a lot of attention from system",
  "designers. Some ﬁle systems are general purpose, in that they can provide\nreasonable performance and functionality for a wide variety of ﬁle sizes, ﬁle\ntypes, and I/O loads. Others are optimized for speciﬁc tasks in an attempt to\nprovide better performance in those areas than general-purpose ﬁle systems.\nThe write-anywhere ﬁle layout (WAFL) from Network Appliance is an example\nof this sort of optimization. WAFL is a powerful, elegant ﬁle system optimized\nfor random writes.\nWAFL is used exclusively on network ﬁle servers produced by Network\nAppliance and is meant for use as a distributed ﬁle system. It can provide ﬁles\nto clients via the NFS, CIFS, ftp, and http protocols, although it was designed\njust for NFS and CIFS. When many clients use these protocols to talk to a ﬁle",
  "server, the server may see a very large demand for random reads and an even\nlarger demand for random writes. The NFS and CIFS protocols cache data from\nread operations, so writes are of the greatest concern to ﬁle-server creators. 578\nChapter 12\nFile-System Implementation\nWAFL is used on ﬁle servers that include an NVRAM cache for writes.\nThe WAFL designers took advantage of running on a speciﬁc architecture to\noptimize the ﬁle system for random I/O, with a stable-storage cache in front.\nEase of use is one of the guiding principles of WAFL. Its creators also designed it\nto include a new snapshot functionality that creates multiple read-only copies\nof the ﬁle system at different points in time, as we shall see.\nThe ﬁle system is similar to the Berkeley Fast File System, with many",
  "modiﬁcations. It is block-based and uses inodes to describe ﬁles. Each inode\ncontains 16 pointers to blocks (or indirect blocks) belonging to the ﬁle described\nby the inode. Each ﬁle system has a root inode. All of the metadata lives in\nﬁles. All inodes are in one ﬁle, the free-block map in another, and the free-inode\nmap in a third, as shown in Figure 12.16. Because these are standard ﬁles, the\ndata blocks are not limited in location and can be placed anywhere. If a ﬁle\nsystem is expanded by addition of disks, the lengths of the metadata ﬁles are\nautomatically expanded by the ﬁle system.\nThus, a WAFL ﬁle system is a tree of blocks with the root inode as its\nbase. To take a snapshot, WAFL creates a copy of the root inode. Any ﬁle or",
  "metadata updates after that go to new blocks rather than overwriting their\nexisting blocks. The new root inode points to metadata and data changed as a\nresult of these writes. Meanwhile, the snapshot (the old root inode) still points\nto the old blocks, which have not been updated. It therefore provides access to\nthe ﬁle system just as it was at the instant the snapshot was made—and takes\nvery little disk space to do so. In essence, the extra disk space occupied by a\nsnapshot consists of just the blocks that have been modiﬁed since the snapshot\nwas taken.\nAn important change from more standard ﬁle systems is that the free-block\nmap has more than one bit per block. It is a bitmap with a bit set for each\nsnapshot that is using the block. When all snapshots that have been using the",
  "block are deleted, the bit map for that block is all zeros, and the block is free to\nbe reused. Used blocks are never overwritten, so writes are very fast, because\na write can occur at the free block nearest the current head location. There are\nmany other performance optimizations in WAFL as well.\nMany snapshots can exist simultaneously, so one can be taken each hour\nof the day and each day of the month. A user with access to these snapshots\ncan access ﬁles as they were at any of the times the snapshots were taken.\nThe snapshot facility is also useful for backups, testing, versioning, and so on.\nfree block map\nfree inode map\nfile in the file system...\nroot inode\ninode file\n•••\n•••\n•••\nFigure 12.16\nThe WAFL ﬁle layout. 12.9\nExample: The WAFL File System\n579\nblock A\nB\nC\nD\nE\nroot inode",
  "579\nblock A\nB\nC\nD\nE\nroot inode\n(a) Before a snapshot.\nblock A\nB\nC\nD\nE\nroot inode\n(b) After a snapshot, before any blocks change.\nnew snapshot\nblock A\nB\nC\nD\nD´\nE\nroot inode\n(c) After block D has changed to D´.\nnew snapshot\nFigure 12.17\nSnapshots in WAFL.\nWAFL’s snapshot facility is very efﬁcient in that it does not even require that\ncopy-on-write copies of each data block be taken before the block is modiﬁed.\nOther ﬁle systems provide snapshots, but frequently with less efﬁciency. WAFL\nsnapshots are depicted in Figure 12.17.\nNewer versions of WAFL actually allow read–write snapshots, known as\nclones. Clones are also efﬁcient, using the same techniques as shapshots. In\nthis case, a read-only snapshot captures the state of the ﬁle system, and a clone",
  "refers back to that read-only snapshot. Any writes to the clone are stored in\nnew blocks, and the clone’s pointers are updated to refer to the new blocks.\nThe original snapshot is unmodiﬁed, still giving a view into the ﬁle system as\nit was before the clone was updated. Clones can also be promoted to replace\nthe original ﬁle system; this involves throwing out all of the old pointers and\nany associated old blocks. Clones are useful for testing and upgrades, as the\noriginal version is left untouched and the clone deleted when the test is done\nor if the upgrade fails.\nAnother feature that naturally results from the WAFL ﬁle system implemen-\ntation is replication, the duplication and synchronization of a set of data over a",
  "network to another system. First, a snapshot of a WAFL ﬁle system is duplicated\nto another system. When another snapshot is taken on the source system, it\nis relatively easy to update the remote system just by sending over all blocks\ncontained in the new snapshot. These blocks are the ones that have changed 580\nChapter 12\nFile-System Implementation\nbetween the times the two snapshots were taken. The remote system adds these\nblocks to the ﬁle system and updates its pointers, and the new system then is a\nduplicate of the source system as of the time of the second snapshot. Repeating\nthis process maintains the remote system as a nearly up-to-date copy of the ﬁrst\nsystem. Such replication is used for disaster recovery. Should the ﬁrst system",
  "be destroyed, most of its data are available for use on the remote system.\nFinally, we should note that the ZFS ﬁle system supports similarly efﬁcient\nsnapshots, clones, and replication.\n12.10 Summary\nThe ﬁle system resides permanently on secondary storage, which is designed to\nhold a large amount of data permanently. The most common secondary-storage\nmedium is the disk.\nPhysical disks may be segmented into partitions to control media use\nand to allow multiple, possibly varying, ﬁle systems on a single spindle.\nThese ﬁle systems are mounted onto a logical ﬁle system architecture to make\nthem available for use. File systems are often implemented in a layered or\nmodular structure. The lower levels deal with the physical properties of storage",
  "devices. Upper levels deal with symbolic ﬁle names and logical properties of\nﬁles. Intermediate levels map the logical ﬁle concepts into physical device\nproperties.\nAny ﬁle-system type can have different structures and algorithms. A VFS\nlayer allows the upper layers to deal with each ﬁle-system type uniformly. Even\nremote ﬁle systems can be integrated into the system’s directory structure and\nacted on by standard system calls via the VFS interface.\nThe various ﬁles can be allocated space on the disk in three ways: through\ncontiguous, linked, or indexed allocation. Contiguous allocation can suffer\nfrom external fragmentation. Direct access is very inefﬁcient with linked\nallocation. Indexed allocation may require substantial overhead for its index",
  "block. These algorithms can be optimized in many ways. Contiguous space\ncan be enlarged through extents to increase ﬂexibility and to decrease external\nfragmentation. Indexed allocation can be done in clusters of multiple blocks\nto increase throughput and to reduce the number of index entries needed.\nIndexing in large clusters is similar to contiguous allocation with extents.\nFree-space allocation methods also inﬂuence the efﬁciency of disk-space\nuse, the performance of the ﬁle system, and the reliability of secondary storage.\nThe methods used include bit vectors and linked lists. Optimizations include\ngrouping, counting, and the FAT, which places the linked list in one contiguous\narea.\nDirectory-management routines must consider efﬁciency, performance,",
  "and reliability. A hash table is a commonly used method, as it is fast and\nefﬁcient. Unfortunately, damage to the table or a system crash can result\nin inconsistency between the directory information and the disk’s contents.\nA consistency checker can be used to repair the damage. Operating-system\nbackup tools allow disk data to be copied to tape, enabling the user to recover\nfrom data or even disk loss due to hardware failure, operating system bug, or\nuser error. Practice Exercises\n581\nNetwork ﬁle systems, such as NFS, use client–server methodology to\nallow users to access ﬁles and directories from remote machines as if they\nwere on local ﬁle systems. System calls on the client are translated into\nnetwork protocols and retranslated into ﬁle-system operations on the server.",
  "Networking and multiple-client access create challenges in the areas of data\nconsistency and performance.\nDue to the fundamental role that ﬁle systems play in system operation,\ntheir performance and reliability are crucial. Techniques such as log structures\nand caching help improve performance, while log structures and RAID improve\nreliability. The WAFL ﬁle system is an example of optimization of performance\nto match a speciﬁc I/O load.\nPractice Exercises\n12.1\nConsider a ﬁle currently consisting of 100 blocks. Assume that the ﬁle-\ncontrol block (and the index block, in the case of indexed allocation)\nis already in memory. Calculate how many disk I/O operations are\nrequired for contiguous, linked, and indexed (single-level) allocation",
  "strategies, if, for one block, the following conditions hold. In the\ncontiguous-allocation case, assume that there is no room to grow at\nthe beginning but there is room to grow at the end. Also assume that\nthe block information to be added is stored in memory.\na.\nThe block is added at the beginning.\nb.\nThe block is added in the middle.\nc.\nThe block is added at the end.\nd.\nThe block is removed from the beginning.\ne.\nThe block is removed from the middle.\nf.\nThe block is removed from the end.\n12.2\nWhat problems could occur if a system allowed a ﬁle system to be\nmounted simultaneously at more than one location?\n12.3\nWhy must the bit map for ﬁle allocation be kept on mass storage, rather\nthan in main memory?\n12.4\nConsider a system that supports the strategies of contiguous, linked,",
  "and indexed allocation. What criteria should be used in deciding which\nstrategy is best utilized for a particular ﬁle?\n12.5\nOne problem with contiguous allocation is that the user must preallo-\ncate enough space for each ﬁle. If the ﬁle grows to be larger than the\nspace allocated for it, special actions must be taken. One solution to this\nproblem is to deﬁne a ﬁle structure consisting of an initial contiguous\narea (of a speciﬁed size). If this area is ﬁlled, the operating system\nautomatically deﬁnes an overﬂow area that is linked to the initial\ncontiguous area. If the overﬂow area is ﬁlled, another overﬂow area\nis allocated. Compare this implementation of a ﬁle with the standard\ncontiguous and linked implementations. 582\nChapter 12\nFile-System Implementation\n12.6",
  "Chapter 12\nFile-System Implementation\n12.6\nHow do caches help improve performance? Why do systems not use\nmore or larger caches if they are so useful?\n12.7\nWhy is it advantageous to the user for an operating system to dynami-\ncally allocate its internal tables? What are the penalties to the operating\nsystem for doing so?\n12.8\nExplain how the VFS layer allows an operating system to support\nmultiple types of ﬁle systems easily.\nExercises\n12.9\nConsider a ﬁle system that uses a modifed contiguous-allocation\nscheme with support for extents. A ﬁle is a collection of extents, with\neach extent corresponding to a contiguous set of blocks. A key issue in\nsuch systems is the degree of variability in the size of the extents. What\nare the advantages and disadvantages of the following schemes?\na.",
  "a.\nAll extents are of the same size, and the size is predetermined.\nb.\nExtents can be of any size and are allocated dynamically.\nc.\nExtents can be of a few ﬁxed sizes, and these sizes are predeter-\nmined.\n12.10\nContrast the performance of the three techniques for allocating disk\nblocks (contiguous, linked, and indexed) for both sequential and\nrandom ﬁle access.\n12.11\nWhat are the advantages of the variant of linked allocation that uses a\nFAT to chain together the blocks of a ﬁle?\n12.12\nConsider a system where free space is kept in a free-space list.\na.\nSuppose that the pointer to the free-space list is lost. Can the\nsystem reconstruct the free-space list? Explain your answer.\nb.\nConsider a ﬁle system similar to the one used by UNIX with",
  "indexed allocation. How many disk I/O operations might be\nrequired to read the contents of a small local ﬁle at /a/b/c?\nAssume that none of the disk blocks is currently being cached.\nc.\nSuggest a scheme to ensure that the pointer is never lost as a result\nof memory failure.\n12.13\nSome ﬁle systems allow disk storage to be allocated at different levels\nof granularity. For instance, a ﬁle system could allocate 4 KB of disk\nspace as a single 4-KB block or as eight 512-byte blocks. How could\nwe take advantage of this ﬂexibility to improve performance? What\nmodiﬁcations would have to be made to the free-space management\nscheme in order to support this feature?\n12.14\nDiscuss how performance optimizations for ﬁle systems might result",
  "in difﬁculties in maintaining the consistency of the systems in the event\nof computer crashes. Programming Problems\n583\n12.15\nConsider a ﬁle system on a disk that has both logical and physical\nblock sizes of 512 bytes. Assume that the information about each\nﬁle is already in memory. For each of the three allocation strategies\n(contiguous, linked, and indexed), answer these questions:\na.\nHow is the logical-to-physical address mapping accomplished\nin this system? (For the indexed allocation, assume that a ﬁle is\nalways less than 512 blocks long.)\nb.\nIf we are currently at logical block 10 (the last block accessed was\nblock 10) and want to access logical block 4, how many physical\nblocks must be read from the disk?\n12.16\nConsider a ﬁle system that uses inodes to represent ﬁles. Disk blocks",
  "are 8 KB in size, and a pointer to a disk block requires 4 bytes. This ﬁle\nsystem has 12 direct disk blocks, as well as single, double, and triple\nindirect disk blocks. What is the maximum size of a ﬁle that can be\nstored in this ﬁle system?\n12.17\nFragmentation on a storage device can be eliminated by recompaction\nof the information. Typical disk devices do not have relocation or base\nregisters (such as those used when memory is to be compacted), so\nhow can we relocate ﬁles? Give three reasons why recompacting and\nrelocation of ﬁles are often avoided.\n12.18\nAssume that in a particular augmentation of a remote-ﬁle-access\nprotocol, each client maintains a name cache that caches translations\nfrom ﬁle names to corresponding ﬁle handles. What issues should we",
  "take into account in implementing the name cache?\n12.19\nExplain why logging metadata updates ensures recovery of a ﬁle\nsystem after a ﬁle-system crash.\n12.20\nConsider the following backup scheme:\n• Day 1. Copy to a backup medium all ﬁles from the disk.\n• Day 2. Copy to another medium all ﬁles changed since day 1.\n• Day 3. Copy to another medium all ﬁles changed since day 1.\nThis differs from the schedule given in Section 12.7.4 by having all\nsubsequent backups copy all ﬁles modiﬁed since the ﬁrst full backup.\nWhat are the beneﬁts of this system over the one in Section 12.7.4?\nWhat are the drawbacks? Are restore operations made easier or more\ndifﬁcult? Explain your answer.\nProgramming Problems\nThe following exercise examines the relationship between ﬁles and",
  "inodes on a UNIX or Linux system. On these systems, ﬁles are repre-\nsented with inodes. That is, an inode is a ﬁle (and vice versa). You can\ncomplete this exercise on the Linux virtual machine that is provided\nwith this text. You can also complete the exercise on any Linux, UNIX, or 584\nChapter 12\nFile-System Implementation\nMac OS X system, but it will require creating two simple text ﬁles named\nfile1.txt and file3.txt whose contents are unique sentences.\n12.21\nIn the source code available with this text, open file1.txt and\nexamine its contents. Next, obtain the inode number of this ﬁle with\nthe command\nls -li file1.txt\nThis will produce output similar to the following:\n16980 -rw-r--r-- 2 os os 22 Sep 14 16:13 file1.txt\nwhere the inode number is boldfaced. (The inode number of file1.txt",
  "is likely to be different on your system.)\nThe UNIX ln command creates a link between a source and target ﬁle.\nThis command works as follows:\nln [-s] <source file> <target file>\nUNIX provides two types of links: (1) hard links and (2) soft links.\nA hard link creates a separate target ﬁle that has the same inode as the\nsource ﬁle. Enter the following command to create a hard link between\nfile1.txt and file2.txt:\nln file1.txt file2.txt\nWhat are the inode values of file1.txt and file2.txt? Are they\nthe same or different? Do the two ﬁles have the same—or different—\ncontents?\nNext, edit file2.txt and change its contents. After you have done\nso, examine the contents of file1.txt. Are the contents of file1.txt\nand file2.txt the same or different?",
  "and file2.txt the same or different?\nNext, enter the following command which removes file1.txt:\nrm file1.txt\nDoes file2.txt still exist as well?\nNow examine the man pages for both the rm and unlink commands.\nAfterwards, remove file2.txt by entering the command\nstrace rm file2.txt\nThe strace command traces the execution of system calls as the\ncommand rm file2.txt is run. What system call is used for removing\nfile2.txt?\nA soft link (or symbolic link) creates a new ﬁle that “points” to the\nname of the ﬁle it is linking to. In the source code available with this text,\ncreate a soft link to file3.txt by entering the following command:\nln -s file3.txt file4.txt\nAfter you have done so, obtain the inode numbers of file3.txt and\nfile4.txt using the command\nls -li file*.txt Bibliography\n585",
  "ls -li file*.txt Bibliography\n585\nAre the inodes the same, or is each unique? Next, edit the contents\nof file4.txt. Have the contents of file3.txt been altered as well?\nLast, delete file3.txt. After you have done so, explain what happens\nwhen you attempt to edit file4.txt.\nBibliographical Notes\nThe MS-DOS FAT system is explained in [Norton and Wilton (1988)]. The\ninternals of the BSD UNIX system are covered in full in [McKusick and\nNeville-Neil (2005)]. Details concerning ﬁle systems for Linux can be found in\n[Love (2010)]. The Google ﬁle system is described in [Ghemawat et al. (2003)].\nFUSE can be found at http://fuse.sourceforge.net.\nLog-structured ﬁle organizations for enhancing both performance and\nconsistency are discussed in [Rosenblum and Ousterhout (1991)], [Seltzer",
  "et al. (1993)], and [Seltzer et al. (1995)]. Algorithms such as balanced\ntrees (and much more) are covered by [Knuth (1998)] and [Cormen et al.\n(2009)]. [Silvers (2000)] discusses implementing the page cache in the\nNetBSD operating system. The ZFS source code for space maps can be found at\nhttp://src.opensolaris.org/source/xref/onnv/onnv-gate/usr/src/uts/common/\nfs/zfs/space map.c.\nThe network ﬁle system (NFS) is discussed in [Callaghan (2000)]. NFS Ver-\nsion 4 is a standard described at http://www.ietf.org/rfc/rfc3530.txt. [Ouster-\nhout (1991)] discusses the role of distributed state in networked ﬁle systems.\nLog-structured designs for networked ﬁle systems are proposed in [Hartman\nand Ousterhout (1995)] and [Thekkath et al. (1997)]. NFS and the UNIX ﬁle",
  "system (UFS) are described in [Vahalia (1996)] and [Mauro and McDougall\n(2007)]. The NTFS ﬁle system is explained in [Solomon (1998)]. The Ext3 ﬁle\nsystem used in Linux is described in [Mauerer (2008)] and the WAFL ﬁle\nsystem is covered in [Hitz et al. (1995)]. ZFS documentation can be found\nat http://www.opensolaris.org/os/community/ZFS/docs.\nBibliography\n[Callaghan (2000)]\nB. Callaghan, NFS Illustrated, Addison-Wesley (2000).\n[Cormen et al. (2009)]\nT. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein,\nIntroduction to Algorithms, Third Edition, MIT Press (2009).\n[Ghemawat et al. (2003)]\nS. Ghemawat, H. Gobioff, and S.-T. Leung, “The\nGoogle File System”, Proceedings of the ACM Symposium on Operating Systems\nPrinciples (2003).\n[Hartman and Ousterhout (1995)]",
  "[Hartman and Ousterhout (1995)]\nJ. H. Hartman and J. K. Ousterhout, “The\nZebra Striped Network File System”, ACM Transactions on Computer Systems,\nVolume 13, Number 3 (1995), pages 274–310.\n[Hitz et al. (1995)]\nD. Hitz, J. Lau, and M. Malcolm, “File System Design for an\nNFS File Server Appliance”, Technical report, NetApp (1995). 586\nChapter 12\nFile-System Implementation\n[Knuth (1998)]\nD. E. Knuth, The Art of Computer Programming, Volume 3: Sorting\nand Searching, Second Edition, Addison-Wesley (1998).\n[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[Mauerer (2008)]\nW. Mauerer, Professional Linux Kernel Architecture, John Wiley\nand Sons (2008).\n[Mauro and McDougall (2007)]\nJ. Mauro and R. McDougall, Solaris Internals:",
  "J. Mauro and R. McDougall, Solaris Internals:\nCore Kernel Architecture, Prentice Hall (2007).\n[McKusick and Neville-Neil (2005)]\nM. K. McKusick and G. V. Neville-Neil,\nThe Design and Implementation of the FreeBSD UNIX Operating System, Addison\nWesley (2005).\n[Norton and Wilton (1988)]\nP. Norton and R. Wilton, The New Peter Norton\nProgrammer’s Guide to the IBM PC & PS/2, Microsoft Press (1988).\n[Ousterhout (1991)]\nJ. Ousterhout. “The Role of Distributed State”. In CMU\nComputer Science: a 25th Anniversary Commemorative, R. F. Rashid, Ed., Addison-\nWesley (1991).\n[Rosenblum and Ousterhout (1991)]\nM. Rosenblum and J. K. Ousterhout, “The\nDesign and Implementation of a Log-Structured File System”, Proceedings of the\nACM Symposium on Operating Systems Principles (1991), pages 1–15.",
  "[Seltzer et al. (1993)]\nM. I. Seltzer, K. Bostic, M. K. McKusick, and C. Staelin, “An\nImplementation of a Log-Structured File System for UNIX”, USENIX Winter\n(1993), pages 307–326.\n[Seltzer et al. (1995)]\nM. I. Seltzer, K. A. Smith, H. Balakrishnan, J. Chang,\nS. McMains, and V. N. Padmanabhan, “File System Logging Versus Clustering:\nA Performance Comparison”, USENIX Winter (1995), pages 249–264.\n[Silvers (2000)]\nC. Silvers, “UBC: An Efﬁcient Uniﬁed I/O and Memory Caching\nSubsystem for NetBSD”, USENIX Annual Technical Conference—FREENIX Track\n(2000).\n[Solomon (1998)]\nD. A. Solomon, Inside Windows NT, Second Edition, Microsoft\nPress (1998).\n[Thekkath et al. (1997)]\nC. A. Thekkath, T. Mann, and E. K. Lee, “Frangipani:",
  "A Scalable Distributed File System”, Symposium on Operating Systems Principles\n(1997), pages 224–237.\n[Vahalia (1996)]\nU. Vahalia, Unix Internals: The New Frontiers, Prentice Hall\n(1996). 13\nC H A P T E R\nI/O Systems\nThe two main jobs of a computer are I/O and processing. In many cases, the\nmain job is I/O, and the processing is merely incidental. For instance, when\nwe browse a web page or edit a ﬁle, our immediate interest is to read or enter\nsome information, not to compute an answer.\nThe role of the operating system in computer I/O is to manage and\ncontrol I/O operations and I/O devices. Although related topics appear in\nother chapters, here we bring together the pieces to paint a complete picture\nof I/O. First, we describe the basics of I/O hardware, because the nature of the",
  "hardware interface places constraints on the internal facilities of the operating\nsystem. Next, we discuss the I/O services provided by the operating system\nand the embodiment of these services in the application I/O interface. Then,\nwe explain how the operating system bridges the gap between the hardware\ninterface and the application interface. We also discuss the UNIX System V\nSTREAMS mechanism, which enables an application to assemble pipelines of\ndriver code dynamically. Finally, we discuss the performance aspects of I/O\nand the principles of operating-system design that improve I/O performance.\nCHAPTER OBJECTIVES\n• To explore the structure of an operating system’s I/O subsystem.\n• To discuss the principles and complexities of I/O hardware.",
  "• To explain the performance aspects of I/O hardware and software.\n13.1\nOverview\nThe control of devices connected to the computer is a major concern of\noperating-system designers. Because I/O devices vary so widely in their\nfunction and speed (consider a mouse, a hard disk, and a tape robot), varied\nmethods are needed to control them. These methods form the I/O subsystem\nof the kernel, which separates the rest of the kernel from the complexities of\nmanaging I/O devices.\n587 588\nChapter 13\nI/O Systems\nI/O-device technology exhibits two conﬂicting trends. On the one hand, we\nsee increasing standardization of software and hardware interfaces. This trend\nhelps us to incorporate improved device generations into existing computers",
  "and operating systems. On the other hand, we see an increasingly broad variety\nof I/O devices. Some new devices are so unlike previous devices that it is a\nchallenge to incorporate them into our computers and operating systems. This\nchallenge is met by a combination of hardware and software techniques. The\nbasic I/O hardware elements, such as ports, buses, and device controllers,\naccommodate a wide variety of I/O devices. To encapsulate the details and\noddities of different devices, the kernel of an operating system is structured\nto use device-driver modules. The device drivers present a uniform device-\naccess interface to the I/O subsystem, much as system calls provide a standard\ninterface between the application and the operating system.\n13.2 I/O Hardware",
  "13.2 I/O Hardware\nComputers operate a great many kinds of devices. Most ﬁt into the general\ncategories of storage devices (disks, tapes), transmission devices (network con-\nnections, Bluetooth), and human-interface devices (screen, keyboard, mouse,\naudio in and out). Other devices are more specialized, such as those involved in\nthe steering of a jet. In these aircraft, a human gives input to the ﬂight computer\nvia a joystick and foot pedals, and the computer sends output commands that\ncause motors to move rudders and ﬂaps and fuels to the engines. Despite\nthe incredible variety of I/O devices, though, we need only a few concepts to\nunderstand how the devices are attached and how the software can control the\nhardware.\nA device communicates with a computer system by sending signals over",
  "a cable or even through the air. The device communicates with the machine\nvia a connection point, or port—for example, a serial port. If devices share a\ncommon set of wires, the connection is called a bus. A bus is a set of wires and\na rigidly deﬁned protocol that speciﬁes a set of messages that can be sent on\nthe wires. In terms of the electronics, the messages are conveyed by patterns\nof electrical voltages applied to the wires with deﬁned timings. When device\nA has a cable that plugs into device B, and device B has a cable that plugs into\ndevice C, and device C plugs into a port on the computer, this arrangement is\ncalled a daisy chain. A daisy chain usually operates as a bus.\nBuses are used widely in computer architecture and vary in their signaling",
  "methods, speed, throughput, and connection methods. A typical PC bus\nstructure appears in Figure 13.1. In the ﬁgure, a PCI bus (the common PC\nsystem bus) connects the processor–memory subsystem to fast devices, and\nan expansion bus connects relatively slow devices, such as the keyboard and\nserial and USB ports. In the upper-right portion of the ﬁgure, four disks are\nconnected together on a Small Computer System Interface (SCSI) bus plugged\ninto a SCSI controller. Other common buses used to interconnect main parts of\na computer include PCI Express (PCIe), with throughput of up to 16 GB per\nsecond, and HyperTransport, with throughput of up to 25 GB per second.\nA controller is a collection of electronics that can operate a port, a bus,",
  "or a device. A serial-port controller is a simple device controller. It is a single\nchip (or portion of a chip) in the computer that controls the signals on the 13.2\nI/O Hardware\n589\nexpansion bus\nPCI bus\ndisk\ndisk\ndisk\ndisk\ndisk\ndisk\ndisk\ndisk\nSCSI controller\nSCSI bus\ncache\nmemory\nprocessor\nbridge/memory\ncontroller\nmonitor\nIDE disk controller\nexpansion bus\ninterface\ngraphics\ncontroller\nkeyboard\nparallel\nport\nserial\nport\nFigure 13.1\nA typical PC bus structure.\nwires of a serial port. By contrast, a SCSI bus controller is not simple. Because\nthe SCSI protocol is complex, the SCSI bus controller is often implemented as\na separate circuit board (or a host adapter) that plugs into the computer. It\ntypically contains a processor, microcode, and some private memory to enable",
  "it to process the SCSI protocol messages. Some devices have their own built-in\ncontrollers. If you look at a disk drive, you will see a circuit board attached to\none side. This board is the disk controller. It implements the disk side of the\nprotocol for some kind of connection—SCSI or Serial Advanced Technology\nAttachment (SATA), for instance. It has microcode and a processor to do many\ntasks, such as bad-sector mapping, prefetching, buffering, and caching.\nHow can the processor give commands and data to a controller to\naccomplish an I/O transfer? The short answer is that the controller has one\nor more registers for data and control signals. The processor communicates\nwith the controller by reading and writing bit patterns in these registers. One",
  "way in which this communication can occur is through the use of special\nI/O instructions that specify the transfer of a byte or word to an I/O port\naddress. The I/O instruction triggers bus lines to select the proper device and\nto move bits into or out of a device register. Alternatively, the device controller\ncan support memory-mapped I/O. In this case, the device-control registers\nare mapped into the address space of the processor. The CPU executes I/O\nrequests using the standard data-transfer instructions to read and write the\ndevice-control registers at their mapped locations in physical memory.\nSome systems use both techniques. For instance, PCs use I/O instructions\nto control some devices and memory-mapped I/O to control others. Figure",
  "13.2 shows the usual I/O port addresses for PCs. The graphics controller has\nI/O ports for basic control operations, but the controller has a large memory- 590\nChapter 13\nI/O Systems\nI/O address range (hexadecimal)\n000–00F\n020–021\n040–043\n200–20F\n2F8–2FF\n320–32F\n378–37F\n3D0–3DF\n3F0–3F7\n3F8–3FF\ndevice\nDMA controller\ninterrupt controller\ntimer\ngame controller\nserial port (secondary)\nhard-disk controller\nparallel port\ngraphics controller\ndiskette-drive controller\nserial port (primary)\nFigure 13.2\nDevice I/O port locations on PCs (partial).\nmapped region to hold screen contents. The process sends output to the screen\nby writing data into the memory-mapped region. The controller generates\nthe screen image based on the contents of this memory. This technique is",
  "simple to use. Moreover, writing millions of bytes to the graphics memory\nis faster than issuing millions of I/O instructions. But the ease of writing\nto a memory-mapped I/O controller is offset by a disadvantage. Because a\ncommon type of software fault is a write through an incorrect pointer to an\nunintended region of memory, a memory-mapped device register is vulnerable\nto accidental modiﬁcation. Of course, protected memory helps to reduce this\nrisk.\nAn I/O port typically consists of four registers, called the status, control,\ndata-in, and data-out registers.\n• The data-in register is read by the host to get input.\n• The data-out register is written by the host to send output.\n• The status register contains bits that can be read by the host. These bits",
  "indicate states, such as whether the current command has completed,\nwhether a byte is available to be read from the data-in register, and whether\na device error has occurred.\n• The control register can be written by the host to start a command or to\nchange the mode of a device. For instance, a certain bit in the control\nregister of a serial port chooses between full-duplex and half-duplex\ncommunication, another bit enables parity checking, a third bit sets the\nword length to 7 or 8 bits, and other bits select one of the speeds supported\nby the serial port.\nThe data registers are typically 1 to 4 bytes in size. Some controllers have\nFIFO chips that can hold several bytes of input or output data to expand the\ncapacity of the controller beyond the size of the data register. A FIFO chip can",
  "hold a small burst of data until the device or host is able to receive those data. 13.2\nI/O Hardware\n591\n13.2.1\nPolling\nThe complete protocol for interaction between the host and a controller\ncan be intricate, but the basic handshaking notion is simple. We explain\nhandshaking with an example. Assume that 2 bits are used to coordinate\nthe producer–consumer relationship between the controller and the host. The\ncontroller indicates its state through the busy bit in the status register. (Recall\nthat to set a bit means to write a 1 into the bit and to clear a bit means to write\na 0 into it.) The controller sets the busy bit when it is busy working and clears\nthe busy bit when it is ready to accept the next command. The host signals its",
  "wishes via the command-ready bit in the command register. The host sets the\ncommand-ready bit when a command is available for the controller to execute.\nFor this example, the host writes output through a port, coordinating with the\ncontroller by handshaking as follows.\n1. The host repeatedly reads the busy bit until that bit becomes clear.\n2. The host sets the write bit in the command register and writes a byte into\nthe data-out register.\n3. The host sets the command-ready bit.\n4. When the controller notices that the command-ready bit is set, it sets the\nbusy bit.\n5. The controller reads the command register and sees the write command.\nIt reads the data-out register to get the byte and does the I/O to the\ndevice.\n6. The controller clears the command-ready bit, clears the error bit in the",
  "status register to indicate that the device I/O succeeded, and clears the\nbusy bit to indicate that it is ﬁnished.\nThis loop is repeated for each byte.\nIn step 1, the host is busy-waiting or polling: it is in a loop, reading the\nstatus register over and over until the busy bit becomes clear. If the controller\nand device are fast, this method is a reasonable one. But if the wait may be\nlong, the host should probably switch to another task. How, then, does the\nhost know when the controller has become idle? For some devices, the host\nmust service the device quickly, or data will be lost. For instance, when data\nare streaming in on a serial port or from a keyboard, the small buffer on the\ncontroller will overﬂow and data will be lost if the host waits too long before",
  "returning to read the bytes.\nIn many computer architectures, three CPU-instruction cycles are sufﬁcient\nto poll a device: read a device register, logical--and to extract a status bit,\nand branch if not zero. Clearly, the basic polling operation is efﬁcient. But\npolling becomes inefﬁcient when it is attempted repeatedly yet rarely ﬁnds a\ndevice ready for service, while other useful CPU processing remains undone. In\nsuch instances, it may be more efﬁcient to arrange for the hardware controller to\nnotify the CPU when the device becomes ready for service, rather than to require\nthe CPU to poll repeatedly for an I/O completion. The hardware mechanism\nthat enables a device to notify the CPU is called an interrupt. 592\nChapter 13\nI/O Systems\ndevice driver initiates I/O\nCPU receiving interrupt,",
  "CPU receiving interrupt,\ntransfers control to\ninterrupt handler\nCPU resumes\nprocessing of\ninterrupted task\nCPU\n1\nI/O controller\nCPU executing checks for\ninterrupts between instructions\n5\ninterrupt handler\nprocesses data,\nreturns from interrupt\ninitiates I/O\n3\n2\n4\n7\ninput ready, output\ncomplete, or error\ngenerates interrupt signal\n6\nFigure 13.3\nInterrupt-driven I/O cycle.\n13.2.2\nInterrupts\nThe basic interrupt mechanism works as follows. The CPU hardware has a wire\ncalled the interrupt-request line that the CPU senses after executing every\ninstruction. When the CPU detects that a controller has asserted a signal on\nthe interrupt-request line, the CPU performs a state save and jumps to the\ninterrupt-handler routine at a ﬁxed address in memory. The interrupt handler",
  "determines the cause of the interrupt, performs the necessary processing,\nperforms a state restore, and executes a return from interrupt instruction\nto return the CPU to the execution state prior to the interrupt. We say that\nthe device controller raises an interrupt by asserting a signal on the interrupt\nrequest line, the CPU catches the interrupt and dispatches it to the interrupt\nhandler, and the handler clears the interrupt by servicing the device. Figure 13.3\nsummarizes the interrupt-driven I/O cycle. We stress interrupt management\nin this chapter because even single-user modern systems manage hundreds of\ninterrupts per second and servers hundreds of thousands per second.\nThe basic interrupt mechanism just described enables the CPU to respond",
  "to an asynchronous event, as when a device controller becomes ready for\nservice. In a modern operating system, however, we need more sophisticated\ninterrupt-handling features. 13.2\nI/O Hardware\n593\n1. We need the ability to defer interrupt handling during critical processing.\n2. We need an efﬁcient way to dispatch to the proper interrupt handler for\na device without ﬁrst polling all the devices to see which one raised the\ninterrupt.\n3. We need multilevel interrupts, so that the operating system can distin-\nguish between high- and low-priority interrupts and can respond with\nthe appropriate degree of urgency.\nIn modern computer hardware, these three features are provided by the CPU\nand by the interrupt-controller hardware.\nMost CPUs have two interrupt request lines. One is the nonmaskable",
  "interrupt, which is reserved for events such as unrecoverable memory errors.\nThe second interrupt line is maskable: it can be turned off by the CPU before\nthe execution of critical instruction sequences that must not be interrupted.\nThe maskable interrupt is used by device controllers to request service.\nThe interrupt mechanism accepts an address—a number that selects a\nspeciﬁc interrupt-handling routine from a small set. In most architectures, this\naddress is an offset in a table called the interrupt vector. This vector contains\nthe memory addresses of specialized interrupt handlers. The purpose of a\nvectored interrupt mechanism is to reduce the need for a single interrupt\nhandler to search all possible sources of interrupts to determine which one",
  "needs service. In practice, however, computers have more devices (and, hence,\ninterrupt handlers) than they have address elements in the interrupt vector.\nA common way to solve this problem is to use interrupt chaining, in which\neach element in the interrupt vector points to the head of a list of interrupt\nhandlers. When an interrupt is raised, the handlers on the corresponding list\nare called one by one, until one is found that can service the request. This\nstructure is a compromise between the overhead of a huge interrupt table and\nthe inefﬁciency of dispatching to a single interrupt handler.\nFigure 13.4 illustrates the design of the interrupt vector for the Intel Pentium\nprocessor. The events from 0 to 31, which are nonmaskable, are used to signal",
  "various error conditions. The events from 32 to 255, which are maskable, are\nused for purposes such as device-generated interrupts.\nThe interrupt mechanism also implements a system of interrupt priority\nlevels. These levels enable the CPU to defer the handling of low-priority\ninterrupts without masking all interrupts and makes it possible for a high-\npriority interrupt to preempt the execution of a low-priority interrupt.\nA modern operating system interacts with the interrupt mechanism in\nseveral ways. At boot time, the operating system probes the hardware buses\nto determine what devices are present and installs the corresponding interrupt\nhandlers into the interrupt vector. During I/O, the various device controllers",
  "raise interrupts when they are ready for service. These interrupts signify that\noutput has completed, or that input data are available, or that a failure has\nbeen detected. The interrupt mechanism is also used to handle a wide variety of\nexceptions, such as dividing by 0, accessing a protected or nonexistent memory\naddress, or attempting to execute a privileged instruction from user mode. The\nevents that trigger interrupts have a common property: they are occurrences\nthat induce the operating system to execute an urgent, self-contained routine. 594\nChapter 13\nI/O Systems\ndescription\nvector number\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19–31\n32–255\ndivide error\ndebug exception\nnull interrupt\nbreakpoint\nINTO-detected overflow\nbound range exception\ninvalid opcode\ndevice not available",
  "invalid opcode\ndevice not available\ndouble fault\ncoprocessor segment overrun (reserved)\ninvalid task state segment\nsegment not present\nstack fault\ngeneral protection\npage fault\n(Intel reserved, do not use)\nfloating-point error\nalignment check\nmachine check\n(Intel reserved, do not use)\nmaskable interrupts\nFigure 13.4\nIntel Pentium processor event-vector table.\nAn operating system has other good uses for an efﬁcient hardware and\nsoftware mechanism that saves a small amount of processor state and then\ncalls a privileged routine in the kernel. For example, many operating systems\nuse the interrupt mechanism for virtual memory paging. A page fault is an\nexception that raises an interrupt. The interrupt suspends the current process",
  "and jumps to the page-fault handler in the kernel. This handler saves the state\nof the process, moves the process to the wait queue, performs page-cache\nmanagement, schedules an I/O operation to fetch the page, schedules another\nprocess to resume execution, and then returns from the interrupt.\nAnother example is found in the implementation of system calls. Usually,\na program uses library calls to issue system calls. The library routines check\nthe arguments given by the application, build a data structure to convey\nthe arguments to the kernel, and then execute a special instruction called a\nsoftware interrupt, or trap. This instruction has an operand that identiﬁes\nthe desired kernel service. When a process executes the trap instruction, the",
  "interrupt hardware saves the state of the user code, switches to kernel mode,\nand dispatches to the kernel routine that implements the requested service. The\ntrap is given a relatively low interrupt priority compared with those assigned\nto device interrupts—executing a system call on behalf of an application is less\nurgent than servicing a device controller before its FIFO queue overﬂows and\nloses data.\nInterrupts can also be used to manage the ﬂow of control within the kernel.\nFor example, consider one example of the processing required to complete 13.2\nI/O Hardware\n595\na disk read. One step is to copy data from kernel space to the user buffer.\nThis copying is time consuming but not urgent—it should not block other",
  "high-priority interrupt handling. Another step is to start the next pending I/O\nfor that disk drive. This step has higher priority. If the disks are to be used\nefﬁciently, we need to start the next I/O as soon as the previous one completes.\nConsequently, a pair of interrupt handlers implements the kernel code that\ncompletes a disk read. The high-priority handler records the I/O status, clears\nthe device interrupt, starts the next pending I/O, and raises a low-priority\ninterrupt to complete the work. Later, when the CPU is not occupied with high-\npriority work, the low-priority interrupt will be dispatched. The corresponding\nhandler completes the user-level I/O by copying data from kernel buffers to\nthe application space and then calling the scheduler to place the application",
  "on the ready queue.\nA threaded kernel architecture is well suited to implement multiple\ninterrupt priorities and to enforce the precedence of interrupt handling over\nbackground processing in kernel and application routines. We illustrate this\npoint with the Solaris kernel. In Solaris, interrupt handlers are executed\nas kernel threads. A range of high priorities is reserved for these threads.\nThese priorities give interrupt handlers precedence over application code and\nkernel housekeeping and implement the priority relationships among interrupt\nhandlers. The priorities cause the Solaris thread scheduler to preempt low-\npriority interrupt handlers in favor of higher-priority ones, and the threaded\nimplementation enables multiprocessor hardware to run several interrupt",
  "handlers concurrently. We describe the interrupt architecture of Windows XP\nand UNIX in Chapter 19 and Appendix A, respectively.\nIn summary, interrupts are used throughout modern operating systems to\nhandle asynchronous events and to trap to supervisor-mode routines in the\nkernel. To enable the most urgent work to be done ﬁrst, modern computers\nuse a system of interrupt priorities. Device controllers, hardware faults, and\nsystem calls all raise interrupts to trigger kernel routines. Because interrupts\nare used so heavily for time-sensitive processing, efﬁcient interrupt handling\nis required for good system performance.\n13.2.3\nDirect Memory Access\nFor a device that does large transfers, such as a disk drive, it seems wasteful",
  "to use an expensive general-purpose processor to watch status bits and to\nfeed data into a controller register one byte at a time—a process termed\nprogrammed I/O (PIO). Many computers avoid burdening the main CPU with\nPIO by ofﬂoading some of this work to a special-purpose processor called a\ndirect-memory-access (DMA) controller. To initiate a DMA transfer, the host\nwrites a DMA command block into memory. This block contains a pointer to\nthe source of a transfer, a pointer to the destination of the transfer, and a count\nof the number of bytes to be transferred. The CPU writes the address of this\ncommand block to the DMA controller, then goes on with other work. The DMA\ncontroller proceeds to operate the memory bus directly, placing addresses on",
  "the bus to perform transfers without the help of the main CPU. A simple DMA\ncontroller is a standard component in all modern computers, from smartphones\nto mainframes. 596\nChapter 13\nI/O Systems\nHandshaking between the DMA controller and the device controller is\nperformed via a pair of wires called DMA-request and DMA-acknowledge.\nThe device controller places a signal on the DMA-request wire when a word\nof data is available for transfer. This signal causes the DMA controller to seize\nthe memory bus, place the desired address on the memory-address wires,\nand place a signal on the DMA-acknowledge wire. When the device controller\nreceives the DMA-acknowledge signal, it transfers the word of data to memory\nand removes the DMA-request signal.",
  "and removes the DMA-request signal.\nWhen the entire transfer is ﬁnished, the DMA controller interrupts the CPU.\nThis process is depicted in Figure 13.5. When the DMA controller seizes the\nmemory bus, the CPU is momentarily prevented from accessing main memory,\nalthough it can still access data items in its primary and secondary caches.\nAlthough this cycle stealing can slow down the CPU computation, ofﬂoading\nthe data-transfer work to a DMA controller generally improves the total system\nperformance. Some computer architectures use physical memory addresses for\nDMA, but others perform direct virtual memory access (DVMA), using virtual\naddresses that undergo translation to physical addresses. DVMA can perform\na transfer between two memory-mapped devices without the intervention of",
  "the CPU or the use of main memory.\nOn protected-mode kernels, the operating system generally prevents\nprocesses from issuing device commands directly. This discipline protects data\nfrom access-control violations and also protects the system from erroneous use\nof device controllers that could cause a system crash. Instead, the operating\nsystem exports functions that a sufﬁciently privileged process can use to\naccess low-level operations on the underlying hardware. On kernels without\nmemory protection, processes can access device controllers directly. This direct\naccess can be used to achieve high performance, since it can avoid kernel\ncommunication, context switches, and layers of kernel software. Unfortunately,\nIDE disk\ncontroller\nx\nDMA/bus/\ninterrupt\ncontroller\nbuffer\nx\nmemory",
  "x\nDMA/bus/\ninterrupt\ncontroller\nbuffer\nx\nmemory\nCPU memory bus\nPCI bus\ncache\nCPU\n5. DMA controller\n    transfers bytes to \n    buffer X, increasing \n    memory address \n    and decreasing C\n    until C $ 0\n  \n1. device driver is told\n    to transfer disk data\n    to buffer at address X  \n2. device driver tells\n    disk controller to\n    transfer C bytes\n    from disk to buffer\n    at address X\n6. when C $ 0, DMA\n    interrupts CPU to signal\n    transfer completion\n3. disk controller initiates\n    DMA transfer\n4. disk controller sends\n    each byte to DMA\n    controller\ndisk\ndisk\ndisk\ndisk\nFigure 13.5\nSteps in a DMA transfer. 13.3\nApplication I/O Interface\n597\nit interferes with system security and stability. The trend in general-purpose",
  "operating systems is to protect memory and devices so that the system can try\nto guard against erroneous or malicious applications.\n13.2.4\nI/O Hardware Summary\nAlthough the hardware aspects of I/O are complex when considered at the\nlevel of detail of electronics-hardware design, the concepts that we have\njust described are sufﬁcient to enable us to understand many I/O features\nof operating systems. Let’s review the main concepts:\n• A bus\n• A controller\n• An I/O port and its registers\n• The handshaking relationship between the host and a device controller\n• The execution of this handshaking in a polling loop or via interrupts\n• The ofﬂoading of this work to a DMA controller for large transfers\nWe gave a basic example of the handshaking that takes place between a",
  "device controller and the host earlier in this section. In reality, the wide variety\nof available devices poses a problem for operating-system implementers. Each\nkind of device has its own set of capabilities, control-bit deﬁnitions, and\nprotocols for interacting with the host—and they are all different. How can\nthe operating system be designed so that we can attach new devices to the\ncomputer without rewriting the operating system? And when the devices\nvary so widely, how can the operating system give a convenient, uniform I/O\ninterface to applications? We address those questions next.\n13.3 Application I/O Interface\nIn this section, we discuss structuring techniques and interfaces for the\noperating system that enable I/O devices to be treated in a standard, uniform",
  "way. We explain, for instance, how an application can open a ﬁle on a disk\nwithout knowing what kind of disk it is and how new disks and other devices\ncan be added to a computer without disruption of the operating system.\nLike other complex software-engineering problems, the approach here\ninvolves abstraction, encapsulation, and software layering. Speciﬁcally, we\ncan abstract away the detailed differences in I/O devices by identifying a few\ngeneral kinds. Each general kind is accessed through a standardized set of\nfunctions—an interface. The differences are encapsulated in kernel modules\ncalled device drivers that internally are custom-tailored to speciﬁc devices\nbut that export one of the standard interfaces. Figure 13.6 illustrates how the",
  "I/O-related portions of the kernel are structured in software layers.\nThe purpose of the device-driver layer is to hide the differences among\ndevice controllers from the I/O subsystem of the kernel, much as the I/O\nsystem calls encapsulate the behavior of devices in a few generic classes\nthat hide hardware differences from applications. Making the I/O subsystem 598\nChapter 13\nI/O Systems\nkernel\nhardware\nsoftware\nSCSI\ndevice\ndriver\nkeyboard\ndevice\ndriver\nmouse\ndevice\ndriver\n• • •\n• • •\n• • •\nPCI bus\ndevice\ndriver\nfloppy\ndevice\ndriver\nATAPI\ndevice\ndriver\nSCSI\ndevice\ncontroller\nkeyboard\ndevice\ncontroller\nmouse\ndevice\ncontroller\nPCI bus\ndevice\ncontroller\nfloppy\ndevice\ncontroller\nATAPI\ndevice\ncontroller\nSCSI\ndevices\nkeyboard\nmouse\nPCI bus\nfloppy-\ndisk\ndrives\nATAPI\ndevices\n(disks,\ntapes,",
  "floppy-\ndisk\ndrives\nATAPI\ndevices\n(disks,\ntapes,\ndrives)\nkernel I/O subsystem\nFigure 13.6\nA kernel I/O structure.\nindependent of the hardware simpliﬁes the job of the operating-system\ndeveloper. It also beneﬁts the hardware manufacturers. They either design\nnew devices to be compatible with an existing host controller interface (such\nas SATA), or they write device drivers to interface the new hardware to popular\noperating systems. Thus, we can attach new peripherals to a computer without\nwaiting for the operating-system vendor to develop support code.\nUnfortunately for device-hardware manufacturers, each type of operating\nsystem has its own standards for the device-driver interface. A given device\nmay ship with multiple device drivers—for instance, drivers for Windows,",
  "Linux, AIX, and Mac OS X. Devices vary on many dimensions, as illustrated in\nFigure 13.7.\n• Character-stream or block. A character-stream device transfers bytes one\nby one, whereas a block device transfers a block of bytes as a unit.\n• Sequential or random access. A sequential device transfers data in a ﬁxed\norder determined by the device, whereas the user of a random-access\ndevice can instruct the device to seek to any of the available data storage\nlocations.\n• Synchronous or asynchronous. A synchronous device performs data\ntransfers with predictable response times, in coordination with other\naspects of the system. An asynchronous device exhibits irregular or\nunpredictable response times not coordinated with other computer events.",
  "• Sharable or dedicated. A sharable device can be used concurrently by\nseveral processes or threads; a dedicated device cannot. 13.3\nApplication I/O Interface\n599\naspect\nvariation\nexample\nterminal\ndisk\nmodem\nCD-ROM\ntape\nkeyboard\ntape\nkeyboard\nCD-ROM\ngraphics controller\ndisk\ndata-transfer mode\naccess method\ntransfer schedule\nsharing\nI/O direction\ncharacter\nblock\nsequential\nrandom\nsynchronous\nasynchronous\ndedicated\nsharable\nread only\nwrite only\nread–write\nlatency\nseek time\ntransfer rate\ndelay between operations\ndevice speed\nFigure 13.7\nCharacteristics of I/O devices.\n• Speed of operation. Device speeds range from a few bytes per second to\na few gigabytes per second.\n• Read–write, read only, or write only. Some devices perform both input",
  "and output, but others support only one data transfer direction.\nFor the purpose of application access, many of these differences are hidden\nby the operating system, and the devices are grouped into a few conventional\ntypes. The resulting styles of device access have been found to be useful\nand broadly applicable. Although the exact system calls may differ across\noperating systems, the device categories are fairly standard. The major access\nconventions include block I/O, character-stream I/O, memory-mapped ﬁle\naccess, and network sockets. Operating systems also provide special system\ncalls to access a few additional devices, such as a time-of-day clock and a timer.\nSome operating systems provide a set of system calls for graphical display,\nvideo, and audio devices.",
  "video, and audio devices.\nMost operating systems also have an escape (or back door) that transpar-\nently passes arbitrary commands from an application to a device driver. In\nUNIX, this system call is ioctl() (for “I/O control”). The ioctl() system call\nenables an application to access any functionality that can be implemented by\nany device driver, without the need to invent a new system call. The ioctl()\nsystem call has three arguments. The ﬁrst is a ﬁle descriptor that connects the\napplication to the driver by referring to a hardware device managed by that\ndriver. The second is an integer that selects one of the commands implemented\nin the driver. The third is a pointer to an arbitrary data structure in memory\nthat enables the application and driver to communicate any necessary control",
  "information or data. 600\nChapter 13\nI/O Systems\n13.3.1\nBlock and Character Devices\nThe block-device interface captures all the aspects necessary for accessing disk\ndrives and other block-oriented devices. The device is expected to understand\ncommands such as read() and write(). If it is a random-access device,\nit is also expected to have a seek() command to specify which block to\ntransfer next. Applications normally access such a device through a ﬁle-system\ninterface. We can see that read(), write(), and seek() capture the essential\nbehaviors of block-storage devices, so that applications are insulated from the\nlow-level differences among those devices.\nThe operating system itself, as well as special applications such as database-",
  "management systems, may prefer to access a block device as a simple linear\narray of blocks. This mode of access is sometimes called raw I/O. If the\napplication performs its own buffering, then using a ﬁle system would cause\nextra, unneeded buffering. Likewise, if an application provides its own locking\nof ﬁle blocks or regions, then any operating-system locking services would be\nredundant at the least and contradictory at the worst. To avoid these conﬂicts,\nraw-device access passes control of the device directly to the application, letting\nthe operating system step out of the way. Unfortunately, no operating-system\nservices are then performed on this device. A compromise that is becoming\ncommon is for the operating system to allow a mode of operation on a ﬁle that",
  "disables buffering and locking. In the UNIX world, this is called direct I/O.\nMemory-mapped ﬁle access can be layered on top of block-device drivers.\nRather than offering read and write operations, a memory-mapped interface\nprovides access to disk storage via an array of bytes in main memory. The\nsystem call that maps a ﬁle into memory returns the virtual memory address\nthat contains a copy of the ﬁle. The actual data transfers are performed only\nwhen needed to satisfy access to the memory image. Because the transfers\nare handled by the same mechanism as that used for demand-paged virtual\nmemory access, memory-mapped I/O is efﬁcient. Memory mapping is also\nconvenient for programmers—access to a memory-mapped ﬁle is as simple",
  "as reading from and writing to memory. Operating systems that offer virtual\nmemory commonly use the mapping interface for kernel services. For instance,\nto execute a program, the operating system maps the executable into memory\nand then transfers control to the entry address of the executable. The mapping\ninterface is also commonly used for kernel access to swap space on disk.\nA keyboard is an example of a device that is accessed through a character-\nstream interface. The basic system calls in this interface enable an application\nto get() or put() one character. On top of this interface, libraries can be\nbuilt that offer line-at-a-time access, with buffering and editing services (for\nexample, when a user types a backspace, the preceding character is removed",
  "from the input stream). This style of access is convenient for input devices such\nas keyboards, mice, and modems that produce data for input “spontaneously”\n—that is, at times that cannot necessarily be predicted by the application. This\naccess style is also good for output devices such as printers and audio boards,\nwhich naturally ﬁt the concept of a linear stream of bytes.\n13.3.2\nNetwork Devices\nBecause the performance and addressing characteristics of network I/O differ\nsigniﬁcantly from those of disk I/O, most operating systems provide a network 13.3\nApplication I/O Interface\n601\nI/O interface that is different from the read()–write()–seek() interface used\nfor disks. One interface available in many operating systems, including UNIX\nand Windows, is the network socket interface.",
  "and Windows, is the network socket interface.\nThink of a wall socket for electricity: any electrical appliance can be plugged\nin. By analogy, the system calls in the socket interface enable an application\nto create a socket, to connect a local socket to a remote address (which plugs\nthis application into a socket created by another application), to listen for\nany remote application to plug into the local socket, and to send and receive\npackets over the connection. To support the implementation of servers, the\nsocket interface also provides a function called select() that manages a set\nof sockets. A call to select() returns information about which sockets have a\npacket waiting to be received and which sockets have room to accept a packet",
  "to be sent. The use of select() eliminates the polling and busy waiting that\nwould otherwise be necessary for network I/O. These functions encapsulate the\nessential behaviors of networks, greatly facilitating the creation of distributed\napplications that can use any underlying network hardware and protocol stack.\nMany other approaches to interprocess communication and network\ncommunication have been implemented. For instance, Windows provides one\ninterface to the network interface card and a second interface to the network\nprotocols. In UNIX, which has a long history as a proving ground for network\ntechnology, we ﬁnd half-duplex pipes, full-duplex FIFOs, full-duplex STREAMS,\nmessage queues, and sockets. Information on UNIX networking is given in\nSection A.9.\n13.3.3\nClocks and Timers",
  "Section A.9.\n13.3.3\nClocks and Timers\nMost computers have hardware clocks and timers that provide three basic\nfunctions:\n• Give the current time.\n• Give the elapsed time.\n• Set a timer to trigger operation X at time T.\nThese functions are used heavily by the operating system, as well as by time-\nsensitive applications. Unfortunately, the system calls that implement these\nfunctions are not standardized across operating systems.\nThe hardware to measure elapsed time and to trigger operations is called\na programmable interval timer. It can be set to wait a certain amount of time\nand then generate an interrupt, and it can be set to do this once or to repeat the\nprocess to generate periodic interrupts. The scheduler uses this mechanism to",
  "generate an interrupt that will preempt a process at the end of its time slice.\nThe disk I/O subsystem uses it to invoke the periodic ﬂushing of dirty cache\nbuffers to disk, and the network subsystem uses it to cancel operations that are\nproceeding too slowly because of network congestion or failures. The operating\nsystem may also provide an interface for user processes to use timers. The\noperating system can support more timer requests than the number of timer\nhardware channels by simulating virtual clocks. To do so, the kernel (or the\ntimer device driver) maintains a list of interrupts wanted by its own routines\nand by user requests, sorted in earliest-time-ﬁrst order. It sets the timer for the 602\nChapter 13\nI/O Systems",
  "Chapter 13\nI/O Systems\nearliest time. When the timer interrupts, the kernel signals the requester and\nreloads the timer with the next earliest time.\nOn many computers, the interrupt rate generated by the hardware clock is\nbetween 18 and 60 ticks per second. This resolution is coarse, since a modern\ncomputer can execute hundreds of millions of instructions per second. The\nprecision of triggers is limited by the coarse resolution of the timer, together\nwith the overhead of maintaining virtual clocks. Furthermore, if the timer\nticks are used to maintain the system time-of-day clock, the system clock\ncan drift. In most computers, the hardware clock is constructed from a high-\nfrequency counter. In some computers, the value of this counter can be read",
  "from a device register, in which case the counter can be considered a high-\nresolution clock. Although this clock does not generate interrupts, it offers\naccurate measurements of time intervals.\n13.3.4\nNonblocking and Asynchronous I/O\nAnother aspect of the system-call interface relates to the choice between\nblocking I/O and nonblocking I/O. When an application issues a blocking\nsystem call, the execution of the application is suspended. The application\nis moved from the operating system’s run queue to a wait queue. After the\nsystem call completes, the application is moved back to the run queue, where\nit is eligible to resume execution. When it resumes execution, it will receive\nthe values returned by the system call. The physical actions performed by",
  "I/O devices are generally asynchronous—they take a varying or unpredictable\namount of time. Nevertheless, most operating systems use blocking system\ncalls for the application interface, because blocking application code is easier\nto understand than nonblocking application code.\nSome user-level processes need nonblocking I/O. One example is a user\ninterface that receives keyboard and mouse input while processing and\ndisplaying data on the screen. Another example is a video application that\nreads frames from a ﬁle on disk while simultaneously decompressing and\ndisplaying the output on the display.\nOne way an application writer can overlap execution with I/O is to write\na multithreaded application. Some threads can perform blocking system calls,",
  "while others continue executing. Some operating systems provide nonblocking\nI/O system calls. A nonblocking call does not halt the execution of the\napplication for an extended time. Instead, it returns quickly, with a return\nvalue that indicates how many bytes were transferred.\nAn alternative to a nonblocking system call is an asynchronous system\ncall. An asynchronous call returns immediately, without waiting for the I/O to\ncomplete. The application continues to execute its code. The completion of the\nI/O at some future time is communicated to the application, either through the\nsetting of some variable in the address space of the application or through the\ntriggering of a signal or software interrupt or a call-back routine that is executed",
  "outside the linear control ﬂow of the application. The difference between\nnonblocking and asynchronous system calls is that a nonblocking read()\nreturns immediately with whatever data are available—the full number of\nbytes requested, fewer, or none at all. An asynchronous read() call requests a\ntransfer that will be performed in its entirety but will complete at some future\ntime. These two I/O methods are shown in Figure 13.8. 13.3\nApplication I/O Interface\n603\nrequesting process\nwaiting\nhardware\ndata transfer\nhardware\ndata transfer\ndevice driver\ndevice driver\ninterrupt handler\nrequesting process\nkernel user\n(a)\n(b)\ntime\ntime\nuser\nkernel\ninterrupt handler\nFigure 13.8\nTwo I/O methods: (a) synchronous and (b) asynchronous.\nAsynchronous activities occur throughout modern operating systems.",
  "Frequently, they are not exposed to users or applications but rather are\ncontained within the operating-system operation. Disk and network I/O are\nuseful examples. By default, when an application issues a network send\nrequest or a disk write request, the operating system notes the request, buffers\nthe I/O, and returns to the application. When possible, to optimize overall\nsystem performance, the operating system completes the request. If a system\nfailure occurs in the interim, the application will lose any “in-ﬂight” requests.\nTherefore, operating systems usually put a limit on how long they will buffer\na request. Some versions of UNIX ﬂush their disk buffers every 30 seconds, for\nexample, or each request is ﬂushed within 30 seconds of its occurrence. Data",
  "consistency within applications is maintained by the kernel, which reads data\nfrom its buffers before issuing I/O requests to devices, assuring that data not\nyet written are nevertheless returned to a requesting reader. Note that multiple\nthreads performing I/O to the same ﬁle might not receive consistent data,\ndepending on how the kernel implements its I/O. In this situation, the threads\nmay need to use locking protocols. Some I/O requests need to be performed\nimmediately, so I/O system calls usually have a way to indicate that a given\nrequest, or I/O to a speciﬁc device, should be performed synchronously.\nA good example of nonblocking behavior is the select() system call for\nnetwork sockets. This system call takes an argument that speciﬁes a maximum",
  "waiting time. By setting it to 0, an application can poll for network activity\nwithout blocking. But using select() introduces extra overhead, because\nthe select() call only checks whether I/O is possible. For a data transfer,\nselect() must be followed by some kind of read() or write() command.\nA variation on this approach, found in Mach, is a blocking multiple-read call.\nIt speciﬁes desired reads for several devices in one system call and returns as\nsoon as any one of them completes.\n13.3.5\nVectored I/O\nSome operating systems provide another major variation of I/O via their\napplications interfaces. vectored I/O allows one system call to perform multiple\nI/Ooperations involving multiple locations. For example, the UNIXreadv 604\nChapter 13\nI/O Systems",
  "Chapter 13\nI/O Systems\nsystem call accepts a vector of multiple buffers and either reads from a source to\nthat vector or writes from that vector to a destination. The same transfer could\nbe caused by several individual invocations of system calls, but this scatter–\ngather method is useful for a variety of reasons.\nMultiple separate buffers can have their contents transferred via one\nsystem call, avoiding context-switching and system-call overhead. Without\nvectored I/O, the data might ﬁrst need to be transferred to a larger buffer in\nthe right order and then transmitted, which is inefﬁcient. In addition, some\nversions of scatter–gather provide atomicity, assuring that all the I/O is done\nwithout interruption (and avoiding corruption of data if other threads are also",
  "performing I/Oinvolving those buffers). When possible, programmers make\nuse of scatter–gather I/O features to increase throughput and decrease system\noverhead.\n13.4 Kernel I/O Subsystem\nKernels provide many services related to I/O. Several services—scheduling,\nbuffering, caching, spooling, device reservation, and error handling—are\nprovided by the kernel’s I/O subsystem and build on the hardware and device-\ndriver infrastructure. The I/O subsystem is also responsible for protecting itself\nfrom errant processes and malicious users.\n13.4.1\nI/O Scheduling\nTo schedule a set of I/O requests means to determine a good order in which to\nexecute them. The order in which applications issue system calls rarely is the\nbest choice. Scheduling can improve overall system performance, can share",
  "device access fairly among processes, and can reduce the average waiting time\nfor I/O to complete. Here is a simple example to illustrate. Suppose that a disk\narm is near the beginning of a disk and that three applications issue blocking\nread calls to that disk. Application 1 requests a block near the end of the disk,\napplication 2 requests one near the beginning, and application 3 requests one\nin the middle of the disk. The operating system can reduce the distance that the\ndisk arm travels by serving the applications in the order 2, 3, 1. Rearranging\nthe order of service in this way is the essence of I/O scheduling.\nOperating-system developers implement scheduling by maintaining a wait\nqueue of requests for each device. When an application issues a blocking I/O",
  "system call, the request is placed on the queue for that device. The I/Oscheduler\nrearranges the order of the queue to improve the overall system efﬁciency\nand the average response time experienced by applications. The operating\nsystem may also try to be fair, so that no one application receives especially\npoor service, or it may give priority service for delay-sensitive requests. For\ninstance, requests from the virtual memory subsystem may take priority over\napplication requests. Several scheduling algorithms for disk I/O are detailed\nin Section 10.4.\nWhen a kernel supports asynchronous I/O, it must be able to keep track\nof many I/O requests at the same time. For this purpose, the operating system\nmight attach the wait queue to a device-status table. The kernel manages this",
  "table, which contains an entry for each I/O device, as shown in Figure 13.9. 13.4\nKernel I/O Subsystem\n605\ndevice: keyboard\nstatus: idle\ndevice: laser printer\nstatus: busy\ndevice: mouse\nstatus: idle\ndevice: disk unit 1\nstatus: idle\ndevice: disk unit 2 \nstatus: busy\n...\nrequest for\nlaser printer\naddress: 38546\nlength: 1372\nrequest for\ndisk unit 2\nfile: xxx\noperation: read\naddress: 43046\nlength: 20000\nrequest for\ndisk unit 2\nfile: yyy\noperation: write\naddress: 03458\nlength: 500\nFigure 13.9\nDevice-status table.\nEach table entry indicates the device’s type, address, and state (not functioning,\nidle, or busy). If the device is busy with a request, the type of request and other\nparameters will be stored in the table entry for that device.",
  "Scheduling I/O operations is one way in which the I/O subsystem improves\nthe efﬁciency of the computer. Another way is by using storage space in main\nmemory or on disk via buffering, caching, and spooling.\n13.4.2\nBuffering\nA buffer, of course, is a memory area that stores data being transferred between\ntwo devices or between a device and an application. Buffering is done for three\nreasons. One reasonistocope withaspeed mismatchbetweenthe producer and\nconsumer of a data stream. Suppose, for example, that a ﬁle is being received\nvia modem for storage on the hard disk. The modem is about a thousand\ntimes slower than the hard disk. So a buffer is created in main memory to\naccumulate the bytes received from the modem. When an entire buffer of data",
  "has arrived, the buffer can be written to disk in a single operation. Since the\ndisk write is not instantaneous and the modem still needs a place to store\nadditional incoming data, two buffers are used. After the modem ﬁlls the ﬁrst\nbuffer, the disk write is requested. The modem then starts to ﬁll the second\nbuffer while the ﬁrst buffer is written to disk. By the time the modem has ﬁlled\nthe second buffer, the disk write from the ﬁrst one should have completed,\nso the modem can switch back to the ﬁrst buffer while the disk writes the\nsecond one. This double buffering decouples the producer of data from the\nconsumer, thus relaxing timing requirements between them. The need for this\ndecoupling is illustrated in Figure 13.10, which lists the enormous differences",
  "in device speeds for typical computer hardware.\nA second use of buffering is to provide adaptations for devices that\nhave different data-transfer sizes. Such disparities are especially common in\ncomputer networking, where buffers are used widely for fragmentation and\nreassembly of messages. At the sending side, a large message is fragmented 606\nChapter 13\nI/O Systems\n0.1\n10\n0.001\n10E6\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0.0\nmodem\nmouse\nkeyboard\nhard disk\nFireWire\nSCSI bus\nGigabit Ethernet\nSerial ATA (SATA-300)\nInfiniband (QDR 12X)\nPCI Express 2.0 (\n32)\nHyperTransport (32-pair)\nsystem bus\nFigure 13.10\nSun Enterprise 6000 device-transfer rates (logarithmic).\ninto small network packets. The packets are sent over the network, and the",
  "receiving side places them in a reassembly buffer to form an image of the\nsource data.\nA third use of buffering is to support copy semantics for application I/O.\nAn example will clarify the meaning of “copy semantics.” Suppose that an\napplication has a buffer of data that it wishes to write to disk. It calls the\nwrite() system call, providing a pointer to the buffer and an integer specifying\nthe number of bytes to write. After the system call returns, what happens if\nthe application changes the contents of the buffer? With copy semantics, the\nversion of the data written to disk is guaranteed to be the version at the\ntime of the application system call, independent of any subsequent changes\nin the application’s buffer. A simple way in which the operating system can",
  "guarantee copy semantics is for the write() system call to copy the application\ndata into a kernel buffer before returning control to the application. The disk\nwrite is performed from the kernel buffer, so that subsequent changes to the\napplication buffer have no effect. Copying of data between kernel buffers and\napplication data space is common in operating systems, despite the overhead\nthat this operation introduces, because of the clean semantics. The same effect\ncan be obtained more efﬁciently by clever use of virtual memory mapping and\ncopy-on-write page protection.\n13.4.3\nCaching\nA cacheis aregionoffast memory that holds copies ofdata. Access tothe cached\ncopy is more efﬁcient than access to the original. For instance, the instructions 13.4\nKernel I/O Subsystem\n607",
  "Kernel I/O Subsystem\n607\nof the currently running process are stored on disk, cached in physical memory,\nand copied again in the CPU’s secondary and primary caches. The difference\nbetween a buffer and a cache is that a buffer may hold the only existing copy\nof a data item, whereas a cache, by deﬁnition, holds a copy on faster storage of\nan item that resides elsewhere.\nCaching and buffering are distinct functions, but sometimes a region\nof memory can be used for both purposes. For instance, to preserve copy\nsemantics and to enable efﬁcient scheduling of disk I/O, the operating system\nuses buffers in main memory to hold disk data. These buffers are also used as\na cache, to improve the I/O efﬁciency for ﬁles that are shared by applications",
  "or that are being written and reread rapidly. When the kernel receives a ﬁle\nI/O request, the kernel ﬁrst accesses the buffer cache to see whether that region\nof the ﬁle is already available in main memory. If it is, a physical disk I/O\ncan be avoided or deferred. Also, disk writes are accumulated in the buffer\ncache for several seconds, so that large transfers are gathered to allow efﬁcient\nwrite schedules. This strategy of delaying writes to improve I/O efﬁciency is\ndiscussed, in the context of remote ﬁle access, in Section 17.9.2.\n13.4.4\nSpooling and Device Reservation\nA spool is a buffer that holds output for a device, such as a printer, that cannot\naccept interleaved data streams. Although a printer can serve only one job",
  "at a time, several applications may wish to print their output concurrently,\nwithout having their output mixed together. The operating system solves this\nproblem by intercepting all output to the printer. Each application’s output\nis spooled to a separate disk ﬁle. When an application ﬁnishes printing, the\nspooling system queues the corresponding spool ﬁle for output to the printer.\nThe spooling system copies the queued spool ﬁles to the printer one at a time. In\nsome operating systems, spooling is managed by a system daemon process. In\nothers, it is handled by an in-kernel thread. In either case, the operating system\nprovides a control interface that enables users and system administrators to\ndisplay the queue, remove unwanted jobs before those jobs print, suspend",
  "printing while the printer is serviced, and so on.\nSome devices, such as tape drives and printers, cannot usefully multiplex\nthe I/O requests of multiple concurrent applications. Spooling is one way\noperating systems can coordinate concurrent output. Another way to deal with\nconcurrent device access is to provide explicit facilities for coordination. Some\noperating systems (including VMS) provide support for exclusive device access\nby enabling a process to allocate an idle device and to deallocate that device\nwhen it is no longer needed. Other operating systems enforce a limit of one\nopen ﬁle handle to such a device. Many operating systems provide functions\nthat enable processes to coordinate exclusive access among themselves. For",
  "instance, Windows provides system calls to wait until a device object becomes\navailable. It alsohasaparameter totheOpenFile()systemcall that declaresthe\ntypes of access to be permitted to other concurrent threads. On these systems,\nit is up to the applications to avoid deadlock.\n13.4.5\nError Handling\nAn operating system that uses protected memory can guard against many\nkinds of hardware and application errors, so that a complete system failure is 608\nChapter 13\nI/O Systems\nnot the usual result of each minor mechanical malfunction. Devices and I/O\ntransfers can fail in many ways, either for transient reasons, as when a network\nbecomes overloaded, or for “permanent” reasons, as when a disk controller\nbecomes defective. Operating systems can often compensate effectively for",
  "transient failures. For instance, a disk read() failure results in a read() retry,\nand a network send() error results in a resend(), if the protocol so speciﬁes.\nUnfortunately, if an important component experiences a permanent failure, the\noperating system is unlikely to recover.\nAs a general rule, an I/O system call will return one bit of information\nabout the status of the call, signifying either success or failure. In the UNIX\noperating system, an additional integer variable named errno is used to\nreturn an error code—one of about a hundred values—indicating the general\nnature of the failure (for example, argument out of range, bad pointer, or\nﬁle not open). By contrast, some hardware can provide highly detailed error",
  "information, although many current operating systems are not designed to\nconvey this information to the application. For instance, a failure of a SCSI\ndevice is reported by the SCSI protocol in three levels of detail: a sense key that\nidentiﬁes the general nature of the failure, such as a hardware error or an illegal\nrequest; an additional sense code that states the category of failure, such as a\nbad command parameter or a self-test failure; and an additional sense-code\nqualiﬁer that gives even more detail, such as which command parameter was\nin error or which hardware subsystem failed its self-test. Further, many SCSI\ndevices maintain internal pages of error-log information that can be requested\nby the host—but seldom are.\n13.4.6\nI/O Protection",
  "by the host—but seldom are.\n13.4.6\nI/O Protection\nErrors are closely related to the issue of protection. A user process may\naccidentally or purposely attempt to disrupt the normal operation of a system\nby attempting to issue illegal I/O instructions. We can use various mechanisms\nto ensure that such disruptions cannot take place in the system.\nTo prevent users from performing illegal I/O, we deﬁne all I/O instructions\nto be privileged instructions. Thus, users cannot issue I/O instructions directly;\nthey must do it through the operating system. To do I/O, a user program\nexecutes a system call to request that the operating system perform I/O on its\nbehalf (Figure 13.11). The operating system, executing in monitor mode, checks",
  "that the request is valid and, if it is, does the I/O requested. The operating\nsystem then returns to the user.\nIn addition, any memory-mapped and I/O port memory locations must\nbe protected from user access by the memory-protection system. Note that a\nkernel cannot simply deny all user access. Most graphics games and video\nediting and playback software need direct access to memory-mapped graphics\ncontroller memory to speed the performance of the graphics, for example. The\nkernel might in this case provide a locking mechanism to allow a section of\ngraphics memory (representing a window on screen) to be allocated to one\nprocess at a time.\n13.4.7\nKernel Data Structures\nThe kernel needs to keep state information about the use of I/O components.",
  "It does so through a variety of in-kernel data structures, such as the open-ﬁle 13.4\nKernel I/O Subsystem\n609\nkernel\n2\nperform I/O\ncase n\nsystem call n\nread\n3\nreturn\nto user\n1\ntrap to\nmonitor\nuser\nprogram\n• \n• \n•\n• \n• \n•\n• \n• \n•\n• \n• \n•\nFigure 13.11\nUse of a system call to perform I/O.\ntable structure from Section 12.1. The kernel uses many similar structures to\ntrack network connections, character-device communications, and other I/O\nactivities.\nUNIX provides ﬁle-system access to a variety of entities, such as user ﬁles,\nraw devices, and the address spaces of processes. Although each of these\nentities supports a read() operation, the semantics differ. For instance, to\nread a user ﬁle, the kernel needs to probe the buffer cache before deciding",
  "whether to perform a disk I/O. To read a raw disk, the kernel needs to ensure\nthat the request size is a multiple of the disk sector size and is aligned on a\nsector boundary. To read a process image, it is merely necessary to copy data\nfrom memory. UNIX encapsulates these differences within a uniform structure\nby using an object-oriented technique. The open-ﬁle record, shown in Figure\n13.12, contains a dispatch table that holds pointers to the appropriate routines,\ndepending on the type of ﬁle.\nSome operating systems use object-oriented methods even more exten-\nsively. For instance, Windows uses a message-passing implementation for I/O.\nAn I/O request is converted into a message that is sent through the kernel to\nthe I/O manager and then to the device driver, each of which may change the",
  "message contents. For output, the message contains the data to be written. For\ninput, the message contains a buffer to receive the data. The message-passing\napproach can add overhead, by comparison with procedural techniques that\nuse shared data structures, but it simpliﬁes the structure and design of the I/O\nsystem and adds ﬂexibility. 610\nChapter 13\nI/O Systems\nactive-inode \ntable\nnetwork-\ninformation\ntable\nper-process\nopen-file table\nuser-process memory\nsystem-wide open-file table\nkernel memory\n•\n•\n•\n•\n•\n•\nfile-system record\ninode pointer\npointer to read and write functions\npointer to select function\npointer to ioctl function\npointer to close function\nnetworking (socket) record\npointer to network info\npointer to read and write functions\npointer to select function",
  "pointer to select function\npointer to ioctl function\npointer to close function\nfile descriptor\nFigure 13.12\nUNIX I/O kernel structure.\n13.4.8\nKernel I/O Subsystem Summary\nIn summary, the I/O subsystem coordinates an extensive collection of services\nthat are available to applications and to other parts of the kernel. The I/O\nsubsystem supervises these procedures:\n• Management of the name space for ﬁles and devices\n• Access control to ﬁles and devices\n• Operation control (for example, a modem cannot seek())\n• File-system space allocation\n• Device allocation\n• Buffering, caching, and spooling\n• I/O scheduling\n• Device-status monitoring, error handling, and failure recovery\n• Device-driver conﬁguration and initialization\nThe upper levels of the I/O subsystem access devices via the uniform",
  "interface provided by the device drivers. 13.5\nTransforming I/O Requests to Hardware Operations\n611\n13.5 Transforming I/O Requests to Hardware Operations\nEarlier, we described the handshaking between a device driver and a device\ncontroller, but we did not explain how the operating system connects an\napplication request to a set of network wires or to a speciﬁc disk sector.\nConsider, for example, reading a ﬁle from disk. The application refers to the\ndata by a ﬁle name. Within a disk, the ﬁle system maps from the ﬁle name\nthrough the ﬁle-system directories to obtain the space allocation of the ﬁle. For\ninstance, in MS-DOS, the name maps to a number that indicates an entry in the\nﬁle-access table, and that table entry tells which disk blocks are allocated to",
  "the ﬁle. In UNIX, the name maps to an inode number, and the corresponding\ninode contains the space-allocation information. But how is the connection\nmade from the ﬁle name to the disk controller (the hardware port address or\nthe memory-mapped controller registers)?\nOne method is that used by MS-DOS, a relatively simple operating system.\nThe ﬁrst part of an MS-DOS ﬁle name, preceding the colon, is a string that\nidentiﬁes a speciﬁc hardware device. For example, C: is the ﬁrst part of every\nﬁle name on the primary hard disk. The fact that C: represents the primary hard\ndisk is built into the operating system; C: is mapped to a speciﬁc port address\nthrough a device table. Because of the colon separator, the device name space",
  "is separate from the ﬁle-system name space. This separation makes it easy\nfor the operating system to associate extra functionality with each device. For\ninstance, it is easy to invoke spooling on any ﬁles written to the printer.\nIf, instead, the device name space is incorporated in the regular ﬁle-system\nname space, as it is in UNIX, the normal ﬁle-system name services are provided\nautomatically. If the ﬁle system provides ownership and access control to all\nﬁle names, then devices have owners and access control. Since ﬁles are stored\non devices, such an interface provides access to the I/O system at two levels.\nNames can be used to access the devices themselves or to access the ﬁles stored\non the devices.\nUNIX represents device names in the regular ﬁle-system name space. Unlike",
  "an MS-DOS ﬁle name, which has a colon separator, a UNIX path name has no\nclear separation of the device portion. In fact, no part of the path name is the\nname of a device. UNIX has a mount table that associates preﬁxes of path names\nwith speciﬁc device names. To resolve a path name, UNIX looks up the name in\nthe mount table to ﬁnd the longest matching preﬁx; the corresponding entry\nin the mount table gives the device name. This device name also has the form\nof a name in the ﬁle-system name space. When UNIX looks up this name in the\nﬁle-system directory structures, it ﬁnds not an inode number but a <major,\nminor> device number. The major device number identiﬁes a device driver\nthat should be called to handle I/O to this device. The minor device number",
  "is passed to the device driver to index into a device table. The corresponding\ndevice-table entry gives the port address or the memory-mapped address of\nthe device controller.\nModern operating systems gain signiﬁcant ﬂexibility from the multiple\nstages of lookup tables in the path between a request and a physical device\ncontroller. The mechanisms that pass requests between applications and\ndrivers are general. Thus, we can introduce new devices and drivers into a\ncomputer without recompiling the kernel. In fact, some operating systems\nhave the ability to load device drivers on demand. At boot time, the system 612\nChapter 13\nI/O Systems\nsend request to device\ndriver, block process if\nappropriate\nmonitor device,\ninterrupt when I/O\ncompleted\nprocess request, issue\ncommands to controller,",
  "process request, issue\ncommands to controller,\nconfigure controller to\nblock until interrupted\nrequest I/O\nsystem call\nno\nyes\nI/O completed,\ninput data available, or\noutput completed\nuser\nprocess\nkernel\nI/O subsystem\nkernel\nI/O subsystem\ndevice\ndriver\ndevice\ncontroller\ntime\ninterrupt\nhandler\ntransfer data\n(if appropriate) to process,\nreturn completion\nor error code\ndetermine which I/O\ncompleted, indicate state\nchange to I/O subsystem\nreceive interrupt, store\ndata in device-driver buffer\nif input, signal to unblock\ndevice driver\nI/O completed,\ngenerate interrupt\nreturn from system call\ninterrupt\ndevice-controller commands\ncan already\nsatisfy request?\nFigure 13.13\nThe life cycle of an I/O request.\nﬁrst probes the hardware buses to determine what devices are present. It then",
  "loads in the necessary drivers, either immediately or when ﬁrst required by an\nI/O request.\nWe next describe the typical life cycle of a blocking read request, as depicted\nin Figure 13.13. The ﬁgure suggests that an I/O operation requires a great many\nsteps that together consume a tremendous number of CPU cycles.\n1. A process issues a blocking read() system call to a ﬁle descriptor of a ﬁle\nthat has been opened previously.\n2. The system-call code in the kernel checks the parameters for correctness.\nIn the case of input, if the data are already available in the buffer cache,\nthe data are returned to the process, and the I/O request is completed. 13.6\nSTREAMS\n613\n3. Otherwise, a physical I/O must be performed. The process is removed",
  "from the run queue and is placed on the wait queue for the device, and\nthe I/O request is scheduled. Eventually, the I/O subsystem sends the\nrequest to the device driver. Depending on the operating system, the\nrequest is sent via a subroutine call or an in-kernel message.\n4. The device driver allocates kernel buffer space to receive the data and\nschedules the I/O. Eventually, the driver sends commands to the device\ncontroller by writing into the device-control registers.\n5. The device controller operates the device hardware to perform the data\ntransfer.\n6. The driver may poll for status and data, or it may have set up a DMA\ntransfer into kernel memory. We assume that the transfer is managed\nby a DMA controller, which generates an interrupt when the transfer\ncompletes.",
  "completes.\n7. The correct interrupt handler receives the interrupt via the interrupt-\nvector table, stores any necessary data, signals the device driver, and\nreturns from the interrupt.\n8. The device driver receives the signal, determines which I/O request has\ncompleted, determines the request’s status, and signals the kernel I/O\nsubsystem that the request has been completed.\n9. The kernel transfers data or return codes to the address space of the\nrequesting process and moves the process from the wait queue back to\nthe ready queue.\n10. Moving the process to the ready queue unblocks the process. When the\nscheduler assigns the process to the CPU, the process resumes execution\nat the completion of the system call.\n13.6 STREAMS",
  "13.6 STREAMS\nUNIX System V has an interesting mechanism, called STREAMS, that enables\nan application to assemble pipelines of driver code dynamically. A stream is\na full-duplex connection between a device driver and a user-level process. It\nconsists of a stream head that interfaces with the user process, a driver end\nthat controls the device, and zero or more stream modules between the stream\nhead and the driver end. Each of these components contains a pair of queues\n—a read queue and a write queue. Message passing is used to transfer data\nbetween queues. The STREAMS structure is shown in Figure 13.14.\nModules provide the functionality of STREAMS processing; they are pushed\nonto a stream by use of the ioctl() system call. For example, a process can",
  "open a serial-port device via a stream and can push on a module to handle\ninput editing. Because messages are exchanged between queues in adjacent\nmodules, a queue in one module may overﬂow an adjacent queue. To prevent\nthis from occurring, a queue may support ﬂow control. Without ﬂow control,\na queue accepts all messages and immediately sends them on to the queue\nin the adjacent module without buffering them. A queue that supports ﬂow 614\nChapter 13\nI/O Systems\nuser process\ndevice\nstream head\ndriver end\nread queue\nwrite queue\nSTREAMS\nmodules\nread queue\nread queue\nread queue\nwrite queue\nwrite queue\nwrite queue\nFigure 13.14\nThe STREAMS structure.\ncontrol buffers messages and does not accept messages without sufﬁcient\nbuffer space. This process involves exchanges of control messages between",
  "queues in adjacent modules.\nA user process writes data to a device using either the write() or putmsg()\nsystem call. The write() system call writes raw data to the stream, whereas\nputmsg() allows the user process to specify a message. Regardless of the\nsystem call used by the user process, the stream head copies the data into a\nmessage and delivers it to the queue for the next module in line. This copying of\nmessages continues until the message is copied to the driver end and hence the\ndevice. Similarly, the user process reads data from the stream head using either\nthe read() or getmsg() system call. If read() is used, the stream head gets\na message from its adjacent queue and returns ordinary data (an unstructured",
  "byte stream) to the process. If getmsg() is used, a message is returned to the\nprocess.\nSTREAMS I/O is asynchronous (or nonblocking) except when the user\nprocess communicates with the stream head. When writing to the stream,\nthe user process will block, assuming the next queue uses ﬂow control, until\nthere is room to copy the message. Likewise, the user process will block when\nreading from the stream until data are available.\nAs mentioned, the driver end—like the stream head and modules—has\na read and write queue. However, the driver end must respond to interrupts,\nsuch as one triggered when a frame is ready to be read from a network. Unlike\nthe stream head, which may block if it is unable to copy a message to the\nnext queue in line, the driver end must handle all incoming data. Drivers",
  "must support ﬂow control as well. However, if a device’s buffer is full, the 13.7\nPerformance\n615\ndevice typically resorts to dropping incoming messages. Consider a network\ncard whose input buffer is full. The network card must simply drop further\nmessages until there is enough buffer space to store incoming messages.\nThe beneﬁt of using STREAMS is that it provides a framework for a\nmodular and incremental approach to writing device drivers and network\nprotocols. Modules may be used by different streams and hence by different\ndevices. For example, a networking module may be used by both an Ethernet\nnetwork card and a 802.11 wireless network card. Furthermore, rather than\ntreating character-device I/O as an unstructured byte stream, STREAMS allows",
  "support for message boundaries and control information when communicating\nbetween modules. Most UNIX variants support STREAMS, and it is the preferred\nmethod for writing protocols and device drivers. For example, System V UNIX\nand Solaris implement the socket mechanism using STREAMS.\n13.7 Performance\nI/O is a major factor in system performance. It places heavy demands on the CPU\nto execute device-driver code and to schedule processes fairly and efﬁciently\nas they block and unblock. The resulting context switches stress the CPU and its\nhardware caches. I/O also exposes any inefﬁciencies in the interrupt-handling\nmechanisms in the kernel. In addition, I/O loads down the memory bus during\ndata copies between controllers and physical memory and again during copies",
  "between kernel buffers and application data space. Coping gracefully with all\nthese demands is one of the major concerns of a computer architect.\nAlthough modern computers can handle many thousands of interrupts per\nsecond, interrupt handling is a relatively expensive task. Each interrupt causes\nthe system to perform a state change, to execute the interrupt handler, and then\nto restore state. Programmed I/O can be more efﬁcient than interrupt-driven\nI/O, if the number of cycles spent in busy waiting is not excessive. An I/O\ncompletion typically unblocks a process, leading to the full overhead of a\ncontext switch.\nNetwork trafﬁc can also cause a high context-switch rate. Consider, for\ninstance, a remote login from one machine to another. Each character typed",
  "on the local machine must be transported to the remote machine. On the local\nmachine, the character is typed; a keyboard interrupt is generated; and the\ncharacter is passed through the interrupt handler to the device driver, to the\nkernel, and then to the user process. The user process issues a network I/O\nsystem call to send the character to the remote machine. The character then\nﬂows into the local kernel, through the network layers that construct a network\npacket, and into the network device driver. The network device driver transfers\nthe packet to the network controller, which sends the character and generates\nan interrupt. The interrupt is passed back up through the kernel to cause the\nnetwork I/O system call to complete.",
  "network I/O system call to complete.\nNow, the remote system’s network hardware receives the packet, and an\ninterrupt is generated. The character is unpacked from the network protocols\nand is given to the appropriate network daemon. The network daemon\nidentiﬁes which remote login session is involved and passes the packet to\nthe appropriate subdaemon for that session. Throughout this ﬂow, there are 616\nChapter 13\nI/O Systems\nnetwork\nkernel\nuser\nprocess\nkernel\ndevice\ndriver\ninterrupt\nhandled\ndevice\ndriver\nnetwork\nadapter\ninterrupt\ngenerated\ninterrupt\nhandled\ninterrupt\ngenerated\nsystem call\ncompletes\nsending system\nstate\nsave\nstate\nsave\nstate\nsave\ncontext\nswitch\ncontext\nswitch\nhard-\nware\nhard-\nware\nreceiving system\ncharacter\ntyped\ncontext\nswitch\ncontext\nswitch\ncontext\nswitch\ncontext\nswitch",
  "context\nswitch\ncontext\nswitch\ncontext\nswitch\nkernel\nnetwork\ndaemon\nnetwork\nsubdaemon\nkernel\ndevice\ndriver\ninterrupt\ngenerated\nnetwork\nadapter\nnetwork\npacket\nreceived\nFigure 13.15\nIntercomputer communications.\ncontext switches and state switches (Figure 13.15). Usually, the receiver echoes\nthe character back to the sender; that approach doubles the work.\nTo eliminate the context switches involved in moving each character\nbetween daemons and the kernel, the Solaris developers reimplemented the\ntelnet daemon using in-kernel threads. Sun estimated that this improvement\nincreased the maximum number of network logins from a few hundred to a\nfew thousand on a large server.\nOther systems use separate front-end processors for terminal I/O to reduce",
  "the interrupt burden on the main CPU. For instance, a terminal concentrator\ncan multiplex the trafﬁc from hundreds of remote terminals into one port on a\nlarge computer. An I/O channel is a dedicated, special-purpose CPU found in\nmainframes and in other high-end systems. The job of a channel is to ofﬂoad\nI/O work from the main CPU. The idea is that the channels keep the data ﬂowing\nsmoothly, while the main CPU remains free to process the data. Like the device\ncontrollers and DMA controllers found in smaller computers, a channel can\nprocess more general and sophisticated programs, so channels can be tuned\nfor particular workloads. 13.7\nPerformance\n617\nWe can employ several principles to improve the efﬁciency of I/O:\n• Reduce the number of context switches.",
  "• Reduce the number of context switches.\n• Reduce the number of times that data must be copied in memory while\npassing between device and application.\n• Reduce the frequency of interrupts by using large transfers, smart con-\ntrollers, and polling (if busy waiting can be minimized).\n• Increase concurrency by using DMA-knowledgeable controllers or chan-\nnels to ofﬂoad simple data copying from the CPU.\n• Move processing primitives into hardware, to allow their operation in\ndevice controllers to be concurrent with CPU and bus operation.\n• Balance CPU, memory subsystem, bus, and I/O performance, because an\noverload in any one area will cause idleness in others.\nI/O devices vary greatly in complexity. For instance, a mouse is simple. The",
  "mouse movements and button clicks are converted into numeric values that\nare passed from hardware, through the mouse device driver, to the application.\nBy contrast, the functionality provided by the Windows disk device driver is\ncomplex. It not only manages individual disks but also implements RAID arrays\n(Section 10.7). To do so, it converts an application’s read or write request into a\ncoordinated set of disk I/O operations. Moreover, it implements sophisticated\nerror-handling and data-recovery algorithms and takes many steps to optimize\ndisk performance.\nWhere should the I/O functionality be implemented—in the device hard-\nware, in the device driver, or in application software? Sometimes we observe\nthe progression depicted in Figure 13.16.\napplication code\nkernel code",
  "application code\nkernel code\ndevice-driver code\ndevice-controller code (hardware)\ndevice code (hardware)\nnew algorithm\nincreased flexibility\nincreased abstraction\nincreased development cost\nincreased efficiency\nincreased time (generations)\nFigure 13.16\nDevice functionality progression. 618\nChapter 13\nI/O Systems\n• Initially, we implement experimental I/O algorithms at the application\nlevel, because application code is ﬂexible and application bugs are unlikely\nto cause system crashes. Furthermore, by developing code at the applica-\ntion level, we avoid the need to reboot or reload device drivers after every\nchange to the code. An application-level implementation can be inefﬁcient,\nhowever, because of the overhead of context switches and because the",
  "application cannot take advantage of internal kernel data structures and\nkernel functionality (such as efﬁcient in-kernel messaging, threading, and\nlocking).\n• When an application-level algorithm has demonstrated its worth, we may\nreimplement it in the kernel. This can improve performance, but the devel-\nopment effort is more challenging, because an operating-system kernel is\na large, complex software system. Moreover, an in-kernel implementa-\ntion must be thoroughly debugged to avoid data corruption and system\ncrashes.\n• The highest performance may be obtained through a specialized imple-\nmentation in hardware, either in the device or in the controller. The\ndisadvantages of a hardware implementation include the difﬁculty and",
  "expense of making further improvements or of ﬁxing bugs, the increased\ndevelopment time (months rather than days), and the decreased ﬂexibility.\nFor instance, a hardware RAID controller may not provide any means for\nthe kernel to inﬂuence the order or location of individual block reads and\nwrites, even if the kernel has special information about the workload that\nwould enable it to improve the I/O performance.\n13.8 Summary\nThe basic hardware elements involved in I/O are buses, device controllers, and\nthe devices themselves. The work of moving data between devices and main\nmemory is performed by the CPU as programmed I/O or is ofﬂoaded to a DMA\ncontroller. The kernel module that controls a device is a device driver. The",
  "system-call interface provided to applications is designed to handle several\nbasic categories of hardware, including block devices, character devices,\nmemory-mapped ﬁles, network sockets, and programmed interval timers. The\nsystem calls usually block the processes that issue them, but nonblocking and\nasynchronous calls are used by the kernel itself and by applications that must\nnot sleep while waiting for an I/O operation to complete.\nThe kernel’s I/O subsystem provides numerous services. Among these\nare I/O scheduling, buffering, caching, spooling, device reservation, and error\nhandling. Another service, name translation, makes the connections between\nhardware devices and the symbolic ﬁle names used by applications. It involves",
  "several levels of mapping that translate from character-string names, to speciﬁc\ndevice drivers and device addresses, and then to physical addresses of I/Oports\nor bus controllers. This mapping may occur within the ﬁle-system name space,\nas it does in UNIX, or in a separate device name space, as it does in MS-DOS.\nSTREAMS is an implementation and methodology that provides a frame-\nwork for a modular and incremental approach to writing device drivers and Exercises\n619\nnetwork protocols. Through streams, drivers can be stacked, with data passing\nthrough them sequentially and bidirectionally for processing.\nI/O system calls are costly in terms of CPU consumption because of the\nmany layers of software between a physical device and an application. These",
  "layers imply overhead from several sources: context switching to cross the\nkernel’s protection boundary, signal and interrupt handling to service the I/O\ndevices, and the load on the CPU and memory system to copy data between\nkernel buffers and application space.\nPractice Exercises\n13.1\nState three advantages of placing functionality in a device controller,\nrather than in the kernel. State three disadvantages.\n13.2\nThe example of handshaking in Section 13.2 used two bits: a busy bit\nand a command-ready bit. Is it possible to implement this handshaking\nwith only one bit? If it is, describe the protocol. If it is not, explain why\none bit is insufﬁcient.\n13.3\nWhy might a system use interrupt-driven I/O to manage a single serial",
  "port and polling I/O to manage a front-end processor, such as a terminal\nconcentrator?\n13.4\nPolling for an I/O completion can waste a large number of CPU cycles\nif the processor iterates a busy-waiting loop many times before the I/O\ncompletes. But if the I/O device is ready for service, polling can be much\nmore efﬁcient than is catching and dispatching an interrupt. Describe\na hybrid strategy that combines polling, sleeping, and interrupts for\nI/O device service. For each of these three strategies (pure polling, pure\ninterrupts, hybrid), describe a computing environment in which that\nstrategy is more efﬁcient than is either of the others.\n13.5\nHow does DMA increase system concurrency? How does it complicate\nhardware design?\n13.6",
  "hardware design?\n13.6\nWhy is it important to scale up system-bus and device speeds as CPU\nspeed increases?\n13.7\nDistinguish between a STREAMS driver and a STREAMS module.\nExercises\n13.8\nWhen multiple interrupts from different devices appear at about the\nsame time, a priority scheme could be used to determine the order in\nwhich the interrupts would be serviced. Discuss what issues need to\nbe considered in assigning priorities to different interrupts.\n13.9\nWhat are the advantages and disadvantages of supporting memory-\nmapped I/O to device control registers? 620\nChapter 13\nI/O Systems\n13.10\nConsider the following I/O scenarios on a single-user PC:\na.\nA mouse used with a graphical user interface\nb.\nA tape drive on a multitasking operating system (with no device\npreallocation available)\nc.",
  "preallocation available)\nc.\nA disk drive containing user ﬁles\nd.\nA graphics card with direct bus connection, accessible through\nmemory-mapped I/O\nFor each of these scenarios, would you design the operating system\nto use buffering, spooling, caching, or a combination? Would you use\npolled I/O or interrupt-driven I/O? Give reasons for your choices.\n13.11\nIn most multiprogrammed systems, user programs access memory\nthrough virtual addresses, while the operating system uses raw phys-\nical addresses to access memory. What are the implications of this\ndesign for the initiation of I/O operations by the user program and\ntheir execution by the operating system?\n13.12\nWhat are the various kinds of performance overhead associated with\nservicing an interrupt?\n13.13",
  "servicing an interrupt?\n13.13\nDescribe three circumstances under which blocking I/O should be used.\nDescribe three circumstances under which nonblocking I/O should be\nused. Why not just implement nonblocking I/O and have processes\nbusy-wait until their devices are ready?\n13.14\nTypically, at the completion of a device I/O, a single interrupt is raised\nand appropriately handled by the host processor. In certain settings,\nhowever, the code that is to be executed at the completion of the\nI/O can be broken into two separate pieces. The ﬁrst piece executes\nimmediately after the I/O completes and schedules a second interrupt\nfor the remaining piece of code to be executed at a later time. What is\nthe purpose of using this strategy in the design of interrupt handlers?\n13.15",
  "13.15\nSome DMA controllers support direct virtual memory access, where\nthe targets of I/O operations are speciﬁed as virtual addresses and\na translation from virtual to physical address is performed during\nthe DMA. How does this design complicate the design of the DMA\ncontroller? What are the advantages of providing such functionality?\n13.16\nUNIX coordinates the activities of the kernel I/O components by\nmanipulating shared in-kernel data structures, whereas Windows\nuses object-oriented message passing between kernel I/O components.\nDiscuss three pros and three cons of each approach.\n13.17\nWrite (in pseudocode) an implementation of virtual clocks, including\nthe queueing and management of timer requests for the kernel and",
  "applications. Assume that the hardware provides three timer channels.\n13.18\nDiscuss the advantages and disadvantages of guaranteeing reliable\ntransfer of data between modules in the STREAMS abstraction. Bibliography\n621\nBibliographical Notes\n[Vahalia (1996)] provides a good overview of I/O and networking in UNIX.\n[McKusick and Neville-Neil (2005)] detail the I/O structures and methods\nemployed in FreeBSD. The use and programming of the various interprocess-\ncommunication and network protocols in UNIX are explored in [Stevens (1992)].\n[Hart (2005)] covers Windows programming.\n[Intel (2011)] provides a good source for Intel processors. [Rago (1993)]\nprovides a good discussion of STREAMS. [Hennessy and Patterson (2012)]\ndescribe multiprocessor systems and cache-consistency issues.",
  "Bibliography\n[Hart (2005)]\nJ. M. Hart, Windows System Programming, Third Edition, Addison-\nWesley (2005).\n[Hennessy and Patterson (2012)]\nJ. Hennessy and D. Patterson, Computer Archi-\ntecture: A Quantitative Approach, Fifth Edition, Morgan Kaufmann (2012).\n[Intel (2011)]\nIntel 64 and IA-32 Architectures Software Developer’s Manual, Com-\nbined Volumes: 1, 2A, 2B, 3A and 3B. Intel Corporation (2011).\n[McKusick and Neville-Neil (2005)]\nM. K. McKusick and G. V. Neville-Neil,\nThe Design and Implementation of the FreeBSD UNIX Operating System, Addison\nWesley (2005).\n[Rago (1993)]\nS. Rago, UNIX System V Network Programming, Addison-Wesley\n(1993).\n[Stevens (1992)]\nR. Stevens, Advanced Programming in the UNIX Environment,\nAddison-Wesley (1992).\n[Vahalia (1996)]",
  "Addison-Wesley (1992).\n[Vahalia (1996)]\nU. Vahalia, Unix Internals: The New Frontiers, Prentice Hall\n(1996).  Part Five\nProtection and\nSecurity\nProtection mechanisms control access to a system by limiting the types\nof ﬁle access permitted to users. In addition, protection must ensure\nthat only processes that have gained proper authorization from the\noperating system can operate on memory segments, the CPU, and other\nresources.\nProtection is provided by a mechanism that controls the access of\nprograms, processes, or users to the resources deﬁned by a computer\nsystem. This mechanism must provide a means for specifying the controls\nto be imposed, together with a means of enforcing them.\nSecurity ensures the authentication of system users to protect the",
  "integrity of the information stored in the system (both data and code),\nas well as the physical resources of the computer system. The security\nsystem prevents unauthorized access, malicious destruction or alteration\nof data, and accidental introduction of inconsistency.  14\nC H A P T E R\nProtection\nThe processes in an operating system must be protected from one another’s\nactivities. To provide such protection, we can use various mechanisms to ensure\nthat only processes that have gained proper authorization from the operating\nsystem can operate on the ﬁles, memory segments, CPU, and other resources\nof a system.\nProtection refers to a mechanism for controlling the access of programs,\nprocesses, or users to the resources deﬁned by a computer system. This",
  "mechanism must provide a means for specifying the controls to be imposed,\ntogether with a means of enforcement. We distinguish between protection and\nsecurity, which is a measure of conﬁdence that the integrity of a system and\nits data will be preserved. In this chapter, we focus on protection. Security\nassurance is a much broader topic, and we address it in Chapter 15.\nCHAPTER OBJECTIVES\n• To discuss the goals and principles of protection in a modern computer\nsystem.\n• To explain how protection domains, combined with an access matrix, are\nused to specify the resources a process may access.\n• To examine capability- and language-based protection systems.\n14.1\nGoals of Protection\nAs computer systems have become more sophisticated and pervasive in their",
  "applications, the need to protect their integrity has also grown. Protection was\noriginally conceived as an adjunct to multiprogramming operating systems,\nso that untrustworthy users might safely share a common logical name space,\nsuch as a directory of ﬁles, or share a common physical name space, such as\nmemory. Modern protection concepts have evolved to increase the reliability\nof any complex system that makes use of shared resources.\nWe need to provide protection for several reasons. The most obvious is the\nneed to prevent the mischievous, intentional violation of an access restriction\n625 626\nChapter 14\nProtection\nby a user. Of more general importance, however, is the need to ensure that\neach program component active in a system uses system resources only in",
  "ways consistent with stated policies. This requirement is an absolute one for a\nreliable system.\nProtection can improve reliability by detecting latent errors at the interfaces\nbetween component subsystems. Early detection of interface errors can often\nprevent contamination of a healthy subsystem by a malfunctioning subsystem.\nAlso, an unprotected resource cannot defend against use (or misuse) by an\nunauthorized or incompetent user. A protection-oriented system provides\nmeans to distinguish between authorized and unauthorized usage.\nThe role of protection in a computer system is to provide a mechanism for\nthe enforcement of the policies governing resource use. These policies can be\nestablished in a variety of ways. Some are ﬁxed in the design of the system,",
  "while others are formulated by the management of a system. Still others are\ndeﬁned by the individual users to protect their own ﬁles and programs. A\nprotection system must have the ﬂexibility to enforce a variety of policies.\nPolicies for resource use may vary by application, and they may change\nover time. For these reasons, protection is no longer the concern solely of\nthe designer of an operating system. The application programmer needs to\nuse protection mechanisms as well, to guard resources created and supported\nby an application subsystem against misuse. In this chapter, we describe the\nprotection mechanisms the operating system should provide, but application\ndesigners can use them as well in designing their own protection software.",
  "Note that mechanisms are distinct from policies. Mechanisms determine\nhow something will be done; policies decide what will be done. The separation\nof policy and mechanism is important for ﬂexibility. Policies are likely to\nchange from place to place or time to time. In the worst case, every change\nin policy would require a change in the underlying mechanism. Using general\nmechanisms enables us to avoid such a situation.\n14.2 Principles of Protection\nFrequently, a guiding principle can be used throughout a project, such as\nthe design of an operating system. Following this principle simpliﬁes design\ndecisions and keeps the system consistent and easy to understand. A key,\ntime-tested guiding principle for protection is the principle of least privilege. It",
  "dictates that programs, users, and even systems be given just enough privileges\nto perform their tasks.\nConsider the analogy of a security guard with a passkey. If this key allows\nthe guard into just the public areas that she guards, then misuse of the key\nwill result in minimal damage. If, however, the passkey allows access to all\nareas, then damage from its being lost, stolen, misused, copied, or otherwise\ncompromised will be much greater.\nAn operating system following the principle of least privilege implements\nits features, programs, system calls, and data structures so that failure or\ncompromise of a component does the minimum damage and allows the\nminimum damage to be done. The overﬂow of a buffer in a system daemon",
  "might cause the daemon process to fail, for example, but should not allow the\nexecution of code from the daemon process’s stack that would enable a remote 14.3\nDomain of Protection\n627\nuser to gain maximum privileges and access to the entire system (as happens\ntoo often today).\nSuch an operating system also provides system calls and services that\nallow applications to be written with ﬁne-grained access controls. It provides\nmechanisms to enable privileges when they are needed and to disable them\nwhen they are not needed. Also beneﬁcial is the creation of audit trails for\nall privileged function access. The audit trail allows the programmer, system\nadministrator, or law-enforcement ofﬁcer to trace all protection and security\nactivities on the system.",
  "activities on the system.\nManaging users with the principle of least privilege entails creating a\nseparate account for each user, with just the privileges that the user needs. An\noperator who needs to mount tapes and back up ﬁles on the system has access\nto just those commands and ﬁles needed to accomplish the job. Some systems\nimplement role-based access control (RBAC) to provide this functionality.\nComputers implemented in a computing facility under the principle of least\nprivilege can be limited to running speciﬁc services, accessing speciﬁc remote\nhosts via speciﬁc services, and doing so during speciﬁc times. Typically, these\nrestrictions are implemented through enabling or disabling each service and\nthrough using access control lists, as described in Sections Section 11.6.2 and",
  "Section 14.6.\nThe principle of least privilege can help produce a more secure computing\nenvironment. Unfortunately, it frequently does not. For example, Windows\n2000 has a complex protection scheme at its core and yet has many security\nholes. By comparison, Solaris is considered relatively secure, even though it\nis a variant of UNIX, which historically was designed with little protection\nin mind. One reason for the difference may be that Windows 2000 has more\nlines of code and more services than Solaris and thus has more to secure and\nprotect. Another reason could be that the protection scheme in Windows 2000\nis incomplete or protects the wrong aspects of the operating system, leaving\nother areas vulnerable.\n14.3 Domain of Protection",
  "other areas vulnerable.\n14.3 Domain of Protection\nA computer system is a collection of processes and objects. By objects, we mean\nboth hardware objects (such as the CPU, memory segments, printers, disks, and\ntape drives) and software objects (such as ﬁles, programs, and semaphores).\nEach object has a unique name that differentiates it from all other objects in the\nsystem, and each can be accessed only through well-deﬁned and meaningful\noperations. Objects are essentially abstract data types.\nThe operations that are possible may depend on the object. For example,\non a CPU, we can only execute. Memory segments can be read and written,\nwhereas a CD-ROM or DVD-ROM can only be read. Tape drives can be read,\nwritten, and rewound. Data ﬁles can be created, opened, read, written, closed,",
  "and deleted; program ﬁles can be read, written, executed, and deleted.\nA process should be allowed to access only those resources for which it\nhas authorization. Furthermore, at any time, a process should be able to access\nonly those resources that it currently requires to complete its task. This second\nrequirement, commonly referred to as the need-to-know principle, is useful\nin limiting the amount of damage a faulty process can cause in the system. 628\nChapter 14\nProtection\nFor example, when process p invokes procedure A(), the procedure should be\nallowed to access only its own variables and the formal parameters passed to it;\nit should not be able to access all the variables of process p. Similarly, consider",
  "the case in which process p invokes a compiler to compile a particular ﬁle. The\ncompiler should not be able to access ﬁles arbitrarily but should have access\nonly to a well-deﬁned subset of ﬁles (such as the source ﬁle, listing ﬁle, and\nso on) related to the ﬁle to be compiled. Conversely, the compiler may have\nprivate ﬁles used for accounting or optimization purposes that process p should\nnot be able to access. The need-to-know principle is similar to the principle of\nleast privilege discussed in Section 14.2 in that the goals of protection are to\nminimize the risks of possible security violations.\n14.3.1\nDomain Structure\nTo facilitate the scheme just described, a process operates within a protection\ndomain, which speciﬁes the resources that the process may access. Each",
  "domain deﬁnes a set of objects and the types of operations that may be invoked\non each object. The ability to execute an operation on an object is an access\nright. A domain is a collection of access rights, each of which is an ordered\npair <object-name, rights-set>. For example, if domain D has the access\nright <file F, {read,write}>, then a process executing in domain D can both\nread and write ﬁle F. It cannot, however, perform any other operation on that\nobject.\nDomains may share access rights. For example, in Figure 14.1, we have\nthree domains: D1, D2, and D3. The access right <O4, {print}> is shared by D2\nand D3, implying that a process executing in either of these two domains can\nprint object O4. Note that a process must be executing in domain D1 to read",
  "and write object O1, while only processes in domain D3 may execute object O1.\nThe association between a process and a domain may be either static, if\nthe set of resources available to the process is ﬁxed throughout the process’s\nlifetime, or dynamic. As might be expected, establishing dynamic protection\ndomains is more complicated than establishing static protection domains.\nIf the association between processes and domains is ﬁxed, and we want to\nadhere to the need-to-know principle, then a mechanism must be available to\nchange the content of a domain. The reason stems from the fact that a process\nmay execute in two different phases and may, for example, need read access\nin one phase and write access in another. If a domain is static, we must deﬁne",
  "the domain to include both read and write access. However, this arrangement\nprovides more rights than are needed in each of the two phases, since we have\nread access in the phase where we need only write access, and vice versa.\nD1\n( O3, {read, write} )\n( O1, {read, write} )\n( O2, {execute} )\n( O1, {execute} )\n( O3, {read} )\n( O2, {write} ) ( O4, {print} )\nD2\nD3\nFigure 14.1\nSystem with three protection domains. 14.3\nDomain of Protection\n629\nThus, the need-to-know principle is violated. We must allow the contents of\na domain to be modiﬁed so that the domain always reﬂects the minimum\nnecessary access rights.\nIf the association is dynamic, a mechanism is available to allow domain\nswitching, enabling the process to switch from one domain to another. We may",
  "also want to allow the content of a domain to be changed. If we cannot change\nthe content of a domain, we can provide the same effect by creating a new\ndomain with the changed content and switching to that new domain when we\nwant to change the domain content.\nA domain can be realized in a variety of ways:\n• Each user may be a domain. In this case, the set of objects that can be\naccessed depends on the identity of the user. Domain switching occurs\nwhen the user is changed—generally when one user logs out and another\nuser logs in.\n• Each process may be a domain. In this case, the set of objects that can be\naccessed depends on the identity of the process. Domain switching occurs\nwhen one process sends a message to another process and then waits for\na response.",
  "a response.\n• Each procedure may be a domain. In this case, the set of objects that can be\naccessed corresponds to the local variables deﬁned within the procedure.\nDomain switching occurs when a procedure call is made.\nWe discuss domain switching in greater detail in Section 14.4.\nConsider the standard dual-mode (monitor–user mode) model of\noperating-system execution. When a process executes in monitor mode, it\ncan execute privileged instructions and thus gain complete control of the\ncomputer system. In contrast, when a process executes in user mode, it can\ninvoke only nonprivileged instructions. Consequently, it can execute only\nwithin its predeﬁned memory space. These two modes protect the operating\nsystem (executing in monitor domain) from the user processes (executing",
  "in user domain). In a multiprogrammed operating system, two protection\ndomains are insufﬁcient, since users also want to be protected from one\nanother. Therefore, a more elaborate scheme is needed. We illustrate such a\nscheme by examining two inﬂuential operating systems—UNIX and MULTICS\n—to see how they implement these concepts.\n14.3.2\nAn Example: UNIX\nIn the UNIX operating system, a domain is associated with the user. Switching\nthe domain corresponds to changing the user identiﬁcation temporarily.\nThis change is accomplished through the ﬁle system as follows. An owner\nidentiﬁcation and a domain bit (known as the setuid bit) are associated with\neach ﬁle. When the setuid bit is on, and a user executes that ﬁle, the userID is",
  "set to that of the owner of the ﬁle. When the bit is off, however, the userID\ndoes not change. For example, when a user A (that is, a user with userID =\nA) starts executing a ﬁle owned by B, whose associated domain bit is off, the\nuserID of the process is set to A. When the setuid bit is on, the userID is set to 630\nChapter 14\nProtection\nthat of the owner of the ﬁle: B. When the process exits, this temporary userID\nchange ends.\nOther methods are used to change domains in operating systems in which\nuserIDs are used for domain deﬁnition, because almost all systems need\nto provide such a mechanism. This mechanism is used when an otherwise\nprivileged facility needs to be made available to the general user population.",
  "For instance, it might be desirable to allow users to access a network without\nletting them write their own networking programs. In such a case, on a UNIX\nsystem, the setuid bit on a networking program would be set, causing the\nuserID to change when the program was run. The userID would change to\nthat of a user with network access privilege (such as root, the most powerful\nuserID). One problem with this method is that if a user manages to create a ﬁle\nwith userID root and with its setuid bit on, that user can become root and\ndo anything and everything on the system. The setuid mechanism is discussed\nfurther in Appendix A.\nAn alternative to this method used in some other operating systems is\nto place privileged programs in a special directory. The operating system is",
  "designed to change the userID of any program run from this directory, either\nto the equivalent of root or to the userID of the owner of the directory. This\neliminates one security problem, which occurs when intruders create programs\nto manipulate the setuid feature and hide the programs in the system for later\nuse (using obscure ﬁle or directory names). This method is less ﬂexible than\nthat used in UNIX, however.\nEven more restrictive, and thus more protective, are systems that simply\ndo not allow a change of userID. In these instances, special techniques must\nbe used to allow users access to privileged facilities. For instance, a daemon\nprocess may be started at boot time and run as a special userID. Users then\nrun a separate program, which sends requests to this process whenever they",
  "need to use the facility. This method is used by the TOPS-20 operating system.\nIn any of these systems, great care must be taken in writing privileged\nprograms. Any oversight can result in a total lack of protection on the system.\nGenerally, these programs are the ﬁrst to be attacked by people trying to\nbreak into a system. Unfortunately, the attackers are frequently successful.\nFor example, security has been breached on many UNIX systems because of the\nsetuid feature. We discuss security in Chapter 15.\n14.3.3\nAn Example: MULTICS\nIn the MULTICS system, the protection domains are organized hierarchically\ninto a ring structure. Each ring corresponds to a single domain (Figure 14.2).\nThe rings are numbered from 0 to 7. Let Di and Dj be any two domain rings.",
  "If j < i, then Di is a subset of Dj. That is, a process executing in domain Dj\nhas more privileges than does a process executing in domain Di. A process\nexecuting in domain D0 has the most privileges. If only two rings exist, this\nscheme is equivalent to the monitor–user mode of execution, where monitor\nmode corresponds to D0 and user mode corresponds to D1.\nMULTICS has a segmented address space; each segment is a ﬁle, and each\nsegment is associated with one of the rings. A segment description includes an\nentry that identiﬁes the ring number. In addition, it includes three access bits 14.3\nDomain of Protection\n631\nring 0\nring 1\nring N – 1\n• • •\nFigure 14.2\nMULTICS ring structure.\nto control reading, writing, and execution. The association between segments",
  "and rings is a policy decision with which we are not concerned here.\nA current-ring-number counter is associated with each process, iden-\ntifying the ring in which the process is executing currently. When a process is\nexecuting in ring i, it cannot access a segment associated with ring j (j < i). It\ncan access a segment associated with ring k (k ≥i). The type of access, however,\nis restricted according to the access bits associated with that segment.\nDomain switching in MULTICS occurs when a process crosses from one ring\nto another by calling a procedure in a different ring. Obviously, this switch must\nbe done in a controlled manner; otherwise, a process could start executing in\nring 0, and no protection would be provided. To allow controlled domain",
  "switching, we modify the ring ﬁeld of the segment descriptor to include the\nfollowing:\n• Access bracket. A pair of integers, b1 and b2, such that b1 ≤b2.\n• Limit. An integer b3 such that b3 > b2.\n• List of gates. Identiﬁes the entry points (or gates) at which the segments\nmay be called.\nIf a process executing in ring i calls a procedure (or segment) with access bracket\n(b1,b2), then the call is allowed if b1 ≤i ≤b2, and the current ring number of\nthe process remains i. Otherwise, a trap to the operating system occurs, and\nthe situation is handled as follows:\n• If i < b1, then the call is allowed to occur, because we have a transfer to a\nring (or domain) with fewer privileges. However, if parameters are passed\nthat refer to segments in a lower ring (that is, segments not accessible to",
  "the called procedure), then these segments must be copied into an area\nthat can be accessed by the called procedure.\n• If i > b2, then the call is allowed to occur only if b3 is greater than or equal\nto i and the call has been directed to one of the designated entry points in 632\nChapter 14\nProtection\nthe list of gates. This scheme allows processes with limited access rights to\ncall procedures in lower rings that have more access rights, but only in a\ncarefully controlled manner.\nThe main disadvantage of the ring (or hierarchical) structure is that it does\nnot allow us to enforce the need-to-know principle. In particular, if an object\nmust be accessible in domain Dj but not accessible in domain Di, then we must\nhave j < i. But this requirement means that every segment accessible in Di is",
  "also accessible in Dj.\nThe MULTICS protection system is generally more complex and less efﬁcient\nthan are those used in current operating systems. If protection interferes with\nthe ease of use of the system or signiﬁcantly decreases system performance,\nthen its use must be weighed carefully against the purpose of the system. For\ninstance, we would want to have a complex protection system on a computer\nused by a university to process students’ grades and also used by students for\nclasswork. A similar protection system would not be suited to a computer being\nused for number crunching, in which performance is of utmost importance. We\nwould prefer to separate the mechanism from the protection policy, allowing\nthe same system to have complex or simple protection depending on the needs",
  "of its users. To separate mechanism from policy, we require a more general\nmodel of protection.\n14.4 Access Matrix\nOur general model of protection can be viewed abstractly as a matrix, called\nan access matrix. The rows of the access matrix represent domains, and the\ncolumns represent objects. Each entry in the matrix consists of a set of access\nrights. Because the column deﬁnes objects explicitly, we can omit the object\nname from the access right. The entry access(i,j) deﬁnes the set of operations\nthat a process executing in domain Di can invoke on object Oj.\nTo illustrate these concepts, we consider the access matrix shown in Figure\n14.3. There are four domains and four objects—three ﬁles (F1, F2, F3) and one\nlaser printer. A process executing in domain D1 can read ﬁles F1 and F3. A",
  "process executing in domain D4 has the same privileges as one executing in\nobject\nprinter\nread\nread\nexecute\nread\nwrite\nread\nwrite\nread\nprint\nF1\nD1\nD2\nD3\nD4\nF2\nF3\ndomain\nFigure 14.3\nAccess matrix. 14.4\nAccess Matrix\n633\ndomain D1; but in addition, it can also write onto ﬁles F1 and F3. The laser\nprinter can be accessed only by a process executing in domain D2.\nThe access-matrix scheme provides us with the mechanism for specifying\na variety of policies. The mechanism consists of implementing the access\nmatrix and ensuring that the semantic properties we have outlined hold.\nMore speciﬁcally, we must ensure that a process executing in domain Di can\naccess only those objects speciﬁed in row i, and then only as allowed by the\naccess-matrix entries.",
  "access-matrix entries.\nThe access matrix can implement policy decisions concerning protection.\nThe policy decisions involve which rights should be included in the (i, j)th\nentry. We must also decide the domain in which each process executes. This\nlast policy is usually decided by the operating system.\nThe users normally decide the contents of the access-matrix entries. When\na user creates a new object Oj, the column Oj is added to the access matrix\nwith the appropriate initialization entries, as dictated by the creator. The user\nmay decide to enter some rights in some entries in column j and other rights\nin other entries, as needed.\nThe access matrix provides an appropriate mechanism for deﬁning and\nimplementing strict control for both static and dynamic association between",
  "processes and domains. When we switch a process from one domain to another,\nwe are executing an operation (switch) on an object (the domain). We can\ncontrol domain switching by including domains among the objects of the\naccess matrix. Similarly, when we change the content of the access matrix,\nwe are performing an operation on an object: the access matrix. Again, we\ncan control these changes by including the access matrix itself as an object.\nActually, since each entry in the access matrix can be modiﬁed individually,\nwe must consider each entry in the access matrix as an object to be protected.\nNow, we need to consider only the operations possible on these new objects\n(domains and the access matrix) and decide how we want processes to be able\nto execute these operations.",
  "to execute these operations.\nProcesses should be able to switch from one domain to another. Switching\nfrom domain Di to domain Dj is allowed if and only if the access right switch\n∈access(i, j). Thus, in Figure 14.4, a process executing in domain D2 can switch\nlaser\nprinter\nread\nread\nexecute\nread\nwrite\nread\nwrite\nread\nprint\nswitch\nswitch\nswitch switch\nF1\nD1\nD1\nD2\nD2\nD3\nD3\nD4\nD4\nF2\nF3\nobject\ndomain\nFigure 14.4\nAccess matrix of Figure 14.3 with domains as objects. 634\nChapter 14\nProtection\nto domain D3 or to domain D4. A process in domain D4 can switch to D1, and\none in domain D1 can switch to D2.\nAllowing controlled change in the contents of the access-matrix entries\nrequires three additional operations: copy, owner, and control. We examine\nthese operations next.",
  "these operations next.\nThe ability to copy an access right from one domain (or row) of the access\nmatrix to another is denoted by an asterisk (*) appended to the access right.\nThe copy right allows the access right to be copied only within the column\n(that is, for the object) for which the right is deﬁned. For example, in Figure\n14.5(a), a process executing in domain D2 can copy the read operation into any\nentry associated with ﬁle F2. Hence, the access matrix of Figure 14.5(a) can be\nmodiﬁed to the access matrix shown in Figure 14.5(b).\nThis scheme has two additional variants:\n1. A right is copied from access(i, j) to access(k, j); it is then removed from\naccess(i, j). This action is a of a right, rather than a copy.",
  "2. Propagation of the copy right may be limited. That is, when the right\nR∗is copied from access(i, j) to access(k, j), only the right R (not R∗)\nis created. A process executing in domain Dk cannot further copy the\nright R.\nA system may select only one of these three copy rights, or it may provide\nall three by identifying them as separate rights: copy, transfer, and limited\ncopy.\nWe also need a mechanism to allow addition of new rights and removal of\nsome rights. The owner right controls these operations. If access(i, j) includes\nthe owner right, then a process executing in domain Di can add and remove\nobject\nread*\nwrite*\nexecute\nexecute\nexecute\nexecute\nF1\nD1\nD2\nD3\nF2\nF3\ndomain\n(a)\nobject\nread*\nwrite*\nexecute\nexecute\nexecute\nexecute\nread\nF1\nD1\nD2\nD3\nF2\nF3\ndomain\n(b)\nFigure 14.5",
  "read\nF1\nD1\nD2\nD3\nF2\nF3\ndomain\n(b)\nFigure 14.5\nAccess matrix with copy rights. 14.4\nAccess Matrix\n635\nobject\nread*\nowner\nwrite\nowner\nexecute\nread*\nowner\nwrite\nexecute\nF1\nD1\nD2\nD3\nF2\nF3\ndomain\n(a)\nobject\nowner\nread*\nwrite*\nwrite\nowner\nexecute\nread*\nowner\nwrite\nF1\nD1\nD2\nD3\nF2\nF3\ndomain\n(b)\nwrite\nwrite\nFigure 14.6\nAccess matrix with owner rights.\nany right in any entry in column j. For example, in Figure 14.6(a), domain D1\nis the owner of F1 and thus can add and delete any valid right in column F1.\nSimilarly, domain D2 is the owner of F2 and F3 and thus can add and remove\nany valid right within these two columns. Thus, the access matrix of Figure\n14.6(a) can be modiﬁed to the access matrix shown in Figure 14.6(b).\nThe copy and owner rights allow a process to change the entries in a",
  "column. A mechanism is also needed to change the entries in a row. The\ncontrol right is applicable only to domain objects. If access(i, j) includes the\ncontrol right, then a process executing in domain Di can remove any access\nright from row j. For example, suppose that, in Figure 14.4, we include the\ncontrol right in access(D2, D4). Then, a process executing in domain D2\ncould modify domain D4, as shown in Figure 14.7.\nThe copy and owner rights provide us with a mechanism to limit the\npropagation of access rights. However, they do not give us the appropriate tools\nfor preventing the propagation (or disclosure) of information. The problem of\nguaranteeing that no information initially held in an object can migrate outside",
  "of its execution environment is called the conﬁnement problem. This problem\nis in general unsolvable (see the bibliographical notes at the end of the chapter).\nThese operations on the domains and the access matrix are not in them-\nselves important, but they illustrate the ability of the access-matrix model to\nallow us to implement and control dynamic protection requirements. New\nobjects and new domains can be created dynamically and included in the 636\nChapter 14\nProtection\nlaser\nprinter\nread\nread\nexecute\nwrite\nwrite\nread\nprint\nswitch\nswitch\nswitch switch\ncontrol\nF1\nD1\nD1\nD2\nD2\nD3\nD3\nD4\nD4\nF2\nF3\nobject\ndomain\nFigure 14.7\nModiﬁed access matrix of Figure 14.4.\naccess-matrix model. However, we have shown only that the basic mechanism",
  "exists. System designers and users must make the policy decisions concerning\nwhich domains are to have access to which objects in which ways.\n14.5 Implementation of the Access Matrix\nHow can the access matrix be implemented effectively? In general, the matrix\nwill be sparse; that is, most of the entries will be empty. Although data-\nstructure techniques are available for representing sparse matrices, they are\nnot particularly useful for this application, because of the way in which\nthe protection facility is used. Here, we ﬁrst describe several methods of\nimplementing the access matrix and then compare the methods.\n14.5.1\nGlobal Table\nThe simplest implementation of the access matrix is a global table consisting\nof a set of ordered triples <domain, object, rights-set>. Whenever an",
  "operation M is executed on an object Oj within domain Di, the global table\nis searched for a triple <Di, Oj, Rk>, with M ∈Rk. If this triple is found, the\noperation is allowed to continue; otherwise, an exception (or error) condition\nis raised.\nThis implementation suffers from several drawbacks. The table is usually\nlarge and thus cannot be kept in main memory, so additional I/O is needed.\nVirtual memory techniques are often used for managing this table. In addition,\nit is difﬁcult to take advantage of special groupings of objects or domains.\nFor example, if everyone can read a particular object, this object must have a\nseparate entry in every domain.\n14.5.2\nAccess Lists for Objects\nEach column in the access matrix can be implemented as an access list for",
  "one object, as described in Section 11.6.2. Obviously, the empty entries can be\ndiscarded. The resulting list for each object consists of ordered pairs <domain,\nrights-set>, which deﬁne all domains with a nonempty set of access rights\nfor that object.\nThis approach can be extended easily to deﬁne a list plus a default set of\naccess rights. When an operation M on an object Oj is attempted in domain 14.5\nImplementation of the Access Matrix\n637\nDi, we search the access list for object Oj, looking for an entry <Di, Rk> with\nM ∈Rk. If the entry is found, we allow the operation; if it is not, we check the\ndefault set. If M is in the default set, we allow the access. Otherwise, access is\ndenied, and an exception condition occurs. For efﬁciency, we may check the",
  "default set ﬁrst and then search the access list.\n14.5.3\nCapability Lists for Domains\nRather than associating the columns of the access matrix with the objects as\naccess lists, we can associate each row with its domain. A capability list for\na domain is a list of objects together with the operations allowed on those\nobjects. An object is often represented by its physical name or address, called\na capability. To execute operation M on object Oj, the process executes the\noperation M, specifying the capability (or pointer) for object Oj as a parameter.\nSimple possession of the capability means that access is allowed.\nThe capability list is associated with a domain, but it is never directly\naccessible to a process executing in that domain. Rather, the capability list",
  "is itself a protected object, maintained by the operating system and accessed\nby the user only indirectly. Capability-based protection relies on the fact that\nthe capabilities are never allowed to migrate into any address space directly\naccessible by a user process (where they could be modiﬁed). If all capabilities\nare secure, the object they protect is also secure against unauthorized access.\nCapabilities were originally proposed as a kind of secure pointer, to\nmeet the need for resource protection that was foreseen as multiprogrammed\ncomputer systems came of age. The idea of an inherently protected pointer\nprovides a foundation for protection that can be extended up to the application\nlevel.\nTo provide inherent protection, we must distinguish capabilities from other",
  "kinds of objects, and they must be interpreted by an abstract machine on which\nhigher-level programs run. Capabilities are usually distinguished from other\ndata in one of two ways:\n• Each object has a tag to denote whether it is a capability or accessible\ndata. The tags themselves must not be directly accessible by an application\nprogram. Hardware or ﬁrmware support may be used to enforce this\nrestriction. Although only one bit is necessary to distinguish between\ncapabilities and other objects, more bits are often used. This extension\nallows all objects to be tagged with their types by the hardware. Thus,\nthe hardware can distinguish integers, ﬂoating-point numbers, pointers,\nBooleans, characters, instructions, capabilities, and uninitialized values by\ntheir tags.",
  "their tags.\n• Alternatively, the address space associated with a program can be split into\ntwo parts. One part is accessible to the program and contains the program’s\nnormal data and instructions. The other part, containing the capability list,\nis accessible only by the operating system. A segmented memory space\n(Section 8.4) is useful to support this approach.\nSeveral capability-based protection systems have been developed; we describe\nthem brieﬂy in Section 14.8. The Mach operating system also uses a version of\ncapability-based protection; it is described in Appendix B. 638\nChapter 14\nProtection\n14.5.4\nA Lock–Key Mechanism\nThe lock–key scheme is a compromise between access lists and capability\nlists. Each object has a list of unique bit patterns, called locks. Similarly, each",
  "domain has a list of unique bit patterns, called keys. A process executing in a\ndomain can access an object only if that domain has a key that matches one of\nthe locks of the object.\nAs with capability lists, the list of keys for a domain must be managed\nby the operating system on behalf of the domain. Users are not allowed to\nexamine or modify the list of keys (or locks) directly.\n14.5.5\nComparison\nAs you might expect, choosing a technique for implementing an access matrix\ninvolves various trade-offs. Using a global table is simple; however, the table\ncan be quite large and often cannot take advantage of special groupings of\nobjects or domains. Access lists correspond directly to the needs of users.\nWhen a user creates an object, he can specify which domains can access the",
  "object, as well as what operations are allowed. However, because access-right\ninformation for a particular domain is not localized, determining the set of\naccess rights for each domain is difﬁcult. In addition, every access to the object\nmust be checked, requiring a search of the access list. In a large system with\nlong access lists, this search can be time consuming.\nCapability lists do not correspond directly to the needs of users, but they are\nuseful for localizing information for a given process. The process attempting\naccess must present a capability for that access. Then, the protection system\nneeds only to verify that the capability is valid. Revocation of capabilities,\nhowever, may be inefﬁcient (Section 14.7).\nThe lock–key mechanism, as mentioned, is a compromise between access",
  "lists and capability lists. The mechanism can be both effective and ﬂexible,\ndepending on the length of the keys. The keys can be passed freely from\ndomain to domain. In addition, access privileges can be effectively revoked by\nthe simple technique of changing some of the locks associated with the object\n(Section 14.7).\nMost systems use a combination of access lists and capabilities. When a\nprocess ﬁrst tries to access an object, the access list is searched. If access is\ndenied, an exception condition occurs. Otherwise, a capability is created and\nattached to the process. Additional references use the capability to demonstrate\nswiftly that access is allowed. After the last access, the capability is destroyed.\nThis strategy is used in the MULTICS system and in the CAL system.",
  "As an example of how such a strategy works, consider a ﬁle system in\nwhich each ﬁle has an associated access list. When a process opens a ﬁle, the\ndirectory structure is searched to ﬁnd the ﬁle, access permission is checked, and\nbuffers are allocated. All this information is recorded in a new entry in a ﬁle\ntable associated with the process. The operation returns an index into this table\nfor the newly opened ﬁle. All operations on the ﬁle are made by speciﬁcation\nof the index into the ﬁle table. The entry in the ﬁle table then points to the ﬁle\nand its buffers. When the ﬁle is closed, the ﬁle-table entry is deleted. Since the\nﬁle table is maintained by the operating system, the user cannot accidentally\ncorrupt it. Thus, the user can access only those ﬁles that have been opened. 14.6",
  "Access Control\n639\nSince access is checked when the ﬁle is opened, protection is ensured. This\nstrategy is used in the UNIX system.\nThe right to access must still be checked on each access, and the ﬁle-table\nentry has a capability only for the allowed operations. If a ﬁle is opened for\nreading, then a capability for read access is placed in the ﬁle-table entry. If\nan attempt is made to write onto the ﬁle, the system identiﬁes this protection\nviolation by comparing the requested operation with the capability in the\nﬁle-table entry.\n14.6 Access Control\nIn Section 11.6.2, we described how access controls can be used on ﬁles within\na ﬁle system. Each ﬁle and directory is assigned an owner, a group, or possibly\na list of users, and for each of those entities, access-control information is",
  "assigned. A similar function can be added to other aspects of a computer\nsystem. A good example of this is found in Solaris 10.\nSolaris 10 advances the protection available in the operating system by\nexplicitly adding the principle of least privilege via role-based access control\n(RBAC). This facility revolves around privileges. A privilege is the right to\nexecute a system call or to use an option within that system call (such as opening\na ﬁle with write access). Privileges can be assigned to processes, limiting them\nto exactly the access they need to perform their work. Privileges and programs\ncan also be assigned to roles. Users are assigned roles or can take roles based\non passwords to the roles. In this way, a user can take a role that enables a",
  "privilege, allowing the user to run a program to accomplish a speciﬁc task,\nas depicted in Figure 14.8. This implementation of privileges decreases the\nsecurity risk associated with superusers and setuid programs.\nuser 1\nrole 1\nprivileges 1\nexecutes with role 1 privileges\nprivileges 2\nprocess\nFigure 14.8\nRole-based access control in Solaris 10. 640\nChapter 14\nProtection\nNotice that this facility is similar to the access matrix described in Section\n14.4. This relationship is further explored in the exercises at the end of the\nchapter.\n14.7 Revocation of Access Rights\nIn a dynamic protection system, we may sometimes need to revoke access\nrights to objects shared by different users. Various questions about revocation\nmay arise:",
  "may arise:\n• Immediate versus delayed. Does revocation occur immediately, or is it\ndelayed? If revocation is delayed, can we ﬁnd out when it will take place?\n• Selective versus general. When an access right to an object is revoked,\ndoes it affect all the users who have an access right to that object, or can\nwe specify a select group of users whose access rights should be revoked?\n• Partial versus total. Can a subset of the rights associated with an object be\nrevoked, or must we revoke all access rights for this object?\n• Temporary versus permanent. Can access be revoked permanently (that\nis, the revoked access right will never again be available), or can access be\nrevoked and later be obtained again?\nWith an access-list scheme, revocation is easy. The access list is searched for",
  "any access rights to be revoked, and they are deleted from the list. Revocation\nis immediate and can be general or selective, total or partial, and permanent\nor temporary.\nCapabilities, however, present a much more difﬁcult revocation problem,\nas mentioned earlier. Since the capabilities are distributed throughout the\nsystem, we must ﬁnd them before we can revoke them. Schemes that implement\nrevocation for capabilities include the following:\n• Reacquisition. Periodically, capabilities are deleted from each domain. If\na process wants to use a capability, it may ﬁnd that that capability has been\ndeleted. The process may then try to reacquire the capability. If access has\nbeen revoked, the process will not be able to reacquire the capability.",
  "• Back-pointers. A list of pointers is maintained with each object, pointing\nto all capabilities associated with that object. When revocation is required,\nwe can follow these pointers, changing the capabilities as necessary. This\nscheme was adopted in the MULTICS system. It is quite general, but its\nimplementation is costly.\n• Indirection. The capabilities point indirectly, not directly, to the objects.\nEach capability points to a unique entry in a global table, which in turn\npoints to the object. We implement revocation by searching the global table\nfor the desired entry and deleting it. Then, when an access is attempted,\nthe capability is found to point to an illegal table entry. Table entries can\nbe reused for other capabilities without difﬁculty, since both the capability",
  "and the table entry contain the unique name of the object. The object for a 14.8\nCapability-Based Systems\n641\ncapability and its table entry must match. This scheme was adopted in the\nCAL system. It does not allow selective revocation.\n• Keys. A key is a unique bit pattern that can be associated with a capability.\nThis key is deﬁned when the capability is created, and it can be neither\nmodiﬁed nor inspected by the process that owns the capability. A master\nkey is associated with each object; it can be deﬁned or replaced with\nthe set-key operation. When a capability is created, the current value\nof the master key is associated with the capability. When the capability\nis exercised, its key is compared with the master key. If the keys match,",
  "the operation is allowed to continue; otherwise, an exception condition\nis raised. Revocation replaces the master key with a new value via the\nset-key operation, invalidating all previous capabilities for this object.\nThis scheme does not allow selective revocation, since only one master\nkey is associated with each object. If we associate a list of keys with each\nobject, then selective revocation can be implemented. Finally, we can group\nall keys into one global table of keys. A capability is valid only if its\nkey matches some key in the global table. We implement revocation by\nremoving the matching key from the table. With this scheme, a key can be\nassociated with several objects, and several keys can be associated with\neach object, providing maximum ﬂexibility.",
  "each object, providing maximum ﬂexibility.\nIn key-based schemes, the operations of deﬁning keys, inserting them\ninto lists, and deleting them from lists should not be available to all users.\nIn particular, it would be reasonable to allow only the owner of an object\nto set the keys for that object. This choice, however, is a policy decision\nthat the protection system can implement but should not deﬁne.\n14.8 Capability-Based Systems\nIn this section, we survey two capability-based protection systems. These\nsystems differ in their complexity and in the types of policies that can be\nimplemented on them. Neither system is widely used, but both provide\ninteresting proving grounds for protection theories.\n14.8.1\nAn Example: Hydra",
  "14.8.1\nAn Example: Hydra\nHydra is a capability-based protection system that provides considerable\nﬂexibility. The system implements a ﬁxed set of possible access rights, including\nsuch basic forms of access as the right to read, write, or execute a memory\nsegment. In addition, a user (of the protection system) can declare other rights.\nThe interpretation of user-deﬁned rights is performed solely by the user’s\nprogram, but the system provides access protection for the use of these rights,\nas well as for the use of system-deﬁned rights. These facilities constitute a\nsigniﬁcant development in protection technology.\nOperations on objects are deﬁned procedurally. The procedures that\nimplement such operations are themselves a form of object, and they are",
  "accessed indirectly by capabilities. The names of user-deﬁned procedures must\nbe identiﬁed to the protection system if it is to deal with objects of the user-\ndeﬁned type. When the deﬁnition of an object is made known to Hydra, the\nnames of operations on the type become auxiliary rights. Auxiliary rights 642\nChapter 14\nProtection\ncan be described in a capability for an instance of the type. For a process to\nperform an operation on a typed object, the capability it holds for that object\nmust contain the name of the operation being invoked among its auxiliary\nrights. This restriction enables discrimination of access rights to be made on an\ninstance-by-instance and process-by-process basis.\nHydra also provides rights ampliﬁcation. This scheme allows a procedure",
  "to be certiﬁed as trustworthy to act on a formal parameter of a speciﬁed type\non behalf of any process that holds a right to execute the procedure. The rights\nheld by a trustworthy procedure are independent of, and may exceed, the\nrights held by the calling process. However, such a procedure must not be\nregarded as universally trustworthy (the procedure is not allowed to act on\nother types, for instance), and the trustworthiness must not be extended to any\nother procedures or program segments that might be executed by a process.\nAmpliﬁcation allows implementation procedures access to the representa-\ntion variables of an abstract data type. If a process holds a capability to a typed\nobject A, for instance, this capability may include an auxiliary right to invoke",
  "some operation P but does not include any of the so-called kernel rights, such\nas read, write, or execute, on the segment that represents A. Such a capability\ngives a process a means of indirect access (through the operation P) to the\nrepresentation of A, but only for speciﬁc purposes.\nWhen a process invokes the operation P on an object A, however, the\ncapability for access to A may be ampliﬁed as control passes to the code body\nof P. This ampliﬁcation may be necessary to allow P the right to access the\nstorage segment representing A so as to implement the operation that P deﬁnes\non the abstract data type. The code body of P may be allowed to read or to\nwrite to the segment of A directly, even though the calling process cannot.",
  "On return from P, the capability for A is restored to its original, unampliﬁed\nstate. This case is a typical one in which the rights held by a process for access\nto a protected segment must change dynamically, depending on the task to\nbe performed. The dynamic adjustment of rights is performed to guarantee\nconsistency of a programmer-deﬁned abstraction. Ampliﬁcation of rights can\nbe stated explicitly in the declaration of an abstract type to the Hydra operating\nsystem.\nWhen a user passes an object as an argument to a procedure, we may need\nto ensure that the procedure cannot modify the object. We can implement this\nrestriction readily by passing an access right that does not have the modiﬁcation\n(write) right. However, if ampliﬁcation may occur, the right to modify may",
  "be reinstated. Thus, the user-protection requirement can be circumvented.\nIn general, of course, a user may trust that a procedure performs its task\ncorrectly. This assumption is not always correct, however, because of hardware\nor software errors. Hydra solves this problem by restricting ampliﬁcations.\nThe procedure-call mechanism of Hydra was designed as a direct solution\nto the problem of mutually suspicious subsystems. This problem is deﬁned\nas follows. Suppose that a program can be invoked as a service by a number\nof different users (for example, a sort routine, a compiler, a game). When\nusers invoke this service program, they take the risk that the program will\nmalfunction and will either damage the given data or retain some access right to",
  "the data to be used (without authority) later. Similarly, the service program may\nhave some private ﬁles (for accounting purposes, for example) that should not 14.8\nCapability-Based Systems\n643\nbe accessed directly by the calling user program. Hydra provides mechanisms\nfor directly dealing with this problem.\nA Hydra subsystem is built on top of its protection kernel and may require\nprotection of its own components. A subsystem interacts with the kernel\nthrough calls on a set of kernel-deﬁned primitives that deﬁne access rights to\nresources deﬁned by the subsystem. The subsystem designer can deﬁne policies\nfor use of these resources by user processes, but the policies are enforced by\nuse of the standard access protection provided by the capability system.",
  "Programmers can make direct use of the protection system after acquaint-\ning themselves with its features in the appropriate reference manual. Hydra\nprovides a large library of system-deﬁned procedures that can be called by\nuser programs. Programmers can explicitly incorporate calls on these system\nprocedures into their program code or can use a program translator that has\nbeen interfaced to Hydra.\n14.8.2\nAn Example: Cambridge CAP System\nA different approach to capability-based protection has been taken in the\ndesign of the Cambridge CAP system. CAP’s capability system is simpler and\nsuperﬁcially less powerful than that of Hydra. However, closer examination\nshows that it, too, can be used to provide secure protection of user-deﬁned",
  "objects. CAP has two kinds of capabilities. The ordinary kind is called a\ndata capability. It can be used to provide access to objects, but the only\nrights provided are the standard read, write, and execute of the individual\nstorage segments associated with the object. Data capabilities are interpreted\nby microcode in the CAP machine.\nThe second kind of capability is the so-called software capability, which\nis protected, but not interpreted, by the CAP microcode. It is interpreted\nby a protected (that is, privileged) procedure, which may be written by an\napplication programmer as part of a subsystem. A particular kind of rights\nampliﬁcation is associated with a protected procedure. When executing the\ncode body of such a procedure, a process temporarily acquires the right to",
  "read or write the contents of a software capability itself. This speciﬁc kind\nof rights ampliﬁcation corresponds to an implementation of the seal and\nunseal primitives on capabilities. Of course, this privilege is still subject to\ntype veriﬁcation to ensure that only software capabilities for a speciﬁed abstract\ntype are passed to any such procedure. Universal trust is not placed in any code\nother than the CAP machine’s microcode. (See the bibliographical notes at the\nend of the chapter for references.)\nThe interpretation of a software capability is left completely to the sub-\nsystem, through the protected procedures it contains. This scheme allows a\nvariety of protection policies to be implemented. Although programmers can",
  "deﬁne their own protected procedures (any of which might be incorrect), the\nsecurity of the overall system cannot be compromised. The basic protection\nsystem will not allow an unveriﬁed, user-deﬁned, protected procedure access\nto any storage segments (or capabilities) that do not belong to the protection\nenvironment in which it resides. The most serious consequence of an insecure\nprotected procedure is a protection breakdown of the subsystem for which that\nprocedure has responsibility. 644\nChapter 14\nProtection\nThe designers of the CAP system have noted that the use of software\ncapabilities allowed them to realize considerable economies in formulating\nand implementing protection policies commensurate with the requirements of",
  "abstract resources. However, subsystem designers who want to make use of\nthis facility cannot simply study a reference manual, as is the case with Hydra.\nInstead, they must learn the principles and techniques of protection, since the\nsystem provides them with no library of procedures.\n14.9 Language-Based Protection\nTo the degree that protection is provided in existing computer systems, it is\nusually achieved through an operating-system kernel, which acts as a security\nagent to inspect and validate each attempt to access a protected resource. Since\ncomprehensive access validation may be a source of considerable overhead,\neither we must give it hardware support to reduce the cost of each validation,\nor we must allow the system designer to compromise the goals of protection.",
  "Satisfying all these goals is difﬁcult if the ﬂexibility to implement protection\npolicies is restricted by the support mechanisms provided or if protection\nenvironments are made larger than necessary to secure greater operational\nefﬁciency.\nAs operating systems have become more complex, and particularly as they\nhave attempted to provide higher-level user interfaces, the goals of protection\nhave become much more reﬁned. The designers of protection systems have\ndrawn heavily on ideas that originated in programming languages and\nespecially on the concepts of abstract data types and objects. Protection systems\nare now concerned not only with the identity of a resource to which access is\nattempted but also with the functional nature of that access. In the newest",
  "protection systems, concern for the function to be invoked extends beyond\na set of system-deﬁned functions, such as standard ﬁle-access methods, to\ninclude functions that may be user-deﬁned as well.\nPolicies for resource use may also vary, depending on the application, and\nthey may be subject to change over time. For these reasons, protection can no\nlonger be considered a matter of concern only to the designer of an operating\nsystem. It should also be available as a tool for use by the application designer,\nso that resources of an application subsystem can be guarded against tampering\nor the inﬂuence of an error.\n14.9.1\nCompiler-Based Enforcement\nAt this point, programming languages enter the picture. Specifying the desired",
  "control of access to a shared resource in a system is making a declarative\nstatement about the resource. This kind of statement can be integrated into a\nlanguage by an extension of its typing facility. When protection is declared\nalong with data typing, the designer of each subsystem can specify its\nrequirements for protection, as well as its need for use of other resources in a\nsystem. Such a speciﬁcation should be given directly as a program is composed,\nand in the language in which the program itself is stated. This approach has\nseveral signiﬁcant advantages: 14.9\nLanguage-Based Protection\n645\n1. Protection needs are simply declared, rather than programmed as a\nsequence of calls on procedures of an operating system.",
  "2. Protection requirements can be stated independently of the facilities\nprovided by a particular operating system.\n3. The means for enforcement need not be provided by the designer of a\nsubsystem.\n4. A declarative notation is natural because access privileges are closely\nrelated to the linguistic concept of data type.\nA variety of techniques can be provided by a programming-language\nimplementation to enforce protection, but any of these must depend on some\ndegree of support from an underlying machine and its operating system. For\nexample, suppose a language is used to generate code to run on the Cambridge\nCAP system. On this system, every storage reference made on the underlying\nhardware occurs indirectly through a capability. This restriction prevents any",
  "process from accessing a resource outside of its protection environment at\nany time. However, a program may impose arbitrary restrictions on how\na resource can be used during execution of a particular code segment.\nWe can implement such restrictions most readily by using the software\ncapabilities provided by CAP. A language implementation might provide\nstandard protected procedures to interpret software capabilities that would\nrealize the protection policies that could be speciﬁed in the language. This\nscheme puts policy speciﬁcation at the disposal of the programmers, while\nfreeing them from implementing its enforcement.\nEven if a system does not provide a protection kernel as powerful as those\nof Hydra or CAP, mechanisms are still available for implementing protection",
  "speciﬁcations given in a programming language. The principal distinction is\nthat the security of this protection will not be as great as that supported by\na protection kernel, because the mechanism must rely on more assumptions\nabout the operational state of the system. A compiler can separate references\nfor which it can certify that no protection violation could occur from those\nfor which a violation might be possible, and it can treat them differently. The\nsecurity provided by this form of protection rests on the assumption that the\ncode generated by the compiler will not be modiﬁed prior to or during its\nexecution.\nWhat, then, are the relative merits of enforcement based solely on a kernel,\nas opposed to enforcement provided largely by a compiler?",
  "• Security. Enforcement by a kernel provides a greater degree of security\nof the protection system itself than does the generation of protection-\nchecking code by a compiler. In a compiler-supported scheme, security\nrests on correctness of the translator, on some underlying mechanism of\nstorage management that protects the segments from which compiled\ncode is executed, and, ultimately, on the security of ﬁles from which a\nprogram is loaded. Some of these considerations also apply to a software-\nsupported protection kernel, but to a lesser degree, since the kernel may\nreside in ﬁxed physical storage segments and may be loaded only from\na designated ﬁle. With a tagged-capability system, in which all address 646\nChapter 14\nProtection",
  "Chapter 14\nProtection\ncomputation is performed either by hardware or by a ﬁxed microprogram,\neven greater security is possible. Hardware-supported protection is also\nrelatively immune to protection violations that might occur as a result of\neither hardware or system software malfunction.\n• Flexibility. There are limits to the ﬂexibility of a protection kernel in\nimplementing a user-deﬁned policy, although it may supply adequate\nfacilities for the system to provide enforcement of its own policies.\nWith a programming language, protection policy can be declared and\nenforcement provided as needed by an implementation. If a language\ndoes not provide sufﬁcient ﬂexibility, it can be extended or replaced\nwith less disturbance than would be caused by the modiﬁcation of an\noperating-system kernel.",
  "operating-system kernel.\n• Efﬁciency. The greatest efﬁciency is obtained when enforcement of protec-\ntion is supported directly by hardware (or microcode). Insofar as software\nsupport is required, language-based enforcement has the advantage that\nstatic access enforcement can be veriﬁed off-line at compile time. Also,\nsince an intelligent compiler can tailor the enforcement mechanism to\nmeet the speciﬁed need, the ﬁxed overhead of kernel calls can often be\navoided.\nIn summary, the speciﬁcation of protection in a programming language\nallows the high-level description of policies for the allocation and use of\nresources. A language implementation can provide software for protection\nenforcement when automatic hardware-supported checking is unavailable. In",
  "addition, it can interpret protection speciﬁcations to generate calls on whatever\nprotection system is provided by the hardware and the operating system.\nOne way of making protection available to the application program is\nthrough the use of a software capability that could be used as an object\nof computation. Inherent in this concept is the idea that certain program\ncomponents might have the privilege of creating or examining these software\ncapabilities. A capability-creating program would be able to execute a primitive\noperation that would seal a data structure, rendering the latter’s contents\ninaccessible to any program components that did not hold either the seal or\nthe unseal privilege. Such components might copy the data structure or pass",
  "its address to other program components, but they could not gain access to\nits contents. The reason for introducing such software capabilities is to bring a\nprotection mechanism into the programming language. The only problem with\nthe concept as proposed is that the use of the seal and unseal operations takes\na procedural approach to specifying protection. A nonprocedural or declarative\nnotation seems a preferable way to make protection available to the application\nprogrammer.\nWhat is needed is a safe, dynamic access-control mechanism for distribut-\ning capabilities to system resources among user processes. To contribute to the\noverall reliability of a system, the access-control mechanism should be safe\nto use. To be useful in practice, it should also be reasonably efﬁcient. This",
  "requirement has led to the development of a number of language constructs\nthat allow the programmer to declare various restrictions on the use of a speciﬁc\nmanaged resource. (See the bibliographical notes for appropriate references.)\nThese constructs provide mechanisms for three functions: 14.9\nLanguage-Based Protection\n647\n1. Distributing capabilities safely and efﬁciently among customer processes.\nIn particular, mechanisms ensure that a user process will use the managed\nresource only if it was granted a capability to that resource.\n2. Specifying the type of operations that a particular process may invoke on\nan allocated resource (for example, a reader of a ﬁle should be allowed\nonly to read the ﬁle, whereas a writer should be able both to read and",
  "to write). It should not be necessary to grant the same set of rights to\nevery user process, and it should be impossible for a process to enlarge\nits set of access rights, except with the authorization of the access-control\nmechanism.\n3. Specifying the order in which a particular process may invoke the various\noperations of a resource (for example, a ﬁle must be opened before it can\nbe read). It should be possible to give two processes different restrictions\non the order in which they can invoke the operations of the allocated\nresource.\nThe incorporation of protection concepts into programming languages, as\na practical tool for system design, is in its infancy. Protection will likely become\na matter of greater concern to the designers of new systems with distributed",
  "architectures and increasingly stringent requirements on data security. Then\nthe importance of suitable language notations in which to express protection\nrequirements will be recognized more widely.\n14.9.2\nProtection in Java\nBecause Java was designed to run in a distributed environment, the Java\nvirtual machine—or JVM—has many built-in protection mechanisms. Java\nprograms are composed of classes, each of which is a collection of data ﬁelds\nand functions (called methods) that operate on those ﬁelds. The JVM loads a\nclass in response to a request to create instances (or objects) of that class. One of\nthe most novel and useful features of Java is its support for dynamically loading\nuntrusted classes over a network and for executing mutually distrusting classes\nwithin the same JVM.",
  "within the same JVM.\nBecause of these capabilities, protection is a paramount concern. Classes\nrunning in the same JVM may be from different sources and may not be equally\ntrusted. As a result, enforcing protection at the granularity of the JVM process\nis insufﬁcient. Intuitively, whether a request to open a ﬁle should be allowed\nwill generally depend on which class has requested the open. The operating\nsystem lacks this knowledge.\nThus, such protection decisions are handled within the JVM. When the\nJVM loads a class, it assigns the class to a protection domain that gives\nthe permissions of that class. The protection domain to which the class is\nassigned depends on the URL from which the class was loaded and any digital",
  "signatures on the class ﬁle. (Digital signatures are covered in Section 15.4.1.3.)\nA conﬁgurable policy ﬁle determines the permissions granted to the domain\n(and its classes). For example, classes loaded from a trusted server might be\nplaced in a protection domain that allows them to access ﬁles in the user’s\nhome directory, whereas classes loaded from an untrusted server might have\nno ﬁle access permissions at all. 648\nChapter 14\nProtection\nIt can be complicated for the JVMto determine what class is responsible for a\nrequest to access a protected resource. Accesses are often performed indirectly,\nthrough system libraries or other classes. For example, consider a class that\nis not allowed to open network connections. It could call a system library to",
  "request the load of the contents of a URL. The JVM must decide whether or not\nto open a network connection for this request. But which class should be used\nto determine if the connection should be allowed, the application or the system\nlibrary?\nThe philosophy adopted in Java is to require the library class to explicitly\npermit a network connection. More generally, in order to access a protected\nresource, some method in the calling sequence that resulted in the request must\nexplicitly assert the privilege to access the resource. By doing so, this method\ntakes responsibility for the request. Presumably, it will also perform whatever\nchecks are necessary to ensure the safety of the request. Of course, not every\nmethod is allowed to assert a privilege; a method can assert a privilege only if",
  "its class is in a protection domain that is itself allowed to exercise the privilege.\nThis implementation approach is called stack inspection. Every thread\nin the JVM has an associated stack of its ongoing method invocations. When\na caller may not be trusted, a method executes an access request within a\ndoPrivileged block to perform the access to a protected resource directly or\nindirectly. doPrivileged() is a static method in the AccessController class\nthat is passed a class with a run() method to invoke. When the doPrivileged\nblock is entered, the stack frame for this method is annotated to indicate this\nfact. Then, the contents of the block are executed. When an access to a protected\nresource is subsequently requested, either by this method or a method it",
  "calls, a call to checkPermissions() is used to invoke stack inspection to\ndetermine if the request should be allowed. The inspection examines stack\nframes on the calling thread’s stack, starting from the most recently added\nframe and working toward the oldest. If a stack frame is ﬁrst found that has the\ndoPrivileged() annotation, then checkPermissions() returns immediately\nand silently, allowing the access. If a stack frame is ﬁrst found for which\naccess is disallowed based on the protection domain of the method’s class,\nthen checkPermissions() throws an AccessControlException. If the stack\ninspection exhausts the stack without ﬁnding either type of frame, then\nwhether access is allowed depends on the implementation (for example, some",
  "implementations of the JVM may allow access, while other implementations\nmay not).\nStack inspection is illustrated in Figure 14.9. Here, the gui() method of\na class in the untrusted applet protection domain performs two operations,\nﬁrst a get() and then an open(). The former is an invocation of the\nget() method of a class in the URL loader protection domain, which is\npermitted to open() sessions to sites in the lucent.com domain, in particular\na proxy server proxy.lucent.com for retrieving URLs. For this reason, the\nuntrusted applet’s get() invocation will succeed: the checkPermissions()\ncall in the networking library encounters the stack frame of the get()\nmethod, which performed its open() in a doPrivileged block. However,",
  "the untrusted applet’s open() invocation will result in an exception, because\nthe checkPermissions() call ﬁnds no doPrivileged annotation before\nencountering the stack frame of the gui() method. 14.10\nSummary\n649\nuntrusted\napplet\nprotection\ndomain:\nsocket\npermission:\nclass:\nnone\ngui:\n …\n \nget(url);\n \nopen(addr);\n …\nnetworking\nany\nopen(Addr a):\n …\n \ncheckPermission\n     (a, connect);\n \nconnect (a);\n …\nget(URL u):\n …\n \ndoPrivileged {\n \n    open(‘proxy.lucent.com:80’);\n    }\n    (request u from proxy)\n …\n*.lucent.com:80, connect\nURL loader\nFigure 14.9\nStack inspection.\nOf course, for stack inspection to work, a program must be unable to\nmodify the annotations on its own stack frame or to otherwise manipulate\nstack inspection. This is one of the most important differences between Java",
  "and many other languages (including C++). A Java program cannot directly\naccess memory; it can manipulate only an object for which it has a reference.\nReferences cannot be forged, and manipulations are made only through well-\ndeﬁned interfaces. Compliance is enforced through a sophisticated collection of\nload-time and run-time checks. As a result, an object cannot manipulate its run-\ntime stack, because it cannot get a reference to the stack or other components\nof the protection system.\nMore generally, Java’s load-time and run-time checks enforce type safety of\nJava classes. Type safety ensures that classes cannot treat integers as pointers,\nwrite past the end of an array, or otherwise access memory in arbitrary ways.",
  "Rather, a program can access an object only via the methods deﬁned on that\nobject by its class. This is the foundation of Java protection, since it enables a\nclass to effectively encapsulate and protect its data and methods from other\nclasses loaded in the same JVM. For example, a variable can be deﬁned as\nprivate so that only the class that contains it can access it or protected so\nthat it can be accessed only by the class that contains it, subclasses of that class,\nor classes in the same package. Type safety ensures that these restrictions can\nbe enforced.\n14.10 Summary\nComputer systems contain many objects, and they need to be protected from\nmisuse. Objects may be hardware (such as memory, CPU time, and I/O devices)",
  "or software (such as ﬁles, programs, and semaphores). An access right is\npermission to perform an operation on an object. A domain is a set of access\nrights. Processes execute in domains and may use any of the access rights in\nthe domain to access and manipulate objects. During its lifetime, a process may\nbe either bound to a protection domain or allowed to switch from one domain\nto another. 650\nChapter 14\nProtection\nThe access matrix is a general model of protection that provides a\nmechanism for protection without imposing a particular protection policy on\nthe system or its users. The separation of policy and mechanism is an important\ndesign property.\nThe access matrix is sparse. It is normally implemented either as access lists",
  "associated with each object or as capability lists associated with each domain.\nWe can include dynamic protection in the access-matrix model by considering\ndomains and the access matrix itself as objects. Revocation of access rights in a\ndynamic protection model is typically easier to implement with an access-list\nscheme than with a capability list.\nReal systems are much more limited than the general model and tend to\nprovide protection only for ﬁles. UNIX is representative, providing read, write,\nand execution protection separately for the owner, group, and general public\nfor each ﬁle. MULTICS uses a ring structure in addition to ﬁle access. Hydra, the\nCambridge CAPsystem, and Mach are capability systems that extend protection",
  "to user-deﬁned software objects. Solaris 10 implements the principle of least\nprivilege via role-based access control, a form of the access matrix.\nLanguage-based protection provides ﬁner-grained arbitration of requests\nand privileges than the operating system is able to provide. For example, a\nsingle Java JVM can run several threads, each in a different protection class. It\nenforces the resource requests through sophisticated stack inspection and via\nthe type safety of the language.\nPractice Exercises\n14.1\nWhat are the main differences between capability lists and access lists?\n14.2\nA Burroughs B7000/B6000 MCP ﬁle can be tagged as sensitive data.\nWhen such a ﬁle is deleted, its storage area is overwritten by some\nrandom bits. For what purpose would such a scheme be useful?\n14.3",
  "14.3\nIn a ring-protection system, level 0 has the greatest access to objects,\nand level n (where n > 0) has fewer access rights. The access rights of\na program at a particular level in the ring structure are considered a\nset of capabilities. What is the relationship between the capabilities of\na domain at level j and a domain at level i to an object (for j > i)?\n14.4\nThe RC 4000 system, among others, has deﬁned a tree of processes\n(called a process tree) such that all the descendants of a process can\nbe given resources (objects) and access rights by their ancestors only.\nThus, a descendant can never have the ability to do anything that its\nancestors cannot do. The root of the tree is the operating system, which\nhas the ability to do anything. Assume that the set of access rights is",
  "represented by an access matrix, A. A(x,y) deﬁnes the access rights of\nprocess x to object y. If x is a descendant of z, what is the relationship\nbetween A(x,y) and A(z,y) for an arbitrary object y?\n14.5\nWhat protection problems may arise if a shared stack is used for\nparameter passing? Exercises\n651\n14.6\nConsider a computing environment where a unique number is associ-\nated with each process and each object in the system. Suppose that we\nallow a process with number n to access an object with number m only\nif n > m. What type of protection structure do we have?\n14.7\nConsider a computing environment where a process is given the\nprivilege of accessing an object only n times. Suggest a scheme for\nimplementing this policy.\n14.8",
  "implementing this policy.\n14.8\nIf all the access rights to an object are deleted, the object can no longer\nbe accessed. At this point, the object should also be deleted, and the\nspace it occupies should be returned to the system. Suggest an efﬁcient\nimplementation of this scheme.\n14.9\nWhy is it difﬁcult to protect a system in which users are allowed to do\ntheir own I/O?\n14.10\nCapability lists are usually kept within the address space of the user.\nHow does the system ensure that the user cannot modify the contents\nof the list?\nExercises\n14.11\nConsider the ring-protection scheme in MULTICS. If we were to imple-\nment the system calls of a typical operating system and store them in a\nsegment associated with ring 0, what should be the values stored in the",
  "ring ﬁeld of the segment descriptor? What happens during a system\ncall when a process executing in a higher-numbered ring invokes a\nprocedure in ring 0?\n14.12\nThe access-control matrix can be used to determine whether a process\ncan switch from, say, domain A to domain B and enjoy the access\nprivileges of domain B. Is this approach equivalent to including the\naccess privileges of domain B in those of domain A?\n14.13\nConsider a computer system in which computer games can be played\nby students only between 10 P.M. and 6 A.M., by faculty members\nbetween 5 P.M. and 8 A.M., and by the computer center staff at all\ntimes. Suggest a scheme for implementing this policy efﬁciently.\n14.14\nWhat hardware features does a computer system need for efﬁcient",
  "capability manipulation? Can these features be used for memory\nprotection?\n14.15\nDiscuss the strengths and weaknesses of implementing an access matrix\nusing access lists that are associated with objects.\n14.16\nDiscuss the strengths and weaknesses of implementing an access matrix\nusing capabilities that are associated with domains.\n14.17\nExplain why a capability-based system such as Hydra provides greater\nﬂexibility than the ring-protection scheme in enforcing protection\npolicies. 652\nChapter 14\nProtection\n14.18\nDiscuss the need for rights ampliﬁcation in Hydra. How does this\npractice compare with the cross-ring calls in a ring-protection scheme?\n14.19\nWhat is the need-to-know principle? Why is it important for a protec-\ntion system to adhere to this principle?\n14.20",
  "tion system to adhere to this principle?\n14.20\nDiscuss which of the following systems allow module designers to\nenforce the need-to-know principle.\na.\nThe MULTICS ring-protection scheme\nb.\nHydra’s capabilities\nc.\nJVM’s stack-inspection scheme\n14.21\nDescribe how the Java protection model would be compromised if a\nJava program were allowed to directly alter the annotations of its stack\nframe.\n14.22\nHow are the access-matrix facility and the role-based access-control\nfacility similar? How do they differ?\n14.23\nHow does the principle of least privilege aid in the creation of protection\nsystems?\n14.24\nHow can systems that implement the principle of least privilege still\nhave protection failures that lead to security violations?\nBibliographical Notes",
  "Bibliographical Notes\nThe access-matrix model of protection between domains and objects was\ndeveloped by [Lampson (1969)] and [Lampson (1971)]. [Popek (1974)] and\n[Saltzer and Schroeder (1975)] provided excellent surveys on the subject\nof protection. [Harrison et al. (1976)] used a formal version of the access-\nmatrix model to enable them to prove properties of a protection system\nmathematically.\nThe concept of a capability evolved from Iliffe’s and Jodeit’s codewords,\nwhich were implemented in the Rice University computer ([Iliffe and Jodeit\n(1962)]). The term capability was introduced by [Dennis and Horn (1966)].\nThe Hydra system was described by [Wulf et al. (1981)]. The CAP system\nwas described by [Needham and Walker (1977)]. [Organick (1972)] discussed",
  "the MULTICS ring-protection system.\nRevocation was discussed by [Redell and Fabry (1974)], [Cohen and\nJefferson (1975)], and [Ekanadham and Bernstein (1979)]. The principle of\nseparation of policy and mechanism was advocated by the designer of\nHydra ([Levin et al. (1975)]). The conﬁnement problem was ﬁrst discussed\nby [Lampson (1973)] and was further examined by [Lipner (1975)].\nThe use of higher-level languages for specifying access control was sug-\ngested ﬁrst by [Morris (1973)], who proposed the use of the seal and\nunseal operations discussed in Section 14.9. [Kieburtz and Silberschatz (1978)],\n[Kieburtz and Silberschatz (1983)], and [McGraw and Andrews (1979)] pro-\nposed various language constructs for dealing with general dynamic-resource-",
  "management schemes. [Jones and Liskov (1978)]considered howastaticaccess- Bibliography\n653\ncontrol scheme can be incorporated in a programming language that supports\nabstract data types. The use of minimal operating-system support to enforce\nprotection was advocated by the Exokernel Project ([Ganger et al. (2002)],\n[Kaashoek et al. (1997)]). Extensibility of system code through language-based\nprotection mechanisms was discussed in [Bershad et al. (1995)]. Other tech-\nniques for enforcing protection include sandboxing ([Goldberg et al. (1996)])\nand software fault isolation ([Wahbe et al. (1993)]). The issues of lowering the\noverhead associated with protection costs and enabling user-level access to\nnetworking devices were discussed in [McCanne and Jacobson (1993)] and\n[Basu et al. (1995)].",
  "[Basu et al. (1995)].\nMore detailed analyses of stack inspection, including comparisons with\nother approaches to Java security, can be found in [Wallach et al. (1997)] and\n[Gong et al. (1997)].\nBibliography\n[Basu et al. (1995)]\nA. Basu, V. Buch, W. Vogels, and T. von Eicken, “U-Net:\nA User-Level Network Interface for Parallel and Distributed Computing”,\nProceedings of the ACM Symposium on Operating Systems Principles (1995).\n[Bershad et al. (1995)]\nB. N. Bershad, S. Savage, P. Pardyak, E. G. Sirer,\nM. Fiuczynski, D. Becker, S. Eggers, and C. Chambers, “Extensibility, Safety and\nPerformance in the SPIN Operating System”, Proceedings of the ACM Symposium\non Operating Systems Principles (1995), pages 267–284.\n[Cohen and Jefferson (1975)]\nE. S. Cohen and D. Jefferson, “Protection in the",
  "E. S. Cohen and D. Jefferson, “Protection in the\nHydra Operating System”, Proceedings of the ACM Symposium on Operating\nSystems Principles (1975), pages 141–160.\n[Dennis and Horn (1966)]\nJ. B. Dennis and E. C. V. Horn, “Programming Seman-\ntics for Multiprogrammed Computations”, Communications of the ACM, Volume\n9, Number 3 (1966), pages 143–155.\n[Ekanadham and Bernstein (1979)]\nK. Ekanadham and A. J. Bernstein, “Con-\nditional Capabilities”, IEEE Transactions on Software Engineering, Volume SE-5,\nNumber 5 (1979), pages 458–464.\n[Ganger et al. (2002)]\nG. R. Ganger, D. R. Engler, M. F. Kaashoek, H. M. Briceno,\nR. Hunt, and T. Pinckney, “Fast and Flexible Application-Level Networking on\nExokernel Systems”, ACM Transactions on Computer Systems, Volume 20, Number\n1 (2002), pages 49–83.",
  "1 (2002), pages 49–83.\n[Goldberg et al. (1996)]\nI. Goldberg, D. Wagner, R. Thomas, and E. A. Brewer,\n“A Secure Environment for Untrusted Helper Applications”, Proceedings of the\n6th Usenix Security Symposium (1996).\n[Gong et al. (1997)]\nL. Gong, M. Mueller, H. Prafullchandra, and R. Schemers,\n“Going Beyond the Sandbox: An Overview of the New Security Architecture in\nthe Java Development Kit 1.2”, Proceedings of the USENIX Symposium on Internet\nTechnologies and Systems (1997). 654\nChapter 14\nProtection\n[Harrison et al. (1976)]\nM. A. Harrison, W. L. Ruzzo, and J. D. Ullman, “Protec-\ntion in Operating Systems”, Communications of the ACM, Volume 19, Number 8\n(1976), pages 461–471.\n[Iliffe and Jodeit (1962)]\nJ. K. Iliffe and J. G. Jodeit, “A Dynamic Storage Alloca-",
  "tion System”, Computer Journal, Volume 5, Number 3 (1962), pages 200–209.\n[Jones and Liskov (1978)]\nA. K. Jones and B. H. Liskov, “A Language Extension\nfor Expressing Constraints on Data Access”, Communications of the ACM, Volume\n21, Number 5 (1978), pages 358–367.\n[Kaashoek et al. (1997)]\nM. F. Kaashoek, D. R. Engler, G. R. Ganger, H. M.\nBriceno, R. Hunt, D. Mazieres, T. Pinckney, R. Grimm, J. Jannotti, and K. Macken-\nzie, “Application Performance and Flexibility on Exokernel Systems”, Pro-\nceedings of the ACM Symposium on Operating Systems Principles (1997), pages\n52–65.\n[Kieburtz and Silberschatz (1978)]\nR. B. Kieburtz and A. Silberschatz, “Capabil-\nity Managers”, IEEE Transactions on Software Engineering, Volume SE-4, Number\n6 (1978), pages 467–477.\n[Kieburtz and Silberschatz (1983)]",
  "[Kieburtz and Silberschatz (1983)]\nR. B. Kieburtz and A. Silberschatz, “Access\nRight Expressions”, ACM Transactions on Programming Languages and Systems,\nVolume 5, Number 1 (1983), pages 78–96.\n[Lampson (1969)]\nB. W. Lampson, “Dynamic Protection Structures”, Proceedings\nof the AFIPS Fall Joint Computer Conference (1969), pages 27–38.\n[Lampson (1971)]\nB. W. Lampson, “Protection”, Proceedings of the Fifth Annual\nPrinceton Conference on Information Systems Science (1971), pages 437–443.\n[Lampson (1973)]\nB. W. Lampson, “A Note on the Conﬁnement Problem”,\nCommunications of the ACM, Volume 10, Number 16 (1973), pages 613–615.\n[Levin et al. (1975)]\nR. Levin, E. S. Cohen, W. M. Corwin, F. J. Pollack, and\nW. A. Wulf, “Policy/Mechanism Separation in Hydra”, Proceedings of the ACM",
  "Symposium on Operating Systems Principles (1975), pages 132–140.\n[Lipner (1975)]\nS. Lipner, “A Comment on the Conﬁnement Problem”, Operating\nSystem Review, Volume 9, Number 5 (1975), pages 192–196.\n[McCanne and Jacobson (1993)]\nS. McCanne and V. Jacobson, “The BSD Packet\nFilter: A New Architecture for User-level Packet Capture”, USENIX Winter\n(1993), pages 259–270.\n[McGraw and Andrews (1979)]\nJ. R. McGraw and G. R. Andrews, “Access\nControl in Parallel Programs”, IEEE Transactions on Software Engineering, Volume\nSE-5, Number 1 (1979), pages 1–9.\n[Morris (1973)]\nJ. H. Morris, “Protection in Programming Languages”, Commu-\nnications of the ACM, Volume 16, Number 1 (1973), pages 15–21.\n[Needham and Walker (1977)]\nR. M. Needham and R. D. H. Walker, “The",
  "R. M. Needham and R. D. H. Walker, “The\nCambridge CAP Computer and Its Protection System”, Proceedings of the Sixth\nSymposium on Operating System Principles (1977), pages 1–10. Bibliography\n655\n[Organick (1972)]\nE. I. Organick, The Multics System: An Examination of Its\nStructure, MIT Press (1972).\n[Popek (1974)]\nG. J. Popek, “Protection Structures”, Computer, Volume 7, Num-\nber 6 (1974), pages 22–33.\n[Redell and Fabry (1974)]\nD. D. Redell and R. S. Fabry, “Selective Revocation\nof Capabilities”, Proceedings of the IRIA International Workshop on Protection in\nOperating Systems (1974), pages 197–210.\n[Saltzer and Schroeder (1975)]\nJ. H. Saltzer and M. D. Schroeder, “The Protec-\ntion of Information in Computer Systems”, Proceedings of the IEEE (1975), pages\n1278–1308.\n[Wahbe et al. (1993)]",
  "1278–1308.\n[Wahbe et al. (1993)]\nR. Wahbe, S. Lucco, T. E. Anderson, and S. L. Graham,\n“Efﬁcient Software-Based Fault Isolation”, ACM SIGOPS Operating Systems\nReview, Volume 27, Number 5 (1993), pages 203–216.\n[Wallach et al. (1997)]\nD. S. Wallach, D. Balfanz, D. Dean, and E. W. Felten,\n“Extensible Security Architectures for Java”, Proceedings of the ACM Symposium\non Operating Systems Principles (1997), pages 116–128.\n[Wulf et al. (1981)]\nW. A. Wulf, R. Levin, and S. P. Harbison, Hydra/C.mmp: An\nExperimental Computer System, McGraw-Hill (1981).  15\nC H A P T E R\nSecurity\nProtection, as we discussed in Chapter 14, is strictly an internal problem: How\ndo we provide controlled access to programs and data stored in a computer",
  "system? Security, on the other hand, requires not only an adequate protection\nsystem but also consideration of the external environment within which the\nsystem operates. A protection system is ineffective if user authentication is\ncompromised or a program is run by an unauthorized user.\nComputer resources must be guarded against unauthorized access, mali-\ncious destruction or alteration, and accidental introduction of inconsistency.\nThese resources include information stored in the system (both data and code),\nas well as the CPU, memory, disks, tapes, and networking that are the com-\nputer. In this chapter, we start by examining ways in which resources may\nbe accidentally or purposely misused. We then explore a key security enabler",
  "—cryptography. Finally, we look at mechanisms to guard against or detect\nattacks.\nCHAPTER OBJECTIVES\n• To discuss security threats and attacks.\n• To explain the fundamentals of encryption, authentication, and hashing.\n• To examine the uses of cryptography in computing.\n• To describe various countermeasures to security attacks.\n15.1\nThe Security Problem\nIn many applications, ensuring the security of the computer system is worth\nconsiderable effort. Large commercial systems containing payroll or other\nﬁnancial data are inviting targets to thieves. Systems that contain data pertain-\ning to corporate operations may be of interest to unscrupulous competitors.\nFurthermore, loss of such data, whether by accident or fraud, can seriously\nimpair the ability of the corporation to function.",
  "In Chapter 14, we discussed mechanisms that the operating system can\nprovide (with appropriate aid from the hardware) that allow users to protect\n657 658\nChapter 15\nSecurity\ntheir resources, including programs and data. These mechanisms work well\nonly as long as the users conform to the intended use of and access to these\nresources. We say that a system is secure if its resources are used and accessed\nas intended under all circumstances. Unfortunately, total security cannot be\nachieved. Nonetheless, we must have mechanisms to make security breaches\na rare occurrence, rather than the norm.\nSecurity violations (or misuse) of the system can be categorized as inten-\ntional (malicious) or accidental. It is easier to protect against accidental misuse",
  "than against malicious misuse. For the most part, protection mechanisms are\nthe core of protection from accidents. The following list includes several forms\nof accidental and malicious security violations. We should note that in our dis-\ncussion of security, we use the terms intruder and cracker for those attempting\nto breach security. In addition, a threat is the potential for a security violation,\nsuch as the discovery of a vulnerability, whereas an attack is the attempt to\nbreak security.\n• Breach of conﬁdentiality. This type of violation involves unauthorized\nreading of data (or theft of information). Typically, a breach of conﬁden-\ntiality is the goal of an intruder. Capturing secret data from a system or\na data stream, such as credit-card information or identity information for",
  "identity theft, can result directly in money for the intruder.\n• Breach of integrity. This violation involves unauthorized modiﬁcation\nof data. Such attacks can, for example, result in passing of liability to\nan innocent party or modiﬁcation of the source code of an important\ncommercial application.\n• Breach of availability. This violation involves unauthorized destruction of\ndata. Some crackers would rather wreak havoc and gain status or bragging\nrights than gain ﬁnancially. Website defacement is a common example of\nthis type of security breach.\n• Theft of service. This violation involves unauthorized use of resources.\nFor example, an intruder (or intrusion program) may install a daemon on\na system that acts as a ﬁle server.",
  "a system that acts as a ﬁle server.\n• Denial of service. This violation involves preventing legitimate use of\nthe system. Denial-of-service (DOS) attacks are sometimes accidental. The\noriginal Internet worm turned into a DOS attack when a bug failed to delay\nits rapid spread. We discuss DOS attacks further in Section 15.3.3.\nAttackers use several standard methods in their attempts to breach\nsecurity. The most common is masquerading, in which one participant in\na communication pretends to be someone else (another host or another\nperson). By masquerading, attackers breach authentication, the correctness\nof identiﬁcation; they can then gain access that they would not normally be\nallowed or escalate their privileges—obtain privileges to which they would not",
  "normally be entitled. Another common attack is to replay a captured exchange\nof data. A replay attack consists of the malicious or fraudulent repeat of a\nvalid data transmission. Sometimes the replay comprises the entire attack—\nfor example, in a repeat of a request to transfer money. But frequently it is\ndone along with message modiﬁcation, again to escalate privileges. Consider 15.1\nThe Security Problem\n659\ncommunication\ncommunication\ncommunication\ncommunication\nsender\nreceiver\nattacker\nsender\nreceiver\nattacker\nsender\nreceiver\nattacker\nMasquerading\nMan-in-the-middle\nNormal\nFigure 15.1\nStandard security attacks.\nthe damage that could be done if a request for authentication had a legitimate\nuser’s information replaced with an unauthorized user’s. Yet another kind of",
  "attack is the man-in-the-middle attack, in which an attacker sits in the data\nﬂow of a communication, masquerading as the sender to the receiver, and\nvice versa. In a network communication, a man-in-the-middle attack may be\npreceded by a session hijacking, in which an active communication session is\nintercepted. Several attack methods are depicted in Figure 15.1.\nAs we have already suggested, absolute protection of the system from\nmalicious abuse is not possible, but the cost to the perpetrator can be made\nsufﬁciently high to deter most intruders. In some cases, such as a denial-of-\nservice attack, it is preferable to prevent the attack but sufﬁcient to detect the\nattack so that countermeasures can be taken.\nTo protect a system, we must take security measures at four levels:",
  "1. Physical. The site or sites containing the computer systems must be\nphysically secured against armed or surreptitious entry by intruders.\nBoth the machine rooms and the terminals or workstations that have\naccess to the machines must be secured. 660\nChapter 15\nSecurity\n2. Human. Authorization must be done carefully to assure that only\nappropriate users have access to the system. Even authorized users,\nhowever, may be “encouraged” to let others use their access (in exchange\nfor a bribe, for example). They may also be tricked into allowing\naccess via social engineering. One type of social-engineering attack\nis phishing. Here, a legitimate-looking e-mail or web page misleads\na user into entering conﬁdential information. Another technique is",
  "dumpster diving, a general term for attempting to gather information in\norder to gain unauthorized access to the computer (by looking through\ntrash, ﬁnding phone books, or ﬁnding notes containing passwords, for\nexample). These security problems are management and personnel issues,\nnot problems pertaining to operating systems.\n3. Operating system. The system must protect itself from accidental or\npurposeful security breaches. A runaway process could constitute an\naccidental denial-of-service attack. A query to a service could reveal pass-\nwords. A stack overﬂow could allow the launching of an unauthorized\nprocess. The list of possible breaches is almost endless.\n4. Network. Much computer data in modern systems travels over private",
  "leased lines, shared lines like the Internet, wireless connections, or dial-up\nlines. Intercepting these data could be just as harmful as breaking into a\ncomputer, and interruption of communications could constitute a remote\ndenial-of-service attack, diminishing users’ use of and trust in the system.\nSecurity at the ﬁrst two levels must be maintained if operating-system\nsecurity is to be ensured. A weakness at a high level of security (physical or\nhuman) allows circumvention of strict low-level (operating-system) security\nmeasures. Thus, the old adage that a chain is only as strong as its weakest link\nis especially true of system security. All of these aspects must be addressed for\nsecurity to be maintained.\nFurthermore, the system must provide protection (Chapter 14) to allow the",
  "implementation of security features. Without the ability to authorize users\nand processes, to control their access, and to log their activities, it would\nbe impossible for an operating system to implement security measures or\nto run securely. Hardware protection features are needed to support an overall\nprotection scheme. For example, a system without memory protection cannot\nbe secure. New hardware features are allowing systems to be made more\nsecure, as we shall discuss.\nUnfortunately, little in security is straightforward. As intruders exploit\nsecurity vulnerabilities, security countermeasures are created and deployed.\nThis causes intruders to become more sophisticated in their attacks. For\nexample, recent security incidents include the use of spyware to provide",
  "a conduit for spam through innocent systems (we discuss this practice in\nSection 15.2). This cat-and-mouse game is likely to continue, with more security\ntools needed to block the escalating intruder techniques and activities.\nIn the remainder of this chapter, we address security at the network and\noperating-system levels. Security at the physical and human levels, although\nimportant, is for the most part beyond the scope of this text. Security within the\noperating system and between operating systems is implemented in several 15.2\nProgram Threats\n661\nways, ranging from passwords for authentication through guarding against\nviruses to detecting intrusions. We start with an exploration of security threats.\n15.2 Program Threats",
  "15.2 Program Threats\nProcesses, along with the kernel, are the only means of accomplishing work\non a computer. Therefore, writing a program that creates a breach of security,\nor causing a normal process to change its behavior and create a breach, is a\ncommon goal of crackers. In fact, even most nonprogram security events have\nas their goal causing a program threat. For example, while it is useful to log in\nto a system without authorization, it is quite a lot more useful to leave behind\na back-door daemon that provides information or allows easy access even if\nthe original exploit is blocked. In this section, we describe common methods\nby which programs cause security breaches. Note that there is considerable\nvariation in the naming conventions for security holes and that we use the",
  "most common or descriptive terms.\n15.2.1\nTrojan Horse\nMany systems have mechanisms for allowing programs written by users to\nbe executed by other users. If these programs are executed in a domain that\nprovides the access rights of the executing user, the other users may misuse\nthese rights. A text-editor program, for example, may include code to search\nthe ﬁle to be edited for certain keywords. If any are found, the entire ﬁle\nmay be copied to a special area accessible to the creator of the text editor.\nA code segment that misuses its environment is called a Trojan horse. Long\nsearch paths, such as are common on UNIX systems, exacerbate the Trojan-\nhorse problem. The search path lists the set of directories to search when an",
  "ambiguous program name is given. The path is searched for a ﬁle of that\nname, and the ﬁle is executed. All the directories in such a search path must\nbe secure, or a Trojan horse could be slipped into the user’s path and executed\naccidentally.\nFor instance, consider the use of the “.” character in a search path. The “.”\ntells the shell to include the current directory in the search. Thus, if a user has\n“.” in her search path, has set her current directory to a friend’s directory, and\nenters the name of a normal system command, the command may be executed\nfrom the friend’s directory. The program will run within the user’s domain,\nallowing the program to do anything that the user is allowed to do, including\ndeleting the user’s ﬁles, for instance.",
  "deleting the user’s ﬁles, for instance.\nA variation of the Trojan horse is a program that emulates a login program.\nAn unsuspecting user starts to log in at a terminal and notices that he has\napparently mistyped his password. He tries again and is successful. What\nhas happened is that his authentication key and password have been stolen\nby the login emulator, which was left running on the terminal by the thief.\nThe emulator stored away the password, printed out a login error message,\nand exited; the user was then provided with a genuine login prompt. This\ntype of attack can be defeated by having the operating system print a usage\nmessage at the end of an interactive session or by a nontrappable key sequence, 662\nChapter 15\nSecurity",
  "Chapter 15\nSecurity\nsuch as the control-alt-delete combination used by all modern Windows\noperating systems.\nAnother variation on the Trojan horse is spyware. Spyware sometimes\naccompanies a program that the user has chosen to install. Most frequently, it\ncomes alongwithfreeware or shareware programs, but sometimes it is included\nwith commercial software. The goal of spyware is to download ads to display\non the user’s system, create pop-up browser windows when certain sites are\nvisited, or capture information from the user’s system and return it to a central\nsite. This latter practice is an example of a general category of attacks known as\ncovert channels, in which surreptitious communication occurs. For example,\nthe installation of an innocuous-seeming program on a Windows system could",
  "result in the loading of a spyware daemon. The spyware could contact a central\nsite, be given a message and a list of recipient addresses, and deliver a spam\nmessage to those users from the Windows machine. This process continues\nuntil the user discovers the spyware. Frequently, the spyware is not discovered.\nIn 2010, it was estimated that 90 percent of spam was being delivered by this\nmethod. This theft of service is not even considered a crime in most countries!\nSpyware is a micro example of a macro problem: violation of the principle\nof least privilege. Under most circumstances, a user of an operating system\ndoes not need to install network daemons. Such daemons are installed via\ntwo mistakes. First, a user may run with more privileges than necessary (for",
  "example, as the administrator), allowing programs that she runs to have more\naccess to the system than is necessary. This is a case of human error—a common\nsecurity weakness. Second, an operating system may allow by default more\nprivileges than a normal user needs. This is a case of poor operating-system\ndesign decisions. An operating system (and, indeed, software in general)\nshould allow ﬁne-grained control of access and security, but it must also be easy\nto manage and understand. Inconvenient or inadequate security measures are\nbound to be circumvented, causing an overall weakening of the security they\nwere designed to implement.\n15.2.2\nTrap Door\nThe designer of a program or system might leave a hole in the software that only",
  "she is capable of using. This type of security breach (or trap door) was shown in\nthe movie War Games. For instance, the code might check for a speciﬁc user ID or\npassword, and it might circumvent normal security procedures. Programmers\nhave been arrested for embezzling from banks by including rounding errors\nin their code and having the occasional half-cent credited to their accounts.\nThis account crediting can add up to a large amount of money, considering the\nnumber of transactions that a large bank executes.\nA clever trap door could be included in a compiler. The compiler could\ngenerate standard object code as well as a trap door, regardless of the source\ncode being compiled. This activity is particularly nefarious, since a search of",
  "the source code of the program will not reveal any problems. Only the source\ncode of the compiler would contain the information.\nTrap doors pose a difﬁcult problem because, to detect them, we have to\nanalyze all the source code for all components of a system. Given that software\nsystems may consist of millions of lines of code, this analysis is not done\nfrequently, and frequently it is not done at all! 15.2\nProgram Threats\n663\n15.2.3\nLogic Bomb\nConsider a program that initiates a security incident only under certain\ncircumstances. It would be hard to detect because under normal operations,\nthere would be no security hole. However, when a predeﬁned set of parameters\nwas met, the security hole would be created. This scenario is known as a logic",
  "bomb. A programmer, for example, might write code to detect whether he\nwas still employed; if that check failed, a daemon could be spawned to allow\nremote access, or code could be launched to cause damage to the site.\n15.2.4\nStack and Buffer Overﬂow\nThe stack- or buffer-overﬂow attack is the most common way for an attacker\noutside the system, on a network or dial-up connection, to gain unauthorized\naccess to the target system. An authorized user of the system may also use this\nexploit for privilege escalation.\nEssentially, the attack exploits a bug in a program. The bug can be a simple\ncase of poor programming, in which the programmer neglected to code bounds\nchecking on an input ﬁeld. In this case, the attacker sends more data than the",
  "program was expecting. By using trial and error, or by examining the source\ncode of the attacked program if it is available, the attacker determines the\nvulnerability and writes a program to do the following:\n1. Overﬂow an input ﬁeld, command-line argument, or input buffer—for\nexample, on a network daemon—until it writes into the stack.\n2. Overwrite the current return address on the stack with the address of the\nexploit code loaded in step 3.\n3. Write a simple set of code for the next space in the stack that includes\nthe commands that the attacker wishes to execute—for instance, spawn\na shell.\nThe result of this attack program’s execution will be a root shell or other\nprivileged command execution.\nFor instance, if a web-page form expects a user name to be entered into a",
  "ﬁeld, the attacker could send the user name, plus extra characters to overﬂow\nthe buffer and reach the stack, plus a new return address to load onto the stack,\nplus the code the attacker wants to run. When the buffer-reading subroutine\nreturns from execution, the return address is the exploit code, and the code is\nrun.\nLet’s look at a buffer-overﬂow exploit in more detail. Consider the simple\nC program shown in Figure 15.2. This program creates a character array of\nsize BUFFER SIZE and copies the contents of the parameter provided on the\ncommand line—argv[1]. As long as the size of this parameter is less than\nBUFFER SIZE (we need one byte to store the null terminator), this program\nworks properly. But consider what happens if the parameter provided on the",
  "command line is longer than BUFFER SIZE. In this scenario, the strcpy()\nfunction will begin copying from argv[1] until it encounters a null terminator\n(\\0) or until the program crashes. Thus, this program suffers from a potential\nbuffer-overﬂow problem in which copied data overﬂow the buffer array. 664\nChapter 15\nSecurity\n#include <stdio.h>\n#define BUFFER SIZE 256\nint main(int argc, char *argv[])\n{\nchar buffer[BUFFER SIZE];\nif (argc < 2)\nreturn -1;\nelse {\nstrcpy(buffer,argv[1]);\nreturn 0;\n}\n}\nFigure 15.2\nC program with buffer-overﬂow condition.\nNote that a careful programmer could have performed bounds checking\non the size of argv[1] by using the strncpy() function rather than strcpy(),\nreplacing the line “strcpy(buffer, argv[1]);” with “strncpy(buffer,",
  "argv[1], sizeof(buffer)-1);”. Unfortunately, good bounds checking is\nthe exception rather than the norm.\nFurthermore, lack of bounds checking is not the only possible cause of the\nbehavior of the program in Figure 15.2. The program could instead have been\ncarefully designed to compromise the integrity of the system. We now consider\nthe possible security vulnerabilities of a buffer overﬂow.\nWhen a function is invoked in a typical computer architecture, the variables\ndeﬁned locally to the function (sometimes known as automatic variables), the\nparameters passed to the function, and the address to which control returns\nonce the function exits are stored in a stack frame. The layout for a typical stack\nframe is shown in Figure 15.3. Examining the stack frame from top to bottom,",
  "we ﬁrst see the parameters passed to the function, followed by any automatic\nvariables declared in the function. We next see the frame pointer, which is\nthe address of the beginning of the stack frame. Finally, we have the return\nparameter(s)\nbottom\ngrows\ntop\nautomatic variables\nsaved frame pointer\n frame pointer\nreturn address\nFigure 15.3\nThe layout for a typical stack frame. 15.2\nProgram Threats\n665\naddress, which speciﬁes where to return control once the function exits. The\nframe pointer must be saved on the stack, as the value of the stack pointer can\nvary during the function call. The saved frame pointer allows relative access\nto parameters and automatic variables.\nGiven this standard memory layout, a cracker could execute a buffer-",
  "overﬂow attack. Her goal is to replace the return address in the stack frame so\nthat it now points to the code segment containing the attacking program.\nThe programmer ﬁrst writes a short code segment such as the following:\n#include <stdio.h>\nint main(int argc, char *argv[])\n{\nexecvp(‘‘\\bin\\sh’’,‘‘\\bin \\sh’’, NULL);\nreturn 0;\n}\nUsing the execvp() system call, this code segment creates a shell process. If\nthe program being attacked runs with system-wide permissions, this newly\ncreated shell will gain complete access to the system. Of course, the code\nsegment could do anything allowed by the privileges of the attacked process.\nThis code segment is then compiled so that the assembly language instructions\ncan be modiﬁed. The primary modiﬁcation is to remove unnecessary features",
  "in the code, thereby reducing the code size so that it can ﬁt into a stack frame.\nThis assembled code fragment is now a binary sequence that will be at the\nheart of the attack.\nRefer again to the program shown in Figure 15.2. Let’s assume that when\nthe main() function is called in that program, the stack frame appears as\nshown in Figure 15.4(a). Using a debugger, the programmer then ﬁnds the\nbuffer(0)\nbuffer(1)\nbuffer(BUFFER_SIZE - 1)\nsaved frame pointer\ncopied\nreturn address\nmodified shell code\nNO _OP \naddress of modified \nshell code\n• \n•\n•\n• \n•\n•\n• \n•\n•\n(a)\n(b)\nFigure 15.4\nHypothetical stack frame for Figure 15.2, (a) before and (b) after. 666\nChapter 15\nSecurity\naddress of buffer[0] in the stack. That address is the location of the code the",
  "attacker wants executed. The binary sequence is appended with the necessary\namount of NO-OP instructions (for NO-OPeration) to ﬁll the stack frame up\nto the location of the return address, and the location of buffer[0], the new\nreturn address, is added. The attack is complete when the attacker gives this\nconstructed binary sequence as input to the process. The process then copies\nthe binary sequence from argv[1] to position buffer[0] in the stack frame.\nNow, when control returns from main(), instead of returning to the location\nspeciﬁed by the old value of the return address, we return to the modiﬁed shell\ncode, which runs with the access rights of the attacked process! Figure 15.4(b)\ncontains the modiﬁed shell code.\nThere are many ways to exploit potential buffer-overﬂow problems. In",
  "this example, we considered the possibility that the program being attacked—\nthe code shown in Figure 15.2—ran with system-wide permissions. However,\nthe code segment that runs once the value of the return address has been\nmodiﬁed might perform any type of malicious act, such as deleting ﬁles,\nopening network ports for further exploitation, and so on.\nThis example buffer-overﬂow attack reveals that considerable knowledge\nand programming skill are needed to recognize exploitable code and then\nto exploit it. Unfortunately, it does not take great programmers to launch\nsecurity attacks. Rather, one cracker can determine the bug and then write an\nexploit. Anyone with rudimentary computer skills and access to the exploit—",
  "a so-called script kiddie—can then try to launch the attack at target systems.\nThe buffer-overﬂow attack is especially pernicious because it can be run\nbetween systems and can travel over allowed communication channels. Such\nattacks can occur within protocols that are expected to be used to communicate\nwith the target machine, and they can therefore be hard to detect and prevent.\nThey can even bypass the security added by ﬁrewalls (Section 15.7).\nOne solution to this problem is for the CPU to have a feature that disallows\nexecution of code in a stack section of memory. Recent versions of Sun’s SPARC\nchip include this setting, and recent versions of Solaris enable it. The return\naddress of the overﬂowed routine can still be modiﬁed; but when the return",
  "address is within the stack and the code there attempts to execute, an exception\nis generated, and the program is halted with an error.\nRecent versionsof AMD and Intel x86chipsinclude the NX feature toprevent\nthis type of attack. The use of the feature is supported in several x86 operating\nsystems, including Linux and Windows XP SP2. The hardware implementation\ninvolves the use of a new bit in the page tables of the CPUs. This bit marks the\nassociated page as nonexecutable, so that instructions cannot be read from it\nand executed. As this feature becomes more prevalent, buffer-overﬂow attacks\nshould greatly diminish.\n15.2.5\nViruses\nAnother form of program threat is a virus. A virus is a fragment of code embed-\nded in a legitimate program. Viruses are self-replicating and are designed to",
  "“infect” other programs. They can wreak havoc in a system by modifying or\ndestroying ﬁles and causing system crashes and program malfunctions. As\nwith most penetration attacks, viruses are very speciﬁc to architectures, oper-\nating systems, and applications. Viruses are a particular problem for users of 15.2\nProgram Threats\n667\nPCs. UNIX and other multiuser operating systems generally are not susceptible\nto viruses because the executable programs are protected from writing by the\noperating system. Even if a virus does infect such a program, its powers usually\nare limited because other aspects of the system are protected.\nViruses are usually borne via e-mail, with spam the most common vector.\nThey can also spread when users download viral programs from Internet",
  "ﬁle-sharing services or exchange infected disks.\nAnother common form of virus transmission uses Microsoft Ofﬁce ﬁles,\nsuch as Microsoft Word documents. These documents can contain macros (or\nVisual Basic programs) that programs in the Ofﬁce suite (Word, PowerPoint,\nand Excel) will execute automatically. Because these programs run under the\nuser’s own account, the macros can run largely unconstrained (for example,\ndeleting user ﬁles at will). Commonly, the virus will also e-mail itself to others\nin the user’s contact list. Here is a code sample that shows how simple it is to\nwrite a Visual Basic macro that a virus could use to format the hard drive of a\nWindows computer as soon as the ﬁle containing the macro was opened:\nSub AutoOpen()\nDim oFS",
  "Sub AutoOpen()\nDim oFS\nSet oFS = CreateObject(’’Scripting.FileSystemObject’’)\nvs = Shell(’’c: command.com /k format c:’’,vbHide)\nEnd Sub\nHow do viruses work? Once a virus reaches a target machine, a program\nknown as a virus dropper inserts the virus into the system. The virus dropper\nis usually a Trojan horse, executed for other reasons but installing the virus\nas its core activity. Once installed, the virus may do any one of a number of\nthings. There are literally thousands of viruses, but they fall into several main\ncategories. Note that many viruses belong to more than one category.\n• File. A standard ﬁle virus infects a system by appending itself to a ﬁle.\nIt changes the start of the program so that execution jumps to its code.",
  "After it executes, it returns control to the program so that its execution is\nnot noticed. File viruses are sometimes known as parasitic viruses, as they\nleave no full ﬁles behind and leave the host program still functional.\n• Boot. A boot virus infects the boot sector of the system, executing every\ntime the system is booted and before the operating system is loaded. It\nwatches for other bootable media and infects them. These viruses are also\nknown as memory viruses, because they do not appear in the ﬁle system.\nFigure 15.5 shows how a boot virus works.\n• Macro. Most viruses are written in a low-level language, such as assembly\nor C. Macro viruses are written in a high-level language, such as Visual\nBasic. These viruses are triggered when a program capable of executing",
  "the macro is run. For example, a macro virus could be contained in a\nspreadsheet ﬁle.\n• Source code. A source code virus looks for source code and modiﬁes it to\ninclude the virus and to help spread the virus. 668\nChapter 15\nSecurity\nwhenever new \nremovable R/W disk \nis installed, it infects \nthat as well\nit has a logic bomb to \nwreak havoc at a \ncertain date\nvirus replaces \noriginal boot block \nwith itself\nat system boot, virus \ndecreases physical\nmemory, hides in memory \nabove new limit\nvirus attaches to disk read- \nwrite interrupt, monitors all \ndisk activity\nit blocks any attempts of  \nother programs to write the  \nboot sector\nvirus copies boot \nsector to unused \nlocation X\nFigure 15.5\nA boot-sector computer virus.\n• Polymorphic. A polymorphic virus changes each time it is installed to",
  "avoid detection by antivirus software. The changes do not affect the virus’s\nfunctionality but rather change the virus’s signature. A virus signature is\na pattern that can be used to identify a virus, typically a series of bytes that\nmake up the virus code.\n• Encrypted. An encrypted virus includes decryption code along with the\nencrypted virus, again to avoid detection. The virus ﬁrst decrypts and then\nexecutes.\n• Stealth. This tricky virus attempts to avoid detection by modifying parts\nof the system that could be used to detect it. For example, it could modify\nthe read system call so that if the ﬁle it has modiﬁed is read, the original\nform of the code is returned rather than the infected code.\n• Tunneling. This virus attempts to bypass detection by an antivirus scanner",
  "by installing itself in the interrupt-handler chain. Similar viruses install\nthemselves in device drivers. 15.3\nSystem and Network Threats\n669\n• Multipartite. A virus of this type is able to infect multiple parts of a system,\nincluding boot sectors, memory, and ﬁles. This makes it difﬁcult to detect\nand contain.\n• Armored. An armored virus is coded to make it hard for antivirus\nresearchers to unravel and understand. It can also be compressed to avoid\ndetection and disinfection. In addition, virus droppers and other full ﬁles\nthat are part of a virus infestation are frequently hidden via ﬁle attributes\nor unviewable ﬁle names.\nThis vast variety of viruses has continued to grow. For example, in 2004\na new and widespread virus was detected. It exploited three separate bugs",
  "for its operation. This virus started by infecting hundreds of Windows servers\n(including many trusted sites) running Microsoft Internet Information Server\n(IIS). Any vulnerable Microsoft Explorer web browser visiting those sites\nreceived a browser virus with any download. The browser virus installed\nseveral back-door programs, including a keystroke logger, which records\neverything entered on the keyboard (including passwords and credit-card\nnumbers). It also installed a daemon to allow unlimited remote access by\nan intruder and another that allowed an intruder to route spam through the\ninfected desktop computer.\nGenerally, viruses are the most disruptive security attacks, and because\nthey are effective, they will continue to be written and to spread. An active",
  "security-related debate within the computing community concerns the exis-\ntence of a monoculture, in which many systems run the same hardware,\noperating system, and application software. This monoculture supposedly\nconsists of Microsoft products. One question is whether such a monoculture\neven exists today. Another question is whether, if it does, it increases the threat\nof and damage caused by viruses and other security intrusions.\n15.3 System and Network Threats\nProgram threats typically use a breakdown in the protection mechanisms of a\nsystem to attack programs. In contrast, system and network threats involve the\nabuse of services and network connections. System and network threats create\na situation in which operating-system resources and user ﬁles are misused.",
  "Sometimes, a system and network attack is used to launch a program attack,\nand vice versa.\nThe more open an operating system is—the more services it has enabled\nand the more functions it allows—the more likely it is that a bug is available\nto exploit. Increasingly, operating systems strive to be secure by default.\nFor example, Solaris 10 moved from a model in which many services (FTP,\ntelnet, and others) were enabled by default when the system was installed\nto a model in which almost all services are disabled at installation time and\nmust speciﬁcally be enabled by system administrators. Such changes reduce\nthe system’s attack surface—the set of ways in which an attacker can try to\nbreak into the system.\nIn the remainder of this section, we discuss some examples of system",
  "and network threats, including worms, port scanning, and denial-of-service 670\nChapter 15\nSecurity\nattacks. It is important to note that masquerading and replay attacks are also\ncommonly launched over networks between systems. In fact, these attacks\nare more effective and harder to counter when multiple systems are involved.\nFor example, within a computer, the operating system usually can determine\nthe sender and receiver of a message. Even if the sender changes to the ID of\nsomeone else, there may be a record of that ID change. When multiple systems\nare involved, especially systems controlled by attackers, then such tracing is\nmuch more difﬁcult.\nIn general, we can say that sharing secrets (to prove identity and as keys to",
  "encryption) is required for authentication and encryption, and sharing secrets\nis easier in environments (such as a single operating system) in which secure\nsharing methods exist. These methods include shared memory and interpro-\ncess communications. Creating secure communication and authentication is\ndiscussed in Section 15.4 and Section 15.5.\n15.3.1\nWorms\nA worm is a process that uses the spawn mechanism to duplicate itself. The\nworm spawns copies of itself, using up system resources and perhaps locking\nout all other processes. On computer networks, worms are particularly potent,\nsince they may reproduce themselves among systems and thus shut down an\nentire network. Such an event occurred in 1988 to UNIX systems on the Internet,",
  "causing the loss of system and system-administrator time worth millions of\ndollars.\nAt the close of the workday on November 2, 1988, Robert Tappan Morris,\nJr., a ﬁrst-year Cornell graduate student, unleashed a worm program on one\nor more hosts connected to the Internet. Targeting Sun Microsystems’ Sun 3\nworkstations and VAX computers running variants of Version 4 BSD UNIX, the\nworm quickly spread over great distances. Within a few hours of its release,\nit had consumed system resources to the point of bringing down the infected\nmachines.\nAlthough Morris designed the self-replicating program for rapid reproduc-\ntion and distribution, some of the features of the UNIX networking environment\nprovided the means to propagate the worm throughout the system. It is likely",
  "that Morrischose forinitial infectionanInternet host left openforand accessible\nto outside users. From there, the worm program exploited ﬂaws in the UNIX\noperating system’s security routines and took advantage of UNIX utilities that\nsimplify resource sharing in local-area networks to gain unauthorized access\nto thousands of other connected sites. Morris’s methods of attack are outlined\nnext.\nThe worm was made up of two programs, a grappling hook (also called\na bootstrap or vector) program and the main program. Named l1.c, the\ngrappling hook consisted of 99 lines of C code compiled and run on each\nmachine it accessed. Once established on the computer system under attack,\nthe grappling hook connected to the machine where it originated and uploaded",
  "a copy of the main worm onto the hooked system (Figure 15.6). The main\nprogram proceeded to search for other machines to which the newly infected\nsystem could connect easily. In these actions, Morris exploited the UNIX\nnetworking utility rsh for easy remote task execution. By setting up special ﬁles\nthat list host–login name pairs, users can omit entering a password each time 15.3\nSystem and Network Threats\n671\ngrappling \nhook\nworm\ntarget system\nworm\ninfected system\nrsh attack\nfinger attack\nsendmail attack\nrequest for worm\nworm sent\nFigure 15.6\nThe Morris Internet worm.\nthey access a remote account on the paired list. The worm searched these special\nﬁles for site names that would allow remote execution without a password.",
  "Where remote shells were established, the worm program was uploaded and\nbegan executing anew.\nThe attack via remote access was one of three infection methods built into\nthe worm. The other two methods involved operating-system bugs in the UNIX\nfinger and sendmail programs.\nThe finger utility functions as an electronic telephone directory. The\ncommand\nfinger user-name@hostname\nreturns a person’s real and login names along with other information that\nthe user may have provided, such as ofﬁce and home address and telephone\nnumber, research plan, or clever quotation. Finger runs as a background\nprocess (or daemon) at each BSD site and responds to queries throughout the\nInternet. The worm executed a buffer-overﬂow attack on finger. The program",
  "queried finger with a 536-byte string crafted to exceed the buffer allocated\nfor input and to overwrite the stack frame. Instead of returning to the main\nroutine where it resided before Morris’s call, the finger daemon was routed\nto a procedure within the invading 536-byte string now residing on the stack.\nThe new procedure executed /bin/sh, which, if successful, gave the worm a\nremote shell on the machine under attack.\nThe bug exploited in sendmail also involved using a daemon process\nfor malicious entry. sendmail sends, receives, and routes electronic mail.\nDebugging code in the utility permits testers to verify and display the state of\nthe mail system. The debugging option was useful to system administrators",
  "and was often left on. Morris included in his attack arsenal a call to debug that\n—instead of specifying a user address, as would be normal in testing—issued\na set of commands that mailed and executed a copy of the grappling-hook\nprogram.\nOnce in place, the main worm systematically attempted to discover user\npasswords. It began by trying simple cases of no password or passwords\nconstructed of account–user-name combinations, then used comparisons with\nan internal dictionary of 432 favorite password choices, and then went to the 672\nChapter 15\nSecurity\nﬁnal stage of trying each word in the standard UNIX on-line dictionary as a\npossible password. This elaborate and efﬁcient three-stage password-cracking\nalgorithm enabled the worm to gain access to other user accounts on the",
  "infected system. The worm then searched for rsh data ﬁles in these newly\nbroken accounts and used them as described previously to gain access to user\naccounts on remote systems.\nWith each new access, the worm program searched for already active\ncopies of itself. If it found one, the new copy exited, except in every seventh\ninstance. Had the worm exited on all duplicate sightings, it might have\nremained undetected. Allowing every seventh duplicate to proceed (possibly\nto confound efforts to stop its spread by baiting with “fake” worms) created a\nwholesale infestation of Sun and VAX systems on the Internet.\nThe very features of the UNIX network environment that assisted in the\nworm’s propagation also helped to stop its advance. Ease of electronic commu-",
  "nication, mechanisms to copy source and binary ﬁles to remote machines, and\naccess to both source code and human expertise allowed cooperative efforts to\ndevelop solutions quickly. By the evening of the next day, November 3, methods\nof halting the invading program were circulated to system administrators via\nthe Internet. Within days, speciﬁc software patches for the exploited security\nﬂaws were available.\nWhy did Morris unleash the worm? The action has been characterized\nas both a harmless prank gone awry and a serious criminal offense. Based\non the complexity of the attack, it is unlikely that the worm’s release or the\nscope of its spread was unintentional. The worm program took elaborate steps\nto cover its tracks and to repel efforts to stop its spread. Yet the program",
  "contained no code aimed at damaging or destroying the systems on which it\nran. The author clearly had the expertise to include such commands; in fact,\ndata structures were present in the bootstrap code that could have been used to\ntransfer Trojan-horse or virus programs. The behavior of the program may lead\nto interesting observations, but it does not provide a sound basis for inferring\nmotive. What is not open to speculation, however, is the legal outcome: a\nfederal court convicted Morris and handed down a sentence of three years’\nprobation, 400 hours of community service, and a $10,000 ﬁne. Morris’s legal\ncosts probably exceeded $100,000.\nSecurity experts continue to evaluate methods to decrease or eliminate\nworms. A more recent event, though, shows that worms are still a fact of",
  "life on the Internet. It also shows that as the Internet grows, the damage\nthat even “harmless” worms can do also grows and can be signiﬁcant. This\nexample occurred during August 2003. The ﬁfth version of the “Sobig” worm,\nmore properly known as “W32.Sobig.F@mm,” was released by persons at this\ntime unknown. It was the fastest-spreading worm released to date, at its peak\ninfecting hundreds of thousands of computers and one in seventeen e-mail\nmessages on the Internet. It clogged e-mail inboxes, slowed networks, and\ntook a huge number of hours to clean up.\nSobig.F was launched by being uploaded to a pornography newsgroup via\nan account created with a stolen credit card. It was disguised as a photo. The\nvirus targeted Microsoft Windows systems and used its own SMTP engine to",
  "e-mail itself to all the addresses found on an infected system. It used a variety\nof subject lines to help avoid detection, including “Thank You!” “Your details,” 15.3\nSystem and Network Threats\n673\nand “Re: Approved.” It also used a random address on the host as the “From:”\naddress, making it difﬁcult to determine from the message which machine was\nthe infected source. Sobig.F included an attachment for the target e-mail reader\nto click on, again with a variety of names. If this payload was executed, it stored\na program called WINPPR32.EXE in the default Windows directory, along with\na text ﬁle. It also modiﬁed the Windows registry.\nThe code included in the attachment was also programmed to periodically\nattempt to connect to one of twenty servers and download and execute a",
  "program from them. Fortunately, the servers were disabled before the code\ncould be downloaded. The content of the program from these servers has not\nyet been determined. If the code was malevolent, untold damage to a vast\nnumber of machines could have resulted.\n15.3.2\nPort Scanning\nPort scanning is not an attack but rather a means for a cracker to detect\na system’s vulnerabilities to attack. Port scanning typically is automated,\ninvolving a tool that attempts to create a TCP/IP connection to a speciﬁc port\nor a range of ports. For example, suppose there is a known vulnerability (or\nbug) in sendmail. A cracker could launch a port scanner to try to connect, say,\nto port 25 of a particular system or to a range of systems. If the connection",
  "was successful, the cracker (or tool) could attempt to communicate with the\nanswering service to determine if the service was indeed sendmail and, if so,\nif it was the version with the bug.\nNow imagine a tool in which each bug of every service of every operating\nsystem was encoded. The tool could attempt to connect to every port of one\nor more systems. For every service that answered, it could try to use each\nknown bug. Frequently, the bugs are buffer overﬂows, allowing the creation of\na privileged command shell on the system. From there, of course, the cracker\ncould install Trojan horses, back-door programs, and so on.\nThere is no such tool, but there are tools that perform subsets of that\nfunctionality. For example, nmap (from http://www.insecure.org/nmap/) is",
  "a very versatile open-source utility for network exploration and security\nauditing. When pointed at a target, it will determine what services are running,\nincluding application names and versions. It can identify the host operating\nsystem. It can also provide information about defenses, such as what ﬁrewalls\nare defending the target. It does not exploit any known bugs.\nBecause port scans are detectable (Section 15.6.3), they frequently are\nlaunched from zombie systems. Such systems are previously compromised,\nindependent systems that are serving their owners while being used for nefar-\nious purposes, including denial-of-service attacks and spam relay. Zombies\nmake crackers particularly difﬁcult to prosecute because determining the",
  "source of the attack and the person that launched it is challenging. This is\none of many reasons for securing “inconsequential” systems, not just systems\ncontaining “valuable” information or services.\n15.3.3\nDenial of Service\nAs mentioned earlier, denial-of-service attacks are aimed not at gaining\ninformation or stealing resources but rather at disrupting legitimate use of\na system or facility. Most such attacks involve systems that the attacker has 674\nChapter 15\nSecurity\nnot penetrated. Launching an attack that prevents legitimate use is frequently\neasier than breaking into a machine or facility.\nDenial-of-service attacks are generally network based. They fall into two\ncategories. Attacks in the ﬁrst category use so many facility resources that,",
  "in essence, no useful work can be done. For example, a website click could\ndownload a Java applet that proceeds to use all available CPU time or to pop\nup windows inﬁnitely. The second category involves disrupting the network\nof the facility. There have been several successful denial-of-service attacks of\nthis kind against major websites. These attacks result from abuse of some of the\nfundamental functionality of TCP/IP. For instance, if the attacker sends the part\nof the protocol that says “I want to start a TCP connection,” but never follows\nwith the standard “The connection is now complete,” the result can be partially\nstarted TCP sessions. If enough of these sessions are launched, they can eat up\nall the network resources of the system, disabling any further legitimate TCP",
  "connections. Such attacks, which can last hours or days, have caused partial or\nfull failure of attempts to use the target facility. The attacks are usually stopped\nat the network level until the operating systems can be updated to reduce their\nvulnerability.\nGenerally, it is impossible to prevent denial-of-service attacks. The attacks\nuse the same mechanisms as normal operation. Even more difﬁcult to prevent\nand resolve are distributed denial-of-service (DDOS) attacks. These attacks\nare launched from multiple sites at once, toward a common target, typically\nby zombies. DDOS attacks have become more common and are sometimes\nassociated with blackmail attempts. A site comes under attack, and the\nattackers offer to halt the attack in exchange for money.",
  "Sometimes a site does not even know it is under attack. It can be difﬁcult\nto determine whether a system slowdown is an attack or just a surge in system\nuse. Consider that a successful advertising campaign that greatly increases\ntrafﬁc to a site could be considered a DDOS.\nThere are other interesting aspects of DOS attacks. For example, if an\nauthentication algorithm locks an account for a period of time after several\nincorrect attempts to access the account, then an attacker could cause all\nauthentication to be blocked by purposely making incorrect attempts to access\nall accounts. Similarly, a ﬁrewall that automatically blocks certain kinds of\ntrafﬁc could be induced to block that trafﬁc when it should not. These examples",
  "suggest that programmers and systems managers need to fully understand the\nalgorithms and technologies they are deploying. Finally, computer science\nclasses are notorious sources of accidental system DOS attacks. Consider the\nﬁrst programming exercises in which students learn to create subprocesses\nor threads. A common bug involves spawning subprocesses inﬁnitely. The\nsystem’s free memory and CPU resources don’t stand a chance.\n15.4 Cryptography as a Security Tool\nThere are many defenses against computer attacks, running the gamut from\nmethodology to technology. The broadest tool available to system designers\nand users is cryptography. In this section, we discuss cryptography and its\nuse in computer security. Note that the cryptography discussed here has been",
  "simpliﬁed for educational purposes; readers are cautioned against using any 15.4\nCryptography as a Security Tool\n675\nof the schemes described here in the real world. Good cryptography libraries\nare widely available and would make a good basis for production applications.\nIn an isolated computer, the operating system can reliably determine the\nsender and recipient of all interprocess communication, since it controls all\ncommunication channels in the computer. In a network of computers, the\nsituation is quite different. A networked computer receives bits “from the\nwire” with no immediate and reliable way of determining what machine or\napplication sent those bits. Similarly, the computer sends bits onto the network\nwith no way of knowing who might eventually receive them. Additionally,",
  "when either sending or receiving, the system has no way of knowing if an\neavesdropper listened to the communication.\nCommonly, network addresses are used to infer the potential senders\nand receivers of network messages. Network packets arrive with a source\naddress, such as an IP address. And when a computer sends a message, it\nnames the intended receiver by specifying a destination address. However, for\napplications where security matters, we are asking for trouble if we assume\nthat the source or destination address of a packet reliably determines who sent\nor received that packet. A rogue computer can send a message with a falsiﬁed\nsource address, and numerous computers other than the one speciﬁed by the\ndestination address can (and typically do) receive a packet. For example, all of",
  "the routers on the way to the destination will receive the packet, too. How, then,\nis an operating system to decide whether to grant a request when it cannot trust\nthe named source of the request? And how is it supposed to provide protection\nfor a request or data when it cannot determine who will receive the response\nor message contents it sends over the network?\nIt is generally considered infeasible to build a network of any scale in\nwhich the source and destination addresses of packets can be trusted in this\nsense. Therefore, the only alternative is somehow to eliminate the need to\ntrust the network. This is the job of cryptography. Abstractly, cryptography is\nused to constrain the potential senders and/or receivers of a message. Modern",
  "cryptography is based on secrets called keys that are selectively distributed to\ncomputers in a network and used to process messages. Cryptography enables a\nrecipient of a message to verify that the message was created by some computer\npossessing a certain key. Similarly, a sender can encode its message so that\nonly a computer with a certain key can decode the message. Unlike network\naddresses, however, keys are designed so that it is not computationally feasible\nto derive them from the messages they were used to generate or from any\nother public information. Thus, they provide a much more trustworthy means\nof constraining senders and receivers of messages. Note that cryptography is\na ﬁeld of study unto itself, with large and small complexities and subtleties.",
  "Here, we explore the most important aspects of the parts of cryptography that\npertain to operating systems.\n15.4.1\nEncryption\nBecause it solves a wide variety of communication security problems, encryp-\ntion is used frequently in many aspects of modern computing. It is used to send\nmessages securely across across a network, as well as to protect database data,\nﬁles, and even entire disks from having their contents read by unauthorized\nentities. An encryption algorithm enables the sender of a message to ensure that 676\nChapter 15\nSecurity\nonly a computer possessing a certain key can read the message, or ensure that\nthe writer of data is the only reader of that data. Encryption of messages is an\nancient practice, of course, and there have been many encryption algorithms,",
  "dating back to ancient times. In this section, we describe important modern\nencryption principles and algorithms.\nAn encryption algorithm consists of the following components:\n• A set K of keys.\n• A set M of messages.\n• A set C of ciphertexts.\n• An encrypting function E : K →(M →C). That is, for each k ∈K, Ek is a\nfunction for generating ciphertexts from messages. Both E and Ek for any k\nshould be efﬁciently computable functions. Generally, Ek is a randomized\nmapping from messages to ciphertexts.\n• A decrypting function D : K →(C →M). That is, for each k ∈K, Dk is a\nfunction for generating messages from ciphertexts. Both D and Dk for any\nk should be efﬁciently computable functions.\nAn encryption algorithm must provide this essential property: given a",
  "ciphertext c ∈C, a computer can compute m such that Ek(m) = c only if\nit possesses k. Thus, a computer holding k can decrypt ciphertexts to the\nplaintexts used to produce them, but a computer not holding k cannot decrypt\nciphertexts. Since ciphertexts are generally exposed (for example, sent on a\nnetwork), it is important that it be infeasible to derive k from the ciphertexts.\nThere are two main types of encryption algorithms: symmetric and\nasymmetric. We discuss both types in the following sections.\n15.4.1.1\nSymmetric Encryption\nIn a symmetric encryption algorithm, the same key is used to encrypt and to\ndecrypt. Therefore, the secrecy of k must be protected. Figure 15.7 shows an\nexample of two users communicating securely via symmetric encryption over",
  "an insecure channel. Note that the key exchange can take place directly between\nthe two parties or via a trusted third party (that is, a certiﬁcate authority), as\ndiscussed in Section 15.4.1.4.\nFor the past several decades, the most commonly used symmetric encryp-\ntion algorithm in the United States for civilian applications has been the\ndata-encryption standard (DES) cipher adopted by the National Institute of\nStandards and Technology (NIST). DES works by taking a 64-bit value and\na 56-bit key and performing a series of transformations that are based on\nsubstitution and permutation operations. Because DES works on a block of bits\nat a time, is known as a block cipher, and its transformations are typical of\nblock ciphers. With block ciphers, if the same key is used for encrypting an",
  "extended amount of data, it becomes vulnerable to attack.\nDES is now considered insecure for many applications because its keys can\nbe exhaustively searched with moderate computing resources. (Note, though,\nthat it is still frequently used.) Rather than giving up on DES, NIST created a\nmodiﬁcation called triple DES, in which the DES algorithm is repeated three\ntimes (two encryptions and one decryption) on the same plaintext using two 15.4\nCryptography as a Security Tool\n677\nkey\nexchange\nmessage m\nmessage m\nencryption\nalgorithm\nE\ndecryption\nalgorithm\nD\nwrite\nencryption\nkey k\ndecryption\nkey k\nread\ninsecure\nchannel\nplaintext\nciphertext\nc = Ek(m)\nplaintext\nm = Dk(c)\nattacker\nFigure 15.7\nA secure communication over an insecure medium.",
  "A secure communication over an insecure medium.\nor three keys—for example, c = Ek3(Dk2(Ek1(m))). When three keys are used,\nthe effective key length is 168 bits. Triple DES is in widespread use today.\nIn 2001, NIST adopted a new block cipher, called the advanced encryption\nstandard (AES), to replace DES. AES is another block cipher. It can use key\nlengths of 128, 192, or 256 bits and works on 128-bit blocks. Generally, the\nalgorithm is compact and efﬁcient.\nBlock ciphers are not in themselves secure encryption schemes. In partic-\nular, they do not directly handle messages longer than their required block\nsizes. However, there are many modes of encryption that are based on stream\nciphers, which can be used to securely encrypt longer messages.",
  "RC4 is perhaps the most common stream cipher. A stream cipher is\ndesigned to encrypt and decrypt a stream of bytes or bits rather than a block.\nThis is useful when the length of a communication would make a block cipher\ntoo slow. The key is input into a pseudo–random-bit generator, which is an\nalgorithm that attempts to produce random bits. The output of the generator\nwhen fed a key is a keystream. A keystream is an inﬁnite set of bits that can\nbe used to encrypt a plaintext stream by simply XORing it with the plaintext.\n(XOR, for “eXclusive OR” is an operation that compares two input bits and\ngenerates one output bit. If the bits are the same, the result is 0. If the bits\nare different, the result is 1.) RC4 is used in encrypting steams of data, such",
  "as in WEP, the wireless LAN protocol. Unfortunately, RC4 as used in WEP (IEEE\nstandard 802.11) has been found to be breakable in a reasonable amount of\ncomputer time. In fact, RC4 itself has vulnerabilities. 678\nChapter 15\nSecurity\n15.4.1.2\nAsymmetric Encryption\nIn an asymmetric encryption algorithm, there are different encryption and\ndecryption keys. An entity preparing to receive encrypted communication\ncreates two keys and makes one of them (called the public key) available to\nanyone who wants it. Any sender can use that key to encrypt a communication,\nbut only the key creator can decrypt the communication. This scheme, known\nas public-key encryption, was a breakthrough in cryptography. No longer\nmust a key be kept secret and delivered securely. Instead, anyone can encrypt",
  "a message to the receiving entity, and no matter who else is listening, only that\nentity can decrypt the message.\nAs an example of how public-key encryption works, we describe an\nalgorithm known as RSA, after its inventors, Rivest, Shamir, and Adleman.\nRSA is the most widely used asymmetric encryption algorithm. (Asymmetric\nalgorithms based on elliptic curves are gaining ground, however, because\nthe key length of such an algorithm can be shorter for the same amount of\ncryptographic strength.)\nIn RSA, ke is the public key, and kd is the private key. N is the product of\ntwo large, randomly chosen prime numbers p and q (for example, p and q are\n512 bits each). It must be computationally infeasible to derive kd,N from ke,N, so",
  "that ke need not be kept secret and can be widely disseminated. The encryption\nalgorithm is Eke,N(m) = mke mod N, where ke satisﬁes kekd mod (p−1)(q −1) =\n1. The decryption algorithm is then Dkd,N(c) = ckd mod N.\nAn example using small values is shown in Figure 15.8. In this example, we\nmake p = 7andq = 13. We thencalculate N = 7∗13 = 91and (p−1)(q−1) = 72.\nWe next select ke relatively prime to 72 and < 72, yielding 5. Finally, we calculate\nkd such that kekd mod 72 = 1, yielding 29. We now have our keys: the public\nkey, ke,N = 5, 91, and the private key, kd,N = 29, 91. Encrypting the message 69\nwith the public key results in the message 62, which is then decoded by the\nreceiver via the private key.\nThe use of asymmetric encryption begins with the publication of the public",
  "key of the destination. For bidirectional communication, the source also must\npublish its public key. “Publication” can be as simple as handing over an\nelectronic copy of the key, or it can be more complex. The private key (or “secret\nkey”) must be zealously guarded, as anyone holding that key can decrypt any\nmessage created by the matching public key.\nWe should note that the seemingly small difference in key use between\nasymmetric and symmetric cryptography is quite large in practice. Asymmetric\ncryptography is much more computationally expensive to execute. It is much\nfaster for a computer to encode and decode ciphertext by using the usual\nsymmetric algorithms than by using asymmetric algorithms. Why, then, use",
  "an asymmetric algorithm? In truth, these algorithms are not used for general-\npurpose encryption of large amounts of data. However, they are used not\nonly for encryption of small amounts of data but also for authentication,\nconﬁdentiality, and key distribution, as we show in the following sections.\n15.4.1.3\nAuthentication\nWe have seen that encryption offers a way of constraining the set of possible\nreceivers of a message. Constraining the set of potential senders of a message\nis called authentication. Authentication is thus complementary to encryption. 15.4\nCryptography as a Security Tool\n679\nmessage 69\n69\n695 mod 91\n6229 mod 91\nwrite\nencryption\nkey k5,91\ndecryption\nkey k29,91\nread\ninsecure\nchannel\nplaintext\n62\nFigure 15.8\nEncryption and decryption using RSA asymmetric cryptography.",
  "Authentication is also useful for proving that a message has not been modiﬁed.\nIn this section, we discuss authentication as a constraint on possible senders of\na message. Note that this sort of authentication is similar to but distinct from\nuser authentication, which we discuss in Section 15.5.\nAn authentication algorithm using symmetric keys consists of the follow-\ning components:\n• A set K of keys.\n• A set M of messages.\n• A set A of authenticators.\n• A function S : K →(M →A). That is, for each k ∈K, Sk is a function for\ngenerating authenticators from messages. Both S and Sk for any k should\nbe efﬁciently computable functions.\n• A function V : K →(M × A →{true, false}). That is, for each k ∈K, Vk\nis a function for verifying authenticators on messages. Both V and Vk for",
  "any k should be efﬁciently computable functions.\nThe critical property that an authentication algorithm must possess is this:\nfor a message m, a computer can generate an authenticator a ∈A such\nthat Vk(m, a) = true only if it possesses k. Thus, a computer holding k can 680\nChapter 15\nSecurity\ngenerate authenticators on messages so that any computer possessing k can\nverify them. However, a computer not holdingk cannot generate authenticators\non messages that can be veriﬁed using Vk. Since authenticators are generally\nexposed (for example, sent on a network with the messages themselves), it\nmust not be feasible to derive k from the authenticators. Practically, if Vk(m, a)\n= true, then we know that m has not been modiﬁed, and that the sender of",
  "the message has k. If we share k with only one entity, then we know that the\nmessage originated from k.\nJust as there are two types of encryption algorithms, there are two main\nvarieties of authentication algorithms. The ﬁrst step in understanding these\nalgorithms is to explore hash functions. A hash function H(m) creates a small,\nﬁxed-sized block of data, known as a message digest or hash value, from a\nmessage m. Hash functions work by taking a message, splitting it into blocks,\nand processing the blocks to produce an n-bit hash. H must be collision resistant\n—that is, it must be infeasible to ﬁnd an m′ ̸= m such that H(m) = H(m′). Now,\nif H(m) = H(m′), we know that m = m′—that is, we know that the message\nhas not been modiﬁed. Common message-digest functions include MD5, now",
  "considered insecure, which produces a 128-bit hash, and SHA-1, which outputs\na 160-bit hash. Message digests are useful for detecting changed messages but\nare not useful as authenticators. For example, H(m) can be sent along with a\nmessage; but if H is known, then someone could modify m to m′ and recompute\nH(m′), and the message modiﬁcation would not be detected. Therefore, we\nmust authenticate H(m).\nThe ﬁrst main type of authentication algorithm uses symmetric encryp-\ntion. In a message-authentication code (MAC), a cryptographic checksum is\ngenerated from the message using a secret key. A MAC provides a way to\nsecurely authenticate short values. If we use it to authenticate H(m) for an H\nthat is collision resistant, then we obtain a way to securely authenticate long",
  "messages by hashing them ﬁrst. Note that k is needed to compute both Sk and\nVk, so anyone able to compute one can compute the other.\nThe second main type of authentication algorithm is a digital-signature\nalgorithm, and the authenticators thus produced are called digital signatures.\nDigital signatures are very useful in that they enable anyone to verify the\nauthenticity of the message. In a digital-signature algorithm, it is computa-\ntionally infeasible to derive ks from kv. Thus, kv is the public key, and ks is the\nprivate key.\nConsider as an example the RSA digital-signature algorithm. It is similar\nto the RSA encryption algorithm, but the key use is reversed. The digital\nsignature of a message is derived by computing Sks(m) = H(m)ks mod N.",
  "The key ks again is a pair ⟨d, N⟩, where N is the product of two large,\nrandomly chosen prime numbers p and q. The veriﬁcation algorithm is then\nVkv(m, a)\n?=a kv mod N = H(m)), where kv satisﬁes kvks mod (p −1)(q −1) = 1.\nNote that encryption and authentication may be used together or sepa-\nrately. Sometimes, for instance, we want authentication but not conﬁdentiality.\nFor example, a company could provide a software patch and could “sign” that\npatch to prove that it came from the company and that it hasn’t been modiﬁed.\nAuthentication is a component of many aspects of security. For example,\ndigital signatures are the core of nonrepudiation, which supplies proof that\nan entity performed an action. A typical example of nonrepudiation involves 15.4\nCryptography as a Security Tool\n681",
  "Cryptography as a Security Tool\n681\nthe ﬁlling out of electronic forms as an alternative to the signing of paper\ncontracts. Nonrepudiation assures that a person ﬁlling out an electronic form\ncannot deny that he did so.\n15.4.1.4\nKey Distribution\nCertainly, a good part of the battle between cryptographers (those inventing\nciphers) and cryptanalysts (those trying to break them) involves keys. With\nsymmetric algorithms, both parties need the key, and no one else should\nhave it. The delivery of the symmetric key is a huge challenge. Sometimes\nit is performed out-of-band—say, via a paper document or a conversation.\nThese methods do not scale well, however. Also consider the key-management\nchallenge. Suppose a user wanted to communicate with N other users privately.",
  "That user would need N keys and, for more security, would need to change\nthose keys frequently.\nThese are the very reasons for efforts to create asymmetric key algorithms.\nNot only can the keys be exchanged in public, but a given user needs only\none private key, no matter how many other people she wants to communicate\nwith. There is still the matter of managing a public key for each recipient of the\ncommunication, but since public keys need not be secured, simple storage can\nbe used for that key ring.\nUnfortunately, even the distribution of public keys requires some care.\nConsider the man-in-the-middle attack shown in Figure 15.9. Here, the person\nwho wants to receive an encrypted message sends out his public key, but an",
  "attacker also sends her “bad” public key (which matches her private key). The\nperson who wants to send the encrypted message knows no better and so uses\nthe bad key to encrypt the message. The attacker then happily decrypts it.\nThe problem is one of authentication—what we need is proof of who (or\nwhat) owns a public key. One way to solve that problem involves the use\nof digital certiﬁcates. A digital certiﬁcate is a public key digitally signed by a\ntrusted party. The trusted party receives proof of identiﬁcation from some entity\nand certiﬁes that the public key belongs to that entity. But how do we know\nwe can trust the certiﬁer? These certiﬁcate authorities have their public keys\nincluded within web browsers (and other consumers of certiﬁcates) before they",
  "are distributed. The certiﬁcate authorities can then vouch for other authorities\n(digitally signing the public keys of these other authorities), and so on, creating\na web of trust. The certiﬁcates can be distributed in a standard X.509 digital\ncertiﬁcate format that can be parsed by computer. This scheme is used for\nsecure web communication, as we discuss in Section 15.4.3.\n15.4.2\nImplementation of Cryptography\nNetwork protocols are typically organized in layers, like an onion or a parfait,\nwith each layer acting as a client of the one below it. That is, when one protocol\ngenerates a message to send to its protocol peer on another machine, it hands\nits message to the protocol below it in the network-protocol stack for delivery",
  "to its peer on that machine. For example, in an IP network, TCP (a transport-\nlayer protocol) acts as a client of IP (a network-layer protocol): TCP packets are\npassed down to IP for delivery to the IP peer at the other end of the connection.\nIP encapsulates the TCP packet in an IP packet, which it similarly passes down\nto the data-link layer to be transmitted across the network to its peer on the 682\nChapter 15\nSecurity\nmessage m\nencryption\nalgorithm\nE\ndecryption\nalgorithm\nD\nwrite\n3. Ekbad(m)\nmessage m\nread\nencryption\nkey kbad\ndecryption\nkey kd\ndecryption\nalgorithm\nD\ndecryption\nkey kbad\n2. Public\nkey kbad\n1. Public\nkey ke\nattacker\nFigure 15.9\nA man-in-the-middle attack on asymmetric cryptography.\ndestination computer. This IP peer then delivers the TCP packet up to the TCP",
  "peer on that machine.\nCryptography can be inserted at almost any layer in the OSI model. SSL\n(Section 15.4.3), for example, provides security at the transport layer. Network-\nlayer security generally has been standardized on IPSec, which deﬁnes IPpacket\nformats that allow the insertion of authenticators and the encryption of packet\ncontents. IPSec uses symmetric encryption and uses the Internet Key Exchange\n(IKE) protocol for key exchange. IKE is based on pubic-key encryption. IPSec\nis becoming widely used as the basis for virtual private networks (VPNs), in\nwhich all trafﬁc between two IPSec endpoints is encrypted to make a private\nnetwork out of one that may otherwise be public. Numerous protocols also\nhave been developed for use by applications, such as PGP for encrypting e-mail,",
  "but then the applications themselves must be coded to implement security.\nWhere is cryptographic protection best placed in a protocol stack? In\ngeneral, there is no deﬁnitive answer. On the one hand, more protocols beneﬁt\nfrom protections placed lower in the stack. For example, since IP packets\nencapsulate TCPpackets, encryption of IPpackets (using IPSec, for example) also 15.4\nCryptography as a Security Tool\n683\nhides the contents of the encapsulated TCP packets. Similarly, authenticators\non IP packets detect the modiﬁcation of contained TCP header information.\nOn the other hand, protection at lower layers in the protocol stack\nmay give insufﬁcient protection to higher-layer protocols. For example, an\napplication server that accepts connections encrypted with IPSec might be",
  "able to authenticate the client computers from which requests are received.\nHowever, to authenticate a user at a client computer, the server may need to use\nan application-level protocol—the user may be required to type a password.\nAlso consider the problem of e-mail. E-mail delivered via the industry-standard\nSMTP protocol is stored and forwarded, frequently multiple times, before it is\ndelivered. Each of these transmissions could go over a secure or an insecure\nnetwork. For e-mail to be secure, the e-mail message needs to be encrypted so\nthat its security is independent of the transports that carry it.\n15.4.3\nAn Example: SSL\nSSL 3.0 is a cryptographic protocol that enables two computers to communicate\nsecurely—that is, so that each can limit the sender and receiver of messages",
  "to the other. It is perhaps the most commonly used cryptographic protocol\non the Internet today, since it is the standard protocol by which web browsers\ncommunicate securely with web servers. For completeness, we should note that\nSSL was designed by Netscape and that it evolved into the industry- standard\nTLS protocol. In this discussion, we use SSL to mean both SSL and TLS.\nSSL is a complex protocol with many options. Here, we present only a single\nvariation of it. Even then, we describe it in a very simpliﬁed and abstract form,\nso as to maintain focus on its use of cryptographic primitives. What we are\nabout to see is a complex dance in which asymmetric cryptography is used so\nthat a client and a server can establish a secure session key that can be used",
  "for symmetric encryption of the session between the two—all of this while\navoiding man-in-the-middle and replay attacks. For added cryptographic\nstrength, the session keys are forgotten once a session is completed. Another\ncommunication between the two will require generation of new session keys.\nThe SSL protocol is initiated by a client c to communicate securely with a\nserver. Prior to the protocol’s use, the server s is assumed to have obtained a\ncertiﬁcate, denoted certs, from certiﬁcation authority CA. This certiﬁcate is a\nstructure containing the following:\n• Various attributes (attrs) of the server, such as its unique distinguished\nname and its common (DNS) name\n• The identity of a asymmetric encryption algorithm E() for the server\n• The public key ke of this server",
  "• The public key ke of this server\n• A validity interval (interval) during which the certiﬁcate should be consid-\nered valid\n• A digital signature a on the above information made by the CA—that is,\na = SkC A(⟨attrs, Eke, interval ⟩)\nIn addition, prior to the protocol’s use, the client is presumed to have\nobtained the public veriﬁcation algorithm VkC A for CA. In the case of the Web,\nthe user’s browser is shipped from its vendor containing the veriﬁcation 684\nChapter 15\nSecurity\nalgorithms and public keys of certain certiﬁcation authorities. The user can\nadd or delete these as she chooses.\nWhen c connects to s, it sends a 28-byte random value nc to the server, which\nresponds with a random value ns of its own, plus its certiﬁcate certs. The client",
  "veriﬁes that VkC A(⟨attrs, Eke, interval⟩, a) = true and that the current time is\nin the validity interval interval. If both of these tests are satisﬁed, the server\nhas proved its identity. Then the client generates a random 46-byte premaster\nsecret pms and sends cpms = Eke(pms) to the server. The server recovers pms\n= Dkd(cpms). Now both the client and the server are in possession of nc, ns,\nand pms, and each can compute a shared 48-byte master secret ms = H(nc, ns,\npms). Only the server and client can compute ms, since only they know pms.\nMoreover, the dependence of ms on nc and ns ensures that ms is a fresh value\n—that is, a session key that has not been used in a previous communication.\nAt this point, the client and the server both compute the following keys from\nthe ms:",
  "the ms:\n• A symmetric encryption key kcrypt\ncs\nfor encrypting messages from the client\nto the server\n• A symmetric encryption key kcrypt\nsc\nfor encrypting messages from the server\nto the client\n• A MAC generation key kmac\ncs\nfor generating authenticators on messages\nfrom the client to the server\n• A MAC generation key kmac\nsc\nfor generating authenticators on messages\nfrom the server to the client\nTo send a message m to the server, the client sends\nc = Ekcrypt\ncs\n(⟨m, Skmac\ncs\n(m)⟩).\nUpon receiving c, the server recovers\n⟨m, a⟩= Dkcrypt\ncs\n(c)\nand accepts m if Vkmac\ncs\n(m, a) = true. Similarly, to send a message m to the client,\nthe server sends\nc = Ekcrypt\nsc\n(⟨m, Skmac\nsc\n(m)⟩)\nand the client recovers\n⟨m, a⟩= Dkcrypt\nsc\n(c)\nand accepts m if Vkmac\nsc\n(m, a) = true.",
  "sc\n(c)\nand accepts m if Vkmac\nsc\n(m, a) = true.\nThis protocol enables the server to limit the recipients of its messages to the\nclient that generated pms and to limit the senders of the messages it accepts\nto that same client. Similarly, the client can limit the recipients of the messages\nit sends and the senders of the messages it accepts to the party that knows kd\n(that is, the party that can decrypt cpms). In many applications, such as web\ntransactions, the client needs to verify the identity of the party that knows kd.\nThis is one purpose of the certiﬁcate certs. In particular, the attrs ﬁeld contains\ninformation that the client can use to determine the identity—for example, the 15.5\nUser Authentication\n685\ndomain name—of the server with which it is communicating. For applications",
  "in which the server also needs information about the client, SSL supports an\noption by which a client can send a certiﬁcate to the server.\nIn addition to its use on the Internet, SSL is being used for a wide variety\nof tasks. For example, IPSec VPNs now have a competitor in SSL VPNs. IPSec\nis good for point-to-point encryption of trafﬁc—say, between two company\nofﬁces. SSL VPNs are more ﬂexible but not as efﬁcient, so they might be used\nbetween an individual employee working remotely and the corporate ofﬁce.\n15.5 User Authentication\nOur earlier discussion of authentication involves messages and sessions. But\nwhat about users? If a system cannot authenticate a user, then authenticating\nthat a message came from that user is pointless. Thus, a major security problem",
  "for operating systems is user authentication. The protection system depends\non the ability to identify the programs and processes currently executing,\nwhich in turn depends on the ability to identify each user of the system.\nUsers normally identify themselves. How do we determine whether a user’s\nidentity is authentic? Generally, user authentication is based on one or more\nof three things: the user’s possession of something (a key or card), the user’s\nknowledge of something (a user identiﬁer and password), or an attribute of\nthe user (ﬁngerprint, retina pattern, or signature).\n15.5.1\nPasswords\nThe most common approach to authenticating a user identity is the use of\npasswords. When the user identiﬁes herself by user ID or account name, she",
  "is asked for a password. If the user-supplied password matches the password\nstored in the system, the system assumes that the account is being accessed by\nthe owner of that account.\nPasswords are often used to protect objects in the computer system, in\nthe absence of more complete protection schemes. They can be considered a\nspecial case of either keys or capabilities. For instance, a password may be\nassociated with each resource (such as a ﬁle). Whenever a request is made to\nuse the resource, the password must be given. If the password is correct, access\nis granted. Different passwords may be associated with different access rights.\nFor example, different passwords may be used for reading ﬁles, appending\nﬁles, and updating ﬁles.",
  "ﬁles, and updating ﬁles.\nIn practice, most systems require only one password for a user to gain\nfull rights. Although more passwords theoretically would be more secure,\nsuch systems tend not to be implemented due to the classic trade-off between\nsecurity and convenience. If security makes something inconvenient, then the\nsecurity is frequently bypassed or otherwise circumvented.\n15.5.2\nPassword Vulnerabilities\nPasswords are extremely common because they are easy to understand and use.\nUnfortunately, passwords can often be guessed, accidentally exposed, sniffed\n(read by an eavesdropper), or illegally transferred from an authorized user to\nan unauthorized one, as we show next. 686\nChapter 15\nSecurity\nThere are two common ways to guess a password. One way is for the",
  "intruder (either human or program) to know the user or to have information\nabout the user. All too frequently, people use obvious information (such as the\nnames of their cats or spouses) as their passwords. The other way is to use brute\nforce, trying enumeration—or all possible combinations of valid password\ncharacters (letters, numbers, and punctuation on some systems)—until the\npassword is found. Short passwords are especially vulnerable to this method.\nFor example, a four-character password provides only 10,000 variations. On\naverage, guessing 5,000 times would produce a correct hit. A program that\ncould try a password every millisecond would take only about 5 seconds to\nguess a four-character password. Enumeration is less successful where systems",
  "allow longer passwords that include both uppercase and lowercase letters,\nalong with numbers and all punctuation characters. Of course, users must take\nadvantage of the large password space and must not, for example, use only\nlowercase letters.\nIn addition to being guessed, passwords can be exposed as a result of\nvisual or electronic monitoring. An intruder can look over the shoulder of a\nuser (shoulder surﬁng) when the user is logging in and can learn the password\neasily by watching the keyboard. Alternatively, anyone with access to the\nnetwork on which a computer resides can seamlessly add a network monitor,\nallowing him to sniff, or watch, all data being transferred on the network,\nincluding user IDs and passwords. Encrypting the data stream containing the",
  "password solves this problem. Even such a system could have passwords\nstolen, however. For example, if a ﬁle is used to contain the passwords, it\ncould be copied for off-system analysis. Or consider a Trojan-horse program\ninstalled on the system that captures every keystroke before sending it on to\nthe application.\nExposure is a particularly severe problem if the password is written down\nwhere it can be read or lost. Some systems force users to select hard-to-\nremember or long passwords, or to change their password frequently, which\nmay cause a user to record the password or to reuse it. As a result, such\nsystems provide much less security than systems that allow users to select\neasy passwords!\nThe ﬁnal type of password compromise, illegal transfer, is the result of",
  "human nature. Most computer installations have a rule that forbids users to\nshare accounts. This rule is sometimes implemented for accounting reasons but\nis often aimed at improving security. For instance, suppose one user IDis shared\nby several users, and a security breach occurs from that user ID. It is impossible\nto know who was using the ID at the time the break occurred or even whether\nthe user was an authorized one. With one user per user ID, any user can be\nquestioned directly about use of the account; in addition, the user might notice\nsomething different about the account and detect the break-in. Sometimes,\nusers break account-sharing rules to help friends or to circumvent accounting,\nand this behavior can result in a system’s being accessed by unauthorized users",
  "—possibly harmful ones.\nPasswords can be either generated by the system or selected by a user.\nSystem-generated passwords may be difﬁcult to remember, and thus users may\nwrite them down. As mentioned, however, user-selected passwords are often\neasy to guess (the user’s name or favorite car, for example). Some systems will\ncheck a proposed password for ease of guessing or cracking before accepting 15.5\nUser Authentication\n687\nit. Some systems also age passwords, forcing users to change their passwords\nat regular intervals (every three months, for instance). This method is not\nfoolproof either, because users can easily toggle between two passwords. The\nsolution, as implemented on some systems, is to record a password history for",
  "each user. For instance, the system could record the last N passwords and not\nallow their reuse.\nSeveral variants on these simple password schemes can be used. For\nexample, the password can be changed more frequently. At the extreme, the\npassword is changed from session to session. A new password is selected\n(either by the system or by the user) at the end of each session, and that\npassword must be used for the next session. In such a case, even if a password\nis used by an unauthorized person, that person can use it only once. When\nthe legitimate user tries to use a now-invalid password at the next session, he\ndiscovers the security violation. Steps can then be taken to repair the breached\nsecurity.\n15.5.3\nSecuring Passwords",
  "security.\n15.5.3\nSecuring Passwords\nOne problem with all these approaches is the difﬁculty of keeping the password\nsecret within the computer. How can the system store a password securely yet\nallow its use for authentication when the user presents her password? The UNIX\nsystem uses secure hashing to avoid the necessity of keeping its password list\nsecret. Because the list is hashed rather than encrypted, it is impossible for the\nsystem to decrypt the stored value and determine the original password.\nHere’s how this system works. Each user has a password. The system\ncontains a function that is extremely difﬁcult—the designers hope impossible\n—to invert but is simple to compute. That is, given a value x, it is easy to",
  "compute the hash function value f(x). Given a function value f(x), however,\nit is impossible to compute x. This function is used to encode all passwords.\nOnly encoded passwords are stored. When a user presents a password, it is\nhashed and compared against the stored encoded password. Even if the stored\nencoded password is seen, it cannot be decoded, so the password cannot be\ndetermined. Thus, the password ﬁle does not need to be kept secret.\nThe ﬂaw in this method is that the system no longer has control over the\npasswords. Although the passwords are hashed, anyone with a copy of the\npassword ﬁle can run fast hash routines against it—hashing each word in\na dictionary, for instance, and comparing the results against the passwords.",
  "If the user has selected a password that is also a word in the dictionary, the\npassword is cracked. On sufﬁciently fast computers, or even on clusters of\nslow computers, such a comparison may take only a few hours. Furthermore,\nbecause UNIX systems use a well-known hashing algorithm, a cracker might\nkeep a cache of passwords that have been cracked previously. For these\nreasons, systems include a “salt,” or recorded random number, in the hashing\nalgorithm. The salt value isadded tothe password toensure that iftwoplaintext\npasswords are the same, they result in different hash values. In addition, the\nsalt value makes hashing a dictionary ineffective, because each dictionary term\nwould need to be combined with each salt value for comparison to the stored",
  "passwords. Newer versions of UNIX also store the hashed password entries in\na ﬁle readable only by the superuser. The programs that compare the hash to 688\nChapter 15\nSecurity\nthe stored value are run setuid to root, so they can read this ﬁle, but other\nusers cannot.\nAnother weakness in the UNIX password methods is that many UNIX\nsystems treat only the ﬁrst eight characters as signiﬁcant. It is therefore\nextremely important for users to take advantage of the available password\nspace. Complicating the issue further is the fact that some systems do not allow\nthe use of dictionary words as passwords. A good technique is to generate your\npassword by using the ﬁrst letter of each word of an easily remembered phrase\nusing both upper and lower characters with a number or punctuation mark",
  "thrown in for good measure. For example, the phrase “My mother’s name is\nKatherine” might yield the password “Mmn.isK!”. The password is hard to\ncrack but easy for the user to remember. A more secure system would allow\nmore characters in its passwords. Indeed, a system might also allow passwords\nto include the space character, so that a user could create a passphrase.\n15.5.4\nOne-Time Passwords\nTo avoid the problems of password snifﬁng and shoulder surﬁng, a system can\nuse a set of paired passwords. When a session begins, the system randomly\nselects and presents one part of a password pair; the user must supply the\nother part. In this system, the user is challenged and must respond with the\ncorrect answer to that challenge.",
  "correct answer to that challenge.\nThis approach can be generalized to the use of an algorithm as a password.\nSuch algorithmic passwords are not susceptible to reuse. That is, a user\ncan type in a password, and no entity intercepting that password will be\nable to reuse it. In this scheme, the system and the user share a symmetric\npassword. The password pw is never transmitted over a medium that allows\nexposure. Rather, the password is used as input to the function, along with a\nchallenge ch presented by the system. The user then computes the function\nH(pw, ch). The result of this function is transmitted as the authenticator to\nthe computer. Because the computer also knows pw and ch, it can perform\nthe same computation. If the results match, the user is authenticated. The next",
  "time the user needs to be authenticated, another ch is generated, and the same\nsteps ensue. This time, the authenticator is different. This one-time password\nsystem is one of only a few ways to prevent improper authentication due to\npassword exposure.\nOne-time password systems are implemented in various ways. Commer-\ncial implementations use hardware calculators with a display or a display\nand numeric keypad. These calculators generally take the shape of a credit\ncard, a key-chain dongle, or a USB device. Software running on computers\nor smartphones provides the user with H(pw, ch); pw can be input by the\nuser or generated by the calculator in synchronization with the computer.\nSometimes, pw is just a personal identiﬁcation number (PIN). The output",
  "of any of these systems shows the one-time password. A one-time password\ngenerator that requires input by the user involves two-factor authentication.\nTwo different types of components are needed in this case—for example, a\none-time password generator that generates the correct response only if the PIN\nis valid. Two-factor authentication offers far better authentication protection\nthan single-factor authentication because it requires “something you have” as\nwell as “something you know.” 15.6\nImplementing Security Defenses\n689\nAnother variation on one-time passwords uses a code book, or one-time\npad, which is a list of single-use passwords. Each password on the list is used\nonce and then is crossed out or erased. The commonly used S/Key system",
  "uses either a software calculator or a code book based on these calculations\nas a source of one-time passwords. Of course, the user must protect his code\nbook, and it is helpful if the code book does not identify the system to which\nthe codes are authenticators.\n15.5.5\nBiometrics\nYet another variation on the use of passwords for authentication involves\nthe use of biometric measures. Palm- or hand-readers are commonly used to\nsecure physical access—for example, access to a data center. These readers\nmatch stored parameters against what is being read from hand-reader pads.\nThe parameters can include a temperature map, as well as ﬁnger length, ﬁnger\nwidth, and line patterns. These devices are currently too large and expensive\nto be used for normal computer authentication.",
  "to be used for normal computer authentication.\nFingerprint readers have become accurate and cost-effective and should\nbecome more common in the future. These devices read ﬁnger ridge patterns\nand convert them into a sequence of numbers. Over time, they can store a set of\nsequences to adjust for the location of the ﬁnger on the reading pad and other\nfactors. Software can then scan a ﬁnger on the pad and compare its features\nwith these stored sequences to determine if they match. Of course, multiple\nusers can have proﬁles stored, and the scanner can differentiate among them.\nA very accurate two-factor authentication scheme can result from requiring\na password as well as a user name and ﬁngerprint scan. If this information",
  "is encrypted in transit, the system can be very resistant to spooﬁng or replay\nattack.\nMultifactor authentication is better still. Consider how strong authentica-\ntion can be with a USB device that must be plugged into the system, a PIN, and\na ﬁngerprint scan. Except for having to place ones ﬁnger on a pad and plug the\nUSB into the system, this authentication method is no less convenient than that\nusing normal passwords. Recall, though, that strong authentication by itself is\nnot sufﬁcient to guarantee the ID of the user. An authenticated session can still\nbe hijacked if it is not encrypted.\n15.6 Implementing Security Defenses\nJust as there are myriad threats to system and network security, there are many\nsecurity solutions. The solutions range from improved user education, through",
  "technology, to writing bug-free software. Most security professionals subscribe\nto the theory of defense in depth, which states that more layers of defense are\nbetter than fewer layers. Of course, this theory applies to any kind of security.\nConsider the security of a house without a door lock, with a door lock, and\nwith a lock and an alarm. In this section, we look at the major methods, tools,\nand techniques that can be used to improve resistance to threats.\n15.6.1\nSecurity Policy\nThe ﬁrst step toward improving the security of any aspect of computing is to\nhave a security policy. Policies vary widely but generally include a statement 690\nChapter 15\nSecurity\nof what is being secured. For example, a policy might state that all outside-",
  "accessible applications must have a code review before being deployed, or that\nusers should not share their passwords, or that all connection points between a\ncompany and the outside must have port scans run every six months. Without\na policy in place, it is impossible for users and administrators to know what\nis permissible, what is required, and what is not allowed. The policy is a road\nmap to security, and if a site is trying to move from less secure to more secure,\nit needs a map to know how to get there.\nOnce the security policy is in place, the people it affects should know it\nwell. It should be their guide. The policy should also be a living document\nthat is reviewed and updated periodically to ensure that it is still pertinent and\nstill followed.\n15.6.2\nVulnerability Assessment",
  "still followed.\n15.6.2\nVulnerability Assessment\nHow can we determine whether a security policy has been correctly imple-\nmented? The best way is to execute a vulnerability assessment. Such assess-\nments can cover broad ground, from social engineering through risk assess-\nment to port scans. Risk assessment, for example, attempts to value the assets\nof the entity in question (a program, a management team, a system, or a\nfacility) and determine the odds that a security incident will affect the entity\nand decrease its value. When the odds of suffering a loss and the amount of the\npotential loss are known, a value can be placed on trying to secure the entity.\nThe core activity of most vulnerability assessments is a penetration test,",
  "in which the entity is scanned for known vulnerabilities. Because this book\nis concerned with operating systems and the software that runs on them, we\nconcentrate on those aspects of vulnerability assessment.\nVulnerability scans typically are done at times when computer use is\nrelatively low, to minimize their impact. When appropriate, they are done on\ntest systems rather than production systems, because they can induce unhappy\nbehavior from the target systems or network devices.\nA scan within an individual system can check a variety of aspects of the\nsystem:\n• Short or easy-to-guess passwords\n• Unauthorized privileged programs, such as setuid programs\n• Unauthorized programs in system directories\n• Unexpectedly long-running processes",
  "• Unexpectedly long-running processes\n• Improper directory protections on user and system directories\n• Improper protections on system data ﬁles, such as the password ﬁle, device\ndrivers, or the operating-system kernel itself\n• Dangerous entries in the program search path (for example, the Trojan\nhorse discussed in Section 15.2.1)\n• Changes to system programs detected with checksum values\n• Unexpected or hidden network daemons\nAny problems found by a security scan can be either ﬁxed automatically or\nreported to the managers of the system. 15.6\nImplementing Security Defenses\n691\nNetworked computers are much more susceptible to security attacks than\nare standalone systems. Rather than attacks from a known set of access",
  "points, such as directly connected terminals, we face attacks from an unknown\nand large set of access points—a potentially severe security problem. To a\nlesser extent, systems connected to telephone lines via modems are also more\nexposed.\nIn fact, the U.S. government considers a system to be only as secure as its\nmost far-reaching connection. For instance, a top-secret system may be accessed\nonly from within a building also considered top-secret. The system loses its top-\nsecret rating if any form of communication can occur outside that environment.\nSome government facilities take extreme security precautions. The connectors\nthat plug a terminal into the secure computer are locked in a safe in the ofﬁce\nwhen the terminal is not in use. A person must have proper ID to gain access to",
  "the building and her ofﬁce, must know a physical lock combination, and must\nknow authentication information for the computer itself to gain access to the\ncomputer—an example of multifactor authentication.\nUnfortunately for system administrators and computer-security profes-\nsionals, it is frequently impossible to lock a machine in a room and disallow\nall remote access. For instance, the Internet currently connects millions of\ncomputers and has become a mission-critical, indispensable resource for many\ncompanies and individuals. If you consider the Internet a club, then, as in any\nclub with millions of members, there are many good members and some bad\nmembers. The bad members have many tools they can use to attempt to gain",
  "access to the interconnected computers, just as Morris did with his worm.\nVulnerability scans can be applied to networks to address some of the\nproblems with network security. The scans search a network for ports that\nrespond to a request. If services are enabled that should not be, access to them\ncan be blocked, or they can be disabled. The scans then determine the details of\nthe application listening on that port and try to determine if it has any known\nvulnerabilities. Testing those vulnerabilities can determine if the system is\nmisconﬁgured or lacks needed patches.\nFinally, though, consider the use of port scanners in the hands of a cracker\nrather than someone trying to improve security. These tools could help crackers",
  "ﬁnd vulnerabilities to attack. (Fortunately, it is possible to detect port scans\nthrough anomaly detection, as we discuss next.) It is a general challenge to\nsecurity that the same tools can be used for good and for harm. In fact, some\npeople advocate security through obscurity, stating that no tools should be\nwritten to test security, because such tools can be used to ﬁnd (and exploit)\nsecurity holes. Others believe that this approach to security is not a valid one,\npointing out, for example, that crackers could write their own tools. It seems\nreasonable that security through obscurity be considered one of the layers\nof security only so long as it is not the only layer. For example, a company\ncould publish its entire network conﬁguration, but keeping that information",
  "secret makes it harder for intruders to know what to attack or to determine\nwhat might be detected. Even here, though, a company assuming that such\ninformation will remain a secret has a false sense of security.\n15.6.3\nIntrusion Detection\nSecuring systems and facilities is intimately linked to intrusion detection. Intru-\nsion detection, as its name suggests, strives to detect attempted or successful 692\nChapter 15\nSecurity\nintrusions into computer systems and to initiate appropriate responses to the\nintrusions. Intrusion detection encompasses a wide array of techniques that\nvary on a number of axes, including the following:\n• The time at which detection occurs. Detection can occur in real time (while\nthe intrusion is occurring) or after the fact.",
  "the intrusion is occurring) or after the fact.\n• The types of inputs examined to detect intrusive activity. These may\ninclude user-shell commands, process system calls, and network packet\nheaders or contents. Some forms of intrusion might be detected only by\ncorrelating information from several such sources.\n• The range of response capabilities. Simple forms of response include\nalerting an administrator to the potential intrusion or somehow halting\nthe potentially intrusive activity—for example, killing a process engaged\nin such activity. In a sophisticated form of response, a system might\ntransparently divert an intruder’s activity to a honeypot—a false resource\nexposed to the attacker. The resource appears real to the attacker and",
  "enables the system to monitor and gain information about the attack.\nThese degrees of freedom in the design space for detecting intrusions have\nyielded a wide range of solutions, known as intrusion-detection systems\n(IDSs) and intrusion-prevention systems (IDPs). IDS systems raise an alarm\nwhen an intrusion is detected, while IDP systems act as routers, passing trafﬁc\nunless an intrusion is detected (at which point that trafﬁc is blocked).\nBut just what constitutes an intrusion? Deﬁning a suitable speciﬁcation of\nintrusion turns out to be quite difﬁcult, and thus automatic IDSs and IDPs today\ntypically settle for one of two less ambitious approaches. In the ﬁrst, called\nsignature-based detection, system input or network trafﬁc is examined for",
  "speciﬁc behavior patterns (or signatures) known to indicate attacks. A simple\nexample of signature-based detection is scanning network packets for the string\n/etc/passwd/ targeted for a UNIX system. Another example is virus-detection\nsoftware, which scans binaries or network packets for known viruses.\nThe second approach, typically called anomaly detection, attempts\nthrough various techniques to detect anomalous behavior within computer\nsystems. Of course, not all anomalous system activity indicates an intrusion,\nbut the presumption is that intrusions often induce anomalous behavior. An\nexample of anomaly detection is monitoring system calls of a daemon process\nto detect whether the system-call behavior deviates from normal patterns,",
  "possibly indicating that a buffer overﬂow has been exploited in the daemon\nto corrupt its behavior. Another example is monitoring shell commands to\ndetect anomalous commands for a given user or detecting an anomalous login\ntime for a user, either of which may indicate that an attacker has succeeded in\ngaining access to that user’s account.\nSignature-based detection and anomaly detection can be viewed as two\nsides of the same coin. Signature-based detection attempts to characterize\ndangerous behaviors and to detect when one of these behaviors occurs,\nwhereas anomaly detection attempts to characterize normal (or nondangerous)\nbehaviors and to detect when something other than these behaviors occurs.\nThese different approaches yield IDSs and IDPs with very different proper-",
  "ties, however. In particular, anomaly detection can ﬁnd previously unknown 15.6\nImplementing Security Defenses\n693\nmethods of intrusion (so-called zero-day attacks). Signature-based detection,\nin contrast, will identify only known attacks that can be codiﬁed in a rec-\nognizable pattern. Thus, new attacks that were not contemplated when the\nsignatures were generated will evade signature-based detection. This problem\nis well known to vendors of virus-detection software, who must release new\nsignatures with great frequency as new viruses are detected manually.\nAnomaly detection is not necessarily superior to signature-based detection,\nhowever. Indeed, a signiﬁcant challenge for systems that attempt anomaly\ndetection is to benchmark “normal” system behavior accurately. If the system",
  "has already been penetrated when it is benchmarked, then the intrusive activity\nmay be included in the “normal” benchmark. Even if the system is bench-\nmarked cleanly, without inﬂuence from intrusive behavior, the benchmark\nmust give a fairly complete picture of normal behavior. Otherwise, the number\nof false positives (false alarms) or, worse, false negatives (missed intrusions)\nwill be excessive.\nTo illustrate the impact of even a marginally high rate of false alarms,\nconsider an installation consisting of a hundred UNIX workstations from which\nsecurity-relevant events are recorded for purposes of intrusion detection. A\nsmall installation such as this could easily generate a million audit records per\nday. Only one or two might be worthy of an administrator’s investigation. If we",
  "suppose, optimistically, that each actual attack is reﬂected in ten audit records,\nwe can roughly compute the rate of occurrence of audit records reﬂecting truly\nintrusive activity as follows:\n2intrusions\nday\n· 10 records\nintrusion\n106 records\nday\n= 0.00002.\nInterpreting this as a “probability of occurrence of intrusive records,” we\ndenote it as P(I); that is, event I is the occurrence of a record reﬂecting truly\nintrusive behavior. Since P(I) = 0.00002, we also know that P(¬I) = 1−P(I) =\n0.99998. Now we let Adenote the raising of an alarm by an IDS. An accurate IDS\nshould maximize both P(I|A) and P(¬I|¬A)—that is, the probabilities that an\nalarm indicates an intrusion and that no alarm indicates no intrusion. Focusing\non P(I|A) for the moment, we can compute it using Bayes’ theorem:",
  "P(I|A) =\nP(I) · P(A|I)\nP(I) · P(A|I) + P(¬I) · P(A|¬I)\n=\n0.00002 · P(A|I)\n0.00002 · P(A|I) + 0.99998 · P(A|¬I)\nNow consider the impact of the false-alarm rate P(A|¬I) on P(I|A). Even\nwith a very good true-alarm rate of P(A|I) = 0.8, a seemingly good false-\nalarm rate of P(A|¬I) = 0.0001 yields P(I|A) ≈0.14. That is, fewer than one\nin every seven alarms indicates a real intrusion! In systems where a security\nadministrator investigates each alarm, a high rate of false alarms—called a\n“Christmas tree effect”—is exceedingly wasteful and will quickly teach the\nadministrator to ignore alarms. 694\nChapter 15\nSecurity\nThis example illustrates a general principle for IDSs and IDPs: for usability,\nthey must offer an extremely low false-alarm rate. Achieving a sufﬁciently",
  "low false-alarm rate is an especially serious challenge for anomaly-detection\nsystems, as mentioned, because of the difﬁculties of adequately benchmarking\nnormal system behavior. However, research continues to improve anomaly-\ndetection techniques. Intrusion detection software is evolving to implement\nsignatures, anomaly algorithms, and other algorithms and to combine the\nresults to arrive at a more accurate anomaly-detection rate.\n15.6.4\nVirus Protection\nAs we have seen, viruses can and do wreak havoc on systems. Protection from\nviruses thus is an important security concern. Antivirus programs are often\nused to provide this protection. Some of these programs are effective against\nonly particular known viruses. They work by searching all the programs on",
  "a system for the speciﬁc pattern of instructions known to make up the virus.\nWhen they ﬁnd a known pattern, they remove the instructions, disinfecting\nthe program. Antivirus programs may have catalogs of thousands of viruses\nfor which they search.\nBothvirusesand antivirussoftware continue tobecome more sophisticated.\nSome viruses modify themselves as they infect other software to avoid the basic\npattern-match approach of antivirus programs. Antivirus programs in turn\nnow look for families of patterns rather than a single pattern to identify a virus.\nIn fact, some antivirus programs implement a variety of detection algorithms.\nThey can decompress compressed viruses before checking for a signature.\nSome also look for process anomalies. A process opening an executable ﬁle",
  "for writing is suspicious, for example, unless it is a compiler. Another popular\ntechnique is to run a program in a sandbox, which is a controlled or emulated\nsection of the system. The antivirus software analyzes the behavior of the code\nin the sandbox before letting it run unmonitored. Some antivirus programs also\nput up a complete shield rather than just scanning ﬁles within a ﬁle system.\nThey search boot sectors, memory, inbound and outbound e-mail, ﬁles as they\nare downloaded, ﬁles on removable devices or media, and so on.\nThe best protection against computer viruses is prevention, or the practice\nof safe computing. Purchasing unopened software from vendors and avoiding\nfree or pirated copies from public sources or disk exchange offer the safest",
  "route to preventing infection. However, even new copies of legitimate software\napplications are not immune to virus infection: in a few cases, disgruntled\nemployees of a software company have infected the master copies of software\nprograms to do economic harm to the company. For macro viruses, one defense\nis to exchange Microsoft Word documents in an alternative ﬁle format called\nrich text format (RTF). Unlike the native Word format, RTF does not include the\ncapability to attach macros.\nAnother defense is to avoid opening any e-mail attachments from unknown\nusers. Unfortunately, history has shown that e-mail vulnerabilities appear as\nfast as they are ﬁxed. For example, in 2000, the love bug virus became very\nwidespread by traveling in e-mail messages that pretended to be love notes",
  "sent by friends of the receivers. Once a receiver opened the attached Visual\nBasic script, the virus propagated by sending itself to the ﬁrst addresses in the\nreceiver’s e-mail contact list. Fortunately, except for clogging e-mail systems 15.6\nImplementing Security Defenses\n695\nTHE TRIPWIRE FILE SYSTEM\nAn example of an anomaly-detection tool is the Tripwire ﬁle system integrity-\nchecking tool for UNIX, developed at Purdue University. Tripwire operates on\nthe premise that many intrusions result in modiﬁcation of system directories\nand ﬁles. For example, an attacker might modify the system programs,\nperhaps inserting copies with Trojan horses, or might insert new programs\ninto directories commonly found in user-shell search paths. Or an intruder",
  "might remove system log ﬁles to cover his tracks. Tripwire is a tool to\nmonitor ﬁle systems for added, deleted, or changed ﬁles and to alert system\nadministrators to these modiﬁcations.\nThe operation of Tripwire is controlled by a conﬁguration ﬁle tw.config\nthat enumerates the directories and ﬁles to be monitored for changes,\ndeletions, or additions. Each entry in this conﬁguration ﬁle includes a\nselection mask to specify the ﬁle attributes (inode attributes) that will be\nmonitored for changes. For example, the selection mask might specify that a\nﬁle’s permissions be monitored but its access time be ignored. In addition, the\nselection mask can instruct that the ﬁle be monitored for changes. Monitoring\nthe hash of a ﬁle for changes is as good as monitoring the ﬁle itself, and storing",
  "hashes of ﬁles requires far less room than copying the ﬁles themselves.\nWhen run initially, Tripwire takes as input the tw.config ﬁle and\ncomputes a signature for each ﬁle or directory consisting of its monitored\nattributes (inode attributes and hash values). These signatures are stored in a\ndatabase. When run subsequently, Tripwire inputs both tw.config and the\npreviously stored database, recomputes the signature for each ﬁle or directory\nnamed in tw.config, and compares this signature with the signature (if any)\nin the previously computed database. Events reported to an administrator\ninclude any monitored ﬁle or directory whose signature differs from that in\nthe database (a changed ﬁle), any ﬁle or directory in a monitored directory",
  "for which a signature does not exist in the database (an added ﬁle), and any\nsignature in the database for which the corresponding ﬁle or directory no\nlonger exists (a deleted ﬁle).\nAlthough effective for a wide class of attacks, Tripwire does have limita-\ntions. Perhaps the most obvious is the need to protect the Tripwire program\nand its associated ﬁles, especially the database ﬁle, from unauthorized mod-\niﬁcation. For this reason, Tripwire and its associated ﬁles should be stored\non some tamper-proof medium, such as a write-protected disk or a secure\nserver where logins can be tightly controlled. Unfortunately, this makes it\nless convenient to update the database after authorized updates to monitored\ndirectories and ﬁles. A second limitation is that some security-relevant ﬁles",
  "—for example, system log ﬁles—are supposed to change over time, and\nTripwire does not provide a way to distinguish between an authorized and\nan unauthorized change. So, for example, an attack that modiﬁes (without\ndeleting) a system log that would normally change anyway would escape\nTripwire’s detection capabilities. The best Tripwire can do in this case is to\ndetect certain obvious inconsistencies (for example, a shrinking log ﬁle). Free\nand commercial versions of Tripwire are available from http://tripwire.org\nand http://tripwire.com. 696\nChapter 15\nSecurity\nand users’ inboxes, it was relatively harmless. It did, however, effectively\nnegate the defensive strategy of opening attachments only from people known\nto the receiver. A more effective defense method is to avoid opening any e-mail",
  "attachment that contains executable code. Some companies now enforce this\nas policy by removing all incoming attachments to e-mail messages.\nAnother safeguard, although it does not prevent infection, does permit\nearly detection. A user must begin by completely reformatting the hard disk,\nespecially the boot sector, which is often targeted for viral attack. Only secure\nsoftware is uploaded, and a signature of each program is taken via a secure\nmessage-digest computation. The resulting ﬁle name and associated message-\ndigest list must then be kept free from unauthorized access. Periodically, or\neach time a program is run, the operating system recomputes the signature and\ncompares it with the signature on the original list; any differences serve as a",
  "warning of possible infection. This technique can be combined with others. For\nexample, a high-overhead antivirus scan, such as a sandbox, can be used; and\nif a program passes the test, a signature can be created for it. If the signatures\nmatch the next time the program is run, it does not need to be virus-scanned\nagain.\n15.6.5\nAuditing, Accounting, and Logging\nAuditing, accounting, and logging can decrease system performance, but they\nare useful in several areas, including security. Logging can be general or\nspeciﬁc. All system-call executions can be logged for analysis of program\nbehavior (or misbehavior). More typically, suspicious events are logged.\nAuthentication failures and authorization failures can tell us quite a lot about\nbreak-in attempts.",
  "break-in attempts.\nAccounting is another potential tool in a security administrator’s kit. It\ncan be used to ﬁnd performance changes, which in turn can reveal security\nproblems. One of the early UNIX computer break-ins was detected by Cliff\nStoll when he was examining accounting logs and spotted an anomaly.\n15.7 Firewalling to Protect Systems and Networks\nWe turn next to the question of how a trusted computer can be connected\nsafely to an untrustworthy network. One solution is the use of a ﬁrewall to\nseparate trusted and untrusted systems. A ﬁrewall is a computer, appliance,\nor router that sits between the trusted and the untrusted. A network ﬁrewall\nlimits network access between the two security domains and monitors and",
  "logs all connections. It can also limit connections based on source or destination\naddress, source or destination port, or direction of the connection. For instance,\nweb servers use HTTP to communicate with web browsers. A ﬁrewall therefore\nmay allow only HTTP to pass from all hosts outside the ﬁrewall to the web\nserver within the ﬁrewall. The Morris Internet worm used the finger protocol\nto break into computers, so finger would not be allowed to pass, for example.\nIn fact, a network ﬁrewall can separate a network into multiple domains.\nA common implementation has the Internet as the untrusted domain; a\nsemitrusted and semisecure network, called the demilitarized zone (DMZ),\nas another domain; and a company’s computers as a third domain (Figure 15.7",
  "Firewalling to Protect Systems and Networks\n697\nInternet access from company’s\ncomputers\ncompany computers\nDMZ access from Internet\nfirewall\nDMZ\naccess between DMZ and\ncompany’s computers\nInternet\nFigure 15.10\nDomain separation via ﬁrewall.\n15.10). Connections are allowed from the Internet to the DMZ computers and\nfrom the company computers to the Internet but are not allowed from the\nInternet or DMZ computers to the company computers. Optionally, controlled\ncommunications may be allowed between the DMZ and one company computer\nor more. For instance, a web server on the DMZ may need to query a database\nserver on the corporate network. With a ﬁrewall, however, access is contained,\nand any DMZ systems that are broken into still are unable to access the company\ncomputers.",
  "computers.\nOf course, a ﬁrewall itself must be secure and attack-proof. Otherwise,\nits ability to secure connections can be compromised. Furthermore, ﬁrewalls\ndo not prevent attacks that tunnel, or travel within protocols or connections\nthat the ﬁrewall allows. A buffer-overﬂow attack to a web server will not be\nstopped by the ﬁrewall, for example, because the HTTP connection is allowed;\nit is the contents of the HTTP connection that house the attack. Likewise, denial-\nof-service attacks can affect ﬁrewalls as much as any other machines. Another\nvulnerability of ﬁrewalls is spooﬁng, in which an unauthorized host pretends\nto be an authorized host by meeting some authorization criterion. For example,\nif a ﬁrewall rule allows a connection from a host and identiﬁes that host by its",
  "IP address, then another host could send packets using that same address and\nbe allowed through the ﬁrewall.\nIn addition to the most common network ﬁrewalls, there are other, newer\nkinds of ﬁrewalls, each with its pros and cons. A personal ﬁrewall is a\nsoftware layer either included with the operating system or added as an\napplication. Rather than limiting communication between security domains, it\nlimits communication to (and possibly from) a given host. A user could add\na personal ﬁrewall to her PC so that a Trojan horse would be denied access to\nthe network to which the PC is connected, for example. An application proxy\nﬁrewall understands the protocols that applications speak across the network.\nFor example, SMTP is used for mail transfer. An application proxy accepts a",
  "connection just as an SMTP server would and then initiates a connection to\nthe original destination SMTP server. It can monitor the trafﬁc as it forwards\nthe message, watching for and disabling illegal commands, attempts to exploit 698\nChapter 15\nSecurity\nbugs, and so on. Some ﬁrewalls are designed for one speciﬁc protocol. An\nXML ﬁrewall, for example, has the speciﬁc purpose of analyzing XML trafﬁc\nand blocking disallowed or malformed XML. System-call ﬁrewalls sit between\napplications and the kernel, monitoring system-call execution. For example,\nin Solaris 10, the “least privilege” feature implements a list of more than ﬁfty\nsystem calls that processes may or may not be allowed to make. A process that\ndoes not need to spawn other processes can have that ability taken away, for\ninstance.",
  "instance.\n15.8 Computer-Security Classiﬁcations\nThe U.S. Department of Defense Trusted Computer System Evaluation Criteria\nspecify four security classiﬁcations in systems: A, B, C, and D. This speciﬁcation\nis widely used to determine the security of a facility and to model security\nsolutions, so we explore it here. The lowest-level classiﬁcation is division D, or\nminimal protection. Division D includes only one class and is used for systems\nthat have failed to meet the requirements of any of the other security classes.\nFor instance, MS-DOS and Windows 3.1 are in division D.\nDivision C, the next level of security, provides discretionary protection and\naccountability of users and their actions through the use of audit capabilities.",
  "Division C has two levels: C1 and C2. A C1-class system incorporates some\nform of controls that allow users to protect private information and to\nkeep other users from accidentally reading or destroying their data. A C1\nenvironment is one in which cooperating users access data at the same levels\nof sensitivity. Most versions of UNIX are C1 class.\nThe total of all protection systems within a computer system (hardware,\nsoftware, ﬁrmware) that correctly enforce a security policy is known as a\ntrusted computer base (TCB). The TCB of a C1 system controls access between\nusers and ﬁles by allowing the user to specify and control sharing of objects\nby named individuals or deﬁned groups. In addition, the TCB requires that the",
  "users identify themselves before they start any activities that the TCBis expected\nto mediate. This identiﬁcation is accomplished via a protected mechanism or\npassword. The TCB protects the authentication data so that they are inaccessible\nto unauthorized users.\nA C2-class system adds an individual-level access control to the require-\nments of a C1 system. For example, access rights of a ﬁle can be speciﬁed\nto the level of a single individual. In addition, the system administrator can\nselectively audit the actions of any one or more users based on individual\nidentity. The TCB also protects itself from modiﬁcation of its code or data\nstructures. In addition, no information produced by a prior user is available\nto another user who accesses a storage object that has been released back to",
  "the system. Some special, secure versions of UNIX have been certiﬁed at the C2\nlevel.\nDivision-B mandatory-protection systems have all the properties of a\nclass-C2 system. In addition, they attach a sensitivity label to each object\nin the system. The B1-class TCB maintains these labels, which are used for\ndecisions pertaining to mandatory access control. For example, a user at the\nconﬁdential level could not access a ﬁle at the more sensitive secret level.\nThe TCB also denotes the sensitivity level at the top and bottom of each 15.9\nAn Example: Windows 7\n699\npage of any human-readable output. In addition to the normal user-name–\npassword authentication information, the TCB also maintains the clearance\nand authorizations of individual users and will support at least two levels of",
  "security. These levels are hierarchical, so that a user may access any objects\nthat carry sensitivity labels equal to or lower than his security clearance. For\nexample, a secret-level user could access a ﬁle at the conﬁdential level in the\nabsence of other access controls. Processes are also isolated through the use of\ndistinct address spaces.\nA B2-class system extends the sensitivity labels to each system resource,\nsuch as storage objects. Physical devices are assigned minimum and maximum\nsecurity levels that the system uses to enforce constraints imposed by the\nphysical environments in which the devices are located. In addition, a B2\nsystem supports covert channels and the auditing of events that could lead to\nthe exploitation of a covert channel.",
  "the exploitation of a covert channel.\nA B3-class system allows the creation of access-control lists that denote\nusers or groups not granted access to a given named object. The TCB also\ncontains a mechanism to monitor events that may indicate a violation of\nsecurity policy. The mechanism notiﬁes the security administrator and, if\nnecessary, terminates the event in the least disruptive manner.\nThe highest-level classiﬁcation is division A. Architecturally, a class-A1\nsystem is functionally equivalent to a B3 system, but it uses formal design\nspeciﬁcations and veriﬁcation techniques, granting a high degree of assurance\nthat the TCB has been implemented correctly. A system beyond class A1 might\nbe designed and developed in a trusted facility by trusted personnel.",
  "The use of a TCB merely ensures that the system can enforce aspects of a\nsecurity policy; the TCB does not specify what the policy should be. Typically,\na given computing environment develops a security policy for certiﬁcation\nand has the plan accredited by a security agency, such as the National\nComputer Security Center. Certain computing environments may require other\ncertiﬁcation, such as that supplied by TEMPEST, which guards against electronic\neavesdropping. For example, a TEMPEST-certiﬁed system has terminals that\nare shielded to prevent electromagnetic ﬁelds from escaping. This shielding\nensures that equipment outside the room or building where the terminal is\nhoused cannot detect what information is being displayed by the terminal.\n15.9 An Example: Windows 7",
  "15.9 An Example: Windows 7\nMicrosoft Windows 7 is a general-purpose operating system designed to\nsupport a variety of security features and methods. In this section, we\nexamine features that Windows 7 uses to perform security functions. For more\ninformation and background on Windows 7, see Chapter 19.\nThe Windows 7 security model is based on the notion of user accounts.\nWindows 7 allows the creation of any number of user accounts, which can\nbe grouped in any manner. Access to system objects can then be permitted or\ndenied as desired. Users are identiﬁed to the system by a unique security ID.\nWhen a user logs on, Windows 7 creates a security access token that includes\nthe security ID for the user, security IDs for any groups of which the user is",
  "a member, and a list of any special privileges that the user has. Examples\nof special privileges include backing up ﬁles and directories, shutting down 700\nChapter 15\nSecurity\nthe computer, logging on interactively, and changing the system clock. Every\nprocess that Windows 7 runs on behalf of a user will receive a copy of the\naccess token. The system uses the security IDs in the access token to permit or\ndeny access to system objects whenever the user, or a process on behalf of the\nuser, attempts to access the object. Authentication of a user account is typically\naccomplished via a user name and password, although the modular design of\nWindows 7 allows the development of custom authentication packages. For\nexample, a retinal (or eye) scanner might be used to verify that the user is who",
  "she says she is.\nWindows 7 uses the idea of a subject to ensure that programs run by a user\ndo not get greater access to the system than the user is authorized to have.\nA subject is used to track and manage permissions for each program that a\nuser runs. It is composed of the user’s access token and the program acting\non behalf of the user. Since Windows 7 operates with a client–server model,\ntwo classes of subjects are used to control access: simple subjects and server\nsubjects. An example of a simple subject is the typical application program\nthat a user executes after she logs on. The simple subject is assigned a security\ncontext based on the security access token of the user. A server subject is a\nprocess implemented as a protected server that uses the security context of the",
  "client when acting on the client’s behalf.\nAs mentioned in Section 15.7, auditing is a useful security technique.\nWindows 7 has built-in auditing that allows many common security threats to\nbe monitored. Examples include failure auditing for login and logoff events\nto detect random password break-ins, success auditing for login and logoff\nevents to detect login activity at strange hours, success and failure write-access\nauditing for executable ﬁles to track a virus outbreak, and success and failure\nauditing for ﬁle access to detect access to sensitive ﬁles.\nWindows added mandatory integrity control, which works by assigning an\nintegrity label to each securable object and subject. In order for a given subject",
  "to have access to an object, it must have the access requested in the discretionary\naccess-control list, and its integrity label must be equal to or higher than that\nof the secured object (for the given operation). The integrity labels in Windows\n7 are (in ascending order): untrusted, low, medium, high, and system. In\naddition, three access mask bits are permitted for integrity labels: NoReadUp,\nNoWriteUp, and NoExecuteUp. NoWriteUp is automatically enforced, so a\nlower-integrity subject cannot perform a write operation on a higher-integrity\nobject. However, unless explictly blocked by the security descriptor, it can\nperform read or execute operations.\nFor securable objects without an explicit integrity label, a default label",
  "of medium is assigned. The label for a given subject is assigned during\nlogon. For instance, a nonadministrative user will have an integrity label\nof medium. In addition to integrity labels, Windows Vista also added User\nAccount Control (UAC), which represents an administrative account (not the\nbuilt-in Administrators account) with two separate tokens. One, for normal\nusage, has the built-in Administrators group disabled and has an integrity\nlabel of medium. The other, for elevated usage, has the built-in Administrators\ngroup enabled and an integrity label of high.\nSecurity attributes of an object in Windows 7 are described by a security\ndescriptor. The security descriptor contains the security ID of the owner of",
  "the object (who can change the access permissions), a group security ID used 15.10\nSummary\n701\nonly by the POSIX subsystem, a discretionary access-control list that identiﬁes\nwhich users or groups are allowed (and which are explicitly denied) access, and\na system access-control list that controls which auditing messages the system\nwill generate. Optionally, the system access-control list can set the integrity of\nthe object and identify which operations to block from lower-integrity subjects:\nread, write (always enforced), or execute. For example, the security descriptor\nof the ﬁle foo.bar might have owner avi and this discretionary access-control\nlist:\n• avi—all access\n• group cs—read–write access\n• user cliff—no access",
  "• user cliff—no access\nIn addition, it might have a system access-control list that tells the system to\naudit writes by everyone, along with an integrity label of medium that denies\nread, write, and execute to lower-integrity subjects.\nAn access-control list is composed of access-control entries that contain\nthe security ID of the individual and an access mask that deﬁnes all possible\nactions on the object, with a value of AccessAllowed or AccessDenied for\neach action. Files in Windows 7 may have the following access types: Read-\nData, WriteData, AppendData, Execute, ReadExtendedAttribute, Write-\nExtendedAttribute, ReadAttributes, and WriteAttributes. We can see\nhow this allows a ﬁne degree of control over access to objects.",
  "Windows 7 classiﬁes objects as either container objects or noncontainer\nobjects. Container objects, such as directories, can logically contain other\nobjects. By default, when an object is created within a container object, the new\nobject inherits permissions from the parent object. Similarly, if the user copies a\nﬁle from one directory to a new directory, the ﬁle will inherit the permissions of\nthe destination directory. Noncontainer objects inherit no other permissions.\nFurthermore, if a permission is changed on a directory, the new permissions\ndo not automatically apply to existing ﬁles and subdirectories; the user may\nexplicitly apply them if he so desires.\nThe system administrator can prohibit printing to a printer on the system",
  "for all or part of a day and can use the Windows 7 Performance Monitor to\nhelp her spot approaching problems. In general, Windows 7 does a good job of\nproviding features to help ensure a secure computing environment. Many of\nthese features are not enabled by default, however, which may be one reason\nfor the myriad security breaches on Windows 7 systems. Another reason is the\nvast number of services Windows 7 starts at system boot time and the number\nof applications that typically are installed on a Windows 7 system. For a real\nmultiuser environment, the system administrator should formulate a security\nplan and implement it, using the features that Windows 7 provides and other\nsecurity tools.\n15.10 Summary\nProtection is an internal problem. Security, in contrast, must consider both",
  "the computer system and the environment—people, buildings, businesses,\nvaluable objects, and threats—within which the system is used. 702\nChapter 15\nSecurity\nThe data stored in the computer system must be protected from unautho-\nrized access, malicious destruction or alteration, and accidental introduction of\ninconsistency. It is easier to protect against accidental loss of data consistency\nthan to protect against malicious access to the data. Absolute protection of the\ninformation stored in a computer system from malicious abuse is not possible;\nbut the cost to the perpetrator can be made sufﬁciently high to deter most, if\nnot all, attempts to access that information without proper authority.\nSeveral types of attacks can be launched against programs and against",
  "individual computers or the masses. Stack- and buffer-overﬂow techniques\nallow successful attackers to change their level of system access. Viruses and\nworms are self-perpetuating, sometimes infecting thousands of computers.\nDenial-of-service attacks prevent legitimate use of target systems.\nEncryption limits the domain of receivers of data, while authentication\nlimits the domain of senders. Encryption is used to provide conﬁdentiality\nof data being stored or transferred. Symmetric encryption requires a shared\nkey, while asymmetric encryption provides a public key and a private key.\nAuthentication, when combined with hashing, can prove that data have not\nbeen changed.\nUser authentication methods are used to identify legitimate users of a",
  "system. In addition to standard user-name and password protection, several\nauthentication methods are used. One-time passwords, for example, change\nfrom session to session to avoid replay attacks. Two-factor authentication\nrequires two forms of authentication, such as a hardware calculator with an\nactivation PIN. Multifactor authentication uses three or more forms. These\nmethods greatly decrease the chance of authentication forgery.\nMethods of preventing or detecting security incidents include intrusion-\ndetection systems, antivirus software, auditing and logging of system events,\nmonitoring of system software changes, system-call monitoring, and ﬁrewalls.\nExercises\n15.1\nBuffer-overﬂow attacks can be avoided by adopting a better program-",
  "ming methodology or by using special hardware support. Discuss these\nsolutions.\n15.2\nA password may become known to other users in a variety of ways. Is\nthere a simple method for detecting that such an event has occurred?\nExplain your answer.\n15.3\nWhat is the purpose of using a “salt” along with the user-provided\npassword? Where should the “salt” be stored, and how should it be\nused?\n15.4\nThe list of all passwords is kept within the operating system. Thus,\nif a user manages to read this list, password protection is no longer\nprovided. Suggest a scheme that will avoid this problem. (Hint: Use\ndifferent internal and external representations.)\n15.5\nAn experimental addition to UNIX allows a user to connect a watchdog",
  "program to a ﬁle. The watchdog is invoked whenever a program Bibliographical Notes\n703\nrequests access to the ﬁle. The watchdog then either grants or denies\naccess to the ﬁle. Discuss two pros and two cons of using watchdogs\nfor security.\n15.6\nThe UNIX program COPS scans a given system for possible security\nholes and alerts the user to possible problems. What are two potential\nhazards of using such a system for security? How can these problems\nbe limited or eliminated?\n15.7\nDiscuss a means by which managers of systems connected to the\nInternet could design their systems to limit or eliminate the damage\ndone by worms. What are the drawbacks of making the change that\nyou suggest?\n15.8\nArgue for or against the judicial sentence handed down against Robert",
  "Morris, Jr., for his creation and execution of the Internet worm discussed\nin Section 15.3.1.\n15.9\nMake a list of six security concerns for a bank’s computer system. For\neach item on your list, state whether this concern relates to physical,\nhuman, or operating-system security.\n15.10\nWhat are two advantages of encrypting data stored in the computer\nsystem?\n15.11\nWhat commonly used computer programs are prone to man-in-the-\nmiddle attacks? Discuss solutions for preventing this form of attack.\n15.12\nCompare symmetric and asymmetric encryption schemes, and discuss\nthe circumstances under which a distributed system would use one or\nthe other.\n15.13\nWhy doesn’t Dkd,N(Eke,N(m)) provide authentication of the sender? To\nwhat uses can such an encryption be put?\n15.14",
  "what uses can such an encryption be put?\n15.14\nDiscuss how the asymmetric encryption algorithm can be used to\nachieve the following goals.\na.\nAuthentication: the receiver knows that only the sender could\nhave generated the message.\nb.\nSecrecy: only the receiver can decrypt the message.\nc.\nAuthentication and secrecy: only the receiver can decrypt the\nmessage, and the receiver knows that only the sender could have\ngenerated the message.\n15.15\nConsider a system that generates 10 million audit records per day.\nAssume that, on average, there are 10 attacks per day on this system\nand each attack is reﬂected in 20 records. If the intrusion-detection\nsystem has a true-alarm rate of 0.6 and a false-alarm rate of 0.0005,\nwhat percentage of alarms generated by the system correspond to real",
  "intrusions? 704\nChapter 15\nSecurity\nBibliographical Notes\nGeneral discussions concerning security are given by [Denning (1982)],\n[Pﬂeeger and Pﬂeeger (2006)] and [Tanenbaum (2010)]. Computer networking\nis discussed in [Kurose and Ross (2013)].\nIssues concerning the design and veriﬁcation of secure systems are dis-\ncussed by [Rushby (1981)] and by [Silverman (1983)]. A security kernel for a\nmultiprocessor microcomputer is described by [Schell (1983)]. A distributed\nsecure system is described by [Rushby and Randell (1983)].\n[Morris and Thompson (1979)] discuss password security. [Morshedian\n(1986)] presents methods to ﬁght password pirates. Password authentication\nwith insecure communications is considered by [Lamport (1981)]. The issue",
  "of password cracking is examined by [Seely (1989)]. Computer break-ins are\ndiscussed by [Lehmann (1987)] and by [Reid (1987)]. Issues related to trusting\ncomputer programs are discussed in [Thompson (1984)].\nDiscussions concerning UNIX security are offered by [Grampp and Morris\n(1984)], [Wood and Kochan (1985)], [Farrow (1986)], [Filipski and Hanko\n(1986)], [Hecht et al. (1988)], [Kramer (1988)], and [Garﬁnkel et al. (2003)].\n[Bershad and Pinkerton (1988)] present the watchdog extension to BSD UNIX.\n[Spafford (1989)] presents a detailed technical discussion of the Internet\nworm. The Spafford article appears with three others in a special section on\nthe Morris Internet worm in Communications of the ACM (Volume 32, Number\n6, June 1989).",
  "6, June 1989).\nSecurity problems associated with the TCP/IPprotocol suite are described in\n[Bellovin (1989)]. The mechanisms commonly used to prevent such attacks are\ndiscussed in [Cheswick et al. (2003)]. Another approach to protecting networks\nfrom insider attacks is to secure topology or route discovery. [Kent et al.\n(2000)], [Hu et al. (2002)], [Zapata and Asokan (2002)], and [Hu and Perrig\n(2004)] present solutions for secure routing. [Savage et al. (2000)] examine\nthe distributed denial-of-service attack and propose IP trace-back solutions to\naddress the problem. [Perlman (1988)] proposes an approach to diagnose faults\nwhen the network contains malicious routers.\nInformation\nabout\nviruses\nand\nworms\ncan\nbe\nfound\nat\nhttp://www.securelist.com, as well as in [Ludwig (1998)] and [Ludwig",
  "(2002)].\nAnother\nwebsite\ncontaining\nup-to-date\nsecurity\ninforma-\ntion\nis\nhttp://www.eeye.com/resources/security-center/research.\nA\npaper on the dangers of a computer monoculture can be found at\nhttp://cryptome.org/cyberinsecurity.htm.\n[Difﬁe and Hellman (1976)] and [Difﬁe and Hellman (1979)] were the\nﬁrst researchers to propose the use of the public-key encryption scheme. The\nalgorithm presented in Section 15.4.1 is based on the public-key encryption\nscheme; it was developed by [Rivest et al. (1978)]. [C. Kaufman (2002)]\nand [Stallings (2011)] explore the use of cryptography in computer systems.\nDiscussions concerning protection of digital signatures are offered by [Akl\n(1983)], [Davies (1983)], [Denning (1983)], and [Denning (1984)]. Complete",
  "cryptography information is presented in [Schneier (1996)] and [Katz and\nLindell (2008)].\nThe RSA algorithm is presented in [Rivest et al. (1978)]. Information about\nNIST’s AES activities can be found at http://www.nist.gov/aes; information\nabout other cryptographic standards for the United States can also be found Bibliography\n705\nat that site. In 1999, SSL 3.0 was modiﬁed slightly and presented in an IETF\nRequest for Comments (RFC) under the name TLS.\nThe example in Section 15.6.3 illustrating the impact of false-alarm rate\non the effectiveness of IDSs is based on [Axelsson (1999)]. The description of\nTripwire in Section 15.6.5 is based on [Kim and Spafford (1993)]. Research into\nsystem-call-based anomaly detection is described in [Forrest et al. (1996)].",
  "The U.S. government is, of course, concerned about security. The Depart-\nment of Defense Trusted Computer System Evaluation Criteria ([DoD\n(1985)]), known also as the Orange Book, describes a set of security levels and\nthe features that an operating system must have to qualify for each security\nrating. Reading it is a good starting point for understanding security concerns.\nThe Microsoft Windows NT Workstation Resource Kit ([Microsoft (1996)])\ndescribes the security model of NT and how to use that model.\nBibliography\n[Akl (1983)]\nS. G. Akl, “Digital Signatures: A Tutorial Survey”, Computer, Volume\n16, Number 2 (1983), pages 15–24.\n[Axelsson (1999)]\nS. Axelsson, “The Base-Rate Fallacy and Its Implications\nfor Intrusion Detection”, Proceedings of the ACM Conference on Computer and",
  "Communications Security (1999), pages 1–7.\n[Bellovin (1989)]\nS. M. Bellovin, “Security Problems in the TCP/IP Protocol\nSuite”, Computer Communications Review, Volume 19:2, (1989), pages 32–48.\n[Bershad and Pinkerton (1988)]\nB. N. Bershad and C. B. Pinkerton, “Watchdogs:\nExtending the Unix File System”, Proceedings of the Winter USENIX Conference\n(1988).\n[C. Kaufman (2002)]\nM. S. C. Kaufman, R. Perlman, Network Security: Private\nCommunication in a Public World, Second Edition, Prentice Hall (2002).\n[Cheswick et al. (2003)]\nW. Cheswick, S. Bellovin, and A. Rubin, Firewalls and\nInternet Security: Repelling the Wily Hacker, Second Edition, Addison-Wesley\n(2003).\n[Davies (1983)]\nD. W. Davies, “Applying the RSA Digital Signature to Electronic",
  "Mail”, Computer, Volume 16, Number 2 (1983), pages 55–62.\n[Denning (1982)]\nD. E. Denning, Cryptography and Data Security, Addison-\nWesley (1982).\n[Denning (1983)]\nD. E. Denning, “Protecting Public Keys and Signature Keys”,\nComputer, Volume 16, Number 2 (1983), pages 27–35.\n[Denning (1984)]\nD. E. Denning, “Digital Signatures with RSA and Other\nPublic-Key Cryptosystems”, Communications of the ACM, Volume 27, Number 4\n(1984), pages 388–392.\n[Difﬁe and Hellman (1976)]\nW. Difﬁe and M. E. Hellman, “New Directions in\nCryptography”, IEEE Transactions on Information Theory, Volume 22, Number 6\n(1976), pages 644–654. 706\nChapter 15\nSecurity\n[Difﬁe and Hellman (1979)]\nW. Difﬁe and M. E. Hellman, “Privacy and Authen-\ntication”, Proceedings of the IEEE (1979), pages 397–427.\n[DoD (1985)]",
  "[DoD (1985)]\nTrusted Computer System Evaluation Criteria.\nDepartment of\nDefense (1985).\n[Farrow (1986)]\nR. Farrow, “Security Issues and Strategies for Users”, UNIX\nWorld (April 1986), pages 65–71.\n[Filipski and Hanko (1986)]\nA. Filipski and J. Hanko, “Making UNIX Secure”,\nByte (April 1986), pages 113–128.\n[Forrest et al. (1996)]\nS. Forrest, S. A. Hofmeyr, and T. A. Longstaff, “A Sense\nof Self for UNIX Processes”, Proceedings of the IEEE Symposium on Security and\nPrivacy (1996), pages 120–128.\n[Garﬁnkel et al. (2003)]\nS. Garﬁnkel, G. Spafford, and A. Schwartz, Practical\nUNIX & Internet Security, O’Reilly & Associates (2003).\n[Grampp and Morris (1984)]\nF. T. Grampp and R. H. Morris, “UNIX Oper-\nating-System Security”, AT&T Bell Laboratories Technical Journal, Volume 63,",
  "Number 8 (1984), pages 1649–1672.\n[Hecht et al. (1988)]\nM. S. Hecht, A. Johri, R. Aditham, and T. J. Wei, “Experience\nAdding C2 Security Features to UNIX”, Proceedings of the Summer USENIX\nConference (1988), pages 133–146.\n[Hu and Perrig (2004)]\nY.-C. Hu and A. Perrig, “SPV: A Secure Path Vector\nRouting Scheme for Securing BGP”, Proceedings of ACM SIGCOMM Conference\non Data Communication (2004).\n[Hu et al. (2002)]\nY.-C. Hu, A. Perrig, and D. Johnson, “Ariadne: A Secure\nOn-Demand Routing Protocol for Ad Hoc Networks”, Proceedings of the Annual\nInternational Conference on Mobile Computing and Networking (2002).\n[Katz and Lindell (2008)]\nJ. Katz and Y. Lindell, Introduction to Modern Cryptog-\nraphy, Chapman & Hall/CRC Press (2008).\n[Kent et al. (2000)]",
  "[Kent et al. (2000)]\nS. Kent, C. Lynn, and K. Seo, “Secure Border Gateway\nProtocol (Secure-BGP)”, IEEE Journal on Selected Areas in Communications, Volume\n18, Number 4 (2000), pages 582–592.\n[Kim and Spafford (1993)]\nG. H. Kim and E. H. Spafford, “The Design and\nImplementation of Tripwire: A File System Integrity Checker”, Technical report,\nPurdue University (1993).\n[Kramer (1988)]\nS. M. Kramer, “Retaining SUID Programs in a Secure UNIX”,\nProceedings of the Summer USENIX Conference (1988), pages 107–118.\n[Kurose and Ross (2013)]\nJ. Kurose and K. Ross, Computer Networking—A Top–\nDown Approach, Sixth Edition, Addison-Wesley (2013).\n[Lamport (1981)]\nL. Lamport, “Password Authentication with Insecure Com-\nmunications”, Communications of the ACM, Volume 24, Number 11 (1981), pages\n770–772.",
  "770–772.\n[Lehmann (1987)]\nF. Lehmann, “Computer Break-Ins”, Communications of the\nACM, Volume 30, Number 7 (1987), pages 584–585. Bibliography\n707\n[Ludwig (1998)]\nM. Ludwig, The Giant Black Book of Computer Viruses, Second\nEdition, American Eagle Publications (1998).\n[Ludwig (2002)]\nM. Ludwig, The Little Black Book of Email Viruses, American\nEagle Publications (2002).\n[Microsoft (1996)]\nMicrosoft Windows NT Workstation Resource Kit.\nMicrosoft\nPress (1996).\n[Morris and Thompson (1979)]\nR. Morris and K. Thompson, “Password Secu-\nrity: A Case History”, Communications of the ACM, Volume 22, Number 11 (1979),\npages 594–597.\n[Morshedian (1986)]\nD. Morshedian, “How to Fight Password Pirates”, Com-\nputer, Volume 19, Number 1 (1986).\n[Perlman (1988)]",
  "[Perlman (1988)]\nR. Perlman, Network Layer Protocols with Byzantine Robustness.\nPhD thesis, Massachusetts Institute of Technology (1988).\n[Pﬂeeger and Pﬂeeger (2006)]\nC. Pﬂeeger and S. Pﬂeeger, Security in Computing,\nFourth Edition, Prentice Hall (2006).\n[Reid (1987)]\nB. Reid, “Reﬂections on Some Recent Widespread Computer\nBreak-Ins”, Communications of the ACM, Volume 30, Number 2 (1987), pages\n103–105.\n[Rivest et al. (1978)]\nR. L. Rivest, A. Shamir, and L. Adleman, “On Digital\nSignatures and Public Key Cryptosystems”, Communications of the ACM, Volume\n21, Number 2 (1978), pages 120–126.\n[Rushby (1981)]\nJ. M. Rushby, “Design and Veriﬁcation of Secure Systems”,\nProceedings of the ACM Symposium on Operating Systems Principles (1981), pages\n12–21.\n[Rushby and Randell (1983)]",
  "12–21.\n[Rushby and Randell (1983)]\nJ. Rushby and B. Randell, “A Distributed Secure\nSystem”, Computer, Volume 16, Number 7 (1983), pages 55–67.\n[Savage et al. (2000)]\nS. Savage, D. Wetherall, A. R. Karlin, and T. Anderson,\n“Practical Network Support for IP Traceback”, Proceedings of ACM SIGCOMM\nConference on Data Communication (2000), pages 295–306.\n[Schell (1983)]\nR. R. Schell, “A Security Kernel for a Multiprocessor Microcom-\nputer”, Computer (1983), pages 47–53.\n[Schneier (1996)]\nB. Schneier, Applied Cryptography, Second Edition, John Wiley\nand Sons (1996).\n[Seely (1989)]\nD. Seely, “Password Cracking: A Game of Wits”, Communications\nof the ACM, Volume 32, Number 6 (1989), pages 700–704.\n[Silverman (1983)]\nJ. M. Silverman, “Reﬂections on the Veriﬁcation of the",
  "Security of an Operating System Kernel”, Proceedings of the ACM Symposium\non Operating Systems Principles (1983), pages 143–154.\n[Spafford (1989)]\nE. H. Spafford, “The Internet Worm: Crisis and Aftermath”,\nCommunications of the ACM, Volume 32, Number 6 (1989), pages 678–687. 708\nChapter 15\nSecurity\n[Stallings (2011)]\nW. Stallings, Operating Systems, Seventh Edition, Prentice Hall\n(2011).\n[Tanenbaum (2010)]\nA. S. Tanenbaum, Computer Networks, Fifth Edition, Pren-\ntice Hall (2010).\n[Thompson (1984)]\nK. Thompson, “Reﬂections on Trusting Trust”, Communica-\ntions of ACM, Volume 27, Number 8 (1984), pages 761–763.\n[Wood and Kochan (1985)]\nP. Wood and S. Kochan, UNIX System Security,\nHayden (1985).\n[Zapata and Asokan (2002)]\nM. Zapata and N. Asokan, “Securing Ad Hoc",
  "M. Zapata and N. Asokan, “Securing Ad Hoc\nRouting Protocols”, Proc. 2002 ACM Workshop on Wireless Security (2002), pages\n1–10. Part Six\nAdvanced Topics\nVirtualization permeates all aspects of computing. Virtual machines are\none instance of this trend. Generally, with a virtual machine, guest oper-\nating systems and applications run in an environment that appears to\nthem to be native hardware. This environment behaves toward them as\nnative hardware would but also protects, manages, and limits them.\nA distributed system is a collection of processors that do not share\nmemory or a clock. Instead, each processor has its own local memory,\nand the processors communicate with one another through communica-\ntion lines such as local-area or wide-area networks. Distributed systems",
  "offer several beneﬁts: they give users access to more of the resources\nmaintained by the system, speed computation, and improve data avail-\nability and reliability.  16\nC H A P T E R\nVirtual Machines\nThe term virtualization has many meanings, and aspects of virtualization\npermeate all aspects of computing. Virtual machines are one instance of\nthis trend. Generally, with a virtual machine, guest operating systems and\napplications run in an environment that appears to them to be native hardware\nand that behaves toward them as native hardware would but that also protects,\nmanages, and limits them.\nThis chapter delves into the uses, features, and implementation of virtual\nmachines. Virtual machines can be implemented in several ways, and this",
  "chapter describes these options. One option is to add virtual machine support\nto the kernel. Because that implementation method is the most pertinent to this\nbook, we explore it most fully. Additionally, hardware features provided by\nthe CPU and even by I/O devices can support virtual machine implementation,\nso we discuss how those features are used by the appropriate kernel modules.\nCHAPTER OBJECTIVES\n• To explore the history and beneﬁts of virtual machines.\n• To discuss the various virtual machine technologies.\n• To describe the methods used to implement virtualization.\n• To show the most common hardware features that support virtualization\nand explain how they are used by operating-system modules.\n16.1\nOverview\nThe fundamental idea behind a virtual machine is to abstract the hardware",
  "of a single computer (the CPU, memory, disk drives, network interface cards,\nand so forth) into several different execution environments, thereby creating\nthe illusion that each separate environment is running on its own private\ncomputer. This concept may seem similar to the layered approach of operating\nsystem implementation (see Section 2.7.2), and in some ways it is. In the case of\nvirtualization, there is a layer that creates a virtual system on which operating\nsystems or applications can run.\n711 712\nChapter 16\nVirtual Machines\nVirtual machine implementations involve several components. At the base\nis the host, the underlying hardware system that runs the virtual machines.\nThe virtual machine manager (VMM) (also known as a hypervisor) creates and",
  "runs virtual machines by providing an interface that is identical to the host\n(except in the case of paravirtualization, discussed later). Each guest process\nis provided with a virtual copy of the host (Figure 16.1). Usually, the guest\nprocess is in fact an operating system. A single physical machine can thus run\nmultiple operating systems concurrently, each in its own virtual machine.\nTake a moment to note that with virtualization, the deﬁnition of “operating\nsystem” once again blurs. For example, consider VMM software such as VMware\nESX. This virtualization software is installed on the hardware, runs when the\nhardware boots, and provides services to applications. The services include\ntraditional ones, such as scheduling and memory management, along with",
  "new types, such as migration of applications between systems. Furthermore,\nthe applications are in fact guest operating systems. Is the VMware ESX VMM\nan operating system that, in turn, runs other operating systems? Certainly it\nacts like an operating system. For clarity, however, we call the component that\nprovides virtual environments a VMM.\nThe implementation of VMMs varies greatly. Options include the following:\n• Hardware-based solutions that provide support for virtual machine cre-\nation and management via ﬁrmware. These VMMs, which are commonly\nfound in mainframe and large to midsized servers, are generally known\nas type 0 hypervisors. IBM LPARs and Oracle LDOMs are examples.\n• Operating-system-like software built to provide virtualization, including",
  "VMware ESX(mentioned above), Joyent SmartOS, and Citrix XenServer.\nThese VMMs are known as type 1 hypervisors.\n(a)\nprocesses\nhardware\nkernel\n(b)\nprogramming\ninterface\nprocesses\nprocesses\nprocesses\nkernel\nkernel\nkernel\nVM2\nVM1\nVM3\nmanager\nhardware\nvirtual machine\nFigure 16.1\nSystem models. (a) Nonvirtual machine. (b) Virtual machine. 16.2\nHistory\n713\nINDIRECTION\n“All problems in computer science can be solved by another level of\nindirection”—David Wheeler “. . . except for the problem of too many layers\nof indirection.”—Kevlin Henney\n• General-purpose operating systems that provide standard functions as\nwell as VMM functions, including Microsoft Windows Server with HyperV\nand RedHat Linux with the KVM feature. Because such systems have a",
  "feature set similar to type 1 hypervisors, they are also known as type 1.\n• Applications that run on standard operating systems but provide VMM\nfeatures to guest operating systems. These applications, which include\nVMware Workstation and Fusion, Parallels Desktop, and Oracle Virtual-\nBox, are type 2 hypervisors.\n• Paravirtualization, a technique in which the guest operating system is\nmodiﬁed to work in cooperation with the VMM to optimize performance.\n• Programming-environment virtualization, in which VMMs do not virtu-\nalize real hardware but instead create an optimized virtual system. This\ntechnique is used by Oracle Java and Microsoft.Net.\n• Emulators that allow applications written for one hardware environment\nto run on a very different hardware environment, such as a different type",
  "of CPU.\n• Application containment, which is not virtualization at all but rather\nprovides virtualization-like features by segregating applications from the\noperating system. Oracle Solaris Zones, BSD Jails, and IBM AIX WPARs\n“contain” applications, making them more secure and manageable.\nThe variety of virtualization techniques in use today is a testament to\nthe breadth, depth, and importance of virtualization in modern computing.\nVirtualization is invaluable for data-center operations, efﬁcient application\ndevelopment, and software testing, among many other uses.\n16.2 History\nVirtual machines ﬁrst appeared commercially on IBM mainframes in 1972.\nVirtualization was provided by the IBM VM operating system. This system has",
  "evolved and is still available. In addition, many of its original concepts are\nfound in other systems, making it worth exploring.\nIBM VM370 divided a mainframe into multiple virtual machines, each\nrunning its own operating system. A major difﬁculty with the VM approach\ninvolved disk systems. Suppose that the physical machine had three disk drives\nbut wanted to support seven virtual machines. Clearly, it could not allocate a\ndisk drive to each virtual machine. The solution was to provide virtual disks—\ntermed minidisks in IBM’s VM operating system. The minidisks are identical 714\nChapter 16\nVirtual Machines\nto the system’s hard disks in all respects except size. The system implemented\neach minidisk by allocating as many tracks on the physical disks as the minidisk\nneeded.",
  "needed.\nOnce the virtual machines were created, users could run any of the\noperating systems or software packages that were available on the underlying\nmachine. For the IBM VM system, a user normally ran CMS—a single-user\ninteractive operating system.\nFor many years after IBM introduced this technology, virtualization\nremained in its domain. Most systems could not support virtualization.\nHowever, a formal deﬁnition of virtualization helped to establish system\nrequirements and a target for functionality. The virtualization requirements\nstated that:\n1. A VMM provides an environment for programs that is essentially identical\nto the original machine.\n2. Programs running within that environment show only minor perfor-\nmance decreases.\n3. The VMM is in complete control of system resources.",
  "These requirements of ﬁdelity, performance, and safety still guide virtualiza-\ntion efforts today.\nBy the late 1990s, Intel 80x86 CPUs had become common, fast, and rich\nin features. Accordingly, developers launched multiple efforts to implement\nvirtualization on that platform. Both Xen and VMware created technologies,\nstill used today, to allow guest operating systems to run on the 80x86. Since\nthat time, virtualization has expanded to include all common CPUs, many\ncommercial and open-source tools, and many operating systems. For example,\nthe open-source VirtualBox project (http://www.virtualbox.org) provides a\nprogram than runs on Intel x86 and AMD64 CPUs and on Windows, Linux,\nMac OS X, and Solaris host operating systems. Possible guest operating systems",
  "include many versions of Windows, Linux, Solaris, and BSD, including even\nMS-DOS and IBM OS/2.\n16.3 Beneﬁts and Features\nSeveral advantages make virtualization attractive. Most of them are fundamen-\ntally related to the ability to share the same hardware yet run several different\nexecution environments (that is, different operating systems) concurrently.\nOne important advantage of virtualization is that the host system is\nprotected from the virtual machines, just as the virtual machines are protected\nfrom each other. A virus inside a guest operating system might damage that\noperating system but is unlikely to affect the host or the other guests. Because\neach virtual machine is almost completely isolated from all other virtual\nmachines, there are almost no protection problems.",
  "A potential disadvantage of isolation is that it can prevent sharing of\nresources. Two approaches to provide sharing have been implemented. First,\nit is possible to share a ﬁle-system volume and thus to share ﬁles. Second,\nit is possible to deﬁne a network of virtual machines, each of which can 16.3\nBeneﬁts and Features\n715\nsend information over the virtual communications network. The network\nis modeled after physical communication networks but is implemented in\nsoftware. Of course, the VMM is free to allow any number of its guests to\nuse physical resources, such as a physical network connection (with sharing\nprovided by the VMM), in which case the allowed guests could communicate\nwith each other via the physical network.",
  "with each other via the physical network.\nOne feature common to most virtualization implementations is the ability\nto freeze, or suspend, a running virtual machine. Many operating systems\nprovide that basic feature for processes, but VMMs go one step further and\nallow copies and snapshots to be made of the guest. The copy can be used to\ncreate a new VM or to move a VM from one machine to another with its current\nstate intact. The guest can then resume where it was, as if on its original\nmachine, creating a clone. The snapshot records a point in time, and the guest\ncan be reset to that point if necessary (for example, if a change was made\nbut is no longer wanted). Often, VMMs allow many snapshots to be taken. For\nexample, snapshots might record a guest’s state every day for a month, making",
  "restoration to any of those snapshot states possible. These abilities are used to\ngood advantage in virtual environments.\nA virtual machine system is a perfect vehicle for operating-system research\nand development. Normally, changing an operating system is a difﬁcult task.\nOperating systems are large and complex programs, and a change in one\npart may cause obscure bugs to appear in some other part. The power of\nthe operating system makes changing it particularly dangerous. Because the\noperating system executes in kernel mode, a wrong change in a pointer could\ncause an error that would destroy the entire ﬁle system. Thus, it is necessary\nto test all changes to the operating system carefully.\nFurthermore, the operating system runs on and controls the entire machine,",
  "meaning that the system must be stopped and taken out of use while changes\nare made and tested. This period is commonly called system-development\ntime. Since it makes the system unavailable to users, system-development\ntime on shared systems is often scheduled late at night or on weekends, when\nsystem load is low.\nA virtual-machine system can eliminate much of this latter problem.\nSystemprogrammersare giventheirownvirtual machine, and systemdevelop-\nment is done on the virtual machine instead of on a physical machine. Normal\nsystem operation is disrupted only when a completed and tested change is\nready to be put into production.\nAnother advantage of virtual machines for developers is that multiple\noperating systems can run concurrently on the developer’s workstation. This",
  "virtualized workstation allows for rapid porting and testing of programs in\nvarying environments. In addition, multiple versions of a program can run,\neach in its own isolated operating system, within one system. Similarly, quality-\nassurance engineers can test their applications in multiple environments\nwithout buying, powering, and maintaining a computer for each environment.\nA major advantage of virtual machines in production data-center use is\nsystem consolidation, which involves taking two or more separate systems\nand running them in virtual machines on one system. Such physical-to-virtual\nconversions result in resource optimization, since many lightly used systems\ncan be combined to create one more heavily used system. 716\nChapter 16\nVirtual Machines",
  "Chapter 16\nVirtual Machines\nConsider, too, that management tools that are part of the VMM allow system\nadministrators to manage many more systems than they otherwise could.\nA virtual environment might include 100 physical servers, each running 20\nvirtual servers. Without virtualization, 2,000 servers would require several\nsystem administrators. With virtualization and its tools, the same work can be\nmanaged by one or two administrators. One of the tools that make this possible\nis templating, in which one standard virtual machine image, including an\ninstalled and conﬁgured guest operating system and applications, is saved and\nused as a source for multiple running VMs. Other features include managing\nthe patching of all guests, backing up and restoring the guests, and monitoring",
  "their resource use.\nVirtualization can improve not only resource utilization but also resource\nmanagement. Some VMMs include a live migration feature that moves a\nrunning guest from one physical server to another without interrupting\nits operation or active network connections. If a server is overloaded, live\nmigration can thus free resources on the source host while not disrupting the\nguest. Similarly, when host hardware must be repaired or upgraded, guests\ncan be migrated to other servers, the evacuated host can be maintained, and\nthen the guests can be migrated back. This operation occurs without downtime\nand without interruption to users.\nThink about the possible effects of virtualization on how applications are\ndeployed. If a system can easily add, remove, and move a virtual machine,",
  "then why install applications on that system directly? Instead, the application\ncould be preinstalled on a tuned and customized operating system in a virtual\nmachine. This method would offer several beneﬁts for application developers.\nApplication management would become easier, less tuning would be required,\nand technical support of the application would be more straightforward.\nSystem administrators would ﬁnd the environment easier to manage as well.\nInstallation would be simple, and redeploying the application to another\nsystem would be much easier than the usual steps of uninstalling and\nreinstalling. For widespread adoption of this methodology to occur, though, the\nformat of virtual machines must be standardized so that any virtual machine",
  "will run on any virtualization platform. The “Open Virtual Machine Format” is\nan attempt to provide such standardization, and it could succeed in unifying\nvirtual machine formats.\nVirtualization has laid the foundation for many other advances in computer\nfacility implementation, management, and monitoring. Cloud computing,\nfor example, is made possible by virtualization in which resources such as\nCPU, memory, and I/O are provided as services to customers using Internet\ntechnologies. By using APIs, a program can tell a cloud computing facility to\ncreate thousands of VMs, all running a speciﬁc guest operating system and\napplication, which others can access via the Internet. Many multiuser games,\nphoto-sharing sites, and other web services use this functionality.",
  "In the area of desktop computing, virtualization is enabling desktop and\nlaptop computer users to connect remotely to virtual machines located in\nremote data centers and access their applications as if they were local. This\npractice can increase security, because no data are stored on local disks at the\nuser’s site. The cost of the user’s computing resource may also decrease. The\nuser must have networking, CPU, and some memory, but all that these system\ncomponents need to do is display an image of the guest as its runs remotely (via 16.4\nBuilding Blocks\n717\na protocol such as RDP). Thus, they need not be expensive, high-performance\ncomponents. Other uses of virtualization are sure to follow as it becomes more\nprevalent and hardware support continues to improve.\n16.4 Building Blocks",
  "16.4 Building Blocks\nAlthough the virtual machine concept is useful, it is difﬁcult to implement.\nMuch work is required to provide an exact duplicate of the underlying\nmachine. This is especially a challenge on dual-mode systems, where the\nunderlying machine has only user mode and kernel mode. In this section,\nwe examine the building blocks that are needed for efﬁcient virtualization.\nNote that these building blocks are not required by type 0 hypervisors, as\ndiscussed in Section 16.5.2.\nThe ability to virtualize depends on the features provided by the CPU. If\nthe features are sufﬁcient, then it is possible to write a VMM that provides\na guest environment. Otherwise, virtualization is impossible. VMMs use\nseveral techniques to implement virtualization, including trap-and-emulate",
  "and binary translation. We discuss each of these techniques in this section,\nalong with the hardware support needed to support virtualization.\nOne important concept found in most virtualization options is the imple-\nmentation of a virtual CPU (VCPU). The VCPU does not execute code. Rather,\nit represents the state of the CPU as the guest machine believes it to be. For\neach guest, the VMM maintains a VCPU representing that guest’s current CPU\nstate. When the guest is context-switched onto a CPU by the VMM, information\nfrom the VCPU is used to load the right context, much as a general-purpose\noperating system would use the PCB.\n16.4.1\nTrap-and-Emulate\nOn a typical dual-mode system, the virtual machine guest can execute only in",
  "user mode (unless extra hardware support is provided). The kernel, of course,\nruns in kernel mode, and it is not safe to allow user-level code to run in kernel\nmode. Just as the physical machine has two modes, however, so must the virtual\nmachine. Consequently, we must have a virtual user mode and a virtual kernel\nmode, both of which run in physical user mode. Those actions that cause a\ntransfer from user mode to kernel mode on a real machine (such as a system\ncall, an interrupt, or an attempt to execute a privileged instruction) must also\ncause a transfer from virtual user mode to virtual kernel mode in the virtual\nmachine.\nHow can such a transfer be accomplished? The procedure is as follows:\nWhen the kernel in the guest attempts to execute a privileged instruction, that",
  "is an error (because the system is in user mode) and causes a trap to the VMM\nin the real machine. The VMM gains control and executes (or “emulates”) the\naction that was attempted by the guest kernel on the part of the guest. It then\nreturns control to the virtual machine. This is called the trap-and-emulate\nmethod and is shown in Figure 16.2. Most virtualization products use this\nmethod to one extent or other.\nWith privileged instructions, time becomes an issue. All nonprivileged\ninstructions run natively on the hardware, providing the same performance 718\nChapter 16\nVirtual Machines\nPrivileged Instruction\nOperating\nSystem\nVCPU\nVMM\nVMM\nGuest\nKernel Mode\nUser Mode\nEmulate Action\nReturn\nTrap\nUpdate\nUser Processes\nFigure 16.2\nTrap-and-emulate virtualization implementation.",
  "Trap-and-emulate virtualization implementation.\nfor guests as native applications. Privileged instructions create extra overhead,\nhowever, causing the guest to run more slowly than it would natively. In\naddition, the CPU is being multiprogrammed among many virtual machines,\nwhich can further slow down the virtual machines in unpredictable ways.\nThis problem has been approached in various ways. IBM VM, for example,\nallows normal instructions for the virtual machines to execute directly on\nthe hardware. Only the privileged instructions (needed mainly for I/O) must\nbe emulated and hence execute more slowly. In general, with the evolution\nof hardware, the performance of trap-and-emulate functionality has been\nimproved, and cases in which it is needed have been reduced. For example,",
  "many CPUs now have extra modes added to their standard dual-mode\noperation. The VCPU need not keep track of what mode the guest operating\nsystem is in, because the physical CPU performs that function. In fact, some\nCPUs provide guest CPU state management in hardware, so the VMM need not\nsupply that functionality, removing the extra overhead.\n16.4.2\nBinary Translation\nSome CPUs do not have a clean separation of privileged and nonprivileged\ninstructions. Unfortunately for virtualization implementers, the Intel x86 CPU\nline is one of them. No thought was given to running virtualization on the\nx86 when it was designed. (In fact, the ﬁrst CPU in the family—the Intel\n4004, released in 1971—was designed to be the core of a calculator.) The chip",
  "has maintained backward compatibility throughout its lifetime, preventing\nchanges that would have made virtualization easier through many generations.\nLet’s consider an example of the problem. The command popf loads the ﬂag\nregister from the contents of the stack. If the CPU is in privileged mode, all\nof the ﬂags are replaced from the stack. If the CPU is in user mode, then only\nsome ﬂags are replaced, and others are ignored. Because no trap is generated\nif popf is executed in user mode, the trap-and-emulate procedure is rendered 16.4\nBuilding Blocks\n719\nuseless. Other x86 instructions cause similar problems. For the purposes of this\ndiscussion, we will call this set of instructions special instructions. As recently",
  "as 1998, /Judi 1998 doesnt seem that recent using the trap-and-emulate method\nto implement virtualization on the x86 was considered impossible because of\nthese special instructions.\nThis previously insurmountable problem was solved with the implemen-\ntation of the binary translation technique. Binary translation is fairly simple\nin concept but complex in implementation. The basic steps are as follows:\n1. If the guest VCPU is in user mode, the guest can run its instructions\nnatively on a physical CPU.\n2. If the guest VCPU is in kernel mode, then the guest believes that it is\nrunning in kernel mode. The VMM examines every instruction the guest\nexecutes in virtual kernel mode by reading the next few instructions that\nthe guest is going to execute, based on the guest’s program counter.",
  "Instructions other than special instructions are run natively. Special\ninstructions are translated into a new set of instructions that perform\nthe equivalent task—for example changing the ﬂags in the VCPU.\nBinary translation is shown in Figure 16.3. It is implemented by translation\ncode within the VMM. The code reads native binary instructions dynamically\nfrom the guest, on demand, and generates native binary code that executes in\nplace of the original code.\nThe basic method of binary translation just described would execute\ncorrectly but perform poorly. Fortunately, the vast majority of instructions\nwould execute natively. But how could performance be improved for the other\ninstructions? We can turn to a speciﬁc implementation of binary translation,",
  "the VMware method, to see one way of improving performance. Here, caching\nUser Processes\nSpecial Instruction\n(VMM Reads Instructions)\nOperating\nSystem\nVCPU\nVMM\nVMM\nGuest\nKernel Mode\nUser Mode\nTranslate\nExecute Translation\nReturn\nUpdate\nFigure 16.3\nBinary translation virtualization implementation. 720\nChapter 16\nVirtual Machines\nprovides the solution. The replacement code for each instruction that needs to\nbe translated is cached. All later executions of that instruction run from the\ntranslation cache and need not be translated again. If the cache is large enough,\nthis method can greatly improve performance.\nLet’s consider another issue in virtualization: memory management, specif-\nically the page tables. How can the VMM keep page-table state both for guests",
  "that believe they are managing the page tables and for the VMM itself? A\ncommon method, used with both trap-and-emulate and binary translation, is\nto use nested page tables (NPTs). Each guest operating system maintains one\nor more page tables to translate from virtual to physical memory. The VMM\nmaintains NPTs to represent the guest’s page-table state, just as it creates a\nVCPU to represent the guest’s CPU state. The VMM knows when the guest tries\nto change its page table, and it makes the equivalent change in the NPT. When\nthe guest is on the CPU, the VMM puts the pointer to the appropriate NPT into\nthe appropriate CPU register to make that table the active page table. If the\nguest needs to modify the page table (for example, fulﬁlling a page fault), then",
  "that operation must be intercepted by the VMM and appropriate changes made\nto the nested and system page tables. Unfortunately, the use of NPTs can cause\nTLB misses to increase, and many other complexities need to be addressed to\nachieve reasonable performance.\nAlthough it might seem that the binary translation method creates large\namounts of overhead, it performed well enough to launch a new industry\naimed at virtualizing Intel x86-based systems. VMware tested the performance\nimpact of binary translation by booting one such system, Windows XP, and\nimmediately shutting it down while monitoring the elapsed time and the\nnumber of translations produced by the binary translation method. The result\nwas\n950,000 translations, taking 3 microseconds each, for a total increase",
  "of 3 seconds (about 5%) over native execution of Windows XP. To achieve\nthat result, developers used many performance improvements that we do not\ndiscuss here. For more information, consult the bibliographical notes at the\nend of this chapter.\n16.4.3\nHardware Assistance\nWithout some level of hardware support, virtualization would be impossible.\nThe more hardware support available within a system, the more feature-rich\nand stable the virtual machines can be and the better they can perform. In\nthe Intel x86 CPU family, Intel added new virtualization support in successive\ngenerations (the VT-x instructions) beginning in 2005. Now, binary translation\nis no longer needed.\nIn fact, all major general-purpose CPUs are providing extended amounts",
  "of hardware support for virtualization. For example,AMD virtualization tech-\nnology (AMD-V) has appeared in several AMD processors starting in 2006. It\ndeﬁnes two new modes of operation—host and guest—thus moving from a\ndual-mode to a multimode processor. The VMM can enable host mode, deﬁne\nthe characteristics of each guest virtual machine, and then switch the system\nto guest mode, passing control of the system to a guest operating system that\nis running in the virtual machine. In guest mode, the virtualized operating\nsystem thinks it is running on native hardware and sees whatever devices\nare included in the host’s deﬁnition of the guest. If the guest tries to access a 16.5\nTypes of Virtual Machines and Their Implementations\n721",
  "721\nvirtualized resource, then control is passed to the VMM to manage that inter-\naction. The functionality in Intel VT-x is similar, providing root and nonroot\nmodes, equivalent to host and guest modes. Both provide guest VCPU state\ndata structures to load and save guest CPU state automatically during guest\ncontext switches. In addition, virtual machine control structures (VMCSs) are\nprovided to manage guest and host state, as well as the various guest execution\ncontrols, exit controls, and information about why guests exit back to the host.\nIn the latter case, for example, a nested page-table violation caused by an\nattempt to access unavailable memory can result in the guest’s exit.\nAMD and Intel have also addressed memory management in the virtual",
  "environment. With AMD’s RVI and Intel’s EPT memory management enhance-\nments, VMMs no longer need to implement software NPTs. In essence, these\nCPUs implement nested page tables in hardware to allow the VMM to fully\ncontrol paging while the CPUs accelerate the translation from virtual to physical\naddresses. The NPTs add a new layer, one representing the guest’s view of\nlogical-to-physical address translation. The CPU page-table walking function\nincludes this new layer as necessary, walking through the guest table to the\nVMM table to ﬁnd the physical address desired. A TLB miss results in a per-\nformance penalty, because more tables must be traversed (the guest and host\npage tables) to complete the lookup. Figure 16.4 shows the extra translation",
  "work performed by the hardware to translate from a guest virtual address to a\nﬁnal physical address.\nI/O is another area improved by hardware assistance. Consider that the\nstandard direct-memory-access (DMA) controller accepts a target memory\naddress and a source I/O device and transfers data between the two without\noperating-system action. Without hardware assistance, a guest might try to set\nup a DMA transfer that affects the memory of the VMM or other guests. In CPUs\nthat provide hardware-assisted DMA (such as Intel CPUs with VT-d), even DMA\nhas a level of indirection. First, the VMM sets up protection domains to tell\nthe CPU which physical memory belongs to each guest. Next, it assigns the\nI/O devices to the protection domains, allowing them direct access to those",
  "memory regions and only those regions. The hardware then transforms the\naddress in a DMA request issued by an I/O device to the host physical memory\naddress associated with the I/O. In this manner DMA transfers are passed\nthrough between a guest and a device without VMM interference.\nSimilarly, interrupts must be delivered to the appropriate guest and\nmust not be visible to other guests. By providing an interrupt remapping\nfeature, CPUs with virtualization hardware assistance automatically deliver an\ninterrupt destined for a guest to a core that is currently running a thread of that\nguest. That way, the guest receives interrupts without the VMM’s needing to\nintercede in their delivery. Without interrupt remapping, malicious guests can",
  "generate interrupts that can be used to gain control of the host system. (See the\nbibliographical notes at the end of this chapter for more details.)\n16.5 Types of Virtual Machines and Their Implementations\nWe’ve now looked at some of the techniques used to implement virtualization.\nNext, we consider the major types of virtual machines, their implementation,\ntheir functionality, and how they use the building blocks just described to 722\nChapter 16\nVirtual Machines\nVMM Nested Page Table Data Structure\nPML4E\nPDPTE\nPDE\nPTE\nPhy Addr\nHost Physical Address\nOffset\nTable\nDirectory\nDirectory Ptr\nPML4\nGuest Virtual Address\nKernel Paging Data\nStructures\nGuest Physical Address\nGuest\n1\n2\n3\n4\n5\n1\n1\n2\n2\n3\n3\n4\n5\n4\nFigure 16.4\nNested page tables.",
  "1\n1\n2\n2\n3\n3\n4\n5\n4\nFigure 16.4\nNested page tables.\ncreate a virtual environment. Of course, the hardware on which the virtual\nmachines are running can cause great variation in implementation methods.\nHere, we discuss the implementations in general, with the understanding that\nVMMs take advantage of hardware assistance where it is available.\n16.5.1\nThe Virtual Machine Life Cycle\nLet’s begin with the virtual machine life cycle. Whatever the hypervisor type,\nat the time a virtual machine is created, its creator gives the VMM certain\nparameters. These parameters usually include the number of CPUs, amount of\nmemory, networking details, and storage details that the VMM will take into\naccount when creating the guest. For example, a user might want to create a",
  "new guest with two virtual CPUs, 4 GB of memory, 10 GB of disk space, one\nnetwork interface that gets its IP address via DHCP, and access to the DVD drive.\nThe VMM then creates the virtual machine with those parameters. In the\ncase of a type 0 hypervisor, the resources are usually dedicated. In this situation,\nif there are not two virtual CPUs available and unallocated, the creation request 16.5\nTypes of Virtual Machines and Their Implementations\n723\nin our example will fail. For other hypervisor types, the resources are dedicated\nor virtualized, depending on the type. Certainly, an IPaddress cannot be shared,\nbut the virtual CPUs are usually multiplexed on the physical CPUs as discussed\nin Section 16.6.1. Similarly, memory management usually involves allocating",
  "more memory to guests than actually exists in physical memory. This is more\ncomplicated and is described in Section 16.6.2.\nFinally, when the virtual machine is no longer needed, it can be deleted.\nWhen this happens, the VMM ﬁrst frees up any used disk space and then\nremoves the conﬁguration associated with the virtual machine, essentially\nforgetting the virtual machine.\nThese steps are quite simple compared with building, conﬁguring, running,\nand removing physical machines. Creating a virtual machine from an existing\none can be as easy as clicking the “clone” button and providing a new name\nand IP address. This ease of creation can lead to virtual machine sprawl, which\noccurs when there are so many virtual machines on a system that their use,",
  "history, and state become confusing and difﬁcult to track.\n16.5.2\nType 0 Hypervisor\nType 0 hypervisors have existed for many years under many names, including\n“partitions” and “domains”. They are a hardware feature, and that brings its\nown positives and negatives. Operating systems need do nothing special to\ntake advantage of their features. The VMM itself is encoded in the ﬁrmware\nand loaded at boot time. In turn, it loads the guest images to run in each\npartition. The feature set of a type 0 hypervisor tends to be smaller than those\nof the other types because it is implemented in hardware. For example, a system\nmight be split into four virtual systems, each with dedicated CPUs, memory,\nand I/O devices. Each guest believes that it has dedicated hardware because it",
  "does, simplifying many implementation details.\nI/O presents some difﬁculty, because it is not easy to dedicate I/O devices\nto guests if there are not enough. What if a system has two Ethernet ports and\nmore than two guests, for example? Either all guests must get their own I/O\ndevices, or the system must provided I/O device sharing. In these cases, the\nhypervisor manages shared access or grants all devices to a control partition.\nIn the control partition, a guest operating system provides services (such\nas networking) via daemons to other guests, and the hypervisor routes I/O\nrequests appropriately. Some type 0 hypervisors are even more sophisticated\nand can move physical CPUs and memory between running guests. In these",
  "cases, the guests are paravirtualized, aware of the virtualization and assisting\nin its execution. For example, a guest must watch for signals from the hardware\nor VMM that a hardware change has occurred, probe its hardware devices to\ndetect the change, and add or subtract CPUs or memory from its available\nresources.\nBecause type 0 virtualization is very close to raw hardware execution,\nit should be considered separately from the other methods discussed here.\nA type 0 hypervisor can run multiple guest operating systems (one in each\nhardware partition). All of those guests, because they are running on raw\nhardware, can in turn be VMMs. Essentially, the guest operating systems in\na type 0 hypervisor are native operating systems with a subset of hardware",
  "made available to them. Because of that, each can have its own guest operating 724\nChapter 16\nVirtual Machines\nGuest 1\nGuest 2\nCPUs\nmemory\nCPUs\nmemory\nHypervisor (in firmware)\nI/O\nCPUs\nmemory\nCPUs\nmemory\nGuest 3\nGuest 4\nGuest Guest Guest\nGuest\nGuest\nFigure 16.5\nType 0 hypervisor.\nsystems (Figure 16.5). Other types of hypervisors usually cannot provide this\nvirtualization-within-virtualization functionality.\n16.5.3\nType 1 Hypervisor\nType 1 hypervisors are commonly found in company data centers and are in a\nsense becoming “the data-center operating system.” They are special-purpose\noperating systems that run natively on the hardware, but rather than providing\nsystem calls and other interfaces for running programs, they create, run, and",
  "manage guest operating systems. In addition to running on standard hardware,\nthey can run on type 0 hypervisors, but not on other type 1 hypervisors.\nWhatever the platform, guests generally do not know they are running on\nanything but the native hardware.\nType 1 hypervisors run in kernel mode, taking advantage of hardware\nprotection. Where the host CPU allows, they use multiple modes to give guest\noperating systems their own control and improved performance. They imple-\nment device drivers for the hardware they run on, because no other component\ncould do so. Because they are operating systems, they must also provide\nCPU scheduling, memory management, I/O management, protection, and even\nsecurity. Frequently, they provide APIs, but those APIs support applications in",
  "guests or external applications that supply features like backups, monitoring,\nand security. Many type 1 hypervisors are closed-source commercial offerings,\nsuch as VMware ESX while some are open source or hybrids of open and closed\nsource, such as Citrix XenServer and its open Xen counterpart.\nBy using type 1 hypervisors, data-center managers can control and manage\nthe operating systems and applications in new and sophisticated ways. An\nimportant beneﬁt is the ability to consolidate more operating systems and\napplications onto fewer systems. For example, rather than having ten systems\nrunning at 10 percent utilization each, a data center might have one server\nmanage the entire load. If utilization increases, guests and their applications can",
  "be moved to less-loaded systems live, without interruption of service. Using\nsnapshots and cloning, the system can save the states of guests and duplicate\nthose states—a much easier task than restoring from backups or installing\nmanually or via scripts and tools. The price of this increased manageability 16.5\nTypes of Virtual Machines and Their Implementations\n725\nis the cost of the VMM (if it is a commercial product), the need to learn new\nmanagement tools and methods, and the increased complexity.\nAnother type of type 1 hypervisor includes various general-purpose\noperating systems with VMMfunctionality. In this instance, an operating system\nsuch as RedHat Enterprise Linux, Windows, or Oracle Solaris performs its\nnormal duties as well as providing a VMM allowing other operating systems",
  "to run as guests. Because of their extra duties, these hypervisors typically\nprovide fewer virtualization features than other type 1 hypervisors. In many\nways, they treat a guest operating system as just another process, albeit with\nspecial handling provided when the guest tries to execute special instructions.\n16.5.4\nType 2 Hypervisor\nType 2 hypervisors are less interesting to us as operating-system explorers,\nbecause there is very little operating-system involvement in these application-\nlevel virtual machine managers. This type of VMM is simply another process\nrun and managed by the host, and even the host does not know virtualization\nis happening within the VMM.\nType 2 hypervisors have limits not associated with some of the other types.",
  "For example, a user needs administrative privileges to access many of the\nhardware assistance features of modern CPUs. If the VMM is being run by a\nstandard user without additional privileges, the VMM cannot take advantage\nof these features. Due to this limitation, as well as the extra overhead of running\na general-purpose operating system as well as guest operating systems, type 2\nhypervisors tend to have poorer overall performance than type 0 or 1.\nAs is often the case, the limitations of type 2 hypervisors also provide\nsome beneﬁts. They run on a variety of general-purpose operating systems,\nand running them requires no changes to the host operating system. A student\ncan use a type 2 hypervisor, for example, to test a non-native operating system",
  "without replacing the native operating system. In fact, on an Apple laptop,\na student could have versions of Windows, Linux, Unix, and less common\noperating systems all available for learning and experimentation.\n16.5.5\nParavirtualization\nAs we’ve seen, paravirtualization takes a different tack than the other types of\nvirtualization. Rather than try to trick a guest operating system into believing\nit has a system to itself, paravirtualization presents the guest with a system\nthat is similar but not identical to the guest’s preferred system. The guest must\nbe modiﬁed to run on the paravirtualized virtual hardware. The gain for this\nextra work is more efﬁcient use of resources and a smaller virtualization layer.\nThe Xen VMM, which is the leader in paravirtualization, has implemented",
  "several techniques to optimize the performance of guests as well as of the host\nsystem. For example, as we have seen, some VMMs present virtual devices to\nguests that appear to be real devices. Instead of taking that approach, the Xen\nVMM presents clean and simple device abstractions that allow efﬁcient I/O, as\nwell as good communication between the guest and the VMM about device\nI/O. For each device used by each guest, there is a circular buffer shared by the\nguest and the VMM via shared memory. Read and write data are placed in this\nbuffer, as shown in Figure 16.6. 726\nChapter 16\nVirtual Machines\nRequest Producer\nShared pointer\nupdated by guest OS\nRequest Consumer\nPrivate pointer\nin Xen\nResponse Producer\nShared pointer\nupdated by\nXen\nResponse Consumer\nPrivate pointer\nin guest OS",
  "Xen\nResponse Consumer\nPrivate pointer\nin guest OS\nRequest queue - Descriptors queued by the VM but not yet accepted by Xen\nOutstanding descriptors - Descriptor slots awaiting a response from Xen\nResponse queue - Descriptors returned by Xen in response to serviced requests\nUnused descriptors\nFigure 16.6\nXen I/O via shared circular buffer.\nFor memory management, Xen does not implement nested page tables.\nRather, each guest has its own set of page tables, set to read-only. Xen requires\nthe guest to use a speciﬁc mechanism, a hypercall from the guest to the\nhypervisor VMM, when a page-table change is needed. This means that the\nguest operating system’s kernel code must be changed from the default code\nto these Xen-speciﬁc methods. To optimize performance, Xen allows the guest",
  "to queue up multiple page-table changes asynchronously via hypercalls and\nthen check to ensure that the changes are complete before continuing operation.\nXen allowed virtualization of x86 CPUs without the use of binary transla-\ntion, instead requiring modiﬁcations in the guest operating systems like the\none described above. Over time, Xen has taken advantage of hardware features\nsupporting virtualization. As a result, it no longer requires modiﬁed guests and\nessentially does not need the paravirtualization method. Paravirtualization is\nstill used in other solutions, however, such as type 0 hypervisors.\n16.5.6\nProgramming-Environment Virtualization\nAnother kind of virtualization, based on a different execution model, is the",
  "virtualization of programming environments. Here, a programming language\nis designed to run within a custom-built virtualized environment. For example,\nOracle’s Java has many features that depend on its running in the Java\nvirtual machine (JVM), including speciﬁc methods for security and memory\nmanagement.\nIf we deﬁne virtualization as including only duplication of hardware, this is\nnot really virtualization at all. But we need not limit ourselves to that deﬁnition.\nInstead, we can deﬁne a virtual environment, based on APIs, that provides\na set of features that we want to have available for a particular language\nand programs written in that language. Java programs run within the JVM 16.5\nTypes of Virtual Machines and Their Implementations\n727",
  "727\nenvironment, and the JVM is compiled to be a native program on systems on\nwhich it runs. This arrangement means that Java programs are written once\nand then can run on any system (including all of the major operating systems)\non which a JVM is available. The same can be said for interpreted languages,\nwhich run inside programs that read each instruction and interpret it into\nnative operations.\n16.5.7\nEmulation\nVirtualization is probably the most common method for running applications\ndesigned for one operating system on a different operating system, but on the\nsame CPU. This method works relatively efﬁciently because the applications\nwere compiled for the same instruction set as the target system uses.\nBut what if an application or operating system needs to run on a different",
  "CPU? Here, it is necessary to translate all of the source CPU’s instructions so\nthat they are turned into the equivalent instructions of the target CPU. Such an\nenvironment is no longer virtualized but rather is fully emulated.\nEmulation is useful when the host system has one system architecture\nand the guest system was compiled for a different architecture. For example,\nsuppose a company has replaced its outdated computer system with a new\nsystem but would like to continue to run certain important programs that were\ncompiled for the old system. The programs could be run in an emulator that\ntranslates each of the outdated system’s instructions into the native instruction\nset of the new system. Emulation can increase the life of programs and allow",
  "us to explore old architectures without having an actual old machine.\nAs may be expected, the major challenge of emulation is performance.\nInstruction-set emulation can run an order of magnitude slower than native\ninstructions, because it may take ten instructions on the new system to read,\nparse, and simulate an instruction from the old system. Thus, unless the new\nmachine is ten times faster than the old, the program running on the new\nmachine will run more slowly than it did on its native hardware. Another\nchallenge for emulator writers is that it is difﬁcult to create a correct emulator\nbecause, in essence, this task involves writing an entire CPU in software.\nIn spite of these challenges, emulation is very popular, particularly in",
  "gaming circles. Many popular video games were written for platforms that are\nno longer in production. Users who want to run those games frequently can\nﬁnd an emulator of such a platform and then run the game unmodiﬁed within\nthe emulator. Modern systems are so much faster than old game consoles that\neven the Apple iPhone has game emulators and games available to run within\nthem.\n16.5.8\nApplication Containment\nThe goal of virtualization in some instances is to provide a method to segregate\napplications, manage their performance and resource use, and create an easy\nway to start, stop, move, and manage them. In such cases, perhaps full-ﬂedged\nvirtualization is not needed. If the applications are all compiled for the same",
  "operating system, then we do not need complete virtualization to provide these\nfeatures. We can instead use application containment. 728\nChapter 16\nVirtual Machines\nvirtual platform\ndevice management\nzone 1\nglobal zone\nSolaris kernel\nnetwork addresses\nzone 2\nzone management\nuser programs\nsystem programs\nCPU resources\nmemory resources\nuser programs\nsystem programs\nnetwork addresses\ndevice access\nCPU resources\nmemory resources\nuser programs\nsystem programs\nnetwork addresses\ndevice access\nCPU resources\nmemory resources\ndevice\ndevice\n…\nFigure 16.7\nSolaris 10 with two zones.\nConsider one example of application containment. Starting with version\n10, Oracle Solaris has included containers, or zones, that create a virtual layer",
  "between the operating system and the applications. In this system, only one\nkernel is installed, and the hardware is not virtualized. Rather, the operating\nsystem and its devices are virtualized, providing processes within a zone with\nthe impression that they are the only processes on the system. One or more\ncontainers can be created, and each can have its own applications, network\nstacks, network address and ports, user accounts, and so on. CPU and memory\nresources can be divided among the zones and the system-wide processes.\nEach zone in fact can run its own scheduler to optimize the performance of its\napplications on the allotted resources. Figure 16.7 shows a Solaris 10 system\nwith two containers and the standard “global” user space.\n16.6 Virtualization and Operating-System Components",
  "Thus far, we have explored the building blocks of virtualization and the various\ntypes of virtualization. In this section, we take a deeper dive into the operating-\nsystem aspects of virtualization, including how the VMM provides core\noperating-system functions like scheduling, I/O, and memory management.\nHere, we answer questions such as these: How do VMMs schedule CPU use\nwhen guest operating systems believe they have dedicated CPUs? How can\nmemory management work when many guests require large amounts of\nmemory? 16.6\nVirtualization and Operating-System Components\n729\n16.6.1\nCPU Scheduling\nA system with virtualization, even a single-CPU system, frequently acts like\na multiprocessor system. The virtualization software presents one or more",
  "virtual CPUs to each of the virtual machines running on the system and then\nschedules the use of the physical CPUs among the virtual machines.\nThe signiﬁcant variations among virtualization technologies make it difﬁ-\ncult to summarize the effect of virtualization on scheduling. First, let’s consider\nthe general case of VMM scheduling. The VMM has a number of physical CPUs\navailable and a number of threads to run on those CPUs. The threads can be\nVMM threads or guest threads. Guests are conﬁgured with a certain number of\nvirtual CPUs at creation time, and that number can be adjusted throughout the\nlife of the VM. When there are enough CPUs to allocate the requested number to\neach guest, the VMM can treat the CPUs as dedicated and schedule only a given",
  "guest’s threads on that guest’s CPUs. In this situation, the guests act much like\nnative operating systems running on native CPUs.\nOf course, in other situations, there may not be enough CPUs to go\naround. The VMM itself needs some CPU cycles for guest management and I/O\nmanagement and can steal cycles from the guests by scheduling its threads\nacross all of the system CPUs, but the impact of this action is relatively\nminor. More difﬁcult is the case of overcommitment, in which the guests\nare conﬁgured for more CPUs than exist in the system. Here, a VMM can\nuse standard scheduling algorithms to make progress on each thread but\ncan also add a fairness aspect to those algorithms. For example, if there are\nsix hardware CPUs and 12 guest-allocated CPUs, the VMM could allocate CPU",
  "resources proportionally, giving each guest half of the CPU resources it believes\nit has. The VMM can still present all 12 virtual CPUs to the guests, but in\nmapping them onto physical CPUs, the VMM can use its scheduler to share\nthem appropriately.\nEven given a scheduler that provides fairness, any guest operating-system\nscheduling algorithm that assumes a certain amount of progress in a given\namount of time will be negatively affected by virtualization. Consider a time-\nsharing operating system that tries to allot 100 milliseconds to each time slice to\ngive users a reasonable response time. Within a virtual machine, this operating\nsystem is at the mercy of the virtualization system as to what CPU resources it\nactually receives. A given 100-millisecond time slice may take much more than",
  "100 milliseconds of virtual CPU time. Depending on how busy the system is,\nthe time slice may take a second or more, resulting in very poor response times\nfor users logged into that virtual machine. The effect on a real-time operating\nsystem can be even more serious.\nThe net effect of such scheduling layering is that individual virtualized\noperating systems receive only a portion of the available CPU cycles, even\nthough they believe they are receiving all of the cycles and indeed that they\nare scheduling all of those cycles. Commonly, the time-of-day clocks in virtual\nmachines are incorrect because timers take longer to trigger than they would on\ndedicated CPUs. Virtualization can thus undo the good scheduling-algorithm\nefforts of the operating systems within virtual machines.",
  "To correct for this, a VMM will have an application available for each type\nof operating system that system administrators install into the guests. This 730\nChapter 16\nVirtual Machines\napplication corrects clock drift and can have other functions such as virtual\ndevice management.\n16.6.2\nMemory Management\nEfﬁcient memory use in general-purpose operating systems is one of the major\nkeys to performance. In virtualized environments, there are more users of\nmemory (the guests and their applications, as well as the VMM), leading to\nmore pressure on memory use. Further adding to this pressure is that VMMs\ntypically overcommit memory, so that the total memory with which guests are\nconﬁgured exceeds the amount of memory that physically exists in the system.",
  "The extra need for efﬁcient memory use is not lost on the implementers of\nVMMs, who take great measures to ensure the optimal use of memory.\nFor example, VMware ESX uses at least three methods of memory manage-\nment. Before memory optimization can occur, the VMM must establish how\nmuch real memory each guest should use. To do that, the VMM ﬁrst evaluates\nthe maximum memory size of each guest as dictated when it is conﬁgured.\nGeneral-purpose operating systems do not expect the amount of memory\nin the system to change, so VMMs must maintain the illusion that the guest\nhas that amount of memory. Next, the VMM computes a target real memory\nallocation for each guest based on the conﬁgured memory for that guest and\nother factors, such as overcommitment and system load. It then uses the three",
  "low-level mechanisms below to reclaim memory from the guests. The overall\neffect is to enable guests to behave and perform as if they had the full amount\nof memory requested although in reality they have less.\n1. Recall that a guest believes it controls memory allocation via its page-\ntable management, whereas in reality the VMM maintains a nested page\ntable that re-translates the guest page table to the real page table. The\nVMM can use this extra level of indirection to optimize the guest’s use\nof memory without the guest’s knowledge or help. One approach is to\nprovide double paging, in which the VMM has its own page-replacement\nalgorithms and pages to backing-store pages that the guest believes are\nin physical memory. Of course, the VMM has knows less about the guest’s",
  "memory access patterns than the guest does, so its paging is less efﬁcient,\ncreating performance problems. VMMs do use this method when other\nmethods are not available or are not providing enough free memory.\nHowever, it is not the preferred approach.\n2. A common solution is for the VMM to install in each guest a pseudo–\ndevice driver or kernel module that it controls. (A pseudo–device driver\nuses device-driver interfaces, appearing to the kernel to be a device driver,\nbut does not actually control a device. Rather, it is an easy way to add\nkernel-mode code without directly modifying the kernel.) This balloon\nmemory manager communicates with the VMM and is told to allocate\nor deallocate memory. If told to allocate, it allocates memory and tells",
  "the operating system to pin the allocated pages into physical memory.\nRecall that pinning locks a page into physical memory so that it cannot be\nmoved or paged out. The guest sees memory pressure becauses of these\npinned pages, essentially decreasing the amount of physical memory it\nhas available to use. The guest then may free up other physical memory 16.6\nVirtualization and Operating-System Components\n731\nto be sure it has a sufﬁcient pool of free memory. Meanwhile, the VMM,\nknowing that the pages pinned by the balloon process will never be\nused, removes those physical pages from the guest and allocates them\nto another guest. At the same time, the guest is using its own memory-\nmanagement and paging algorithms to manage the available memory,",
  "which is the most efﬁcient option. If memory pressure within the entire\nsystem decreases, the VMM will tell the balloon process within the guest\nto unpin and free some or all of the memory, allowing the guest more\npages for its use.\n3. Another common method for reducing memory pressure is for the VMM\nto determine if the same page has been loaded more than once. If this\nis the case, to the VMM reduces the number of copies of the page to\none and maps the other users of the page to that one copy. VMware, for\nexample, randomly samples guest memory and creates a hash for each\npage sampled. That hash value is a “thumbprint” of the page. The hash\nof every page examined is compared with other hashes already stored\nin a hash table. If there is a match, the pages are compared byte by byte",
  "to see if they really are identical. If they are, one page is freed, and its\nlogical address is mapped to the other’s physical address. This technique\nmight seem at ﬁrst to be ineffective, but consider that guests run operating\nsystems. If multiple guests run the same operating system, then only one\ncopy of the active operating-system pages need be in memory. Similarly,\nmultiple guests could be running the same set of applications, again a\nlikely source of memory sharing.\n16.6.3\nI/O\nIn the area of I/O, hypervisors have some leeway and can be less concerned\nwith exactly representing the underlying hardware to their guests. Because of\nall the variation in I/O devices, operating systems are used to dealing with\nvarying and ﬂexible I/O mechanisms. For example, operating systems have",
  "a device-driver mechanism that provides a uniform interface to the operating\nsystem whatever the I/O device. Device-driver interfaces are designed to allow\nthird-party hardware manufacturers to provide device drivers connecting their\ndevices to the operating system. Usually, device drivers can be dynamically\nloaded and unloaded. Virtualization takes advantage of such built-in ﬂexibility\nby providing speciﬁc virtualized devices to guest operating systems.\nAs described in Section 16.5, VMMs vary greatly in how they provide I/O to\ntheir guests. I/O devices may be dedicated to guests, for example, or the VMM\nmay have device drivers onto which it maps guest I/O. The VMM may also\nprovide idealized device drivers to guests, which allows easy provision and",
  "management of guest I/O. In this case, the guest sees an easy-to-control device,\nbut in reality that simple device driver communicates to the VMM which sends\nthose requests to a more complicated real device through a more complex real\ndevice driver. I/O in virtual environments is complicated and requires careful\nVMM design and implementation.\nConsider the case of a hypervisor and hardware combination that allows\ndevices to be dedicated to a guest and allows the guest to access those devices\ndirectly. Of course, a device dedicated to one guest is not available to any\nother guests, but this direct access can still be useful in some circumstances. 732\nChapter 16\nVirtual Machines\nThe reason to allow direct access is to improve I/O performance. The less the",
  "hypervisor has to do to enable I/O for its guests, the faster the I/O can occur.\nWith Type 0 hypervisors that provide direct device access, guests can often\nrun at the same speed as native operating systems. When type 0 hypervisors\ninstead provide shared devices, performance can suffer by comparison.\nWith direct device access in type 1 and 2 hypervisors, performance can\nbe similar to that of native operating systems if certain hardware support\nis present. The hardware needs to provide DMA pass-through with facilities\nlike VT-d, as well as direct interrupt delivery to speciﬁc guests. Given how\nfrequently interrupts occur, it should be no surprise that the guests on hardware\nwithout these features have worse performance than if they were running\nnatively.",
  "natively.\nIn addition to direct access, VMMs provide shared access to devices.\nConsider a disk drive to which multiple guests have access. The VMM must\nprovide protection while sharing the device, assuring that a guest can access\nonly the blocks speciﬁed in the guest’s conﬁguration. In such instances, the\nVMM must be part of every I/O, checking it for correctness as well as routing\nthe data to and from the appropriate devices and guests.\nIn the area of networking, VMMs also have work to do. General-purpose\noperating systems typically have one Internet protocol (IP) address, although\nthey sometimes have more than one—for example, to connect to a management\nnetwork, backup network, and production network. With virtualization, each",
  "guest needs at least one IP address, because that is the guest’s main mode\nof communication. Therefore, a server running a VMM may have dozens of\naddresses, and the VMM acts as a virtual switch to route the network packets\nto the addressed guest.\nThe guests can be “directly” connected to the network by an IP address that\nis seen by the broader network (this is known as bridging). Alternatively,\nthe VMM can provide a network address translation (NAT) address. The\nNAT address is local to the server on which the guest is running, and the\nVMM provides routing between the broader network and the guest. The VMM\nalso provides ﬁrewalling, moderating connections between guests within the\nsystem and between guests and external systems.\n16.6.4\nStorage Management",
  "16.6.4\nStorage Management\nAn important question in determining how virtualization works is this: If\nmultiple operating systems have been installed, what and where is the boot\ndisk? Clearly, virtualized environments need to approach the area of storage\nmanagement differently from native operating systems. Even the standard\nmultiboot method of slicing the root disk into partitions, installing a boot\nmanager in one partition, and installing each other operating system in another\npartition is not sufﬁcient, because partitioning has limits that would prevent it\nfrom working for tens or hundreds of virtual machines.\nOnce again, the solution to this problem depends on the type of hypervisor.\nType 0 hypervisors do tend to allow root disk partitioning, partly because these",
  "systems tend to run fewer guests than other systems. Alternatively, they may\nhave a disk manager as part of the control partition, and that disk manager\nprovides disk space (including boot disks) to the other partitions. 16.6\nVirtualization and Operating-System Components\n733\nType 1 hypervisors store the guest root disk (and conﬁguration informa-\ntion) in one or more ﬁles within the ﬁle systems provided by the VMM. Type 2\nhypervisors store the same information within the host operating system’s ﬁle\nsystems. In essence, a disk image, containing all of the contents of the root disk\nof the guest, is contained within one ﬁle in the VMM. Aside from the potential\nperformance problems that causes, it is a clever solution, because it simpliﬁes",
  "copying and moving guests. If the administrator wants a duplicate of the guest\n(for testing, for example), she simply copies the associated disk image of the\nguest and tells the VMM about the new copy. Booting that new VM brings up\nan identical guest. Moving a virtual machine from one system to another that\nruns the same VMM is as simple as halting the guest, copying the image to the\nother system, and starting the guest there.\nGuests sometimes need more disk space than is available in their root\ndisk image. For example, a nonvirtualized database server might use several\nﬁle systems spread across many disks to store various parts of the database.\nVirtualizing such a database usually involves creating several ﬁles and having",
  "the VMM present those to the guest as disks. The guest then executes as usual,\nwith the VMM translating the disk I/O requests coming from the guest into ﬁle\nI/O commands to the correct ﬁles.\nFrequently, VMMs provide a mechanism to capture a physical system as\nit is currently conﬁgured and convert it to a guest that the VMM can manage\nand run. Based on the discussion above, it should be clear that this physical-\nto-virtual (P-to-V) conversion reads the disk blocks of the physical system’s\ndisks and stores them within ﬁles on the VMM’s system or on shared storage\nthat the VMM can access. Perhaps not as obvious is the need for a virtual-to-\nphysical (V-to-P) procedure for converting a guest to a physical system. This\nstep is sometimes needed for debugging: a problem could be caused by the",
  "VMM or associated components, and the administrator could attempt to solve\nthe problem by removing virtualization from the problem variables. V-to-P\nconversion can take the ﬁles containing all of the guest data and generate disk\nblocks on a system’s disk, recreating the guest as a native operating system and\napplications. Once the testing is concluded, the native system can be reused\nfor other purposes when the virtual machine returns to service, or the virtual\nmachine can be deleted and the native system can continue to run.\n16.6.5\nLive Migration\nOne feature not found in general-purpose operating systems but found in type\n0 and type 1 hypervisors is the live migration of a running guest from one\nsystem to another. We mentioned this capability earlier. Here, we explore the",
  "details of how live migration works and why VMMs have a relatively easy time\nimplementing it while general-purpose operating systems, in spite of some\nresearch attempts, do not.\nFirst, consider how live migration works. A running guest on one system\nis copied to another system running the same VMM. The copy occurs with so\nlittle interruption of service that users logged in to the guest, and network\nconnections to the guest, continue without noticeable impact. This rather\nastonishing ability is very powerful in resource management and hardware\nadministration. After all, compare it with the steps necessary without virtu-\nalization: warning users, shutting down the processes, possibly moving the 734\nChapter 16\nVirtual Machines",
  "Chapter 16\nVirtual Machines\nbinaries, and restarting the processes on the new system, with users only\nthen able to use the services again. With live migration, an overloaded system\ncan have its load decreased live with no discernible disruption. Similarly, a\nsystem needing hardware or system changes (for example, a ﬁrmware upgrade,\nhardware addition or removal, or hardware repair) can have guests migrated\noff, the work done, and guests migrated back without noticeable impact on\nusers or remote connections.\nLive migration is made possible because of the well-deﬁned interfaces\nbetween guests and VMMs and the limited state the VMM maintains for the\nguest. The VMM migrates a guest via the following steps:\n1. The source VMM establishes a connection with the target VMM and",
  "conﬁrms that it is allowed to send a guest.\n2. The target creates a new guest by creating a new VCPU, new nested page\ntable, and other state storage.\n3. The source sends all read-only memory pages to the target.\n4. The source sends all read-write pages to the target, marking them as\nclean.\n5. The source repeats step 4, as during that step some pages were probably\nmodiﬁed by the guest and are now dirty. These pages need to be sent\nagain and marked again as clean.\n6. When the cycle of steps 4 and 5 becomes very short, the source VMM\nfreezes the guest, sends the VCPU’s ﬁnal state, sends other state details,\nsends the ﬁnal dirty pages, and tells the target to start running the\nguest. Once the target acknowledges that the guest is running, the source\nterminates the guest.",
  "terminates the guest.\nThis sequence is shown in Figure 16.8.\nWe conclude this discussion with a few interesting details and limita-\ntions concerning live migration. First, for network connections to continue\nuninterrupted, the network infrastructure needs to understand that a MAC\nGuest Target running\n5 – Send Dirty Pages (repeatedly)\n4 – Send R/W Pages\n3 – Send R/O Pages\n1 – Establish\n0 – Running\nGuest Source\nVMM Source\n7 – Terminate\nGuest Source\nVMM Target\n2 – Create\nGuest Target\n6 – Running\nGuest Target\nFigure 16.8\nLive migration of a guest between two servers. 16.7\nExamples\n735\naddress—the hardware networking address—can move between systems.\nBefore virtualization, this did not happen, as the MAC address was tied to",
  "physical hardware. With virtualization, the MAC must be movable for exist-\ning networking connections to continue without resetting. Modern network\nswitches understand this and route trafﬁc wherever the MAC address is, even\naccommodating a move.\nA limitation of live migration is that no disk state is transferred. One reason\nlive migration is possible is that most of the guest’s state is maintained within\nthe guest—for example, open ﬁle tables, system-call state, kernel state, and so\non. Because disk I/O is so much slower than memory access, and used disk\nspace is usually much larger than used memory, disks associated with the guest\ncannot be moved as part of a live migration. Rather, the disk must be remote to\nthe guest, accessed over the network. Inthat case, diskaccessstate ismaintained",
  "within the guest, and network connections are all that matter to the VMM. The\nnetwork connections are maintained during the migration, so remote disk\naccess continues. Typically, NFS, CIFS, or iSCSI is used to store virtual machine\nimages and any other storage a guest needs access to. Those network-based\nstorage accesses simply continue when the network connections are continued\nonce the guest has been migrated.\nLive migration enables entirely new ways of managing data centers.\nFor example, virtualization management tools can monitor all the VMMs in\nan environment and automatically balance resource use by moving guests\nbetween the VMMs. They can also optimize the use of electricity and cooling\nby migrating all guests off selected servers if other servers can handle the load",
  "and powering down the selected servers entirely. If the load increases, these\ntools can power up the servers and migrate guests back to them.\n16.7 Examples\nDespite the advantages of virtual machines, they received little attention for\na number of years after they were ﬁrst developed. Today, however, virtual\nmachines are coming into fashion as a means of solving system compatibility\nproblems. In this section, we explore two popular contemporary virtual\nmachines: the VMware Workstation and the Java virtual machine. As you will\nsee, these virtual machines can typically run on top of operating systems of\nany of the design types discussed in earlier chapters. Thus, operating-system\ndesign methods—simple layers, microkernels, modules, and virtual machines\n—are not mutually exclusive.\n16.7.1",
  "—are not mutually exclusive.\n16.7.1\nVMware\nVMware Workstation is a popular commercial application that abstracts\nIntel X86 and compatible hardware into isolated virtual machines. VMware\nWorkstation is a prime example of a Type 2 hypervisor. It runs as an application\non a host operating system such as Windows or Linux and allows this\nhost system to run several different guest operating systems concurrently as\nindependent virtual machines.\nThe architecture of such a system is shown in Figure 16.9. In this scenario,\nLinux is running as the host operating system, and FreeBSD, Windows NT, and 736\nChapter 16\nVirtual Machines\nvirtualization layer\nhost operating system\n(Linux)\nCPU\nmemory\nhardware\nI/O devices\napplication\napplication\napplication\napplication\nguest operating\nsystem\n(free BSD)",
  "application\nguest operating\nsystem\n(free BSD)\nvirtual CPU\nvirtual memory\nvirtual devices\nguest operating\nsystem\n(Windows NT)\nvirtual CPU\nvirtual memory\nvirtual devices\nguest operating\nsystem\n(Windows XP)\nvirtual CPU\nvirtual memory\nvirtual devices\nFigure 16.9\nVMware Workstation architecture.\nWindows XP are running as guest operating systems. At the heart of VMware\nis the virtualization layer, which abstracts the physical hardware into isolated\nvirtual machines running as guest operating systems. Each virtual machine\nhas its own virtual CPU, memory, disk drives, network interfaces, and so forth.\nThe physical disk that the guest owns and manages is really just a ﬁle within\nthe ﬁle system of the host operating system. To create an identical guest, we",
  "can simply copy the ﬁle. Copying the ﬁle to another location protects the guest\nagainst a disaster at the original site. Moving the ﬁle to another location moves\nthe guest system. These scenarios show how virtualization can improve the\nefﬁciency of system administration as well as system resource use.\n16.7.2\nThe Java Virtual Machine\nJava is a popular object-oriented programming language introduced by Sun\nMicrosystems in 1995. In addition to a language speciﬁcation and a large\nAPI library, Java provides a speciﬁcation for a Java virtual machine, or JVM.\nJava therefore is an example of programming-environment virtualization, as\ndiscussed in Section 16.5.6.\nJava objects are speciﬁed with the class construct; a Java program",
  "consists of one or more classes. For each Java class, the compiler produces\nan architecture-neutral bytecode output (.class) ﬁle that will run on any\nimplementation of the JVM.\nThe JVM is a speciﬁcation for an abstract computer. It consists of a class\nloader and a Java interpreter that executes the architecture-neutral bytecodes,\nas diagrammed in Figure 16.10. The class loader loads the compiled .class\nﬁles from both the Java program and the Java API for execution by the Java\ninterpreter. After a class is loaded, the veriﬁer checks that the .class ﬁle is\nvalid Java bytecode and that it does not overﬂow or underﬂow the stack. It also 16.8\nSummary\n737\nhost system \n(Windows, Linux, etc.)\nclass loader\nJava\ninterpreter\nJava program\n.class files\nJava API\n.class files\nFigure 16.10",
  ".class files\nJava API\n.class files\nFigure 16.10\nThe Java virtual machine.\nensures that the bytecode does not perform pointer arithmetic, which could\nprovide illegal memory access. If the class passes veriﬁcation, it is run by the\nJava interpreter. The JVM also automatically manages memory by performing\ngarbage collection—the practice of reclaiming memory from objects no longer\nin use and returning it to the system. Much research focuses on garbage\ncollection algorithms for increasing the performance of Java programs in the\nvirtual machine.\nThe JVM may be implemented in software on top of a host operating\nsystem, such as Windows, Linux, or Mac OS X, or as part of a Web browser.\nAlternatively, the JVM may be implemented in hardware on a chip speciﬁcally",
  "designed to run Java programs. If the JVM is implemented in software, the\nJava interpreter interprets the bytecode operations one at a time. A faster\nsoftware technique is to use a just-in-time (JIT) compiler. Here, the ﬁrst time a\nJava method is invoked, the bytecodes for the method are turned into native\nmachine language for the host system. These operations are then cached so that\nsubsequent invocations of a method are performed using the native machine\ninstructions, and the bytecode operations need not be interpreted all over again.\nRunning the JVM in hardware is potentially even faster. Here, a special Java\nchip executes the Java bytecode operations as native code, thus bypassing the\nneed for either a software interpreter or a just-in-time compiler.\n16.8 Summary",
  "16.8 Summary\nVirtualization is a method of providing a guest with a duplicate of a system’s\nunderlying hardware. Multiple guests can run on a given system, each\nbelieving it is the native operating system in full control of the system.\nVirtualization started as a method to allow IBM to segregate users and\nprovide them with their own execution environments on IBM mainframes.\nSince then, with improvements in system and CPU performance and through\ninnovative software techniques, virtualization has become a common feature\nin data centers and even on personal computers. Because of the popularity of\nvirtualization, CPU designers have added features to support virtualization.\nThis snowball effect is likely to continue, with virtualization and its hardware\nsupport increasing over time.",
  "support increasing over time.\nType 0 virtualization is implemented in the hardware and requires modiﬁ-\ncations to the operating system to ensure proper operation. These modiﬁcations 738\nChapter 16\nVirtual Machines\noffer an example of paravirtualization, in which the operating system is not\nblind to virtualization but instead has features added and algorithms changed\nto improve virtualization’s features and performance. In Type 1 virtualization,\na host virtual machine monitor (VMM) provides the environment and features\nneeded to create, run, and destroy guest virtual machines. Each guest includes\nall of the software typically associated with a full native system, including the\noperating system, device drivers, applications, user accounts, and so on.",
  "Type 2 hypervisors are simply applications that run on other operating\nsystems, which do not know that virtualization is taking place. These hypervi-\nsors do not enjoy hardware or host support so must perform all virtualization\nactivities in the context of a process.\nOther facilities that are similar to virtualization but do not meet the full\ndeﬁnition of replicating hardware exactly are also common. Programming-\nenvironment virtualization is part of the design of a programming language.\nThe language speciﬁes a containing application in which programs run, and\nthis application provides services to the programs. Emulation is used when a\nhost system has one architecture and the guest was compiled for a different\narchitecture. Every instruction the guest wants to execute must be translated",
  "from its instruction set to that of the native hardware. Although this method\ninvolves some perform penalty, it is balanced by the usefulness of being able\nto run old programs on newer, incompatible hardware or run games designed\nfor old consoles on modern hardware.\nImplementing virtualization is challenging, especially when hardware\nsupport is minimal. Some hardware support must exist for virtualization,\nbut the more features provided by the system, the easier virtualization is to\nimplement and the better the performance of the guests. VMMs take advantage\nof whatever hardware support is available when optimizing CPU scheduling,\nmemory management, and I/O modules to provide guests with optimum\nresource use while protecting the VMM from the guests and the guests from\none another.\nExercises",
  "one another.\nExercises\n16.1\nDescribe the three types of traditional virtualization.\n16.2\nDescribe the four virtualization-like execution environments and why\nthey are not “true” virtualization.\n16.3\nDescribe four beneﬁts of virtualization.\n16.4\nWhy can VMMs not implement trap-and-emulate-based virtualization\non some CPUs? Lacking the ability to trap-and-emulate, what method\ncan a VMM use to implement virtualization?\n16.5\nWhat hardware assistance for virtualization can be provided by modern\nCPUs?\n16.6\nWhy is live migration possible in virtual environments but much less\npossible for a native operating system? Bibliography\n739\nBibliographical Notes\nThe original IBM VM system was described in [Meyer and Seawright (1970)].",
  "[Popek and Goldberg (1974)] established the characteristics that help deﬁne\nVMMs. Methods of implementing virtual machines are discussed in [Agesen\net al. (2010)].\nVirtualization has been an active research area for many years. Disco was\none of the ﬁrst attempts to use virtualization to enforce logical isolation and\nprovide scalability on multicore systems ([Bugnion et al. (1997)]). Based on that\nand and other work, Quest-V used virtualization to create an entire distributed\noperating system within a multicore system ([Li et al. (2011)]).\nIntel x86 hardware virtualization support is described in [Neiger et al.\n(2006)]. AMD hardware virtualization support is described in a white paper\n(http://developer.amd.com/assets/NPT-WP-1%201-ﬁnal-TM.pdf).",
  "KVM is described in [Kivity et al. (2007)]. Xen is described in [Barham\net al. (2003)]. Oracle Solaris containers are similar to BSD jails, as described in\n[Poul-henning Kamp (2000)].\n[Agesen et al. (2010)] discuss the performance of binary translation.\nMemory management in VMware is described in [Waldspurger (2002)]. The\nproblem of I/O overhead in virtualized environments has a proposed solution\nin [Gordon et al. (2012)]. Some protection challenges and attacks in virtual\nenvironments are discussed in [Wojtczuk and Ruthkowska (2011)].\nLive process migration research occurred in the 1980s and was ﬁrst dis-\ncussed in [Powell and Miller (1983)]. Problems identiﬁed in that research\nleft migration in a functionally limited state, as described in [Milojicic",
  "et al. (2000)]. VMware realized that virtualization could allow functional\nlive migration and described prototype work in [Chandra et al. (2002)].\nVMware shipped the vMotion live migration feature as part of VMware\nvCenter, as described in VMware VirtualCenter User’s Manual Version 1.0\n(http://www.vmware.com/pdf/VirtualCenter Users Manual.pdf). The details\nof the implementation of a similar feature in the Xen VMM are found in [Clark\net al. (2005)].\nResearch showing that, without interrupt remapping, malicious guests\ncan generate interrupts that can be used to gain control of the host system is\ndiscussed in [Wojtczuk and Ruthkowska (2011)].\nBibliography\n[Agesen et al. (2010)]\nO. Agesen, A. Garthwaite, J. Sheldon, and P. Subrah-",
  "manyam, “The Evolution of an x86 Virtual Machine Monitor”, Proceedings of\nthe ACM Symposium on Operating Systems Principles (2010), pages 3–18.\n[Barham et al. (2003)]\nP. Barham, B. Dragovic, K. Fraser, S. Hand, T. Harris,\nA. Ho, R. Neugebauer, I. Pratt, and A. Warﬁeld, “Xen and the Art of Virtu-\nalization”, Proceedings of the ACM Symposium on Operating Systems Principles\n(2003), pages 164–177. 740\nChapter 16\nVirtual Machines\n[Bugnion et al. (1997)]\nE. Bugnion, S. Devine, and M. Rosenblum, “Disco: Run-\nning Commodity Operating Systems on Scalable Multiprocessors”, Proceedings\nof the ACM Symposium on Operating Systems Principles (1997), pages 143–156.\n[Chandra et al. (2002)]\nR. Chandra, B. Pfaff, J. Chow, M. Lam, and M. Rosen-",
  "blum, “Optimizing the Migration of Virtual Computers” (2002), pages 377–390.\n[Clark et al. (2005)]\nC. Clark, K. Fraser, S. Hand, J. G. Hansen, E. Jul, C. Limpach,\nI. Pratt, and A. Warﬁeld, “Live Migration of Virtual Machines”, Proceedings of\nthe 2nd Conference on Symposium on Networked Systems Design & Implementation\n(2005), pages 273–286.\n[Gordon et al. (2012)]\nA. Gordon, N. A. N. Har’El, M. Ben-Yehuda, A. Landau,\nand A. S. andDan Tsafrir, “ELI: Bare-metal Performance for I/O Virtualization”,\nProceedings of the International Conference on Architectural Support for Programming\nLanguages and Operating Systems (2012), pages 411–422.\n[Kivity et al. (2007)]\nA. Kivity, Y. Kamay, D. Laor, U. Lublin, and A. Liguori,\n“kvm: the Linux Virtual Machine Monitor”, Proceedings of the Linux Symposium",
  "(2007).\n[Li et al. (2011)]\nY. Li, M. Danish, and R. West, “Quest-V: A Virtualized Mul-\ntikernel for High-Conﬁdence Systems”, Technical report, Boston University\n(2011).\n[Meyer and Seawright (1970)]\nR. A. Meyer and L. H. Seawright, “A Virtual\nMachine Time-Sharing System”, IBM Systems Journal, Volume 9, Number 3\n(1970), pages 199–218.\n[Milojicic et al. (2000)]\nD. S. Milojicic, F. Douglis, Y. Paindaveine, R. Wheeler,\nand S. Zhou, “Process Migration”, ACM Computing Surveys, Volume 32, Number\n3 (2000), pages 241–299.\n[Neiger et al. (2006)]\nG. Neiger, A. Santoni, F. Leung, D. Rodgers, and R. Uhlig,\n“Intel Virtualization Technology: Hardware Support for Efﬁcient Orocessor\nVirtualization”, Intel Technology Journal, Volume 10, (2006).\n[Popek and Goldberg (1974)]",
  "[Popek and Goldberg (1974)]\nG. J. Popek and R. P. Goldberg, “Formal Require-\nments for Virtualizable Third Generation Architectures”, Communications of the\nACM, Volume 17, Number 7 (1974), pages 412–421.\n[Poul-henning Kamp (2000)]\nR. N. M. W. Poul-henning Kamp, “Jails: Conﬁning\nthe Omnipotent Root”, Proceedings of the 2nd International System Administration\nand Networking Conferenc (2000).\n[Powell and Miller (1983)]\nM. Powell and B. Miller, “Process Migration in\nDEMOS/MP”, Proceedings of the ACM Symposium on Operating Systems Principles\n(1983).\n[Waldspurger (2002)]\nC. Waldspurger, “Memory Resource Management in\nVMware ESX Server”, Operating Systems Review, Volume 36, Number 4 (2002),\npages 181–194.\n[Wojtczuk and Ruthkowska (2011)]\nR. Wojtczuk and J. Ruthkowska, “Follow-",
  "R. Wojtczuk and J. Ruthkowska, “Follow-\ning the White Rabbit: Software Attacks Against Intel VT-d Technology”, The\nInvisible Things Lab’s blog (2011). 17\nC H A P T E R\nDistributed\nSystems\nA distributed system is a collection of processors that do not share memory or\na clock. Instead, each node has its own local memory. The nodes communicate\nwith one another through various networks, such as high-speed buses and the\nInternet. In this chapter, we discuss the general structure of distributed systems\nand the networks that interconnect them. We also contrast the main differences\nin operating-system design between these systems and centralized systems.\nCHAPTER OBJECTIVES\n• To provide a high-level overview of distributed systems and the networks\nthat interconnect them.",
  "that interconnect them.\n• To describe the general structure of distributed operating systems.\n• To explain general communication structure and communication protocols.\n• To discuss issues concerning the design of distributed systems.\n17.1\nAdvantages of Distributed Systems\nA distributed system is a collection of loosely coupled nodes interconnected\nby a communication network. From the point of view of a speciﬁc node in\na distributed system, the rest of the nodes and their respective resources are\nremote, whereas its own resources are local.\nThe nodes in a distributed system may vary in size and function. They may\ninclude small microprocessors, personal computers, and large general-purpose\ncomputer systems. These processors are referred to by a number of names, such",
  "as processors, sites, machines, and hosts, depending on the context in which they\nare mentioned. We mainly use site to indicate the location of a machine and node\nto refer to a speciﬁc system at a site. Generally, one node at one site, the server,\nhas a resource that another node at another site, the client (or user), would like\nto use. A general structure of a distributed system is shown in Figure 17.1.\nThere are four major reasons for building distributed systems: resource\nsharing, computation speedup, reliability, and communication. In this section,\nwe brieﬂy discuss each of them.\n741 742\nChapter 17\nDistributed Systems\nsite C\nresources\nsite B\nclient\ncommunication\nsite A    \nserver\nnetwork\nFigure 17.1\nA distributed system.\n17.1.1\nResource Sharing",
  "A distributed system.\n17.1.1\nResource Sharing\nIf a number of different sites (with different capabilities) are connected to one\nanother, then a user at one site may be able to use the resources available at\nanother. For example, a user at site A may be using a laser printer located at\nsite B. Meanwhile, a user at B may access a ﬁle that resides at A. In general,\nresource sharing in a distributed system provides mechanisms for sharing\nﬁles at remote sites, processing information in a distributed database, printing\nﬁles at remote sites, using remote specialized hardware devices (such as a\nsupercomputer), and performing other operations.\n17.1.2\nComputation Speedup\nIf a particular computation can be partitioned into subcomputations that",
  "can run concurrently, then a distributed system allows us to distribute\nthe subcomputations among the various sites. The subcomputations can be\nrun concurrently and thus provide computation speedup. In addition, if a\nparticular site is currently overloaded with jobs, some of them can be moved\nto other, lightly loaded sites. This movement of jobs is called load sharing or\njob migration. Automated load sharing, in which the distributed operating\nsystem automatically moves jobs, is not yet common in commercial systems.\n17.1.3\nReliability\nIf one site fails in a distributed system, the remaining sites can continue\noperating, giving the system better reliability. If the system is composed of\nmultiple large autonomous installations (that is, general-purpose computers),",
  "the failure of one of them should not affect the rest. If, however, the system\nis composed of small machines, each of which is responsible for some crucial\nsystem function (such as the web server or the ﬁle system), then a single\nfailure may halt the operation of the whole system. In general, with enough 17.2\nTypes of Network-based Operating Systems\n743\nredundancy (in both hardware and data), the system can continue operation,\neven if some of its sites have failed.\nThe failure of a site must be detected by the system, and appropriate action\nmay be needed to recover from the failure. The system must no longer use the\nservices of that site. In addition, if the function of the failed site can be taken\nover by another site, the system must ensure that the transfer of function occurs",
  "correctly. Finally, when the failed site recovers or is repaired, mechanisms must\nbe available to integrate it back into the system smoothly.\n17.1.4\nCommunication\nWhen several sites are connected to one another by a communication network,\nusers at the various sites have the opportunity to exchange information. At\na low level, messages are passed between systems, much as messages are\npassed between processes in the single-computer message system discussed\nin Section 3.4. Given message passing, all the higher-level functionality found\nin standalone systems can be expanded to encompass the distributed system.\nSuch functions include ﬁle transfer, login, mail, and remote procedure calls\n(RPCs).\nThe advantage of a distributed system is that these functions can be",
  "carried out over great distances. Two people at geographically distant sites can\ncollaborate on a project, for example. By transferring the ﬁles of the project,\nlogging in to each other’s remote systems to run programs, and exchanging\nmail to coordinate the work, users minimize the limitations inherent in long-\ndistance work. We wrote this book by collaborating in such a manner.\nThe advantages of distributed systems have resulted in an industry-wide\ntrend toward downsizing. Many companies are replacing their mainframes\nwith networks of workstations or personal computers. Companies get a bigger\nbang for the buck (that is, better functionality for the cost), more ﬂexibility in\nlocating resources and expanding facilities, better user interfaces, and easier\nmaintenance.",
  "maintenance.\n17.2 Types of Network-based Operating Systems\nIn this section, we describe the two general categories of network-oriented\noperating systems: network operating systems and distributed operating\nsystems. Network operating systems are simpler to implement but generally\nmore difﬁcult for users to access and utilize than are distributed operating\nsystems, which provide more features.\n17.2.1\nNetwork Operating Systems\nA network operating system provides an environment in which users, who\nare aware of the multiplicity of machines, can access remote resources by\neither logging in to the appropriate remote machine or transferring data from\nthe remote machine to their own machines. Currently, all general-purpose\noperating systems, and even embedded operating systems such as Android",
  "and iOS, are network operating systems. 744\nChapter 17\nDistributed Systems\n17.2.1.1\nRemote Login\nAn important function of a network operating system is to allow users to\nlog in remotely. The Internet provides the ssh facility for this purpose. To\nillustrate, let’s suppose that a user at Westminster College wishes to compute\non cs.yale.edu, a computer that is located at Yale University. To do so, the\nuser must have a valid account on that machine. To log in remotely, the user\nissues the command\nssh cs.yale.edu\nThis command results in the formation of an encrypted socket connection\nbetween the local machine at Westminster College and the “cs.yale.edu”\ncomputer. After this connection has been established, the networking software",
  "creates a transparent, bidirectional link so that all characters entered by the user\nare sent to a process on “cs.yale.edu” and all the output from that process is sent\nback to the user. The process on the remote machine asks the user for a login\nname and a password. Once the correct information has been received, the\nprocess acts as a proxy for the user, who can compute on the remote machine\njust as any local user can.\n17.2.1.2\nRemote File Transfer\nAnother major function of a network operating system is to provide a\nmechanism for remote ﬁle transfer from one machine to another. In such\nan environment, each computer maintains its own local ﬁle system. If a user at\none site (say, cs.uvm.edu) wants to access a ﬁle located on another computer",
  "(say, cs.yale.edu), then the ﬁle must be copied explicitly from the computer\nat Yale to the computer at the University of Vermont.\nThe Internet provides a mechanism for such a transfer with the ﬁle transfer\nprotocol (FTP) program and the more private secure ﬁle transfer protocol (SFTP)\nprogram. Suppose that a user on “cs.uvm.edu” wants to copy a Java program\nServer.java that resides on “cs.yale.edu.” The user must ﬁrst invoke the sftp\nprogram by executing\nsftp cs.yale.edu\nThe program then asks the user for a login name and a password. Once\nthe correct information has been received, the user must connect to the\nsubdirectory where the ﬁle Server.java resides and then copy the ﬁle by\nexecuting\nget Server.java\nIn this scheme, the ﬁle location is not transparent to the user; users must know",
  "exactly where each ﬁle is. Moreover, there is no real ﬁle sharing, because a user\ncan only copy a ﬁle from one site to another. Thus, several copies of the same\nﬁle may exist, resulting in a waste of space. In addition, if these copies are\nmodiﬁed, the various copies will be inconsistent.\nNotice that, in our example, the user at the University of Vermont must\nhave login permission on “cs.yale.edu.” FTP also provides a way to allow a user 17.2\nTypes of Network-based Operating Systems\n745\nwho does not have an account on the Yale computer to copy ﬁles remotely. This\nremote copying is accomplished through the “anonymous FTP” method, which\nworks as follows. The ﬁle to be copied (that is, Server.java) must be placed",
  "in a special subdirectory (say, ftp) with the protection set to allow the public to\nread the ﬁle. A user who wishes to copy the ﬁle uses the ftp command. When\nthe user is asked for the login name, the user supplies the name “anonymous”\nand an arbitrary password.\nOnce anonymous login is accomplished, the system must ensure that this\npartially authorized user does not access inappropriate ﬁles. Generally, the\nuser is allowed to access only those ﬁles that are in the directory tree of user\n“anonymous.” Any ﬁles placed here are accessible to any anonymous users,\nsubject to the usual ﬁle-protection scheme used on that machine. Anonymous\nusers, however, cannot access ﬁles outside of this directory tree.\nImplementation of the FTP mechanism is similar to ssh implementation.",
  "A daemon on the remote site watches for requests to connect to the system’s\nFTP port. Login authentication is accomplished, and the user is allowed to\nexecute transfer commands remotely. Unlike the ssh daemon, which executes\nany command for the user, the FTP daemon responds only to a predeﬁned set\nof ﬁle-related commands. These include the following:\n• get—Transfer a ﬁle from the remote machine to the local machine.\n• put—Transfer from the local machine to the remote machine.\n• ls or dir—List ﬁles in the current directory on the remote machine.\n• cd—Change the current directory on the remote machine.\nThere are also various commands to change transfer modes (for binary or ASCII\nﬁles) and to determine connection status.\nAn important point about ssh and FTP is that they require the user to",
  "change paradigms. FTP requires the user to know a command set entirely\ndifferent from the normal operating-system commands. With ssh, the user\nmust know appropriate commands on the remote system. For instance, a user\non a Windows machine who connects remotely to a UNIX machine must switch\nto UNIX commands for the duration of the ssh session. (In networking, a\nsession is a complete round of communication, frequently beginning with a\nlogin to authenticate and ending with a logoff to terminate the communication.)\nObviously, users would ﬁnd it more convenient not to be required to use\na different set of commands. Distributed operating systems are designed to\naddress this problem.\n17.2.2\nDistributed Operating Systems\nIn a distributed operating system, users access remote resources in the same",
  "way they access local resources. Data and process migration from one site to\nanother is under the control of the distributed operating system.\n17.2.2.1\nData Migration\nSuppose a user on site A wants to access data (such as a ﬁle) that reside at site\nB. The system can transfer the data by one of two basic methods. One approach 746\nChapter 17\nDistributed Systems\nto data migration is to transfer the entire ﬁle to site A. From that point on, all\naccess to the ﬁle is local. When the user no longer needs access to the ﬁle, a\ncopy of the ﬁle (if it has been modiﬁed) is sent back to site B. Even if only a\nmodest change has been made to a large ﬁle, all the data must be transferred.\nThis mechanism can be thought of as an automated FTP system. This approach",
  "was used in the Andrew ﬁle system, but it was found to be too inefﬁcient.\nThe other approach is to transfer to site A only those portions of the ﬁle\nthat are actually necessary for the immediate task. If another portion is required\nlater, another transfer will take place. When the user no longer wants to access\nthe ﬁle, any part of it that has been modiﬁed must be sent back to site B.\n(Note the similarity to demand paging.) The Sun Microsystems network ﬁle\nsystem (NFS) protocol uses this method (Section 12.8), as do newer versions\nof Andrew. The Microsoft SMB protocol (also known as Common Internet File\nSystem, or CIFS) also allows ﬁle sharing over a network. SMB is described in\nSection 19.6.2.1.\nClearly, if only a small part of a large ﬁle is being accessed, the latter",
  "approach is preferable. If signiﬁcant portions of the ﬁle are being accessed,\nhowever, it is more efﬁcient to copy the entire ﬁle. Whichever method is used,\ndata migration includes more than the mere transfer of data from one site to\nanother. The system must also perform various data translations if the two\nsites involved are not directly compatible (for instance, if they use different\ncharacter-code representations or represent integers with a different number\nor order of bits).\n17.2.2.2\nComputation Migration\nIn some circumstances, we may want to transfer the computation, rather than\nthe data, across the system; this process is called computation migration. For\nexample, consider a job that needs to access various large ﬁles that reside at",
  "different sites, to obtain a summary of those ﬁles. It would be more efﬁcient to\naccess the ﬁles at the sites where they reside and return the desired results to\nthe site that initiated the computation. Generally, if the time to transfer the data\nis longer than the time to execute the remote command, the remote command\nshould be used.\nSuch a computation can be carried out in different ways. Suppose that\nprocess P wants to access a ﬁle at site A. Access to the ﬁle is carried out at site\nA and could be initiated by an RPC. An RPC uses network protocols to execute\na routine on a remote system (Section 3.6.2). Process P invokes a predeﬁned\nprocedure at site A. The procedure executes appropriately and then returns\nthe results to P.",
  "the results to P.\nAlternatively, process P can send a message to site A. The operating system\nat site A then creates a new process Q whose function is to carry out the\ndesignated task. When process Q completes its execution, it sends the needed\nresult back to P via the message system. In this scheme, process P may execute\nconcurrently with process Q. In fact, it may have several processes running\nconcurrently on several sites.\nEither method could be used to access several ﬁles residing at various sites.\nOne RPC might result in the invocation of another RPC or even in the transfer\nof messages to another site. Similarly, process Q could, during the course of its\nexecution, send a message to another site, which in turn would create another",
  "process. This process might either send a message back to Q or repeat the cycle. 17.3\nNetwork Structure\n747\n17.2.2.3\nProcess Migration\nA logical extension of computation migration is process migration. When a\nprocess is submitted for execution, it is not always executed at the site at which\nit is initiated. The entire process, or parts of it, may be executed at different\nsites. This scheme may be used for several reasons:\n• Load balancing. The processes (or subprocesses) may be distributed across\nthe network to even the workload.\n• Computation speedup. If a single process can be divided into a number\nof subprocesses that can run concurrently on different sites, then the total\nprocess turnaround time can be reduced.\n• Hardware preference. The process may have characteristics that make it",
  "more suitable for execution on some specialized processor (such as matrix\ninversion on an array processor) rather than on a microprocessor.\n• Software preference. The process may require software that is available\nat only a particular site, and either the software cannot be moved, or it is\nless expensive to move the process.\n• Data access. Just as in computation migration, if the data being used in the\ncomputation are numerous, it may be more efﬁcient to have a process run\nremotely than to transfer all the data.\nWe use two complementary techniques to move processes in a computer\nnetwork. In the ﬁrst, the system can attempt to hide the fact that the process has\nmigrated from the client. The client then need not code her program explicitly",
  "to accomplish the migration. This method is usually employed for achieving\nload balancing and computation speedup among homogeneous systems, as\nthey do not need user input to help them execute programs remotely.\nThe other approach is to allow (or require) the user to specify explicitly\nhow the process should migrate. This method is usually employed when the\nprocess must be moved to satisfy a hardware or software preference.\nYou have probably realized that the World Wide Web has many aspects of\na distributed computing environment. Certainly it provides data migration\n(between a web server and a web client). It also provides computation\nmigration. For instance, a web client could trigger a database operation on\na web server. Finally, with Java, Javascript, and similar languages, it provides",
  "a form of process migration: Java applets and Javascript scripts are sent from\nthe server to the client, where they are executed. A network operating system\nprovides most of these features, but a distributed operating system makes them\nseamless and easily accessible. The result is a powerful and easy-to-use facility\n—one of the reasons for the huge growth of the World Wide Web.\n17.3 Network Structure\nThere are basically two types of networks: local-area networks (LAN) and\nwide-area networks (WAN). The main difference between the two is the way in\nwhich they are geographically distributed. Local-area networks are composed 748\nChapter 17\nDistributed Systems\nof hosts distributed over small areas (such as a single building or a number",
  "of adjacent buildings), whereas wide-area networks are composed of systems\ndistributed over a large area (such as the United States). These differences imply\nmajor variations in the speed and reliability of the communications networks,\nand they are reﬂected in the distributed operating-system design.\n17.3.1\nLocal-Area Networks\nLocal-area networks emerged in the early 1970s as a substitute for large\nmainframe computer systems. For many enterprises, it is more economical\nto have a number of small computers, each with its own self-contained\napplications, than to have a single large system. Because each small computer\nis likely to need a full complement of peripheral devices (such as disks\nand printers), and because some form of data sharing is likely to occur in",
  "a single enterprise, it was a natural step to connect these small systems into a\nnetwork.\nLANs, as mentioned, are usually designed to cover a small geographical\narea, and they are generally used in an ofﬁce environment. All the sites in\nsuch systems are close to one another, so the communication links tend to have\na higher speed and lower error rate than do their counterparts in wide-area\nnetworks.\nThe most common links in a local-area network are twisted-pair and ﬁber-\noptic cabling. The most common conﬁguration is the star network. In a star\nnetwork, the nodesconnecttoone ormore switches, and the switchesconnectto\neach other, enabling any two nodes to communicate. Communication speeds\nrange from 1 megabit per second for networks such as AppleTalk, infrared,",
  "and the Bluetooth local radio network to 40 gigabits per second for the fastest\nEthernet. Ten megabits per second is the speed of 10BaseT Ethernet. 100BaseT\nEthernet and 1000BaseT Ethernet provide throughputs of 100 megabits and\n1 gigabit per second over twisted-pair copper cable. The use of optical-\nﬁber cabling is growing; it provides higher communication rates over longer\ndistances than are possible with copper.\nA typical LAN may consist of a number of different computers (from\nmainframes to laptops or other mobile devices), various shared peripheral\ndevices (such as laser printers and storage arrays), and one or more routers\n(specialized network communication processors) that provide access to other\nnetworks (Figure 17.2). Ethernet is commonly used to construct LANs. An",
  "Ethernet network has no central controller, because it is a multiaccess bus, so\nnew hosts can be added easily to the network. The Ethernet protocol is deﬁned\nby the IEEE 802.3 standard.\nThe wireless spectrum is increasingly used for designing local-area net-\nworks. Wireless (or WiFi) technology allows us to construct a network using\nonly a wireless router to transmit signals between hosts. Each host has a\nwireless transmitter and receiver that it uses to participate in the network.\nA disadvantage of wireless networks concerns their speed. Whereas Ether-\nnet systems often run at 1 gigabit per second, WiFi networks typically run\nconsiderably slower. There are several IEEE standards for wireless networks.\nThe 802.11g standard can theoretically run at 54 megabits per second, but in",
  "practice, data rates are often less than half that. The recent 802.11n standard\nprovides theoretically much higher data rates. In actual practice, though, these 17.3\nNetwork Structure\n749\nLAN Switch\nFirewall\nRouter\nWAN Link\nLAN\nWAN\nFigure 17.2\nLocal-area network.\nnetworks have typical data rates of around 75 megabits per second. Data\nrates of wireless networks are heavily inﬂuenced by the distance between the\nwireless router and the host, as well as interference in the wireless spectrum. On\nthe positive side, wireless networks often have a physical advantage over wired\nEthernet networks because they require no cabling to connect communicating\nhosts. As a result, wireless networks are popular in homes and businesses, as",
  "well as public areas such as libraries, Internet cafes, sports arenas, and even\nbuses and airplanes.\n17.3.2\nWide-Area Networks\nWide-area networks emerged in the late 1960s, mainly as an academic research\nproject to provide efﬁcient communication among sites, allowing hardware and\nsoftware to be shared conveniently and economically by a wide community\nof users. The ﬁrst WAN to be designed and developed was the Arpanet. Begun\nin 1968, the Arpanet has grown from a four-site experimental network to a\nworldwide network of networks, the Internet, comprising millions of computer\nsystems.\nBecause the sites in a WANare physically distributed over a large geographi-\ncal area, the communication links are, by default, relatively slow and unreliable.",
  "Typical links are telephone lines, leased (dedicated data) lines, optical cable,\nmicrowave links, radio waves, and satellite channels. These communication\nlinks are controlled by special communication processors (Figure 17.3), com-\nmonly known as gateway routers or simply routers, that are responsible for\ndeﬁning the interface through which the sites communicate over the network,\nas well as for transferring information among the various sites.\nFor example, the Internet WAN enables hosts at geographically separated\nsites to communicate with one another. The host computers typically differ\nfrom one another in speed, CPU type, operating system, and so on. Hosts are\ngenerally on LANs, which are, in turn, connected to the Internet via regional",
  "networks. The regional networks, such as NSFnet in the northeast United\nStates, are interlinked with routers (Section 17.4.2) to form the worldwide 750\nChapter 17\nDistributed Systems\ncommunication\nprocessor\ncommunication\nsubsystem\nH\nH\nH\nH\nH\nuser processes\nuser processes\nnetwork host\nnetwork host\nhost operating system\nhost operating system\nCP\nCP\nCP\nCP\nFigure 17.3\nCommunication processors in a wide-area network.\nnetwork. Connections between networks sometimes use a telephone-system\nservice called T1, which provides a transfer rate of 1.544 megabits per second\nover a leased line. For sites requiring faster Internet access, T1s are collected\ninto multiple-T1 units that work in parallel to provide more throughput. For",
  "instance, a T3 is composed of 28 T1 connections and has a transfer rate of 45\nmegabits per second. Connections such as OC-12 are common and provide\n622 megabits per second. Residences can connect to the Internet by either\ntelephone, cable, or specialized Internet service providers that install routers\nto connect the residences to central services. Of course, there are other WANs\nbesides the Internet. A company might create its own private WANfor increased\nsecurity, performance, or reliability.\nAs mentioned, WANs are generally slower than LANs, although backbone\nWAN connections that link major cities may have transfer rates of over 40\ngigabits per second. Frequently, WANs and LANs interconnect, and it is difﬁcult\nto tell where one ends and the other starts. Consider the cellular phone data",
  "network. Cell phones are used for both voice and data communications. Cell\nphones in a given area connect via radio waves to a cell tower that contains\nreceivers and transmitters. This part of the network is similar to a LAN except\nthat the cell phones do not communicate with each other (unless two people\ntalking or exchanging data happen to be connected to the same tower). Rather,\nthe towers are connected to other towers and to hubs that connect the tower\ncommunications to land lines or other communication mediums and route the\npackets toward their destinations. This part of the network is more WAN-like.\nOnce the appropriate tower receives the packets, it uses its transmitters to send\nthem to the correct recipient. 17.4\nCommunication Structure\n751\n17.4 Communication Structure",
  "751\n17.4 Communication Structure\nNow that we have discussed the physical aspects of networking, we turn to\nthe internal workings. The designer of a communication network must address\nﬁve basic issues:\n• Naming and name resolution. How do two processes locate each other to\ncommunicate?\n• Routing strategies. How are messages sent through the network?\n• Packet strategies. Are packets sent individually or as a sequence?\n• Connection strategies. How do two processes send a sequence of mes-\nsages?\nIn the following sections, we elaborate on each of these issues.\n17.4.1\nNaming and Name Resolution\nThe ﬁrst issue in network communication involves the naming of the systems\nin the network. For a process at site A to exchange information with a process",
  "at site B, each must be able to specify the other. Within a computer system,\neach process has a process identiﬁer, and messages may be addressed with the\nprocess identiﬁer. Because networked systems share no memory, however, a\nhost within the system initially has no knowledge about the processes on other\nhosts.\nTo solve this problem, processes on remote systems are generally identiﬁed\nby the pair <host name, identiﬁer>, where host name is a name unique within\nthe network and identiﬁer is a process identiﬁer or other unique number within\nthat host. A host name is usually an alphanumeric identiﬁer, rather than a\nnumber, to make it easier for users to specify. For instance, site A might have\nhosts named homer, marge, bart, and lisa. Bart is certainly easier to remember\nthan is 12814831100.",
  "than is 12814831100.\nNames are convenient for humans to use, but computers prefer numbers for\nspeed and simplicity. For this reason, there must be a mechanism to resolve the\nhost name intoahost-id that describesthe destinationsystemtothe networking\nhardware. This mechanism is similar to the name-to-address binding that\noccurs during program compilation, linking, loading, and execution (Chapter\n8). In the case of host names, two possibilities exist. First, every host may have a\ndata ﬁle containing the names and addresses of all the other hosts reachable on\nthe network (similar to binding at compile time). The problem with this model\nis that adding or removing a host from the network requires updating the data\nﬁles on all the hosts. The alternative is to distribute the information among",
  "systems on the network. The network must then use a protocol to distribute\nand retrieve the information. This scheme is like execution-time binding. The\nﬁrst method was the one originally used on the Internet. As the Internet grew,\nhowever, it became untenable. The second method, the domain-name system\n(DNS), is the one now in use.\nDNS speciﬁes the naming structure of the hosts, as well as name-to-address\nresolution. Hosts on the Internet are logically addressed with multipart names\nknown as IP addresses. The parts of an IP address progress from the most 752\nChapter 17\nDistributed Systems\nspeciﬁc to the most general, with periods separating the ﬁelds. For instance,\nbob.cs.brown.edu refers to host bob in the Department of Computer Science at",
  "Brown University within the top-level domain edu. (Other top-level domains\ninclude com for commercial sites and org for organizations, as well as a domain\nfor each country connected to the network, for systems speciﬁed by country\nrather than organization type.) Generally, the system resolves addresses by\nexamining the host-name components in reverse order. Each component has a\nname server—simply a process on a system—that accepts a name and returns\nthe address of the name server responsible for that name. As the ﬁnal step, the\nname server for the host in question is contacted, and a host-id is returned.\nFor example, a request made by a process on system A to communicate with\nbob.cs.brown.edu would result in the following steps:",
  "1. The system library or the kernel on system A issues a request to the name\nserver for the edu domain, asking for the address of the name server for\nbrown.edu. The name server for the edu domain must be at a known\naddress, so that it can be queried.\n2. The edu name server returns the address of the host on which the\nbrown.edu name server resides.\n3. System A then queries the name server at this address and asks about\ncs.brown.edu.\n4. An address is returned. Now, ﬁnally, a request to that address for\nbob.cs.brown.edu returns an Internet address host-id for that host (for\nexample, 128.148.31.100).\nThis protocol may seem inefﬁcient, but individual hosts cache the IP addresses\nthey have already resolved to speed the process. (Of course, the contents of",
  "these caches must be refreshed over time in case the name server is moved\nor its address changes.) In fact, the protocol is so important that it has been\noptimized many times and has had many safeguards added. Consider what\nwould happen if the primary edu name server crashed. It is possible that\nno edu hosts would be able to have their addresses resolved, making them\nall unreachable! The solution is to use secondary, backup name servers that\nduplicate the contents of the primary servers.\nBefore the domain-name service was introduced, all hosts on the Internet\nneeded to have copies of a ﬁle that contained the names and addresses of each\nhost on the network. All changes to this ﬁle had to be registered at one site (host",
  "SRI-NIC), and periodically all hosts had to copy the updated ﬁle from SRI-NIC\nto be able to contact new systems or ﬁnd hosts whose addresses had changed.\nUnder the domain-name service, each name-server site is responsible for\nupdating the host information for that domain. For instance, any host changes\nat Brown University are the responsibility of the name server for brown.edu and\nneed not be reported anywhere else. DNS lookups will automatically retrieve\nthe updated information because they will contact brown.edu directly. Domains\nmay contain autonomous subdomains to further distribute the responsibility\nfor host-name and host-id changes.\nJava provides the necessary API to design a program that maps IP names to",
  "IP addresses. The program shown in Figure 17.4 is passed an IP name (such as 17.4\nCommunication Structure\n753\n/**\n* Usage: java DNSLookUp <IP name>\n* i.e. java DNSLookUp www.wiley.com\n*/\npublic class DNSLookUp {\npublic static void main(String[] args) {\nInetAddress hostAddress;\ntry {\nhostAddress = InetAddress.getByName(args[0]);\nSystem.out.println(hostAddress.getHostAddress());\n}\ncatch (UnknownHostException uhe) {\nSystem.err.println(\"Unknown host: \" + args[0]);\n}\n}\n}\nFigure 17.4\nJava program illustrating a DNS lookup.\nbob.cs.brown.edu) on the command line and either outputs the IP address of the\nhost or returns a message indicating that the host name could not be resolved.\nAn InetAddress is a Java class representing an IP name or address. The static",
  "method getByName() belonging to the InetAddress class is passed a string\nrepresentation of an IP name, and it returns the corresponding InetAddress.\nThe program then invokes the getHostAddress() method, which internally\nuses DNS to look up the IP address of the designated host.\nGenerally, the operating system is responsible for accepting from its\nprocesses a message destined for <host name, identiﬁer> and for transferring\nthat message to the appropriate host. The kernel on the destination host is then\nresponsible for transferring the message to the process named by the identiﬁer.\nThis exchange is by no means trivial; it is described in Section 17.4.4.\n17.4.2\nRouting Strategies\nWhen a process at site A wants to communicate with a process at site B, how is",
  "the message sent? If there is only one physical path from A to B, the message\nmust be sent through that path. However, if there are multiple physical paths\nfrom A to B, then several routing options exist. Each site has a routing table\nindicating the alternative paths that can be used to send a message to other\nsites. The table may include information about the speed and cost of the various\ncommunication paths, and it may be updated as necessary, either manually or\nvia programs that exchange routing information. The three most common\nrouting schemes are ﬁxed routing, virtual routing, and dynamic routing.\n• Fixed routing. A path from A to B is speciﬁed in advance and does not\nchange unless a hardware failure disables it. Usually, the shortest path is",
  "chosen, so that communication costs are minimized.\n• Virtual routing. A path from A to B is ﬁxed for the duration of one session.\nDifferent sessions involving messages from A to B may use different paths. 754\nChapter 17\nDistributed Systems\nA session could be as short as a ﬁle transfer or as long as a remote-login\nperiod.\n• Dynamic routing. The path used to send a message from site A to site\nB is chosen only when the message is sent. Because the decision is made\ndynamically, separate messages may be assigned different paths. Site A\nwill make a decision to send the message to site C. C, in turn, will decide\nto send it to site D, and so on. Eventually, a site will deliver the message\nto B. Usually, a site sends a message to another site on whatever link is the",
  "least used at that particular time.\nThere are tradeoffs among these three schemes. Fixed routing cannot adapt\nto link failures or load changes. In other words, if a path has been established\nbetween A and B, the messages must be sent along this path, even if the path\nis down or is used more heavily than another possible path. We can partially\nremedy this problem by using virtual routing and can avoid it completely by\nusing dynamic routing. Fixed routing and virtual routing ensure that messages\nfrom A to B will be delivered in the order in which they were sent. In dynamic\nrouting, messages may arrive out of order. We can remedy this problem by\nappending a sequence number to each message.\nDynamic routing is the most complicated to set up and run; however, it is",
  "the best way to manage routing in complicated environments. UNIX provides\nboth ﬁxed routing for use on hosts within simple networks and dynamic\nrouting for complicated network environments. It is also possible to mix the\ntwo. Within a site, the hosts may just need to know how to reach the system that\nconnects the local network to other networks (such as company-wide networks\nor the Internet). Such a node is known as a gateway. Each individual host has\na static route to the gateway, but the gateway itself uses dynamic routing to\nreach any host on the rest of the network.\nA router is the communications processor within the computer network\nresponsible for routing messages. A router can be a host computer with routing",
  "software or a special-purpose device. Either way, a router must have at least\ntwo network connections, or else it would have nowhere to route messages.\nA router decides whether any given message needs to be passed from the\nnetwork on which it is received to any other network connected to the router.\nIt makes this determination by examining the destination Internet address\nof the message. The router checks its tables to determine the location of the\ndestination host, or at least of the network to which it will send the message\ntoward the destination host. In the case of static routing, this table is changed\nonly by manual update (a new ﬁle is loaded onto the router). With dynamic\nrouting, a routing protocol is used between routers to inform them of network",
  "changes and to allow them to update their routing tables automatically.\nGateways and routers have typically been dedicated hardware devices\nthat run code out of ﬁrmware. More recently, routing has been managed by\nsoftware that directs multiple network devices more intelligently than a single\nrouter could. The software is device-independent, enabling network devices\nfrom multiple vendors to cooperate more easily. For example, the OpenFlow\nstandard allows developers to introduce new networking efﬁciencies and\nfeatures by decoupling data-routing decisions from the underlying networking\ndevices. 17.4\nCommunication Structure\n755\n17.4.3\nPacket Strategies\nMessages generally vary in length. To simplify the system design, we com-",
  "monly implement communication with ﬁxed-length messages called packets,\nframes, or datagrams. A communication implemented in one packet can be\nsent to its destination in a connectionless message. A connectionless message\ncan be unreliable, in which case the sender has no guarantee that, and cannot\ntell whether, the packet reached its destination. Alternatively, the packet can be\nreliable. Usually, in this case, an acknowledgement packet is returned from the\ndestination indicating that the original packet arrived. (Of course, the return\npacket could be lost along the way.) If a message is too long to ﬁt within\none packet, or if the packets need to ﬂow back and forth between the two\ncommunicators, a connection is established to allow the reliable exchange of\nmultiple packets.\n17.4.4",
  "multiple packets.\n17.4.4\nConnection Strategies\nOnce messages are able to reach their destinations, processes can institute\ncommunications sessions to exchange information. Pairs of processes that want\nto communicate over the network can be connected in a number of ways. The\nthree most common schemes are circuit switching, message switching, and\npacket switching.\n• Circuit switching. If two processes want to communicate, a permanent\nphysical link is established between them. This link is allocated for the\nduration of the communication session, and no other process can use\nthat link during this period (even if the two processes are not actively\ncommunicating for a while). This scheme is similar to that used in the\ntelephone system. Once a communication line has been opened between",
  "two parties (that is, party A calls party B), no one else can use this circuit\nuntil the communication is terminated explicitly (for example, when the\nparties hang up).\n• Message switching. If two processes want to communicate, a temporary\nlink is established for the duration of one message transfer. Physical\nlinks are allocated dynamically among correspondents as needed and\nare allocated for only short periods. Each message is a block of data\nwith system information—such as the source, the destination, and error-\ncorrection codes (ECC)—that allows the communication network to deliver\nthe message to the destination correctly. This scheme is similar to the\npost-ofﬁce mailing system. Each letter is a message that contains both the",
  "destination address and source (return) address. Many messages (from\ndifferent users) can be shipped over the same link.\n• Packet switching. One logical message may have to be divided into a\nnumber of packets. Each packet may be sent to its destination separately,\nand each therefore must include a source and a destination address with its\ndata. Furthermore, the various packets may take different paths through\nthe network. The packets must be reassembled into messages as they\narrive. Note that it is not harmful for data to be broken into packets,\npossibly routed separately, and reassembled at the destination. Breaking 756\nChapter 17\nDistributed Systems\nup an audio signal (say, a telephone communication), in contrast, could\ncause great confusion if it was not done carefully.",
  "There are obvious tradeoffs among these schemes. Circuit switching requires\nsubstantial setup time and may waste network bandwidth, but it incurs\nless overhead for shipping each message. Conversely, message and packet\nswitching require less setup time but incur more overhead per message. Also,\nin packet switching, each message must be divided into packets and later\nreassembled. Packet switching is the method most commonly used on data\nnetworks because it makes the best use of network bandwidth.\n17.5 Communication Protocols\nWhen we are designing a communication network, we must deal with the\ninherent complexity of coordinating asynchronous operations communicating\nin a potentially slow and error-prone environment. In addition, the systems on",
  "the network must agree on a protocol or a set of protocols for determining\nhost names, locating hosts on the network, establishing connections, and\nso on. We can simplify the design problem (and related implementation)\nby partitioning the problem into multiple layers. Each layer on one system\ncommunicates with the equivalent layer on other systems. Typically, each layer\nhas its own protocols, and communication takes place between peer layers\nusing a speciﬁc protocol. The protocols may be implemented in hardware or\nsoftware. For instance, Figure 17.5 shows the logical communications between\ntwo computers, with the three lowest-level layers implemented in hardware.\nThe International Standards Organization created the OSI model for",
  "describing the various layers of networking. While these layers are not imple-\nmented in practice, they are useful for understanding how networking logically\nworks, and we describe them below:\nreal systems environment\nOSI environment\nnetwork environment\ndata network\ncomputer A\napplication layer\npresentation layer\nsession layer\ntransport layer\nnetwork layer\nlink layer\nphysical layer\nAP\ncomputer B\nA-L (7) \nP-L (6) \nS-L (5) \nT-L (4) \nN-L (3) \nL-L (2) \nP-L (1)\nAP\nFigure 17.5\nTwo computers communicating via the OSI network model. 17.5\nCommunication Protocols\n757\n1. Layer 1: Physical layer. The physical layer is responsible for handling\nboth the mechanical and the electrical details of the physical transmission\nof a bit stream. At the physical layer, the communicating systems must",
  "agree on the electrical representation of a binary 0 and 1, so that when data\nare sent as a stream of electrical signals, the receiver is able to interpret the\ndata properly as binary data. This layer is implemented in the hardware\nof the networking device. It is responsible for delivering bits.\n2. Layer 2: Data-link layer. The data-link layer is responsible for handling\nframes, or ﬁxed-length parts of packets, including any error detection\nand recovery that occurs in the physical layer. It sends frames between\nphysical addresses.\n3. Layer 3: Network layer. The network layer is responsible for breaking\nmessages into packets, providing connections between logical addresses,\nand routing packets in the communication network, including handling",
  "the addresses of outgoing packets, decoding the addresses of incoming\npackets, and maintaining routing information for proper response to\nchanging load levels. Routers work at this layer.\n4. Layer 4: Transport layer. The transport layer is responsible for transfer of\nmessages between nodes, including partitioning messages into packets,\nmaintaining packet order, and controlling ﬂow to avoid congestion.\n5. Layer 5: Session layer. The session layer is responsible for implementing\nsessions, or process-to-process communication protocols.\n6. Layer 6: Presentation layer. The presentation layer is responsible for\nresolving the differences in formats among the various sites in the\nnetwork, including character conversions and half duplex–full duplex\nmodes (character echoing).",
  "modes (character echoing).\n7. Layer 7: Application layer. The application layer is responsible for inter-\nacting directly with users. This layer deals with ﬁle transfer, remote-login\nprotocols, and electronic mail, as well as with schemas for distributed\ndatabases.\nFigure 17.6 summarizes the OSI protocol stack—a set of cooperating\nprotocols—showing the physical ﬂow of data. As mentioned, logically each\nlayer of a protocol stack communicates with the equivalent layer on other\nsystems. But physically, a message starts at or above the application layer and\nis passed through each lower level in turn. Each layer may modify the message\nand include message-header data for the equivalent layer on the receiving\nside. Ultimately, the message reaches the data-network layer and is transferred",
  "as one or more packets (Figure 17.7). The data-link layer of the target system\nreceives these data, and the message is moved up through the protocol stack.\nIt is analyzed, modiﬁed, and stripped of headers as it progresses. It ﬁnally\nreaches the application layer for use by the receiving process.\nThe OSI model formalizes some of the earlier work done in network\nprotocols but was developed in the late 1970s and is currently not in widespread\nuse. Perhaps the most widely adopted protocol stack is the TCP/IPmodel, which\nhas been adopted by virtually all Internet sites. The TCP/IP protocol stack has\nfewer layers than the OSI model. Theoretically, because it combines several 758\nChapter 17\nDistributed Systems\ndata-communication network\nend-to-end message transfer",
  "end-to-end message transfer\n(connection management, error control,\nfragmentation, flow control)\nphysical connection to \nnetwork termination equipment\ntransport layer\nnetwork routing, addressing,\ncall set-up and clearing\ntransfer-syntax negotiation\ndata-representation transformations\nnetwork-independent\nmessage-interchange service\npresentation layer\nfile transfer, access, and management; \ndocument and message interchange; \njob transfer and manipulation\nsyntax-independent message\ninterchange service\nend-user application process\ndistributed information \nservices\napplication layer\ndialog and synchronization\ncontrol for application entities\nsession layer\nnetwork layer\nlink layer\nphysical layer\ndata-link control\n(framing, data transparency, error control)\nmechanical and electrical",
  "mechanical and electrical\nnetwork-interface connections\nFigure 17.6\nThe OSI protocol stack.\nfunctions in each layer, it is more difﬁcult to implement but more efﬁcient than\nOSI networking. The relationship between the OSI and TCP/IP models is shown\nin Figure 17.8.\nThe TCP/IP application layer identiﬁes several protocols in widespread use\nin the Internet, including HTTP, FTP, Telnet, ssh, DNS, and SMTP. The transport\nlayer identiﬁes the unreliable, connectionless user datagram protocol (UDP)\nand the reliable, connection-oriented transmission control protocol (TCP).\nThe Internet protocol (IP) is responsible for routing IP datagrams through the\nInternet. The TCP/IP model does not formally identify a link or physical layer,",
  "allowing TCP/IP trafﬁc to run across any physical network. In Section 17.6, we\nconsider the TCP/IP model running over an Ethernet network.\nSecurity should be a concern in the design and implementation of any\nmodern communication protocol. Both strong authentication and encryption\nare needed for secure communication. Strong authentication ensures that\nthe sender and receiver of a communication are who or what they are 17.5\nCommunication Protocols\n759\ndata-link-layer header\nnetwork-layer header\ntransport-layer header\nsession-layer header\npresentation layer\napplication layer\nmessage\ndata-link-layer trailer\nFigure 17.7\nAn OSI network message.\nsupposed to be. Encryption protects the contents of the communication\nfrom eavesdropping. Weak authentication and clear-text communication are",
  "still very common, however, for a variety of reasons. When most of the\ncommon protocols were designed, security was frequently less important than\nperformance, simplicity, and efﬁciency. This legacy is still showing itself today,\ndata link\ntransport\napplication\nHTTP, DNS, Telnet\nSMTP, FTP\nnot defined\nnot defined\nnot defined\nOSI\nTCP/IP\nnot defined\nIP\nTCP-UDP\nphysical\nnetwork\nsession\npresentation\nFigure 17.8\nThe OSI and TCP/IP protocol stacks. 760\nChapter 17\nDistributed Systems\nas adding security to existing infrastructure is proving to be difﬁcult and\ncomplex.\nStrong authentication requires a multistep handshake protocol or authen-\ntication devices, adding complexity to a protocol. Modern CPUs can efﬁciently\nperform encryption, frequently including cryptographic acceleration instruc-",
  "tions, so system performance is not compromised. Long-distance communica-\ntion can be made secure by authenticating the endpoints and encrypting the\nstream of packets in a virtual private network, as discussed in Section 15.4.2.\nLAN communication remains unencrypted at most sites, but protocols such\nas NFS Version 4, which includes strong native authentication and encryption,\nshould help improve even LAN security.\n17.6 An Example: TCP/IP\nWe now return to the name-resolution issue raised in Section 17.4.1 and\nexamine its operation with respect to the TCP/IP protocol stack on the Internet.\nThen we consider the processing needed to transfer a packet between hosts\non different Ethernet networks. We base our description on the IPV4 protocols,\nwhich are the type most commonly used today.",
  "which are the type most commonly used today.\nIn a TCP/IP network, every host has a name and an associated IP address\n(or host-id). Both of these strings must be unique; and so that the name space\ncan be managed, they are segmented. The name is hierarchical (as explained\nin Section 17.4.1), describing the host name and then the organization with\nwhich the host is associated. The host-id is split into a network number and a\nhost number. The proportion of the split varies, depending on the size of the\nnetwork. Once the Internet administrators assign a network number, the site\nwith that number is free to assign host-ids.\nThe sending system checks its routing tables to locate a router to send the\nframe on its way. The routers use the network part of the host-id to transfer",
  "the packet from its source network to the destination network. The destination\nsystem then receives the packet. The packet may be a complete message, or it\nmay just be a component of a message, with more packets needed before the\nmessage can be reassembled and passed to the TCP/UDP layer for transmission\nto the destination process.\nWithin a network, how does a packet move from sender (host or router) to\nreceiver? Every Ethernet device has a unique byte number, called the medium\naccess control (MAC) address, assigned to it for addressing. Two devices on a\nLAN communicate with each other only with this number. If a system needs\nto send data to another system, the networking software generates an address\nresolution protocol (ARP) packet containing the IP address of the destination",
  "system. This packet is broadcast to all other systems on that Ethernet network.\nA broadcast uses a special network address (usually, the maximum\naddress) to signal that all hosts should receive and process the packet. The\nbroadcast is not re-sent by gateways, so only systems on the local network\nreceive it. Only the system whose IP address matches the IP address of the ARP\nrequest responds and sends back its MAC address to the system that initiated\nthe query. For efﬁciency, the host caches the IP– MAC address pair in an internal\ntable. The cache entries are aged, so that an entry is eventually removed from\nthe cache if an access to that system is not required within a given time. In 17.6\nAn Example: TCP/IP\n761\npreamble—start of packet\nstart of frame delimiter\ndestination address",
  "start of frame delimiter\ndestination address\nsource address\nlength of data section\npad (optional)\nframe checksum\nbytes\n7 \n1 \n2 or 6 \n2 or 6 \n2 \neach byte pattern 10101010\npattern 10101011\nEthernet address or broadcast\nEthernet address\nlength in bytes\nmessage data\nmessage must be > 63 bytes long\nfor error detection\n0–1500\n0–46\n4\ndata\nFigure 17.9\nAn Ethernet packet.\nthis way, hosts that are removed from a network are eventually forgotten. For\nadded performance, ARP entries for heavily used hosts may be pinned in the\nARP cache.\nOnce an Ethernet device has announced its host-id and address, commu-\nnication can begin. A process may specify the name of a host with which to\ncommunicate. Networking software takes that name and determines the IP",
  "address of the target, using a DNS lookup. The message is passed from the\napplication layer, through the software layers, and to the hardware layer. At\nthe hardware layer, the packet (or packets) has the Ethernet address at its start;\na trailer indicates the end of the packet and contains a checksum for detection\nof packet damage (Figure 17.9). The packet is placed on the network by the\nEthernet device. The data section of the packet may contain some or all of the\ndata of the original message, but it may also contain some of the upper-level\nheaders that compose the message. In other words, all parts of the original\nmessage must be sent from source to destination, and all headers above the\n802.3 layer (data-link layer) are included as data in the Ethernet packets.",
  "If the destination is on the same local network as the source, the system\ncan look in its ARP cache, ﬁnd the Ethernet address of the host, and place the\npacket on the wire. The destination Ethernet device then sees its address in the\npacket and reads in the packet, passing it up the protocol stack.\nIf the destination system is on a network different from that of the source,\nthe source system ﬁnds an appropriate router on its network and sends the\npacket there. Routers then pass the packet along the WAN until it reaches its\ndestination network. The router that connects the destination network checks\nits ARP cache, ﬁnds the Ethernet number of the destination, and sends the\npacket to that host. Through all of these transfers, the data-link-layer header",
  "may change as the Ethernet address of the next router in the chain is used, but\nthe other headers of the packet remain the same until the packet is received\nand processed by the protocol stack and ﬁnally passed to the receiving process\nby the kernel. 762\nChapter 17\nDistributed Systems\n17.7\nRobustness\nA distributed system may suffer from various types of hardware failure. The\nfailure of a link, the failure of a site, and the loss of a message are the most\ncommon types. To ensure that the system is robust, we must detect any of these\nfailures, reconﬁgure the system so that computation can continue, and recover\nwhen a site or a link is repaired.\n17.7.1\nFailure Detection\nIn an environment with no shared memory, we are generally unable to",
  "differentiate among link failure, site failure, and message loss. We can usually\ndetect only that one of these failures has occurred. Once a failure has been\ndetected, appropriate action must be taken. What action is appropriate depends\non the particular application.\nTo detect link and site failure, we use a heartbeat procedure. Suppose that\nsites A and B have a direct physical link between them. At ﬁxed intervals, the\nsites send each other an I-am-up message. If site A does not receive this message\nwithin a predetermined time period, it can assume that site B has failed, that\nthe link between A and B has failed, or that the message from B has been lost.\nAt this point, site A has two choices. It can wait for another time period to",
  "receive an I-am-up message from B, or it can send an Are-you-up? message to B.\nIf time goes by and site A still has not received an I-am-up message, or if site\nA has sent an Are-you-up? message and has not received a reply, the procedure\ncan be repeated. Again, the only conclusion that site A can draw safely is that\nsome type of failure has occurred.\nSite A can try to differentiate between link failure and site failure by sending\nan Are-you-up? message to B by another route (if one exists). If and when B\nreceives this message, it immediately replies positively. This positive reply tells\nA that B is up and that the failure is in the direct link between them. Since we\ndo not know in advance how long it will take the message to travel from A to B",
  "and back, we must use a time-out scheme. At the time A sends the Are-you-up?\nmessage, it speciﬁes a time interval during which it is willing to wait for the\nreply from B. If A receives the reply message within that time interval, then it\ncan safely conclude that B is up. If not, however (that is, if a time-out occurs),\nthen A may conclude only that one or more of the following situations has\noccurred:\n• Site B is down.\n• The direct link (if one exists) from A to B is down.\n• The alternative path from A to B is down.\n• The message has been lost.\nSite A cannot, however, determine which of these events has occurred.\n17.7.2\nReconﬁguration\nSuppose that site A has discovered, through the mechanism just described,\nthat a failure has occurred. It must then initiate a procedure that will allow the",
  "system to reconﬁgure and to continue its normal mode of operation. 17.7\nRobustness\n763\n• If a direct link from A to B has failed, this information must be broadcast to\nevery site in the system, so that the various routing tables can be updated\naccordingly.\n• If the system believes that a site has failed (because that site can be reached\nno longer), then all sites in the system must be notiﬁed, so that they will\nno longer attempt to use the services of the failed site. The failure of a site\nthat serves as a central coordinator for some activity (such as deadlock\ndetection) requires the election of a new coordinator. Similarly, if the failed\nsite is part of a logical ring, then a new logical ring must be constructed.",
  "Note that, if the site has not failed (that is, if it is up but cannot be reached),\nthen we may have the undesirable situation in which two sites serve as the\ncoordinator. When the network is partitioned, the two coordinators (each\nfor its own partition) may initiate conﬂicting actions. For example, if the\ncoordinators are responsible for implementing mutual exclusion, we may\nhave a situation in which two processes are executing simultaneously in\ntheir critical sections.\n17.7.3\nRecovery from Failure\nWhen a failed link or site is repaired, it must be integrated into the system\ngracefully and smoothly.\n• Suppose that a link between A and B has failed. When it is repaired, both A\nand B must be notiﬁed. We can accomplish this notiﬁcation by continuously",
  "repeating the heartbeat procedure described in Section 17.7.1.\n• Suppose that site B has failed. When it recovers, it must notify all other\nsites that it is up again. Site B then may have to receive information from\nthe other sites to update its local tables. For example, it may need routing-\ntable information, a list of sites that are down, undelivered messages, a\ntransaction log of unexecuted transactions, and mail. If the site has not\nfailed but simply could not be reached, then it still needs this information.\n17.7.4\nFault Tolerance\nA distributed system must tolerate a certain level of failure and continue to\nfunction normally when faced with various types of failures. Making a facility\nfault tolerant starts at the protocol level, as described above, but continues",
  "through all aspects of the system. We use the term fault tolerance in a broad\nsense. Communication faults, certain machine failures, storage-device crashes,\nand decays of storage media should all be tolerated to some extent. A fault-\ntolerant system should continue to function, perhaps in a degraded form, when\nfaced with such failures. The degradation can affect performance, functionality,\nor both. It should be proportional, however, to the failures that caused it. A\nsystem that grinds to a halt when only one of its components fails is certainly\nnot fault tolerant.\nUnfortunately, fault tolerance can be difﬁcult and expensive to implement.\nAt the network layer, multiple redundant communication paths and network\ndevices such as switches and routers are needed to avoid a communication",
  "failure. A storage failure can cause loss of the operating system, applications,\nor data. Storage units can include redundant hardware components that 764\nChapter 17\nDistributed Systems\nautomatically take over from each other in case of failure. In addition, RAID\nsystems can ensure continued access to the data even in the event of one or\nmore disk failures (Section 10.7).\nA system failure without redundancy can cause an application or an entire\nfacility to stop operation. The most simple system failure involves a system\nrunning only stateless applications. These applications can be restarted without\ncompromising the operation; so as long as the applications can run on more\nthan one computer (node), operation can continue. Such a facility is commonly",
  "known as a compute cluster because it centers on computation.\nIn contrast, datacentric systems involve running applications that access\nand modify shared data. As a result, datacentric computing facilities are more\ndifﬁcult to make fault tolerant. They require failure-monitoring software and\nspecial infrastructure. For instance, high-availability clusters include two or\nmore computers and a set of shared disks. Any given application can be\nstored on the computers or on the shared disk, but the data must be stored\non the shared disk. The running application’s node has exclusive access to\nthe application’s data on disk. The application is monitored by the cluster\nsoftware, and if it fails it is automatically restarted. If it cannot be restarted, or",
  "if the entire computer fails, the node’s exclusive access to the application’s data\nis terminated and is granted to another node in the cluster. The application is\nrestarted on that new node. The application loses whatever state information\nwas in the failed system’s memory but can continue based on whatever state\nit last wrote to the shared disk. From a user’s point of view, a service was\ninterrupted and then restarted, possibly with some data missing.\nSpeciﬁc applications may improve on this functionality by implementing\nlock management along with clustering. With lock management, the applica-\ntion can run on multiple nodes and can use the same data on shared disks\nconcurrently. Clustered databases frequently implement this functionality. If",
  "a node fails, transactions can continue on other nodes, and users notice no\ninterruption of service, as long as the client is able to automatically locate the\nother nodes in the cluster. Any noncommitted transactions on the failed node\nare lost, but again, client applications can be designed to retry noncommitted\ntransactions if they detect a failure of their database node.\n17.8 Design Issues\nMaking the multiplicity of processors and storage devices transparent to the\nusers has been a key challenge to many designers. Ideally, a distributed system\nshould look to its users like a conventional, centralized system. The user\ninterface of a transparent distributed system should not distinguish between\nlocal and remote resources. That is, users should be able to access remote",
  "resources as though these resources were local, and the distributed system\nshould be responsible for locating the resources and for arranging for the\nappropriate interaction.\nAnother aspect of transparency is user mobility. It would be convenient to\nallow users to log into any machine in the system rather than forcing them to use\na speciﬁc machine. A transparent distributed system facilitates user mobility\nby bringing over the user’s environment (for example, home directory) to\nwherever he logs in. Protocols like LDAP provide an authentication system for 17.9\nDistributed File Systems\n765\nlocal, remote, and mobile users. Once the authentication is complete, facilities\nlike desktop virtualization allow users to see their desktop sessions at remote\nfacilities.",
  "facilities.\nStill another issue is scalability—the capability of a system to adapt to\nincreased service load. Systems have bounded resources and can become\ncompletely saturated under increased load. For example, with respect to a ﬁle\nsystem, saturation occurs either when a server’s CPU runs at a high utilization\nrate or when disks’ I/O requests overwhelm the I/O subsystem. Scalability\nis a relative property, but it can be measured accurately. A scalable system\nreacts more gracefully to increased load than does a nonscalable one. First,\nits performance degrades more moderately; and second, its resources reach a\nsaturated state later. Even perfect design cannot accommodate an ever-growing\nload. Adding new resources might solve the problem, but it might generate",
  "additional indirect load on other resources (for example, adding machines to\na distributed system can clog the network and increase service loads). Even\nworse, expanding the system can call for expensive design modiﬁcations. A\nscalable system should have the potential to grow without these problems. In\na distributed system, the ability to scale up gracefully is of special importance,\nsince expanding the network by adding new machines or interconnecting two\nnetworks is commonplace. In short, a scalable design should withstand high\nservice load, accommodate growth of the user community, and allow simple\nintegration of added resources.\nScalability is related to fault tolerance, discussed earlier. A heavily loaded\ncomponent can become paralyzed and behave like a faulty component. In",
  "addition, shifting the load from a faulty component to that component’s\nbackup can saturate the latter. Generally, having spare resources is essential\nfor ensuring reliability as well as for handling peak loads gracefully. Thus, the\nmultiple resources in a distributed system represent an inherent advantage,\ngiving the system a greater potential for fault tolerance and scalability.\nHowever, inappropriate design can obscure this potential. Fault-tolerance and\nscalability considerations call for a design demonstrating distribution of control\nand data.\nFacilities like the Hadoop distributed ﬁle system were created with this\nproblem in mind. Hadoop is based on Google’s MapReduce and Google\nFile System projects that created a facility to track every web page on the",
  "Internet. Hadoop is an open-source programming framework that supports\nthe processing of large data sets in distributed computing environments.\nTraditional systems with traditional databases cannot scale to the capacity and\nperformance needed by “big data” projects (at least not at reasonable prices).\nExamples of big data projects include mining Twitter for information pertinent\nto a company and sifting ﬁnancial data to look for trends in stock pricing.\nWith Hadoop and its related tools, thousands of systems can work together to\nmanage a distributed database of petabytes of information.\n17.9 Distributed File Systems\nAlthough the World Wide Web is the predominant distributed system in use\ntoday, it is not the only one. Another important and popular use of distributed",
  "computing is the distributed ﬁle system, or DFS. In this section, we discuss 766\nChapter 17\nDistributed Systems\ndistributed ﬁle systems. In doing so, we use two running examples—OpenAFS,\nan open-source distributed ﬁle system, and NFS, the most common UNIX-based\nDFS. NFS has several versions, and here we refer to NFS Version 3 unless\notherwise noted.\nTo explain the structure of a DFS, we need to deﬁne the terms service,\nserver, and client in the DFS context. A service is a software entity running on\none or more machines and providing a particular type of function to clients.\nA server is the service software running on a single machine. A client is\na process that can invoke a service using a set of operations that form its",
  "client interface. Sometimes a lower-level interface is deﬁned for the actual\ncross-machine interaction; it is the intermachine interface.\nUsing this terminology, we say that a ﬁle system provides ﬁle services to\nclients. A client interface for a ﬁle service is formed by a set of primitive ﬁle\noperations, such as create a ﬁle, delete a ﬁle, read from a ﬁle, and write to a ﬁle.\nThe primary hardware component that a ﬁle server controls is a set of local\nsecondary-storage devices (usually, magnetic disks) on which ﬁles are stored\nand from which they are retrieved according to the clients’ requests.\nA DFS is a ﬁle system whose clients, servers, and storage devices are\ndispersed among the machines of a distributed system. Accordingly, service",
  "activity has to be carried out across the network. Instead of a single centralized\ndata repository, the system frequently has multiple and independent storage\ndevices. As you will see, the concrete conﬁguration and implementation of a\nDFS may vary from system to system. In some conﬁgurations, servers run on\ndedicated machines. In others, a machine can be both a server and a client. A DFS\ncan be implemented as part of a distributed operating system or, alternatively,\nby a software layer whose task is to manage the communication between\nconventional operating systems and ﬁle systems.\nThe distinctive features of a DFS are the multiplicity and autonomy of\nclients and servers in the system. Ideally, though, a DFS should appear to its",
  "clients to be a conventional, centralized ﬁle system. That is, the client interface\nof a DFS should not distinguish between local and remote ﬁles. It is up to the\nDFS to locate the ﬁles and to arrange for the transport of the data. A transparent\nDFS—like the transparent distributed systems mentioned earlier—facilitates\nuser mobility by bringing a user’s environment (that is, home directory) to\nwherever the user logs in.\nThe most important performance measure of a DFS is the amount of time\nneeded to satisfy service requests. In conventional systems, this time consists of\ndisk-access time and a small amount of CPU-processing time. In a DFS, however,\na remote access has the additional overhead associated with the distributed",
  "structure. This overhead includes the time to deliver the request to a server, as\nwell as the time to get the response across the network back to the client. For\neach direction, in addition to the transfer of the information, there is the CPU\noverhead of running the communication protocol software. The performance\nof a DFS can be viewed as another dimension of the DFS’s transparency. That is,\nthe performance of an ideal DFS would be comparable to that of a conventional\nﬁle system.\nThe fact that a DFS manages a set of dispersed storage devices is the DFS’s\nkey distinguishing feature. The overall storage space managed by a DFS is\ncomposed of different and remotely located smaller storage spaces. Usually,\nthese constituent storage spaces correspond to sets of ﬁles. A component unit 17.9",
  "Distributed File Systems\n767\nis the smallest set of ﬁles that can be stored on a single machine, independently\nfrom other units. All ﬁles belonging to the same component unit must reside\nin the same location.\n17.9.1\nNaming and Transparency\nNaming is a mapping between logical and physical objects. For instance,\nusers deal with logical data objects represented by ﬁle names, whereas the\nsystem manipulates physical blocks of data stored on disk tracks. Usually, a\nuser refers to a ﬁle by a textual name. The latter is mapped to a lower-level\nnumerical identiﬁer that in turn is mapped to disk blocks. This multilevel\nmapping provides users with an abstraction of a ﬁle that hides the details of\nhow and where on the disk the ﬁle is stored.",
  "how and where on the disk the ﬁle is stored.\nIn a transparent DFS, a new dimension is added to the abstraction: that of\nhiding where in the network the ﬁle is located. In a conventional ﬁle system, the\nrange of the naming mapping is an address within a disk. In a DFS, this range\nis expanded to include the speciﬁc machine on whose disk the ﬁle is stored.\nGoing one step further with the concept of treating ﬁles as abstractions leads\nto the possibility of ﬁle replication. Given a ﬁle name, the mapping returns a\nset of the locations of this ﬁle’s replicas. In this abstraction, both the existence\nof multiple copies and their locations are hidden.\n17.9.1.1\nNaming Structures\nWe need to differentiate two related notions regarding name mappings in a\nDFS:",
  "DFS:\n1. Location transparency. The name of a ﬁle does not reveal any hint of the\nﬁle’s physical storage location.\n2. Location independence. The name of a ﬁle does not need to be changed\nwhen the ﬁle’s physical storage location changes.\nBoth deﬁnitions relate to the level of naming discussed previously, since ﬁles\nhave different names at different levels (that is, user-level textual names and\nsystem-level numerical identiﬁers). A location-independent naming scheme is\na dynamic mapping, since it can map the same ﬁle name to different locations\nat two different times. Therefore, location independence is a stronger property\nthan is location transparency.\nIn practice, most of the current DFSs provide a static, location-transparent",
  "mapping for user-level names. Some support ﬁle migration—that is, changing\nthe location of a ﬁle automatically, providing location independence. OpenAFS\nsupports location independence and ﬁle mobility, for example. The Hadoop\ndistributed ﬁle system (HDFS)—a special ﬁle system written for the Hadoop\nframework—is a more recent creation. It includes ﬁle migration but does\nso without following POSIX standards, providing more ﬂexibility in imple-\nmentation and interface. HDFS keeps track of the location of data but hides\nthis information from clients. This dynamic location transparency allows the\nunderlying mechanism to self-tune. In another example, Amazon’s §3 cloud\nstorage facility provides blocks of storage on demand via APIs, placing the",
  "storage where it sees ﬁt and moving the data as necessary to meet performance,\nreliability, and capacity requirements. 768\nChapter 17\nDistributed Systems\nA few aspects can further differentiate location independence and static\nlocation transparency:\n• Divorce of data from location, as exhibited by location independence,\nprovides a better abstraction for ﬁles. A ﬁle name should denote the ﬁle’s\nmost signiﬁcant attributes, which are its contents rather than its location.\nLocation-independent ﬁles can be viewed as logical data containers that\nare not attached to a speciﬁc storage location. If only static location\ntransparency is supported, the ﬁle name still denotes a speciﬁc, although\nhidden, set of physical disk blocks.\n• Static location transparency provides users with a convenient way to",
  "share data. Users can share remote ﬁles by simply naming the ﬁles in a\nlocation-transparent manner, as though the ﬁles were local. Dropbox and\nother cloud-based storage solutions work this way. Location independence\npromotes sharing the storage space itself, as well as the data objects. When\nﬁles can be mobilized, the overall, system-wide storage space looks like\na single virtual resource. A possible beneﬁt is the ability to balance the\nutilization of storage across the system.\n• Location independence separates the naming hierarchy from the storage-\ndevices hierarchy and from the intercomputer structure. By contrast, if\nstatic location transparency is used (although names are transparent),\nwe can easily expose the correspondence between component units and",
  "machines. The machines are conﬁgured in a pattern similar to the naming\nstructure. This conﬁguration may restrict the architecture of the system\nunnecessarily and conﬂict with other considerations. A server in charge of\na root directory is an example of a structure that is dictated by the naming\nhierarchy and contradicts decentralization guidelines.\nOnce the separation of name and location has been completed, clients\ncan access ﬁles residing on remote server systems. In fact, these clients may\nbe diskless and rely on servers to provide all ﬁles, including the operating-\nsystem kernel. Special protocols are needed for the boot sequence, however.\nConsider the problem of getting the kernel to a diskless workstation. The",
  "diskless workstation has no kernel, so it cannot use the DFS code to retrieve\nthe kernel. Instead, a special boot protocol, stored in read-only memory (ROM)\non the client, is invoked. It enables networking and retrieves only one special\nﬁle (the kernel or boot code) from a ﬁxed location. Once the kernel is copied\nover the network and loaded, its DFS makes all the other operating-system ﬁles\navailable. The advantages of diskless clients are many, including lower cost\n(because the client machines require no disks) and greater convenience (when\nan operating-system upgrade occurs, only the server needs to be modiﬁed).\nThe disadvantages are the added complexity of the boot protocols and the\nperformance loss resulting from the use of a network rather than a local disk.\n17.9.1.2\nNaming Schemes",
  "17.9.1.2\nNaming Schemes\nThere are three main approaches to naming schemes in a DFS. In the simplest\napproach, a ﬁle is identiﬁed by some combination of its host name and local\nname, which guarantees a unique system-wide name. In Ibis, for instance, a 17.9\nDistributed File Systems\n769\nﬁle is identiﬁed uniquely by the name host:local-name, where local-name is a\nUNIX-like path. The Internet URL system also uses this approach. This naming\nscheme is neither location transparent nor location independent. The DFS is\nstructured as a collection of isolated component units, each of which is an\nentire conventional ﬁle system. Component units remain isolated, although\nmeans are provided to refer to remote ﬁles. We do not consider this scheme\nany further here.",
  "any further here.\nThe second approach was popularized by Sun’s network ﬁle system,\nNFS. NFS is found in many systems, including UNIX and Linux distributions.\nNFS provides a means to attach remote directories to local directories, thus\ngiving the appearance of a coherent directory tree. Early NFS versions allowed\nonly previously mounted remote directories to be accessed transparently. The\nadvent of the automount feature allowed mounts to be done on demand\nbased on a table of mount points and ﬁle-structure names. Components are\nintegrated to support transparent sharing, but this integration is limited and is\nnot uniform, because each machine may attach different remote directories to\nits tree. The resulting structure is versatile.",
  "its tree. The resulting structure is versatile.\nWe can achieve total integration of the component ﬁle systems by using\nthe third approach. Here, a single global name structure spans all the ﬁles\nin the system. Ideally, the composed ﬁle-system structure is the same as the\nstructure of a conventional ﬁle system. In practice, however, the many special\nﬁles (for example, UNIX device ﬁles and machine-speciﬁc binary directories)\nmake this goal difﬁcult to attain. To evaluate naming structures, we look\nat their administrative complexity. The most complex and most difﬁcult-to-\nmaintain structure is the NFS structure. Because any remote directory can be\nattached anywhere onto the local directory tree, the resulting hierarchy can",
  "be highly unstructured. If a server becomes unavailable, some arbitrary set of\ndirectories on different machines becomes unavailable. In addition, a separate\naccreditation mechanism controls which machine is allowed to attach which\ndirectory to its tree. Thus, a user might be able to access a remote directory tree\non one client but be denied access on another client.\n17.9.1.3\nImplementation Techniques\nImplementation of transparent naming requires a provision for the mapping\nof a ﬁle name to the associated location. To keep this mapping manageable,\nwe must aggregate sets of ﬁles into component units and provide the mapping\non a component-unit basis rather than on a single-ﬁle basis. This aggregation\nserves administrative purposes as well. UNIX-like systems use the hierarchical",
  "directory tree to provide name-to-location mapping and to aggregate ﬁles\nrecursively into directories.\nTo enhance the availability of the crucial mapping information, we can\nuse replication, local caching, or both. As we noted, location independence\nmeans that the mapping changes over time. Hence, replicating the mapping\nmakes a simple yet consistent update of this information impossible. To\novercome this obstacle, we can introduce low-level, location-independent\nﬁle identiﬁers. (OpenAFS uses this approach.) Textual ﬁle names are mapped\nto lower-level ﬁle identiﬁers that indicate to which component unit the ﬁle\nbelongs. These identiﬁers are still location independent. They can be replicated\nand cached freely without being invalidated by migration of component 770\nChapter 17",
  "Chapter 17\nDistributed Systems\nunits. The inevitable price is the need for a second level of mapping, which\nmaps component units to locations and needs a simple yet consistent update\nmechanism. Implementing UNIX-like directory trees using these low-level,\nlocation-independent identiﬁers makes the whole hierarchy invariant under\ncomponent-unit migration. The only aspect that does change is the component-\nunit location mapping.\nA common way to implement low-level identiﬁers is to use structured\nnames. These names are bit strings that usually have two parts. The ﬁrst\npart identiﬁes the component unit to which the ﬁle belongs; the second part\nidentiﬁes the particular ﬁle within the unit. Variants with more parts are",
  "possible. The invariant of structured names, however, is that individual parts of\nthe name are unique at all times only within the context of the rest of the parts.\nWe can obtain uniqueness at all times by taking care not to reuse a name that is\nstill in use, by adding sufﬁciently more bits (this method is used in OpenAFS), or\nby using a timestamp as one part of the name (as was done in Apollo Domain).\nAnother way to view this process is that we are taking a location-transparent\nsystem, such as Ibis, and adding another level of abstraction to produce a\nlocation-independent naming scheme.\n17.9.2\nRemote File Access\nNext, let’s consider a user who requests access to a remote ﬁle. The server\nstoring the ﬁle has been located by the naming scheme, and now the actual",
  "data transfer must take place.\nOne way to achieve this transfer is through a remote-service mechanism,\nwhereby requests for accesses are delivered to the server, the server machine\nperforms the accesses, and their results are forwarded back to the user. One of\nthe most common ways of implementing remote service is the RPC paradigm,\nwhich we discussed in Chapter 3. A direct analogy exists between disk-access\nmethods in conventional ﬁle systems and the remote-service method in a DFS:\nusing the remote-service method is analogous to performing a disk access for\neach access request.\nTo ensure reasonable performance of a remote-service mechanism, we can\nuse a form of caching. In conventional ﬁle systems, the rationale for caching is",
  "to reduce disk I/O (thereby increasing performance), whereas in DFSs, the goal\nis to reduce both network trafﬁc and disk I/O. In the following discussion, we\ndescribe the implementation of caching in a DFS and contrast it with the basic\nremote-service paradigm.\n17.9.2.1\nBasic Caching Scheme\nThe concept of caching is simple. If the data needed to satisfy the access\nrequest are not already cached, then a copy of those data is brought from\nthe server to the client system. Accesses are performed on the cached copy.\nThe idea is to retain recently accessed disk blocks in the cache, so that repeated\naccesses to the same information can be handled locally, without additional\nnetwork trafﬁc. A replacement policy (for example, the least-recently-used",
  "algorithm) keeps the cache size bounded. No direct correspondence exists\nbetween accesses and trafﬁc to the server. Files are still identiﬁed with one\nmaster copy residing at the server machine, but copies (or parts) of the ﬁle\nare scattered in different caches. When a cached copy is modiﬁed, the changes 17.9\nDistributed File Systems\n771\nneed to be reﬂected on the master copy to preserve the relevant consistency\nsemantics. The problem of keeping the cached copies consistent with the master\nﬁle is the cache-consistency problem, which we discuss in Section 17.9.2.4. DFS\ncaching could just as easily be called network virtual memory. It acts similarly\nto demand-paged virtual memory, except that the backing store usually is a",
  "remote server rather than a local disk. NFS allows the swap space to be mounted\nremotely, so it actually can implement virtual memory over a network, though\nwith a resulting performance penalty.\nThe granularity of the cached data in a DFS can vary from blocks of a ﬁle\nto an entire ﬁle. Usually, more data are cached than are needed to satisfy a\nsingle access, so that many accesses can be served by the cached data. This\nprocedure is much like disk read-ahead (Section 12.6.2). OpenAFS caches ﬁles\nin large chunks (64 KB). The other systems discussed here support caching\nof individual blocks driven by client demand. Increasing the caching unit\nincreases the hit ratio, but it also increases the miss penalty, because each miss",
  "requires more data to be transferred. It increases the potential for consistency\nproblems as well. Selecting the unit of caching involves considering parameters\nsuch as the network transfer unit and the RPC protocol service unit (if an RPC\nprotocol is used). The network transfer unit (for Ethernet, a packet) is about\n1.5 KB, so larger units of cached data need to be disassembled for delivery and\nreassembled on reception.\nBlock size and total cache size are obviously of importance for block-\ncaching schemes. In UNIX-like systems, common block sizes are 4 KB and 8\nKB. For large caches (over 1 MB), large block sizes (over 8 KB) are beneﬁcial. For\nsmaller caches, large block sizes are less beneﬁcial because they result in fewer\nblocks in the cache and a lower hit ratio.\n17.9.2.2",
  "17.9.2.2\nCache Location\nWhere should the cached data be stored—on disk or in main memory? Disk\ncaches have one clear advantage over main-memory caches: they are reliable.\nModiﬁcations to cached data are lost in a crash if the cache is kept in volatile\nmemory. Moreover, ifthe cached dataare kept ondisk, theyare still there during\nrecovery, and there is no need to fetch them again. Main-memory caches have\nseveral advantages of their own, however:\n• Main-memory caches permit workstations to be diskless.\n• Data can be accessed more quickly from a cache in main memory than\nfrom one on a disk.\n• Technology is moving toward larger and less expensive memory. The\nresulting performance speedup is predicted to outweigh the advantages\nof disk caches.",
  "of disk caches.\n• The server caches (used to speed up disk I/O) will be in main memory\nregardless of where user caches are located; if we use main-memory caches\non the user machine, too, we can build a single caching mechanism for use\nby both servers and users.\nMany remote-access implementations can be thought of as hybrids of\ncaching and remote service. In NFS, for instance, the implementation is based on 772\nChapter 17\nDistributed Systems\nremote service but is augmented with client- and server-side memory caching\nfor performance. Similarly, Sprite’s implementation is based on caching, but\nunder certain circumstances, a remote-service method is adopted. Thus, to\nevaluate the two methods, we must evaluate the degree to which either method",
  "is emphasized. The NFS protocol and most implementations do not provide\ndisk caching.\n17.9.2.3\nCache-Update Policy\nThe policy used to write modiﬁed data blocks back to the server’s master copy\nhas a critical effect on the system’s performance and reliability. The simplest\npolicy is to write data through to disk as soon as they are placed in any cache.\nThe advantage of a write-through policy is reliability: little information is\nlost when a client system crashes. However, this policy requires each write\naccess to wait until the information is sent to the server, so it causes poor write\nperformance. Caching with write-through is equivalent to using remote service\nfor write accesses and exploiting caching only for read accesses.",
  "An alternative is the delayed-write policy, also known as write-back\ncaching, where we delay updates to the master copy. Modiﬁcations are written\nto the cache and then are written through to the server at a later time. This\npolicy has two advantages over write-through. First, because writes are made\nto the cache, write accesses complete much more quickly. Second, data may be\noverwritten before they are written back, in which case only the last update\nneeds to be written at all. Unfortunately, delayed-write schemes introduce\nreliability problems, since unwritten data are lost whenever a user machine\ncrashes.\nVariations of the delayed-write policy differ in when modiﬁed data blocks\nare ﬂushed to the server. One alternative is to ﬂush a block when it is about to",
  "be ejected from the client’s cache. This option can result in good performance,\nbut some blocks can reside in the client’s cache a long time before they are\nwritten back to the server. A compromise between this alternative and the\nwrite-through policy is to scan the cache at regular intervals and to ﬂush\nblocks that have been modiﬁed since the most recent scan, just as UNIX scans\nits local cache. Sprite uses this policy with a 30-second interval. NFS uses the\npolicy for ﬁle data, but once a write is issued to the server during a cache\nﬂush, the write must reach the server’s disk before it is considered complete.\nNFS treats metadata (directory data and ﬁle-attribute data) differently. Any\nmetadata changes are issued synchronously to the server. Thus, ﬁle-structure",
  "loss and directory-structure corruption are avoided when a client or the server\ncrashes.\nYet another variation on delayed write is to write data back to the server\nwhen the ﬁle is closed. This write-on-close policy is used in OpenAFS. In the\ncase of ﬁles that are open for short periods or are modiﬁed rarely, this policy\ndoes not signiﬁcantly reduce network trafﬁc. In addition, the write-on-close\npolicy requires the closing process to delay while the ﬁle is written through,\nwhich reduces the performance advantages of delayed writes. For ﬁles that are\nopen for long periods and are modiﬁed frequently, however, the performance\nadvantages of this policy over delayed write with more frequent ﬂushing are\napparent. 17.10\nSummary\n773\n17.9.2.4\nConsistency",
  "apparent. 17.10\nSummary\n773\n17.9.2.4\nConsistency\nA client machine is sometimes faced with the problem of deciding whether a\nlocally cached copy of data is consistent with the master copy (and hence can\nbe used). If the client machine determines that its cached data are out of date,\nit must cache an up-to-date copy of the data before allowing further accesses.\nThere are two approaches to verifying the validity of cached data:\n1. Client-initiated approach. The client initiates a validity check, in which it\ncontacts the server and checks whether the local data are consistent with\nthe master copy. The frequency of the validity checking is the crux of\nthis approach and determines the resulting consistency semantics. It can",
  "range from a check before every access to a check only on ﬁrst access to\na ﬁle (on ﬁle open, basically). Every access coupled with a validity check\nis delayed, compared with an access served immediately by the cache.\nAlternatively, checks can be initiated at ﬁxed time intervals. Depending\non its frequency, the validity check can load both the network and the\nserver.\n2. Server-initiated approach. The server records, for each client, the ﬁles\n(or parts of ﬁles) that it caches. When the server detects a potential\ninconsistency, it must react. A potential for inconsistency occurs when\ntwo different clients in conﬂicting modes cache a ﬁle. If UNIX semantics\n(Section 11.5.3) is implemented, we can resolve the potential inconsistency",
  "by having the server play an active role. The server must be notiﬁed\nwhenever a ﬁle is opened, and the intended mode (read or write) must\nbe indicated for every open. The server can then act when it detects that\na ﬁle has been opened simultaneously in conﬂicting modes by disabling\ncaching for that particular ﬁle. Actually, disabling caching results in\nswitching to a remote-service mode of operation.\nDistributed ﬁle systems are in common use today, providing ﬁle sharing\nwithin LANs and across WANs as well. The complexity of implementing such\na system should not be underestimated, especially considering that it must be\noperating-system independent for widespread adoption and must provide\navailability and good performance in the presence of long distances and\nsometimes-frail networking.",
  "sometimes-frail networking.\n17.10 Summary\nA distributed system is a collection of processors that do not share memory or\na clock. Instead, each processor has its own local memory, and the processors\ncommunicate with one another through various communication lines, such\nas high-speed buses and the Internet. The processors in a distributed system\nvary in size and function. They may include small microprocessors, personal\ncomputers, and large general-purpose computer systems. The processors in\nthe system are connected through a communication network.\nA distributed system provides the user with access to all system resources.\nAccess to a shared resource can be provided by data migration, computation 774\nChapter 17\nDistributed Systems",
  "Chapter 17\nDistributed Systems\nmigration, or process migration. The access can be speciﬁed by the user or\nimplicitly supplied by the operating system and applications.\nCommunications within a distributed system may occur via circuit switch-\ning, message switching, or packet switching. Packet switching is the method\nmost commonly used on data networks. Through these methods, messages\ncan be exchanged by nodes in the system.\nProtocol stacks, as speciﬁed by network layering models, add information\nto a message to ensure that it reaches its destination. A naming system (such\nas DNS) must be used to translate from a host name to a network address, and\nanother protocol (such as ARP) may be needed to translate the network number",
  "to a network device address (an Ethernet address, for instance). If systems are\nlocated on separate networks, routers are needed to pass packets from source\nnetwork to destination network.\nThere are many challenges to overcome for a distributed system to work\ncorrectly. Issues include naming of nodes and processes in the system, fault\ntolerance, error recovery, and scalability.\nA DFS is a ﬁle-service system whose clients, servers, and storage devices\nare dispersed among the sites of a distributed system. Accordingly, service\nactivity has to be carried out across the network; instead of a single centralized\ndata repository, there are multiple independent storage devices.\nIdeally, a DFS should look to its clients like a conventional, centralized",
  "ﬁle system. The multiplicity and dispersion of its servers and storage devices\nshould be transparent. A transparent DFS facilitates client mobility by bringing\nthe client’s environment to the site where the client logs in.\nThere are several approaches to naming schemes in a DFS. In the simplest\napproach, ﬁles are named by some combination of their host name and local\nname, which guarantees a unique system-wide name. Another approach,\npopularized by NFS, provides a means to attach remote directories to local\ndirectories, thus giving the appearance of a coherent directory tree.\nRequests to access a remote ﬁle are usually handled by two complementary\nmethods. With remote service, requests for accesses are delivered to the server.",
  "The server machine performs the accesses, and the results are forwarded back\nto the client. With caching, if the data needed to satisfy the access request are\nnot already cached, then a copy of the data is brought from the server to the\nclient. Accesses are performed on the cached copy. The problem of keeping the\ncached copies consistent with the master ﬁle is the cache-consistency problem.\nPractice Exercises\n17.1\nWhy would it be a bad idea for gateways to pass broadcast packets\nbetween networks? What would be the advantages of doing so?\n17.2\nDiscuss the advantages and disadvantages of caching name transla-\ntions for computers located in remote domains.\n17.3\nWhat are the advantages and disadvantages of using circuit switching?",
  "For what kinds of applications is circuit switching a viable strategy?\n17.4\nWhat are two formidable problems that designers must solve to\nimplement a network system that has the quality of transparency? Exercises\n775\n17.5\nProcess migration within a heterogeneous network is usually impos-\nsible, given the differences in architectures and operating systems.\nDescribe a method for process migration across different architectures\nrunning:\na.\nThe same operating system\nb.\nDifferent operating systems\n17.6\nTo build a robust distributed system, you must know what kinds of\nfailures can occur.\na.\nList three possible types of failure in a distributed system.\nb.\nSpecify which of the entries in your list also are applicable to a\ncentralized system.\n17.7",
  "centralized system.\n17.7\nIs it always crucial to know that the message you have sent has arrived\nat its destination safely? If your answer is “yes,” explain why. If your\nanswer is “no,” give appropriate examples.\n17.8\nA distributed system has two sites, A and B. Consider whether site A\ncan distinguish among the following:\na.\nB goes down.\nb.\nThe link between A and B goes down.\nc.\nB is extremely overloaded, and its response time is 100 times\nlonger than normal.\nWhat implications does your answer have for recovery in distributed\nsystems?\nExercises\n17.9\nWhat is the difference between computation migration and process\nmigration? Which is easier to implement, and why?\n17.10\nEven though the OSI model of networking speciﬁes seven layers of",
  "functionality, most computer systems use fewer layers to implement a\nnetwork. Why do they use fewer layers? What problems could the use\nof fewer layers cause?\n17.11\nExplain why doubling the speed of the systems on an Ethernet segment\nmay result in decreased network performance. What changes could\nhelp solve this problem?\n17.12\nWhat are the advantages of using dedicated hardware devices for\nrouters and gateways? What are the disadvantages of using these\ndevices compared with using general-purpose computers?\n17.13\nIn what ways is using a name server better than using static host tables?\nWhat problems or complications are associated with name servers?\nWhat methods could you use to decrease the amount of trafﬁc name\nservers generate to satisfy translation requests? 776\nChapter 17",
  "Chapter 17\nDistributed Systems\n17.14\nName servers are organized in a hierarchical manner. What is the\npurpose of using a hierarchical organization?\n17.15\nThe lower layers of the OSI network model provide datagram service,\nwith no delivery guarantees for messages. A transport-layer protocol\nsuch as TCP is used to provide reliability. Discuss the advantages and\ndisadvantages of supporting reliable message delivery at the lowest\npossible layer.\n17.16\nHow does using a dynamic routing strategy affect application behav-\nior? For what type of applications is it beneﬁcial to use virtual routing\ninstead of dynamic routing?\n17.17\nRun the program shown in Figure 17.4 and determine the IP addresses\nof the following host names:\n• www.wiley.com\n• www.cs.yale.edu\n• www.apple.com",
  "• www.wiley.com\n• www.cs.yale.edu\n• www.apple.com\n• www.westminstercollege.edu\n• www.ietf.org\n17.18\nThe original HTTP protocol used TCP/IP as the underlying network\nprotocol. For each page, graphic, or applet, a separate TCP session was\nconstructed, used, and torn down. Because of the overhead of building\nand destroying TCP/IP connections, performance problems resulted\nfrom this implementation method. Would using UDP rather than TCP\nbe a good alternative? What other changes could you make to improve\nHTTP performance?\n17.19\nWhat are the advantages and the disadvantages of making the com-\nputer network transparent to the user?\n17.20\nWhat are the beneﬁts of a DFS compared with a ﬁle system in a\ncentralized system?\n17.21\nWhich of the example DFSs discussed in this chapter would handle a",
  "large, multiclient database application most efﬁciently? Explain your\nanswer.\n17.22\nDiscuss whether OpenAFS and NFS provide the following: (a) location\ntransparency and (b) location independence.\n17.23\nUnder\nwhat\ncircumstances\nwould\na\nclient\nprefer\na\nlocation-\ntransparent DFS? Under what circumstances would she prefer a\nlocation-independent DFS? Discuss the reasons for these preferences.\n17.24\nWhat aspects of a distributed system would you select for a system\nrunning on a totally reliable network?\n17.25\nConsider OpenAFS, which is a stateful distributed ﬁle system. What\nactions need to be performed to recover from a server crash in order to\npreserve the consistency guaranteed by the system? Bibliography\n777\n17.26\nCompare and contrast the techniques of caching disk blocks locally, on",
  "a client system, and remotely, on a server.\n17.27\nOpenAFS is designed to support a large number of clients. Discuss three\ntechniques used to make OpenAFS a scalable system.\n17.28\nWhat are the beneﬁts of mapping objects into virtual memory, as Apollo\nDomain does? What are the drawbacks?\n17.29\nDescribe some of the fundamental differences between OpenAFS and\nNFS (see Chapter 12).\nBibliographical Notes\n[Tanenbaum (2010)] and [Kurose and Ross (2013)] provide general overviews\nof computer networks. The Internet and its protocols are described in [Comer\n(1999)] and [Comer (2000)]. Coverage of TCP/IP can be found in [Fall and\nStevens (2011)] and [Stevens (1995)]. UNIX network programming is described\nthoroughly in [Steven et al. ()] and [Stevens (1998)].",
  "Load balancing and load sharing are discussed by [Harchol-Balter and\nDowney (1997)] and [Vee and Hsu (2000)]. [Harish and Owens (1999)] describe\nload-balancing DNS servers.\nSun’s network ﬁle system (NFS) is described by [Callaghan (2000)] and\n[Sandberg et al. (1985)]. The OpenAFS system is discussed by [Morris et al.\n(1986)], [Howard et al. (1988)], and [Satyanarayanan (1990)]. Information about\nOpenAFS is available from http://www.openafs.org. The Andrew ﬁle system\nis discussed in [Howard et al. (1988)]. The Google MapReduce method is\ndescribed in http://research.google.com/archive/mapreduce.html.\nBibliography\n[Callaghan (2000)]\nB. Callaghan, NFS Illustrated, Addison-Wesley (2000).\n[Comer (1999)]\nD. Comer, Internetworking with TCP/IP, Volume II, Third Edition,\nPrentice Hall (1999).",
  "Prentice Hall (1999).\n[Comer (2000)]\nD. Comer, Internetworking with TCP/IP, Volume I, Fourth Edition,\nPrentice Hall (2000).\n[Fall and Stevens (2011)]\nK. Fall and R. Stevens, TCP/IP Illustrated, Volume 1: The\nProtocols, Second Edition, John Wiley and Sons (2011).\n[Harchol-Balter and Downey (1997)]\nM. Harchol-Balter and A. B. Downey,\n“Exploiting Process Lifetime Distributions for Dynamic Load Balancing”, ACM\nTransactions on Computer Systems, Volume 15, Number 3 (1997), pages 253–285.\n[Harish and Owens (1999)]\nV. C. Harish and B. Owens, “Dynamic Load Balanc-\ning DNS”, Linux Journal, Volume 1999, Number 64 (1999).\n[Howard et al. (1988)]\nJ. H. Howard, M. L. Kazar, S. G. Menees, D. A. Nichols,\nM. Satyanarayanan, and R. N. Sidebotham, “Scale and Performance in a 778\nChapter 17",
  "Chapter 17\nDistributed Systems\nDistributed File System”, ACM Transactions on Computer Systems, Volume 6,\nNumber 1 (1988), pages 55–81.\n[Kurose and Ross (2013)]\nJ. Kurose and K. Ross, Computer Networking—A Top–\nDown Approach, Sixth Edition, Addison-Wesley (2013).\n[Morris et al. (1986)]\nJ. H. Morris, M. Satyanarayanan, M. H. Conner, J. H.\nHoward, D. S. H. Rosenthal, and F. D. Smith, “Andrew: A Distributed Personal\nComputing Environment”, Communications of the ACM, Volume 29, Number 3\n(1986), pages 184–201.\n[Sandberg et al. (1985)]\nR. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and\nB. Lyon, “Design and Implementation of the Sun Network Filesystem”, Proceed-\nings of the Summer USENIX Conference (1985), pages 119–130.\n[Satyanarayanan (1990)]\nM. Satyanarayanan, “Scalable, Secure and Highly",
  "M. Satyanarayanan, “Scalable, Secure and Highly\nAvailable Distributed File Access”, Computer, Volume 23, Number 5 (1990), pages\n9–21.\n[Steven et al. ()]\nR. Steven, B. Fenner, and A. Rudoff, Unix Network Programming,\nVolume 1: The Sockets Networking API, Third Edition), publisher = wiley, year = 2003.\n[Stevens (1995)]\nR. Stevens, TCP/IP Illustrated, Volume 2: The Implementation,\nAddison-Wesley (1995).\n[Stevens (1998)]\nW. R. Stevens, UNIX Network Programming—Volume II, Prentice\nHall (1998).\n[Tanenbaum (2010)]\nA. S. Tanenbaum, Computer Networks, Fifth Edition, Pren-\ntice Hall (2010).\n[Vee and Hsu (2000)]\nV. Vee and W. Hsu, “Locality-Preserving Load-Balancing\nMechanisms for Synchronous Simulations on Shared-Memory Multiproces-",
  "sors”, Proceedings of the Fourteenth Workshop on Parallel and Distributed Simulation\n(2000), pages 131–138. Part Seven\nCase Studies\nIn the ﬁnal part of the book, we integrate the concepts described earlier\nby examining real operating systems. We cover two such systems in\ndetail—Linux and Windows 7. We chose Linux for several reasons: it is\npopular, it is freely available, and it represents a full-featured UNIX system.\nThis gives a student of operating systems an opportunity to read—and\nmodify—real operating-system source code.\nWe also cover Windows 7 in detail. This recent operating system from\nMicrosoft is gaining popularity not only in the standalone-machine market\nbut also in the workgroup–server market. We chose Windows 7 because",
  "it provides an opportunity to study a modern operating system that has\na design and implementation drastically different from those of UNIX.\nIn addition, we brieﬂy discuss other highly inﬂuential operating sys-\ntems. Finally, we provide on-line coverage of two more systems: FreeBSD\nand Mach. The FreeBSD system is another UNIX system. However,\nwhereas Linux combines features from several UNIX systems, FreeBSD\nis based on the BSD model. FreeBSD source code, like Linux source\ncode, is freely available. Mach is a modern operating system that provides\ncompatibility with BSD UNIX.  18\nC H A P T E R\nThe Linux\nSystem\nUpdated by Robert Love\nThis chapter presents an in-depth examination of the Linux operating system.\nBy examining a complete, real system, we can see how the concepts we have",
  "discussed relate both to one another and to practice.\nLinux is a variant of UNIX that has gained popularity over the last several\ndecades, powering devices as small as mobile phones and as large as room-\nﬁlling supercomputers. In this chapter, we look at the history and development\nof Linux and cover the user and programmer interfaces that Linux presents\n—interfaces that owe a great deal to the UNIX tradition. We also discuss the\ndesign and implementation of these interfaces. Linux is a rapidly evolving\noperating system. This chapter describes developments through the Linux 3.2\nkernel, which was released in 2012.\nCHAPTER OBJECTIVES\n• To explore the history of the UNIX operating system from which Linux is\nderived and the principles upon which Linux’s design is based.",
  "• To examine the Linux process model and illustrate how Linux schedules\nprocesses and provides interprocess communication.\n• To look at memory management in Linux.\n• To explore how Linux implements ﬁle systems and manages I/O devices.\n18.1\nLinux History\nLinux looks and feels much like any other UNIX system; indeed, UNIX\ncompatibility has been a major design goal of the Linux project. However,\nLinux is much younger than most UNIX systems. Its development began in\n1991, when a Finnish university student, Linus Torvalds, began developing\na small but self-contained kernel for the 80386 processor, the ﬁrst true 32-bit\nprocessor in Intel’s range of PC-compatible CPUs.\n781 782\nChapter 18\nThe Linux System\nEarly in its development, the Linux source code was made available free—",
  "both at no cost and with minimal distributional restrictions—on the Internet.\nAs a result, Linux’s history has been one of collaboration by many developers\nfrom all around the world, corresponding almost exclusively over the Internet.\nFrom an initial kernel that partially implemented a small subset of the UNIX\nsystem services, the Linux system has grown to include all of the functionality\nexpected of a modern UNIX system.\nIn its early days, Linux development revolved largely around the central\noperating-system kernel—the core, privileged executive that manages all\nsystem resources and interacts directly with the computer hardware. We\nneed much more than this kernel, of course, to produce a full operating\nsystem. We thus need to make a distinction between the Linux kernel and",
  "a complete Linux system. The Linux kernel is an original piece of software\ndeveloped from scratch by the Linux community. The Linux system, as we\nknow it today, includes a multitude of components, some written from scratch,\nothers borrowed from other development projects, and still others created in\ncollaboration with other teams.\nThe basic Linux system is a standard environment for applications and\nuser programming, but it does not enforce any standard means of managing\nthe available functionality as a whole. As Linux has matured, a need has arisen\nfor another layer of functionality on top of the Linux system. This need has\nbeen met by various Linux distributions. A Linux distribution includes all the\nstandard components of the Linux system, plus a set of administrative tools",
  "to simplify the initial installation and subsequent upgrading of Linux and to\nmanage installation and removal of other packages on the system. A modern\ndistribution also typically includes tools for management of ﬁle systems,\ncreation and management of user accounts, administration of networks, Web\nbrowsers, word processors, and so on.\n18.1.1\nThe Linux Kernel\nThe ﬁrst Linux kernel released to the public was version 0.01, dated May 14,\n1991. It had no networking, ran only on 80386-compatible Intel processors\nand PC hardware, and had extremely limited device-driver support. The\nvirtual memory subsystem was also fairly basic and included no support\nfor memory-mapped ﬁles; however, even this early incarnation supported\nshared pages with copy-on-write and protected address spaces. The only ﬁle",
  "system supported was the Minix ﬁle system, as the ﬁrst Linux kernels were\ncross-developed on a Minix platform.\nThe next milestone, Linux 1.0, was released on March 14, 1994. This release\nculminated three years of rapid development of the Linux kernel. Perhaps the\nsingle biggest new feature was networking: 1.0 included support for UNIX’s\nstandard TCP/IP networking protocols, as well as a BSD-compatible socket\ninterface for networking programming. Device-driver support was added for\nrunning IP over Ethernet or (via the PPP or SLIP protocols) over serial lines or\nmodems.\nThe 1.0 kernel also included a new, much enhanced ﬁle system without the\nlimitations of the original Minix ﬁle system, and it supported a range of SCSI",
  "controllers for high-performance disk access. The developers extended the vir-\ntual memory subsystem to support paging to swap ﬁles and memory mapping 18.1\nLinux History\n783\nof arbitrary ﬁles (but only read-only memory mapping was implemented in\n1.0).\nA range of extra hardware support was included in this release. Although\nstill restricted to the Intel PC platform, hardware support had grown to include\nﬂoppy-disk and CD-ROM devices, as well as sound cards, a range of mice, and\ninternational keyboards. Floating-point emulation was provided in the kernel\nfor 80386 users who had no 80387 math coprocessor. System V UNIX-style\ninterprocess communication (IPC), including shared memory, semaphores,\nand message queues, was implemented.",
  "and message queues, was implemented.\nAt this point, development started on the 1.1 kernel stream, but numerous\nbug-ﬁx patches were released subsequently for 1.0. A pattern was adopted as\nthe standard numbering convention for Linux kernels. Kernels with an odd\nminor-version number, such as 1.1 or 2.5, are development kernels; even-\nnumbered minor-version numbers are stable production kernels. Updates\nfor the stable kernels are intended only as remedial versions, whereas the\ndevelopment kernels may include newer and relatively untested functionality.\nAs we will see, this pattern remained in effect until version 3.\nIn March 1995, the 1.2 kernel was released. This release did not offer\nnearly the same improvement in functionality as the 1.0 release, but it did",
  "support a much wider variety of hardware, including the new PCI hardware\nbus architecture. Developers added another PC-speciﬁc feature—support for\nthe 80386 CPU’s virtual 8086 mode—to allow emulation of the DOS operating\nsystem for PC computers. They also updated the IP implementation with\nsupport for accounting and ﬁrewalling. Simple support for dynamically\nloadable and unloadable kernel modules was supplied as well.\nThe 1.2 kernel was the ﬁnal PC-only Linux kernel. The source distribution\nfor Linux 1.2 included partially implemented support for SPARC, Alpha, and\nMIPS CPUs, but full integration of these other architectures did not begin until\nafter the 1.2 stable kernel was released.\nThe Linux 1.2 release concentrated on wider hardware support and more",
  "complete implementations of existing functionality. Much new functionality\nwas under development at the time, but integration of the new code into the\nmain kernel source code was deferred until after the stable 1.2 kernel was\nreleased. As a result, the 1.3 development stream saw a great deal of new\nfunctionality added to the kernel.\nThis work was released in June 1996 as Linux version 2.0. This release\nwas given a major version-number increment because of two major new\ncapabilities: support for multiple architectures, including a 64-bit native Alpha\nport, and symmetricmultiprocessing(SMP) support. Additionally, the memory-\nmanagement code was substantially improved to provide a uniﬁed cache for\nﬁle-system data independent of the caching of block devices. As a result",
  "of this change, the kernel offered greatly increased ﬁle-system and virtual-\nmemory performance. For the ﬁrst time, ﬁle-system caching was extended\nto networked ﬁle systems, and writable memory-mapped regions were also\nsupported. Other major improvements included the addition of internal kernel\nthreads, a mechanism exposing dependencies between loadable modules,\nsupport for the automatic loading of modules on demand, ﬁle-system quotas,\nand POSIX-compatible real-time process-scheduling classes. 784\nChapter 18\nThe Linux System\nImprovements continued with the release of Linux 2.2 in 1999. A port to\nUltraSPARC systems was added. Networking was enhanced with more ﬂexible\nﬁrewalling, improved routing and trafﬁc management, and support for TCP",
  "large window and selective acknowledgement. Acorn, Apple, and NT disks\ncould now be read, and NFS was enhanced with a new kernel-mode NFS\ndaemon. Signal handling, interrupts, and some I/O were locked at a ﬁner\nlevel than before to improve symmetric multiprocessor (SMP) performance.\nAdvances in the 2.4 and 2.6 releases of the kernel included increased\nsupport for SMP systems, journaling ﬁle systems, and enhancements to the\nmemory-management and block I/O systems. The process scheduler was\nmodiﬁed in version 2.6, providing an efﬁcient O(1) scheduling algorithm. In\naddition, the 2.6 kernel was preemptive, allowing a process to be preempted\neven while running in kernel mode.\nLinux kernel version 3.0 was released in July 2011. The major version bump",
  "from 2 to 3 occurred to commemorate the twentieth anniversary of Linux.\nNew features include improved virtualization support, a new page write-back\nfacility, improvements to the memory-management system, and yet another\nnew process scheduler—the Completely Fair Scheduler (CFS). We focus on this\nnewest kernel in the remainder of this chapter.\n18.1.2\nThe Linux System\nAs we noted earlier, the Linux kernel forms the core of the Linux project, but\nother components make up a complete Linux operating system. Whereas the\nLinux kernel is composed entirely of code written from scratch speciﬁcally for\nthe Linux project, much of the supporting software that makes up the Linux\nsystem is not exclusive to Linux but is common to a number of UNIX-like",
  "operating systems. In particular, Linux uses many tools developed as part\nof Berkeley’s BSD operating system, MIT’s X Window System, and the Free\nSoftware Foundation’s GNU project.\nThis sharing of tools has worked in both directions. The main system\nlibraries of Linux were originated by the GNU project, but the Linux community\ngreatly improved the libraries by addressing omissions, inefﬁciencies, and\nbugs. Other components, such as the GNU C compiler (gcc), were already of\nsufﬁciently high quality to be used directly in Linux. The network administra-\ntion tools under Linux were derived from code ﬁrst developed for 4.3 BSD, but\nmore recent BSD derivatives, such as FreeBSD, have borrowed code from Linux\nin return. Examples of this sharing include the Intel ﬂoating-point-emulation",
  "math library and the PC sound-hardware device drivers.\nThe Linux system as a whole is maintained by a loose network of\ndevelopers collaborating over the Internet, with small groups or individuals\nhaving responsibility for maintaining the integrity of speciﬁc components.\nA small number of public Internet ﬁle-transfer-protocol (FTP) archive sites\nact as de facto standard repositories for these components. The File System\nHierarchy Standard document is also maintained by the Linux community\nas a means of ensuring compatibility across the various system components.\nThis standard speciﬁes the overall layout of a standard Linux ﬁle system; it\ndetermines under which directory names conﬁguration ﬁles, libraries, system\nbinaries, and run-time data ﬁles should be stored. 18.1\nLinux History\n785",
  "Linux History\n785\n18.1.3\nLinux Distributions\nIn theory, anybody can install a Linux system by fetching the latest revisions\nof the necessary system components from the FTP sites and compiling them. In\nLinux’s early days, this is precisely what a Linux user had to do. As Linux has\nmatured, however, various individuals and groups have attempted to make\nthis job less painful by providing standard, precompiled sets of packages for\neasy installation.\nThese collections, or distributions, include much more than just the\nbasic Linux system. They typically include extra system-installation and\nmanagement utilities, as well as precompiled and ready-to-install packages\nof many of the common UNIX tools, such as news servers, web browsers,\ntext-processing and editing tools, and even games.",
  "The ﬁrst distributions managed these packages by simply providing\na means of unpacking all the ﬁles into the appropriate places. One of\nthe important contributions of modern distributions, however, is advanced\npackage management. Today’s Linux distributions include a package-tracking\ndatabase that allows packages to be installed, upgraded, or removed painlessly.\nThe SLS distribution, dating back to the early days of Linux, was the ﬁrst\ncollection of Linux packages that was recognizable as a complete distribution.\nAlthough it could be installed as a single entity, SLS lacked the package-\nmanagement tools now expected of Linux distributions. The Slackware\ndistribution represented a great improvement in overall quality, even though",
  "it also had poor package management. In fact, it is still one of the most widely\ninstalled distributions in the Linux community.\nSince Slackware’s release, many commercial and noncommercial Linux\ndistributions have become available. Red Hat and Debian are particularly pop-\nular distributions; the ﬁrst comes from a commercial Linux support company\nand the second from the free-software Linux community. Other commercially\nsupported versions of Linux include distributions from Canonical and SuSE,\nand others too numerous to list here. There are too many Linux distributions in\ncirculation for us to list all of them here. The variety of distributions does not\nprevent Linux distributions from being compatible, however. The RPM package",
  "ﬁle format is used, or at least understood, by the majority of distributions, and\ncommercial applications distributed in this format can be installed and run on\nany distribution that can accept RPM ﬁles.\n18.1.4\nLinux Licensing\nThe Linux kernel is distributed under version 2.0 of the GNU General Public\nLicense (GPL), the terms of which are set out by the Free Software Foundation.\nLinux is not public-domain software. Public domain implies that the authors\nhave waived copyright rights in the software, but copyright rights in Linux\ncode are still held by the code’s various authors. Linux is free software, however,\nin the sense that people can copy it, modify it, use it in any manner they want,\nand give away (or sell) their own copies.",
  "and give away (or sell) their own copies.\nThe main implication of Linux’s licensing terms is that nobody using Linux,\nor creating a derivative of Linux (a legitimate exercise), can distribute the\nderivative without including the source code. Software released under the GPL\ncannot be redistributed as a binary-only product. If you release software that\nincludes any components covered by the GPL, then, under the GPL, you must 786\nChapter 18\nThe Linux System\nmake source code available alongside any binary distributions. (This restriction\ndoes not prohibit making—or even selling—binary software distributions, as\nlong as anybody who receives binaries is also given the opportunity to get the\noriginating source code for a reasonable distribution charge.)\n18.2 Design Principles",
  "18.2 Design Principles\nIn its overall design, Linux resembles other traditional, nonmicrokernel UNIX\nimplementations. It is a multiuser, preemptively multitasking system with a\nfull set of UNIX-compatible tools. Linux’s ﬁle system adheres to traditional UNIX\nsemantics, and the standard UNIX networking model is fully implemented. The\ninternal details of Linux’s design have been inﬂuenced heavily by the history\nof this operating system’s development.\nAlthough Linux runs on a wide variety of platforms, it was originally\ndeveloped exclusively on PC architecture. A great deal of that early devel-\nopment was carried out by individual enthusiasts rather than by well-funded\ndevelopment or research facilities, so from the start Linux attempted to squeeze",
  "as much functionality as possible from limited resources. Today, Linux can run\nhappily on a multiprocessor machine with many gigabytes of main memory\nand many terabytes of disk space, but it is still capable of operating usefully in\nunder 16 MB of RAM.\nAs PCs became more powerful and as memory and hard disks became\ncheaper, the original, minimalist Linux kernels grew to implement more\nUNIX functionality. Speed and efﬁciency are still important design goals, but\nmuch recent and current work on Linux has concentrated on a third major\ndesign goal: standardization. One of the prices paid for the diversity of UNIX\nimplementations currently available is that source code written for one may not\nnecessarily compile or run correctly on another. Even when the same system",
  "calls are present on two different UNIX systems, they do not necessarily behave\nin exactly the same way. The POSIX standards comprise a set of speciﬁcations\nfor different aspects of operating-system behavior. There are POSIX documents\nfor common operating-system functionality and for extensions such as process\nthreads and real-time operations. Linux is designed to comply with the relevant\nPOSIX documents, and at least two Linux distributions have achieved ofﬁcial\nPOSIX certiﬁcation.\nBecause it gives standard interfaces to both the programmer and the user,\nLinux presents few surprises to anybody familiar with UNIX. We do not detail\nthese interfaces here. The sections on the programmer interface (Section A.3)\nand user interface (Section A.4) of BSD apply equally well to Linux. By default,",
  "however, the Linux programming interface adheres to SVR4 UNIX semantics,\nrather than to BSD behavior. A separate set of libraries is available to implement\nBSD semantics in places where the two behaviors differ signiﬁcantly.\nMany other standards exist in the UNIX world, but full certiﬁcation of\nLinux with respect to these standards is sometimes slowed because certiﬁcation\nis often available only for a fee, and the expense involved in certifying an\noperating system’s compliance with most standards is substantial. However,\nsupporting a wide base of applications is important for any operating system,\nso implementation of standards is a major goal for Linux development, even\nif the implementation is not formally certiﬁed. In addition to the basic POSIX 18.2\nDesign Principles\n787",
  "Design Principles\n787\nstandard, Linux currently supports the POSIX threading extensions—Pthreads\n—and a subset of the POSIX extensions for real-time process control.\n18.2.1\nComponents of a Linux System\nThe Linux system is composed of three main bodies of code, in line with most\ntraditional UNIX implementations:\n1. Kernel. The kernel is responsible for maintaining all the important\nabstractions of the operating system, including such things as virtual\nmemory and processes.\n2. System libraries. The system libraries deﬁne a standard set of functions\nthrough which applications can interact with the kernel. These functions\nimplement much of the operating-system functionality that does not need\nthe full privileges of kernel code. The most important system library is",
  "the C library, known as libc. In addition to providing the standard C\nlibrary, libc implements the user mode side of the Linux system call\ninterface, as well as other critical system-level interfaces.\n3. System utilities. The system utilities are programs that perform indi-\nvidual, specialized management tasks. Some system utilities are invoked\njust once to initialize and conﬁgure some aspect of the system. Others\n—known as daemons in UNIX terminology—run permanently, handling\nsuch tasks as responding to incoming network connections, accepting\nlogon requests from terminals, and updating log ﬁles.\nFigure 18.1 illustrates the various components that make up a full Linux\nsystem. The most important distinction here is between the kernel and",
  "everything else. All the kernel code executes in the processor’s privileged\nmode with full access to all the physical resources of the computer. Linux\nrefers to this privileged mode as kernel mode. Under Linux, no user code is\nbuilt into the kernel. Any operating-system-support code that does not need to\nrun in kernel mode is placed into the system libraries and runs in user mode.\nUnlike kernel mode, user mode has access only to a controlled subset of the\nsystem’s resources.\nsystem shared libraries\nLinux kernel\nloadable kernel modules\nsystem-\nmanagement\nprograms\nuser\nprocesses\nuser\nutility\nprograms\ncompilers\nFigure 18.1\nComponents of the Linux system. 788\nChapter 18\nThe Linux System\nAlthough various modern operating systems have adopted a message-",
  "passing architecture for their kernel internals, Linux retains UNIX’s historical\nmodel: the kernel is created as a single, monolithic binary. The main reason\nis performance. Because all kernel code and data structures are kept in a\nsingle address space, no context switches are necessary when a process calls an\noperating-system function or when a hardware interrupt is delivered. More-\nover, the kernel can pass data and make requests between various subsystems\nusing relatively cheap C function invocation and not more complicated inter-\nprocess communication (IPC). This single address space contains not only the\ncore scheduling and virtual memory code but all kernel code, including all\ndevice drivers, ﬁle systems, and networking code.",
  "device drivers, ﬁle systems, and networking code.\nEven though all the kernel components share this same melting pot, there\nis still room for modularity. In the same way that user applications can load\nshared libraries at run time to pull in a needed piece of code, so the Linux\nkernel can load (and unload) modules dynamically at run time. The kernel\ndoes not need to know in advance which modules may be loaded—they are\ntruly independent loadable components.\nThe Linux kernel forms the core of the Linux operating system. It provides\nall the functionality necessary to run processes, and it provides system services\nto give arbitrated and protected access to hardware resources. The kernel\nimplements all the features required to qualify as an operating system. On",
  "its own, however, the operating system provided by the Linux kernel is not\na complete UNIX system. It lacks much of the functionality and behavior of\nUNIX, and the features that it does provide are not necessarily in the format\nin which a UNIX application expects them to appear. The operating-system\ninterface visible to running applications is not maintained directly by the\nkernel. Rather, applications make calls to the system libraries, which in turn\ncall the operating-system services as necessary.\nThe system libraries provide many types of functionality. At the simplest\nlevel, they allow applications to make system calls to the Linux kernel. Making\na system call involves transferring control from unprivileged user mode to",
  "privileged kernel mode; the details of this transfer vary from architecture to\narchitecture. The libraries take care of collecting the system-call arguments and,\nif necessary, arranging those arguments in the special form necessary to make\nthe system call.\nThe libraries may also provide more complex versions of the basic system\ncalls. For example, the C language’s buffered ﬁle-handling functions are all\nimplemented in the system libraries, providing more advanced control of ﬁle\nI/O than the basic kernel system calls. The libraries also provide routines that do\nnot correspond to system calls at all, such as sorting algorithms, mathematical\nfunctions, and string-manipulation routines. All the functions necessary to\nsupport the running of UNIX or POSIX applications are implemented in the",
  "system libraries.\nThe Linux system includes a wide variety of user-mode programs—both\nsystem utilities and user utilities. The system utilities include all the programs\nnecessary to initialize and then administer the system, such as those to set\nup networking interfaces and to add and remove users from the system.\nUser utilities are also necessary to the basic operation of the system but do\nnot require elevated privileges to run. They include simple ﬁle-management\nutilities such as those to copy ﬁles, create directories, and edit text ﬁles. One 18.3\nKernel Modules\n789\nof the most important user utilities is the shell, the standard command-line\ninterface on UNIX systems. Linux supports many shells; the most common is\nthe bourne-Again shell (bash).\n18.3 Kernel Modules",
  "18.3 Kernel Modules\nThe Linux kernel has the ability to load and unload arbitrary sections of kernel\ncode on demand. These loadable kernel modules run in privileged kernel mode\nand as a consequence have full access to all the hardware capabilities of the\nmachine on which they run. In theory, there is no restriction on what a kernel\nmodule is allowed to do. Among other things, a kernel module can implement\na device driver, a ﬁle system, or a networking protocol.\nKernel modules are convenient for several reasons. Linux’s source code is\nfree, so anybody wanting to write kernel code is able to compile a modiﬁed\nkernel and to reboot into that new functionality. However, recompiling,\nrelinking, and reloading the entire kernel is a cumbersome cycle to undertake",
  "when you are developing a new driver. If you use kernel modules, you do not\nhave to make a new kernel to test a new driver—the driver can be compiled\non its own and loaded into the already running kernel. Of course, once a new\ndriver is written, it can be distributed as a module so that other users can\nbeneﬁt from it without having to rebuild their kernels.\nThis latter point has another implication. Because it is covered by the\nGPL license, the Linux kernel cannot be released with proprietary components\nadded to it unless those new components are also released under the GPL and\nthe source code for them is made available on demand. The kernel’s module\ninterface allows third parties to write and distribute, on their own terms, device",
  "drivers or ﬁle systems that could not be distributed under the GPL.\nKernel modules allow a Linux system to be set up with a standard minimal\nkernel, without any extra device drivers built in. Any device drivers that the\nuser needs can be either loaded explicitly by the system at startup or loaded\nautomatically by the system on demand and unloaded when not in use. For\nexample, a mouse driver can be loaded when a USB mouse is plugged into the\nsystem and unloaded when the mouse is unplugged.\nThe module support under Linux has four components:\n1. The module-management system allows modules to be loaded into\nmemory and to communicate with the rest of the kernel.\n2. The module loader and unloader, which are user-mode utilities, work",
  "with the module-management system to load a module into memory.\n3. The driver-registration system allows modules to tell the rest of the\nkernel that a new driver has become available.\n4. Aconﬂict-resolution mechanism allows different device drivers to\nreserve hardware resources and to protect those resources from accidental\nuse by another driver.\n18.3.1\nModule Management\nLoading a module requires more than just loading its binary contents into\nkernel memory. The system must also make sure that any references the 790\nChapter 18\nThe Linux System\nmodule makes to kernel symbols or entry points are updated to point to the\ncorrect locations in the kernel’s address space. Linux deals with this reference\nupdating by splitting the job of module loading into two separate sections: the",
  "management of sections of module code in kernel memory and the handling\nof symbols that modules are allowed to reference.\nLinux maintains an internal symbol table in the kernel. This symbol table\ndoes not contain the full set of symbols deﬁned in the kernel during the latter’s\ncompilation; rather, a symbol must be explicitly exported. The set of exported\nsymbols constitutes a well-deﬁned interface by which a module can interact\nwith the kernel.\nAlthough exporting symbols from a kernel function requires an explicit\nrequest by the programmer, no special effort is needed to import those symbols\ninto a module. A module writer just uses the standard external linking of the\nC language. Any external symbols referenced by the module but not declared",
  "by it are simply marked as unresolved in the ﬁnal module binary produced by\nthe compiler. When a module is to be loaded into the kernel, a system utility\nﬁrst scans the module for these unresolved references. All symbols that still\nneed to be resolved are looked up in the kernel’s symbol table, and the correct\naddresses of those symbols in the currently running kernel are substituted into\nthe module’s code. Only then is the module passed to the kernel for loading. If\nthe system utility cannot resolve all references in the module by looking them\nup in the kernel’s symbol table, then the module is rejected.\nThe loading of the module is performed in two stages. First, the module-\nloader utility asks the kernel to reserve a continuous area of virtual kernel",
  "memory for the module. The kernel returns the address of the memory\nallocated, and the loader utility can use this address to relocate the module’s\nmachine code to the correct loading address. A second system call then passes\nthe module, plus any symbol table that the new module wants to export, to the\nkernel. The module itself is now copied verbatim into the previously allocated\nspace, and the kernel’s symbol table is updated with the new symbols for\npossible use by other modules not yet loaded.\nThe ﬁnal module-management component is the module requester. The\nkernel deﬁnes a communication interface to which a module-management\nprogram can connect. With this connection established, the kernel will inform\nthe management process whenever a process requests a device driver, ﬁle",
  "system, or network service that is not currently loaded and will give the\nmanager the opportunity to load that service. The original service request will\ncomplete once the module is loaded. The manager process regularly queries\nthe kernel to see whether a dynamically loaded module is still in use and\nunloads that module when it is no longer actively needed.\n18.3.2\nDriver Registration\nOnce a module is loaded, it remains no more than an isolated region of memory\nuntil it lets the rest of the kernel know what new functionality it provides.\nThe kernel maintains dynamic tables of all known drivers and provides a\nset of routines to allow drivers to be added to or removed from these tables\nat any time. The kernel makes sure that it calls a module’s startup routine",
  "when that module is loaded and calls the module’s cleanup routine before 18.3\nKernel Modules\n791\nthat module is unloaded. These routines are responsible for registering the\nmodule’s functionality.\nA module may register many types of functionality; it is not limited\nto only one type. For example, a device driver might want to register two\nseparate mechanisms for accessing the device. Registration tables include,\namong others, the following items:\n• Device drivers. These drivers include character devices (such as printers,\nterminals, and mice), block devices (including all disk drives), and network\ninterface devices.\n• File systems. The ﬁle system may be anything that implements Linux’s\nvirtual ﬁle system calling routines. It might implement a format for storing",
  "ﬁles on a disk, but it might equally well be a network ﬁle system, such as\nNFS, or a virtual ﬁle system whose contents are generated on demand, such\nas Linux’s /proc ﬁle system.\n• Network protocols. A module may implement an entire networking\nprotocol, such as TCP or simply a new set of packet-ﬁltering rules for\na network ﬁrewall.\n• Binary format. This format speciﬁes a way of recognizing, loading, and\nexecuting a new type of executable ﬁle.\nIn addition, a module can register a new set of entries in the sysctl and /proc\ntables, to allow that module to be conﬁgured dynamically (Section 18.7.4).\n18.3.3\nConﬂict Resolution\nCommercial UNIX implementations are usually sold to run on a vendor’s own\nhardware. One advantage of a single-supplier solution is that the software",
  "vendor has a good idea about what hardware conﬁgurations are possible.\nPC hardware, however, comes in a vast number of conﬁgurations, with\nlarge numbers of possible drivers for devices such as network cards and\nvideo display adapters. The problem of managing the hardware conﬁguration\nbecomes more severe when modular device drivers are supported, since the\ncurrently active set of devices becomes dynamically variable.\nLinux provides a central conﬂict-resolution mechanism to help arbitrate\naccess to certain hardware resources. Its aims are as follows:\n• To prevent modules from clashing over access to hardware resources\n• To prevent autoprobes—device-driver probes that auto-detect device\nconﬁguration—from interfering with existing device drivers",
  "• To resolve conﬂicts among multiple drivers trying to access the same\nhardware—as, for example, when both the parallel printer driver and\nthe parallel line IP (PLIP) network driver try to talk to the parallel port\nTo these ends, the kernel maintains lists of allocated hardware resources.\nThe PC has a limited number of possible I/O ports (addresses in its hardware\nI/O address space), interrupt lines, and DMA channels. When any device driver\nwants to access such a resource, it is expected to reserve the resource with 792\nChapter 18\nThe Linux System\nthe kernel database ﬁrst. This requirement incidentally allows the system\nadministrator to determine exactly which resources have been allocated by\nwhich driver at any given point.",
  "which driver at any given point.\nA module is expected to use this mechanism to reserve in advance any\nhardware resources that it expects to use. If the reservation is rejected because\nthe resource is not present or is already in use, then it is up to the module\nto decide how to proceed. It may fail in its initialization attempt and request\nthat it be unloaded if it cannot continue, or it may carry on, using alternative\nhardware resources.\n18.4 Process Management\nA process is the basic context in which all user-requested activity is serviced\nwithin the operating system. To be compatible with other UNIX systems, Linux\nmust use a process model similar to those of other versions of UNIX. Linux\noperates differently from UNIX in a few key places, however. In this section,",
  "we review the traditional UNIX process model (Section A.3.2) and introduce\nLinux’s threading model.\n18.4.1\nThe fork() and exec() Process Model\nThe basic principle of UNIX process management is to separate into two steps\ntwo operations that are usually combined into one: the creation of a new\nprocess and the running of a new program. A new process is created by the\nfork() system call, and a new program is run after a call to exec(). These are\ntwo distinctly separate functions. We can create a new process with fork()\nwithout running a new program—the new subprocess simply continues to\nexecute exactly the same program, at exactly the same point, that the ﬁrst\n(parent) process was running. In the same way, running a new program does",
  "not require that a new process be created ﬁrst. Any process may call exec() at\nany time. A new binary object is loaded into the process’s address space and\nthe new executable starts executing in the context of the existing process.\nThis model has the advantage of great simplicity. It is not necessary to\nspecify every detail of the environment of a new program in the system call that\nruns that program. The new program simply runs in its existing environment.\nIf a parent process wishes to modify the environment in which a new program\nis to be run, it can fork and then, still running the original executable in a child\nprocess, make any system calls it requires to modify that child process before\nﬁnally executing the new program.",
  "ﬁnally executing the new program.\nUnder UNIX, then, a process encompasses all the information that the\noperating system must maintain to track the context of a single execution of a\nsingle program. Under Linux, we can break down this context into a number of\nspeciﬁc sections. Broadly, process properties fall into three groups: the process\nidentity, environment, and context.\n18.4.1.1\nProcess Identity\nA process identity consists mainly of the following items:\n• Process ID (PID). Each process has a unique identiﬁer. The PID is used to\nspecify the process to the operating system when an application makes a 18.4\nProcess Management\n793\nsystem call to signal, modify, or wait for the process. Additional identiﬁers\nassociate the process with a process group (typically, a tree of processes",
  "forked by a single user command) and login session.\n• Credentials. Each process must have an associated user ID and one or more\ngroup IDs (user groups are discussed in Section 11.6.2) that determine the\nrights of a process to access system resources and ﬁles.\n• Personality. Process personalities are not traditionally found on UNIX\nsystems, but under Linux each process has an associated personality\nidentiﬁer that can slightly modify the semantics of certain system calls.\nPersonalities are primarily used by emulation libraries to request that\nsystem calls be compatible with certain varieties of UNIX.\n• Namespace. Each process is associated with a speciﬁc view of the ﬁle-\nsystem hierarchy, called its namespace. Most processes share a common",
  "namespace and thus operate on a shared ﬁle-system hierarchy. Processes\nand their children can, however, have different namespaces, each with a\nunique ﬁle-system hierarchy—their own root directory and set of mounted\nﬁle systems.\nMost of these identiﬁers are under the limited control of the process itself.\nThe process group and session identiﬁers can be changed if the process\nwants to start a new group or session. Its credentials can be changed, subject\nto appropriate security checks. However, the primary PID of a process is\nunchangeable and uniquely identiﬁes that process until termination.\n18.4.1.2\nProcess Environment\nA process’s environment is inherited from its parent and is composed of two\nnull-terminated vectors: the argument vector and the environment vector. The",
  "argument vector simply lists the command-line arguments used to invoke the\nrunning program; it conventionally starts with the name of the program itself.\nThe environment vector is a list of “NAME=VALUE” pairs that associates named\nenvironment variables with arbitrary textual values. The environment is not\nheld in kernel memory but is stored in the process’s own user-mode address\nspace as the ﬁrst datum at the top of the process’s stack.\nThe argument and environment vectors are not altered when a new process\nis created. The new child process will inherit the environment of its parent.\nHowever, a completely new environment is set up when a new program\nis invoked. On calling exec(), a process must supply the environment for",
  "the new program. The kernel passes these environment variables to the next\nprogram, replacing the process’s current environment. The kernel otherwise\nleaves the environment and command-line vectors alone—their interpretation\nis left entirely to the user-mode libraries and applications.\nThe passing of environment variables from one process to the next and the\ninheriting of these variables by the children of a process provide ﬂexible ways\nto pass information to components of the user-mode system software. Various\nimportant environment variables have conventional meanings to related parts\nof the system software. For example, the TERM variable is set up to name the\ntype of terminal connected to a user’s login session. Many programs use this 794\nChapter 18\nThe Linux System",
  "Chapter 18\nThe Linux System\nvariable to determine how to perform operations on the user’s display, such as\nmoving the cursor and scrolling a region of text. Programs with multilingual\nsupport use the LANG variable to determine the language in which to display\nsystem messages for programs that include multilingual support.\nThe environment-variable mechanism custom-tailors the operating system\non a per-process basis. Users can choose their own languages or select their\nown editors independently of one another.\n18.4.1.3\nProcess Context\nThe process identity and environment properties are usually set up when a\nprocess is created and not changed until that process exits. A process may\nchoose to change some aspects of its identity if it needs to do so, or it may",
  "alter its environment. In contrast, process context is the state of the running\nprogram at any one time; it changes constantly. Process context includes the\nfollowing parts:\n• Scheduling context. The most important part of the process context is its\nscheduling context—the information that the scheduler needs to suspend\nand restart the process. This information includes saved copies of all the\nprocess’s registers. Floating-point registers are stored separately and are\nrestored only when needed. Thus, processes that do not use ﬂoating-point\narithmetic do not incur the overhead of saving that state. The scheduling\ncontext also includes information about scheduling priority and about any\noutstanding signals waiting to be delivered to the process. A key part of",
  "the scheduling context is the process’s kernel stack, a separate area of\nkernel memory reserved for use by kernel-mode code. Both system calls\nand interrupts that occur while the process is executing will use this stack.\n• Accounting. The kernel maintains accounting information about the\nresources currently being consumed by each process and the total resources\nconsumed by the process in its entire lifetime so far.\n• File table. The ﬁle table is an array of pointers to kernel ﬁle structures\nrepresenting open ﬁles. When making ﬁle-I/O system calls, processes refer\nto ﬁles by an integer, known as a ﬁle descriptor (fd), that the kernel uses\nto index into this table.\n• File-system context. Whereas the ﬁle table lists the existing open ﬁles, the",
  "ﬁle-system context applies to requests to open new ﬁles. The ﬁle-system\ncontext includes the process’s root directory, current working directory,\nand namespace.\n• Signal-handler table. UNIX systems can deliver asynchronous signals to\na process in response to various external events. The signal-handler table\ndeﬁnes the action to take in response to a speciﬁc signal. Valid actions\ninclude ignoring the signal, terminating the process, and invoking a routine\nin the process’s address space.\n• Virtual memory context. The virtual memory context describes the full\ncontents of a process’s private address space; we discuss it in Section 18.6. 18.5\nScheduling\n795\n18.4.2\nProcesses and Threads\nLinux provides the fork() system call, which duplicates a process without",
  "loading a new executable image. Linux also provides the ability to create\nthreads via the clone() system call. Linux does not distinguish between\nprocesses and threads, however. In fact, Linux generally uses the term task\n—rather than process or thread—when referring to a ﬂow of control within a\nprogram. The clone() system call behaves identically to fork(), except that\nit accepts as arguments a set of ﬂags that dictate what resources are shared\nbetween the parent and child (whereas a process created with fork() shares\nno resources with its parent). The ﬂags include:\nflag\nmeaning\nCLONE_FS\nCLONE_VM\nCLONE_SIGHAND\nCLONE_FILES\nFile-system information is shared.\nThe same memory space is shared.\nSignal handlers are shared.\nThe set of open files is shared.",
  "The set of open files is shared.\nThus, if clone() is passed the ﬂags CLONE FS, CLONE VM, CLONE SIGHAND,\nand CLONE FILES, the parent and child tasks will share the same ﬁle-system\ninformation (such as the current working directory), the same memory space,\nthe same signal handlers, and the same set of open ﬁles. Using clone() in this\nfashion is equivalent to creating a thread in other systems, since the parent\ntask shares most of its resources with its child task. If none of these ﬂags is set\nwhen clone() is invoked, however, the associated resources are not shared,\nresulting in functionality similar to that of the fork() system call.\nThe lack of distinction between processes and threads is possible because\nLinux does not hold a process’s entire context within the main process data",
  "structure. Rather, it holds the context within independent subcontexts. Thus,\na process’s ﬁle-system context, ﬁle-descriptor table, signal-handler table, and\nvirtual memory context are held in separate data structures. The process data\nstructure simply contains pointers to these other structures, so any number of\nprocesses can easily share a subcontext by pointing to the same subcontext and\nincrementing a reference count.\nThe arguments to the clone() system call tell it which subcontexts to copy\nand which to share. The new process is always given a new identity and a new\nscheduling context—these are the essentials of a Linux process. According to\nthe arguments passed, however, the kernel may either create new subcontext",
  "data structures initialized so as to be copies of the parent’s or set up the new\nprocess to use the same subcontext data structures being used by the parent.\nThe fork() system call is nothing more than a special case of clone() that\ncopies all subcontexts, sharing none.\n18.5 Scheduling\nScheduling is the job of allocating CPU time to different tasks within an operat-\ning system. Linux, like all UNIX systems, supports preemptive multitasking.\nIn such a system, the process scheduler decides which process runs and when. 796\nChapter 18\nThe Linux System\nMaking these decisions in a way that balances fairness and performance across\nmany different workloads is one of the more complicated challenges in modern\noperating systems.",
  "operating systems.\nNormally, we think of scheduling as the running and interrupting of user\nprocesses, but another aspect of scheduling is also important to Linux: the\nrunning of the various kernel tasks. Kernel tasks encompass both tasks that are\nrequested by a running process and tasks that execute internally on behalf of\nthe kernel itself, such as tasks spawned by Linux’s I/O subsystem.\n18.5.1\nProcess Scheduling\nLinux has two separate process-scheduling algorithms. One is a time-sharing\nalgorithm for fair, preemptive scheduling among multiple processes. The other\nis designed for real-time tasks, where absolute priorities are more important\nthan fairness.\nThe scheduling algorithm used for routine time-sharing tasks received",
  "a major overhaul with version 2.6 of the kernel. Earlier versions ran a\nvariation of the traditional UNIX scheduling algorithm. This algorithm does\nnot provide adequate support for SMP systems, does not scale well as the\nnumber of tasks on the system grows, and does not maintain fairness among\ninteractive tasks, particularly on systems such as desktops and mobile devices.\nThe process scheduler was ﬁrst overhauled with version 2.5 of the kernel.\nVersion 2.5 implemented a scheduling algorithm that selects which task to\nrun in constant time—known as O(1)—regardless of the number of tasks\nor processors in the system. The new scheduler also provided increased\nsupport for SMP, including processor afﬁnity and load balancing. These",
  "changes, while improving scalability, did not improve interactive performance\nor fairness—and, in fact, made these problems worse under certain workloads.\nConsequently, the process scheduler was overhauled a second time, with Linux\nkernel version 2.6. This version ushered in the Completely Fair Scheduler\n(CFS).\nThe Linux scheduler is a preemptive, priority-based algorithm with two\nseparate priority ranges: a real-time range from 0 to 99 and a nice value\nranging from −20 to 19. Smaller nice values indicate higher priorities. Thus,\nby increasing the nice value, you are decreasing your priority and being “nice”\nto the rest of the system.\nCFS is a signiﬁcant departure from the traditional UNIX process scheduler.\nIn the latter, the core variables in the scheduling algorithm are priority and",
  "time slice. The time slice is the length of time—the slice of the processor—\nthat a process is afforded. Traditional UNIX systems give processes a ﬁxed\ntime slice, perhaps with a boost or penalty for high- or low-priority processes,\nrespectively. A process may run for the length of its time slice, and higher-\npriority processes run before lower-priority processes. It is a simple algorithm\nthat many non-UNIX systems employ. Such simplicity worked well for early\ntime-sharing systems but has proved incapable of delivering good interactive\nperformance and fairness on today’s modern desktops and mobile devices.\nCFS introduced a new scheduling algorithm called fair scheduling that\neliminates time slices in the traditional sense. Instead of time slices, all processes",
  "are allotted a proportion of the processor’s time. CFS calculates how long a\nprocess should run as a function of the total number of runnable processes. 18.5\nScheduling\n797\nTo start, CFS says that if there are N runnable processes, then each should\nbe afforded 1/N of the processor’s time. CFS then adjusts this allotment by\nweighting each process’s allotment by its nice value. Processes with the default\nnice value have a weight of 1—their priority is unchanged. Processes with a\nsmaller nice value (higher priority) receive a higher weight, while processes\nwith a larger nice value (lower priority) receive a lower weight. CFS then runs\neach process for a “time slice” proportional to the process’s weight divided by\nthe total weight of all runnable processes.",
  "the total weight of all runnable processes.\nTo calculate the actual length of time a process runs, CFS relies on a\nconﬁgurable variable called target latency, which is the interval of time during\nwhich every runnable task should run at least once. For example, assume\nthat the target latency is 10 milliseconds. Further assume that we have two\nrunnable processes of the same priority. Each of these processes has the same\nweight and therefore receives the same proportion of the processor’s time. In\nthis case, with a target latency of 10 milliseconds, the ﬁrst process runs for\n5 milliseconds, then the other process runs for 5 milliseconds, then the ﬁrst\nprocess runs for 5 milliseconds again, and so forth. If we have 10 runnable",
  "processes, then CFS will run each for a millisecond before repeating.\nBut what if we had, say, 1, 000 processes? Each process would run for 1\nmicrosecond if we followed the procedure just described. Due to switching\ncosts, scheduling processes for such short lengths of time is inefﬁcient.\nCFS consequently relies on a second conﬁgurable variable, the minimum\ngranularity, which is a minimum length of time any process is allotted the\nprocessor. All processes, regardless of the target latency, will run for at least the\nminimum granularity. In this manner, CFS ensures that switching costs do not\ngrow unacceptably large when the number of runnable processes grows too\nlarge. In doing so, it violates its attempts at fairness. In the usual case, however,",
  "the number of runnable processes remains reasonable, and both fairness and\nswitching costs are maximized.\nWith the switch to fair scheduling, CFS behaves differently from traditional\nUNIX process schedulers in several ways. Most notably, as we have seen, CFS\neliminates the concept of a static time slice. Instead, each process receives\na proportion of the processor’s time. How long that allotment is depends on\nhow many other processes are runnable. This approach solves several problems\nin mapping priorities to time slices inherent in preemptive, priority-based\nscheduling algorithms. It is possible, of course, to solve these problems in other\nways without abandoning the classic UNIX scheduler. CFS, however, solves the",
  "problems with a simple algorithm that performs well on interactive workloads\nsuch as mobile devices without compromising throughput performance on the\nlargest of servers.\n18.5.2\nReal-Time Scheduling\nLinux’s real-time scheduling algorithm is signiﬁcantly simpler than the fair\nscheduling employed for standard time-sharing processes. Linux implements\nthe two real-time scheduling classes required by POSIX.1b: ﬁrst-come, ﬁrst-\nserved (FCFS) and round-robin (Section 6.3.1 and Section 6.3.4, respectively). In\nboth cases, each process has a priority in addition to its scheduling class. The\nscheduler always runs the process with the highest priority. Among processes\nof equal priority, it runs the process that has been waiting longest. The only 798\nChapter 18\nThe Linux System",
  "Chapter 18\nThe Linux System\ndifference between FCFS and round-robin scheduling is that FCFS processes\ncontinue to run until they either exit or block, whereas a round-robin process\nwill be preempted after a while and will be moved to the end of the scheduling\nqueue, so round-robin processes of equal priority will automatically time-share\namong themselves.\nLinux’s real-time scheduling is soft—rather than hard—real time. The\nscheduler offers strict guarantees about the relative priorities of real-time\nprocesses, but the kernel does not offer any guarantees about how quickly\na real-time process will be scheduled once that process becomes runnable. In\ncontrast, a hard real-time system can guarantee a minimum latency between\nwhen a process becomes runnable and when it actually runs.\n18.5.3",
  "18.5.3\nKernel Synchronization\nThe way the kernel schedules its own operations is fundamentally different\nfrom the way it schedules processes. A request for kernel-mode execution\ncan occur in two ways. A running program may request an operating-system\nservice, either explicitly via a system call or implicitly—for example, when a\npage fault occurs. Alternatively, a device controller may deliver a hardware\ninterrupt that causes the CPU to start executing a kernel-deﬁned handler for\nthat interrupt.\nThe problem for the kernel is that all these tasks may try to access the same\ninternal data structures. If one kernel task is in the middle of accessing some\ndata structure when an interrupt service routine executes, then that service",
  "routine cannot access or modify the same data without risking data corruption.\nThis fact relates to the idea of critical sections—portions of code that access\nshared data and thus must not be allowed to execute concurrently. As a result,\nkernel synchronization involves much more than just process scheduling. A\nframework is required that allows kernel tasks to run without violating the\nintegrity of shared data.\nPrior to version 2.6, Linux was a nonpreemptive kernel, meaning that a\nprocess running in kernel mode could not be preempted—even if a higher-\npriority process became available to run. With version 2.6, the Linux kernel\nbecame fully preemptive. Now, a task can be preempted when it is running in\nthe kernel.\nThe Linux kernel provides spinlocks and semaphores (as well as reader–",
  "writer versions of these two locks) for locking in the kernel. On SMP machines,\nthe fundamental locking mechanism is a spinlock, and the kernel is designed\nso that spinlocks are held for only short durations. On single-processor\nmachines, spinlocks are not appropriate for use and are replaced by enabling\nand disabling kernel preemption. That is, rather than holding a spinlock, the\ntask disables kernel preemption. When the task would otherwise release the\nspinlock, it enables kernel preemption. This pattern is summarized below:\nsingle processor\nmultiple processors\nAcquire spin lock.\nRelease spin lock.\nDisable kernel preemption.\nEnable kernel preemption. 18.5\nScheduling\n799\nLinux uses an interesting approach to disable and enable kernel pre-",
  "emption. It provides two simple kernel interfaces—preempt disable() and\npreempt enable(). In addition, the kernel is not preemptible if a kernel-mode\ntask is holding a spinlock. To enforce this rule, each task in the system has\na thread-info structure that includes the ﬁeld preempt count, which is a\ncounter indicating the number of locks being held by the task. The counter is\nincremented when a lock is acquired and decremented when a lock is released.\nIf the value of preempt count for the task currently running is greater than\nzero, it is not safe to preempt the kernel, as this task currently holds a lock. If\nthe count is zero, the kernel can safely be interrupted, assuming there are no\noutstanding calls to preempt disable().",
  "outstanding calls to preempt disable().\nSpinlocks—along with the enabling and disabling of kernel preemption—\nare used in the kernel only when the lock is held for short durations. When a\nlock must be held for longer periods, semaphores are used.\nThe second protection technique used by Linux applies to critical sections\nthat occur in interrupt service routines. The basic tool is the processor’s\ninterrupt-control hardware. By disabling interrupts (or using spinlocks) during\na critical section, the kernel guarantees that it can proceed without the risk of\nconcurrent access to shared data structures.\nHowever, there is a penalty for disabling interrupts. On most hardware\narchitectures, interrupt enable and disable instructions are not cheap. More",
  "importantly, as long as interrupts remain disabled, all I/O is suspended, and\nany device waiting for servicing will have to wait until interrupts are reenabled;\nthus, performance degrades. To address this problem, the Linux kernel uses a\nsynchronization architecture that allows long critical sections to run for their\nentire duration without having interrupts disabled. This ability is especially\nuseful in the networking code. An interrupt in a network device driver can\nsignal the arrival of an entire network packet, which may result in a great deal\nof code being executed to disassemble, route, and forward that packet within\nthe interrupt service routine.\nLinux implements this architecture by separating interrupt service routines",
  "into two sections: the top half and the bottom half. The top half is the\nstandard interrupt service routine that runs with recursive interrupts disabled.\nInterrupts of the same number (or line) are disabled, but other interrupts may\nrun. The bottom half of a service routine is run, with all interrupts enabled,\nby a miniature scheduler that ensures that bottom halves never interrupt\nthemselves. The bottom-half scheduler is invoked automatically whenever\nan interrupt service routine exits.\nThis separation means that the kernel can complete any complex processing\nthat has to be done in response to an interrupt without worrying about being\ninterrupted itself. If another interrupt occurs while a bottom half is executing,",
  "then that interrupt can request that the same bottom half execute, but the\nexecution will be deferred until the one currently running completes. Each\nexecution of the bottom half can be interrupted by a top half but can never be\ninterrupted by a similar bottom half.\nThe top-half/bottom-half architecture is completed by a mechanism for\ndisabling selected bottom halves while executing normal, foreground kernel\ncode. The kernel can code critical sections easily using this system. Interrupt\nhandlers can code their critical sections as bottom halves; and when the\nforeground kernel wants to enter a critical section, it can disable any relevant 800\nChapter 18\nThe Linux System\ntop-half interrupt handlers\nbottom-half interrupt handlers\nkernel-system service routines (preemptible)",
  "kernel-system service routines (preemptible)\nuser-mode programs (preemptible)\nincreasing priority\nFigure 18.2\nInterrupt protection levels.\nbottom halves to prevent any other critical sections from interrupting it. At\nthe end of the critical section, the kernel can reenable the bottom halves and\nrun any bottom-half tasks that have been queued by top-half interrupt service\nroutines during the critical section.\nFigure 18.2 summarizes the various levels of interrupt protection within\nthe kernel. Each level may be interrupted by code running at a higher level\nbut will never be interrupted by code running at the same or a lower level.\nExcept for user-mode code, user processes can always be preempted by another\nprocess when a time-sharing scheduling interrupt occurs.\n18.5.4",
  "18.5.4\nSymmetric Multiprocessing\nThe Linux 2.0 kernel was the ﬁrst stable Linux kernel to support symmetric\nmultiprocessor (SMP) hardware, allowing separate processes to execute in\nparallel on separate processors. The original implementation of SMP imposed\nthe restriction that only one processor at a time could be executing kernel code.\nIn version 2.2 of the kernel, a single kernel spinlock (sometimes termed\nBKL for “big kernel lock”) was created to allow multiple processes (running on\ndifferent processors) to be active in the kernel concurrently. However, the BKL\nprovided a very coarse level of locking granularity, resulting in poor scalability\nto machines with many processors and processes. Later releases of the kernel",
  "made the SMP implementation more scalable by splitting this single kernel\nspinlock into multiple locks, each of which protects only a small subset of the\nkernel’s data structures. Such spinlocks are described in Section 18.5.3. The 3.0\nkernel provides additional SMP enhancements, including ever-ﬁner locking,\nprocessor afﬁnity, and load-balancing algorithms.\n18.6 Memory Management\nMemory management under Linux has two components. The ﬁrst deals with\nallocating and freeing physical memory—pages, groups of pages, and small\nblocks of RAM. The second handles virtual memory, which is memory-mapped\ninto the address space of running processes. In this section, we describe these\ntwo components and then examine the mechanisms by which the loadable",
  "components of a new program are brought into a process’s virtual memory in\nresponse to an exec() system call. 18.6\nMemory Management\n801\n18.6.1\nManagement of Physical Memory\nDue to speciﬁc hardware constraints, Linux separates physical memory into\nfour different zones, or regions:\n• ZONE DMA\n• ZONE DMA32\n• ZONE NORMAL\n• ZONE HIGHMEM\nThese zones are architecture speciﬁc. For example, on the Intel x86-32 architec-\nture, certain ISA (industry standard architecture) devices can only access the\nlower 16 MB of physical memory using DMA. On these systems, the ﬁrst 16\nMB of physical memory comprise ZONE DMA. On other systems, certain devices\ncan only access the ﬁrst 4 GB of physical memory, despite supporting 64-\nbit addresses. On such systems, the ﬁrst 4 GB of physical memory comprise",
  "ZONE DMA32. ZONE HIGHMEM (for “high memory”) refers to physical memory\nthat is not mapped into the kernel addressspace. For example, on the 32-bit Intel\narchitecture (where 232 provides a 4-GB address space), the kernel is mapped\ninto the ﬁrst 896 MB of the address space; the remaining memory is referred\nto as high memory and is allocated from ZONE HIGHMEM. Finally, ZONE NORMAL\ncomprises everything else—the normal, regularly mapped pages. Whether\nan architecture has a given zone depends on its constraints. A modern, 64-bit\narchitecture such as Intel x86-64 has a small 16 MB ZONE DMA (for legacy devices)\nand all the rest of its memory in ZONE NORMAL, with no “high memory”.\nThe relationship of zones and physical addresses on the Intel x86-32",
  "architecture is shown in Figure 18.3. The kernel maintains a list of free pages\nfor each zone. When a request for physical memory arrives, the kernel satisﬁes\nthe request using the appropriate zone.\nThe primary physical-memory manager in the Linux kernel is the page\nallocator. Each zone has its own allocator, which is responsible for allocating\nand freeing all physical pages for the zone and is capable of allocating ranges\nof physically contiguous pages on request. The allocator uses a buddy system\n(Section 9.8.1) to keep track of available physical pages. In this scheme,\nadjacent units of allocatable memory are paired together (hence its name). Each\nallocatable memory region has an adjacent partner (or buddy). Whenever two",
  "allocated partner regions are freed up, they are combined to form a larger\nregion—a buddy heap. That larger region also has a partner, with which it can\ncombine to form a still larger free region. Conversely, if a small memory request\nzone\nphysical memory\n< 16 MB\n16 .. 896 MB\n> 896  MB\nZONE_DMA\nZONE_NORMAL\nZONE_HIGHMEM\nFigure 18.3\nRelationship of zones and physical addresses in Intel x86-32. 802\nChapter 18\nThe Linux System\ncannot be satisﬁed by allocation of an existing small free region, then a larger\nfree region will be subdivided into two partners to satisfy the request. Separate\nlinked lists are used to record the free memory regions of each allowable size.\nUnder Linux, the smallest size allocatable under this mechanism is a single",
  "physical page. Figure 18.4 shows an example of buddy-heap allocation. A 4-KB\nregion is being allocated, but the smallest available region is 16 KB. The region\nis broken up recursively until a piece of the desired size is available.\nUltimately, all memory allocations in the Linux kernel are made either\nstatically, by drivers that reserve a contiguous area of memory during system\nboot time, or dynamically, by the page allocator. However, kernel functions\ndo not have to use the basic allocator to reserve memory. Several specialized\nmemory-management subsystems use the underlying page allocator to man-\nage their own pools of memory. The most important are the virtual memory\nsystem, described in Section 18.6.2; the kmalloc() variable-length allocator;",
  "the slab allocator, used for allocating memory for kernel data structures; and\nthe page cache, used for caching pages belonging to ﬁles.\nMany components of the Linux operating system need to allocate entire\npages on request, but often smaller blocks of memory are required. The kernel\nprovides an additional allocator for arbitrary-sized requests, where the size of\na request is not known in advance and may be only a few bytes. Analogous\nto the C language’s malloc() function, this kmalloc() service allocates entire\nphysical pages on demand but then splits them into smaller pieces. The kernel\nmaintains lists of pages in use by the kmalloc() service. Allocating memory\ninvolves determining the appropriate list and either taking the ﬁrst free piece",
  "available on the list or allocating a new page and splitting it up. Memory regions\nclaimed by the kmalloc() system are allocated permanently until they are\nfreed explicitly with a corresponding call to kfree(); the kmalloc() system\ncannot reallocate or reclaim these regions in response to memory shortages.\nAnother strategy adopted by Linux for allocating kernel memory is known\nas slab allocation. A slab is used for allocating memory for kernel data\nstructures and is made up of one or more physically contiguous pages. A\ncache consists of one or more slabs. There is a single cache for each unique\nkernel data structure—for example, a cache for the data structure representing\nprocess descriptors, a cache for ﬁle objects, a cache for inodes, and so forth.\n16KB\n8KB\n8KB\n8KB\n4KB\n4KB\nFigure 18.4",
  "16KB\n8KB\n8KB\n8KB\n4KB\n4KB\nFigure 18.4\nSplitting of memory in the buddy system. 18.6\nMemory Management\n803\n3-KB\nobjects\n7-KB\nobjects\nkernel objects\ncaches\nslabs\nphysically\ncontiguous\npages\nFigure 18.5\nSlab allocator in Linux.\nEach cache is populated with objects that are instantiations of the kernel\ndata structure the cache represents. For example, the cache representing\ninodes stores instances of inode structures, and the cache representing process\ndescriptors stores instances of process descriptor structures. The relationship\namong slabs, caches, and objects is shown in Figure 18.5. The ﬁgure shows two\nkernel objects 3 KB in size and three objects 7 KB in size. These objects are stored\nin the respective caches for 3-KB and 7-KB objects.",
  "The slab-allocation algorithm uses caches to store kernel objects. When a\ncache is created, a number of objects are allocated to the cache. The number of\nobjects in the cache depends on the size of the associated slab. For example,\na 12-KB slab (made up of three contiguous 4-KB pages) could store six 2-KB\nobjects. Initially, all the objects in the cache are marked as free. When a new\nobject for a kernel data structure is needed, the allocator can assign any free\nobject from the cache to satisfy the request. The object assigned from the cache\nis marked as used.\nLet’s consider a scenario in which the kernel requests memory from the\nslab allocator for an object representing a process descriptor. In Linux systems,\na process descriptor is of the type struct task struct, which requires",
  "approximately 1.7 KB of memory. When the Linux kernel creates a new task,\nit requests the necessary memory for the struct task struct object from its\ncache. The cache will fulﬁll the request using a struct task struct object\nthat has already been allocated in a slab and is marked as free.\nIn Linux, a slab may be in one of three possible states:\n1. Full. All objects in the slab are marked as used.\n2. Empty. All objects in the slab are marked as free.\n3. Partial. The slab consists of both used and free objects.\nThe slab allocator ﬁrst attempts to satisfy the request with a free object in a\npartial slab. If none exist, a free object is assigned from an empty slab. If no\nempty slabs are available, a new slab is allocated from contiguous physical 804\nChapter 18\nThe Linux System",
  "Chapter 18\nThe Linux System\npages and assigned to a cache; memory for the object is allocated from this\nslab.\nTwo other main subsystems in Linux do their own management of physical\npages: the page cache and the virtual memory system. These systems are closely\nrelated to each other. The page cache is the kernel’s main cache for ﬁles and\nis the main mechanism through which I/O to block devices (Section 18.8.1)\nis performed. File systems of all types, including the native Linux disk-based\nﬁle systems and the NFS networked ﬁle system, perform their I/O through\nthe page cache. The page cache stores entire pages of ﬁle contents and is not\nlimited to block devices. It can also cache networked data. The virtual memory\nsystem manages the contents of each process’s virtual address space. These",
  "two systems interact closely with each other because reading a page of data\ninto the page cache requires mapping pages in the page cache using the virtual\nmemory system. In the following section, we look at the virtual memory system\nin greater detail.\n18.6.2\nVirtual Memory\nThe Linux virtual memory system is responsible for maintaining the address\nspace accessible to each process. It creates pages of virtual memory on demand\nand manages loading those pages from disk and swapping them back out to\ndisk as required. Under Linux, the virtual memory manager maintains two\nseparate views of a process’s address space: as a set of separate regions and as\na set of pages.\nThe ﬁrst view of an address space is the logical view, describing instructions",
  "that the virtual memory system has received concerning the layout of the\naddressspace. Inthisview, the address space consistsofaset ofnonoverlapping\nregions, each region representing a continuous, page-aligned subset of the\naddress space. Each region is described internally by a single vm area struct\nstructure that deﬁnes the properties of the region, including the process’s read,\nwrite, and execute permissions in the region as well as information about any\nﬁles associated with the region. The regions for each address space are linked\ninto a balanced binary tree to allow fast lookup of the region corresponding to\nany virtual address.\nThe kernel also maintains a second, physical view of each address space.\nThis view is stored in the hardware page tables for the process. The page-",
  "table entries identify the exact current location of each page of virtual memory,\nwhether it is on disk or in physical memory. The physical view is managed by a\nset of routines, which are invoked from the kernel’s software-interrupt handlers\nwhenever a process tries to access a page that is not currently present in the\npage tables. Each vm area struct in the address-space description contains a\nﬁeld pointing to a table of functions that implement the key page-management\nfunctionality for any given virtual memory region. All requests to read or write\nan unavailable page are eventually dispatched to the appropriate handler\nin the function table for the vm area struct, so that the central memory-\nmanagement routines do not have to know the details of managing each",
  "possible type of memory region. 18.6\nMemory Management\n805\n18.6.2.1\nVirtual Memory Regions\nLinux implements several types of virtual memory regions. One property\nthat characterizes virtual memory is the backing store for the region, which\ndescribes where the pages for the region come from. Most memory regions\nare backed either by a ﬁle or by nothing. A region backed by nothing is the\nsimplest type of virtual memory region. Such a region represents demand-zero\nmemory: when a process tries to read a page in such a region, it is simply given\nback a page of memory ﬁlled with zeros.\nA region backed by a ﬁle acts as a viewport onto a section of that ﬁle.\nWhenever the process tries to access a page within that region, the page table",
  "is ﬁlled with the address of a page within the kernel’s page cache corresponding\nto the appropriate offset in the ﬁle. The same page of physical memory is used\nby both the page cache and the process’s page tables, so any changes made to\nthe ﬁle by the ﬁle system are immediately visible to any processes that have\nmapped that ﬁle into their address space. Any number of processes can map\nthe same region of the same ﬁle, and they will all end up using the same page\nof physical memory for the purpose.\nA virtual memory region is also deﬁned by its reaction to writes. The\nmapping of a region into the process’s address space can be either private or\nshared. If a process writes to a privately mapped region, then the pager detects",
  "that a copy-on-write is necessary to keep the changes local to the process. In\ncontrast, writes to a shared region result in updating of the object mapped into\nthat region, so that the change will be visible immediately to any other process\nthat is mapping that object.\n18.6.2.2\nLifetime of a Virtual Address Space\nThe kernel creates a new virtual address space in two situations: when a process\nruns a new program with the exec() system call and when a new process is\ncreated by the fork() system call. The ﬁrst case is easy. When a new program is\nexecuted, the process is given a new, completely empty virtual address space.\nIt is up to the routines for loading the program to populate the address space\nwith virtual memory regions.",
  "with virtual memory regions.\nThe second case, creating a new process with fork(), involves creating\na complete copy of the existing process’s virtual address space. The kernel\ncopies the parent process’s vm area struct descriptors, then creates a new set\nof page tables for the child. The parent’s page tables are copied directly into\nthe child’s, and the reference count of each page covered is incremented. Thus,\nafter the fork, the parent and child share the same physical pages of memory\nin their address spaces.\nA special case occurs when the copying operation reaches a virtual memory\nregion that is mapped privately. Any pages to which the parent process has\nwritten within such a region are private, and subsequent changes to these pages",
  "by either the parent or the child must not update the page in the other process’s\naddress space. When the page-table entries for such regions are copied, they\nare set to be read only and are marked for copy-on-write. As long as neither\nprocess modiﬁes these pages, the two processes share the same page of physical\nmemory. However, if either process tries to modify a copy-on-write page, the\nreference count on the page is checked. If the page is still shared, then the 806\nChapter 18\nThe Linux System\nprocess copies the page’s contents to a brand-new page of physical memory\nand uses its copy instead. This mechanism ensures that private data pages are\nshared between processes whenever possible and copies are made only when\nabsolutely necessary.\n18.6.2.3\nSwapping and Paging",
  "18.6.2.3\nSwapping and Paging\nAn important task for a virtual memory system is to relocate pages of memory\nfrom physical memory out to disk when that memory is needed. Early UNIX\nsystems performed this relocation by swapping out the contents of entire\nprocesses at once, but modern versions of UNIX rely more on paging—the\nmovement of individual pages of virtual memory between physical memory\nand disk. Linux does not implement whole-process swapping; it uses the newer\npaging mechanism exclusively.\nThe paging system can be divided into two sections. First, the policy\nalgorithm decides which pages to write out to disk and when to write them.\nSecond, the paging mechanism carries out the transfer and pages data back\ninto physical memory when they are needed again.",
  "into physical memory when they are needed again.\nLinux’s pageout policy uses a modiﬁed version of the standard clock (or\nsecond-chance) algorithm described in Section 9.4.5.2. Under Linux, a multiple-\npass clock is used, and every page has an age that is adjusted on each pass of\nthe clock. The age is more precisely a measure of the page’s youthfulness, or\nhow much activity the page has seen recently. Frequently accessed pages will\nattain a higher age value, but the age of infrequently accessed pages will drop\ntoward zero with each pass. This age valuing allows the pager to select pages\nto page out based on a least frequently used (LFU) policy.\nThe paging mechanism supports paging both to dedicated swap devices\nand partitions and to normal ﬁles, although swapping to a ﬁle is signiﬁcantly",
  "slower due to the extra overhead incurred by the ﬁle system. Blocks are\nallocated from the swap devices according to a bitmap of used blocks, which\nis maintained in physical memory at all times. The allocator uses a next-ﬁt\nalgorithm to try to write out pages to continuous runs of disk blocks for\nimproved performance. The allocator records the fact that a page has been\npaged out to disk by using a feature of the page tables on modern processors:\nthe page-table entry’s page-not-present bit is set, allowing the rest of the page-\ntable entry to be ﬁlled with an index identifying where the page has been\nwritten.\n18.6.2.4\nKernel Virtual Memory\nLinux reserves for its own internal use a constant, architecture-dependent\nregion of the virtual address space of every process. The page-table entries",
  "that map to these kernel pages are marked as protected, so that the pages are\nnot visible or modiﬁable when the processor is running in user mode. This\nkernel virtual memory area contains two regions. The ﬁrst is a static area that\ncontains page-table references to every available physical page of memory\nin the system, so that a simple translation from physical to virtual addresses\noccurs when kernel code is run. The core of the kernel, along with all pages\nallocated by the normal page allocator, resides in this region. 18.6\nMemory Management\n807\nThe remainder of the kernel’s reserved section of address space is not\nreserved for any speciﬁc purpose. Page-table entries in this address range\ncan be modiﬁed by the kernel to point to any other areas of memory. The",
  "kernel provides a pair of facilities that allow kernel code to use this virtual\nmemory. The vmalloc() function allocates an arbitrary number of physical\npages of memory that may not be physically contiguous into a single region of\nvirtually contiguous kernel memory. The vremap() function maps a sequence\nof virtual addresses to point to an area of memory used by a device driver for\nmemory-mapped I/O.\n18.6.3\nExecution and Loading of User Programs\nThe Linux kernel’s execution of user programs is triggered by a call to\nthe exec() system call. This exec() call commands the kernel to run a\nnew program within the current process, completely overwriting the current\nexecution context with the initial context of the new program. The ﬁrst job of",
  "this system service is to verify that the calling process has permission rights to\nthe ﬁle being executed. Once that matter has been checked, the kernel invokes\na loader routine to start running the program. The loader does not necessarily\nload the contents of the program ﬁle into physical memory, but it does at least\nset up the mapping of the program into virtual memory.\nThere is no single routine in Linux for loading a new program. Instead,\nLinux maintains a table of possible loader functions, and it gives each such\nfunction the opportunity to try loading the given ﬁle when an exec() system\ncall is made. The initial reason for this loader table was that, between the\nreleases of the 1.0 and 1.2 kernels, the standard format for Linux’s binary ﬁles",
  "was changed. Older Linux kernels understood the a.out format for binary\nﬁles—a relatively simple format common on older UNIX systems. Newer\nLinux systems use the more modern ELF format, now supported by most\ncurrent UNIX implementations. ELF has a number of advantages over a.out,\nincluding ﬂexibility and extendability. New sections can be added to an ELF\nbinary (for example, to add extra debugging information) without causing\nthe loader routines to become confused. By allowing registration of multiple\nloader routines, Linux can easily support the ELF and a.out binary formats in\na single running system.\nIn Section 18.6.3.1 and Section 18.6.3.2, we concentrate exclusively on the\nloading and running of ELF-format binaries. The procedure for loading a.out",
  "binaries is simpler but similar in operation.\n18.6.3.1\nMapping of Programs into Memory\nUnder Linux, the binaryloader doesnot load abinaryﬁle intophysical memory.\nRather, the pages of the binary ﬁle are mapped into regions of virtual memory.\nOnly when the program tries to access a given page will a page fault result in\nthe loading of that page into physical memory using demand paging.\nIt is the responsibility of the kernel’s binary loader to set up the initial\nmemory mapping. An ELF-format binary ﬁle consists of a header followed by\nseveral page-aligned sections. The ELF loader works by reading the header and\nmapping the sections of the ﬁle into separate regions of virtual memory.\nFigure 18.6 shows the typical layout of memory regions set up by the ELF",
  "loader. In a reserved region at one end of the address space sits the kernel, in 808\nChapter 18\nThe Linux System\nkernel virtual memory\n memory invisible to user-mode code\nstack\nmemory-mapped region\nmemory-mapped region\nmemory-mapped region\nrun-time data\nuninitialized data\ninitialized data\nprogram text\nthe ‘brk’ pointer\nforbidden region\nFigure 18.6\nMemory layout for ELF programs.\nits own privileged region of virtual memory inaccessible to normal user-mode\nprograms. The rest of virtual memory is available to applications, which can use\nthe kernel’s memory-mapping functions to create regions that map a portion\nof a ﬁle or that are available for application data.\nThe loader’s job is to set up the initial memory mapping to allow the",
  "execution of the program to start. The regions that need to be initialized include\nthe stack and the program’s text and data regions.\nThe stack is created at the top of the user-mode virtual memory; it\ngrows downward toward lower-numbered addresses. It includes copies of the\narguments and environment variables given to the program in the exec()\nsystem call. The other regions are created near the bottom end of virtual\nmemory. The sections of the binary ﬁle that contain program text or read-only\ndata are mapped into memory as a write-protected region. Writable initialized\ndata are mapped next; then any uninitialized data are mapped in as a private\ndemand-zero region.\nDirectly beyond these ﬁxed-sized regions is a variable-sized region that",
  "programs can expand as needed to hold data allocated at run time. Each\nprocess has a pointer, brk, that points to the current extent of this data region,\nand processes can extend or contract their brk region with a single system call\n—sbrk().\nOnce these mappings have been set up, the loader initializes the process’s\nprogram-counter register with the starting point recorded in the ELF header,\nand the process can be scheduled.\n18.6.3.2\nStatic and Dynamic Linking\nOnce the program has been loaded and has started running, all the necessary\ncontents of the binary ﬁle have been loaded into the process’s virtual address 18.7\nFile Systems\n809\nspace. However, most programs also need to run functions from the system\nlibraries, and these library functions must also be loaded. In the simplest",
  "case, the necessary library functions are embedded directly in the program’s\nexecutable binary ﬁle. Such a program is statically linked to its libraries, and\nstatically linked executables can commence running as soon as they are loaded.\nThe main disadvantage of static linking is that every program generated\nmust contain copies of exactly the same common system library functions. It is\nmuch more efﬁcient, in terms of both physical memory and disk-space usage,\nto load the system libraries into memory only once. Dynamic linking allows\nthat to happen.\nLinux implements dynamic linking in user mode through a special linker\nlibrary. Every dynamically linked program contains a small, statically linked\nfunction that is called when the program starts. This static function just maps",
  "the link library into memory and runs the code that the function contains. The\nlink library determines the dynamic libraries required by the program and the\nnames of the variables and functions needed from those libraries by reading the\ninformation contained in sections of the ELF binary. It then maps the libraries\ninto the middle of virtual memory and resolves the references to the symbols\ncontained in those libraries. It does not matter exactly where in memory these\nshared libraries are mapped: they are compiled into position-independent\ncode (PIC), which can run at any address in memory.\n18.7 File Systems\nLinux retains UNIX’s standard ﬁle-system model. In UNIX, a ﬁle does not have\nto be an object stored on disk or fetched over a network from a remote ﬁle",
  "server. Rather, UNIX ﬁles can be anything capable of handling the input or\noutput of a stream of data. Device drivers can appear as ﬁles, and interprocess-\ncommunication channels or network connections also look like ﬁles to the\nuser.\nThe Linux kernel handles all these types of ﬁles by hiding the implemen-\ntation details of any single ﬁle type behind a layer of software, the virtual ﬁle\nsystem (VFS). Here, we ﬁrst cover the virtual ﬁle system and then discuss the\nstandard Linux ﬁle system—ext3.\n18.7.1\nThe Virtual File System\nThe Linux VFS is designed around object-oriented principles. It has two\ncomponents: a set of deﬁnitions that specify what ﬁle-system objects are\nallowed to look like and a layer of software to manipulate the objects. The\nVFS deﬁnes four main object types:",
  "VFS deﬁnes four main object types:\n• An inode object represents an individual ﬁle.\n• A ﬁle object represents an open ﬁle.\n• A superblock object represents an entire ﬁle system.\n• A dentry object represents an individual directory entry. 810\nChapter 18\nThe Linux System\nFor each of these four object types, the VFS deﬁnes a set of operations.\nEvery object of one of these types contains a pointer to a function table. The\nfunction table lists the addresses of the actual functions that implement the\ndeﬁned operations for that object. For example, an abbreviated API for some of\nthe ﬁle object’s operations includes:\n• int open(. . .) — Open a ﬁle.\n• ssize t read(. . .) — Read from a ﬁle.\n• ssize t write(. . .) — Write to a ﬁle.\n• int mmap(. . .) — Memory-map a ﬁle.",
  "• int mmap(. . .) — Memory-map a ﬁle.\nThe complete deﬁnition of the ﬁle object is speciﬁed in the struct\nfile operations, which is located in the ﬁle /usr/include/linux/fs.h.\nAn implementation of the ﬁle object (for a speciﬁc ﬁle type) is required to\nimplement each function speciﬁed in the deﬁnition of the ﬁle object.\nThe VFS software layer can perform an operation on one of the ﬁle-system\nobjects by calling the appropriate function from the object’s function table,\nwithout having to know in advance exactly what kind of object it is dealing\nwith. The VFS does not know, or care, whether an inode represents a networked\nﬁle, a disk ﬁle, a network socket, or a directory ﬁle. The appropriate function\nfor that ﬁle’s read() operation will always be at the same place in its function",
  "table, and the VFS software layer will call that function without caring how the\ndata are actually read.\nThe inode and ﬁle objects are the mechanisms used to access ﬁles. An inode\nobject is a data structure containing pointers to the disk blocks that contain the\nactual ﬁle contents, and a ﬁle object represents a point of access to the data in an\nopen ﬁle. A process cannot access an inode’s contents without ﬁrst obtaining a\nﬁle object pointing to the inode. The ﬁle object keeps track of where in the ﬁle\nthe process is currently reading or writing, to keep track of sequential ﬁle I/O.\nIt also remembers the permissions (for example, read or write) requested when\nthe ﬁle was opened and tracks the process’s activity if necessary to perform",
  "adaptive read-ahead, fetching ﬁle data into memory before the process requests\nthe data, to improve performance.\nFile objects typically belong to a single process, but inode objects do not.\nThere is one ﬁle object for every instance of an open ﬁle, but always only a\nsingle inode object. Even when a ﬁle is no longer in use by any process, its\ninode object may still be cached by the VFS to improve performance if the ﬁle\nis used again in the near future. All cached ﬁle data are linked onto a list in the\nﬁle’s inode object. The inode also maintains standard information about each\nﬁle, such as the owner, size, and time most recently modiﬁed.\nDirectory ﬁles are dealt with slightly differently from other ﬁles. The UNIX\nprogramming interface deﬁnes a number of operations on directories, such as",
  "creating, deleting, and renaming a ﬁle in a directory. The system calls for these\ndirectory operations do not require that the user open the ﬁles concerned,\nunlike the case for reading or writing data. The VFS therefore deﬁnes these\ndirectory operations in the inode object, rather than in the ﬁle object.\nThe superblock object represents a connected set of ﬁles that form a\nself-contained ﬁle system. The operating-system kernel maintains a single 18.7\nFile Systems\n811\nsuperblock object for each disk device mounted as a ﬁle system and for each\nnetworked ﬁle system currently connected. The main responsibility of the\nsuperblock object is to provide access to inodes. The VFS identiﬁes every\ninode by a unique ﬁle-system/inode number pair, and it ﬁnds the inode",
  "corresponding to a particular inode number by asking the superblock object to\nreturn the inode with that number.\nFinally, a dentry object represents a directory entry, which may include the\nname of a directory in the path name of a ﬁle (such as /usr) or the actual ﬁle\n(such as stdio.h). For example, the ﬁle /usr/include/stdio.h contains the\ndirectory entries (1) /, (2) usr, (3) include, and (4) stdio.h. Each of these\nvalues is represented by a separate dentry object.\nAs an example of how dentry objects are used, consider the situ-\nation in which a process wishes to open the ﬁle with the pathname\n/usr/include/stdio.h using an editor. Because Linux treats directory names\nas ﬁles, translating this path requires ﬁrst obtaining the inode for the root—",
  "/. The operating system must then read through this ﬁle to obtain the inode\nfor the ﬁle include. It must continue this process until it obtains the inode for\nthe ﬁle stdio.h. Because path-name translation can be a time-consuming task,\nLinux maintains a cache of dentry objects, which is consulted during path-name\ntranslation. Obtaining the inode from the dentry cache is considerably faster\nthan having to read the on-disk ﬁle.\n18.7.2\nThe Linux ext3 File System\nThe standard on-disk ﬁle system used by Linux is called ext3, for historical\nreasons. Linux was originally programmed with a Minix-compatible ﬁle\nsystem, to ease exchanging data with the Minix development system, but\nthat ﬁle system was severely restricted by 14-character ﬁle-name limits and a",
  "maximum ﬁle-system size of 64 MB. The Minix ﬁle system was superseded by\na new ﬁle system, which was christened the extended ﬁle system (extfs). A\nlater redesign to improve performance and scalability and to add a few missing\nfeatures led to the second extended ﬁle system (ext2). Further development\nadded journaling capabilities, and the system was renamed the third extended\nﬁle system (ext3). Linux kernel developers are working on augmenting ext3\nwith modern ﬁle-system features such as extents. This new ﬁle system is called\nthe fourth extended ﬁle system (ext4). The rest of this section discusses ext3,\nhowever, since it remains the most-deployed Linux ﬁle system. Most of the\ndiscussion applies equally to ext4.\nLinux’s ext3 has much in common with the BSD Fast File System (FFS)",
  "(Section A.7.7). It uses a similar mechanism for locating the data blocks\nbelonging to a speciﬁc ﬁle, storing data-block pointers in indirect blocks\nthroughout the ﬁle system with up to three levels of indirection. As in FFS,\ndirectory ﬁles are stored on disk just like normal ﬁles, although their contents\nare interpreted differently. Each block in a directory ﬁle consists of a linked list\nof entries. In turn, each entry contains the length of the entry, the name of a\nﬁle, and the inode number of the inode to which that entry refers.\nThe main differences between ext3 and FFS lie in their disk-allocation\npolicies. In FFS, the disk is allocated to ﬁles in blocks of 8 KB. These blocks\nare subdivided into fragments of 1 KB for storage of small ﬁles or partially",
  "ﬁlled blocks at the ends of ﬁles. In contrast, ext3 does not use fragments at all 812\nChapter 18\nThe Linux System\nbut performs all its allocations in smaller units. The default block size on ext3\nvaries as a function of the total size of the ﬁle system. Supported block sizes\nare 1, 2, 4, and 8 KB.\nTo maintain high performance, the operating system must try to perform\nI/O operations in large chunks whenever possible by clustering physically\nadjacent I/O requests. Clustering reduces the per-request overhead incurred by\ndevice drivers, disks, and disk-controller hardware. A block-sized I/O request\nsize is too small to maintain good performance, so ext3 uses allocation policies\ndesigned to place logically adjacent blocks of a ﬁle into physically adjacent",
  "blocks on disk, so that it can submit an I/O request for several disk blocks as a\nsingle operation.\nThe ext3 allocation policy works as follows: As in FFS, an ext3 ﬁle system is\npartitioned into multiple segments. In ext3, these are called block groups. FFS\nuses the similar concept of cylinder groups, where each group corresponds to\na single cylinder of a physical disk. (Note that modern disk-drive technology\npacks sectors onto the disk at different densities, and thus with different\ncylinder sizes, depending on how far the disk head is from the center of the\ndisk. Therefore, ﬁxed-sized cylinder groups do not necessarily correspond to\nthe disk’s geometry.)\nWhen allocating a ﬁle, ext3 must ﬁrst select the block group for that ﬁle.",
  "For data blocks, it attempts to allocate the ﬁle to the block group to which the\nﬁle’s inode has been allocated. For inode allocations, it selects the block group\nin which the ﬁle’s parent directory resides for nondirectory ﬁles. Directory\nﬁles are not kept together but rather are dispersed throughout the available\nblock groups. These policies are designed not only to keep related information\nwithin the same block group but also to spread out the disk load among the\ndisk’s block groups to reduce the fragmentation of any one area of the disk.\nWithin a block group, ext3 tries to keep allocations physically contiguous\nif possible, reducing fragmentation if it can. It maintains a bitmap of all free\nblocks in a block group. When allocating the ﬁrst blocks for a new ﬁle, it",
  "starts searching for a free block from the beginning of the block group. When\nextending a ﬁle, it continues the search from the block most recently allocated\nto the ﬁle. The search is performed in two stages. First, ext3 searches for an\nentire free byte in the bitmap; if it fails to ﬁnd one, it looks for any free bit.\nThe search for free bytes aims to allocate disk space in chunks of at least eight\nblocks where possible.\nOnce a free block has been identiﬁed, the search is extended backward until\nan allocated block is encountered. When a free byte is found in the bitmap,\nthis backward extension prevents ext3 from leaving a hole between the most\nrecently allocated block in the previous nonzero byte and the zero byte found.",
  "Once the next block to be allocated has been found by either bit or byte search,\next3 extends the allocation forward for up to eight blocks and preallocates\nthese extra blocks to the ﬁle. This preallocation helps to reduce fragmentation\nduring interleaved writes to separate ﬁles and also reduces the CPU cost of\ndisk allocation by allocating multiple blocks simultaneously. The preallocated\nblocks are returned to the free-space bitmap when the ﬁle is closed.\nFigure 18.7 illustrates the allocation policies. Each row represents a\nsequence of set and unset bits in an allocation bitmap, indicating used and\nfree blocks on disk. In the ﬁrst case, if we can ﬁnd any free blocks sufﬁciently\nnear the start of the search, then we allocate them no matter how fragmented 18.7\nFile Systems\n813",
  "File Systems\n813\nallocating scattered free blocks\nallocating continuous free blocks\nblock in use\nbit boundary\nblock selected\nby allocator\nfree block\nbyte boundary\nbitmap search\nFigure 18.7\next3 block-allocation policies.\nthey may be. The fragmentation is partially compensated for by the fact that\nthe blocks are close together and can probably all be read without any disk\nseeks. Furthermore, allocating them all to one ﬁle is better in the long run than\nallocating isolated blocks to separate ﬁles once large free areas become scarce\non disk. In the second case, we have not immediately found a free block close\nby, so we search forward for an entire free byte in the bitmap. If we allocated\nthat byte as a whole, we would end up creating a fragmented area of free space",
  "between it and the allocation preceding it. Thus, before allocating, we back\nup to make this allocation ﬂush with the allocation preceding it, and then we\nallocate forward to satisfy the default allocation of eight blocks.\n18.7.3\nJournaling\nThe ext3 ﬁle system supports a popular feature called journaling, whereby\nmodiﬁcations to the ﬁle system are written sequentially to a journal. A set of\noperations that performs a speciﬁc task is a transaction. Once a transaction\nis written to the journal, it is considered to be committed. Meanwhile, the\njournal entries relating to the transaction are replayed across the actual ﬁle-\nsystem structures. As the changes are made, a pointer is updated to indicate\nwhich actions have completed and which are still incomplete. When an entire",
  "committed transaction is completed, it is removed from the journal. The journal,\nwhich is actually a circular buffer, may be in a separate section of the ﬁle\nsystem, or it may even be on a separate disk spindle. It is more efﬁcient, but\nmore complex, to have it under separate read–write heads, thereby decreasing\nhead contention and seek times.\nIf the system crashes, some transactions may remain in the journal. Those\ntransactions were never completed to the ﬁle system even though they were\ncommitted by the operating system, so they must be completed once the system 814\nChapter 18\nThe Linux System\nrecovers. The transactions can be executed from the pointer until the work is\ncomplete, and the ﬁle-system structures remain consistent. The only problem",
  "occurs when a transaction has been aborted—that is, it was not committed\nbefore the system crashed. Any changes from those transactions that were\napplied to the ﬁle system must be undone, again preserving the consistency of\nthe ﬁle system. This recovery is all that is needed after a crash, eliminating all\nproblems with consistency checking.\nJournaling ﬁle systems may perform some operations faster than non-\njournaling systems, as updates proceed much faster when they are applied\nto the in-memory journal rather than directly to the on-disk data structures.\nThe reason for this improvement is found in the performance advantage of\nsequential I/O over random I/O. Costly synchronous random writes to the ﬁle\nsystem are turned into much less costly synchronous sequential writes to the",
  "ﬁle system’s journal. Those changes, in turn, are replayed asynchronously via\nrandom writes to the appropriate structures. The overall result is a signiﬁcant\ngain in performance of ﬁle-system metadata-oriented operations, such as ﬁle\ncreation and deletion. Due to this performance improvement, ext3 can be\nconﬁgured to journal only metadata and not ﬁle data.\n18.7.4\nThe Linux Process File System\nThe ﬂexibility of the Linux VFS enables us to implement a ﬁle system that does\nnot store data persistently at all but rather provides an interface to some other\nfunctionality. The Linux process ﬁle system, known as the /proc ﬁle system,\nis an example of a ﬁle system whose contents are not actually stored anywhere\nbut are computed on demand according to user ﬁle I/O requests.",
  "A /proc ﬁle system is not unique to Linux. SVR4 UNIX introduced a /proc\nﬁle system as an efﬁcient interface to the kernel’s process debugging support.\nEach subdirectory of the ﬁle system corresponded not to a directory on any\ndisk but rather to an active process on the current system. A listing of the ﬁle\nsystem reveals one directory per process, with the directory name being the\nASCII decimal representation of the process’s unique process identiﬁer (PID).\nLinux implements such a /proc ﬁle system but extends it greatly by\nadding a number of extra directories and text ﬁles under the ﬁle system’s root\ndirectory. These new entries correspond to various statistics about the kernel\nand the associated loaded drivers. The /proc ﬁle system provides a way for",
  "programs to access this information as plain text ﬁles; the standard UNIX user\nenvironment provides powerful tools to process such ﬁles. For example, in\nthe past, the traditional UNIX ps command for listing the states of all running\nprocesses has been implemented as a privileged process that reads the process\nstate directly from the kernel’s virtual memory. Under Linux, this command\nis implemented as an entirely unprivileged program that simply parses and\nformats the information from /proc.\nThe /proc ﬁle system must implement two things: a directory structure\nand the ﬁle contents within. Because a UNIX ﬁle system is deﬁned as a set of ﬁle\nand directory inodes identiﬁed by their inode numbers, the /proc ﬁle system\nmust deﬁne a unique and persistent inode number for each directory and the",
  "associated ﬁles. Once such a mapping exists, the ﬁle system can use this inode\nnumber to identify just what operation is required when a user tries to read\nfrom a particular ﬁle inode or to perform a lookup in a particular directory 18.8\nInput and Output\n815\ninode. When data are read from one of these ﬁles, the /proc ﬁle system will\ncollect the appropriate information, format it into textual form, and place it\ninto the requesting process’s read buffer.\nThe mapping from inode number to information type splits the inode\nnumber into two ﬁelds. In Linux, a PID is 16 bits in size, but an inode number\nis 32 bits. The top 16 bits of the inode number are interpreted as a PID, and the\nremaining bits deﬁne what type of information is being requested about that\nprocess.",
  "process.\nA PID of zero is not valid, so a zero PID ﬁeld in the inode number is\ntaken to mean that this inode contains global—rather than process-speciﬁc—\ninformation. Separate global ﬁles exist in /proc to report information such as\nthe kernel version, free memory, performance statistics, and drivers currently\nrunning.\nNot all the inode numbers in this range are reserved. The kernel can allocate\nnew /proc inode mappings dynamically, maintaining a bitmap of allocated\ninode numbers. It also maintains a tree data structure of registered global /proc\nﬁle-system entries. Each entry contains the ﬁle’s inode number, ﬁle name, and\naccess permissions, along with the special functions used to generate the ﬁle’s\ncontents. Drivers can register and deregister entries in this tree at any time,",
  "and a special section of the tree—appearing under the /proc/sys directory\n—is reserved for kernel variables. Files under this tree are managed by a set\nof common handlers that allow both reading and writing of these variables,\nso a system administrator can tune the value of kernel parameters simply by\nwriting out the new desired values in ASCII decimal to the appropriate ﬁle.\nTo allow efﬁcient access to these variables from within applications, the\n/proc/sys subtree is made available through a special system call, sysctl(),\nthat reads and writes the same variables in binary, rather than in text, without\nthe overhead of the ﬁle system. sysctl() is not an extra facility; it simply reads\nthe /proc dynamic entry tree to identify the variables to which the application\nis referring.",
  "is referring.\n18.8 Input and Output\nTo the user, the I/O system in Linux looks much like that in any UNIX system.\nThat is, to the extent possible, all device drivers appear as normal ﬁles. Users\ncan open an access channel to a device in the same way they open any\nother ﬁle—devices can appear as objects within the ﬁle system. The system\nadministrator can create special ﬁles within a ﬁle system that contain references\nto a speciﬁc device driver, and a user opening such a ﬁle will be able to read\nfrom and write to the device referenced. By using the normal ﬁle-protection\nsystem, which determines who can access which ﬁle, the administrator can set\naccess permissions for each device.\nLinux splits all devices into three classes: block devices, character devices,",
  "and network devices. Figure 18.8 illustrates the overall structure of the device-\ndriver system.\nBlock devices include all devices that allow random access to completely\nindependent, ﬁxed-sized blocks of data, including hard disks and ﬂoppy disks,\nCD-ROMs and Blu-ray discs, and ﬂash memory. Block devices are typically 816\nChapter 18\nThe Linux System\nused to store ﬁle systems, but direct access to a block device is also allowed\nso that programs can create and repair the ﬁle system that the device contains.\nApplications can also access these block devices directly if they wish. For\nexample, a database application may prefer to perform its own ﬁne-tuned\nlayout of data onto a disk rather than using the general-purpose ﬁle system.",
  "Character devices include most other devices, such as mice and keyboards.\nThe fundamental difference between block and character devices is random\naccess—block devices are accessed randomly, while character devices are\naccessed serially. For example, seeking to a certain position in a ﬁle might\nbe supported for a DVD but makes no sense for a pointing device such as a\nmouse.\nNetwork devices are dealt with differently from block and character\ndevices. Users cannot directly transfer data to network devices. Instead,\nthey must communicate indirectly by opening a connection to the kernel’s\nnetworking subsystem. We discuss the interface to network devices separately\nin Section 18.10.\n18.8.1\nBlock Devices\nBlock devices provide the main interface to all disk devices in a system.",
  "Performance is particularly important for disks, and the block-device system\nmust provide functionality to ensure that disk access is as fast as possible. This\nfunctionality is achieved through the scheduling of I/O operations.\nIn the context of block devices, a block represents the unit with which the\nkernel performs I/O. When a block is read into memory, it is stored in a buffer.\nThe request manager is the layer of software that manages the reading and\nwriting of buffer contents to and from a block-device driver.\nA separate list of requests is kept for each block-device driver. Traditionally,\nthese requests have been scheduled according to a unidirectional-elevator\n(C-SCAN) algorithm that exploits the order in which requests are inserted in",
  "and removed from the lists. The request lists are maintained in sorted order of\nincreasing starting-sector number. When a request is accepted for processing\nby a block-device driver, it is not removed from the list. It is removed only after\nthe I/O is complete, at which point the driver continues with the next request\nin the list, even if new requests have been inserted in the list before the active\nfile system\nblock\ndevice file\ncharacter\ndevice file\nprotocol\ndriver\nline\ndiscipline\nTTY driver\nI/O scheduler\nSCSI manager\nSCSI device\ndriver\nblock\ndevice\ndriver\ncharacter\ndevice\ndriver\nnetwork\nsocket\nnetwork\ndevice\ndriver\nuser application\nFigure 18.8\nDevice-driver block structure. 18.8\nInput and Output\n817\nrequest. As new I/O requests are made, the request manager attempts to merge",
  "requests in the lists.\nLinux kernel version 2.6 introduced a new I/O scheduling algorithm.\nAlthough a simple elevator algorithm remains available, the default I/O\nscheduler is now the Completely Fair Queueing (CFQ) scheduler. The CFQ I/O\nscheduler is fundamentally different from elevator-based algorithms. Instead\nof sorting requests into a list, CFQ maintains a set of lists—by default, one\nfor each process. Requests originating from a process go in that process’s list.\nFor example, if two processes are issuing I/O requests, CFQ will maintain\ntwo separate lists of requests, one for each process. The lists are maintained\naccording to the C-SCAN algorithm.\nCFQ services the lists differently as well. Where a traditional C-SCAN",
  "algorithm is indifferent to a speciﬁc process, CFQ services each process’s list\nround-robin. It pulls a conﬁgurable number of requests (by default, four)\nfrom each list before moving on to the next. This method results in fairness\nat the process level—each process receives an equal fraction of the disk’s\nbandwidth. The result is beneﬁcial with interactive workloads where I/O\nlatency is important. In practice, however, CFQ performs well with most\nworkloads.\n18.8.2\nCharacter Devices\nA character-device driver can be almost any device driver that does not offer\nrandom access to ﬁxed blocks of data. Any character-device drivers registered\nto the Linux kernel must also register a set of functions that implement the\nﬁle I/O operations that the driver can handle. The kernel performs almost no",
  "preprocessing of a ﬁle read or write request to a character device. It simply\npasses the request to the device in question and lets the device deal with the\nrequest.\nThe main exception to this rule is the special subset of character-device\ndrivers that implement terminal devices. The kernel maintains a standard\ninterface to these drivers by means of a set of tty struct structures. Each of\nthese structures provides buffering and ﬂow control on the data stream from\nthe terminal device and feeds those data to a line discipline.\nA line discipline is an interpreter for the information from the terminal\ndevice. The most common line discipline is the tty discipline, which glues the\nterminal’s data stream onto the standard input and output streams of a user’s",
  "running processes, allowing those processes to communicate directly with the\nuser’s terminal. This job is complicated by the fact that several such processes\nmay be running simultaneously, and the tty line discipline is responsible for\nattaching and detaching the terminal’s input and output from the various\nprocesses connected to it as those processes are suspended or awakened by the\nuser.\nOther line disciplines also are implemented that have nothing to do with\nI/O to a user process. The PPP and SLIP networking protocols are ways of\nencoding a networking connection over a terminal device such as a serial\nline. These protocols are implemented under Linux as drivers that at one end\nappear to the terminal system as line disciplines and at the other end appear",
  "to the networking system as network-device drivers. After one of these line\ndisciplines has been enabled on a terminal device, any data appearing on that\nterminal will be routed directly to the appropriate network-device driver. 818\nChapter 18\nThe Linux System\n18.9 Interprocess Communication\nLinux provides a rich environment for processes to communicate with each\nother. Communication may be just a matter of letting another process know\nthat some event has occurred, or it may involve transferring data from one\nprocess to another.\n18.9.1\nSynchronization and Signals\nThe standard Linux mechanism for informing a process that an event has\noccurred is the signal. Signals can be sent from any process to any other\nprocess, with restrictions on signals sent to processes owned by another user.",
  "However, a limited number of signals are available, and they cannot carry\ninformation. Only the fact that a signal has occurred is available to a process.\nSignals are not generated only by processes. The kernel also generates signals\ninternally. For example, it can send a signal to a server process when data\narrive on a network channel, to a parent process when a child terminates, or to\na waiting process when a timer expires.\nInternally, the Linux kernel does not use signals to communicate with\nprocesses running in kernel mode. If a kernel-mode process is expecting an\nevent to occur, it will not use signals to receive notiﬁcation of that event.\nRather, communication about incoming asynchronous events within the kernel",
  "takes place through the use of scheduling states and wait queue structures.\nThese mechanisms allow kernel-mode processes to inform one another about\nrelevant events, and they also allow events to be generated by device drivers or\nby the networking system. Whenever a process wants to wait for some event\nto complete, it places itself on a wait queue associated with that event and\ntells the scheduler that it is no longer eligible for execution. Once the event has\ncompleted, every process on the wait queue will be awoken. This procedure\nallows multiple processes to wait for a single event. For example, if several\nprocesses are trying to read a ﬁle from a disk, then they will all be awakened\nonce the data have been read into memory successfully.",
  "Although signals have always been the main mechanism for commu-\nnicating asynchronous events among processes, Linux also implements the\nsemaphore mechanism of System V UNIX. A process can wait on a semaphore\nas easily as it can wait for a signal, but semaphores have two advantages:\nlarge numbers of semaphores can be shared among multiple independent pro-\ncesses, and operations on multiple semaphores can be performed atomically.\nInternally, the standard Linux wait queue mechanism synchronizes processes\nthat are communicating with semaphores.\n18.9.2\nPassing of Data among Processes\nLinux offers several mechanisms for passing data among processes. The stan-\ndard UNIX pipe mechanism allows a child process to inherit a communication",
  "channel from its parent; data written to one end of the pipe can be read at the\nother. Under Linux, pipes appear as just another type of inode to virtual ﬁle\nsystem software, and each pipe has a pair of wait queues to synchronize the\nreader and writer. UNIX also deﬁnes a set of networking facilities that can send\nstreams of data to both local and remote processes. Networking is covered in\nSection 18.10. 18.10\nNetwork Structure\n819\nAnother process communications method, shared memory, offers an\nextremely fast way to communicate large or small amounts of data. Any data\nwritten by one process to a shared memory region can be read immediately by\nany other process that has mapped that region into its address space. The main",
  "disadvantage of shared memory is that, on its own, it offers no synchronization.\nA process can neither ask the operating system whether a piece of shared\nmemory has been written to nor suspend execution until such a write occurs.\nShared memory becomes particularly powerful when used in conjunction with\nanother interprocess-communication mechanism that provides the missing\nsynchronization.\nA shared-memory region in Linux is a persistent object that can be created\nor deleted by processes. Such an object is treated as though it were a small,\nindependent address space. The Linux paging algorithms can elect to page\nshared-memory pages out to disk, just as they can page out a process’s data\npages. The shared-memory object acts as a backing store for shared-memory",
  "regions, just as a ﬁle can act as a backing store for a memory-mapped memory\nregion. When a ﬁle is mapped into a virtual address space region, then any\npage faults that occur cause the appropriate page of the ﬁle to be mapped into\nvirtual memory. Similarly, shared-memory mappings direct page faults to map\nin pages from a persistent shared-memory object. Also just as for ﬁles, shared-\nmemory objects remember their contents even if no processes are currently\nmapping them into virtual memory.\n18.10 Network Structure\nNetworking is a key area of functionality for Linux. Not only does Linux\nsupport the standard Internet protocols used for most UNIX-to-UNIX com-\nmunications, but it also implements a number of protocols native to other,",
  "non-UNIX operating systems. In particular, since Linux was originally imple-\nmented primarily on PCs, rather than on large workstations or on server-class\nsystems, it supports many of the protocols typically used on PC networks, such\nas AppleTalk and IPX.\nInternally, networking in the Linux kernel is implemented by three layers\nof software:\n1. The socket interface\n2. Protocol drivers\n3. Network-device drivers\nUser applications perform all networking requests through the socket\ninterface. This interface is designed to look like the 4.3 BSD socket layer, so\nthat any programs designed to make use of Berkeley sockets will run on Linux\nwithout any source-code changes. This interface is described in Section A.9.1.\nThe BSD socket interface is sufﬁciently general to represent network addresses",
  "for a wide range of networking protocols. This single interface is used in Linux\nto access not just those protocols implemented on standard BSD systems but all\nthe protocols supported by the system. 820\nChapter 18\nThe Linux System\nThe next layer of software is the protocol stack, which is similar in\norganization to BSD’s own framework. Whenever any networking data arrive at\nthis layer, either from an application’s socket or from a network-device driver,\nthe data are expected to have been tagged with an identiﬁer specifying which\nnetwork protocol they contain. Protocols can communicate with one another\nif they desire; for example, within the Internet protocol set, separate protocols\nmanage routing, error reporting, and reliable retransmission of lost data.",
  "The protocol layer may rewrite packets, create new packets, split or\nreassemble packets into fragments, or simply discard incoming data. Ulti-\nmately, once the protocol layer has ﬁnished processing a set of packets, it\npasses them on, either upward to the socket interface if the data are destined\nfor a local connection or downward to a device driver if the data need to be\ntransmitted remotely. The protocol layer decides to which socket or device it\nwill send the packet.\nAll communication between the layers of the networking stack is per-\nformed by passing single skbuff (socket buffer) structures. Each of these\nstructures contains a set of pointers into a single continuous area of memory,\nrepresenting a buffer inside which network packets can be constructed. The",
  "valid data in a skbuff do not need to start at the beginning of the skbuff’s\nbuffer, and they do not need to run to the end. The networking code can\nadd data to or trim data from either end of the packet, as long as the result\nstill ﬁts into the skbuff. This capacity is especially important on modern\nmicroprocessors, where improvements in CPU speed have far outstripped the\nperformance of main memory. The skbuff architecture allows ﬂexibility in\nmanipulating packet headers and checksums while avoiding any unnecessary\ndata copying.\nThe most important set of protocols in the Linux networking system is the\nTCP/IP protocol suite. This suite comprises a number of separate protocols.\nThe IP protocol implements routing between different hosts anywhere on the",
  "network. On top of the routing protocol are the UDP, TCP, and ICMP protocols.\nThe UDP protocol carries arbitrary individual datagrams between hosts. The\nTCP protocol implements reliable connections between hosts with guaranteed\nin-order delivery of packets and automatic retransmission of lost data. The\nICMP protocol carries various error and status messages between hosts.\nEach packet (skbuff) arriving at the networking stack’s protocol software\nis expected to be already tagged with an internal identiﬁer indicating the\nprotocol to which the packet is relevant. Different networking-device drivers\nencode the protocol type in different ways; thus, the protocol for incoming\ndata must be identiﬁed in the device driver. The device driver uses a hash table",
  "of known networking-protocol identiﬁers to look up the appropriate protocol\nand passes the packet to that protocol. New protocols can be added to the hash\ntable as kernel-loadable modules.\nIncoming IP packets are delivered to the IP driver. The job of this layer\nis to perform routing. After deciding where the packet is to be sent, the IP\ndriver forwards the packet to the appropriate internal protocol driver to be\ndelivered locally or injects it back into a selected network-device-driver queue\nto be forwarded to another host. It performs the routing decision using two\ntables: the persistent forwarding information base (FIB) and a cache of recent\nrouting decisions. The FIB holds routing-conﬁguration information and can",
  "specify routes based either on a speciﬁc destination address or on a wildcard 18.11\nSecurity\n821\nrepresenting multiple destinations. The FIB is organized as a set of hash tables\nindexed by destination address; the tables representing the most speciﬁc routes\nare always searched ﬁrst. Successful lookups from this table are added to\nthe route-caching table, which caches routes only by speciﬁc destination. No\nwildcards are stored in the cache, so lookups can be made quickly. An entry in\nthe route cache expires after a ﬁxed period with no hits.\nAt various stages, the IP software passes packets to a separate section\nof code for ﬁrewall management—selective ﬁltering of packets according\nto arbitrary criteria, usually for security purposes. The ﬁrewall manager",
  "maintains a number of separate ﬁrewall chains and allows a skbuff to be\nmatched against any chain. Chains are reserved for separate purposes: one is\nused for forwarded packets, one for packets being input to this host, and one\nfor data generated at this host. Each chain is held as an ordered list of rules,\nwhere a rule speciﬁes one of a number of possible ﬁrewall-decision functions\nplus some arbitrary data for matching purposes.\nTwo other functions performed by the IP driver are disassembly and\nreassembly of large packets. If an outgoing packet is too large to be queued to\na device, it is simply split up into smaller fragments, which are all queued to\nthe driver. At the receiving host, these fragments must be reassembled. The IP",
  "driver maintains an ipfrag object for each fragment awaiting reassembly and\nan ipq for each datagram being assembled. Incoming fragments are matched\nagainst each known ipq. If a match is found, the fragment is added to it;\notherwise, a new ipq is created. Once the ﬁnal fragment has arrived for a\nipq, a completely new skbuff is constructed to hold the new packet, and this\npacket is passed back into the IP driver.\nPackets identiﬁed by the IP as destined for this host are passed on to one\nof the other protocol drivers. The UDP and TCP protocols share a means of\nassociating packets with source and destination sockets: each connected pair\nof sockets is uniquely identiﬁed by its source and destination addresses and\nby the source and destination port numbers. The socket lists are linked to",
  "hash tables keyed on these four address and port values for socket lookup on\nincoming packets. The TCP protocol has to deal with unreliable connections, so\nit maintains ordered lists of unacknowledged outgoing packets to retransmit\nafter a timeout and of incoming out-of-order packets to be presented to the\nsocket when the missing data have arrived.\n18.11 Security\nLinux’s security model is closely related to typical UNIX security mechanisms.\nThe security concerns can be classiﬁed in two groups:\n1. Authentication. Making sure that nobody can access the system without\nﬁrst proving that she has entry rights\n2. Access control. Providing a mechanism for checking whether a user has\nthe right to access a certain object and preventing access to objects as\nrequired 822\nChapter 18\nThe Linux System",
  "required 822\nChapter 18\nThe Linux System\n18.11.1\nAuthentication\nAuthentication in UNIX has typically been performed through the use of a\npublicly readable password ﬁle. A user’s password is combined with a random\n“salt” value, and the result is encoded with a one-way transformation function\nand stored in the password ﬁle. The use of the one-way function means that\nthe original password cannot be deduced from the password ﬁle except by\ntrial and error. When a user presents a password to the system, the password is\nrecombined with the salt value stored in the password ﬁle and passed through\nthe same one-way transformation. If the result matches the contents of the\npassword ﬁle, then the password is accepted.\nHistorically, UNIX implementations of this mechanism have had several",
  "drawbacks. Passwords were often limited to eight characters, and the number\nof possible salt values was so low that an attacker could easily combine a\ndictionary of commonly used passwords with every possible salt value and\nhave a good chance of matching one or more passwords in the password\nﬁle, gaining unauthorized access to any accounts compromised as a result.\nExtensions to the password mechanism have been introduced that keep the\nencrypted password secret in a ﬁle that is not publicly readable, that allow\nlonger passwords, or that use more secure methods of encoding the password.\nOther authentication mechanisms have been introduced that limit the periods\nduring which a user is permitted to connect to the system. Also, mechanisms",
  "exist to distribute authentication information to all the related systems in a\nnetwork.\nA new security mechanism has been developed by UNIX vendors to\naddress authentication problems. The pluggable authentication modules\n(PAM) system is based on a shared library that can be used by any system\ncomponent that needs to authenticate users. An implementation of this system\nis available under Linux. PAM allows authentication modules to be loaded on\ndemand as speciﬁed in a system-wide conﬁguration ﬁle. If a new authentication\nmechanism is added at a later date, it can be added to the conﬁguration ﬁle,\nand all system components will immediately be able to take advantage of it.\nPAM modules can specify authentication methods, account restrictions, session-",
  "setup functions, and password-changing functions (so that, when users change\ntheir passwords, all the necessary authentication mechanisms can be updated\nat once).\n18.11.2\nAccess Control\nAccess control under UNIX systems, including Linux, is performed through the\nuse of unique numeric identiﬁers. A user identiﬁer (UID) identiﬁes a single user\nor a single set of access rights. A group identiﬁer (GID) is an extra identiﬁer\nthat can be used to identify rights belonging to more than one user.\nAccess control is applied to various objects in the system. Every ﬁle\navailable in the system is protected by the standard access-control mecha-\nnism. In addition, other shared objects, such as shared-memory sections and\nsemaphores, employ the same access system.",
  "semaphores, employ the same access system.\nEvery object in a UNIX system under user and group access control has a\nsingle UID and a single GID associated with it. User processes also have a single\nUID, but they may have more than one GID. If a process’s UID matches the UID\nof an object, then the process has user rights or owner rights to that object. 18.11\nSecurity\n823\nIf the UIDs do not match but any GID of the process matches the object’s GID,\nthen group rights are conferred; otherwise, the process has world rights to the\nobject.\nLinux performs access control by assigning objects a protection mask that\nspeciﬁes which access modes—read, write, or execute—are to be granted to\nprocesses with owner, group, or world access. Thus, the owner of an object",
  "might have full read, write, and execute access to a ﬁle; other users in a certain\ngroup might be given read access but denied write access; and everybody else\nmight be given no access at all.\nThe only exception is the privileged root UID. A process with this special\nUID is granted automatic access to any object in the system, bypassing\nnormal access checks. Such processes are also granted permission to perform\nprivileged operations, such as reading any physical memory or opening\nreserved network sockets. This mechanism allows the kernel to prevent normal\nusers from accessing these resources: most of the kernel’s key internal resources\nare implicitly owned by the root UID.\nLinux implements the standard UNIX setuid mechanism described in",
  "Section A.3.2. This mechanism allows a program to run with privileges different\nfrom those of the user running the program. For example, the lpr program\n(which submits a job to a print queue) has access to the system’s print queues\neven if the user running that program does not. The UNIX implementation of\nsetuid distinguishes between a process’s real and effective UID. The real\nUID is that of the user running the program; the effective UID is that of the ﬁle’s\nowner.\nUnder Linux, this mechanism is augmented in two ways. First, Linux\nimplements the POSIX speciﬁcation’s saved user-id mechanism, which\nallows a process to drop and reacquire its effective UID repeatedly. For security\nreasons, a program may want to perform most of its operations in a safe mode,",
  "waiving the privileges granted by its setuid status; but it may wish to perform\nselected operations with all its privileges. Standard UNIX implementations\nachieve this capacity only by swapping the real and effective UIDs. When this\nis done, the previous effective UID is remembered, but the program’s real UID\ndoes not always correspond to the UID of the user running the program. Saved\nUIDs allow a process to set its effective UID to its real UID and then return to\nthe previous value of its effective UID without having to modify the real UID at\nany time.\nThe second enhancement provided by Linux is the addition of a process\ncharacteristic that grants just a subset of the rights of the effective UID. The\nfsuid and fsgid process properties are used when access rights are granted",
  "to ﬁles. The appropriate property is set every time the effective UID or GID is\nset. However, the fsuid and fsgid can be set independently of the effective ids,\nallowing a process to access ﬁles on behalf of another user without taking on the\nidentity of that other user in any other way. Speciﬁcally, server processes can\nuse this mechanism to serve ﬁles to a certain user without becoming vulnerable\nto being killed or suspended by that user.\nFinally, Linux provides a mechanism for ﬂexible passing of rights from\none program to another—a mechanism that has become common in modern\nversions of UNIX. When a local network socket has been set up between any\ntwo processes on the system, either of those processes may send to the other",
  "process a ﬁle descriptor for one of its open ﬁles; the other process receives a 824\nChapter 18\nThe Linux System\nduplicate ﬁle descriptor for the same ﬁle. This mechanism allows a client to\npass access to a single ﬁle selectively to some server process without granting\nthat process any other privileges. For example, it is no longer necessary for a\nprint server to be able to read all the ﬁles of a user who submits a new print\njob. The print client can simply pass the server ﬁle descriptors for any ﬁles to\nbe printed, denying the server access to any of the user’s other ﬁles.\n18.12 Summary\nLinux is a modern, free operating system based on UNIX standards. It has been\ndesigned to run efﬁciently and reliably on common PC hardware; it also runs on",
  "a variety of other platforms, such as mobile phones. It provides a programming\ninterface and user interface compatible with standard UNIX systems and can\nrun a large number of UNIX applications, including an increasing number of\ncommercially supported applications.\nLinux has not evolved in a vacuum. A complete Linux system includes\nmany components that were developed independently of Linux. The core\nLinux operating-system kernel is entirely original, but it allows much existing\nfree UNIX software to run, resulting in an entire UNIX-compatible operating\nsystem free from proprietary code.\nThe Linux kernel is implemented as a traditional monolithic kernel for\nperformance reasons, but it is modular enough in design to allow most drivers\nto be dynamically loaded and unloaded at run time.",
  "Linux is a multiuser system, providing protection between processes and\nrunning multiple processes according to a time-sharing scheduler. Newly\ncreated processes can share selective parts of their execution environment\nwith their parent processes, allowing multithreaded programming. Interpro-\ncess communication is supported by both System V mechanisms—message\nqueues, semaphores, and shared memory—and BSD’s socket interface. Multi-\nple networking protocols can be accessed simultaneously through the socket\ninterface.\nThe memory-management system uses page sharing and copy-on-write\nto minimize the duplication of data shared by different processes. Pages are\nloaded on demand when they are ﬁrst referenced and are paged back out to",
  "backing store according to an LFU algorithm if physical memory needs to be\nreclaimed.\nTo the user, the ﬁle system appears as a hierarchical directory tree that\nobeys UNIX semantics. Internally, Linux uses an abstraction layer to manage\nmultiple ﬁle systems. Device-oriented, networked, and virtual ﬁle systems are\nsupported. Device-oriented ﬁle systems access disk storage through a page\ncache that is uniﬁed with the virtual memory system.\nPractice Exercises\n18.1\nDynamically loadable kernel modules give ﬂexibility when drivers are\nadded to a system, but do they have disadvantages too? Under what\ncircumstances would a kernel be compiled into a single binary ﬁle, and\nwhen would it be better to keep it split into modules? Explain your\nanswer. Exercises\n825\n18.2",
  "answer. Exercises\n825\n18.2\nMultithreading is a commonly used programming technique. Describe\nthree different ways to implement threads, and compare these three\nmethods with the Linux clone() mechanism. When might using each\nalternative mechanism be better or worse than using clones?\n18.3\nThe Linux kernel does not allow paging out of kernel memory. What\neffect does this restriction have on the kernel’s design? What are two\nadvantages and two disadvantages of this design decision?\n18.4\nDiscuss three advantages of dynamic (shared) linkage of libraries\ncompared with static linkage. Describe two cases in which static linkage\nis preferable.\n18.5\nCompare the use of networking sockets with the use of shared memory\nas a mechanism for communicating data between processes on a single",
  "computer. What are the advantages of each method? When might each\nbe preferred?\n18.6\nAt one time, UNIX systems used disk-layout optimizations based\non the rotation position of disk data, but modern implementations,\nincluding Linux, simply optimize for sequential data access. Why do\nthey do so? Of what hardware characteristics does sequential access\ntake advantage? Why is rotational optimization no longer so useful?\nExercises\n18.7\nWhat are the advantages and disadvantages of writing an operating\nsystem in a high-level language, such as C?\n18.8\nIn what circumstances is the system-call sequence fork() exec() most\nappropriate? When is vfork() preferable?\n18.9\nWhat socket type should be used to implement an intercomputer\nﬁle-transfer program? What type should be used for a program that",
  "periodically tests to see whether another computer is up on the\nnetwork? Explain your answer.\n18.10\nLinux runs on a variety of hardware platforms. What steps must\nLinux developers take to ensure that the system is portable to different\nprocessors and memory-management architectures and to minimize\nthe amount of architecture-speciﬁc kernel code?\n18.11\nWhat are the advantages and disadvantages of making only some of the\nsymbols deﬁned inside a kernel accessible to a loadable kernel module?\n18.12\nWhat are the primary goals of the conﬂict-resolution mechanism used\nby the Linux kernel for loading kernel modules?\n18.13\nDiscuss how the clone() operation supported by Linux is used to\nsupport both processes and threads.\n18.14\nWould youclassifyLinuxthreadsasuser-level threadsor askernel-level",
  "threads? Support your answer with the appropriate arguments.\n18.15\nWhat extra costs are incurred in the creation and scheduling of a\nprocess, compared with the cost of a cloned thread? 826\nChapter 18\nThe Linux System\n18.16\nHow does Linux’s Completely Fair Scheduler (CFS) provide improved\nfairness over a traditional UNIX process scheduler? When is the fairness\nguaranteed?\n18.17\nWhat are the two conﬁgurable variables of the Completely Fair Sched-\nuler (CFS)? What are the pros and cons of setting each of them to very\nsmall and very large values?\n18.18\nThe Linux scheduler implements “soft” real-time scheduling. What\nfeatures necessary for certain real-time programming tasks are missing?\nHow might they be added to the kernel? What are the costs (downsides)\nof such features?\n18.19",
  "of such features?\n18.19\nUnder what circumstances would a user process request an operation\nthat results in the allocation of a demand-zero memory region?\n18.20\nWhat scenarios would cause a page of memory to be mapped into a user\nprogram’s address space with the copy-on-write attribute enabled?\n18.21\nIn Linux, shared libraries perform many operations central to the\noperating system. What is the advantage of keeping this functionality\nout of the kernel? Are there any drawbacks? Explain your answer.\n18.22\nWhat are the beneﬁts of a journaling ﬁle system such as Linux’s ext3?\nWhat are the costs? Why does ext3 provide the option to journal only\nmetadata?\n18.23\nThe directory structure of a Linux operating system could include ﬁles",
  "corresponding to several different ﬁle systems, including the Linux\n/proc ﬁle system. How might the need to support different ﬁle-system\ntypes affect the structure of the Linux kernel?\n18.24\nIn what ways does the Linux setuid feature differ from the setuid\nfeature SVR4?\n18.25\nThe Linux source code is freely and widely available over the Inter-\nnet and from CD-ROM vendors. What are three implications of this\navailability for the security of the Linux system?\nBibliographical Notes\nThe Linux system is a product of the Internet; as a result, much of the\navailable documentation on Linux is available in some form on the Internet.\nThe following key sites reference most of the useful information available:\n• The Linux Cross-Reference Page (LXR) (http://lxr.linux.no) maintains current",
  "listings of the Linux kernel, browsable via the Web and fully cross-\nreferenced.\n• The Kernel Hackers’ Guide provides a helpful overview of the Linux kernel\ncomponents and internals and is located at http://tldp.org/LDP/tlk/tlk.html. Bibliography\n827\n• The Linux Weekly News (LWN) (http://lwn.net) provides weekly Linux-\nrelated news, including a very well researched subsection on Linux kernel\nnews.\nMany mailing lists devoted to Linux are also available. The most important\nare maintained by a mailing-list manager that can be reached at the e-mail\naddress majordomo@vger.rutgers.edu. Send e-mail to this address with the\nsingle line “help” in the mail’s body for information on how to access the list\nserver and to subscribe to any lists.",
  "server and to subscribe to any lists.\nFinally, the Linux system itself can be obtained over the Internet. Complete\nLinux distributions are available from the home sites of the companies\nconcerned, and the Linux community also maintains archives of current\nsystem components at several places on the Internet. The most important is\nftp://ftp.kernel.org/pub/linux.\nIn addition to investigating Internet resources, you can read about the\ninternals of the Linux kernel in [Mauerer (2008)] and [Love (2010)].\nBibliography\n[Love (2010)]\nR. Love, Linux Kernel Development, Third Edition, Developer’s\nLibrary (2010).\n[Mauerer (2008)]\nW. Mauerer, Professional Linux Kernel Architecture, John Wiley\nand Sons (2008).  19\nC H A P T E R\nWindows 7\nUpdated by Dave Probert",
  "C H A P T E R\nWindows 7\nUpdated by Dave Probert\nThe Microsoft Windows 7 operating system is a 32-/64-bit preemptive mul-\ntitasking client operating system for microprocessors implementing the Intel\nIA-32 and AMD64 instruction set architectures (ISAs). Microsoft’s corresponding\nserver operating system, Windows Server 2008 R2, is based on the same code\nas Windows 7 but supports only the 64-bit AMD64 and IA64 (Itanium) ISAs.\nWindows 7 is the latest in a series of Microsoft operating systems based on its\nNT code, which replaced the earlier systems based on Windows 95/98. In this\nchapter, we discuss the key goals of Windows 7, the layered architecture of the\nsystem that has made it so easy to use, the ﬁle system, the networking features,\nand the programming interface.\nCHAPTER OBJECTIVES",
  "and the programming interface.\nCHAPTER OBJECTIVES\n• To explore the principles underlying Windows 7’s design and the speciﬁc\ncomponents of the system.\n• To provide a detailed discussion of the Windows 7 ﬁle system.\n• To illustrate the networking protocols supported in Windows 7.\n• To describe the interface available in Windows 7 to system and application\nprogrammers.\n• To describe the important algorithms implemented with Windows 7.\n19.1\nHistory\nIn the mid-1980s, Microsoft and IBM cooperated to develop the OS/2 operating\nsystem, which was written in assembly language for single-processor Intel\n80286 systems. In 1988, Microsoft decided to end the joint effort with IBM\nand develop its own “new technology” (or NT) portable operating system to",
  "support both the OS/2 and POSIX application-programming interfaces (APIs). In\n829 830\nChapter 19\nWindows 7\nOctober 1988, Dave Cutler, the architect of the DEC VAX/VMS operating system,\nwas hired and given the charter of building Microsoft’s new operating system.\nOriginally, the teamplanned touse the OS/2 API as NT’snative environment,\nbut during development, NT was changed to use a new 32-bit Windows API\n(called Win32), based on the popular 16-bit API used in Windows 3.0. The ﬁrst\nversions of NT were Windows NT 3.1 and Windows NT 3.1 Advanced Server.\n(At that time, 16-bit Windows was at Version 3.1.) Windows NT Version 4.0\nadopted the Windows 95 user interface and incorporated Internet web-server\nand web-browser software. In addition, user-interface routines and all graphics",
  "code were moved into the kernel to improve performance, with the side effect of\ndecreased system reliability. Although previous versions of NT had been ported\nto other microprocessor architectures, the Windows 2000 version, released\nin February 2000, supported only Intel (and compatible) processors due to\nmarketplace factors. Windows 2000 incorporated signiﬁcant changes. It added\nActive Directory (an X.500-based directory service), better networking and\nlaptop support, support for plug-and-play devices, a distributed ﬁle system,\nand support for more processors and more memory.\nIn October 2001, Windows XP was released as both an update to the\nWindows 2000 desktop operating system and a replacement for Windows\n95/98. In 2002, the server edition of Windows XP became available (called",
  "Windows .Net Server). Windows XP updated the graphical user interface\n(GUI) with a visual design that took advantage of more recent hardware\nadvances and many new ease-of-use features. Numerous features were added\nto automatically repair problems in applications and the operating system\nitself. As a result of these changes, Windows XPprovided better networking and\ndevice experience (including zero-conﬁguration wireless, instant messaging,\nstreaming media, and digital photography/video), dramatic performance\nimprovements for both the desktop and large multiprocessors, and better\nreliability and security than earlier Windows operating systems.\nThe long-awaited update to Windows XP, called Windows Vista, was\nreleased in November 2006, but it was not well received. Although Win-",
  "dows Vista included many improvements that later showed up in Windows\n7, these improvements were overshadowed by Windows Vista’s perceived\nsluggishness and compatibility problems. Microsoft responded to criticisms\nof Windows Vista by improving its engineering processes and working more\nclosely with the makers of Windows hardware and applications. The result was\nWindows 7, which was released in October 2009, along with corresponding\nserver editions of Windows. Among the signiﬁcant engineering changes is the\nincreased use of execution tracing rather than counters or proﬁling to analyze\nsystem behavior. Tracing runs constantly in the system, watching hundreds of\nscenarios execute. When one of these scenarios fails, or when it succeeds but",
  "does not perform well, the traces can be analyzed to determine the cause.\nWindows 7 uses a client–server architecture (like Mach) to implement two\noperating-system personalities, Win32 and POSIX, with user-level processes\ncalled subsystems. (At one time, Windows also supported an OS/2 subsystem,\nbut it was removed in Windows XP due to the demise of OS/2.) The subsystem\narchitecture allows enhancements to be made to one operating-system person-\nality without affecting the application compatibility of the other. Although the\nPOSIX subsystem continues to be available for Windows 7, the Win32 API has\nbecome very popular, and the POSIX APIs are used by only a few sites. The\nsubsystem approach continues to be interesting to study from an operating- 19.2\nDesign Principles\n831",
  "Design Principles\n831\nsystem perspective, but machine-virtualization technologies are now becoming\nthe dominant way of running multiple operating systems on a single machine.\nWindows 7 is a multiuser operating system, supporting simultaneous\naccess through distributed services or through multiple instances of the GUI\nvia the Windows terminal services. The server editions of Windows 7 support\nsimultaneous terminal server sessions from Windows desktop systems. The\ndesktop editions of terminal server multiplex the keyboard, mouse, and\nmonitor between virtual terminal sessions for each logged-on user. This feature,\ncalled fast user switching, allows users to preempt each other at the console of\na PC without having to log off and log on.",
  "a PC without having to log off and log on.\nWe noted earlier that some GUI implementation moved into kernel mode\nin Windows NT 4.0. It started to move into user mode again with Windows\nVista, which included the desktop window manager (DWM) as a user-mode\nprocess. DWM implements the desktop compositing of Windows, providing\nthe Windows Aero interface look on top of the Windows DirectX graphic\nsoftware. DirectX continues to run in the kernel, as does the code implementing\nWindows’ previous windowing and graphics models (Win32k and GDI).\nWindows 7 made substantial changes to the DWM, signiﬁcantly reducing its\nmemory footprint and improving its performance.\nWindows XP was the ﬁrst version of Windows to ship a 64-bit version (for",
  "the IA64 in 2001 and the AMD64 in 2005). Internally, the native NT ﬁle system\n(NTFS) and many of the Win32 APIs have always used 64-bit integers where\nappropriate—so the major extension to 64-bit in Windows XP was support\nfor large virtual addresses. However, 64-bit editions of Windows also support\nmuchlargerphysical memories. Bythe time Windows7shipped, the AMD64 ISA\nhad become available on almost all CPUs from both Intel and AMD. In addition,\nby that time, physical memories on client systems frequently exceeded the\n4-GB limit of the IA-32. As a result, the 64-bit version of Windows 7 is now\ncommonly installed on larger client systems. Because the AMD64 architecture\nsupports high-ﬁdelity IA-32 compatibility at the level of individual processes,",
  "32- and 64-bit applications can be freely mixed in a single system.\nIn the rest of our description of Windows 7, we will not distinguish between\nthe client editions of Windows 7 and the corresponding server editions. They\nare based on the same core components and run the same binary ﬁles for\nthe kernel and most drivers. Similarly, although Microsoft ships a variety of\ndifferent editions of each release to address different market price points, few\nof the differences between editions are reﬂected in the core of the system. In\nthis chapter, we focus primarily on the core components of Windows 7.\n19.2 Design Principles\nMicrosoft’s design goals for Windows included security, reliability, Windows\nand POSIX application compatibility, high performance, extensibility, porta-",
  "bility, and international support. Some additional goals, energy efﬁciency and\ndynamic device support, have recently been added to this list. Next, we discuss\neach of these goals and how it is achieved in Windows 7.\n19.2.1\nSecurity\nWindows 7 security goals required more than just adherence to the design\nstandards that had enabled Windows NT 4.0 to receive a C2 security classiﬁca- 832\nChapter 19\nWindows 7\ntion from the U.S. government (A C2 classiﬁcation signiﬁes a moderate level of\nprotection from defective software and malicious attacks. Classiﬁcations were\ndeﬁned by the Department of Defense Trusted Computer System Evaluation\nCriteria, also known as the Orange Book, as described in Section 15.8.) Exten-\nsive code review and testing were combined with sophisticated automatic",
  "analysis tools to identify and investigate potential defects that might represent\nsecurity vulnerabilities.\nWindows bases security on discretionary access controls. System objects,\nincluding ﬁles, registry settings, and kernel objects, are protected by access-\ncontrol lists (ACLs) (see Section 11.6.2). ACLs are vulnerable to user and\nprogrammer errors, however, as well as to the most common attacks on\nconsumer systems, in which the user is tricked into running code, often while\nbrowsing the Web. Windows 7 includes a mechanism called integrity levels\nthat acts as a rudimentary capability system for controlling access. Objects and\nprocesses are marked as having low, medium, or high integrity. Windows does\nnot allow a process to modify an object with a higher integrity level, no matter",
  "what the setting of the ACL.\nOther security measures include address-space layout randomization\n(ASLR), nonexecutable stacks and heaps, and encryption and digital signature\nfacilities. ASLR thwarts many forms of attack by preventing small amounts of\ninjected code from jumping easily to code that is already loaded in a process as\npart of normal operation. This safeguard makes it likely that a system under\nattack will fail or crash rather than let the attacking code take control.\nRecent chips from both Intel and AMD are based on the AMD64 architecture,\nwhich allows memory pages to be marked so that they cannot contain\nexecutable instruction code. Windows tries to mark stacks and memory heaps\nso that they cannot be used to execute code, thus preventing attacks in which",
  "a program bug allows a buffer to overﬂow and then is tricked into executing\nthe contents of the buffer. This technique cannot be applied to all programs,\nbecause some rely on modifying data and executing it. A column labeled “data\nexecution prevention” in the Windows task manager shows which processes\nare marked to prevent these attacks.\nWindows uses encryption as part of common protocols, such as those used\nto communicate securely with websites. Encryption is also used to protect\nuser ﬁles stored on disk from prying eyes. Windows 7 allows users to easily\nencrypt virtually a whole disk, as well as removable storage devices such as USB\nﬂash drives, with a feature called BitLocker. If a computer with an encrypted",
  "disk is stolen, the thieves will need very sophisticated technology (such as an\nelectron microscope) to gain access to any of the computer’s ﬁles. Windows\nuses digital signatures to sign operating system binaries so it can verify that the\nﬁles were produced by Microsoft or another known company. In some editions\nof Windows, a code integrity module is activated at boot to ensure that all the\nloaded modules in the kernel have valid signatures, assuring that they have\nnot been tampered with by an off-line attack.\n19.2.2\nReliability\nWindows matured greatly as an operating system in its ﬁrst ten years, leading\nto Windows 2000. At the same time, its reliability increased due to such factors\nas maturity in the source code, extensive stress testing of the system, improved",
  "CPU architectures, and automatic detection of many serious errors in drivers 19.2\nDesign Principles\n833\nfrom both Microsoft and third parties. Windows has subsequently extended\nthe tools for achieving reliability to include automatic analysis of source code\nfor errors, tests that include providing invalid or unexpected input parameters\n(known as fuzzing to detect validation failures, and an application version\nof the driver veriﬁer that applies dynamic checking for an extensive set of\ncommon user-mode programming errors. Other improvements in reliability\nhave resulted from moving more code out of the kernel and into user-mode\nservices. Windowsprovidesextensive support forwritingdriversinusermode.\nSystem facilities that were once in the kernel and are now in user mode include",
  "the Desktop Window Manager and much of the software stack for audio.\nOne of the most signiﬁcant improvements in the Windows experience\ncame from adding memory diagnostics as an option at boot time. This\naddition is especially valuable because so few consumer PCs have error-\ncorrecting memory. When bad RAM starts to drop bits here and there, the\nresult is frustratingly erratic behavior in the system. The availability of memory\ndiagnostics has greatly reduced the stress levels of users with bad RAM.\nWindows 7 introduced a fault-tolerant memory heap. The heap learns from\napplication crashes and automatically inserts mitigations into future execution\nof an application that has crashed. This makes the application more reliable",
  "even if it contains common bugs such as using memory after freeing it or\naccessing past the end of the allocation.\nAchieving high reliability in Windows is particularly challenging because\nalmost one billion computers run Windows. Even reliability problems that\naffect only a small percentage of users still impact tremendous numbers of\nhuman beings. The complexity of the Windows ecosystem also adds to the\nchallenges. Millions of instances of applications, drivers, and other software are\nbeing constantly downloaded and run on Windows systems. Of course, there\nis also a constant stream of malware attacks. As Windows itself has become\nharder to attack directly, exploits increasingly target popular applications.\nTo cope with these challenges, Microsoft is increasingly relying on com-",
  "munications from customer machines to collect large amounts of data from\nthe ecosystem. Machines can be sampled to see how they are performing,\nwhat software they are running, and what problems they are encountering.\nCustomers can send data to Microsoft when systems or software crashes or\nhangs. This constant stream of data from customer machines is collected very\ncarefully, with the users’ consent and without invading privacy. The result is\nthat Microsoft is building an ever-improving picture of what is happening in the\nWindows ecosystem that allows continuous improvements through software\nupdates, as well as providing data to guide future releases of Windows.\n19.2.3\nWindows and POSIX Application Compatibility\nAs mentioned, Windows XP was both an update of Windows 2000 and",
  "a replacement for Windows 95/98. Windows 2000 focused primarily on\ncompatibility for business applications. The requirements for Windows XP\nincluded a much higher compatibility with the consumer applications that ran\non Windows 95/98. Application compatibility is difﬁcult to achieve because\nmany applications check for a particular version of Windows, may depend\nto some extent on the quirks of the implementation of APIs, may have\nlatent application bugs that were masked in the previous system, and so 834\nChapter 19\nWindows 7\nforth. Applications may also have been compiled for a different instruction\nset. Windows 7 implements several strategies to run applications despite\nincompatibilities.\nLike Windows XP, Windows 7 has a compatibility layer that sits between",
  "applications and the Win32 APIs. This layer makes Windows 7 look (almost)\nbug-for-bug compatible with previous versions of Windows. Windows 7, like\nearlier NT releases, maintains support for running many 16-bit applications\nusing a thunking, or conversion, layer that translates 16-bit API calls into\nequivalent 32-bit calls. Similarly, the 64-bit version of Windows 7 provides\na thunking layer that translates 32-bit API calls into native 64-bit calls.\nThe Windows subsystem model allows multiple operating-system person-\nalities to be supported. As noted earlier, although the API most commonly\nused with Windows is the Win32 API, some editions of Windows 7 support a\nPOSIX subsystem. POSIX is a standard speciﬁcation for UNIX that allows most",
  "available UNIX-compatible software to compile and run without modiﬁcation.\nAs a ﬁnal compatibility measure, several editions of Windows 7 provide\na virtual machine that runs Windows XP inside Windows 7. This allows\napplications to get bug-for-bug compatibility with Windows XP.\n19.2.4\nHigh Performance\nWindows was designed to provide high performance on desktop systems\n(which are largely constrained by I/O performance), server systems (where\nthe CPU is often the bottleneck), and large multithreaded and multiprocessor\nenvironments (where locking performance and cache-line management are\nkeys to scalability). To satisfy performance requirements, NT used a variety\nof techniques, such as asynchronous I/O, optimized protocols for networks,",
  "kernel-based graphics rendering, and sophisticated caching of ﬁle-system data.\nThe memory-management and synchronization algorithms were designed\nwith an awareness of the performance considerations related to cache lines\nand multiprocessors.\nWindows NT was designed for symmetrical multiprocessing (SMP); on\na multiprocessor computer, several threads can run at the same time, even\nin the kernel. On each CPU, Windows NT uses priority-based preemptive\nscheduling of threads. Except while executing in the kernel dispatcher or at\ninterrupt level, threads in any process running in Windows can be preempted\nby higher-priority threads. Thus, the system responds quickly (see Chapter 6).\nThe subsystems that constitute Windows NT communicate with one",
  "another efﬁciently through a local procedure call (LPC) facility that provides\nhigh-performance message passing. When a thread requests a synchronous\nservice from another process through an LPC, the servicing thread is marked\nready, and its priority is temporarily boosted to avoid the scheduling delays\nthat would occur if it had to wait for threads already in the queue.\nWindows XP further improved performance by reducing the code-path\nlength in critical functions, using better algorithms and per-processor data\nstructures, using memory coloring for non-uniform memory access (NUMA)\nmachines, and implementing more scalable locking protocols, such as queued\nspinlocks. The new locking protocols helped reduce system bus cycles and",
  "included lock-free lists and queues, atomic read–modify–write operations\n(like interlocked increment), and other advanced synchronization techniques. 19.2\nDesign Principles\n835\nBy the time Windows 7 was developed, several major changes had come\nto computing. Client/server computing had increased in importance, so an\nadvanced local procedure call (ALPC) facility was introduced to provide\nhigher performance and more reliability than LPC. The number of CPUs\nand the amount of physical memory available in the largest multiprocessors\nhad increased substantially, so quite a lot of effort was put into improving\noperating-system scalability.\nThe implementation of SMP in Windows NT used bitmasks to represent\ncollections of processors and to identify, for example, which set of processors a",
  "particular thread could be scheduled on. These bitmasks were deﬁned as ﬁtting\nwithin a single word of memory, limiting the number of processors supported\nwithin a system to 64. Windows 7 added the concept of processor groups to\nrepresent arbitrary numbers of CPUs, thus accommodating more CPU cores.\nThe number of CPU cores within single systems has continued to increase not\nonly because of more cores but also because of cores that support more than\none logical thread of execution at a time.\nAll these additional CPUs created a great deal of contention for the locks\nused for scheduling CPUs and memory. Windows 7 broke these locks apart. For\nexample, before Windows 7, a single lock was used by the Windows scheduler\nto synchronize access to the queues containing threads waiting for events. In",
  "Windows 7, each object has its own lock, allowing the queues to be accessed\nconcurrently. Also, many execution paths in the scheduler were rewritten to be\nlock-free. This change resulted in good scalability performance for Windows\neven on systems with 256 hardware threads.\nOther changes are due to the increasing importance of support for parallel\ncomputing. For years, the computer industry has been dominated by Moore’s\nLaw, leading to higher densities of transistors that manifest themselves as faster\nclock rates for each CPU. Moore’s Law continues to hold true, but limits have\nbeen reached that prevent CPU clock rates from increasing further. Instead,\ntransistors are being used to build more and more CPUs into each chip. New",
  "programming models for achieving parallel execution, such as Microsoft’s\nConcurrency RunTime (ConcRT) and Intel’s Threading Building Blocks (TBB),\nare being used to express parallelism in C++ programs. Where Moore’s Law\nhas governed computing for forty years, it now seems that Amdahl’s Law,\nwhich governs parallel computing, will rule the future.\nTo support task-based parallelism, Windows 7 provides a new form of\nuser-mode scheduling (UMS). UMS allows programs to be decomposed into\ntasks, and the tasks are then scheduled on the available CPUs by a scheduler\nthat operates in user mode rather than in the kernel.\nThe advent of multiple CPUs on the smallest computers is only part of\nthe shift taking place to parallel computing. Graphics processing units (GPUs)",
  "accelerate the computational algorithms needed for graphics by using SIMD\narchitectures to execute a single instruction for multiple data at the same\ntime. This has given rise to the use of GPUs for general computing, not just\ngraphics. Operating-system support for software like OpenCL and CUDA is\nallowing programs to take advantage of the GPUs. Windows supports use of\nGPUs through software in its DirectX graphics support. This software, called\nDirectCompute, allows programs to specify computational kernels using the\nsame HLSL (high-level shader language) programming model used to program\nthe SIMD hardware for graphics shaders. The computational kernels run very 836\nChapter 19\nWindows 7\nquickly on the GPU and return their results to the main computation running\non the CPU.\n19.2.5",
  "on the CPU.\n19.2.5\nExtensibility\nExtensibility refers to the capacity of an operating system to keep up with\nadvances in computing technology. To facilitate change over time, the devel-\nopers implemented Windows using a layered architecture. The Windows\nexecutive runs in kernel mode and provides the basic system services and\nabstractions that support shared use of the system. On top of the executive,\nseveral server subsystems operate in user mode. Among them are environ-\nmental subsystems that emulate different operating systems. Thus, programs\nwritten for the Win32 APIs and POSIX all run on Windows in the appropriate\nenvironment. Because of the modular structure, additional environmental sub-\nsystems can be added without affecting the executive. In addition, Windows",
  "uses loadable drivers in the I/O system, so new ﬁle systems, new kinds of\nI/O devices, and new kinds of networking can be added while the system\nis running. Windows uses a client–server model like the Mach operating\nsystem and supports distributed processing by remote procedure calls (RPCs)\nas deﬁned by the Open Software Foundation.\n19.2.6\nPortability\nAn operating system is portable if it can be moved from one CPU architecture\nto another with relatively few changes. Windows was designed to be portable.\nLike the UNIX operating system, Windows is written primarily in C and C++.\nThe architecture-speciﬁc source code is relatively small, and there is very\nlittle use of assembly code. Porting Windows to a new architecture mostly",
  "affects the Windows kernel, since the user-mode code in Windows is almost\nexclusively written to be architecture independent. To port Windows, the\nkernel’s architecture-speciﬁc code must be ported, and sometimes conditional\ncompilation is needed in other parts of the kernel because of changes in major\ndata structures, such as the page-table format. The entire Windows system\nmust then be recompiled for the new CPU instruction set.\nOperating systems are sensitive not only to CPU architecture but also to CPU\nsupport chips and hardware boot programs. The CPU and support chips are\ncollectively known as a chipset. These chipsets and the associated boot code\ndetermine how interrupts are delivered, describe the physical characteristics of",
  "each system, and provide interfaces to deeper aspects of the CPU architecture,\nsuch as error recovery and power management. It would be burdensome to\nhave to port Windows to each type of support chip as well as to each CPU\narchitecture. Instead, Windows isolates most of the chipset-dependent code in\na dynamic link library (DLL), called the hardware-abstraction layer (HAL), that\nis loaded with the kernel. The Windows kernel depends on the HAL interfaces\nrather than on the underlying chipset details. This allows the single set of kernel\nand driver binariesfor aparticular CPU tobe used withdifferentchipsetssimply\nby loading a different version of the HAL.\nOver the years, Windows has been ported to a number of different CPU",
  "architectures: Intel IA-32-compatible 32-bit CPUs, AMD64-compatible and IA64\n64-bit CPUs, the DEC Alpha, and the MIPS and PowerPC CPUs. Most of these\nCPU architectures failed in the market. When Windows 7 shipped, only the 19.2\nDesign Principles\n837\nIA-32 and AMD64 architectures were supported on client computers, along\nwith AMD64 and IA64 on servers.\n19.2.7\nInternational Support\nWindows was designed for international and multinational use. It provides\nsupport for different locales via the national-language-support (NLS) API.\nThe NLS API provides specialized routines to format dates, time, and money\nin accordance with national customs. String comparisons are specialized to\naccount for varying character sets. UNICODE is Windows’s native character",
  "code. Windows supports ANSI characters by converting them to UNICODE\ncharacters before manipulating them (8-bit to 16-bit conversion). System text\nstrings are kept in resource ﬁles that can be replaced to localize the system\nfor different languages. Multiple locales can be used concurrently, which is\nimportant to multilingual individuals and businesses.\n19.2.8\nEnergy Efﬁciency\nIncreasing energy efﬁciency for computers causes batteries to last longer for\nlaptops and netbooks, saves signiﬁcant operating costs for power and cooling\nof data centers, and contributes to green initiatives aimed at lowering energy\nconsumption by businesses and consumers. For some time, Windows has\nimplemented several strategies for decreasing energy use. The CPUs are moved",
  "to lower power states—for example, by lowering clock frequency—whenever\npossible. In addition, when a computer is not being actively used, Windows\nmay put the entire computer into a low-power state (sleep) or may even save\nall of memory to disk and shut the computer off (hibernation). When the user\nreturns, the computer powers up and continues from its previous state, so the\nuser does not need to reboot and restart applications.\nWindows 7 added some new strategies for saving energy. The longer a\nCPU can stay unused, the more energy can be saved. Because computers are so\nmuch faster than human beings, a lot of energy can be saved just while humans\nare thinking. The problem is that too many programs are constantly polling to",
  "see what is happening in the system. A swarm of software timers are ﬁring,\nkeeping the CPU from staying idle long enough to save much energy. Windows\n7 extends CPU idle time by skipping clock ticks, coalescing software timers into\nsmaller numbers of events, and “parking” entire CPUs when systems are not\nheavily loaded.\n19.2.9\nDynamic Device Support\nEarly in the history of the PC industry, computer conﬁgurations were fairly\nstatic. Occasionally, new devices might be plugged into the serial, printer, or\ngame ports on the back of a computer, but that was it. The next steps toward\ndynamic conﬁguration of PCs were laptop docks and PCMIA cards. A PC could\nsuddenly be connected to or disconnected from a whole set of peripherals. In",
  "a contemporary PC, the situation has completely changed. PCs are designed\nto enable users to plug and unplug a huge host of peripherals all the time;\nexternal disks, thumb drives, cameras, and the like are constantly coming and\ngoing. 838\nChapter 19\nWindows 7\nOS/2\napplications\nOS/2\nsubsystem\nWin16\napplications\nMS-DOS\napplications\nWin18\nVDM\nwindow\nmanager\nuser mode\nfile system\nI/O manager\nMS-DOS\nVDM\nWin32\nsubsystem\nPOSIX\nsubsystem\nlogon\nprocess\nsecurity\nsubsystem\nauthentication\npackage\nsecurity account\nmanager database\nWin32\napplications\nPOSIX\napplications\ngraphic\ndevice\ndrivers\nkernel\nexecutive\nhardware abstraction layer\nhardware\ncache\nmanager\ndevice\ndrivers\nnetwork\ndrivers\nobject\nmanager\nsecurity\nreference\nmonitor\nprocess\nmanager\nplug and\nplay\nmanager\nvirtual\nmemory\nmanager\nlocal",
  "play\nmanager\nvirtual\nmemory\nmanager\nlocal\nprocedure\ncall\nfacility\nFigure 19.1\nWindows block diagram.\nSupport for dynamic conﬁguration of devices is continually evolving\nin Windows. The system can automatically recognize devices when they\nare plugged in and can ﬁnd, install, and load the appropriate drivers—\noften without user intervention. When devices are unplugged, the drivers\nautomatically unload, and system execution continues without disrupting\nother software.\n19.3 System Components\nThe architecture of Windows is a layered system of modules, as shown in Figure\n19.1. The main layers are the HAL, the kernel, and the executive, all of which\nrun in kernel mode, and a collection of subsystems and services that run in user",
  "mode. The user-mode subsystems fall into two categories: the environmental\nsubsystems, which emulate different operating systems, and the protection\nsubsystems, which provide security functions. One of the chief advantages of\nthis type of architecture is that interactions between modules are kept simple.\nThe remainder of this section describes these layers and subsystems.\n19.3.1\nHardware-Abstraction Layer\nThe HAL is the layer of software that hides hardware chipset differences from\nupper levels of the operating system. The HAL exports a virtual hardware 19.3\nSystem Components\n839\ninterface that is used by the kernel dispatcher, the executive, and the device\ndrivers. Only a single version of each device driver is required for each",
  "CPU architecture, no matter what support chips might be present. Device\ndrivers map devices and access them directly, but the chipset-speciﬁc details\nof mapping memory, conﬁguring I/O buses, setting up DMA, and coping with\nmotherboard-speciﬁc facilities are all provided by the HAL interfaces.\n19.3.2\nKernel\nThe kernel layer of Windows has four main responsibilities: thread scheduling,\nlow-level processor synchronization, interrupt and exception handling, and\nswitching between user mode and kernel mode. The kernel is implemented in\nthe C language, using assembly language only where absolutely necessary to\ninterface with the lowest level of the hardware architecture.\nThe kernel is organized according to object-oriented design principles. An",
  "object type in Windows is a system-deﬁned data type that has a set of attributes\n(data values) and a set of methods (for example, functions or operations). An\nobject is an instance of an object type. The kernel performs its job by using a\nset of kernel objects whose attributes store the kernel data and whose methods\nperform the kernel activities.\n19.3.2.1\nKernel Dispatcher\nThe kernel dispatcher provides the foundation for the executive and the\nsubsystems. Most of the dispatcher is never paged out of memory, and its exe-\ncution is never preempted. Its main responsibilities are thread scheduling and\ncontext switching, implementation of synchronization primitives, timer man-\nagement, software interrupts (asynchronous and deferred procedure calls), and\nexception dispatching.\n19.3.2.2",
  "exception dispatching.\n19.3.2.2\nThreads and Scheduling\nLike many other modern operating systems, Windows uses processes and\nthreads for executable code. Each process has one or more threads, and each\nthread has its own scheduling state, including actual priority, processor afﬁnity,\nand CPU usage information.\nThere are six possible thread states: ready, standby, running, waiting,\ntransition, and terminated. Ready indicates that the thread is waiting to\nrun. The highest-priority ready thread is moved to the standby state, which\nmeans it is the next thread to run. In a multiprocessor system, each processor\nkeeps one thread in a standby state. A thread is running when it is executing\non a processor. It runs until it is preempted by a higher-priority thread, until",
  "it terminates, until its allotted execution time (quantum) ends, or until it waits\non a dispatcher object, such as an event signaling I/O completion. A thread is\nin the waiting state when it is waiting for a dispatcher object to be signaled.\nA thread is in the transition state while it waits for resources necessary for\nexecution; for example, it may be waiting for its kernel stack to be swapped in\nfrom disk. A thread enters the terminated state when it ﬁnishes execution.\nThe dispatcher uses a 32-level priority scheme to determine the order of\nthread execution. Priorities are divided into two classes: variable class and\nreal-time class. The variable class contains threads having priorities from 1 to 840\nChapter 19\nWindows 7",
  "Chapter 19\nWindows 7\n15, and the real-time class contains threads with priorities ranging from 16\nto 31. The dispatcher uses a queue for each scheduling priority and traverses\nthe set of queues from highest to lowest until it ﬁnds a thread that is ready\nto run. If a thread has a particular processor afﬁnity but that processor is not\navailable, the dispatcher skips past it and continues looking for a ready thread\nthat is willing to run on the available processor. If no ready thread is found,\nthe dispatcher executes a special thread called the idle thread. Priority class 0\nis reserved for the idle thread.\nWhen a thread’s time quantum runs out, the clock interrupt queues a\nquantum-end deferred procedure call (DPC) to the processor. Queuing the",
  "DPC results in a software interrupt when the processor returns to normal\ninterrupt priority. The software interrupt causes the dispatcher to reschedule\nthe processor to execute the next available thread at the preempted thread’s\npriority level.\nThe priority of the preempted thread may be modiﬁed before it is placed\nback on the dispatcher queues. If the preempted thread is in the variable-\npriority class, its priority is lowered. The priority is never lowered below the\nbase priority. Lowering the thread’s priority tends to limit the CPUconsumption\nof compute-bound threads versus I/O-bound threads. When a variable-priority\nthread is released from a wait operation, the dispatcher boosts the priority. The\namount of the boost depends on the device for which the thread was waiting.",
  "For example, a thread waiting for keyboard I/O would get a large priority\nincrease, whereas a thread waiting for a disk operation would get a moderate\none. This strategy tends to give good response times to interactive threads\nusing a mouse and windows. It also enables I/O-bound threads to keep the I/O\ndevices busy while permitting compute-bound threads to use spare CPU cycles\nin the background. In addition, the thread associated with the user’s active GUI\nwindow receives a priority boost to enhance its response time.\nScheduling occurs when a thread enters the ready or wait state, when\na thread terminates, or when an application changes a thread’s priority or\nprocessor afﬁnity. If a higher-priority thread becomes ready while a lower-",
  "priority thread is running, the lower-priority thread is preempted. This\npreemption gives the higher-priority thread preferential access to the CPU.\nWindows is not a hard real-time operating system, however, because it does\nnot guarantee that a real-time thread will start to execute within a particular\ntime limit; threads are blocked indeﬁnitely while DPCs and interrupt service\nroutines (ISRs) are running (as further discussed below).\nTraditionally, operating-system schedulers used sampling to measure CPU\nutilization by threads. The system timer would ﬁre periodically, and the timer\ninterrupt handler would take note of what thread was currently scheduled and\nwhether it was executing in user or kernel mode when the interrupt occurred.",
  "This sampling technique was necessary because either the CPU did not have\na high-resolution clock or the clock was too expensive or unreliable to access\nfrequently. Although efﬁcient, sampling was inaccurate and led to anomalies\nsuch as incorporating interrupt servicing time as thread time and dispatching\nthreads that had run for only a fraction of the quantum. Starting with Windows\nVista, CPU time in Windows has been tracked using the hardware timestamp\ncounter (TSC) included in recent processors. Using the TSC results in more\naccurate accounting of CPU usage, and the scheduler will not preempt threads\nbefore they have run for a full quantum. 19.3\nSystem Components\n841\n19.3.2.3\nImplementation of Synchronization Primitives",
  "Implementation of Synchronization Primitives\nKey operating-system data structures are managed as objects using common\nfacilities for allocation, reference counting, and security. Dispatcher objects\ncontrol dispatching and synchronization in the system. Examples of these\nobjects include the following:\n• The event object is used to record an event occurrence and to synchronize\nthis occurrence with some action. Notiﬁcation events signal all waiting\nthreads, and synchronization events signal a single waiting thread.\n• The mutant provides kernel-mode or user-mode mutual exclusion associ-\nated with the notion of ownership.\n• The mutex, available only in kernel mode, provides deadlock-free mutual\nexclusion.\n• The semaphore object acts as a counter or gate to control the number of",
  "threads that access a resource.\n• The thread object is the entity that is scheduled by the kernel dispatcher.\nIt is associated with a process object, which encapsulates a virtual address\nspace. The thread object is signaled when the thread exits, and the process\nobject, when the process exits.\n• The timer object is used to keep track of time and to signal timeouts when\noperations take too long and need to be interrupted or when a periodic\nactivity needs to be scheduled.\nMany of the dispatcher objects are accessed from user mode via an open\noperation that returns a handle. The user-mode code polls or waits on handles\nto synchronize with other threads as well as with the operating system (see\nSection 19.7.1).\n19.3.2.4\nSoftware Interrupts: Asynchronous and Deferred Procedure Calls",
  "The dispatcher implements two types of software interrupts: asynchronous\nprocedure calls (APCs) and deferred procedure calls (DPCs, mentioned earlier).\nAn asynchronous procedure call breaks into an executing thread and calls\na procedure. APCs are used to begin execution of new threads, suspend or\nresume existing threads, terminate threads or processes, deliver notiﬁcation\nthat an asynchronous I/O has completed, and extract the contents of the CPU\nregisters from a running thread. APCs are queued to speciﬁc threads and allow\nthe system to execute both system and user code within a process’s context.\nUser-mode execution of an APC cannot occur at arbitrary times, but only when\nthe thread is waiting in the kernel and marked alertable.",
  "DPCsare used to postpone interrupt processing. After handling all urgent\ndevice-interrupt processing, the ISR schedules the remaining processing by\nqueuing a DPC. The associated software interrupt will not occur until the CPU\nis next at a priority lower than the priority of all I/O device interrupts but higher\nthan the priority at which threads run. Thus, DPCs do not block other device\nISRs. In addition to deferring device-interrupt processing, the dispatcher uses 842\nChapter 19\nWindows 7\nDPCs to process timer expirations and to preempt thread execution at the end\nof the scheduling quantum.\nExecution of DPCs prevents threads from being scheduled on the current\nprocessor and also keeps APCs from signaling the completion of I/O. This is",
  "done so that completion of DPC routines does not take an extended amount\nof time. As an alternative, the dispatcher maintains a pool of worker threads.\nISRs and DPCs may queue work items to the worker threads where they will be\nexecuted using normal thread scheduling. DPC routines are restricted so that\nthey cannot take page faults (be paged out of memory), call system services,\nor take any other action that might result in an attempt to wait for a dispatcher\nobject to be signaled. Unlike APCs, DPC routines make no assumptions about\nwhat process context the processor is executing.\n19.3.2.5\nExceptions and Interrupts\nThe kernel dispatcher also provides trap handling for exceptions and interrupts\ngenerated by hardware or software. Windows deﬁnes several architecture-",
  "independent exceptions, including:\n• Memory-access violation\n• Integer overﬂow\n• Floating-point overﬂow or underﬂow\n• Integer divide by zero\n• Floating-point divide by zero\n• Illegal instruction\n• Data misalignment\n• Privileged instruction\n• Page-read error\n• Access violation\n• Paging ﬁle quota exceeded\n• Debugger breakpoint\n• Debugger single step\nThe trap handlers deal with simple exceptions. Elaborate exception handling\nis performed by the kernel’s exception dispatcher. The exception dispatcher\ncreates an exception record containing the reason for the exception and ﬁnds\nan exception handler to deal with it.\nWhen an exception occurs in kernel mode, the exception dispatcher simply\ncalls a routine to locate the exception handler. If no handler is found, a fatal",
  "system error occurs, and the user is left with the infamous “blue screen of\ndeath” that signiﬁes system failure.\nException handling is more complex for user-mode processes, because\nan environmental subsystem (such as the POSIX system) sets up a debugger\nport and an exception port for every process it creates. (For details on ports, 19.3\nSystem Components\n843\nsee Section 19.3.3.4.) If a debugger port is registered, the exception handler\nsends the exception to the port. If the debugger port is not found or does not\nhandle that exception, the dispatcher attempts to ﬁnd an appropriate exception\nhandler. If no handler is found, the debugger is called again to catch the error\nfor debugging. If no debugger is running, a message is sent to the process’s",
  "exception port to give the environmental subsystem a chance to translate the\nexception. For example, the POSIX environment translates Windows exception\nmessages into POSIX signals before sending them to the thread that caused\nthe exception. Finally, if nothing else works, the kernel simply terminates the\nprocess containing the thread that caused the exception.\nWhen Windows fails to handle an exception, it may construct a description\nof the error that occurred and request permission from the user to send the\ninformation back to Microsoft for further analysis. In some cases, Microsoft’s\nautomated analysis may be able to recognize the error immediately and suggest\na ﬁx or workaround.\nThe interrupt dispatcher in the kernel handles interrupts by calling either",
  "an interrupt service routine (ISR) supplied by a device driver or a kernel\ntrap-handler routine. The interrupt is represented by an interrupt object that\ncontains all the information needed to handle the interrupt. Using an interrupt\nobject makes it easy to associate interrupt-service routines with an interrupt\nwithout having to access the interrupt hardware directly.\nDifferent processor architectures have different types and numbers of inter-\nrupts. For portability, the interrupt dispatcher maps the hardware interrupts\ninto a standard set. The interrupts are prioritized and are serviced in priority\norder. There are 32 interrupt request levels (IRQLs) in Windows. Eight are\nreserved for use by the kernel; the remaining 24 represent hardware interrupts",
  "via the HAL (although most IA-32 systems use only 16). The Windows interrupts\nare deﬁned in Figure 19.2.\nThe kernel uses an interrupt-dispatch table to bind each interrupt level\nto a service routine. In a multiprocessor computer, Windows keeps a separate\ninterrupt-dispatch table (IDT) for each processor, and each processor’s IRQL can\nbe set independently to mask out interrupts. All interrupts that occur at a level\nequal to or less than the IRQL of a processor are blocked until the IRQL is lowered\ninterrupt levels\ntypes of interrupts\n31 \n30 \n29\nmachine check or bus error\npower fail\nclock (used to keep track of time)\nprofile\ntraditional PC IRQ hardware interrupts\ndispatch and deferred procedure call (DPC) (kernel)\nasynchronous procedure call (APC)\npassive\n28 \n27 \n3–26 \n2 \n1 \n0",
  "passive\n28 \n27 \n3–26 \n2 \n1 \n0\ninterprocessor notification (request another processor\nto act; e.g., dispatch a process or update the TLB)\nFigure 19.2\nWindows interrupt-request levels. 844\nChapter 19\nWindows 7\nby a kernel-level thread or by an ISR returning from interrupt processing.\nWindows takes advantage of this property and uses software interrupts to\ndeliver APCs and DPCs, to perform system functions such as synchronizing\nthreads with I/O completion, to start thread execution, and to handle timers.\n19.3.2.6\nSwitching between User-Mode and Kernel-Mode Threads\nWhat the programmer thinks of as a thread in traditional Windows is actually\ntwo threads: a user-mode thread (UT) and a kernel-mode thread (KT). Each has\nits own stack, register values, and execution context. A UT requests a system",
  "service by executing an instruction that causes a trap to kernel mode. The kernel\nlayer runs a trap handler that switches between the UT and the corresponding\nKT. When a KT has completed its kernel execution and is ready to switch back\nto the corresponding UT, the kernel layer is called to make the switch to the UT,\nwhich continues its execution in user mode.\nWindows 7 modiﬁes the behavior of the kernel layer to support user-\nmode scheduling of the UTs. User-mode schedulers in Windows 7 support\ncooperative scheduling. A UT can explicitly yield to another UT by calling\nthe user-mode scheduler; it is not necessary to enter the kernel. User-mode\nscheduling is explained in more detail in Section 19.7.3.7.\n19.3.3\nExecutive\nThe Windows executive provides a set of services that all environmental",
  "subsystems use. The services are grouped as follows: object manager, virtual\nmemory manager, process manager, advanced local procedure call facility, I/O\nmanager, cache manager, security reference monitor, plug-and-play and power\nmanagers, registry, and booting.\n19.3.3.1\nObject Manager\nFor managing kernel-mode entities, Windows uses a generic set of interfaces\nthat are manipulated by user-mode programs. Windows calls these entities\nobjects, and the executive component that manipulates them is the object\nmanager. Examples of objects are semaphores, mutexes, events, processes,\nand threads; all these are dispatcher objects. Threads can block in the kernel\ndispatcher waiting for any of these objects to be signaled. The process, thread,",
  "and virtual memory APIs use process and thread handles to identify the process\nor thread to be operated on. Other examples of objects include ﬁles, sections,\nports, and various internal I/O objects. File objects are used to maintain the open\nstate of ﬁles and devices. Sections are used to map ﬁles. Local-communication\nendpoints are implemented as port objects.\nUser-mode code accesses these objects using an opaque value called a\nhandle, which is returned by many APIs. Each process has a handle table\ncontaining entries that track the objects used by the process. The system\nprocess, which contains the kernel, has its own handle table, which is protected\nfrom user code. The handle tables in Windows are represented by a tree",
  "structure, which can expand from holding 1,024 handles to holding over 16\nmillion. Kernel-mode code can access an object by using either a handle or a\nreferenced pointer. 19.3\nSystem Components\n845\nA process gets a handle by creating an object, by opening an existing\nobject, by receiving a duplicated handle from another process, or by inheriting\na handle from the parent process. When a process exits, all its open handles\nare implicitly closed. Since the object manager is the only entity that generates\nobject handles, it is the natural place to check security. The object manager\nchecks whether a process has the right to access an object when the process\ntries to open the object. The object manager also enforces quotas, such as the",
  "maximum amount of memory a process may use, by charging a process for the\nmemory occupied by all its referenced objects and refusing to allocate more\nmemory when the accumulated charges exceed the process’s quota.\nThe object manager keeps track of two counts for each object: the number\nof handles for the object and the number of referenced pointers. The handle\ncount is the number of handles that refer to the object in the handle tables\nof all processes, including the system process that contains the kernel. The\nreferenced pointer count is incremented whenever a new pointer is needed\nby the kernel and decremented when the kernel is done with the pointer. The\npurpose of these reference counts is to ensure that an object is not freed while",
  "it is still referenced by either a handle or an internal kernel pointer.\nThe object manager maintains the Windows internal name space. In\ncontrast to UNIX, which roots the system name space in the ﬁle system,\nWindows uses an abstract name space and connects the ﬁle systems as devices.\nWhether a Windows object has a name is up to its creator. Processes and\nthreads are created without names and referenced either by handle or through\na separate numerical identiﬁer. Synchronization events usually have names,\nso that they can be opened by unrelated processes. A name can be either\npermanent or temporary. A permanent name represents an entity, such as a\ndisk drive, that remains even if no process is accessing it. A temporary name",
  "exists only while a process holds a handle to the object. The object manager\nsupports directories and symbolic links in the name space. As an example,\nMS-DOS drive letters are implemented using symbolic links; \\Global??\\C: is\na symbolic link to the device object \\Device\\HarddiskVolume2, representing a\nmounted ﬁle-system volume in the \\Device directory.\nEach object, as mentioned earlier, is an instance of an object type. The\nobject type speciﬁes how instances are to be allocated, how the data ﬁelds are\nto be deﬁned, and how the standard set of virtual functions used for all objects\nare to be implemented. The standard functions implement operations such as\nmapping names to objects, closing and deleting, and applying security checks.",
  "Functions that are speciﬁc to a particular type of object are implemented by\nsystem services designed to operate on that particular object type, not by the\nmethods speciﬁed in the object type.\nThe parse() function is the most interesting of the standard object\nfunctions. It allows the implementation of an object. The ﬁle systems, the\nregistry conﬁguration store, and GUI objects are the most notable users of\nparse functions to extend the Windows name space.\nReturning to our Windows naming example, device objects used to\nrepresent ﬁle-system volumes provide a parse function. This allows a name like\n\\Global??\\C:\\foo\\bar.doc to be interpreted as the ﬁle \\foo\\bar.doc on the\nvolume represented by the device object HarddiskVolume2. We can illustrate",
  "how naming, parse functions, objects, and handles work together by looking\nat the steps to open the ﬁle in Windows: 846\nChapter 19\nWindows 7\n1. An application requests that a ﬁle named C:\\foo\\bar.doc be opened.\n2. The object manager ﬁnds the device object HarddiskVolume2, looks up\nthe parse procedure IopParseDevice from the object’s type, and invokes\nit with the ﬁle’s name relative to the root of the ﬁle system.\n3. IopParseDevice() allocates a ﬁle object and passes it to the ﬁle system,\nwhich ﬁlls in the details of how to access C:\\foo\\bar.doc on the volume.\n4. When the ﬁle system returns, IopParseDevice() allocates an entry for\nthe ﬁle object in the handle table for the current process and returns the\nhandle to the application.",
  "handle to the application.\nIf the ﬁle cannot successfully be opened, IopParseDevice() deletes the\nﬁle object it allocated and returns an error indication to the application.\n19.3.3.2\nVirtual Memory Manager\nThe executive component that manages the virtual address space, physical\nmemory allocation, and paging is the virtual memory (VM) manager. The\ndesign of the VM manager assumes that the underlying hardware supports\nvirtual-to-physical mapping, a paging mechanism, and transparent cache\ncoherence on multiprocessor systems, as well as allowing multiple page-table\nentries to map to the same physical page frame. The VM manager in Windows\nuses a page-based management scheme with page sizes of 4 KB and 2 MB on\nAMD64 and IA-32-compatible processors and 8 KB on the IA64. Pages of data",
  "allocated to a process that are not in physical memory are either stored in the\npaging ﬁles on disk or mapped directly to a regular ﬁle on a local or remote\nﬁle system. A page can also be marked zero-ﬁll-on-demand, which initializes\nthe page with zeros before it is allocated, thus erasing the previous contents.\nOn IA-32 processors, each process has a 4-GB virtual address space. The\nupper 2 GB are mostly identical for all processes and are used by Windows in\nkernel mode to access the operating-system code and data structures. For the\nAMD64 architecture, Windows provides a 8-TB virtual address space for user\nmode out of the 16 EB supported by existing hardware for each process.\nKey areas of the kernel-mode region that are not identical for all processes",
  "are the self-map, hyperspace, and session space. The hardware references a\nprocess’s page table using physical page-frame numbers, and the page table\nself-map makes the contents of the process’s page table accessible using virtual\naddresses. Hyperspace maps the current process’s working-set information\ninto the kernel-mode address space. Session space is used to share an instance\nof the Win32 and other session-speciﬁc drivers among all the processes in\nthe same terminal-server (TS) session. Different TS sessions share different\ninstances of these drivers, yet they are mapped at the same virtual addresses.\nThe lower, user-mode region of virtual address space is speciﬁc to each process\nand accessible by both user- and kernel-mode threads.",
  "The Windows VM manager uses a two-step process to allocate virtual\nmemory. The ﬁrst step reserves one or more pages of virtual addresses in\nthe process’s virtual address space. The second step commits the allocation by\nassigning virtual memory space (physical memory or space in the paging ﬁles).\nWindows limits the amount of virtual memory space a process consumes by\nenforcing a quota on committed memory. A process decommits memory that it 19.3\nSystem Components\n847\nis no longer using to free up virtual memory space for use by other processes.\nThe APIs used to reserve virtual addresses and commit virtual memory take a\nhandle on a process object as a parameter. This allows one process to control the\nvirtual memory of another. Environmental subsystems manage the memory of",
  "their client processes in this way.\nWindows implements shared memory by deﬁning a section object. After\ngetting a handle to a section object, a process maps the memory of the section to\na range of addresses, called a view. A process can establish a view of the entire\nsection or only the portion it needs. Windows allows sections to be mapped\nnot just into the current process but into any process for which the caller has a\nhandle.\nSections can be used in many ways. A section can be backed by disk space\neither in the system-paging ﬁle or in a regular ﬁle (a memory-mapped ﬁle). A\nsection can be based, meaning that it appears at the same virtual address for all\nprocesses attempting to access it. Sections can also represent physical memory,",
  "allowing a 32-bit process to access more physical memory than can ﬁt in its\nvirtual address space. Finally, the memory protection of pages in the section\ncan be set to read-only, read-write, read-write-execute, execute-only, no access,\nor copy-on-write.\nLet’s look more closely at the last two of these protection settings:\n• A no-access page raises an exception if accessed. The exception can be\nused, for example, to check whether a faulty program iterates beyond\nthe end of an array or simply to detect that the program attempted to\naccess virtual addresses that are not committed to memory. User- and\nkernel-mode stacks use no-access pages as guard pages to detect stack\noverﬂows. Another use is to look for heap buffer overruns. Both the user-",
  "mode memory allocator and the special kernel allocator used by the device\nveriﬁer can be conﬁgured to map each allocation onto the end of a page,\nfollowed by a no-access page to detect programming errors that access\nbeyond the end of an allocation.\n• The copy-on-write mechanism enables the VM manager to use physical\nmemory more efﬁciently. When two processes want independent copies of\ndata from the same section object, the VM manager places a single shared\ncopy into virtual memory and activates the copy-on-write property for\nthat region of memory. If one of the processes tries to modify data in a\ncopy-on-write page, the VM manager makes a private copy of the page for\nthe process.\nThe virtual addresstranslationinWindowsusesamultilevel page table. For",
  "IA-32 and AMD64 processors, each process has a page directory that contains\n512 page-directory entries (PDEs) 8 bytes in size. Each PDE points to a PTE table\nthat contains 512 page-table entries (PTEs) 8 bytes in size. Each PTE points to\na 4-KB page frame in physical memory. For a variety of reasons, the hardware\nrequires that the page directories or PTE tables at each level of a multilevel page\ntable occupy a single page. Thus, the number of PDEs or PTEs that ﬁt in a page\ndetermine how many virtual addresses are translated by that page. See Figure\n19.3 for a diagram of this structure.\nThe structure described so far can be used to represent only 1 GB of\nvirtual address translation. For IA-32, a second page-directory level is needed, 848\nChapter 19\nWindows 7\nPage table\nentry 0\nPage\ntable 0",
  "Windows 7\nPage table\nentry 0\nPage\ntable 0\nPage table\nentry 0\n4 KB\npage\n4 KB\npage\n4 KB\npage\n4 KB\npage\nPage\ntable 511\nPage table\nentry 511\nPage table\nentry 511\nPage\ndirectory\nentry 0\nPage\ndirectory\n0\nPage\ndirectory\nentry 0\nPage\ndirectory\n3\nPage\ndirectory\nentry 511\nPage\ndirectory\nentry 511\nPointer 0\nPointer 1\nPointer 2\nPointer 3\nPage directory pointer table \nFigure 19.3\nPage-table layout.\ncontaining only four entries, as shown in the diagram. On 64-bit processors,\nmore levels are needed. For AMD64, Windows uses a total of four full levels.\nThe total size of all page-table pages needed to fully represent even a 32-bit\nvirtual address space for a process is 8 MB. The VM manager allocates pages of\nPDEs and PTEs as needed and moves page-table pages to disk when not in use.",
  "The page-table pages are faulted back into memory when referenced.\nWe next consider how virtual addresses are translated into physical\naddresses on IA-32-compatible processors. A 2-bit value can represent the\nvalues 0, 1, 2, 3. A 9-bit value can represent values from 0 to 511; a 12-bit\nvalue, values from 0 to 4,095. Thus, a 12-bit value can select any byte within a\n4-KB page of memory. A 9-bit value can represent any of the 512 PDEs or PTEs\nin a page directory or PTE-table page. As shown in Figure 19.4, translating a\nvirtual address pointer to a byte address in physical memory involves breaking\nthe 32-bit pointer into four values, starting from the most signiﬁcant bits:\n• Two bits are used to index into the four PDEs at the top level of the page",
  "table. The selected PDE will contain the physical page number for each of\nthe four page-directory pages that map 1 GB of the address space.\nPTR\nPTE index\nPDE index\npage offset\n31\n0\nFigure 19.4\nVirtual-to-physical address translation on IA-32. 19.3\nSystem Components\n849\n• Nine bits are used to select another PDE, this time from a second-level page\ndirectory. This PDE will contain the physical page numbers of up to 512\nPTE-table pages.\n• Nine bits are used to select one of 512 PTEs from the selected PTE-table\npage. The selected PTE will contain the physical page number for the byte\nwe are accessing.\n• Twelve bits are used as the byte offset into the page. The physical address\nof the byte we are accessing is constructed by appending the lowest 12 bits",
  "of the virtual address to the end of the physical page number we found in\nthe selected PTE.\nThe number of bits in a physical address may be different from the number\nof bits in a virtual address. In the original IA-32 architecture, the PTE and PDE\nwere 32-bit structures that had room for only 20 bits of physical page number,\nso the physical address size and the virtual address size were the same. Such\nsystems could address only 4 GB of physical memory. Later, the IA-32 was\nextended to the larger 64-bit PTE size used today, and the hardware supported\n24-bit physical addresses. These systems could support 64 GB and were used\non server systems. Today, all Windows servers are based on either the AMD64\nor the IA64 and support very, very large physical addresses—more than we",
  "can possibly use. (Of course, once upon a time 4 GB seemed optimistically large\nfor physical memory.)\nTo improve performance, the VM manager maps the page-directory and\nPTE-table pages into the same contiguous region of virtual addresses in every\nprocess. This self-map allows the VM manager to use the same pointer to access\nthe current PDE or PTE corresponding to a particular virtual address no matter\nwhat process is running. The self-map for the IA-32 takes a contiguous 8-MB\nregion of kernel virtual address space; the AMD64 self-map occupies 512 GB.\nAlthough the self-map occupies signiﬁcant address space, it does not require\nany additional virtual memory pages. It also allows the page table’s pages to\nbe automatically paged in and out of physical memory.",
  "In the creation of a self-map, one of the PDEs in the top-level page directory\nrefers to the page-directory page itself, forming a “loop” in the page-table\ntranslations. The virtual pages are accessed if the loop is not taken, the PTE-table\npages are accessed if the loop is taken once, the lowest-level page-directory\npages are accessed if the loop is taken twice, and so forth.\nThe additional levels of page directories used for 64-bit virtual memory are\ntranslated in the same way except that the virtual address pointer is broken up\ninto even more values. For the AMD64, Windows uses four full levels, each of\nwhich maps 512 pages, or 9+9+9+9+12 = 48 bits of virtual address.\nTo avoid the overhead of translating every virtual address by looking up",
  "the PDE and PTE, processors use translation look-aside buffer (TLB) hardware,\nwhich contains an associative memory cache for mapping virtual pages to\nPTEs. The TLB is part of the memory-management unit (MMU) within each\nprocessor. The MMU needs to “walk” (navigate the data structures of) the page\ntable in memory only when a needed translation is missing from the TLB.\nThe PDEs and PTEs contain more than just physical page numbers. They\nalso have bits reserved for operating-system use and bits that control how the\nhardware uses memory, such as whether hardware caching should be used for 850\nChapter 19\nWindows 7\neach page. In addition, the entries specify what kinds of access are allowed for\nboth user and kernel modes.\nA PDE can also be marked to say that it should function as a PTE rather",
  "than a PDE. On a IA-32, the ﬁrst 11 bits of the virtual address pointer select a\nPDE in the ﬁrst two levels of translation. If the selected PDE is marked to act\nas a PTE, then the remaining 21 bits of the pointer are used as the offset of\nthe byte. This results in a 2-MB size for the page. Mixing and matching 4-KB\nand 2-MB page sizes within the page table is easy for the operating system and\ncan signiﬁcantly improve the performance of some programs by reducing how\noften the MMU needs to reload entries in the TLB, since one PDE mapping 2 MB\nreplaces 512 PTEs each mapping 4 KB.\nManaging physical memory so that 2-MB pages are available when needed\nis difﬁcult, however, as they may continually be broken up into 4 KB pages,",
  "causing external fragmentation of memory. Also, the large pages can result\nin very signiﬁcant internal fragmentation. Because of these problems, it is\ntypically only Windows itself, along with large server applications, that use\nlarge pages to improve the performance of the TLB. They are better suited to do\nso because operating-system and server applications start running when the\nsystem boots, before memory has become fragmented.\nWindows manages physical memory by associating each physical page\nwith one of seven states: free, zeroed, modiﬁed, standby, bad, transition, or\nvalid.\n• A free page is a page that has no particular content.\n• A zeroed page is a free page that has been zeroed out and is ready for\nimmediate use to satisfy zero-on-demand faults.",
  "immediate use to satisfy zero-on-demand faults.\n• A modiﬁed page has been written by a process and must be sent to the\ndisk before it is allocated for another process.\n• A standby page is a copy of information already stored on disk. Standby\npages may be pages that were not modiﬁed, modiﬁed pages that have\nalready been written to the disk, or pages that were prefetched because\nthey are expected to be used soon.\n• A bad page is unusable because a hardware error has been detected.\n• A transition page is on its way in from disk to a page frame allocated in\nphysical memory.\n• A valid page is part of the working set of one or more processes and is\ncontained within these processes’ page tables.\nWhile valid pages are contained in processes’ page tables, pages in other",
  "states are kept in separate lists according to state type. The lists are constructed\nby linking the corresponding entries in the page frame number (PFN) database,\nwhich includes an entry for each physical memory page. The PFN entries also\ninclude information such as reference counts, locks, and NUMA information.\nNote that the PFN database represents pages of physical memory, whereas the\nPTEs represent pages of virtual memory.\nWhen the valid bit in a PTE is zero, hardware ignores all the other bits, and\nthe VM manager can deﬁne them for its own use. Invalid pages can have a\nnumber of states represented by bits in the PTE. Page-ﬁle pages that have never 19.3\nSystem Components\n851\n63\nV\n32\nprot\nT  P\nPage\nfile\n31\n0\nPage file offset\nFigure 19.5\nPage-ﬁle page-table entry. The valid bit is zero.",
  "Page-ﬁle page-table entry. The valid bit is zero.\nbeen faulted in are marked zero-on-demand. Pages mapped through section\nobjects encode a pointer to the appropriate section object. PTEs for pages that\nhave been written to the page ﬁle contain enough information to locate the\npage on disk, and so forth. The structure of the page-ﬁle PTE is shown in Figure\n19.5. The T, P, and V bits are all zero for this type of PTE. The PTE includes 5 bits\nfor page protection, 32 bits for page-ﬁle offset, and 4 bits to select the paging\nﬁle. There are also 20 bits reserved for additional bookkeeping.\nWindows uses a per-working-set, least-recently-used (LRU) replacement\npolicy to take pages from processes as appropriate. When a process is started,",
  "it is assigned a default minimum working-set size. The working set of each\nprocess is allowed to grow until the amount of remaining physical memory\nstarts to run low, at which point the VM manager starts to track the age of\nthe pages in each working set. Eventually, when the available memory runs\ncritically low, the VM manager trims the working set to remove older pages.\nHow old a page is depends not on how long it has been in memory but on\nwhen it was last referenced. This is determined by periodically making a pass\nthrough the working set of each process and incrementing the age for pages\nthat have not been marked in the PTE as referenced since the last pass. When\nit becomes necessary to trim the working sets, the VM manager uses heuristics",
  "to decide how much to trim from each process and then removes the oldest\npages ﬁrst.\nA process can have its working set trimmed even when plenty of memory\nis available, if it was given a hard limit on how much physical memory it could\nuse. In Windows 7, the VM manager will also trim processes that are growing\nrapidly, even if memory is plentiful. This policy change signiﬁcantly improves\nthe responsiveness of the system for other processes.\nWindows tracks working sets not only for user-mode processes but also\nfor the system process, which includes all the pageable data structures and\ncode that run in kernel mode. Windows 7 created additional working sets for\nthe system process and associated them with particular categories of kernel",
  "memory; the ﬁle cache, kernel heap, and kernel code now have their own\nworking sets. The distinct working sets allow the VM manager to use different\npolicies to trim the different categories of kernel memory. 852\nChapter 19\nWindows 7\nThe VM manager does not fault in only the page immediately needed.\nResearch shows that the memory referencing of a thread tends to have a locality\nproperty. That is, when a page is used, it is likely that adjacent pages will be\nreferenced in the near future. (Think of iterating over an array or fetching\nsequential instructions that form the executable code for a thread.) Because of\nlocality, when the VM manager faults in a page, it also faults in a few adjacent\npages. This prefetching tends to reduce the total number of page faults and",
  "allows reads to be clustered to improve I/O performance.\nIn addition to managing committed memory, the VM manager manages\neach process’s reserved memory, or virtual address space. Each process has an\nassociated tree that describes the ranges of virtual addresses in use and what\nthe uses are. This allows the VM manager to fault in page-table pages as needed.\nIf the PTE for a faulting address is uninitialized, the VM manager searches for\nthe address in the process’s tree of virtual address descriptors (VADs) and\nuses this information to ﬁll in the PTE and retrieve the page. In some cases, a\nPTE-table page itself may not exist; such a page must be transparently allocated\nand initialized by the VM manager. In other cases, the page may be shared as",
  "part of a section object, and the VAD will contain a pointer to that section object.\nThe section object contains information on how to ﬁnd the shared virtual page\nso that the PTE can be initialized to point at it directly.\n19.3.3.3\nProcess Manager\nThe Windows process manager provides services for creating, deleting, and\nusing processes, threads, and jobs. It has no knowledge about parent–child\nrelationships or process hierarchies; those reﬁnements are left to the particular\nenvironmental subsystem that owns the process. The process manager is also\nnot involved in the scheduling of processes, other than setting the priorities and\nafﬁnities in processes and threads when they are created. Thread scheduling\ntakes place in the kernel dispatcher.",
  "takes place in the kernel dispatcher.\nEach process contains one or more threads. Processes themselves can be\ncollected into larger units called job objects. The use of job objects allows\nlimits to be placed on CPU usage, working-set size, and processor afﬁnities\nthat control multiple processes at once. Job objects are used to manage large\ndata-center machines.\nAn example of process creation in the Win32 environment is as follows:\n1. A Win32 application calls CreateProcess().\n2. A message is sent to the Win32 subsystem to notify it that the process is\nbeing created.\n3. CreateProcess() in the original process then calls an API in the process\nmanager of the NT executive to actually create the process.\n4. The process manager calls the object manager to create a process object",
  "and returns the object handle to Win32.\n5. Win32 calls the process manager again to create a thread for the process\nand returns handles to the new process and thread.\nThe Windows APIs for manipulating virtual memory and threads and\nfor duplicating handles take a process handle, so subsystems can perform 19.3\nSystem Components\n853\noperations on behalf of a new process without having to execute directly in\nthe new process’s context. Once a new process is created, the initial thread\nis created, and an asynchronous procedure call is delivered to the thread to\nprompt the start of execution at the user-mode image loader. The loader is\nin ntdll.dll, which is a link library automatically mapped into every newly\ncreated process. Windows also supports a UNIX fork() style of process creation",
  "in order to support the POSIX environmental subsystem. Although the Win32\nenvironment calls the process manager directly from the client process, POSIX\nuses the cross-process nature of the Windows APIs to create the new process\nfrom within the subsystem process.\nThe process manager relies on the asynchronous procedure calls (APCs)\nimplemented by the kernel layer. APCs are used to initiate thread execution,\nsuspend and resume threads, access thread registers, terminate threads and\nprocesses, and support debuggers.\nThe debugger support in the process manager includes the APIs to suspend\nand resume threads and to create threads that begin in suspended mode. There\nare also process-manager APIs that get and set a thread’s register context and",
  "access another process’s virtual memory. Threads can be created in the current\nprocess; they can also be injected into another process. The debugger makes\nuse of thread injection to execute code within a process being debugged.\nWhile running in the executive, a thread can temporarily attach to a\ndifferent process. Thread attach is used by kernel worker threads that need to\nexecute in the context of the process originating a work request. For example,\nthe VM manager might use thread attach when it needs access to a process’s\nworking set or page tables, and the I/O manager might use it in updating the\nstatus variable in a process for asynchronous I/O operations.\nThe process manager also supports impersonation. Each thread has an",
  "associated security token. When the login process authenticates a user, the\nsecurity token is attached to the user’s process and inherited by its child\nprocesses. The token contains the security identity (SID) of the user, the SIDs of\nthe groups the user belongs to, the privileges the user has, and the integrity level\nof the process. By default, all threads within a process share a common token,\nrepresenting the user and the application that started the process. However, a\nthread running in a process with a security token belonging to one user can set\na thread-speciﬁc token belonging to another user to impersonate that user.\nThe impersonation facility is fundamental to the client–server RPC model,\nwhere services must act on behalf of a variety of clients with different security",
  "IDs. The right to impersonate a user is most often delivered as part of an RPC\nconnection from a client process to a server process. Impersonation allows the\nserver to access system services as if it were the client in order to access or create\nobjects and ﬁles on behalf of the client. The server process must be trustworthy\nand must be carefully written to be robust against attacks. Otherwise, one client\ncould take over a server process and then impersonate any user who made a\nsubsequent client request.\n19.3.3.4\nFacilities for Client–Server Computing\nThe implementation of Windows uses a client–server model throughout. The\nenvironmental subsystems are servers that implement particular operating-\nsystem personalities. Many other services, such as user authentication, network 854\nChapter 19",
  "Chapter 19\nWindows 7\nfacilities, printer spooling, web services, network ﬁle systems, and plug-\nand-play, are also implemented using this model. To reduce the memory\nfootprint, multiple services are often collected into a few processes running\nthe svchost.exe program. Each service is loaded as a dynamic-link library\n(DLL), which implements the service by relying on the user-mode thread-pool\nfacilities to share threads and wait for messages (see Section 19.3.3.3).\nThe normal implementation paradigm for client–server computing is to\nuse RPCs to communicate requests. The Win32 API supports a standard RPC\nprotocol, as described in Section 19.6.2.7. RPC uses multiple transports (for\nexample, named pipes and TCP/IP) and can be used to implement RPCs between",
  "systems. When an RPC always occurs between a client and server on the local\nsystem, the advanced local procedure call facility (ALPC) can be used as the\ntransport. At the lowest level of the system, in the implementation of the\nenvironmental systems, and for services that must be available in the early\nstages of booting, RPC is not available. Instead, native Windows services use\nALPC directly.\nALPC is a message-passing mechanism. The server process publishes a\nglobally visible connection-port object. When a client wants services from\na subsystem or service, it opens a handle to the server’s connection-port\nobject and sends a connection request to the port. The server creates a\nchannel and returns a handle to the client. The channel consists of a pair of",
  "private communication ports: one for client-to-server messages and the other\nfor server-to-client messages. Communication channels support a callback\nmechanism, so the client and server can accept requests when they would\nnormally be expecting a reply.\nWhen an ALPC channel is created, one of three message-passing techniques\nis chosen.\n1. The ﬁrst technique is suitable for small to medium messages (up to 63\nKB). In this case, the port’s message queue is used as intermediate storage,\nand the messages are copied from one process to the other.\n2. The second technique is for larger messages. In this case, a shared-\nmemory section object is created for the channel. Messages sent through\nthe port’s message queue contain a pointer and size information referring",
  "to the section object. This avoids the need to copy large messages. The\nsender places data into the shared section, and the receiver views them\ndirectly.\n3. The third technique uses APIs that read and write directly into a process’s\naddress space. ALPC provides functions and synchronization so that a\nserver can access the data in a client. This technique is normally used by\nRPC to achieve higher performance for speciﬁc scenarios.\nThe Win32 window manager uses its own form of message passing, which is\nindependent of the executive ALPCfacilities. When a client asks for a connection\nthat uses window-manager messaging, the server sets up three objects: (1) a\ndedicated server thread to handle requests, (2) a 64-KB shared section object,",
  "and (3) an event-pair object. An event-pair object is a synchronization object\nused by the Win32 subsystem to provide notiﬁcation when the client thread 19.3\nSystem Components\n855\nhas copied a message to the Win32 server, or vice versa. The section object is\nused to pass the messages, and the event-pair object provides synchronization.\nWindow-manager messaging has several advantages:\n• The section object eliminates message copying, since it represents a region\nof shared memory.\n• The event-pair object eliminates the overhead of using the port object to\npass messages containing pointers and lengths.\n• The dedicated server thread eliminates the overhead of determining which\nclient thread is calling the server, since there is one server thread per client\nthread.",
  "thread.\n• The kernel gives scheduling preference to these dedicated server threads\nto improve performance.\n19.3.3.5\nI/O Manager\nThe I/O manager is responsible for managing ﬁle systems, device drivers, and\nnetwork drivers. It keeps track of which device drivers, ﬁlter drivers, and ﬁle\nsystems are loaded, and it also manages buffers for I/O requests. It works\nwith the VM manager to provide memory-mapped ﬁle I/O and controls the\nWindows cache manager, which handles caching for the entire I/O system. The\nI/O manager is fundamentally asynchronous, providing synchronous I/O by\nexplicitly waiting for an I/O operation to complete. The I/O manager provides\nseveral models of asynchronous I/O completion, including setting of events,",
  "updating of a status variable in the calling process, delivery of APCs to initiating\nthreads, and use of I/O completion ports, which allow a single thread to process\nI/O completions from many other threads.\nDevice drivers are arranged in a list for each device (called a driver or\nI/O stack). A driver is represented in the system as a driver object. Because a\nsingle driver can operate on multiple devices, the drivers are represented in\nthe I/O stack by a device object, which contains a link to the driver object.\nThe I/O manager converts the requests it receives into a standard form called\nan I/O request packet (IRP). It then forwards the IRP to the ﬁrst driver in the\ntargeted I/O stack for processing. After a driver processes the IRP, it calls the",
  "I/O manager either to forward the IRP to the next driver in the stack or, if all\nprocessing is ﬁnished, to complete the operation represented by the IRP.\nThe I/O request may be completed in a context different from the one in\nwhich it was made. For example, if a driver is performing its part of an I/O\noperation and is forced to block for an extended time, it may queue the IRP to\na worker thread to continue processing in the system context. In the original\nthread, the driver returns a status indicating that the I/O request is pending so\nthat the thread can continue executing in parallel with the I/O operation. An\nIRP may also be processed in interrupt-service routines and completed in an\narbitrary process context. Because some ﬁnal processing may need to take place",
  "in the context that initiated the I/O, the I/O manager uses an APC to do ﬁnal\nI/O-completion processing in the process context of the originating thread.\nThe I/O stack model is very ﬂexible. As a driver stack is built, various\ndrivers have the opportunity to insert themselves into the stack as ﬁlter drivers.\nFilter drivers can examine and potentially modify each I/O operation. Mount 856\nChapter 19\nWindows 7\nmanagement, partition management, and disk striping and mirroring are all\nexamples of functionality implemented using ﬁlter drivers that execute beneath\nthe ﬁle system in the stack. File-system ﬁlter drivers execute above the ﬁle\nsystem and have been used to implement functionalities such as hierarchical\nstorage management, single instancing of ﬁles for remote boot, and dynamic",
  "format conversion. Third parties also use ﬁle-system ﬁlter drivers to implement\nvirus detection.\nDevice drivers for Windows are written to the Windows Driver Model\n(WDM)speciﬁcation. This model lays outall the requirementsfordevice drivers,\nincluding how to layer ﬁlter drivers, share common code for handling power\nand plug-and-play requests, build correct cancellation logic, and so forth.\nBecause of the richness of the WDM, writing a full WDM device driver\nfor each new hardware device can involve a great deal of work. Fortunately,\nthe port/miniport model makes it unnecessary to do this. Within a class of\nsimilar devices, such as audio drivers, SATA devices, or Ethernet controllers,\neach instance of a device shares a common driver for that class, called a port",
  "driver. The port driver implements the standard operations for the class and\nthen calls device-speciﬁc routines in the device’s miniport driver to implement\ndevice-speciﬁc functionality. The TCP/IP network stack is implemented in\nthis way, with the ndis.sys class driver implementing much of the network\ndriver functionality and calling out to the network miniport drivers for speciﬁc\nhardware.\nRecent versions of Windows, including Windows 7, provide additional\nsimpliﬁcations for writing device drivers for hardware devices. Kernel-mode\ndrivers can now be written using the Kernel-Mode Driver Framework (KMDF),\nwhich provides a simpliﬁed programming model for drivers on top of WDM.\nAnother option is the User-Mode Driver Framework (UMDF). Many drivers",
  "do not need to operate in kernel mode, and it is easier to develop and deploy\ndrivers in user mode. It also makes the system more reliable, because a failure\nin a user-mode driver does not cause a kernel-mode crash.\n19.3.3.6\nCache Manager\nIn many operating systems, caching is done by the ﬁle system. Instead,\nWindows provides a centralized caching facility. The cache manager works\nclosely with the VM manager to provide cache services for all components\nunder the control of the I/O manager. Caching in Windows is based on ﬁles\nrather than raw blocks. The size of the cache changes dynamically according\nto how much free memory is available in the system. The cache manager\nmaintains a private working set rather than sharing the system process’s",
  "working set. The cache manager memory-maps ﬁles into kernel memory and\nthen uses special interfaces to the VM manager to fault pages into or trim them\nfrom this private working set.\nThe cache is divided into blocks of 256 KB. Each cache block can hold a\nview (that is, a memory-mapped region) of a ﬁle. Each cache block is described\nby a virtual address control block (VACB) that stores the virtual address and\nﬁle offset for the view, as well as the number of processes using the view. The\nVACBs reside in a single array maintained by the cache manager.\nWhen the I/O manager receives a ﬁle’s user-level read request, the I/O\nmanager sends an IRP to the I/O stack for the volume on which the ﬁle resides. 19.3\nSystem Components\n857",
  "System Components\n857\nFor ﬁles that are marked as cacheable, the ﬁle system calls the cache manager to\nlookup the requested datainitscached ﬁle views. The cache manager calculates\nwhich entry of that ﬁle’s VACB index array corresponds to the byte offset of\nthe request. The entry either points to the view in the cache or is invalid. If it\nis invalid, the cache manager allocates a cache block (and the corresponding\nentry in the VACB array) and maps the view into the cache block. The cache\nmanager then attempts to copy data from the mapped ﬁle to the caller’s buffer.\nIf the copy succeeds, the operation is completed.\nIf the copy fails, it does so because of a page fault, which causes the VM\nmanager to send a noncached read request to the I/Omanager. The I/O manager",
  "sends another request down the driver stack, this time requesting a paging\noperation, which bypasses the cache manager and reads the data from the ﬁle\ndirectly into the page allocated for the cache manager. Upon completion, the\nVACB is set to point at the page. The data, now in the cache, are copied to the\ncaller’s buffer, and the original I/O request is completed. Figure 19.6 shows an\noverview of these operations.\nA kernel-level read operation is similar, except that the data can be accessed\ndirectly from the cache rather than being copied to a buffer in user space. To\nuse ﬁle-system metadata (data structures that describe the ﬁle system), the\nkernel uses the cache manager’s mapping interface to read the metadata.\nTo modify the metadata, the ﬁle system uses the cache manager’s pinning",
  "interface. Pinning a page locks the page into a physical-memory page frame\nso that the VM manager cannot move the page or page it out. After updating\nthe metadata, the ﬁle system asks the cache manager to unpin the page. A\nmodiﬁed page is marked dirty, and so the VM manager ﬂushes the page to\ndisk.\nTo improve performance, the cache manager keeps a small history of read\nrequests and from this history attempts to predict future requests. If the cache\nmanager ﬁnds a pattern in the previous three requests, such as sequential\naccess forward or backward, it prefetches data into the cache before the next\ncache manager\nVM manager\nprocess\nfile system\ndisk driver\nnoncached I/O\nI/O manager\ndata copy\ncached I/O\npage fault\nI/O\nFigure 19.6\nFile I/O. 858\nChapter 19\nWindows 7",
  "Figure 19.6\nFile I/O. 858\nChapter 19\nWindows 7\nrequest is submitted by the application. In this way, the application may ﬁnd\nits data already cached and not need to wait for disk I/O.\nThe cache manager is also responsible for telling the VM manager to ﬂush\nthe contents of the cache. The cache manager’s default behavior is write-back\ncaching: it accumulates writes for 4 to 5 seconds and then wakes up the cache-\nwriter thread. When write-through caching is needed, a process can set a ﬂag\nwhen opening the ﬁle, or the process can call an explicit cache-ﬂush function.\nA fast-writing process could potentially ﬁll all the free cache pages before\nthe cache-writer thread had a chance to wake up and ﬂush the pages to disk.\nThe cache writer prevents a process from ﬂooding the system in the following",
  "way. When the amount of free cache memory becomes low, the cache manager\ntemporarily blocks processes attempting to write data and wakes the cache-\nwriter thread to ﬂush pages to disk. If the fast-writing process is actually a\nnetwork redirector for a network ﬁle system, blocking it for too long could\ncause network transfers to time out and be retransmitted. This retransmission\nwould waste network bandwidth. To prevent such waste, network redirectors\ncan instruct the cache manager to limit the backlog of writes in the cache.\nBecause a network ﬁle system needs to move data between a disk and the\nnetwork interface, the cache manager also provides a DMA interface to move\nthe data directly. Moving data directly avoids the need to copy data through\nan intermediate buffer.\n19.3.3.7",
  "an intermediate buffer.\n19.3.3.7\nSecurity Reference Monitor\nCentralizing management of system entities in the object manager enables\nWindows to use a uniform mechanism to perform run-time access validation\nand audit checks for every user-accessible entity in the system. Whenever a\nprocess opens a handle to an object, the security reference monitor (SRM)\nchecks the process’s security token and the object’s access-control list to see\nwhether the process has the necessary access rights.\nThe SRM is also responsible for manipulating the privileges in security\ntokens. Special privileges are required for users to perform backup or restore\noperations on ﬁle systems, debug processes, and so forth. Tokens can also be\nmarked as being restricted in their privileges so that they cannot access objects",
  "that are available to most users. Restricted tokens are used primarily to limit\nthe damage that can be done by execution of untrusted code.\nThe integrity level of the code executing in a process is also represented\nby a token. Integrity levels are a type of capability mechanism, as mentioned\nearlier. A process cannot modify an object with an integrity level higher than\nthat of the code executing in the process, whatever other permissions have\nbeen granted. Integrity levels were introduced to make it harder for code that\nsuccessfully attacks outward-facing software, like Internet Explorer, to take\nover a system.\nAnother responsibility of the SRM is logging security audit events. The\nDepartment of Defense’s Common Criteria (the 2005 successor to the Orange",
  "Book) requires that a secure system have the ability to detect and log all\nattempts to access system resources so that it can more easily trace attempts at\nunauthorized access. Because the SRM is responsible for making access checks,\nit generates most of the audit records in the security-event log. 19.3\nSystem Components\n859\n19.3.3.8\nPlug-and-Play Manager\nThe operating system uses the plug-and-play (PnP) manager to recognize\nand adapt to changes in the hardware conﬁguration. PnP devices use standard\nprotocols to identify themselves to the system. The PnP manager automatically\nrecognizes installed devices and detects changes in devices as the system\noperates. The manager also keeps track of hardware resources used by a",
  "device, as well as potential resources that could be used, and takes care of\nloading the appropriate drivers. This management of hardware resources—\nprimarily interrupts and I/O memory ranges—has the goal of determining a\nhardware conﬁguration in which all devices are able to operate successfully.\nThe PnP manager handles dynamic reconﬁguration as follows. First, it\ngets a list of devices from each bus driver (for example, PCI or USB). It loads\nthe installed driver (after ﬁnding one, if necessary) and sends an add-device\nrequest to the appropriate driver for each device. The PnP manager then ﬁgures\nout the optimal resource assignments and sends a start-device request to\neach driver specifying the resource assignments for the device. If a device",
  "needs to be reconﬁgured, the PnP manager sends a query-stop request, which\nasks the driver whether the device can be temporarily disabled. If the driver\ncan disable the device, then all pending operations are completed, and new\noperations are prevented from starting. Finally, the PnP manager sends a stop\nrequest and can then reconﬁgure the device with a new start-device request.\nThe PnP manager also supports other requests. For example, query-\nremove, which operates similarly to query-stop, is employed when a user\nis getting ready to eject a removable device, such as a USB storage device. The\nsurprise-remove request is used when a device fails or, more likely, when a\nuser removes a device without telling the system to stop it ﬁrst. Finally, the",
  "remove request tells the driver to stop using a device permanently.\nMany programs in the system are interested in the addition or removal\nof devices, so the PnP manager supports notiﬁcations. Such a notiﬁcation, for\nexample, gives GUI ﬁle menus the information they need to update their list\nof disk volumes when a new storage device is attached or removed. Installing\ndevices often results in adding new services to the svchost.exe processes in\nthe system. These services frequently set themselves up to run whenever the\nsystem boots and continue to run even if the original device is never plugged\ninto the system. Windows 7 introduced a service-trigger mechanism in the\nservice control manager (SCM), which manages the system services. With this",
  "mechanism, services can register themselves to start only when SCM receives a\nnotiﬁcation from the PnP manager that the device of interest has been added\nto the system.\n19.3.3.9\nPower Manager\nWindows works with the hardware to implement sophisticated strategies\nfor energy efﬁciency, as described in Section 19.2.8. The policies that drive\nthese strategies are implemented by the power manager. The power manager\ndetects current system conditions, such as the load on CPUs or I/O devices, and\nimproves energy efﬁciency by reducing the performance and responsiveness of\nthe system when need is low. The power manager can also put the entire system\ninto a very efﬁcient sleep mode and can even write all the contents of memory",
  "to disk and turn off the power to allow the system to go into hibernation. 860\nChapter 19\nWindows 7\nThe primary advantage of sleep is that the system can enter fairly quickly,\nperhaps just a few seconds after the lid closes on a laptop. The return from\nsleep is also fairly quick. The power is turned down low on the CPUs and I/O\ndevices, but the memory continues to be powered enough that its contents are\nnot lost.\nHibernation takes considerably longer because the entire contents of\nmemory must be transferred to disk before the system is turned off. However,\nthe fact that the system is, in fact, turned off is a signiﬁcant advantage. If\nthere is a loss of power to the system, as when the battery is swapped on a\nlaptop or a desktop system is unplugged, the saved system data will not be",
  "lost. Unlike shutdown, hibernation saves the currently running system so a\nuser can resume where she left off, and because hibernation does not require\npower, a system can remain in hibernation indeﬁnitely.\nLike the PnP manager, the power manager provides notiﬁcations to the\nrest of the system about changes in the power state. Some applications want to\nknow when the system is about to be shut down so they can start saving their\nstates to disk.\n19.3.3.10\nRegistry\nWindows keeps much of its conﬁguration information in internal databases,\ncalled hives, that are managed by the Windows conﬁguration manager, which\nis commonly known as the registry. There are separate hives for system\ninformation, default user preferences, software installation, security, and boot",
  "options. Because the information in the system hive is required to boot the\nsystem, the registry manager is implemented as a component of the executive.\nThe registry represents the conﬁguration state in each hive as a hierarchical\nnamespace of keys (directories), each of which can contain a set of typed values,\nsuch as UNICODE string, ANSI string, integer, or untyped binary data. In theory,\nnew keys and values are created and initialized as new software is installed;\nthen they are modiﬁed to reﬂect changes in the conﬁguration of that software.\nIn practice, the registry is often used as a general-purpose database, as an\ninterprocess-communication mechanism, and for many other such inventive\npurposes.\nRestarting applications, or even the system, every time a conﬁguration",
  "change was made would be a nuisance. Instead, programs rely on various\nkinds of notiﬁcations, such as those provided by the PnP and power managers,\nto learn about changes in the system conﬁguration. The registry also supplies\nnotiﬁcations; it allows threads to register to be notiﬁed when changes are\nmade to some part of the registry. The threads can thus detect and adapt to\nconﬁguration changes recorded in the registry itself.\nWhenever signiﬁcant changes are made to the system, such as when\nupdates to the operating system or drivers are installed, there is a danger that\nthe conﬁguration data may be corrupted (for example, if a working driver is\nreplaced by a nonworking driver or an application fails to install correctly and",
  "leaves partial information in the registry). Windows creates a system restore\npoint before making such changes. The restore point contains a copy of the\nhives before the change and can be used to return to this version of the hives\nand thereby get a corrupted system working again. 19.3\nSystem Components\n861\nTo improve the stability of the registry conﬁguration, Windows added a\ntransaction mechanism beginning with Windows Vista that can be used to\nprevent the registry from being partially updated with a collection of related\nconﬁguration changes. Registry transactions can be part of more general\ntransactions administered by the kernel transaction manager (KTM), which\ncan also include ﬁle-system transactions. KTM transactions do not have the",
  "full semantics found in normal database transactions, and they have not\nsupplanted the system restore facility for recovering from damage to the\nregistry conﬁguration caused by software installation.\n19.3.3.11\nBooting\nThe booting of a Windows PC begins when the hardware powers on and\nﬁrmware begins executing from ROM. In older machines, this ﬁrmware was\nknown as the BIOS, but more modern systems use UEFI (the Uniﬁed Extensible\nFirmware Interface), which is faster and more general and makes better use of\nthe facilities in contemporary processors. The ﬁrmware runs power-on self-test\n(POST) diagnostics; identiﬁes many of the devices attached to the system and\ninitializes them to a clean, power-up state; and then builds the description",
  "used by the advanced conﬁguration and power interface (ACPI). Next, the\nﬁrmware ﬁnds the system disk, loads the Windows bootmgr program, and\nbegins executing it.\nIn a machine that has been hibernating, the winresume program is loaded\nnext. It restores the running system from disk, and the system continues\nexecution at the point it had reached right before hibernating. In a machine\nthat has been shut down, the bootmgr performs further initialization of the\nsystem and then loads winload. This program loads hal.dll, the kernel\n(ntoskrnl.exe), any drivers needed in booting, and the system hive. winload\nthen transfers execution to the kernel.\nThe kernel initializes itself and creates two processes. The system process",
  "contains all the internal kernel worker threads and never executes in user mode.\nThe ﬁrst user-mode process created is SMSS, for session manager subsystem,\nwhich is similar to the INIT (initialization) process in UNIX. SMSS performs\nfurther initialization of the system, including establishing the paging ﬁles,\nloading more device drivers, and managing the Windows sessions. Each\nsession is used to represent a logged-on user, except for session 0, which is\nused to run system-wide background services, such as LSASS and SERVICES.\nA session is anchored by an instance of the CSRSS process. Each session other\nthan 0 initially runs the WINLOGON process. This process logs on a user and\nthen launches the EXPLORER process, which implements the Windows GUI",
  "experience. The following list itemizes some of these aspects of booting:\n• SMSS completes system initialization and then starts up session 0 and the\nﬁrst login session.\n• WININIT runs in session 0 to initialize user mode and start LSASS, SERVICES,\nand the local session manager, LSM.\n• LSASS, the security subsystem, implements facilities such as authentication\nof users. 862\nChapter 19\nWindows 7\n• SERVICES contains the service control manager, or SCM, which supervises\nall background activities in the system, including user-mode services. A\nnumber of services will have registered to start when the system boots.\nOthers will be started only on demand or when triggered by an event such\nas the arrival of a device.\n• CSRSS is the Win32 environmental subsystem process. It is started in every",
  "session—unlike the POSIX subsystem, which is started only on demand\nwhen a POSIX process is created.\n• WINLOGON is run in each Windows session other than session 0 to log on\na user.\nThe system optimizes the boot process by prepaging from ﬁles on disk\nbased on previous boots of the system. Disk access patterns at boot are also\nused to lay out system ﬁles on disk to reduce the number of I/O operations\nrequired. The processes necessary to start the system are reduced by grouping\nservices into fewer processes. All of these approaches contribute to a dramatic\nreduction in system boot time. Of course, system boot time is less important\nthan it once was because of the sleep and hibernation capabilities of Windows.\n19.4 Terminal Services and Fast User Switching",
  "19.4 Terminal Services and Fast User Switching\nWindows supports a GUI-based console that interfaces with the user via\nkeyboard, mouse, and display. Most systems also support audio and video.\nAudio input is used by Windows voice-recognition software; voice recognition\nmakes the system more convenient and increases its accessibility for users with\ndisabilities. Windows 7 added support for multi-touch hardware, allowing\nusers to input data by touching the screen and making gestures with one or\nmore ﬁngers. Eventually, the video-input capability, which is currently used\nfor communication applications, is likely to be used for visually interpreting\ngestures, as Microsoft has demonstrated for its Xbox 360 Kinect product. Other",
  "future input experiences may evolve from Microsoft’s surface computer. Most\noften installed at public venues, such as hotels and conference centers, the\nsurface computer is a table surface with special cameras underneath. It can\ntrack the actions of multiple users at once and recognize objects that are placed\non top.\nThe PC was, of course, envisioned as a personal computer—an inherently\nsingle-user machine. Modern Windows, however, supports the sharing of a PC\namong multiple users. Each user that is logged on using the GUI has a session\ncreated to represent the GUI environment he will be using and to contain all the\nprocesses created to run his applications. Windows allows multiple sessions to\nexist at the same time on a single machine. However, Windows only supports",
  "a single console, consisting of all the monitors, keyboards, and mice connected\nto the PC. Only one session can be connected to the console at a time. From the\nlogon screen displayed on the console, users can create new sessions or attach\nto an existing session that was previously created. This allows multiple users\nto share a single PC without having to log off and on between users. Microsoft\ncalls this use of sessions fast user switching. 19.5\nFile System\n863\nUsers can also create new sessions, or connect to existing sessions, on one\nPC from a session running on another Windows PC. The terminal server (TS)\nconnects one of the GUI windows in a user’s local session to the new or existing\nsession, called a remote desktop, on the remote computer. The most common",
  "use of remote desktops is for users to connect to a session on their work PC\nfrom their home PC.\nMany corporations use corporate terminal-server systems maintained in\ndata centers to run all user sessions that access corporate resources, rather than\nallowing users to access those resources from the PCs in each user’s ofﬁce. Each\nserver computer may handle many dozens of remote-desktop sessions. This\nis a form of thin-client computing, in which individual computers rely on a\nserver for many functions. Relying on data-center terminal servers improves\nreliability, manageability, and security of the corporate computing resources.\nThe TS is also used by Windows to implement remote assistance. A remote\nuser can be invited to share a session with the user logged on to the session on",
  "the console. The remote user can watch the user’s actions and even be given\ncontrol of the desktop to help resolve computing problems.\n19.5 File System\nThe native ﬁle system in Windows is NTFS. It is used for all local volumes.\nHowever, associated USB thumb drives, ﬂash memory on cameras, and external\ndisks may be formatted with the 32-bit FAT ﬁle system for portability. FAT is\na much older ﬁle-system format that is understood by many systems besides\nWindows, such as the software running on cameras. A disadvantage is that\nthe FAT ﬁle system does not restrict ﬁle access to authorized users. The only\nsolution for securing data with FAT is to run an application to encrypt the data\nbefore storing it on the ﬁle system.",
  "before storing it on the ﬁle system.\nIn contrast, NTFS uses ACLs to control access to individual ﬁles and supports\nimplicit encryption of individual ﬁles or entire volumes (using Windows\nBitLocker feature). NTFS implements many other features as well, including\ndata recovery, fault tolerance, very large ﬁles and ﬁle systems, multiple data\nstreams, UNICODE names, sparse ﬁles, journaling, volume shadow copies, and\nﬁle compression.\n19.5.1\nNTFS Internal Layout\nThe fundamental entity in NTFS is a volume. A volume is created by the\nWindows logical disk management utility and is based on a logical disk\npartition. A volume may occupy a portion of a disk or an entire disk, or may\nspan several disks.\nNTFS does not deal with individual sectors of a disk but instead uses clusters",
  "as the units of disk allocation. A cluster is a number of disk sectors that is a\npower of 2. The cluster size is conﬁgured when an NTFS ﬁle system is formatted.\nThe default cluster size is based on the volume size—4 KB for volumes larger\nthan 2 GB. Given the size of today’s disks, it may make sense to use cluster sizes\nlarger than the Windows defaults to achieve better performance, although these\nperformance gains will come at the expense of more internal fragmentation.\nNTFS uses logical cluster numbers (LCNs) as disk addresses. It assigns them\nby numbering clusters from the beginning of the disk to the end. Using this 864\nChapter 19\nWindows 7\nscheme, the system can calculate a physical disk offset (in bytes) by multiplying\nthe LCN by the cluster size.",
  "the LCN by the cluster size.\nA ﬁle in NTFS is not a simple byte stream as it is in UNIX; rather, it is a\nstructured object consisting of typed attributes. Each attribute of a ﬁle is an\nindependent byte stream that can be created, deleted, read, and written. Some\nattribute types are standard for all ﬁles, including the ﬁle name (or names, if\nthe ﬁle has aliases, such as an MS-DOS short name), the creation time, and the\nsecurity descriptor that speciﬁes the access control list. User data are stored in\ndata attributes.\nMost traditional data ﬁles have an unnamed data attribute that contains\nall the ﬁle’s data. However, additional data streams can be created with\nexplicit names. For instance, in Macintosh ﬁles stored on a Windows server, the",
  "resource fork is a named data stream. The IProp interfaces of the Component\nObject Model (COM) use a named data stream to store properties on ordinary\nﬁles, including thumbnails of images. In general, attributes may be added as\nnecessary and are accessed using a ﬁle-name:attribute syntax. NTFS returns\nonly the size of the unnamed attribute in response to ﬁle-query operations,\nsuch as when running the dir command.\nEvery ﬁle in NTFS is described by one or more records in an array stored in a\nspecial ﬁle called the master ﬁle table (MFT). The size of a record is determined\nwhen the ﬁle system is created; it ranges from 1 to 4 KB. Small attributes\nare stored in the MFT record itself and are called resident attributes. Large",
  "attributes, such as the unnamed bulk data, are called nonresident attributes\nand are stored in one or more contiguous extents on the disk. A pointer to\neach extent is stored in the MFT record. For a small ﬁle, even the data attribute\nmay ﬁt inside the MFT record. If a ﬁle has many attributes—or if it is highly\nfragmented, so that many pointers are needed to point to all the fragments\n—one record in the MFT might not be large enough. In this case, the ﬁle is\ndescribed by a record called the base ﬁle record, which contains pointers to\noverﬂow records that hold the additional pointers and attributes.\nEach ﬁle in an NTFS volume has a unique ID called a ﬁle reference. The ﬁle\nreference is a 64-bit quantity that consists of a 48-bit ﬁle number and a 16-bit",
  "sequence number. The ﬁle number is the record number (that is, the array slot)\nin the MFT that describes the ﬁle. The sequence number is incremented every\ntime an MFT entry is reused. The sequence number enables NTFS to perform\ninternal consistency checks, such as catching a stale reference to a deleted ﬁle\nafter the MFT entry has been reused for a new ﬁle.\n19.5.1.1\nNTFS B+ Tree\nAs in UNIX, the NTFS namespace is organized as a hierarchy of directories. Each\ndirectory uses a data structure called a B+ tree to store an index of the ﬁle names\nin that directory. In a B+ tree, the length of every path from the root of the tree to\na leaf is the same, and the cost of reorganizing the tree is eliminated. The index\nroot of a directory contains the top level of the B+ tree. For a large directory,",
  "this top level contains pointers to disk extents that hold the remainder of the\ntree. Each entry in the directory contains the name and ﬁle reference of the\nﬁle, as well as a copy of the update timestamp and ﬁle size taken from the\nﬁle’s resident attributes in the MFT. Copies of this information are stored in the\ndirectory so that a directory listing can be efﬁciently generated. Because all the\nﬁle names, sizes, and update times are available from the directory itself, there\nis no need to gather these attributes from the MFT entries for each of the ﬁles. 19.5\nFile System\n865\n19.5.1.2\nNTFS Metadata\nThe NTFS volume’s metadata are all stored in ﬁles. The ﬁrst ﬁle is the MFT. The\nsecond ﬁle, which is used during recovery if the MFT is damaged, contains a",
  "copy of the ﬁrst 16 entries of the MFT. The next few ﬁles are also special in\npurpose. They include the ﬁles described below.\n• The log ﬁle records all metadata updates to the ﬁle system.\n• The volume ﬁle contains the name of the volume, the version of NTFS that\nformatted the volume, and a bit that tells whether the volume may have\nbeen corrupted and needs to be checked for consistency using the chkdsk\nprogram.\n• The attribute-deﬁnition table indicates which attribute types are used in\nthe volume and what operations can be performed on each of them.\n• The root directory is the top-level directory in the ﬁle-system hierarchy.\n• The bitmap ﬁle indicates which clusters on a volume are allocated to ﬁles\nand which are free.",
  "and which are free.\n• The boot ﬁle contains the startup code for Windows and must be located\nat a particular disk address so that it can be found easily by a simple ROM\nbootstrap loader. The boot ﬁle also contains the physical address of the\nMFT.\n• The bad-cluster ﬁle keeps track of any bad areas on the volume; NTFS uses\nthis record for error recovery.\nKeeping all the NTFS metadata in actual ﬁles has a useful property. As\ndiscussed in Section 19.3.3.6, the cache manager caches ﬁle data. Since all\nthe NTFS metadata reside in ﬁles, these data can be cached using the same\nmechanisms used for ordinary data.\n19.5.2\nRecovery\nIn many simple ﬁle systems, a power failure at the wrong time can damage\nthe ﬁle-system data structures so severely that the entire volume is scrambled.",
  "Many UNIX ﬁle systems, including UFS but not ZFS, store redundant metadata\non the disk, and they recover from crashes by using the fsck program to check\nall the ﬁle-system data structures and restore them forcibly to a consistent\nstate. Restoring them often involves deleting damaged ﬁles and freeing data\nclusters that had been written with user data but not properly recorded in the\nﬁle system’s metadata structures. This checking can be a slow process and can\ncause the loss of signiﬁcant amounts of data.\nNTFS takes a different approach to ﬁle-system robustness. In NTFS, all ﬁle-\nsystem data-structure updates are performed inside transactions. Before a data\nstructure is altered, the transaction writes a log record that contains redo and",
  "undo information. After the data structure has been changed, the transaction\nwrites a commit record to the log to signify that the transaction succeeded.\nAfter a crash, the system can restore the ﬁle-system data structures to\na consistent state by processing the log records, ﬁrst redoing the operations\nfor committed transactions and then undoing the operations for transactions 866\nChapter 19\nWindows 7\nthat did not commit successfully before the crash. Periodically (usually every\n5 seconds), a checkpoint record is written to the log. The system does not\nneed log records prior to the checkpoint to recover from a crash. They can be\ndiscarded, so the log ﬁle does not grow without bounds. The ﬁrst time after\nsystem startup that an NTFS volume is accessed, NTFS automatically performs",
  "ﬁle-system recovery.\nThis scheme does not guarantee that all the user-ﬁle contents are correct\nafter a crash. It ensures only that the ﬁle-system data structures (the metadata\nﬁles) are undamaged and reﬂect some consistent state that existed prior to the\ncrash. It would be possible to extend the transaction scheme to cover user ﬁles,\nand Microsoft took some steps to do this in Windows Vista.\nThe log is stored in the third metadata ﬁle at the beginning of the volume.\nIt is created with a ﬁxed maximum size when the ﬁle system is formatted. It\nhas two sections: the logging area, which is a circular queue of log records, and\nthe restart area, which holds context information, such as the position in the\nlogging area where NTFS should start reading during a recovery. In fact, the",
  "restart area holds two copies of its information, so recovery is still possible if\none copy is damaged during the crash.\nThe logging functionality is provided by the log-ﬁle service. In addition\nto writing the log records and performing recovery actions, the log-ﬁle service\nkeeps track of the free space in the log ﬁle. If the free space gets too low,\nthe log-ﬁle service queues pending transactions, and NTFS halts all new I/O\noperations. After the in-progress operations complete, NTFS calls the cache\nmanager to ﬂush all data and then resets the log ﬁle and performs the queued\ntransactions.\n19.5.3\nSecurity\nThe security of an NTFS volume is derived from the Windows object model.\nEach NTFS ﬁle references a security descriptor, which speciﬁes the owner of the",
  "ﬁle, and an access-control list, which contains the access permissions granted\nor denied to each user or group listed. Early versions of NTFS used a separate\nsecurity descriptor as an attribute of each ﬁle. Beginning with Windows 2000,\nthe security-descriptors attribute points to a shared copy, with a signiﬁcant\nsavings in disk and caching space; many, many ﬁles have identical security\ndescriptors.\nIn normal operation, NTFS does not enforce permissions on traversal of\ndirectories in ﬁle path names. However, for compatibility with POSIX, these\nchecks can be enabled. Traversal checks are inherently more expensive, since\nmodern parsing of ﬁle path names uses preﬁx matching rather than directory-\nby-directory parsing of path names. Preﬁx matching is an algorithm that looks",
  "up strings in a cache and ﬁnds the entry with the longest match—for example,\nan entry for \\foo\\bar\\dir would be a match for \\foo\\bar\\dir2\\dir3\\myfile.\nThe preﬁx-matching cache allows path-name traversal to begin much deeper\nin the tree, saving many steps. Enforcing traversal checks means that the user’s\naccess must be checked at each directory level. For instance, a user might lack\npermission to traverse \\foo\\bar, so starting at the access for \\foo\\bar\\dir\nwould be an error. 19.5\nFile System\n867\n19.5.4\nVolume Management and Fault Tolerance\nFtDisk is the fault-tolerant disk driver for Windows. When installed, it\nprovides several ways to combine multiple disk drives into one logical volume\nso as to improve performance, capacity, or reliability.\n19.5.4.1\nVolume Sets and RAID Sets",
  "19.5.4.1\nVolume Sets and RAID Sets\nOne way to combine multiple disks is to concatenate them logically to form a\nlarge logical volume, as shown in Figure 19.7. In Windows, this logical volume,\ncalled a volume set, can consist of up to 32 physical partitions. A volume set\nthat contains an NTFS volume can be extended without disturbance of the data\nalready stored in the ﬁle system. The bitmap metadata on the NTFS volume are\nsimply extended to cover the newly added space. NTFS continues to use the\nsame LCN mechanism that it uses for a single physical disk, and the FtDisk\ndriver supplies the mapping from a logical-volume offset to the offset on one\nparticular disk.\nAnother way to combine multiple physical partitions is to interleave",
  "their blocks in round-robin fashion to form a stripe set. This scheme is also\ncalled RAID level 0, or disk striping. (For more on RAID (redundant arrays of\ninexpensive disks), see Section 10.7.) FtDisk uses a stripe size of 64 KB. The\nﬁrst 64 KB of the logical volume are stored in the ﬁrst physical partition, the\nsecond 64 KB in the second physical partition, and so on, until each partition\nhas contributed 64 KB of space. Then, the allocation wraps around to the ﬁrst\ndisk, allocating the second 64-KB block. A stripe set forms one large logical\nvolume, but the physical layout can improve the I/O bandwidth, because for\na large I/O, all the disks can transfer data in parallel. Windows also supports\nRAID level 5, stripe set with parity, and RAID level 1, mirroring.\nLCNs 0–128000",
  "LCNs 0–128000\nLCNs 128001–783361\ndisk 1 (2.5 GB)\ndisk 2 (2.5 GB)\ndisk C: (FAT) 2 GB\nlogical drive D: (NTFS) 3 GB\nFigure 19.7\nVolume set on two drives. 868\nChapter 19\nWindows 7\n19.5.4.2\nSector Sparing and Cluster Remapping\nTo deal with disk sectors that go bad, FtDisk uses a hardware technique called\nsector sparing, and NTFS uses a software technique called cluster remapping.\nSector sparing is a hardware capability provided by many disk drives. When\na disk drive is formatted, it creates a map from logical block numbers to good\nsectors on the disk. It also leaves extra sectors unmapped, as spares. If a sector\nfails, FtDisk instructs the disk drive to substitute a spare. Cluster remapping\nis a software technique performed by the ﬁle system. If a disk block goes",
  "bad, NTFS substitutes a different, unallocated block by changing any affected\npointers in the MFT. NTFS also makes a note that the bad block should never be\nallocated to any ﬁle.\nWhen a disk block goes bad, the usual outcome is a data loss. But sector\nsparing or cluster remapping can be combined with fault-tolerant volumes to\nmask the failure of a disk block. If a read fails, the system reconstructs the\nmissing data by reading the mirror or by calculating the exclusive or parity\nin a stripe set with parity. The reconstructed data are stored in a new location\nthat is obtained by sector sparing or cluster remapping.\n19.5.5\nCompression\nNTFS can perform data compression on individual ﬁles or on all data ﬁles in\na directory. To compress a ﬁle, NTFS divides the ﬁle’s data into compression",
  "units, which are blocks of 16 contiguous clusters. When a compression unit\nis written, a data-compression algorithm is applied. If the result ﬁts into\nfewer than 16 clusters, the compressed version is stored. When reading, NTFS\ncan determine whether data have been compressed: if they have been, the\nlength of the stored compression unit is less than 16 clusters. To improve\nperformance when reading contiguous compression units, NTFS prefetches\nand decompresses ahead of the application requests.\nFor sparse ﬁles or ﬁles that contain mostly zeros, NTFS uses another\ntechnique to save space. Clusters that contain only zeros because they have\nnever been written are not actually allocated or stored on disk. Instead, gaps",
  "are left in the sequence of virtual-cluster numbers stored in the MFT entry for\nthe ﬁle. When reading a ﬁle, if NTFS ﬁnds a gap in the virtual-cluster numbers,\nit just zero-ﬁlls that portion of the caller’s buffer. This technique is also used\nby UNIX.\n19.5.6\nMount Points, Symbolic Links, and Hard Links\nMount points are a form of symbolic link speciﬁc to directories on NTFS that\nwere introduced in Windows 2000. They provide a mechanism for organizing\ndisk volumes that is more ﬂexible than the use of global names (like drive\nletters). A mount point is implemented as a symbolic link with associated\ndata that contains the true volume name. Ultimately, mount points will\nsupplant drive letters completely, but there will be a long transition due to",
  "the dependence of many applications on the drive-letter scheme.\nWindows Vista introduced support for a more general form of symbolic\nlinks, similar to those found in UNIX. The links can be absolute or relative, can\npoint to objects that do not exist, and can point to both ﬁles and directories 19.6\nNetworking\n869\neven across volumes. NTFS also supports hard links, where a single ﬁle has an\nentry in more than one directory of the same volume.\n19.5.7\nChange Journal\nNTFS keeps a journal describing all changes that have been made to the ﬁle\nsystem. User-mode services can receive notiﬁcations of changes to the journal\nand then identify what ﬁles have changed by reading from the journal. The\nsearch indexer service uses the change journal to identify ﬁles that need to be",
  "re-indexed. The ﬁle-replication service uses it to identify ﬁles that need to be\nreplicated across the network.\n19.5.8\nVolume Shadow Copies\nWindows implements the capability of bringing a volume to a known state\nand then creating a shadow copy that can be used to back up a consistent\nview of the volume. This technique is known as snapshots in some other ﬁle\nsystems. Making a shadow copy of a volume is a form of copy-on-write, where\nblocks modiﬁed after the shadow copy is created are stored in their original\nform in the copy. To achieve a consistent state for the volume requires the\ncooperation of applications, since the system cannot know when the data used\nby the application are in a stable state from which the application could be\nsafely restarted.",
  "safely restarted.\nThe server version of Windows uses shadow copies to efﬁciently maintain\nold versions of ﬁles stored on ﬁle servers. This allows users to see documents\nstored on ﬁle servers as they existed at earlier points in time. The user can use\nthis feature to recover ﬁles that were accidentally deleted or simply to look at\na previous version of the ﬁle, all without pulling out backup media.\n19.6 Networking\nWindows supports both peer-to-peer and client–server networking. It also has\nfacilities for network management. The networking components in Windows\nprovide data transport, interprocess communication, ﬁle sharing across a\nnetwork, and the ability to send print jobs to remote printers.\n19.6.1\nNetwork Interfaces",
  "19.6.1\nNetwork Interfaces\nTo describe networking in Windows, we must ﬁrst mention two of the internal\nnetworking interfaces: the network device interface speciﬁcation (NDIS) and\nthe transport driver interface (TDI). The NDIS interface was developed in 1989\nby Microsoft and 3Com to separate network adapters from transport protocols\nso that either could be changed without affecting the other. NDIS resides at\nthe interface between the data-link and network layers in the ISO model and\nenables many protocols to operate over many different network adapters. In\nterms of the ISO model, the TDI is the interface between the transport layer\n(layer 4) and the session layer (layer 5). This interface enables any session-layer\ncomponent to use any available transport mechanism. (Similar reasoning led",
  "to the streams mechanism in UNIX.) The TDI supports both connection-based\nand connectionless transport and has functions to send any type of data. 870\nChapter 19\nWindows 7\n19.6.2\nProtocols\nWindows implements transport protocols as drivers. These drivers can be\nloaded and unloaded from the system dynamically, although in practice the\nsystem typically has to be rebooted after a change. Windows comes with several\nnetworking protocols. Next, we discuss a number of these protocols.\n19.6.2.1\nServer-Message Block\nThe server-message-block (SMB) protocol was ﬁrst introduced in MS-DOS 3.1.\nThe system uses the protocol to send I/O requests over the network. The SMB\nprotocol has four message types. Session control messages are commands",
  "that start and end a redirector connection to a shared resource at the server. A\nredirector uses File messages to access ﬁles at the server. Printer messages\nare used to send data to a remote print queue and to receive status information\nfrom the queue, and Message messages are used to communicate with another\nworkstation. A version of the SMB protocol was published as the common\nInternet ﬁle system (CIFS) and is supported on a number of operating systems.\n19.6.2.2\nTransmission Control Protocol/Internet Protocol\nThe transmission control protocol/Internet protocol (TCP/IP) suite that is used\non the Internet has become the de facto standard networking infrastructure.\nWindows uses TCP/IP to connect to a wide variety of operating systems",
  "and hardware platforms. The Windows TCP/IP package includes the simple\nnetwork-management protocol (SNM), the dynamic host-conﬁguration proto-\ncol (DHCP), and the older Windows Internet name service (WINS). Windows\nVista introduced a new implementation of TCP/IP that supports both IPv4\nand IPv6 in the same network stack. This new implementation also supports\nofﬂoading of the network stack onto advanced hardware, to achieve very high\nperformance for servers.\nWindows provides a software ﬁrewall that limits the TCP ports that can\nbe used by programs for network communication. Network ﬁrewalls are\ncommonly implemented in routers and are a very important security measure.\nHaving a ﬁrewall built into the operating system makes a hardware router",
  "unnecessary, and it also provides more integrated management and easier use.\n19.6.2.3\nPoint-to-Point Tunneling Protocol\nThe point-to-point tunneling protocol (PPTP) is a protocol provided by\nWindows to communicate between remote-access server modules running\non Windows server machines and other client systems that are connected\nover the Internet. The remote-access servers can encrypt data sent over the\nconnection, and they support multiprotocol virtual private networks (VPNs)\nover the Internet.\n19.6.2.4\nHTTP Protocol\nThe HTTP protocol is used to get/put information using the World Wide Web.\nWindows implements HTTP using a kernel-mode driver, so web servers can\noperate with a low-overhead connection to the networking stack. HTTP is a 19.6\nNetworking\n871",
  "Networking\n871\nfairly general protocol, which Windows makes available as a transport option\nfor implementing RPC.\n19.6.2.5\nWeb-Distributed Authoring and Versioning Protocol\nWeb-distributed authoringand versioning(WebDAV)isan HTTP-based protocol\nfor collaborative authoring across a network. Windows builds a WebDAV\nredirector into the ﬁle system. Being built directly into the ﬁle system enables\nWebDAV to work with other ﬁle-system features, such as encryption. Personal\nﬁles can then be stored securely in a public place. Because WebDAV uses HTTP,\nwhich is a get/put protocol, Windows has to cache the ﬁles locally so programs\ncan use read and write operations on parts of the ﬁles.\n19.6.2.6\nNamed Pipes\nNamed pipes are a connection-oriented messaging mechanism. A process can",
  "use named pipes to communicate with other processes on the same machine.\nSince named pipes are accessed through the ﬁle-system interface, the security\nmechanisms used for ﬁle objects also apply to named pipes. The SMB protocol\nsupports named pipes, so named pipes can also be used for communication\nbetween processes on different systems.\nThe format of pipe names follows the uniform naming convention\n(UNC). A UNC name looks like a typical remote ﬁle name. The format is\n\\\\server name\\share name\\x\\y\\z, where server name identiﬁes a server\non the network; share name identiﬁes any resource that is made available\nto network users, such as directories, ﬁles, named pipes, and printers; and\n\\x\\y\\z is a normal ﬁle path name.\n19.6.2.7\nRemote Procedure Calls",
  "19.6.2.7\nRemote Procedure Calls\nA remote procedure call (RPC) is a client–server mechanism that enables an\napplication on one machine to make a procedure call to code on another\nmachine. The client calls a local procedure—a stub routine—that packs its\narguments into a message and sends them across the network to a particular\nserver process. The client-side stub routine then blocks. Meanwhile, the server\nunpacks the message, calls the procedure, packs the return results into a\nmessage, and sends them back to the client stub. The client stub unblocks,\nreceives the message, unpacks the results of the RPC, and returns them to the\ncaller. This packing of arguments is sometimes called marshaling. The client\nstub code and the descriptors necessary to pack and unpack the arguments for",
  "an RPC are compiled from a speciﬁcation written in the Microsoft Interface\nDeﬁnition Language.\nThe Windows RPC mechanism follows the widely used distributed-\ncomputing-environment standard for RPC messages, so programs written to\nuse Windows RPCs are highly portable. The RPC standard is detailed. It hides\nmany of the architectural differences among computers, such as the sizes\nof binary numbers and the order of bytes and bits in computer words, by\nspecifying standard data formats for RPC messages. 872\nChapter 19\nWindows 7\n19.6.2.8\nComponent Object Model\nThe component object model (COM) is a mechanism for interprocess commu-\nnication that was developed for Windows. COM objects provide a well-deﬁned\ninterface to manipulate the data in the object. For instance, COM is the infras-",
  "tructure used by Microsoft’s object linking and embedding (OLE) technology\nfor inserting spreadsheets into Microsoft Word documents. Many Windows\nservices provide COM interfaces. Windows has a distributed extension called\nDCOM that can be used over a network utilizing RPC to provide a transparent\nmethod of developing distributed applications.\n19.6.3\nRedirectors and Servers\nIn Windows, an application can use the Windows I/O API to access ﬁles from a\nremote computer as though they were local, provided that the remote computer\nis running a CIFS server such as those provided by Windows. A redirector is the\nclient-side object that forwards I/O requests to a remote system, where they are\nsatisﬁed by a server. For performance and security, the redirectors and servers\nrun in kernel mode.",
  "run in kernel mode.\nIn more detail, access to a remote ﬁle occurs as follows:\n1. The application calls the I/O manager to request that a ﬁle be opened with\na ﬁle name in the standard UNC format.\n2. The I/O manager builds an I/O request packet, as described in Section\n19.3.3.5.\n3. The I/O manager recognizes that the access is for a remote ﬁle and calls a\ndriver called a multiple universal-naming-convention provider (MUP).\n4. The MUP sends the I/O request packet asynchronously to all registered\nredirectors.\n5. A redirector that can satisfy the request responds to the MUP. To avoid\nasking all the redirectors the same question in the future, the MUP uses a\ncache to remember which redirector can handle this ﬁle.\n6. The redirector sends the network request to the remote system.",
  "7. The remote-system network drivers receive the request and pass it to the\nserver driver.\n8. The server driver hands the request to the proper local ﬁle-system driver.\n9. The proper device driver is called to access the data.\n10. The results are returned to the server driver, which sends the data back\nto the requesting redirector. The redirector then returns the data to the\ncalling application via the I/O manager.\nA similar process occurs for applications that use the Win32 network API, rather\nthan the UNC services, except that a module called a multi-provider router is\nused instead of a MUP.\nFor portability, redirectors and servers use the TDI API for network\ntransport. The requests themselves are expressed in a higher-level protocol, 19.6\nNetworking\n873",
  "Networking\n873\nwhich by default is the SMB protocol described in Section 19.6.2. The list of\nredirectors is maintained in the system hive of the registry.\n19.6.3.1\nDistributed File System\nUNC names are not always convenient, because multiple ﬁle servers may be\navailable to serve the same content and UNC names explicitly include the name\nof the server. Windows supports a distributed ﬁle-system (DFS) protocol that\nallows a network administrator to serve up ﬁles from multiple servers using a\nsingle distributed name space.\n19.6.3.2\nFolder Redirection and Client-Side Caching\nTo improve the PC experience for users who frequently switch among com-\nputers, Windows allows administrators to give users roaming proﬁles, which",
  "keep users’ preferences and other settings on servers. Folder redirection is\nthen used to automatically store a user’s documents and other ﬁles on a server.\nThis works well until one of the computers is no longer attached to the\nnetwork, as when a user takes a laptop onto an airplane. To give users off-line\naccess to their redirected ﬁles, Windows uses client-side caching (CSC). CSC\nis also used when the computer is on-line to keep copies of the server ﬁles\non the local machine for better performance. The ﬁles are pushed up to the\nserver as they are changed. If the computer becomes disconnected, the ﬁles are\nstill available, and the update of the server is deferred until the next time the\ncomputer is online.\n19.6.4\nDomains",
  "computer is online.\n19.6.4\nDomains\nMany networked environments have natural groups of users, such as students\nin a computer laboratory at school or employees in one department in a\nbusiness. Frequently, we want all the members of the group to be able to\naccess shared resources on their various computers in the group. To manage\nthe global access rights within such groups, Windows uses the concept of\na domain. Previously, these domains had no relationship whatsoever to the\ndomain-name system (DNS) that maps Internet host names to IP addresses.\nNow, however, they are closely related.\nSpeciﬁcally, a Windows domain is a group of Windows workstations\nand servers that share a common security policy and user database. Since",
  "Windows uses the Kerberos protocol for trust and authentication, a Windows\ndomain is the same thing as a Kerberos realm. Windows uses a hierarchical\napproach for establishing trust relationships between related domains. The\ntrust relationships are based on DNS and allow transitive trusts that can ﬂow up\nand down the hierarchy. This approach reduces the number of trusts required\nfor n domains from n ∗(n −1) to O(n). The workstations in the domain trust\nthe domain controller to give correct information about the access rights of\neach user (loaded into the user’s access token by LSASS). All users retain the\nability to restrict access to their own workstations, however, no matter what\nany domain controller may say. 874\nChapter 19\nWindows 7\n19.6.5\nActive Directory",
  "Chapter 19\nWindows 7\n19.6.5\nActive Directory\nActive Directory is the Windows implementation of lightweight directory-\naccess protocol (LDAP) services. Active Directory stores the topology infor-\nmation about the domain, keeps the domain-based user and group accounts\nand passwords, and provides a domain-based store for Windows features that\nneed it, such as Windows group policy. Administrators use group policies to\nestablish uniform standards for desktop preferences and software. For many\ncorporate information-technology groups, uniformity drastically reduces the\ncost of computing.\n19.7 Programmer Interface\nThe Win32 API is the fundamental interface to the capabilities of Windows. This\nsection describes ﬁve main aspects of the Win32 API: access to kernel objects,",
  "sharing of objects between processes, process management, interprocess com-\nmunication, and memory management.\n19.7.1\nAccess to Kernel Objects\nThe Windows kernel provides many services that application programs can\nuse. Application programs obtain these services by manipulating kernel\nobjects. A process gains access to a kernel object named XXX by calling the\nCreateXXX function to open a handle to an instance of XXX. This handle is\nunique to the process. Depending on which object is being opened, if the\nCreate() function fails, it may return 0, or it may return a special constant\nnamed INVALID HANDLE VALUE. A process can close any handle by calling the\nCloseHandle() function, and the system may delete the object if the count of",
  "handles referencing the object in all processes drops to zero.\n19.7.2\nSharing Objects between Processes\nWindows provides three ways to share objects between processes. The ﬁrst\nway is for a child process to inherit a handle to the object. When the parent\ncalls the CreateXXX function, the parent supplies a SECURITIES ATTRIBUTES\nstructure with the bInheritHandle ﬁeld set to TRUE. This ﬁeld creates an\ninheritable handle. Next, the child process is created, passing a value of TRUE\nto the CreateProcess() function’s bInheritHandle argument. Figure 19.8\nshows a code sample that creates a semaphore handle inherited by a child\nprocess.\nAssuming the child process knows which handles are shared, the parent\nand child can achieve interprocess communication through the shared objects.",
  "In the example in Figure 19.8, the child process gets the value of the handle\nfrom the ﬁrst command-line argument and then shares the semaphore with\nthe parent process.\nThe second way to share objects is for one process to give the object a\nname when the object is created and for the second process to open the name.\nThis method has two drawbacks: Windows does not provide a way to check\nwhether an object with the chosen name already exists, and the object name\nspace is global, without regard to the object type. For instance, two applications 19.7\nProgrammer Interface\n875\nSECURITY ATTRIBUTES sa;\nsa.nlength = sizeof(sa);\nsa.lpSecurityDescriptor = NULL;\nsa.bInheritHandle = TRUE;\nHandle a semaphore = CreateSemaphore(&sa, 1, 1, NULL);\nchar comand line[132];",
  "char comand line[132];\nostrstream ostring(command line, sizeof(command line));\nostring << a semaphore << ends;\nCreateProcess(\"another process.exe\", command line,\nNULL, NULL, TRUE, . . .);\nFigure 19.8\nCode enabling a child to share an object by inheriting a handle.\nmay create and share a single object named “foo” when two distinct objects—\npossibly of different types—were desired.\nNamed objects have the advantage that unrelated processes can readily\nshare them. The ﬁrst process calls one of the CreateXXX functions and supplies\na name as a parameter. The second process gets a handle to share the object\nby calling OpenXXX() (or CreateXXX) with the same name, as shown in the\nexample in Figure 19.9.\nThe third way to share objects is via the DuplicateHandle() function.",
  "This method requires some other method of interprocess communication to\npass the duplicated handle. Given a handle to a process and the value of a\nhandle within that process, a second process can get a handle to the same\nobject and thus share it. An example of this method is shown in Figure 19.10.\n19.7.3\nProcess Management\nIn Windows, a process is a loaded instance of an application and a thread is an\nexecutable unit of code that can be scheduled by the kernel dispatcher. Thus,\na process contains one or more threads. A process is created when a thread\nin some other process calls the CreateProcess() API. This routine loads\nany dynamic link libraries used by the process and creates an initial thread\nin the process. Additional threads can be created by the CreateThread()",
  "function. Each thread is created with its own stack, which defaults to 1 MB\nunless otherwise speciﬁed in an argument to CreateThread().\n// Process A\n. . .\nHANDLE a semaphore = CreateSemaphore(NULL, 1, 1, \"MySEM1\");\n. . .\n// Process B\n. . .\nHANDLE b semaphore = OpenSemaphore(SEMAPHORE ALL ACCESS,\nFALSE, \"MySEM1\");\n. . .\nFigure 19.9\nCode for sharing an object by name lookup. 876\nChapter 19\nWindows 7\n// Process A wants to give Process B access to a semaphore\n// Process A\nHANDLE a semaphore = CreateSemaphore(NULL, 1, 1, NULL);\n// send the value of the semaphore to Process B\n// using a message or shared memory object\n. . .\n// Process B\nHANDLE process a = OpenProcess(PROCESS ALL ACCESS, FALSE,\nprocess id of A);\nHANDLE b semaphore;\nDuplicateHandle(process a, a semaphore,",
  "DuplicateHandle(process a, a semaphore,\nGetCurrentProcess(), &b semaphore,\n0, FALSE, DUPLICATE SAME ACCESS);\n// use b semaphore to access the semaphore\n. . .\nFigure 19.10\nCode for sharing an object by passing a handle.\n19.7.3.1\nScheduling Rule\nPriorities in the Win32 environment are based on the native kernel (NT)\nscheduling model, but not all priority values may be chosen. The Win32 API\nuses four priority classes:\n1. IDLE PRIORITY CLASS (NT priority level 4)\n2. NORMAL PRIORITY CLASS (NT priority level 8)\n3. HIGH PRIORITY CLASS (NT priority level 13)\n4. REALTIME PRIORITY CLASS (NT priority level 24)\nProcesses are typically members of the NORMAL PRIORITY CLASS unless the\nparent of the process was of the IDLE PRIORITY CLASS or another class was",
  "speciﬁed when CreateProcess was called. The priority class of a process is\nthe default for all threads that execute in the process. It can be changed with\nthe SetPriorityClass() function or by passing an argument to the START\ncommand. Only users with the increase scheduling priority privilege can move\na process into the REALTIME PRIORITY CLASS. Administrators and power users\nhave this privilege by default.\nWhen a user is running an interactive process, the system needs to schedule\nthe process’s threads to provide good responsiveness. For this reason, Windows\nhas a special scheduling rule for processes in the NORMAL PRIORITY CLASS.\nWindows distinguishes between the process associated with the foreground\nwindow on the screen and the other (background) processes. When a process",
  "moves into the foreground, Windows increases the scheduling quantum for all\nits threads by a factor of 3; CPU-bound threads in the foreground process will\nrun three times longer than similar threads in background processes. 19.7\nProgrammer Interface\n877\n19.7.3.2\nThread Priorities\nA thread starts with an initial priority determined by its class. The priority\ncan be altered by the SetThreadPriority() function. This function takes an\nargument that speciﬁes a priority relative to the base priority of its class:\n• THREAD PRIORITY LOWEST: base −2\n• THREAD PRIORITY BELOW NORMAL: base −1\n• THREAD PRIORITY NORMAL: base + 0\n• THREAD PRIORITY ABOVE NORMAL: base + 1\n• THREAD PRIORITY HIGHEST: base + 2\nTwo other designations are also used to adjust the priority. Recall from",
  "Section 19.3.2.2 that the kernel has two priority classes: 16–31 for the real-\ntime class and 1–15 for the variable class. THREAD PRIORITY IDLE sets the\npriority to 16 for real-time threads and to 1 for variable-priority threads.\nTHREAD PRIORITY TIME CRITICAL sets the priority to 31 for real-time threads\nand to 15 for variable-priority threads.\nAs discussed in Section 19.3.2.2, the kernel adjusts the priority of a variable\nclass thread dynamically depending on whether the thread is I/O bound or\nCPU bound. The Win32 API provides a method to disable this adjustment via\nSetProcessPriorityBoost() and SetThreadPriorityBoost() functions.\n19.7.3.3\nThread Suspend and Resume\nA thread can be created in a suspended state or can be placed in a suspended",
  "state later by use of the SuspendThread() function. Before a suspended\nthread can be scheduled by the kernel dispatcher, it must be moved out of\nthe suspended state by use of the ResumeThread() function. Both functions\nset a counter so that if a thread is suspended twice, it must be resumed twice\nbefore it can run.\n19.7.3.4\nThread Synchronization\nTo synchronize concurrent access to shared objects by threads, the kernel pro-\nvides synchronization objects, such as semaphores and mutexes. These are dis-\npatcher objects, as discussed in Section 19.3.2.2. Threads can also synchronize\nwith kernel services operating on kernel objects—such as threads, processes,\nand ﬁles—because these are also dispatcher objects. Synchronization with ker-",
  "nel dispatcher objects can be achieved by use of the WaitForSingleObject()\nand WaitForMultipleObjects() functions; these functions wait for one or\nmore dispatcher objects to be signaled.\nAnother method of synchronization is available to threads within the same\nprocess that want to execute code exclusively. The Win32 critical section object\nis a user-mode mutex object that can often be acquired and released without\nentering the kernel. On a multiprocessor, a Win32 critical section will attempt\nto spin while waiting for a critical section held by another thread to be released.\nIf the spinning takes too long, the acquiring thread will allocate a kernel mutex\nand yield its CPU. Critical sections are particularly efﬁcient because the kernel",
  "mutex is allocated only when there is contention and then used only after 878\nChapter 19\nWindows 7\nattempting to spin. Most mutexes in programs are never actually contended,\nso the savings are signiﬁcant.\nBefore using a critical section, some thread in the process must call Ini-\ntializeCriticalSection(). Each thread that wants to acquire the mutex\ncalls EnterCriticalSection() and then later calls LeaveCriticalSec-\ntion() to release the mutex. There is also a TryEnterCriticalSection()\nfunction, which attempts to acquire the mutex without blocking.\nFor programs that want user-mode reader–writer locks rather than a\nmutex, Win32 supports slim reader–writer (SRW) locks. SRW locks have APIs\nsimilar to those for critical sections, such as InitializeSRWLock, AcquireS-",
  "RWLockXXX, and ReleaseSRWLockXXX, where XXX is either Exclusive or\nShared, depending on whether the thread wants write access or just read\naccess to the object protected by the lock. The Win32 API also supports condition\nvariables, which can be used with either critical sections or SRW locks.\n19.7.3.5\nThread Pool\nRepeatedly creating and deleting threads can be expensive for applications and\nservices that perform small amounts of work in each instantiation. The Win32\nthread pool provides user-mode programs with three services: a queue to\nwhich work requests may be submitted (via the SubmitThreadpoolWork()\nfunction), an API that can be used to bind callbacks to waitable handles\n(RegisterWaitForSingleObject()), and APIs to work with timers (Cre-",
  "ateThreadpoolTimer() and WaitForThreadpoolTimerCallbacks()) and\nto bind callbacks to I/O completion queues (BindIoCompletionCallback()).\nThe goal of using a thread pool is to increase performance and reduce\nmemory footprint. Threads are relatively expensive, and each processor can\nonly be executing one thread at a time no matter how many threads are\navailable. The thread pool attempts to reduce the number of runnable threads\nby slightly delaying work requests (reusing each thread for many requests)\nwhile providing enough threads to effectively utilize the machine’s CPUs. The\nwait and I/O- and timer-callback APIs allow the thread pool to further reduce\nthe number of threads in a process, using far fewer threads than would be",
  "necessary if a process were to devote separate threads to servicing each waitable\nhandle, timer, or completion port.\n19.7.3.6\nFibers\nA ﬁber is user-mode code that is scheduled according to a user-deﬁned\nscheduling algorithm. Fibers are completely a user-mode facility; the kernel\nis not aware that they exist. The ﬁber mechanism uses Windows threads as\nif they were CPUs to execute the ﬁbers. Fibers are cooperatively scheduled,\nmeaning that they are never preempted but must explicitly yield the thread\non which they are running. When a ﬁber yields a thread, another ﬁber can be\nscheduled on it by the run-time system (the programming language run-time\ncode).\nThe system creates a ﬁber by calling either ConvertThreadToFiber()",
  "or CreateFiber(). The primary difference between these functions is that\nCreateFiber() does not begin executing the ﬁber that was created. To begin\nexecution, the application must call SwitchToFiber(). The application can\nterminate a ﬁber by calling DeleteFiber(). 19.7\nProgrammer Interface\n879\nFibers are not recommended for threads that use Win32 APIs rather than\nstandard C-library functions because of potential incompatibilities. Win32 user-\nmode threads have a thread-environment block (TEB) that contains numerous\nper-thread ﬁelds used by the Win32 APIs. Fibers must share the TEB of the thread\non which they are running. This can lead to problems when a Win32 interface\nputs state information into the TEB for one ﬁber and then the information is",
  "overwritten by a different ﬁber. Fibers are included in the Win32 API to facilitate\nthe porting of legacy UNIX applications that were written for a user-mode\nthread model such as Pthreads.\n19.7.3.7\nUser-Mode Scheduling (UMS) and ConcRT\nA new mechanism in Windows 7, user-mode scheduling (UMS), addresses\nseveral limitations of ﬁbers. First, recall that ﬁbers are unreliable for executing\nWin32 APIs because they do not have their own TEBs. When a thread running\na ﬁber blocks in the kernel, the user scheduler loses control of the CPU for a\ntime as the kernel dispatcher takes over scheduling. Problems may result when\nﬁbers change the kernel state of a thread, such as the priority or impersonation\ntoken, or when they start asynchronous I/O.",
  "token, or when they start asynchronous I/O.\nUMS provides an alternative model by recognizing that each Windows\nthread is actually two threads: a kernel thread (KT) and a user thread (UT).\nEach type of thread has its own stack and its own set of saved registers. The\nKT and UT appear as a single thread to the programmer because UTs can\nnever block but must always enter the kernel, where an implicit switch to the\ncorresponding KT takes place. UMS uses each UT’s TEB to uniquely identify\nthe UT. When a UT enters the kernel, an explicit switch is made to the KT that\ncorresponds to the UT identiﬁed by the current TEB. The reason the kernel does\nnot know which UT is running is that UTs can invoke a user-mode scheduler,\nas ﬁbers do. But in UMS, the scheduler switches UTs, including switching the",
  "TEBs.\nWhen a UT enters the kernel, its KT may block. When this happens, the\nkernel switches to a scheduling thread, which UMS calls a primary, and uses\nthis thread to reenter the user-mode scheduler so that it can pick another UT\nto run. Eventually, a blocked KT will complete its operation and be ready to\nreturn to user mode. Since UMS has already reentered the user-mode scheduler\nto run a different UT, UMS queues the UT corresponding to the completed KT\nto a completion list in user mode. When the user-mode scheduler is choosing\na new UT to switch to, it can examine the completion list and treat any UT on\nthe list as a candidate for scheduling.\nUnlike ﬁbers, UMS is not intended to be used directly by the program-\nmer. The details of writing user-mode schedulers can be very challenging,",
  "and UMS does not include such a scheduler. Rather, the schedulers come\nfrom programming language libraries that build on top of UMS. Microsoft\nVisual Studio 2010 shipped with Concurrency Runtime (ConcRT), a concurrent\nprogramming framework for C++. ConcRT provides a user-mode scheduler\ntogether with facilities for decomposing programs into tasks, which can then\nbe scheduled on the available CPUs. ConcRT provides support for par for\nstyles of constructs, as well as rudimentary resource management and task\nsynchronization primitives. The key features of UMS are depicted in Figure\n19.11. 880\nChapter 19\nWindows 7\nNTOS executive\nOnly primary thread runs in user-mode\nTrap code switches to parked KT\nKT blocks ⇒ primary returns to user-mode\nKT unblocks & parks ⇒ queue UT completion\nThread parking",
  "Thread parking\nUT completion list\nkernel\nuser\nUser-mode\nscheduler\ntrap code\nPrimary\nthread\nKT0\nUT0\nUT1\nUT0\nKT1\nKT2\nKT0 blocks\nFigure 19.11\nUser-mode scheduling.\n19.7.3.8\nWinsock\nWinsock is the Windows sockets API. Winsock is a session-layer interface that is\nlargely compatible with UNIX sockets but has some added Windows extensions.\nIt provides a standardized interface to many transport protocols that may have\ndifferent addressing schemes, so that any Winsock application can run on\nany Winsock-compliant protocol stack. Winsock underwent a major update in\nWindows Vista to add tracing, IPv6 support, impersonation, new security APIs\nand many other features.\nWinsock follows the Windows Open System Architecture (WOSA) model,",
  "which provides a standard service provider interface (SPI) between applications\nand networking protocols. Applications can load and unload layered protocols\nthat build additional functionality, such as additional security, on top of the\ntransport protocol layers. Winsock supports asynchronous operations and\nnotiﬁcations, reliable multicasting, secure sockets, and kernel mode sockets.\nThere is also support for simpler usage models, like the WSAConnectByName()\nfunction, which accepts the target as strings specifying the name or IP address\nof the server and the service or port number of the destination port.\n19.7.4\nInterprocess Communication Using Windows Messaging\nWin32 applications handle interprocess communication in several ways. One",
  "way is by using shared kernel objects. Another is by using the Windows\nmessaging facility, an approach that is particularly popular for Win32 GUI\napplications. One thread can send a message to another thread or to a\nwindow by calling PostMessage(), PostThreadMessage(), SendMessage(),\nSendThreadMessage(), or SendMessageCallback(). Posting a message and\nsending a message differ in this way: the post routines are asynchronous; they\nreturn immediately, and the calling thread does not know when the message\nis actually delivered. The send routines are synchronous: they block the caller\nuntil the message has been delivered and processed. 19.7\nProgrammer Interface\n881\n// allocate 16 MB at the top of our address space\nvoid *buf = VirtualAlloc(0, 0x1000000, MEM RESERVE | MEM TOP DOWN,",
  "PAGE READWRITE);\n// commit the upper 8 MB of the allocated space\nVirtualAlloc(buf + 0x800000, 0x800000, MEM COMMIT, PAGE READWRITE);\n// do something with the memory\n. . .\n// now decommit the memory\nVirtualFree(buf + 0x800000, 0x800000, MEM DECOMMIT);\n// release all of the allocated address space\nVirtualFree(buf, 0, MEM RELEASE);\nFigure 19.12\nCode fragments for allocating virtual memory.\nIn addition to sending a message, a thread can send data with the message.\nSince processes have separate address spaces, the data must be copied. The\nsystem copies data by calling SendMessage() to send a message of type\nWM COPYDATA with a COPYDATASTRUCT data structure that contains the length\nand address of the data to be transferred. When the message is sent, Windows",
  "copies the data to a new block of memory and gives the virtual address of the\nnew block to the receiving process.\nEvery Win32 thread has its own input queue from which it receives\nmessages. If a Win32 application does not call GetMessage() to handle events\non its input queue, the queue ﬁlls up; and after about ﬁve seconds, the system\nmarks the application as “Not Responding”.\n19.7.5\nMemory Management\nThe Win32 API provides several ways for an application to use memory: virtual\nmemory, memory-mapped ﬁles, heaps, and thread-local storage.\n19.7.5.1\nVirtual Memory\nAn application calls VirtualAlloc() to reserve or commit virtual memory\nand VirtualFree() to decommit or release the memory. These functions\nenable the application to specify the virtual address at which the memory",
  "is allocated. They operate on multiples of the memory page size. Examples of\nthese functions appear in Figure 19.12.\nA process may lock some of its committed pages into physical memory\nby calling VirtualLock(). The maximum number of pages a process can lock\nis 30, unless the process ﬁrst calls SetProcessWorkingSetSize() to increase\nthe maximum working-set size.\n19.7.5.2\nMemory-Mapping Files\nAnother way for an application to use memory is by memory-mapping a ﬁle\ninto its address space. Memory mapping is also a convenient way for two\nprocesses to share memory: both processes map the same ﬁle into their virtual\nmemory. Memory mapping is a multistage process, as you can see in the\nexample in Figure 19.13. 882\nChapter 19\nWindows 7\n// open the file or create it if it does not exist",
  "HANDLE hfile = CreateFile(\"somefile\", GENERIC READ | GENERIC WRITE,\nFILE SHARE READ | FILE SHARE WRITE, NULL,\nOPEN ALWAYS, FILE ATTRIBUTE NORMAL, NULL);\n// create the file mapping 8 MB in size\nHANDLE hmap = CreateFileMapping(hfile, PAGE READWRITE,\nSEC COMMIT, 0, 0x800000, \"SHM 1\");\n// now get a view of the space mapped\nvoid *buf = MapViewOfFile(hmap, FILE MAP ALL ACCESS,\n0, 0, 0, 0x800000);\n// do something with the mapped file\n. . .\n// now unmap the file\nUnMapViewOfFile(buf);\nCloseHandle(hmap);\nCloseHandle(hfile);\nFigure 19.13\nCode fragments for memory mapping of a ﬁle.\nIf a process wants to map some address space just to share a memory region\nwith another process, no ﬁle is needed. The process calls CreateFileMap-",
  "ping() with a ﬁle handle of 0xffffffff and a particular size. The resulting\nﬁle-mapping object can be shared by inheritance, by name lookup, or by handle\nduplication.\n19.7.5.3\nHeaps\nHeaps provide a third way for applications to use memory, just as with\nmalloc() and free() in standard C. A heap in the Win32 environment is\na region of reserved address space. When a Win32 process is initialized, it is\ncreated with a default heap. Since most Win32 applications are multithreaded,\naccess to the heap is synchronized to protect the heap’s space-allocation data\nstructures from being damaged by concurrent updates by multiple threads.\nWin32 provides several heap-management functions so that a process can\nallocate and manage a private heap. These functions are HeapCreate(), Hea-",
  "pAlloc(), HeapRealloc(), HeapSize(), HeapFree(), and HeapDestroy().\nThe Win32 API also provides the HeapLock() and HeapUnlock() functions to\nenable a thread to gain exclusive access to a heap. Unlike VirtualLock(), these\nfunctions perform only synchronization; they do not lock pages into physical\nmemory.\nThe original Win32 heap was optimized for efﬁcient use of space. This led to\nsigniﬁcant problems with fragmentation of the address space for larger server\nprograms that ran for long periods of time. A new low-fragmentation heap\n(LFH) design introduced in Windows XP greatly reduced the fragmentation\nproblem. The Windows 7 heap manager automatically turns on LFH as\nappropriate.\n19.7.5.4\nThread-Local Storage\nA fourth way for applications to use memory is through a thread-local storage",
  "(TLS) mechanism. Functions that rely on global or static data typically fail Practice Exercises\n883\n// reserve a slot for a variable\nDWORD var index = T1sAlloc();\n// set it to the value 10\nT1sSetValue(var index, 10);\n// get the value\nint var T1sGetValue(var index);\n// release the index\nT1sFree(var index);\nFigure 19.14\nCode for dynamic thread-local storage.\nto work properly in a multithreaded environment. For instance, the C run-\ntime function strtok() uses a static variable to keep track of its current\npositionwhile parsingastring. Fortwoconcurrentthreadstoexecutestrtok()\ncorrectly, they need separate current position variables. TLS provides a way\nto maintain instances of variables that are global to the function being executed\nbut not shared with any other thread.",
  "but not shared with any other thread.\nTLS provides both dynamic and static methods of creating thread-local\nstorage. The dynamic method is illustrated in Figure 19.14. The TLS mechanism\nallocates global heap storage and attaches it to the thread environment block\nthat Windows allocates to every user-mode thread. The TEB is readily accessible\nby each thread and is used not just for TLS but for all the per-thread state\ninformation in user mode.\nTo use a thread-local static variable, the application declares the variable\nas follows to ensure that every thread has its own private copy:\ndeclspec(thread) DWORD cur pos = 0;\n19.8 Summary\nMicrosoft designed Windows to be an extensible, portable operating system\n—one able to take advantage of new techniques and hardware. Windows",
  "supports multiple operating environments and symmetric multiprocessing,\nincluding both 32-bit and 64-bit processors and NUMA computers. The use\nof kernel objects to provide basic services, along with support for client–\nserver computing, enables Windows to support a wide variety of applica-\ntion environments. Windows provides virtual memory, integrated caching,\nand preemptive scheduling. It supports elaborate security mechanisms and\nincludes internationalization features. Windows runs on a wide variety of\ncomputers, so users can choose and upgrade hardware to match their budgets\nand performance requirements without needing to alter the applications they\nrun.\nPractice Exercises\n19.1\nWhat type of operating system is Windows? Describe two of its major\nfeatures.\n19.2",
  "features.\n19.2\nList the design goals of Windows. Describe two in detail. 884\nChapter 19\nWindows 7\n19.3\nDescribe the booting process for a Windows system.\n19.4\nDescribe the three main architectural layers of the Windows kernel.\n19.5\nWhat is the job of the object manager?\n19.6\nWhat types of services does the process manager provide?\n19.7\nWhat is a local procedure call?\n19.8\nWhat are the responsibilities of the I/O manager?\n19.9\nWhat types of networking does Windows support? How does Windows\nimplement transport protocols? Describe two networking protocols.\n19.10\nHow is the NTFS namespace organized?\n19.11\nHow does NTFS handle data structures? How does NTFS recover from\na system crash? What is guaranteed after a recovery takes place? ‘\n19.12\nHow does Windows allocate user memory?\n19.13",
  "How does Windows allocate user memory?\n19.13\nDescribe some of the ways in which an application can use memory\nvia the Win32 API.\nExercises\n19.14\nUnder what circumstances would one use the deferred procedure calls\nfacility in Windows?\n19.15\nWhat is a handle, and how does a process obtain a handle?\n19.16\nDescribe the management scheme of the virtual memory manager. How\ndoes the VM manager improve performance?\n19.17\nDescribe a useful application of the no-access page facility provided in\nWindows.\n19.18\nDescribe the three techniques used for communicating data in a local\nprocedure call. What settings are most conducive to the application of\nthe different message-passing techniques?\n19.19\nWhat manages caching in Windows? How is the cache managed?\n19.20",
  "19.20\nHow does the NTFS directory structure differ from the directory\nstructure used in UNIX operating systems?\n19.21\nWhat is a process, and how is it managed in Windows?\n19.22\nWhat is the ﬁber abstraction provided by Windows? How does it differ\nfrom the thread abstraction?\n19.23\nHow does user-mode scheduling (UMS) in Windows 7 differ from\nﬁbers? What are some trade-offs between ﬁbers and UMS?\n19.24\nUMS considers a thread to have two parts, a UT and a KT. How might it\nbe useful to allow UTs to continue executing in parallel with their KTs?\n19.25\nWhat is the performance trade-off of allowing KTs and UTs to execute\non different processors? Bibliography\n885\n19.26\nWhy does the self-map occupy large amounts of virtual address space\nbut no additional virtual memory?\n19.27",
  "but no additional virtual memory?\n19.27\nHow does the self-map make it easy for the VM manager to move the\npage-table pages to and from disk? Where are the page-table pages\nkept on disk?\n19.28\nWhen a Windows system hibernates, the system is powered off.\nSuppose you changed the CPU or the amount of RAM on a hibernating\nsystem. Do you think that would work? Why or why not?\n19.29\nGive an example showing how the use of a suspend count is helpful in\nsuspending and resuming threads in Windows.\nBibliographical Notes\n[Russinovich and Solomon (2009)] give an overview of Windows 7 and\nconsiderable technical detail about system internals and components.\n[Brown (2000)] presents details of the security architecture of Windows.\nThe Microsoft Developer Network Library (http://msdn.microsoft.com)",
  "supplies a wealth of information on Windows and other Microsoft products,\nincluding documentation of all the published APIs.\n[Iseminger (2000)] provides a good reference on the Windows Active\nDirectory. Detailed discussions of writing programs that use the Win32 API\nappear in [Richter (1997)]. [Silberschatz et al. (2010)] supply a good discussion\nof B+ trees.\nThe source code for a 2005 WRK version of the Windows kernel, together\nwith a collection of slides and other CRK curriculum materials, is available from\nwww.microsoft.com/WindowsAcademic for use by universities.\nBibliography\n[Brown (2000)]\nK. Brown, Programming Windows Security, Addison-Wesley\n(2000).\n[Iseminger (2000)]\nD. Iseminger, Active Directory Services for Microsoft Windows\n2000. Technical Reference, Microsoft Press (2000).",
  "[Richter (1997)]\nJ. Richter, Advanced Windows, Microsoft Press (1997).\n[Russinovich and Solomon (2009)]\nM. E. Russinovich and D. A. Solomon, Win-\ndows Internals: Including Windows Server 2008 and Windows Vista, Fifth Edition,\nMicrosoft Press (2009).\n[Silberschatz et al. (2010)]\nA. Silberschatz, H. F. Korth, and S. Sudarshan,\nDatabase System Concepts, Sixth Edition, McGraw-Hill (2010).  20\nC H A P T E R\nInfluential\nOperating\nSystems\nNow that you understand the fundamental concepts of operating systems (CPU\nscheduling, memory management, processes, and so on), we are in a position\nto examine how these concepts have been applied in several older and highly\ninﬂuential operating systems. Some of them (such as the XDS-940 and the THE",
  "system) were one-of-a-kind systems; others (such as OS/360) are widely used.\nThe order of presentation highlights the similarities and differences of the\nsystems; it is not strictly chronological or ordered by importance. The serious\nstudent of operating systems should be familiar with all these systems.\nIn the bibliographical notes at the end of the chapter, we include references\nto further reading about these early systems. The papers, written by the\ndesigners of the systems, are important both for their technical content and\nfor their style and ﬂavor.\nCHAPTER OBJECTIVES\n• To explain how operating-system features migrate over time from large\ncomputer systems to smaller ones.\n• To discuss the features of several historically important operating systems.\n20.1 Feature Migration",
  "20.1 Feature Migration\nOne reason to study early architectures and operating systems is that a feature\nthat once ran only on huge systems may eventually make its way into very\nsmall systems. Indeed, an examination of operating systems for mainframes\nand microcomputers shows that many features once available only on main-\nframes have been adopted for microcomputers. The same operating-system\nconcepts are thus appropriate for various classes of computers: mainframes,\nminicomputers, microcomputers, and handhelds. To understand modern oper-\nating systems, then, you need to recognize the theme of feature migration and\nthe long history of many operating-system features, as shown in Figure 20.1.\nA good example of feature migration started with the Multiplexed Infor-",
  "mation and Computing Services (MULTICS) operating system. MULTICS was\n887 888\nChapter 20\nInﬂuential Operating Systems\nmainframes\n1950\nno\nsoftware\nno\nsoftware\nr\no\ns\ns\ne\nc\no\nr\npitlu\nm\nh\nct\na\nb\ncompilers\ntime\nshared\ndistributed\nsystems\nresident\nmonitors\nfault tolerant\nnetworked\nmultiuser\nno\nsoftware\ncompilers\nno\nsoftware\ninteractive\ncompilers\ncompilers\ninteractive\nnetworked\ntime\nshared\nresident\nmonitors\nfault tolerant\nmultiuser\nnetworked\nclustered\nmultiuser\nmultiprocessor\nmultiprocessor\n1960\n1970\nMULTICS\n1980\n1990\n2000 \nminicomputers\ndesktop computers\nhandheld computers\nUNIX\nUNIX\nnetworked\nUNIX\nsmart phones\n2010\nLINUX\nmultiprocessor\nnetworked\ninteractive\nLINUX\nFigure 20.1\nMigration of operating-system concepts and features.",
  "developed from 1965 to 1970 at the Massachusetts Institute of Technology (MIT)\nas a computing utility. It ran on a large, complex mainframe computer (the GE\n645). Many of the ideas that were developed for MULTICS were subsequently\nused at Bell Laboratories (one of the original partners in the development of\nMULTICS) in the design of UNIX. The UNIX operating system was designed\naround 1970 for a PDP-11 minicomputer. Around 1980, the features of UNIX\nbecame the basisfor UNIX-like operatingsystemsonmicrocomputers; and these\nfeatures are included in several more recent operating systems for microcom-\nputers, such as Microsoft Windows, Windows XP, and the Mac OS X operating\nsystem. Linux includes some of these same features, and they can now be found\non PDAs.\n20.2 Early Systems",
  "on PDAs.\n20.2 Early Systems\nWe turn our attention now to a historical overview of early computer systems.\nWe should note that the history of computing starts far before “computers” with\nlooms and calculators. We begin our discussion, however, with the computers\nof the twentieth century.\nBefore the 1940s, computing devices were designed and implemented to\nperform speciﬁc, ﬁxed tasks. Modifying one of those tasks required a great deal\nof effort and manual labor. All that changed in the 1940s when Alan Turing and\nJohn von Neumann (and colleagues), both separately and together, worked on\nthe idea of a more general-purpose stored program computer. Such a machine 20.2\nEarly Systems\n889\nhas both a program store and a data store, where the program store provides",
  "instructions about what to do to the data.\nThis fundamental computer concept quickly generated a number of\ngeneral-purpose computers, but much of the history of these machines is\nblurred by time and the secrecy of their development during World War II. It\nis likely that the ﬁrst working stored-program general-purpose computer was\nthe Manchester Mark 1, which ran successfully in 1949. The ﬁrst commercial\ncomputer— the Ferranti Mark 1, which went on sale in 1951—was it offspring.\nEarly computers were physically enormous machines run from consoles.\nThe programmer, who was also the operator of the computer system, would\nwrite a program and then would operate the program directly from the\noperator’s console. First, the program would be loaded manually into memory",
  "from the front panel switches (one instruction at a time), from paper tape, or\nfrom punched cards. Then the appropriate buttons would be pushed to set the\nstarting address and to start the execution of the program. As the program ran,\nthe programmer/operator could monitor its execution by the display lights on\nthe console. If errors were discovered, the programmer could halt the program,\nexamine the contents of memory and registers, and debug the program directly\nfrom the console. Output was printed or was punched onto paper tape or cards\nfor later printing.\n20.2.1\nDedicated Computer Systems\nAs time went on, additional software and hardware were developed. Card\nreaders, line printers, and magnetic tape became commonplace. Assemblers,",
  "loaders, and linkers were designed to ease the programming task. Libraries\nof common functions were created. Common functions could then be copied\ninto a new program without having to be written again, providing software\nreusability.\nThe routines that performed I/O were especially important. Each new I/O\ndevice had its own characteristics, requiring careful programming. A special\nsubroutine—called a device driver—was written for each I/O device. A device\ndriver knows how the buffers, ﬂags, registers, control bits, and status bits for\na particular device should be used. Each type of device has its own driver.\nA simple task, such as reading a character from a paper-tape reader, might\ninvolve complex sequences of device-speciﬁc operations. Rather than writing",
  "the necessary code every time, the device driver was simply used from the\nlibrary.\nLater, compilers for FORTRAN, COBOL, and other languages appeared,\nmaking the programming task much easier but the operation of the computer\nmore complex. To prepare a FORTRAN program for execution, for example,\nthe programmer would ﬁrst need to load the FORTRAN compiler into the\ncomputer. The compiler was normally kept on magnetic tape, so the proper\ntape would need to be mounted on a tape drive. The program would be read\nthrough the card reader and written onto another tape. The FORTRAN compiler\nproduced assembly-language output, which then had to be assembled. This\nprocedure required mounting another tape with the assembler. The output of",
  "the assembler would need to be linked to supporting library routines. Finally,\nthe binary object form of the program would be ready to execute. It could be\nloaded into memory and debugged from the console, as before. 890\nChapter 20\nInﬂuential Operating Systems\nA signiﬁcant amount of setup time could be involved in the running of a\njob. Each job consisted of many separate steps:\n1. Loading the FORTRAN compiler tape\n2. Running the compiler\n3. Unloading the compiler tape\n4. Loading the assembler tape\n5. Running the assembler\n6. Unloading the assembler tape\n7. Loading the object program\n8. Running the object program\nIf an error occurred during any step, the programmer/operator might have\nto start over at the beginning. Each job step might involve the loading and",
  "unloading of magnetic tapes, paper tapes, and punch cards.\nThe job setup time was a real problem. While tapes were being mounted or\nthe programmer was operating the console, the CPU sat idle. Remember that,\nin the early days, few computers were available, and they were expensive. A\ncomputer might have cost millions of dollars, not including the operational\ncosts of power, cooling, programmers, and so on. Thus, computer time was\nextremely valuable, and owners wanted their computers to be used as much\nas possible. They needed high utilization to get as much as they could from\ntheir investments.\n20.2.2\nShared Computer Systems\nThe solution was twofold. First, a professional computer operator was hired.\nThe programmer no longer operated the machine. As soon as one job was",
  "ﬁnished, the operator could start the next. Since the operator had more\nexperience with mounting tapes than a programmer, setup time was reduced.\nThe programmer provided whatever cards or tapes were needed, as well as a\nshort description of how the job was to be run. Of course, the operator could\nnot debug an incorrect program at the console, since the operator would not\nunderstand the program. Therefore, in the case of program error, a dump of\nmemory and registers was taken, and the programmer had to debug from the\ndump. Dumping the memory and registers allowed the operator to continue\nimmediately with the next job but left the programmer with the more difﬁcult\ndebugging problem.\nSecond, jobs with similar needs were batched together and run through the",
  "computer as a group to reduce setup time. For instance, suppose the operator\nreceived one FORTRAN job, one COBOL job, and another FORTRAN job. If she ran\nthem in that order, she would have to set up for FORTRAN (load the compiler\ntapes and so on), then set up for COBOL, and then set up for FORTRAN again. If\nshe ran the two FORTRAN programs as a batch, however, she could setup only\nonce for FORTRAN, saving operator time. 20.2\nEarly Systems\n891\nloader\njob sequencing\ncontrol card\ninterpreter\nuser \nprogram \narea\nmonitor\nFigure 20.2\nMemory layout for a resident monitor.\nBut there were still problems. For example, when a job stopped, the\noperator would have to notice that it had stopped (by observing the console),\ndetermine why it stopped (normal or abnormal termination), dump memory",
  "and register (if necessary), load the appropriate device with the next job, and\nrestart the computer. During this transition from one job to the next, the CPU\nsat idle.\nTo overcome this idle time, people developed automatic job sequencing.\nWith this technique, the ﬁrst rudimentary operating systems were created.\nA small program, called a resident monitor, was created to transfer control\nautomatically from one job to the next (Figure 20.2). The resident monitor is\nalways in memory (or resident).\nWhen the computer was turned on, the resident monitor was invoked,\nand it would transfer control to a program. When the program terminated, it\nwould return control to the resident monitor, which would then go on to the\nnext program. Thus, the resident monitor would automatically sequence from",
  "one program to another and from one job to another.\nBut how would the resident monitor know which program to execute?\nPreviously, the operator had been given a short description of what programs\nwere to be run on what data. Control cards were introduced to provide this\ninformation directly to the monitor. The idea is simple. In addition to the\nprogram or data for a job, the programmer supplied control cards, which\ncontained directives to the resident monitor indicating what program to run.\nFor example, a normal user program might require one of three programs to\nrun: the FORTRAN compiler (FTN), the assembler (ASM), or the user’s program\n(RUN). We could use a separate control card for each of these:\n$FTN—Execute the FORTRAN compiler.\n$ASM—Execute the assembler.",
  "$ASM—Execute the assembler.\n$RUN—Execute the user program.\nThese cards tell the resident monitor which program to run. 892\nChapter 20\nInﬂuential Operating Systems\nWe can use two additional control cards to deﬁne the boundaries of each\njob:\n$JOB—First card of a job\n$END—Final card of a job\nThese two cards might be useful in accounting for the machine resources used\nby the programmer. Parameters can be used to deﬁne the job name, account\nnumber to be charged, and so on. Other control cards can be deﬁned for other\nfunctions, such as asking the operator to load or unload a tape.\nOne problem with control cards is how to distinguish them from data or\nprogram cards. The usual solution is to identify them by a special character or",
  "pattern on the card. Several systems used the dollar-sign character ($) in the\nﬁrst column to identify a control card. Others used a different code. IBM’s Job\nControl Language (JCL) used slash marks (//) in the ﬁrst two columns. Figure\n20.3 shows a sample card-deck setup for a simple batch system.\nA resident monitor thus has several identiﬁable parts:\n• The control-card interpreter is responsible for reading and carrying out\nthe instructions on the cards at the point of execution.\n• The loader is invoked by the control-card interpreter to load system\nprograms and application programs into memory at intervals.\n• The device drivers are used by both the control-card interpreter and the\nloader for the system’s I/O devices. Often, the system and application",
  "programs are linked to these same device drivers, providing continuity in\ntheir operation, as well as saving memory space and programming time.\nThese batch systems work fairly well. The resident monitor provides\nautomatic job sequencing as indicated by the control cards. When a control\ncard indicates that a program is to be run, the monitor loads the program\ninto memory and transfers control to it. When the program completes, it\n$END\n$RUN\ndata for program\n$LOAD\n$FTN\n$JOB\nprogram to be compiled\nFigure 20.3\nCard deck for a simple batch system. 20.2\nEarly Systems\n893\ntransfers control back to the monitor, which reads the next control card, loads\nthe appropriate program, and so on. This cycle is repeated until all control",
  "cards are interpreted for the job. Then the monitor automatically continues\nwith the next job.\nThe switch to batch systems with automatic job sequencing was made\nto improve performance. The problem, quite simply, is that humans are\nconsiderably slower than computers. Consequently, it is desirable to replace\nhuman operation with operating-system software. Automatic job sequencing\neliminates the need for human setup time and job sequencing.\nEven with this arrangement, however, the CPU is often idle. The problem\nis the speed of the mechanical I/O devices, which are intrinsically slower\nthan electronic devices. Even a slow CPU works in the microsecond range,\nwith thousands of instructions executed per second. A fast card reader, in",
  "contrast, might read 1,200 cards per minute (or 20 cards per second). Thus, the\ndifference in speed between the CPU and its I/O devices may be three orders of\nmagnitude or more. Over time, of course, improvements in technology resulted\nin faster I/O devices. Unfortunately, CPU speeds increased even faster, so that\nthe problem was not only unresolved but also exacerbated.\n20.2.3\nOverlapped I/O\nOne common solution to the I/O problem was to replace slow card readers\n(input devices) and line printers (output devices) with magnetic-tape units.\nMost computer systems in the late 1950s and early 1960s were batch systems\nreading from card readers and writing to line printers or card punches. The CPU\ndid not read directly from cards, however; instead, the cards were ﬁrst copied",
  "onto a magnetic tape via a separate device. When the tape was sufﬁciently full,\nit was taken down and carried over to the computer. When a card was needed\nfor input to a program, the equivalent record was read from the tape. Similarly,\noutput was written to the tape, and the contents of the tape were printed later.\nThe card readers and line printers were operated off-line, rather than by the\nmain computer (Figure 20.4).\nAn obvious advantage of off-line operation was that the main computer\nwas no longer constrained by the speed of the card readers and line printers\nbut was limited only by the speed of the much faster magnetic tape units.\n(b)\n(a)\nCPU\ncard reader\ncard reader\nline printer\ntape drives\ntape drives\nline printer\nCPU\non-line\non-line\nFigure 20.4",
  "line printer\nCPU\non-line\non-line\nFigure 20.4\nOperation of I/O devices (a) on-line and (b) off-line. 894\nChapter 20\nInﬂuential Operating Systems\nThe technique of using magnetic tape for all I/O could be applied with any\nsimilar equipment (such as card readers, card punches, plotters, paper tape,\nand printers).\nThe real gain in off-line operation comes from the possibility of using\nmultiple reader-to-tape and tape-to-printer systems for one CPU. If the CPU\ncan process input twice as fast as the reader can read cards, then two readers\nworking simultaneously can produce enough tape to keep the CPU busy. There\nis a disadvantage, too, however—a longer delay in getting a particular job run.\nThe job must ﬁrst be read onto tape. Then it must wait until enough additional",
  "jobs are read onto the tape to “ﬁll” it. The tape must then be rewound, unloaded,\nhand-carried to the CPU, and mounted on a free tape drive. This process is not\nunreasonable for batch systems, of course. Many similar jobs can be batched\nonto a tape before it is taken to the computer.\nAlthough off-line preparation of jobs continued for some time, it was\nquickly replaced in most systems. Disk systems became widely available and\ngreatly improved on off-line operation. One problem with tape systems was\nthat the card reader could not write onto one end of the tape while the CPU\nread from the other. The entire tape had to be written before it was rewound\nand read, because tapes are by nature sequential-access devices. Disk systems",
  "eliminated this problem by being random-access devices. Because the head is\nmoved from one area of the disk to another, it can switch rapidly from the area\non the disk being used by the card reader to store new cards to the position\nneeded by the CPU to read the “next” card.\nIn a disk system, cards are read directly from the card reader onto the\ndisk. The location of card images is recorded in a table kept by the operating\nsystem. When a job is executed, the operating system satisﬁes its requests for\ncard-reader input by reading from the disk. Similarly, when the job requests the\nprinter to output a line, that line is copied into a system buffer and is written\nto the disk. When the job is completed, the output is actually printed. This",
  "form of processing is called spooling (Figure 20.5); the name is an acronym for\nsimultaneous peripheral operation on-line. Spooling, in essence, uses the disk\nCPU\ncard reader\nline printer\ndisk\nI/O\non-line\nFigure 20.5\nSpooling. 20.3\nAtlas\n895\nas a huge buffer for reading as far ahead as possible on input devices and for\nstoring output ﬁles until the output devices are able to accept them.\nSpooling is also used for processing data at remote sites. The CPU sends\nthe data via communication paths to a remote printer (or accepts an entire\ninput job from a remote card reader). The remote processing is done at its own\nspeed, with no CPU intervention. The CPU just needs to be notiﬁed when the\nprocessing is completed, so that it can spool the next batch of data.",
  "Spooling overlaps the I/O of one job with the computation of other jobs.\nEven in a simple system, the spooler may be reading the input of one job while\nprinting the output of a different job. During this time, still another job (or\nother jobs) may be executed, reading its “cards” from disk and “printing” its\noutput lines onto the disk.\nSpooling has a direct beneﬁcial effect on the performance of the system.\nFor the cost of some disk space and a few tables, the computation of one job\nand the I/O of other jobs can take place at the same time. Thus, spooling can\nkeep both the CPU and the I/O devices working at much higher rates. Spooling\nleads naturally to multiprogramming, which is the foundation of all modern\noperating systems.\n20.3 Atlas",
  "operating systems.\n20.3 Atlas\nThe Atlas operating system was designed at the University of Manchester in\nEngland in the late 1950s and early 1960s. Many of its basic features that were\nnovel at the time have become standard parts of modern operating systems.\nDevice drivers were a major part of the system. In addition, system calls were\nadded by a set of special instructions called extra codes.\nAtlas was a batch operating system with spooling. Spooling allowed the\nsystem to schedule jobs according to the availability of peripheral devices, such\nas magnetic tape units, paper tape readers, paper tape punches, line printers,\ncard readers, and card punches.\nThe most remarkable feature of Atlas, however, was its memory manage-\nment. Core memory was new and expensive at the time. Many computers,",
  "like the IBM 650, used a drum for primary memory. The Atlas system used a\ndrum for its main memory, but it had a small amount of core memory that was\nused as a cache for the drum. Demand paging was used to transfer information\nbetween core memory and the drum automatically.\nThe Atlas system used a British computer with 48-bit words. Addresses\nwere 24 bits but were encoded in decimal, which allowed 1 million words to\nbe addressed. At that time, this was an extremely large address space. The\nphysical memory for Atlas was a 98-KB-word drum and 16-KB words of core.\nMemory was divided into 512-word pages, providing 32 frames in physical\nmemory. An associative memory of 32 registers implemented the mapping\nfrom a virtual address to a physical address.",
  "from a virtual address to a physical address.\nIf a page fault occurred, a page-replacement algorithm was invoked. One\nmemory frame was always kept empty, so that a drum transfer could start\nimmediately. The page-replacement algorithm attempted to predict future\nmemory-accessing behavior based on past behavior. A reference bit for each\nframe was set whenever the frame was accessed. The reference bits were read 896\nChapter 20\nInﬂuential Operating Systems\ninto memory every 1,024 instructions, and the last 32 values of these bits were\nretained. This history was used to deﬁne the time since the most recent reference\n(t1) and the interval between the last two references (t2). Pages were chosen for\nreplacement in the following order:",
  "replacement in the following order:\n1. Any page with t1 > t2 + 1 is considered to be no longer in use and is\nreplaced.\n2. If t1 ≤t2 for all pages, then replace the page with the largest value for t2\n−t1.\nThe page-replacement algorithm assumes that programs access memory in\nloops. If the time between the last two references is t2, then another reference is\nexpected t2 time units later. If a reference does not occur (t1 > t2), it is assumed\nthat the page is no longer being used, and the page is replaced. If all pages\nare still in use, then the page that will not be needed for the longest time is\nreplaced. The time to the next reference is expected to be t2 −t1.\n20.4 XDS-940\nThe XDS-940 operating system was designed at the University of California at",
  "Berkeley in the early 1960’s. Like the Atlas system, it used paging for memory\nmanagement. Unlike the Atlas system, it was a time-shared system. The paging\nwas used only for relocation; it was not used for demand paging. The virtual\nmemory of any user process was made up of 16-KB words, whereas the physical\nmemory was made up of 64-KB words. Each page was made up of 2-KB words.\nThe page table was kept in registers. Since physical memory was larger than\nvirtual memory, several user processes could be in memory at the same time.\nThe number of users could be increased by page sharing when the pages\ncontained read-only reentrant code. Processes were kept on a drum and were\nswapped in and out of memory as necessary.\nThe XDS-940 system was constructed from a modiﬁed XDS-930. The mod-",
  "iﬁcations were typical of the changes made to a basic computer to allow an\noperating system to be written properly. A user-monitor mode was added.\nCertain instructions, such as I/O and halt, were deﬁned to be privileged. An\nattempt to execute a privileged instruction in user mode would trap to the\noperating system.\nA system-call instruction was added to the user-mode instruction set.\nThis instruction was used to create new resources, such as ﬁles, allowing the\noperating system to manage the physical resources. Files, for example, were\nallocated in 256-word blocks on the drum. A bit map was used to manage\nfree drum blocks. Each ﬁle had an index block with pointers to the actual data\nblocks. Index blocks were chained together.",
  "blocks. Index blocks were chained together.\nThe XDS-940 system also provided system calls to allow processes to create,\nstart, suspend, and destroy subprocesses. A programmer could construct a\nsystem of processes. Separate processes could share memory for communica-\ntion and synchronization. Process creation deﬁned a tree structure, where a\nprocess is the root and its subprocesses are nodes below it in the tree. Each of\nthe subprocesses could, in turn, create more subprocesses. 20.6\nRC 4000\n897\n20.5 THE\nThe THE operating system was designed at the Technische Hogeschool in\nEindhoven in the Netherlands in the mid-1960’s. It was a batch system running\non a Dutch computer, the EL X8, with 32 KB of 27-bit words. The system was",
  "mainly noted for its clean design, particularly its layer structure, and its use of\na set of concurrent processes employing semaphores for synchronization.\nUnlike the processes in the XDS-940 system, the set of processes in the\nTHE system was static. The operating system itself was designed as a set of\ncooperating processes. In addition, ﬁve user processes were created that served\nas the active agents to compile, execute, and print user programs. When one\njob was ﬁnished, the process would return to the input queue to select another\njob.\nA priority CPU-scheduling algorithm was used. The priorities were recom-\nputed every 2 seconds and were inversely proportional to the amount of CPU\ntime used recently (in the last 8 to 10 seconds). This scheme gave higher priority",
  "to I/O-bound processes and to new processes.\nMemory management was limited by the lack of hardware support. How-\never, since the system was limited and user programs could be written only in\nAlgol, a software paging scheme was used. The Algol compiler automatically\ngenerated calls to system routines, which made sure the requested information\nwas in memory, swapping if necessary. The backing store was a 512-KB-word\ndrum. A 512-word page was used, with an LRU page-replacement strategy.\nAnother major concern of the THE system was deadlock control. The\nbanker’s algorithm was used to provide deadlock avoidance.\nClosely related to the THE system is the Venus system. The Venus system\nwas also a layer-structured design, using semaphores to synchronize processes.",
  "The lower levels of the design were implemented in microcode, however,\nproviding a much faster system. Paged-segmented memory was used for\nmemory management. In addition, the system was designed as a time-sharing\nsystem, rather than a batch system.\n20.6 RC 4000\nThe RC 4000 system, like the THE system, was notable primarily for its design\nconcepts. It was designed in the late 1960’s for the Danish 4000 computer\nby Regnecentralen, particularly by Brinch-Hansen. The objective was not to\ndesign a batch system, or a time-sharing system, or any other speciﬁc system.\nRather, the goal was to create an operating-system nucleus, or kernel, on which\na complete operating system could be built. Thus, the system structure was\nlayered, and only the lower levels—comprising the kernel—were provided.",
  "The kernel supported a collection of concurrent processes. A round-robin\nCPU scheduler was used. Although processes could share memory, the primary\ncommunication and synchronization mechanism was the message system\nprovided by the kernel. Processes could communicate with each other by\nexchanging ﬁxed-sized messages of eight words in length. All messages were\nstored in buffers from a common buffer pool. When a message buffer was no\nlonger required, it was returned to the common pool. 898\nChapter 20\nInﬂuential Operating Systems\nA message queue was associated with each process. It contained all the\nmessages that had been sent to that process but had not yet been received.\nMessages were removed from the queue in FIFO order. The system supported",
  "four primitive operations, which were executed atomically:\n• send-message (in receiver, in message, out buffer)\n• wait-message (out sender, out message, out buffer)\n• send-answer (out result, in message, in buffer)\n• wait-answer (out result, out message, in buffer)\nThe last two operations allowed processes to exchange several messages at a\ntime.\nThese primitives required that a process service its message queue in\nFIFO order and that it block itself while other processes were handling its\nmessages. To remove these restrictions, the developers provided two additional\ncommunication primitives that allowed a process to wait for the arrival of the\nnext message or to answer and service its queue in any order:\n• wait-event (in previous-buffer, out next-buffer, out result)",
  "• get-event (out buffer)\nI/O devices were also treated as processes. The device drivers were code\nthat converted the device interrupts and registers into messages. Thus, a\nprocess would write to a terminal by sending that terminal a message. The\ndevice driver would receive the message and output the character to the\nterminal. An input character would interrupt the system and transfer to\na device driver. The device driver would create a message from the input\ncharacter and send it to a waiting process.\n20.7 CTSS\nThe Compatible Time-Sharing System (CTSS) was designed at MIT as an experi-\nmental time-sharing system and ﬁrst appeared in 1961. It was implemented on\nan IBM 7090 and eventually supported up to 32 interactive users. The users were",
  "provided with a set of interactive commands that allowed them to manipulate\nﬁles and to compile and run programs through a terminal.\nThe 7090 had a 32-KB memory made up of 36-bit words. The monitor used\n5 KB words, leaving 27 KB for the users. User memory images were swapped\nbetween memory and a fast drum. CPU scheduling employed a multilevel-\nfeedback-queue algorithm. The time quantum for level i was 2 ∗i time units.\nIf a program did not ﬁnish its CPU burst in one time quantum, it was moved\ndown to the next level of the queue, giving it twice as much time. The program\nat the highest level (with the shortest quantum) was run ﬁrst. The initial level\nof a program was determined by its size, so that the time quantum was at least\nas long as the swap time.",
  "as long as the swap time.\nCTSS was extremely successful and was in use as late as 1972. Although\nit was limited, it succeeded in demonstrating that time sharing was a con- 20.9\nIBM OS/360\n899\nvenient and practical mode of computing. One result of CTSS was increased\ndevelopment of time-sharing systems. Another result was the development of\nMULTICS.\n20.8 MULTICS\nThe MULTICS operating system was designed from 1965 to 1970 at MIT as a\nnatural extension of CTSS. CTSS and other early time-sharing systems were so\nsuccessful that they created an immediate desire to proceed quickly to bigger\nand better systems. As larger computers became available, the designers of\nCTSS set out to create a time-sharing utility. Computing service would be",
  "provided like electrical power. Large computer systems would be connected\nby telephone wires to terminals in ofﬁces and homes throughout a city. The\noperating system would be a time-shared system running continuously with a\nvast ﬁle system of shared programs and data.\nMULTICS was designed by a team from MIT, GE (which later sold its\ncomputer department to Honeywell), and Bell Laboratories (which dropped\nout of the project in 1969). The basic GE 635 computer was modiﬁed to a\nnew computer system called the GE 645, mainly by the addition of paged-\nsegmentation memory hardware.\nIn MULTICS, a virtual address was composed of an 18-bit segment number\nand a 16-bit word offset. The segments were then paged in 1-KB-word pages.\nThe second-chance page-replacement algorithm was used.",
  "The segmented virtual address space was merged into the ﬁle system; each\nsegment was a ﬁle. Segments were addressed by the name of the ﬁle. The ﬁle\nsystem itself was a multilevel tree structure, allowing users to create their own\nsubdirectory structures.\nLike CTSS, MULTICS used a multilevel feedback queue for CPU scheduling.\nProtection was accomplished through an access list associated with each ﬁle\nand a set of protection rings for executing processes. The system, which was\nwritten almost entirely in PL/1, comprised about 300,000 lines of code. It was\nextended to a multiprocessor system, allowing a CPU to be taken out of service\nfor maintenance while the system continued running.\n20.9 IBM OS/360\nThe longest line of operating-system development is undoubtedly that of IBM",
  "computers. The early IBM computers, such as the IBM 7090 and the IBM 7094, are\nprime examples of the development of common I/O subroutines, followed by\ndevelopmentofaresidentmonitor, privileged instructions, memory protection,\nand simple batch processing. These systems were developed separately, often\nat independent sites. As a result, IBM was faced with many different computers,\nwith different languages and different system software.\nThe IBM/360 —which ﬁrst appeared in the mid 1960’s — was designed to\nalter this situation. The IBM/360 ([Mealy et al. (1966)]) was designed as a family\nof computers spanning the complete range from small business machines to\nlarge scientiﬁc machines. Only one set of software would be needed for these",
  "systems, which all used the same operating system: OS/360. This arrangement 900\nChapter 20\nInﬂuential Operating Systems\nwas intended to reduce maintenance problems for IBM and to allow users to\nmove programs and applications freely from one IBM system to another.\nUnfortunately, OS/360 tried to be all things to all people. As a result, it\ndid none of its tasks especially well. The ﬁle system included a type ﬁeld\nthat deﬁned the type of each ﬁle, and different ﬁle types were deﬁned for\nﬁxed-length and variable-length records and for blocked and unblocked ﬁles.\nContiguous allocation was used, so the user had to guess the size of each output\nﬁle. The Job Control Language (JCL) added parameters for every possible\noption, making it incomprehensible to the average user.",
  "The memory-management routines were hampered by the architecture.\nAlthough a base-register addressing mode was used, the program could access\nand modify the base register, so that absolute addresses were generated by the\nCPU. This arrangement prevented dynamic relocation; the program was bound\nto physical memory at load time. Two separate versions of the operating system\nwere produced: OS/MFT used ﬁxed regions and OS/MVT used variable regions.\nThe system was written in assembly language by thousands of program-\nmers, resulting in millions of lines of code. The operating system itself required\nlarge amounts of memory for its code and tables. Operating-system overhead\noften consumed one-half of the total CPU cycles. Over the years, new versions",
  "were released to add new features and to ﬁx errors. However, ﬁxing one error\noften caused another in some remote part of the system, so that the number of\nknown errors in the system remained fairly constant.\nVirtual memory was added to OS/360 with the change to the IBM/370\narchitecture. The underlying hardware provided a segmented-paged virtual\nmemory. New versions of OS used this hardware in different ways. OS/VS1\ncreated one large virtual address space and ran OS/MFT in that virtual memory.\nThus, the operating system itself was paged, as well as user programs. OS/VS2\nRelease 1 ran OS/MVT in virtual memory. Finally, OS/VS2 Release 2, which is\nnow called MVS, provided each user with his own virtual memory.\nMVS is still basically a batch operating system. The CTSS system was run",
  "on an IBM 7094, but the developers at MIT decided that the address space of the\n360, IBM’s successor to the 7094, was too small for MULTICS, so they switched\nvendors. IBM then decided to create its own time-sharing system, TSS/360. Like\nMULTICS, TSS/360 was supposed to be a large, time-shared utility. The basic 360\narchitecture was modiﬁed in the model 67 to provide virtual memory. Several\nsites purchased the 360/67 in anticipation of TSS/360.\nTSS/360 was delayed, however, so other time-sharing systems were devel-\noped as temporary systems until TSS/360 was available. A time-sharing option\n(TSO) was added to OS/360. IBM’s Cambridge Scientiﬁc Center developed CMS\nas a single-user system and CP/67 to provide a virtual machine to run it on.",
  "When TSS/360 was eventually delivered, it was a failure. It was too large\nand too slow. As a result, no site would switch from its temporary system to\nTSS/360. Today, time sharing on IBM systems is largely provided either by TSO\nunder MVS or by CMS under CP/67 (renamed VM).\nNeither TSS/360 nor MULTICS achieved commercial success. What went\nwrong? Part of the problem was that these advanced systems were too large\nand too complex to be understood. Another problem was the assumption\nthat computing power would be available from a large, remote source. 20.11\nCP/M and MS/DOS\n901\nMinicomputers came along and decreased the need for large monolithic\nsystems. They were followed by workstations and then personal computers,\nwhich put computing power closer and closer to the end users.\n20.10 TOPS-20",
  "20.10 TOPS-20\nDEC created many inﬂuential computer systems during its history. Probably\nthe most famous operating system associated with DEC is VMS, a popular\nbusiness-oriented system that is still in use today as OpenVMS, a product of\nHewlett-Packard. But perhaps the most inﬂuential of DEC’s operating systems\nwas TOPS-20.\nTOPS-20 started life as a research project at Bolt, Beranek, and Newman\n(BBN) around 1970. BBN took the business-oriented DEC PDP-10 computer\nrunning TOPS-10, added a hardware memory-paging system to implement\nvirtual memory, and wrote a new operating system for that computer to take\nadvantage of the new hardware features. The result was TENEX, a general-\npurpose timesharing system. DEC then purchased the rights to TENEX and",
  "created a new computer with a built-in hardware pager. The resulting system\nwas the DECSYSTEM-20 and the TOPS-20 operating system.\nTOPS-20 had an advanced command-line interpreter that provided help as\nneeded to users. That, in combination with the power of the computer and\nits reasonable price, made the DECSYSTEM-20 the most popular time-sharing\nsystem of its time. In 1984, DEC stopped work on its line of 36-bit PDP-10\ncomputers to concentrate on 32-bit VAX systems running VMS.\n20.11 CP/M and MS/DOS\nEarly hobbyist computers were typically built from kits and ran a single\nprogram at a time. The systems evolved into more advanced systems as\ncomputer components improved. An early “standard” operating system for\nthese computers of the 1970s was CP/M, short for Control Program/Monitor,",
  "written by Gary Kindall of Digital Research, Inc. CP/M ran primarily on the\nﬁrst “personal computer” CPU, the 8-bit Intel 8080. CP/M originally supported\nonly 64 KB of memory and ran only one program at a time. Of course, it was\ntext-based, with a command interpreter. The command interpreter resembled\nthose in other operating systems of the time, such as the TOPS-10 from DEC.\nWhen IBM entered the personal computer business, it decided to have Bill\nGates and company write a new operating system for its 16-bit CPU of choice\n—the Intel 8086. This operating system, MS-DOS, was similar to CP/M but\nhad a richer set of built-in commands, again mostly modeled after TOPS-10.\nMS-DOS became the most popular personal-computer operating system of its",
  "time, starting in 1981 and continuing development until 2000. It supported\n640 KB of memory, with the ability to address “extended” and “expanded”\nmemory to get somewhat beyond that limit. It lacked fundamental current\noperating-system features, however, especially protected memory. 902\nChapter 20\nInﬂuential Operating Systems\n20.12 Macintosh Operating System and Windows\nWith the advent of 16-bit CPUs, operating systems for personal computers\ncould become more advanced, feature rich, and usable. The Apple Macintosh\ncomputer was arguably the ﬁrst computer with a GUI designed for home users.\nIt was certainly the most successful for a while, starting at its launch in 1984.\nIt used a mouse for screen pointing and selecting and came with many utility",
  "programs that took advantage of the new user interface. Hard-disk drives were\nrelatively expensive in 1984, so it came only with a 400-KB-capacity ﬂoppy\ndrive by default.\nThe original Mac OS ran only on Apple computers and slowly was\neclipsed by Microsoft Windows (starting with Version 1.0 in 1985), which\nwas licensed to run on many different computers from a multitude of\ncompanies. As microprocessor CPUs evolved to 32-bit chips with advanced\nfeatures, such as protected memory and context switching, these operating\nsystems added features that had previously been found only on mainframes\nand minicomputers. Over time, personal computers became as powerful as\nthose systems and more useful for many purposes. Minicomputers died",
  "out, replaced by general and special-purpose “servers.” Although personal\ncomputers continue to increase in capacity and performance, servers tend to\nstay ahead of them in amount of memory, disk space, and number and speed of\navailable CPUs. Today, servers typically run in data centers or machine rooms,\nwhile personal computers sit on or next to desks and talk to each other and\nservers across a network.\nThe desktop rivalry between Apple and Microsoft continues today, with\nnew versions of Windows and Mac OS trying to outdo each other in features,\nusability, and application functionality. Other operating systems, such as\nAmigaOS and OS/2, have appeared over time but have not been long-term\ncompetitors to the two leading desktop operating systems. Meanwhile, Linux",
  "in its many forms continues to gain in popularity among more technical users\n—and even with nontechnical users on systems like the One Laptop per Child\n(OLPC) children’s connected computer network (http://laptop.org/).\n20.13 Mach\nThe Mach operating system traces its ancestry to the Accent operating system\ndeveloped at Carnegie Mellon University (CMU). Mach’s communication\nsystem and philosophy are derived from Accent, but many other signiﬁcant\nportions of the system (for example, the virtual memory system and task and\nthread management) were developed from scratch.\nWork on Mach began in the mid 1980’s and the operating system was\ndesigned with the following three critical goals in mind:\n1. Emulate 4.3 BSD UNIX so that the executable ﬁles from a UNIX system can\nrun correctly under Mach.",
  "run correctly under Mach.\n2. Be a modern operating system that supports many memory models, as\nwell as parallel and distributed computing.\n3. Have a kernel that is simpler and easier to modify than 4.3 BSD. 20.13\nMach\n903\nMach’s development followed an evolutionary path from BSD UNIX sys-\ntems. Mach code was initially developed inside the 4.2BSD kernel, with BSD\nkernel components replaced by Mach components as the Mach components\nwere completed. The BSD components were updated to 4.3BSD when that\nbecame available. By 1986, the virtual memory and communication subsys-\ntems were running on the DEC VAX computer family, including multiprocessor\nversions of the VAX. Versions for the IBM RT/PC and for SUN 3 workstations\nfollowed shortly. Then, 1987 saw the completion of the Encore Multimax and",
  "Sequent Balance multiprocessor versions, including task and thread support,\nas well as the ﬁrst ofﬁcial releases of the system, Release 0 and Release 1.\nThrough Release 2, Mach provided compatibility with the corresponding\nBSD systems by including much of BSD’s code in the kernel. The new features\nand capabilities of Mach made the kernels in these releases larger than the\ncorresponding BSD kernels. Mach 3 moved the BSD code outside the kernel,\nleaving a much smaller microkernel. This system implements only basic\nMach features in the kernel; all UNIX-speciﬁc code has been evicted to run\nin user-mode servers. Excluding UNIX-speciﬁc code from the kernel allows\nthe replacement of BSD with another operating system or the simultaneous",
  "execution of multiple operating-system interfaces on top of the microkernel. In\naddition to BSD, user-mode implementations have been developed for DOS, the\nMacintosh operating system, and OSF/1. This approach has similarities to the\nvirtual machine concept, but here the virtual machine is deﬁned by software\n(the Mach kernel interface), rather than by hardware. With Release 3.0, Mach\nbecame available on a wide variety of systems, including single-processor SUN,\nIntel, IBM, and DEC machines and multiprocessor DEC, Sequent, and Encore\nsystems.\nMach was propelled to the forefront of industry attention when the Open\nSoftware Foundation (OSF) announced in 1989 that it would use Mach 2.5 as\nthe basis for its new operating system, OSF/1. (Mach 2.5 was also the basis for",
  "the operating system on the NeXT workstation, the brainchild of Steve Jobs of\nApple Computer fame.) The initial release of OSF/1 occurred a year later, and\nthis system competed with UNIX System V, Release 4, the operating system\nof choice at that time among UNIX International (UI) members. OSF members\nincluded key technological companies such as IBM, DEC, and HP. OSF has since\nchanged its direction, and only DEC UNIX is based on the Mach kernel.\nUnlike UNIX, which was developed without regard for multiprocessing,\nMach incorporates multiprocessing support throughout. This support is also\nexceedingly ﬂexible, ranging from shared-memory systems to systems with\nno memory shared between processors. Mach uses lightweight processes,",
  "in the form of multiple threads of execution within one task (or address\nspace), to support multiprocessing and parallel computation. Its extensive\nuse of messages as the only communication method ensures that protection\nmechanisms are complete and efﬁcient. By integrating messages with the\nvirtual memory system, Mach also ensures that messages can be handled\nefﬁciently. Finally, by having the virtual memory system use messages to\ncommunicate with the daemons managing the backing store, Mach provides\ngreat ﬂexibility in the design and implementation of these memory-object-\nmanaging tasks. By providing low-level, or primitive, system calls from which\nmore complex functions can be built, Mach reduces the size of the kernel 904\nChapter 20\nInﬂuential Operating Systems",
  "Chapter 20\nInﬂuential Operating Systems\nwhile permitting operating-system emulation at the user level, much like IBM’s\nvirtual machine systems.\nSome previous editions of Operating System Concepts included an entire\nchapter on Mach. This chapter, as it appeared in the fourth edition, is available\non the Web (http://www.os-book.com).\n20.14 Other Systems\nThere are, of course, other operating systems, and most of them have interesting\nproperties. The MCP operating system for the Burroughs computer family\nwas the ﬁrst to be written in a system programming language. It supported\nsegmentation and multiple CPUs. The SCOPE operating system for the CDC\n6600 was also a multi-CPU system. The coordination and synchronization of\nthe multiple processes were surprisingly well designed.",
  "History is littered with operating systems that suited a purpose for a time\n(be it a long or a short time) and then, when faded, were replaced by operating\nsystems that had more features, supported newer hardware, were easier to use,\nor were better marketed. We are sure this trend will continue in the future.\nExercises\n20.1\nDiscuss what considerations the computer operator took into account\nin deciding on the sequences in which programs would be run on early\ncomputer systems that were manually operated.\n20.2\nWhat optimizations were used to minimize the discrepancy between\nCPU and I/O speeds on early computer systems?\n20.3\nConsider the page-replacement algorithm used by Atlas. In what ways\nis it different from the clock algorithm discussed in Section 9.4.5.2?\n20.4",
  "20.4\nConsider the multilevel feedback queue used by CTSS and MULTICS.\nSuppose a program consistently uses seven time units every time it\nis scheduled before it performs an I/O operation and blocks. How\nmany time units are allocated to this program when it is scheduled for\nexecution at different points in time?\n20.5\nWhat are the implications of supporting BSD functionality in user-mode\nservers within the Mach operating system?\n20.6\nWhat conclusions can be drawn about the evolution of operating\nsystems? What causes some operating systems to gain in popularity\nand others to fade?\nBibliographical Notes\nLooms and calculators are described in [Frah (2001)] and shown graphically in\n[Frauenfelder (2005)].\nThe Manchester Mark 1 is discussed by [Rojas and Hashagen (2000)], and",
  "its offspring, the Ferranti Mark 1, is described by [Ceruzzi (1998)]. Bibliography\n905\n[Kilburn et al. (1961)] and [Howarth et al. (1961)] examine the Atlas\noperating system.\nThe XDS-940 operating system is described by [Lichtenberger and Pirtle\n(1965)].\nThe THE operating system is covered by [Dijkstra (1968)] and by [McKeag\nand Wilson (1976)].\nThe Venus system is described by [Liskov (1972)].\n[Brinch-Hansen (1970)] and [Brinch-Hansen (1973)] discuss the RC 4000\nsystem.\nThe Compatible Time-Sharing System (CTSS) is presented by [Corbato et al.\n(1962)].\nThe MULTICS operating system is described by [Corbato and Vyssotsky\n(1965)] and [Organick (1972)].\n[Mealy et al. (1966)] presented the IBM/360. [Lett and Konigsford (1968)]\ncover TSS/360.",
  "cover TSS/360.\nCP/67 is described by [Meyer and Seawright (1970)] and [Parmelee et al.\n(1972)].\nDEC VMS is discussed by [Kenah et al. (1988)], and TENEX is described by\n[Bobrow et al. (1972)].\nA description of the Apple Macintosh appears in [Apple (1987)]. For more\ninformation on these operating systems and their history, see [Freiberger and\nSwaine (2000)].\nThe Mach operating system and its ancestor, the Accent operating sys-\ntem, are described by [Rashid and Robertson (1981)]. Mach’s communi-\ncation system is covered by [Rashid (1986)], [Tevanian et al. (1989)], and\n[Accetta et al. (1986)]. The Mach scheduler is described in detail by [Tevanian\net al. (1987a)] and [Black (1990)]. An early version of the Mach shared-\nmemory and memory-mapping system is presented by [Tevanian et al.",
  "(1987b)]. A good resource describing the Mach project can be found at\nhttp://www.cs.cmu.edu/afs/cs/project/mach/public/www/mach.html.\n[McKeag and Wilson (1976)] discuss the MCP operating system for the\nBurroughs computer family as well as the SCOPE operating system for the CDC\n6600.\nBibliography\n[Accetta et al. (1986)]\nM. Accetta, R. Baron, W. Bolosky, D. B. Golub, R. Rashid,\nA. Tevanian, and M. Young, “Mach: A New Kernel Foundation for UNIX\nDevelopment”, Proceedings of the Summer USENIX Conference (1986), pages\n93–112.\n[Apple (1987)]\nApple Technical Introduction to the Macintosh Family.\nAddison-\nWesley (1987).\n[Black (1990)]\nD. L. Black, “Scheduling Support for Concurrency and Parallelism\nin the Mach Operating System”, IEEE Computer, Volume 23, Number 5 (1990),\npages 35–43. 906",
  "pages 35–43. 906\nChapter 20\nInﬂuential Operating Systems\n[Bobrow et al. (1972)]\nD. G. Bobrow, J. D. Burchﬁel, D. L. Murphy, and R. S. Tom-\nlinson, “TENEX, a Paged Time Sharing System for the PDP-10”, Communications\nof the ACM, Volume 15, Number 3 (1972).\n[Brinch-Hansen (1970)]\nP. Brinch-Hansen, “The Nucleus of a Multiprogram-\nming System”, Communications of the ACM, Volume 13, Number 4 (1970), pages\n238–241 and 250.\n[Brinch-Hansen (1973)]\nP. Brinch-Hansen, Operating System Principles, Prentice\nHall (1973).\n[Ceruzzi (1998)]\nP. E. Ceruzzi, A History of Modern Computing, MIT Press (1998).\n[Corbato and Vyssotsky (1965)]\nF. J. Corbato and V. A. Vyssotsky, “Introduction\nand Overview of the MULTICS System”, Proceedings of the AFIPS Fall Joint\nComputer Conference (1965), pages 185–196.",
  "Computer Conference (1965), pages 185–196.\n[Corbato et al. (1962)]\nF. J. Corbato, M. Merwin-Daggett, and R. C. Daley, “An\nExperimental Time-Sharing System”, Proceedings of the AFIPS Fall Joint Computer\nConference (1962), pages 335–344.\n[Dijkstra (1968)]\nE. W. Dijkstra, “The Structure of the THE Multiprogramming\nSystem”, Communications of the ACM, Volume 11, Number 5 (1968), pages\n341–346.\n[Frah (2001)]\nG. Frah, The Universal History of Computing, John Wiley and Sons\n(2001).\n[Frauenfelder (2005)]\nM. Frauenfelder, The Computer—An Illustrated History,\nCarlton Books (2005).\n[Freiberger and Swaine (2000)]\nP. Freiberger and M. Swaine, Fire in the Valley—\nThe Making of the Personal Computer, McGraw-Hill (2000).\n[Howarth et al. (1961)]\nD. J. Howarth, R. B. Payne, and F. H. Sumner, “The",
  "Manchester University Atlas Operating System, Part II: User’s Description”,\nComputer Journal, Volume 4, Number 3 (1961), pages 226–229.\n[Kenah et al. (1988)]\nL. J. Kenah, R. E. Goldenberg, and S. F. Bate, VAX/VMS\nInternals and Data Structures, Digital Press (1988).\n[Kilburn et al. (1961)]\nT. Kilburn, D. J. Howarth, R. B. Payne, and F. H. Sumner,\n“The Manchester University Atlas Operating System, Part I: Internal Organiza-\ntion”, Computer Journal, Volume 4, Number 3 (1961), pages 222–225.\n[Lett and Konigsford (1968)]\nA. L. Lett and W. L. Konigsford, “TSS/360: A\nTime-Shared Operating System”, Proceedings of the AFIPS Fall Joint Computer\nConference (1968), pages 15–28.\n[Lichtenberger and Pirtle (1965)]\nW. W. Lichtenberger and M. W. Pirtle, “A",
  "W. W. Lichtenberger and M. W. Pirtle, “A\nFacility for Experimentation in Man-Machine Interaction”, Proceedings of the\nAFIPS Fall Joint Computer Conference (1965), pages 589–598.\n[Liskov (1972)]\nB. H. Liskov, “The Design of the Venus Operating System”,\nCommunications of the ACM, Volume 15, Number 3 (1972), pages 144–149.\n[McKeag and Wilson (1976)]\nR. M. McKeag and R. Wilson, Studies in Operating\nSystems, Academic Press (1976). Bibliography\n907\n[Mealy et al. (1966)]\nG. H. Mealy, B. I. Witt, and W. A. Clark, “The Functional\nStructure of OS/360”, IBM Systems Journal, Volume 5, Number 1 (1966), pages\n3–11.\n[Meyer and Seawright (1970)]\nR. A. Meyer and L. H. Seawright, “A Virtual\nMachine Time-Sharing System”, IBM Systems Journal, Volume 9, Number 3\n(1970), pages 199–218.\n[Organick (1972)]",
  "(1970), pages 199–218.\n[Organick (1972)]\nE. I. Organick, The Multics System: An Examination of Its\nStructure, MIT Press (1972).\n[Parmelee et al. (1972)]\nR. P. Parmelee, T. I. Peterson, C. C. Tillman, and D. Hat-\nﬁeld, “Virtual Storage and Virtual Machine Concepts”, IBM Systems Journal,\nVolume 11, Number 2 (1972), pages 99–130.\n[Rashid (1986)]\nR. F. Rashid, “From RIG to Accent to Mach: The Evolution of a\nNetwork Operating System”, Proceedings of the ACM/IEEE Computer Society, Fall\nJoint Computer Conference (1986), pages 1128–1137.\n[Rashid and Robertson (1981)]\nR. Rashid and G. Robertson, “Accent: A Com-\nmunication-Oriented Network Operating System Kernel”, Proceedings of the\nACM Symposium on Operating System Principles (1981), pages 64–75.\n[Rojas and Hashagen (2000)]",
  "[Rojas and Hashagen (2000)]\nR. Rojas and U. Hashagen, The First Computers—\nHistory and Architectures, MIT Press (2000).\n[Tevanian et al. (1987a)]\nA. Tevanian, Jr., R. F. Rashid, D. B. Golub, D. L. Black,\nE. Cooper, and M. W. Young, “Mach Threads and the Unix Kernel: The Battle\nfor Control”, Proceedings of the Summer USENIX Conference (1987).\n[Tevanian et al. (1987b)]\nA. Tevanian, Jr., R. F. Rashid, M. W. Young, D. B.\nGolub, M. R. Thompson, W. Bolosky, and R. Sanzi, “A UNIX Interface for\nShared Memory and Memory Mapped Files Under Mach”, Technical report,\nCarnegie-Mellon University (1987).\n[Tevanian et al. (1989)]\nA. Tevanian, Jr., and B. Smith, “Mach: The Model for\nFuture Unix”, Byte (1989).  Credits\n• Figure 1.11: From Hennesy and Patterson, Computer Architecture: A Quanti-",
  "tative Approach, Third Edition, C⃝2002, Morgan Kaufmann Publishers, Figure\n5.3, p. 394. Reprinted with permission of the publisher.\n• Figure 6.24 adapted with permission from Sun Microsystems, Inc.\n• Figure 9.18: From IBM Systems Journal, Vol. 10, No. 3,\nC⃝1971, Interna-\ntional Business Machines Corporation. Reprinted by permission of IBM\nCorporation.\n• Figure 12.9: From Lefﬂer/McKusick/Karels/Quarterman, The Design and\nImplementation of the 4.3BSD UNIX Operating System,\nC⃝1989 by Addison-\nWesley Publishing Co., Inc., Reading, Massachusetts. Figure 7.6, p. 196.\nReprinted with permission of the publisher.\n• Figure 13.4: From Pentium Processor User’s Manual: Architecture and Pro-\ngramming Manual, Volume 3, Copyright 1993. Reprinted by permission of\nIntel Corporation.",
  "Intel Corporation.\n• Figures 17.5, 17.6, and 17.8: From Halsall, Data Communications, Computer\nNetworks, and Open Systems, Third Edition,\nC⃝1992, Addison-Wesley Pub-\nlishing Co., Inc., Reading, Massachusetts. Figure 1.9, p. 14, Figure 1.10, p.\n15, and Figure 1.11, p. 18. Reprinted with permission of the publisher.\n• Figure 6.14: From Khanna/Sebree/Zolnowsky, “Realtime Scheduling in\nSunOS 5.0,” Proceedings of Winter USENIX, January 1992, San Francisco,\nCalifornia. Derived with permission of the authors.\n909  A\naccess-control lists (ACLs), 832\nACLs (access-control lists), 832\nACPI (advanced conﬁguration and\npower interface), 862\naddress space layout randomization \n(ASLR), 832\nadmission-control algorithms, 286\nadvanced conﬁguration and power\ninterface (ACPI), 862\nadvanced encryption standard",
  "advanced encryption standard\n(AES), 677\nadvanced local procedure call\n(ALPC), 135, 854\nALPC (advanced local procedure\ncall), 135, 854\nAMD64 architecture, 387\nAmdahl’s Law, 167\nAMD virtualization technology\n(AMD-V), 720\nAndroid operating system, 85–86\nAPI (application program interface),\n63–64\nApple iPad, 60, 84\napplication containment, 713,\n727–728\nAqua interface, 59, 84\nARM architecture, 388\narrays, 31\nASIDs (address-space identiﬁers),\n374\nASLR (address space layout\nrandomization), 832\nassembly language, 77\nasynchronous threading, 172\naugmented-reality applications, 36\nauthentication:\nmultifactor, 689\nautomatic working-set trimming, 446\nB\nbackground processes, 74–75, 115,\n296\nbalanced binary search trees, 33\nbinary search trees, 33\nbinary translation, 718–720\nbinary trees, 33\nbitmaps, 34",
  "binary trees, 33\nbitmaps, 34\nbourne-Again shell (bash), 789\nbridging, 732\nbugs, 66\nC\nCFQ (Completely Fair Queueing),\n817\nchildren, 33\nchipsets, 836\nChrome, 123\nCIFS (common internet ﬁle system),\n871\ncircularly linked lists, 32\nclient(s):\nthin, 35\nclient-server model, 854–855\nclock algorithm, 418–419\nclones, 715\ncloud computing, 41–42, 716\nCocoa Touch, 84\n911\nIndex code integrity module (Windows 7),\n832\nCOM (component object model), 873\ncommon internet ﬁle system (CIFS),\n871\nCompletely Fair Queueing (CFQ),\n817\ncomputational kernels, 835–836\ncomputer environments:\ncloud computing, 41–42\ndistributed systems, 37–38\nmobile computing, 36–37\nreal-time embedded systems, 43\nvirtualization, 40–41\ncomputing:\nmobile, 36–37\nconcurrency, 166\nConcurrency Runtime (ConcRT),\n297, 880–881",
  "Concurrency Runtime (ConcRT),\n297, 880–881\ncondition variables, 879\nconﬂict phase (of dispatch latency),\n285\ncontainers, 728\ncontrol partitions, 723\ncoupling, symmetric, 17\nCPU scheduling:\nreal-time, 283–290\nearliest-deadline-ﬁrst scheduling,\n288–289\nand minimizing latency, 283–285\nPOSIX real-time scheduling, 290\npriority-based scheduling,\n285–287\nproportional share scheduling,\n289–290\nrate-monotonic scheduling,\n287–288\nvirtual machines, 729\ncritical-section problem:\nand mutex locks, 212–213\nD\nDalvik virtual machine, 86\ndata parallelism, 168–169\ndefense in depth, 689\ndesktop window manager (DWM),\n831\ndevice objects, 855\nDigital Equipment Corporation\n(DEC), 379\ndigital signatures, 832\nDirectCompute, 835\ndiscovery protocols, 39\ndisk(s):\nsolid-state, 469\ndispatcher, 294\nDMA controller, 595",
  "dispatcher, 294\nDMA controller, 595\ndoubly linked lists, 32\ndriver objects, 855\nDWM (desktop window manager),\n831\ndynamic conﬁgurations, 837, 838\nE\nearliest-deadline-ﬁrst (EDF)\nscheduling, 288–289\nEC2, 41\nEDF (earliest-deadline-ﬁrst)\nscheduling, 288–289\nefﬁciency, 837\nemulation, 40, 727\nemulators, 713\nencryption:\npublic-key, 678\nenergy efﬁciency, 837\nErlang language, 241–242\nevent latency, 283–284\nevent-pair objects, 855\nexit() system call, 120, 121\n912\nIndex ext2 (second extended ﬁle system),\n811\next3 (third extended ﬁle system),\n811–813\next4 (fourth extended ﬁle system),\n811\nextended ﬁle attributes, 505\nextensibility, 736\nF\nfast-user switching, 863–864\nFIFO, 32\nﬁle info window (Mac OS X), 505\nﬁle replication, 767\nﬁle systems:\nWindows 7, see Windows 7\nforeground processes, 115, 296",
  "foreground processes, 115, 296\nfork-join strategy, 172\nfourth extended ﬁle system (ext4),\n811\nG\nGCD (Grand Central Dispatch),\n182–183\ngeneral trees, 33\ngestures, 60\nglobal positioning system \n(GPS), 36\nGNOME desktop, 60\nGPS (global positioning system), 36\nGrand Central Dispatch (GCD),\n182–183\ngranularity, minimum, 797\ngraphics shaders, 835\nguard pages, 847\nGUIs (graphical user interfaces),\n59–62\nH\nHadoop, 765\nHadoop distributed ﬁle system\n(HDFS), 767\nhandle tables, 844\nhands-on computer systems, 20\nhardware:\nvirtual machines, 720–721\nhash collisions, 471\nhash functions, 33–34\nhash maps, 471\nHDFS (Hadoop distributed ﬁle\nsystem), 767\nhibernation, 860–861\nhybrid cloud, 42\nhybrid operating systems, 83–86\nAndroid, 85–86\niOS, 84–85\nMac OS X, 84\nhypercalls, 726\nhypervisors, 712\ntype 0, 723–724",
  "hypercalls, 726\nhypervisors, 712\ntype 0, 723–724\ntype 1, 724–725\ntype 2, 725\nI\nIA-32 architecture, 384–387\npaging in, 385–387\nsegmentation in, 384–385\nIA-64 architecture, 387\nIaaS (infrastructure as a service), \n42\nidle threads, 840\nIDSs (intrusion-detection systems),\n691–694\nimperative languages, 241\nimpersonation, 853\nimplicit threading, 177–183\nIndex\n913 Grand Central Dispatch (GCD),\n182–183\nOpenMP and, 181–182\nthread pools and, 179–181\ninfrastructure as a service (IaaS), 42\nIntel processors:\nIA-32 architecture, 384–387\nIA-64 architecture, 387\ninterface(s):\nchoice of, 61–62\nInternet Key Exchange (IKE), 682\ninterpretation, 40\ninterpreted languages, 727\ninterrupt latency, 284–285\ninterrupt service routines (ISRs), 840\nI/O (input/output):\nvirtual machines, 731–732",
  "I/O (input/output):\nvirtual machines, 731–732\niOS operating system, 84–85\nI/O system(s):\napplication interface:\nvectored I/O, 603–604\nIP (Internet Protocol), 681–683\niPad, see Apple iPad\nISRs (interrupt service routines), 840\nJ\nJava Virtual Machine (JVM), 107,\n726, 736–737\njournaling ﬁle systems, 569–570\njust-in-time (JIT) compilers, 727\nJVM, see Java Virtual Machine\nK\nK Desktop Environment (KDE), 60\nkernel(s):\ncomputational, 835\nkernel code, 96\nkernel data structures, 31–34\narrays, 31\nbitmaps, 34\nhash functions and maps, 33–34\nlists, 31–33\nqueues, 32\nstacks, 32\ntrees, 31–33\nkernel environment, 84\nKernel-Mode Driver Framework\n(KMDF), 856\nkernel-mode threads (KT), 844\nkernel modules:\nLinux, 96–101\nkernel transaction manager (KTM),\n862\nKMDF (Kernel-Mode Driver\nFramework), 856",
  "862\nKMDF (Kernel-Mode Driver\nFramework), 856\nKT (kernel-mode threads), 844\nKTM (kernel transaction manager),\n862\nL\nlatency:\nin real-time systems, 283–285\ntarget, 797\nleft child, 33\nLFH design, 883–884\nLIFO, 32\nLinux:\nkernel modules, 96–101\nLinux system(s):\nobtaining page size on, 370\nlists, 31–32\nlive migration (virtual machines),\n716, 733–735\nlock(s):\nmutex, 212–214\nloosely-coupled systems, 17\nlove bug virus, 694\nlow-fragmentation heap (LFH)\ndesign, 883–884\nLPCs (local procedure calls), 834\n914\nIndex M\nMac OS X operating system, 84\nmain memory:\npaging for management of:\nand Oracle SPARC Solaris, 383\nmemory:\ntransactional, 239–240\nmemory leaks, 101\nmemory management:\nwith virtual machines, 730–731\nmemory-management unit (MMU),\n384\nmicro TLBs, 388\nmigration:\nwith virtual machines, 733–735",
  "migration:\nwith virtual machines, 733–735\nminimum granularity, 797\nmobile computing, 36–37\nmobile systems:\nmultitasking in, 115\nswapping on, 360, 407\nmodule entry point, 97\nmodule exit point, 97\nMoore’s Law, 6, 835\nmulticore systems, 14, 16, 166\nmultifactor authentication, 689\nmultiprocessor systems (parallel\nsystems, tightly coupled\nsystems), 166\nmulti-touch hardware, 863\nmutant (Windows 7), 841\nmutex locks, 212–214\nN\nnamespaces, 793\nNAT (network address translation),\n732\nnested page tables (NPTs), 720\nnetwork address translation (NAT),\n732\nnon-uniform memory access\n(NUMA), 834\nNPTs (nested page tables), 720\nO\nOLE (object linking and\nembedding), 873\nopen-ﬁle table, 546–547\nOpenMP, 181–182, 240–241\nOpenSolaris, 46\noperating system(s):\nhybrid systems, 83–86\nportability of, 836–837",
  "hybrid systems, 83–86\nportability of, 836–837\nOracle SPARC Solaris, 383\nOrange Book, 832\nOSI model, 757–758\nOSI Reference Model, 682\novercommitment, 729\nP\nPaaS (platform as a service), 42\npage address extension (PAE), 396\npage directory pointer table, 386\npage-frame number (PFN) database,\n850–851\npage-table entries (PTEs), 847\npaging:\nand Oracle SPARC Solaris, 383\nparallelism, 166, 168–169\nparavirtualization, 713, 725–726\npartition(s):\ncontrol, 723\nPC systems, 863\nPDAs (personal digital assistants), 11\nperiodic processes, 286\nperiodic task rate, 286\npersonal computer (PC) systems, 863\npersonalities, 83\nPFF (page-fault-frequency), 429–430\nIndex\n915 PFN database, 850–851\nplatform as a service (PaaS), 42\npop, 32\nPOSIX:\nreal-time scheduling, 290\nPOST (power-on self-test), 862",
  "POST (power-on self-test), 862\npower manager (Windows 7),\n860–861\npower-on self-test (POST), 862\npriority-based scheduling, 285–287\nprivate cloud, 42\nprivilege levels, 23\nprocedural languages, 241\nprocess(es):\nbackground, 74–75, 115, 296\nforeground, 115, 296\nsystem, 8\nprocessor groups, 835\nprocess synchronization:\nalternative approaches to, 238–242\nfunctional programming\nlanguages, 241–242\nOpenMP, 240–241\ntransactional memory, 239–240\ncritical-section problem:\nsoftware solution to, 212–213\nprogramming-environment\nvirtualization, 713, 726–727\nproportional share scheduling,\n289–290\nprotection domain, 721\nprotocols:\ndiscovery, 39\npseudo-device driver, 730–731\nPTEs (page-table entries), 847\nPTE tables, 847\nPthreads:\nthread cancellation in, 186–187\npublic cloud, 41\npublic-key encryption, 678",
  "public cloud, 41\npublic-key encryption, 678\npush, 32\nR\nRAID sets, 868\nrate, periodic task, 286\nrate-monotonic scheduling, 287–288\nrate-monotonic scheduling\nalgorithm, 287–288\nRC4, 677\nRDP, 717\nreal-time CPU scheduling, 283–290\nearliest-deadline-ﬁrst scheduling,\n288–289\nand minimizing latency, 283–285\nPOSIX real-time scheduling, 290\npriority-based scheduling, 285–287\nproportional share scheduling,\n289–290\nrate-monotonic scheduling, 287–288\nred-black trees, 35\nresume, 715\nright child, 33\nROM (read-only memory), 93, 480\nrouters, 754\nRR scheduling algorithm, 271–273\nS\nSaaS (software as a service), 42\nScala language, 241–242\nscheduling:\nearliest-deadline-ﬁrst, 288–289\npriority-based, 285–287\nproportional share, 289–290\nrate-monotonic, 287–288\nSSDs and, 478\nSCM (Service Control Manager), 860",
  "SSDs and, 478\nSCM (Service Control Manager), 860\nsecond extended ﬁle system (ext2),\n811\nsecurity identity (SID), 853\nsecurity tokens, 853\nService Control Manager (SCM), 860\n916\nIndex services, operating system, 115\nsession manager subsystem (SMSS),\n862\nSID (security identity), 853\nsingly linked lists, 32\nSJF scheduling algorithm, 267–270\nSkype, 40\nslim reader-writer (SRW) locks, 879\nSLOB allocator, 439\nSLUB allocator, 439\nSMB (server-message-block), 871\nSMSS (session manager subsystem),\n862\nsoftware as a service (SaaS), 42\nsolid-state disks (SSDs), 11, 469, 478\nSPARC, 383\nSRM (security reference monitor),\n858–859\nSRW (slim reader-writer) locks, 879\nSSTF scheduling algorithm, 474–475\nstandard swapping, 358–360\nstorage:\nthread-local, 187\nstorage management:\nwith virtual machines, 732–733",
  "with virtual machines, 732–733\nsubsystems, 135\nsuperuser, 688\nSurface Computer, 863\nsuspended state, 715\nswapping:\non mobile systems, 360, 407\nstandard, 358–360\nswitching:\nfast-user, 863–864\nsymmetric coupling, 17\nsymmetric encryption algorithm, 676\nsynchronous threading, 172\nSYSGEN, 91–92\nsystem daemons, 8\nsystem-development time, 715\nsystem hive, 861\nsystem processes, 8, 844–845\nsystem restore point, 861\nT\ntarget latency, 797\ntask parallelism, 168–169\nTEBs (thread environment blocks),\n880\nterminal applications, 96\nterminal server systems, 864\nthin clients, 35\nthird extended ﬁle system (ext3),\n811–813\nthreads:\nimplicit threading, 177–183\nthread attach, 853\nthread environment blocks (TEBs),\n880\nthread-local storage, 187\nthread pools, 179–181\nthunking, 834\ntime sharing (multitasking), 115",
  "thunking, 834\ntime sharing (multitasking), 115\ntime slice, 796\ntimestamp counters (TSCs),\n840–841\ntouch screen (touchscreen\ncomputing), 5, 60\ntransactions:\natomic, 210\ntransactional memory, 239–240\nTransmission Control\nProtocol/Internet Protocol\n(TCP/IP), 758–761\ntrap-and-emulate method, 717–718\ntrees, 33, 35\nTSCs (timestamp counters), 840–841\ntype 0 hypervisors, 712, 723–724\ntype 1 hypervisors, 712, 724–725\ntype 2 hypervisors, 713, 725\nIndex\n917 U\nUAC (User Account Control), 701\nUI (user interface), 52–55\nUMDF (User-Mode Driver\nFramework), 856\nUMS, see user-mode scheduling\nUSBs (universal serial buses), 469\nUser Account Control (UAC), 701\nuser mode, 787\nUser-Mode Driver Framework\n(UMDF), 856\nuser-mode scheduling (UMS),\n296–297, 835, 880–881\nuser-mode threads (UT), 844",
  "296–297, 835, 880–881\nuser-mode threads (UT), 844\nUT (user-mode threads), 844\nV\nVACB (virtual address control\nblock), 857\nvariables:\ncondition, 879\nVAX minicomputer, 379–380\nVCPU (virtual CPU), 717\nvectored I/O, 603–604\nvirtual CPU (VCPU), 717\nvirtualization, 40–41\nadvantages and disadvantages of,\n714–716\nand application containment,\n727–728\nand emulation, 727\nand operating-system components,\n728–735\nCPU scheduling, 729\nI/O, 731–732\nlive migration, 733–735\nmemory management, 730–731\nstorage management, 732–733\npara-, 725–726\nprogramming-environment,\n726–727\nvirtual machines, 711–738. See also\nvirtualization\nadvantages and disadvantages of,\n714–716\nand binary translation, 718–720\nexamples, 735–737\nfeatures of, 715–717\nand hardware assistance, 720–721\nhistory of, 713–714",
  "history of, 713–714\nJava Virtual Machine, 736–737\nlife cycle of, 722–723\ntrap-and-emulate systems,\n717–718\ntype 0 hypervisors, 723–724\ntype 1 hypervisors, 724–725\ntype 2 hypervisors, 725\nVMware, 735–736\nvirtual machine control structures\n(VMCSs), 721\nvirtual machine manager (VMM),\n22–23, 41, 712\nvirtual machine sprawl, 723\nVMCSs (virtual machine control\nstructures), 721\nVMM, see virtual machine manager\nVM manager, 846–852\nVMware, 714, 735–736\nW\nwait() system call, 120–122\nWin32 API, 875\nWindows 7:\ndynamic device support, 837, 838\nand energy efﬁciency, 837\nfast-user switching with, 863–864\nsecurity in, 700–701\n918\nIndex synchronization in, 833–834,\n878–879\nterminal services, 863–864\nuser-mode scheduling in, 296–297\nWindows executive:\nbooting, 862–863\npower manager, 860–861",
  "booting, 862–863\npower manager, 860–861\nWindows group policy, 875\nWindows Task Manager, 87, 88\nWindows Vista, 830\nsecurity in, 700\nsymbolic links in, 869–870\nWindows XP, 830\nWinsock, 881\nWorkstation (VMWare), 735–736\nX\nx86-64 architecture, 387\nXen, 714\nZ\nzones, 728\nIndex\n919"
]